no FSDP port specified; using open port for FSDP: 36761
seed: 0
exp_name: pythia2.8b_sfted0_dpo3_seed0
batch_size: 32
eval_batch_size: 16
debug: false
fsdp_port: 36761
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933
lr: 4.0e-07
gradient_accumulation_steps: 2
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 3
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 12000
minimum_log_interval_secs: 1.0
revision: main
model:
  name_or_path: EleutherAI/pythia-2.8b
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: bfloat16
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-206-39:.cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933
================================================================================
building policy
building reference model
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240319_013736-sb8hm4my
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia2.8b_sfted0_dpo3_seed0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: üöÄ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/sb8hm4my
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer EleutherAI/pythia-2.8b
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2781/5103 [00:00<00:00, 27796.24it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5103/5103 [00:00<00:00, 27373.50it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   1%|          | 487/96256 [00:00<00:40, 2363.29it/s]Processing HH static:   3%|‚ñé         | 3367/96256 [00:00<00:07, 13186.71it/s]Processing HH static:   6%|‚ñã         | 6239/96256 [00:00<00:04, 18916.82it/s]Processing HH static:   9%|‚ñâ         | 9140/96256 [00:00<00:03, 22399.49it/s]Processing HH static:  13%|‚ñà‚ñé        | 12037/96256 [00:00<00:03, 24566.57it/s]Processing HH static:  16%|‚ñà‚ñå        | 14922/96256 [00:00<00:03, 25937.19it/s]Processing HH static:  19%|‚ñà‚ñä        | 17834/96256 [00:00<00:02, 26934.18it/s]Processing HH static:  22%|‚ñà‚ñà‚ñè       | 20741/96256 [00:00<00:02, 27593.11it/s]Processing HH static:  25%|‚ñà‚ñà‚ñç       | 23658/96256 [00:01<00:02, 28074.08it/s]Processing HH static:  28%|‚ñà‚ñà‚ñä       | 26572/96256 [00:01<00:02, 28397.83it/s]Processing HH static:  31%|‚ñà‚ñà‚ñà       | 29488/96256 [00:01<00:02, 28626.15it/s]Processing HH static:  34%|‚ñà‚ñà‚ñà‚ñé      | 32412/96256 [00:01<00:02, 28808.72it/s]Processing HH static:  37%|‚ñà‚ñà‚ñà‚ñã      | 35310/96256 [00:01<00:03, 19288.25it/s]Processing HH static:  40%|‚ñà‚ñà‚ñà‚ñâ      | 38166/96256 [00:01<00:02, 21360.27it/s]Processing HH static:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 41036/96256 [00:01<00:02, 23135.74it/s]Processing HH static:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 43872/96256 [00:01<00:02, 24477.74it/s]Processing HH static:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 46678/96256 [00:01<00:01, 25437.42it/s]Processing HH static:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 49460/96256 [00:02<00:01, 26097.15it/s]Processing HH static:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 52198/96256 [00:02<00:02, 19565.87it/s]Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 54874/96256 [00:02<00:01, 21223.04it/s]Processing HH static:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 57673/96256 [00:02<00:01, 22898.92it/s]Processing HH static:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 60461/96256 [00:02<00:01, 24200.45it/s]Processing HH static:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 63217/96256 [00:02<00:01, 25111.50it/s]Processing HH static:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 65981/96256 [00:02<00:01, 25818.17it/s]Processing HH static:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 68736/96256 [00:02<00:01, 26311.30it/s]Processing HH static:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 71475/96256 [00:02<00:00, 26621.51it/s]Processing HH static:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 74189/96256 [00:03<00:01, 14872.10it/s]Processing HH static:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 76307/96256 [00:04<00:02, 7541.73it/s] Processing HH static:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 77881/96256 [00:04<00:02, 6166.48it/s]Processing HH static:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 79092/96256 [00:04<00:03, 4858.96it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 80016/96256 [00:05<00:03, 4414.77it/s]Processing HH static:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80754/96256 [00:05<00:03, 4006.37it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 81352/96256 [00:05<00:03, 3795.06it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 81860/96256 [00:05<00:03, 3830.84it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82336/96256 [00:06<00:03, 3640.44it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82758/96256 [00:06<00:03, 3643.33it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83163/96256 [00:06<00:03, 3608.20it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83551/96256 [00:06<00:03, 3240.67it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83894/96256 [00:06<00:04, 3021.81it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84241/96256 [00:06<00:03, 3116.71it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84781/96256 [00:06<00:03, 3639.42it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85168/96256 [00:06<00:03, 3458.20it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85530/96256 [00:07<00:03, 3072.43it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85853/96256 [00:07<00:03, 3063.23it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86170/96256 [00:07<00:03, 2928.14it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86470/96256 [00:07<00:03, 2879.81it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86858/96256 [00:07<00:02, 3138.13it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87179/96256 [00:07<00:03, 3024.87it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87487/96256 [00:07<00:03, 2799.28it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87801/96256 [00:07<00:02, 2888.82it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88123/96256 [00:07<00:02, 2977.62it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88426/96256 [00:08<00:02, 2709.76it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88879/96256 [00:08<00:02, 3188.45it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89209/96256 [00:08<00:02, 3159.98it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89533/96256 [00:08<00:02, 2980.04it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89919/96256 [00:08<00:01, 3218.62it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90248/96256 [00:08<00:02, 2943.54it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90551/96256 [00:08<00:02, 2754.75it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90834/96256 [00:08<00:02, 2669.76it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91106/96256 [00:09<00:01, 2598.49it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91376/96256 [00:09<00:01, 2624.50it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91759/96256 [00:09<00:01, 2949.54it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92124/96256 [00:09<00:01, 3139.18it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92442/96256 [00:09<00:01, 2850.34it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 92796/96256 [00:09<00:01, 3035.78it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93107/96256 [00:09<00:01, 2972.69it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93436/96256 [00:09<00:00, 3056.86it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 93898/96256 [00:09<00:00, 3501.72it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94254/96256 [00:09<00:00, 3316.19it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94591/96256 [00:10<00:00, 3107.79it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94908/96256 [00:10<00:00, 3029.51it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95215/96256 [00:10<00:00, 2873.16it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95750/96256 [00:10<00:00, 3541.06it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96232/96256 [00:10<00:00, 3896.17it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96256/96256 [00:10<00:00, 9128.37it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:01<00:28,  1.92s/it]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:02<00:13,  1.04it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:02<00:08,  1.58it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:02<00:05,  2.06it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:02<00:04,  2.44it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:03<00:03,  2.89it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:03<00:02,  3.10it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:03<00:02,  3.31it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:03<00:01,  3.52it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:04<00:01,  3.58it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:04<00:01,  3.64it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:04<00:01,  3.73it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:05<00:00,  3.80it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:05<00:00,  3.74it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:05<00:00,  3.75it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:05<00:00,  3.72it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:05<00:00,  2.74it/s]
eval after 0: {'rewards_eval/chosen': '-0.040938', 'rewards_eval/rejected': '-0.0238', 'rewards_eval/accuracies': '0.45703', 'rewards_eval/margins': '-0.017138', 'logps_eval/rejected': '-115.63', 'logps_eval/chosen': '-135.96', 'loss/eval': '0.70447'}
train stats after 32 examples: {'rewards_train/chosen': '-0.037558', 'rewards_train/rejected': '-0.039878', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0023205', 'logps_train/rejected': '-104.53', 'logps_train/chosen': '-122.25', 'loss/train': '0.69396', 'examples_per_second': '20.719', 'grad_norm': '19', 'counters/examples': 32, 'counters/updates': 1}
train stats after 64 examples: {'rewards_train/chosen': '-0.027055', 'rewards_train/rejected': '-0.012854', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014201', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-138.96', 'loss/train': '0.70253', 'examples_per_second': '28.004', 'grad_norm': '23', 'counters/examples': 64, 'counters/updates': 2}
train stats after 96 examples: {'rewards_train/chosen': '-0.014282', 'rewards_train/rejected': '-0.039144', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024862', 'logps_train/rejected': '-114.87', 'logps_train/chosen': '-113.86', 'loss/train': '0.68362', 'examples_per_second': '30.888', 'grad_norm': '19.25', 'counters/examples': 96, 'counters/updates': 3}
train stats after 128 examples: {'rewards_train/chosen': '-0.027773', 'rewards_train/rejected': '-0.022295', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0054781', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-162.86', 'loss/train': '0.69967', 'examples_per_second': '30.941', 'grad_norm': '22.875', 'counters/examples': 128, 'counters/updates': 4}
train stats after 160 examples: {'rewards_train/chosen': '-0.051709', 'rewards_train/rejected': '-0.034893', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016816', 'logps_train/rejected': '-110.26', 'logps_train/chosen': '-149.53', 'loss/train': '0.70347', 'examples_per_second': '31.771', 'grad_norm': '20.875', 'counters/examples': 160, 'counters/updates': 5}
train stats after 192 examples: {'rewards_train/chosen': '0.0079253', 'rewards_train/rejected': '-0.0084705', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016396', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-115.87', 'loss/train': '0.68641', 'examples_per_second': '31.051', 'grad_norm': '20.125', 'counters/examples': 192, 'counters/updates': 6}
train stats after 224 examples: {'rewards_train/chosen': '-0.053772', 'rewards_train/rejected': '-0.013858', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.039914', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-127.53', 'loss/train': '0.71475', 'examples_per_second': '30.904', 'grad_norm': '20.125', 'counters/examples': 224, 'counters/updates': 7}
train stats after 256 examples: {'rewards_train/chosen': '-0.007128', 'rewards_train/rejected': '-0.048695', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041567', 'logps_train/rejected': '-150.09', 'logps_train/chosen': '-187.61', 'loss/train': '0.6762', 'examples_per_second': '31.077', 'grad_norm': '24', 'counters/examples': 256, 'counters/updates': 8}
train stats after 288 examples: {'rewards_train/chosen': '-0.014723', 'rewards_train/rejected': '-0.029431', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014708', 'logps_train/rejected': '-149.38', 'logps_train/chosen': '-125.66', 'loss/train': '0.68938', 'examples_per_second': '31.651', 'grad_norm': '20.25', 'counters/examples': 288, 'counters/updates': 9}
train stats after 320 examples: {'rewards_train/chosen': '-0.050219', 'rewards_train/rejected': '-0.022818', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027401', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-154.74', 'loss/train': '0.70912', 'examples_per_second': '29.624', 'grad_norm': '21.5', 'counters/examples': 320, 'counters/updates': 10}
train stats after 352 examples: {'rewards_train/chosen': '-0.071183', 'rewards_train/rejected': '-0.0025913', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.068592', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-124.61', 'loss/train': '0.73023', 'examples_per_second': '31.227', 'grad_norm': '21.5', 'counters/examples': 352, 'counters/updates': 11}
train stats after 384 examples: {'rewards_train/chosen': '-0.025889', 'rewards_train/rejected': '-0.013867', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012022', 'logps_train/rejected': '-111.89', 'logps_train/chosen': '-131.37', 'loss/train': '0.70297', 'examples_per_second': '32.87', 'grad_norm': '20.25', 'counters/examples': 384, 'counters/updates': 12}
train stats after 416 examples: {'rewards_train/chosen': '-0.012192', 'rewards_train/rejected': '-0.019603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0074113', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-156.48', 'loss/train': '0.6921', 'examples_per_second': '30.645', 'grad_norm': '20.25', 'counters/examples': 416, 'counters/updates': 13}
train stats after 448 examples: {'rewards_train/chosen': '-0.010684', 'rewards_train/rejected': '-0.0079015', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0027821', 'logps_train/rejected': '-121.88', 'logps_train/chosen': '-152.07', 'loss/train': '0.69697', 'examples_per_second': '31.154', 'grad_norm': '20.75', 'counters/examples': 448, 'counters/updates': 14}
skipping logging after 480 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '-0.069153', 'rewards_train/rejected': '-0.068128', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0010244', 'logps_train/rejected': '-147.86', 'logps_train/chosen': '-146.83', 'loss/train': '0.69818', 'examples_per_second': '33.363', 'grad_norm': '22', 'counters/examples': 512, 'counters/updates': 16}
train stats after 544 examples: {'rewards_train/chosen': '-0.0096344', 'rewards_train/rejected': '-0.017782', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0081471', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-149.79', 'loss/train': '0.69103', 'examples_per_second': '31.1', 'grad_norm': '21.375', 'counters/examples': 544, 'counters/updates': 17}
train stats after 576 examples: {'rewards_train/chosen': '0.0082636', 'rewards_train/rejected': '-0.040026', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04829', 'logps_train/rejected': '-116.44', 'logps_train/chosen': '-170.22', 'loss/train': '0.67191', 'examples_per_second': '31.111', 'grad_norm': '21.75', 'counters/examples': 576, 'counters/updates': 18}
train stats after 608 examples: {'rewards_train/chosen': '-0.018354', 'rewards_train/rejected': '-0.03304', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014686', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-129.1', 'loss/train': '0.68816', 'examples_per_second': '31.696', 'grad_norm': '19.375', 'counters/examples': 608, 'counters/updates': 19}
train stats after 640 examples: {'rewards_train/chosen': '-0.061057', 'rewards_train/rejected': '-0.01167', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.049387', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-151.4', 'loss/train': '0.72389', 'examples_per_second': '30.372', 'grad_norm': '22.5', 'counters/examples': 640, 'counters/updates': 20}
train stats after 672 examples: {'rewards_train/chosen': '-0.0032496', 'rewards_train/rejected': '-0.038602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035352', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-157.35', 'loss/train': '0.67823', 'examples_per_second': '32.597', 'grad_norm': '21', 'counters/examples': 672, 'counters/updates': 21}
train stats after 704 examples: {'rewards_train/chosen': '0.0047305', 'rewards_train/rejected': '0.0095764', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0048459', 'logps_train/rejected': '-108.33', 'logps_train/chosen': '-103.05', 'loss/train': '0.69814', 'examples_per_second': '32.955', 'grad_norm': '18.75', 'counters/examples': 704, 'counters/updates': 22}
train stats after 736 examples: {'rewards_train/chosen': '-0.010748', 'rewards_train/rejected': '0.021069', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.031818', 'logps_train/rejected': '-104.16', 'logps_train/chosen': '-118.28', 'loss/train': '0.71257', 'examples_per_second': '31.87', 'grad_norm': '20.25', 'counters/examples': 736, 'counters/updates': 23}
skipping logging after 768 examples to avoid logging too frequently
train stats after 800 examples: {'rewards_train/chosen': '-0.037', 'rewards_train/rejected': '-0.028218', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0087827', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-186.63', 'loss/train': '0.70007', 'examples_per_second': '30.119', 'grad_norm': '24', 'counters/examples': 800, 'counters/updates': 25}
train stats after 832 examples: {'rewards_train/chosen': '-0.033349', 'rewards_train/rejected': '-0.0018196', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.031529', 'logps_train/rejected': '-103.11', 'logps_train/chosen': '-123.02', 'loss/train': '0.71112', 'examples_per_second': '31.127', 'grad_norm': '19.625', 'counters/examples': 832, 'counters/updates': 26}
train stats after 864 examples: {'rewards_train/chosen': '-0.052517', 'rewards_train/rejected': '-0.0015159', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051002', 'logps_train/rejected': '-99.961', 'logps_train/chosen': '-112.84', 'loss/train': '0.7227', 'examples_per_second': '30.795', 'grad_norm': '19.875', 'counters/examples': 864, 'counters/updates': 27}
skipping logging after 896 examples to avoid logging too frequently
train stats after 928 examples: {'rewards_train/chosen': '-0.019107', 'rewards_train/rejected': '-0.029905', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010798', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-170.94', 'loss/train': '0.68966', 'examples_per_second': '31.019', 'grad_norm': '22.625', 'counters/examples': 928, 'counters/updates': 29}
train stats after 960 examples: {'rewards_train/chosen': '-0.043912', 'rewards_train/rejected': '-0.025333', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018579', 'logps_train/rejected': '-113.22', 'logps_train/chosen': '-119.99', 'loss/train': '0.70608', 'examples_per_second': '31.7', 'grad_norm': '19.625', 'counters/examples': 960, 'counters/updates': 30}
train stats after 992 examples: {'rewards_train/chosen': '-0.020091', 'rewards_train/rejected': '-0.01495', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0051416', 'logps_train/rejected': '-137.16', 'logps_train/chosen': '-150.7', 'loss/train': '0.6975', 'examples_per_second': '30.848', 'grad_norm': '21', 'counters/examples': 992, 'counters/updates': 31}
train stats after 1024 examples: {'rewards_train/chosen': '-0.022037', 'rewards_train/rejected': '-0.025325', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0032883', 'logps_train/rejected': '-145.02', 'logps_train/chosen': '-122.96', 'loss/train': '0.69347', 'examples_per_second': '31.932', 'grad_norm': '21', 'counters/examples': 1024, 'counters/updates': 32}
skipping logging after 1056 examples to avoid logging too frequently
train stats after 1088 examples: {'rewards_train/chosen': '-0.0047178', 'rewards_train/rejected': '-0.0077944', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0030766', 'logps_train/rejected': '-97.446', 'logps_train/chosen': '-163.69', 'loss/train': '0.69423', 'examples_per_second': '30.195', 'grad_norm': '20.5', 'counters/examples': 1088, 'counters/updates': 34}
skipping logging after 1120 examples to avoid logging too frequently
train stats after 1152 examples: {'rewards_train/chosen': '-0.034262', 'rewards_train/rejected': '-0.049745', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015483', 'logps_train/rejected': '-115.93', 'logps_train/chosen': '-138.46', 'loss/train': '0.68743', 'examples_per_second': '30.284', 'grad_norm': '20.75', 'counters/examples': 1152, 'counters/updates': 36}
train stats after 1184 examples: {'rewards_train/chosen': '-0.019523', 'rewards_train/rejected': '-0.029475', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0099521', 'logps_train/rejected': '-155.47', 'logps_train/chosen': '-156', 'loss/train': '0.69108', 'examples_per_second': '30.744', 'grad_norm': '21.25', 'counters/examples': 1184, 'counters/updates': 37}
train stats after 1216 examples: {'rewards_train/chosen': '-0.04056', 'rewards_train/rejected': '-0.042046', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0014861', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-152.89', 'loss/train': '0.69441', 'examples_per_second': '30.785', 'grad_norm': '21.625', 'counters/examples': 1216, 'counters/updates': 38}
train stats after 1248 examples: {'rewards_train/chosen': '-0.034921', 'rewards_train/rejected': '0.0084583', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043379', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-153.99', 'loss/train': '0.7183', 'examples_per_second': '31.626', 'grad_norm': '21.875', 'counters/examples': 1248, 'counters/updates': 39}
train stats after 1280 examples: {'rewards_train/chosen': '-0.020231', 'rewards_train/rejected': '-0.027871', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0076397', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-122.34', 'loss/train': '0.69141', 'examples_per_second': '30.668', 'grad_norm': '19.75', 'counters/examples': 1280, 'counters/updates': 40}
train stats after 1312 examples: {'rewards_train/chosen': '0.006314', 'rewards_train/rejected': '-0.021852', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028166', 'logps_train/rejected': '-134.74', 'logps_train/chosen': '-142.7', 'loss/train': '0.6813', 'examples_per_second': '30.705', 'grad_norm': '20.875', 'counters/examples': 1312, 'counters/updates': 41}
train stats after 1344 examples: {'rewards_train/chosen': '-0.066091', 'rewards_train/rejected': '-0.0016743', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.064417', 'logps_train/rejected': '-116.26', 'logps_train/chosen': '-224.98', 'loss/train': '0.72976', 'examples_per_second': '30.923', 'grad_norm': '25.625', 'counters/examples': 1344, 'counters/updates': 42}
train stats after 1376 examples: {'rewards_train/chosen': '-0.074646', 'rewards_train/rejected': '-0.064911', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0097346', 'logps_train/rejected': '-164.68', 'logps_train/chosen': '-172.73', 'loss/train': '0.70189', 'examples_per_second': '31.367', 'grad_norm': '24.125', 'counters/examples': 1376, 'counters/updates': 43}
skipping logging after 1408 examples to avoid logging too frequently
train stats after 1440 examples: {'rewards_train/chosen': '0.011602', 'rewards_train/rejected': '-0.018526', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030128', 'logps_train/rejected': '-118.03', 'logps_train/chosen': '-137.89', 'loss/train': '0.68115', 'examples_per_second': '31.398', 'grad_norm': '21.125', 'counters/examples': 1440, 'counters/updates': 45}
train stats after 1472 examples: {'rewards_train/chosen': '-0.031576', 'rewards_train/rejected': '-0.041938', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.010363', 'logps_train/rejected': '-146.86', 'logps_train/chosen': '-141.17', 'loss/train': '0.69016', 'examples_per_second': '30.22', 'grad_norm': '21.5', 'counters/examples': 1472, 'counters/updates': 46}
train stats after 1504 examples: {'rewards_train/chosen': '-0.011178', 'rewards_train/rejected': '-0.015505', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0043272', 'logps_train/rejected': '-114.07', 'logps_train/chosen': '-125.09', 'loss/train': '0.69391', 'examples_per_second': '32.502', 'grad_norm': '19.5', 'counters/examples': 1504, 'counters/updates': 47}
train stats after 1536 examples: {'rewards_train/chosen': '-0.0074175', 'rewards_train/rejected': '-0.032196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024778', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-148.04', 'loss/train': '0.68369', 'examples_per_second': '30.994', 'grad_norm': '20.75', 'counters/examples': 1536, 'counters/updates': 48}
train stats after 1568 examples: {'rewards_train/chosen': '0.011711', 'rewards_train/rejected': '-0.063956', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.075667', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-115.07', 'loss/train': '0.65812', 'examples_per_second': '31.167', 'grad_norm': '18', 'counters/examples': 1568, 'counters/updates': 49}
train stats after 1600 examples: {'rewards_train/chosen': '-0.017395', 'rewards_train/rejected': '-0.015588', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.001807', 'logps_train/rejected': '-161.17', 'logps_train/chosen': '-183.81', 'loss/train': '0.69775', 'examples_per_second': '30.133', 'grad_norm': '24.875', 'counters/examples': 1600, 'counters/updates': 50}
skipping logging after 1632 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '-0.015425', 'rewards_train/rejected': '-0.010418', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0050069', 'logps_train/rejected': '-137.02', 'logps_train/chosen': '-122.83', 'loss/train': '0.698', 'examples_per_second': '30.095', 'grad_norm': '20.375', 'counters/examples': 1664, 'counters/updates': 52}
skipping logging after 1696 examples to avoid logging too frequently
train stats after 1728 examples: {'rewards_train/chosen': '-0.062544', 'rewards_train/rejected': '-0.028094', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03445', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-156.49', 'loss/train': '0.71356', 'examples_per_second': '32.729', 'grad_norm': '22.125', 'counters/examples': 1728, 'counters/updates': 54}
train stats after 1760 examples: {'rewards_train/chosen': '-0.018359', 'rewards_train/rejected': '-0.037685', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019326', 'logps_train/rejected': '-97.634', 'logps_train/chosen': '-176.07', 'loss/train': '0.68673', 'examples_per_second': '31.135', 'grad_norm': '22', 'counters/examples': 1760, 'counters/updates': 55}
skipping logging after 1792 examples to avoid logging too frequently
train stats after 1824 examples: {'rewards_train/chosen': '-0.046038', 'rewards_train/rejected': '-0.00012049', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.045917', 'logps_train/rejected': '-144.67', 'logps_train/chosen': '-156.12', 'loss/train': '0.71962', 'examples_per_second': '31.6', 'grad_norm': '22.375', 'counters/examples': 1824, 'counters/updates': 57}
train stats after 1856 examples: {'rewards_train/chosen': '-0.022332', 'rewards_train/rejected': '-0.037049', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014717', 'logps_train/rejected': '-105.57', 'logps_train/chosen': '-121.3', 'loss/train': '0.68841', 'examples_per_second': '30.736', 'grad_norm': '18.625', 'counters/examples': 1856, 'counters/updates': 58}
train stats after 1888 examples: {'rewards_train/chosen': '-0.028819', 'rewards_train/rejected': '-0.012788', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016031', 'logps_train/rejected': '-96.303', 'logps_train/chosen': '-109.81', 'loss/train': '0.70317', 'examples_per_second': '30.531', 'grad_norm': '18.25', 'counters/examples': 1888, 'counters/updates': 59}
train stats after 1920 examples: {'rewards_train/chosen': '-0.043211', 'rewards_train/rejected': '-0.057195', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013984', 'logps_train/rejected': '-164.05', 'logps_train/chosen': '-123.5', 'loss/train': '0.69002', 'examples_per_second': '32.708', 'grad_norm': '20.875', 'counters/examples': 1920, 'counters/updates': 60}
skipping logging after 1952 examples to avoid logging too frequently
train stats after 1984 examples: {'rewards_train/chosen': '-0.022429', 'rewards_train/rejected': '-0.021468', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00096082', 'logps_train/rejected': '-110.23', 'logps_train/chosen': '-152.52', 'loss/train': '0.69588', 'examples_per_second': '30.779', 'grad_norm': '20.875', 'counters/examples': 1984, 'counters/updates': 62}
train stats after 2016 examples: {'rewards_train/chosen': '-0.007005', 'rewards_train/rejected': '-0.02993', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022925', 'logps_train/rejected': '-157.86', 'logps_train/chosen': '-112.88', 'loss/train': '0.68369', 'examples_per_second': '31.122', 'grad_norm': '19.875', 'counters/examples': 2016, 'counters/updates': 63}
train stats after 2048 examples: {'rewards_train/chosen': '-0.022084', 'rewards_train/rejected': '-0.023345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0012613', 'logps_train/rejected': '-144.72', 'logps_train/chosen': '-177.95', 'loss/train': '0.69542', 'examples_per_second': '31.915', 'grad_norm': '22.5', 'counters/examples': 2048, 'counters/updates': 64}
train stats after 2080 examples: {'rewards_train/chosen': '-0.016519', 'rewards_train/rejected': '-0.022961', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0064423', 'logps_train/rejected': '-132', 'logps_train/chosen': '-123.11', 'loss/train': '0.6931', 'examples_per_second': '31.091', 'grad_norm': '20.375', 'counters/examples': 2080, 'counters/updates': 65}
train stats after 2112 examples: {'rewards_train/chosen': '0.0032062', 'rewards_train/rejected': '0.0070342', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.003828', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-102.19', 'loss/train': '0.69639', 'examples_per_second': '30.999', 'grad_norm': '18', 'counters/examples': 2112, 'counters/updates': 66}
train stats after 2144 examples: {'rewards_train/chosen': '-0.03729', 'rewards_train/rejected': '-0.021085', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016205', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-112.74', 'loss/train': '0.70361', 'examples_per_second': '32.145', 'grad_norm': '19.25', 'counters/examples': 2144, 'counters/updates': 67}
skipping logging after 2176 examples to avoid logging too frequently
train stats after 2208 examples: {'rewards_train/chosen': '-0.025282', 'rewards_train/rejected': '-0.032316', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.0070336', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-136.15', 'loss/train': '0.6942', 'examples_per_second': '31.161', 'grad_norm': '20.75', 'counters/examples': 2208, 'counters/updates': 69}
train stats after 2240 examples: {'rewards_train/chosen': '0.00032035', 'rewards_train/rejected': '-0.022734', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023055', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-151.9', 'loss/train': '0.68364', 'examples_per_second': '31.268', 'grad_norm': '21.125', 'counters/examples': 2240, 'counters/updates': 70}
train stats after 2272 examples: {'rewards_train/chosen': '-0.024267', 'rewards_train/rejected': '-0.044979', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020713', 'logps_train/rejected': '-105.57', 'logps_train/chosen': '-131.12', 'loss/train': '0.68543', 'examples_per_second': '31.888', 'grad_norm': '19.25', 'counters/examples': 2272, 'counters/updates': 71}
train stats after 2304 examples: {'rewards_train/chosen': '-0.048511', 'rewards_train/rejected': '-0.02562', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022891', 'logps_train/rejected': '-93.986', 'logps_train/chosen': '-126.15', 'loss/train': '0.70709', 'examples_per_second': '30.995', 'grad_norm': '19.25', 'counters/examples': 2304, 'counters/updates': 72}
train stats after 2336 examples: {'rewards_train/chosen': '-0.035215', 'rewards_train/rejected': '0.023081', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.058296', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-136.74', 'loss/train': '0.72592', 'examples_per_second': '31.637', 'grad_norm': '21.5', 'counters/examples': 2336, 'counters/updates': 73}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2400 examples: {'rewards_train/chosen': '-0.036172', 'rewards_train/rejected': '0.0019493', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.038121', 'logps_train/rejected': '-116.45', 'logps_train/chosen': '-156.09', 'loss/train': '0.71539', 'examples_per_second': '30.333', 'grad_norm': '21.75', 'counters/examples': 2400, 'counters/updates': 75}
train stats after 2432 examples: {'rewards_train/chosen': '-0.032593', 'rewards_train/rejected': '-0.027757', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0048358', 'logps_train/rejected': '-108.56', 'logps_train/chosen': '-109.07', 'loss/train': '0.69713', 'examples_per_second': '30.576', 'grad_norm': '18.25', 'counters/examples': 2432, 'counters/updates': 76}
skipping logging after 2464 examples to avoid logging too frequently
train stats after 2496 examples: {'rewards_train/chosen': '-0.0276', 'rewards_train/rejected': '-0.023677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0039233', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-138.47', 'loss/train': '0.69661', 'examples_per_second': '31.629', 'grad_norm': '21.625', 'counters/examples': 2496, 'counters/updates': 78}
train stats after 2528 examples: {'rewards_train/chosen': '-0.037318', 'rewards_train/rejected': '-0.011651', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025668', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-159.24', 'loss/train': '0.70805', 'examples_per_second': '30.202', 'grad_norm': '21.75', 'counters/examples': 2528, 'counters/updates': 79}
train stats after 2560 examples: {'rewards_train/chosen': '-0.033151', 'rewards_train/rejected': '-0.024298', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0088532', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-119.37', 'loss/train': '0.6992', 'examples_per_second': '31.289', 'grad_norm': '20.75', 'counters/examples': 2560, 'counters/updates': 80}
skipping logging after 2592 examples to avoid logging too frequently
train stats after 2624 examples: {'rewards_train/chosen': '-0.041638', 'rewards_train/rejected': '-0.0060522', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.035586', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-163.26', 'loss/train': '0.714', 'examples_per_second': '31.407', 'grad_norm': '23.125', 'counters/examples': 2624, 'counters/updates': 82}
train stats after 2656 examples: {'rewards_train/chosen': '-0.0092203', 'rewards_train/rejected': '-0.0074972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0017231', 'logps_train/rejected': '-141.9', 'logps_train/chosen': '-138.94', 'loss/train': '0.69687', 'examples_per_second': '30.668', 'grad_norm': '22', 'counters/examples': 2656, 'counters/updates': 83}
train stats after 2688 examples: {'rewards_train/chosen': '-0.017975', 'rewards_train/rejected': '-0.039212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021236', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-144.82', 'loss/train': '0.68673', 'examples_per_second': '33.194', 'grad_norm': '20.625', 'counters/examples': 2688, 'counters/updates': 84}
train stats after 2720 examples: {'rewards_train/chosen': '-0.033657', 'rewards_train/rejected': '-0.020599', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013058', 'logps_train/rejected': '-120.15', 'logps_train/chosen': '-153.92', 'loss/train': '0.70201', 'examples_per_second': '32.267', 'grad_norm': '21.75', 'counters/examples': 2720, 'counters/updates': 85}
train stats after 2752 examples: {'rewards_train/chosen': '-0.0017028', 'rewards_train/rejected': '-0.038893', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03719', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-129.3', 'loss/train': '0.67728', 'examples_per_second': '32', 'grad_norm': '19.125', 'counters/examples': 2752, 'counters/updates': 86}
train stats after 2784 examples: {'rewards_train/chosen': '-0.037578', 'rewards_train/rejected': '-0.0075134', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.030065', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-132.27', 'loss/train': '0.71072', 'examples_per_second': '31.044', 'grad_norm': '20.875', 'counters/examples': 2784, 'counters/updates': 87}
train stats after 2816 examples: {'rewards_train/chosen': '-0.033865', 'rewards_train/rejected': '-0.037531', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0036663', 'logps_train/rejected': '-138.74', 'logps_train/chosen': '-137.03', 'loss/train': '0.69408', 'examples_per_second': '31.172', 'grad_norm': '21', 'counters/examples': 2816, 'counters/updates': 88}
train stats after 2848 examples: {'rewards_train/chosen': '-0.028859', 'rewards_train/rejected': '-0.0045967', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.024262', 'logps_train/rejected': '-92.396', 'logps_train/chosen': '-151.02', 'loss/train': '0.70809', 'examples_per_second': '31.468', 'grad_norm': '23.375', 'counters/examples': 2848, 'counters/updates': 89}
train stats after 2880 examples: {'rewards_train/chosen': '-0.040933', 'rewards_train/rejected': '-0.020444', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02049', 'logps_train/rejected': '-151.06', 'logps_train/chosen': '-155.7', 'loss/train': '0.707', 'examples_per_second': '31.01', 'grad_norm': '22.125', 'counters/examples': 2880, 'counters/updates': 90}
skipping logging after 2912 examples to avoid logging too frequently
train stats after 2944 examples: {'rewards_train/chosen': '-0.029253', 'rewards_train/rejected': '-0.038175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0089221', 'logps_train/rejected': '-141.32', 'logps_train/chosen': '-157.28', 'loss/train': '0.69169', 'examples_per_second': '32.254', 'grad_norm': '21.625', 'counters/examples': 2944, 'counters/updates': 92}
train stats after 2976 examples: {'rewards_train/chosen': '-0.082017', 'rewards_train/rejected': '-0.029353', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052665', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-135.16', 'loss/train': '0.72259', 'examples_per_second': '31.743', 'grad_norm': '21.5', 'counters/examples': 2976, 'counters/updates': 93}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3040 examples: {'rewards_train/chosen': '-0.01985', 'rewards_train/rejected': '-0.040238', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020388', 'logps_train/rejected': '-100.8', 'logps_train/chosen': '-117.02', 'loss/train': '0.68642', 'examples_per_second': '32.078', 'grad_norm': '18.625', 'counters/examples': 3040, 'counters/updates': 95}
train stats after 3072 examples: {'rewards_train/chosen': '-0.02616', 'rewards_train/rejected': '-0.037131', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010971', 'logps_train/rejected': '-131.97', 'logps_train/chosen': '-152.22', 'loss/train': '0.69008', 'examples_per_second': '32.438', 'grad_norm': '21.125', 'counters/examples': 3072, 'counters/updates': 96}
train stats after 3104 examples: {'rewards_train/chosen': '-0.030965', 'rewards_train/rejected': '-0.026716', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0042498', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-128.51', 'loss/train': '0.69758', 'examples_per_second': '32.781', 'grad_norm': '19.625', 'counters/examples': 3104, 'counters/updates': 97}
train stats after 3136 examples: {'rewards_train/chosen': '-0.03007', 'rewards_train/rejected': '-0.041291', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011222', 'logps_train/rejected': '-94.582', 'logps_train/chosen': '-124.78', 'loss/train': '0.69061', 'examples_per_second': '33.048', 'grad_norm': '19', 'counters/examples': 3136, 'counters/updates': 98}
train stats after 3168 examples: {'rewards_train/chosen': '-0.028222', 'rewards_train/rejected': '-0.061743', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033521', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-109.75', 'loss/train': '0.67846', 'examples_per_second': '31.725', 'grad_norm': '18.625', 'counters/examples': 3168, 'counters/updates': 99}
skipping logging after 3200 examples to avoid logging too frequently
train stats after 3232 examples: {'rewards_train/chosen': '-0.018639', 'rewards_train/rejected': '0.00062323', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019263', 'logps_train/rejected': '-140.1', 'logps_train/chosen': '-159.68', 'loss/train': '0.70572', 'examples_per_second': '30.288', 'grad_norm': '22.5', 'counters/examples': 3232, 'counters/updates': 101}
train stats after 3264 examples: {'rewards_train/chosen': '-0.007258', 'rewards_train/rejected': '-0.025836', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018578', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-142.27', 'loss/train': '0.6869', 'examples_per_second': '31.685', 'grad_norm': '20.25', 'counters/examples': 3264, 'counters/updates': 102}
train stats after 3296 examples: {'rewards_train/chosen': '-0.029588', 'rewards_train/rejected': '-0.023926', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0056622', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-145.44', 'loss/train': '0.69979', 'examples_per_second': '31.052', 'grad_norm': '22.125', 'counters/examples': 3296, 'counters/updates': 103}
train stats after 3328 examples: {'rewards_train/chosen': '-0.073625', 'rewards_train/rejected': '-0.0049662', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.068658', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-144.38', 'loss/train': '0.73363', 'examples_per_second': '30.769', 'grad_norm': '22.375', 'counters/examples': 3328, 'counters/updates': 104}
train stats after 3360 examples: {'rewards_train/chosen': '-0.0078389', 'rewards_train/rejected': '-0.045556', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037717', 'logps_train/rejected': '-114.22', 'logps_train/chosen': '-126.83', 'loss/train': '0.67587', 'examples_per_second': '32.731', 'grad_norm': '19.125', 'counters/examples': 3360, 'counters/updates': 105}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3424 examples: {'rewards_train/chosen': '-0.037241', 'rewards_train/rejected': '-0.0053213', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.03192', 'logps_train/rejected': '-113.78', 'logps_train/chosen': '-158.23', 'loss/train': '0.71138', 'examples_per_second': '33.848', 'grad_norm': '21.375', 'counters/examples': 3424, 'counters/updates': 107}
train stats after 3456 examples: {'rewards_train/chosen': '0.010836', 'rewards_train/rejected': '0.025365', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014529', 'logps_train/rejected': '-155.27', 'logps_train/chosen': '-159.29', 'loss/train': '0.70373', 'examples_per_second': '30.719', 'grad_norm': '22.625', 'counters/examples': 3456, 'counters/updates': 108}
train stats after 3488 examples: {'rewards_train/chosen': '-0.051633', 'rewards_train/rejected': '0.022162', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.073795', 'logps_train/rejected': '-155.93', 'logps_train/chosen': '-156.7', 'loss/train': '0.73406', 'examples_per_second': '30.216', 'grad_norm': '22.875', 'counters/examples': 3488, 'counters/updates': 109}
train stats after 3520 examples: {'rewards_train/chosen': '-0.032303', 'rewards_train/rejected': '-0.00073521', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.031567', 'logps_train/rejected': '-115.69', 'logps_train/chosen': '-136.13', 'loss/train': '0.71175', 'examples_per_second': '30.264', 'grad_norm': '20.625', 'counters/examples': 3520, 'counters/updates': 110}
train stats after 3552 examples: {'rewards_train/chosen': '-0.030299', 'rewards_train/rejected': '0.019295', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049593', 'logps_train/rejected': '-121.99', 'logps_train/chosen': '-145.54', 'loss/train': '0.72115', 'examples_per_second': '31.651', 'grad_norm': '21.25', 'counters/examples': 3552, 'counters/updates': 111}
train stats after 3584 examples: {'rewards_train/chosen': '-0.044855', 'rewards_train/rejected': '-0.026769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018087', 'logps_train/rejected': '-113.59', 'logps_train/chosen': '-187.87', 'loss/train': '0.70405', 'examples_per_second': '33.097', 'grad_norm': '23.375', 'counters/examples': 3584, 'counters/updates': 112}
train stats after 3616 examples: {'rewards_train/chosen': '-0.022089', 'rewards_train/rejected': '0.025402', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047491', 'logps_train/rejected': '-122.67', 'logps_train/chosen': '-145.99', 'loss/train': '0.71919', 'examples_per_second': '32.761', 'grad_norm': '21.375', 'counters/examples': 3616, 'counters/updates': 113}
train stats after 3648 examples: {'rewards_train/chosen': '-0.01578', 'rewards_train/rejected': '0.013033', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.028814', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-124.93', 'loss/train': '0.71043', 'examples_per_second': '31.017', 'grad_norm': '21.375', 'counters/examples': 3648, 'counters/updates': 114}
train stats after 3680 examples: {'rewards_train/chosen': '-0.030651', 'rewards_train/rejected': '-0.0092342', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021417', 'logps_train/rejected': '-109.87', 'logps_train/chosen': '-159.69', 'loss/train': '0.70755', 'examples_per_second': '31.564', 'grad_norm': '22.875', 'counters/examples': 3680, 'counters/updates': 115}
train stats after 3712 examples: {'rewards_train/chosen': '-0.034668', 'rewards_train/rejected': '-0.014126', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020543', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-120.79', 'loss/train': '0.70557', 'examples_per_second': '31.895', 'grad_norm': '20.75', 'counters/examples': 3712, 'counters/updates': 116}
train stats after 3744 examples: {'rewards_train/chosen': '0.0096219', 'rewards_train/rejected': '-0.034853', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044475', 'logps_train/rejected': '-141.9', 'logps_train/chosen': '-130.8', 'loss/train': '0.67312', 'examples_per_second': '31.643', 'grad_norm': '20.625', 'counters/examples': 3744, 'counters/updates': 117}
train stats after 3776 examples: {'rewards_train/chosen': '-0.045464', 'rewards_train/rejected': '-0.053539', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0080754', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-170.32', 'loss/train': '0.69234', 'examples_per_second': '32.652', 'grad_norm': '22.625', 'counters/examples': 3776, 'counters/updates': 118}
train stats after 3808 examples: {'rewards_train/chosen': '0.029311', 'rewards_train/rejected': '0.015653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013658', 'logps_train/rejected': '-110.09', 'logps_train/chosen': '-122.12', 'loss/train': '0.68886', 'examples_per_second': '31.712', 'grad_norm': '19.25', 'counters/examples': 3808, 'counters/updates': 119}
train stats after 3840 examples: {'rewards_train/chosen': '-0.0061498', 'rewards_train/rejected': '-0.01384', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0076903', 'logps_train/rejected': '-100.64', 'logps_train/chosen': '-131.15', 'loss/train': '0.69084', 'examples_per_second': '30.788', 'grad_norm': '19.25', 'counters/examples': 3840, 'counters/updates': 120}
train stats after 3872 examples: {'rewards_train/chosen': '-0.030855', 'rewards_train/rejected': '-0.013021', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.017833', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-130.96', 'loss/train': '0.70514', 'examples_per_second': '30.078', 'grad_norm': '20.75', 'counters/examples': 3872, 'counters/updates': 121}
train stats after 3904 examples: {'rewards_train/chosen': '-0.041919', 'rewards_train/rejected': '-0.010903', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031016', 'logps_train/rejected': '-112.71', 'logps_train/chosen': '-151.19', 'loss/train': '0.71041', 'examples_per_second': '31.688', 'grad_norm': '22', 'counters/examples': 3904, 'counters/updates': 122}
train stats after 3936 examples: {'rewards_train/chosen': '0.016553', 'rewards_train/rejected': '0.0033939', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013159', 'logps_train/rejected': '-104.03', 'logps_train/chosen': '-150.44', 'loss/train': '0.68823', 'examples_per_second': '30.588', 'grad_norm': '20.25', 'counters/examples': 3936, 'counters/updates': 123}
skipping logging after 3968 examples to avoid logging too frequently
train stats after 4000 examples: {'rewards_train/chosen': '-0.00025797', 'rewards_train/rejected': '0.014364', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014622', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-130.99', 'loss/train': '0.70312', 'examples_per_second': '34.398', 'grad_norm': '20.125', 'counters/examples': 4000, 'counters/updates': 125}
train stats after 4032 examples: {'rewards_train/chosen': '0.017162', 'rewards_train/rejected': '-0.0061589', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02332', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-121.5', 'loss/train': '0.68473', 'examples_per_second': '31.632', 'grad_norm': '19.625', 'counters/examples': 4032, 'counters/updates': 126}
skipping logging after 4064 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '-0.013958', 'rewards_train/rejected': '-0.0013072', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012651', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-147.06', 'loss/train': '0.70279', 'examples_per_second': '32.361', 'grad_norm': '20.125', 'counters/examples': 4096, 'counters/updates': 128}
train stats after 4128 examples: {'rewards_train/chosen': '-0.02015', 'rewards_train/rejected': '0.017502', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037652', 'logps_train/rejected': '-117.43', 'logps_train/chosen': '-169.86', 'loss/train': '0.71414', 'examples_per_second': '31.641', 'grad_norm': '21.875', 'counters/examples': 4128, 'counters/updates': 129}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4192 examples: {'rewards_train/chosen': '-0.017327', 'rewards_train/rejected': '0.0014808', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.018808', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-110.49', 'loss/train': '0.70485', 'examples_per_second': '31.676', 'grad_norm': '21', 'counters/examples': 4192, 'counters/updates': 131}
train stats after 4224 examples: {'rewards_train/chosen': '-0.0070122', 'rewards_train/rejected': '-0.031417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024405', 'logps_train/rejected': '-106.44', 'logps_train/chosen': '-117.69', 'loss/train': '0.68394', 'examples_per_second': '31.365', 'grad_norm': '19.75', 'counters/examples': 4224, 'counters/updates': 132}
train stats after 4256 examples: {'rewards_train/chosen': '-0.0063524', 'rewards_train/rejected': '-0.003317', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0030354', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-130.53', 'loss/train': '0.69601', 'examples_per_second': '30.18', 'grad_norm': '19.875', 'counters/examples': 4256, 'counters/updates': 133}
train stats after 4288 examples: {'rewards_train/chosen': '0.0010461', 'rewards_train/rejected': '-0.017954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-122.28', 'loss/train': '0.68632', 'examples_per_second': '30.721', 'grad_norm': '19', 'counters/examples': 4288, 'counters/updates': 134}
skipping logging after 4320 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '-0.022904', 'rewards_train/rejected': '-0.018771', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0041331', 'logps_train/rejected': '-134.39', 'logps_train/chosen': '-133.14', 'loss/train': '0.69669', 'examples_per_second': '32.947', 'grad_norm': '20.375', 'counters/examples': 4352, 'counters/updates': 136}
skipping logging after 4384 examples to avoid logging too frequently
train stats after 4416 examples: {'rewards_train/chosen': '-0.01648', 'rewards_train/rejected': '-0.035669', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01919', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-135', 'loss/train': '0.68599', 'examples_per_second': '30.227', 'grad_norm': '20.125', 'counters/examples': 4416, 'counters/updates': 138}
train stats after 4448 examples: {'rewards_train/chosen': '-0.054255', 'rewards_train/rejected': '-0.030652', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023603', 'logps_train/rejected': '-122.93', 'logps_train/chosen': '-148', 'loss/train': '0.70979', 'examples_per_second': '32.726', 'grad_norm': '21.875', 'counters/examples': 4448, 'counters/updates': 139}
skipping logging after 4480 examples to avoid logging too frequently
train stats after 4512 examples: {'rewards_train/chosen': '-0.038848', 'rewards_train/rejected': '-0.025487', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013361', 'logps_train/rejected': '-99.282', 'logps_train/chosen': '-153.57', 'loss/train': '0.70121', 'examples_per_second': '32.834', 'grad_norm': '21.875', 'counters/examples': 4512, 'counters/updates': 141}
train stats after 4544 examples: {'rewards_train/chosen': '-0.011683', 'rewards_train/rejected': '-0.046077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034394', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-112.64', 'loss/train': '0.67845', 'examples_per_second': '30.78', 'grad_norm': '19.125', 'counters/examples': 4544, 'counters/updates': 142}
train stats after 4576 examples: {'rewards_train/chosen': '-0.006061', 'rewards_train/rejected': '-0.008049', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0019881', 'logps_train/rejected': '-104.28', 'logps_train/chosen': '-142.52', 'loss/train': '0.69411', 'examples_per_second': '31.813', 'grad_norm': '20.125', 'counters/examples': 4576, 'counters/updates': 143}
train stats after 4608 examples: {'rewards_train/chosen': '-0.034817', 'rewards_train/rejected': '-0.032225', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0025918', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-133.33', 'loss/train': '0.69637', 'examples_per_second': '31.453', 'grad_norm': '20.25', 'counters/examples': 4608, 'counters/updates': 144}
train stats after 4640 examples: {'rewards_train/chosen': '-0.014552', 'rewards_train/rejected': '-0.051938', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037386', 'logps_train/rejected': '-150.62', 'logps_train/chosen': '-166.1', 'loss/train': '0.67785', 'examples_per_second': '30.693', 'grad_norm': '21.25', 'counters/examples': 4640, 'counters/updates': 145}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4704 examples: {'rewards_train/chosen': '-0.034961', 'rewards_train/rejected': '0.014285', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.049246', 'logps_train/rejected': '-130.8', 'logps_train/chosen': '-159.09', 'loss/train': '0.72045', 'examples_per_second': '31.779', 'grad_norm': '21.875', 'counters/examples': 4704, 'counters/updates': 147}
train stats after 4736 examples: {'rewards_train/chosen': '-0.041892', 'rewards_train/rejected': '0.011156', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053048', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-156.31', 'loss/train': '0.7231', 'examples_per_second': '30.861', 'grad_norm': '21.875', 'counters/examples': 4736, 'counters/updates': 148}
skipping logging after 4768 examples to avoid logging too frequently
train stats after 4800 examples: {'rewards_train/chosen': '-0.003608', 'rewards_train/rejected': '-0.0041599', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0005519', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-105', 'loss/train': '0.69629', 'examples_per_second': '31.74', 'grad_norm': '18.75', 'counters/examples': 4800, 'counters/updates': 150}
train stats after 4832 examples: {'rewards_train/chosen': '-0.025606', 'rewards_train/rejected': '-0.042287', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016681', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-162.35', 'loss/train': '0.68746', 'examples_per_second': '32.347', 'grad_norm': '21.5', 'counters/examples': 4832, 'counters/updates': 151}
train stats after 4864 examples: {'rewards_train/chosen': '-0.030025', 'rewards_train/rejected': '-0.001316', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028709', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-145.54', 'loss/train': '0.7108', 'examples_per_second': '30.427', 'grad_norm': '22.25', 'counters/examples': 4864, 'counters/updates': 152}
train stats after 4896 examples: {'rewards_train/chosen': '0.017888', 'rewards_train/rejected': '-0.0043301', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022218', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-134.81', 'loss/train': '0.68717', 'examples_per_second': '31.636', 'grad_norm': '20.875', 'counters/examples': 4896, 'counters/updates': 153}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4960 examples: {'rewards_train/chosen': '-0.047468', 'rewards_train/rejected': '0.0098829', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.057351', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-159.93', 'loss/train': '0.72423', 'examples_per_second': '31.707', 'grad_norm': '22', 'counters/examples': 4960, 'counters/updates': 155}
train stats after 4992 examples: {'rewards_train/chosen': '-0.018329', 'rewards_train/rejected': '0.0047062', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.023035', 'logps_train/rejected': '-117.12', 'logps_train/chosen': '-151.69', 'loss/train': '0.70726', 'examples_per_second': '30.893', 'grad_norm': '21.5', 'counters/examples': 4992, 'counters/updates': 156}
train stats after 5024 examples: {'rewards_train/chosen': '0.021006', 'rewards_train/rejected': '-0.030957', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051963', 'logps_train/rejected': '-139.6', 'logps_train/chosen': '-116.79', 'loss/train': '0.67068', 'examples_per_second': '31.552', 'grad_norm': '20.625', 'counters/examples': 5024, 'counters/updates': 157}
train stats after 5056 examples: {'rewards_train/chosen': '-0.01464', 'rewards_train/rejected': '-0.018231', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0035905', 'logps_train/rejected': '-127.75', 'logps_train/chosen': '-124.34', 'loss/train': '0.69398', 'examples_per_second': '31.582', 'grad_norm': '20.125', 'counters/examples': 5056, 'counters/updates': 158}
skipping logging after 5088 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '-0.020789', 'rewards_train/rejected': '0.0065353', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027324', 'logps_train/rejected': '-128.69', 'logps_train/chosen': '-116.74', 'loss/train': '0.71005', 'examples_per_second': '31.567', 'grad_norm': '20', 'counters/examples': 5120, 'counters/updates': 160}
train stats after 5152 examples: {'rewards_train/chosen': '-0.030676', 'rewards_train/rejected': '-0.004099', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026577', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-182.43', 'loss/train': '0.70925', 'examples_per_second': '30.644', 'grad_norm': '22.75', 'counters/examples': 5152, 'counters/updates': 161}
train stats after 5184 examples: {'rewards_train/chosen': '0.0043753', 'rewards_train/rejected': '-0.020021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024396', 'logps_train/rejected': '-116.69', 'logps_train/chosen': '-127.46', 'loss/train': '0.68344', 'examples_per_second': '31.356', 'grad_norm': '18.875', 'counters/examples': 5184, 'counters/updates': 162}
train stats after 5216 examples: {'rewards_train/chosen': '-0.031421', 'rewards_train/rejected': '-0.018425', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012996', 'logps_train/rejected': '-109.62', 'logps_train/chosen': '-145.52', 'loss/train': '0.70145', 'examples_per_second': '31.281', 'grad_norm': '19.875', 'counters/examples': 5216, 'counters/updates': 163}
train stats after 5248 examples: {'rewards_train/chosen': '0.014668', 'rewards_train/rejected': '-0.038713', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053381', 'logps_train/rejected': '-113.29', 'logps_train/chosen': '-107.18', 'loss/train': '0.66876', 'examples_per_second': '31.639', 'grad_norm': '18.125', 'counters/examples': 5248, 'counters/updates': 164}
train stats after 5280 examples: {'rewards_train/chosen': '0.0048108', 'rewards_train/rejected': '-0.0032525', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0080633', 'logps_train/rejected': '-118.4', 'logps_train/chosen': '-146.48', 'loss/train': '0.69249', 'examples_per_second': '31.745', 'grad_norm': '20.75', 'counters/examples': 5280, 'counters/updates': 165}
train stats after 5312 examples: {'rewards_train/chosen': '0.0072856', 'rewards_train/rejected': '0.026103', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018818', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-127.54', 'loss/train': '0.70448', 'examples_per_second': '30.667', 'grad_norm': '20.25', 'counters/examples': 5312, 'counters/updates': 166}
train stats after 5344 examples: {'rewards_train/chosen': '-0.026626', 'rewards_train/rejected': '-0.017009', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0096175', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-187.18', 'loss/train': '0.70226', 'examples_per_second': '31.653', 'grad_norm': '23.875', 'counters/examples': 5344, 'counters/updates': 167}
train stats after 5376 examples: {'rewards_train/chosen': '-0.023569', 'rewards_train/rejected': '-0.022124', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0014451', 'logps_train/rejected': '-109.69', 'logps_train/chosen': '-133.59', 'loss/train': '0.69572', 'examples_per_second': '31.661', 'grad_norm': '19', 'counters/examples': 5376, 'counters/updates': 168}
train stats after 5408 examples: {'rewards_train/chosen': '0.0087405', 'rewards_train/rejected': '-0.050068', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.058809', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-122.76', 'loss/train': '0.66625', 'examples_per_second': '31.851', 'grad_norm': '20.5', 'counters/examples': 5408, 'counters/updates': 169}
train stats after 5440 examples: {'rewards_train/chosen': '-0.0080502', 'rewards_train/rejected': '-0.0028055', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0052447', 'logps_train/rejected': '-93.609', 'logps_train/chosen': '-116.56', 'loss/train': '0.69819', 'examples_per_second': '32.062', 'grad_norm': '18.875', 'counters/examples': 5440, 'counters/updates': 170}
train stats after 5472 examples: {'rewards_train/chosen': '-0.052989', 'rewards_train/rejected': '-0.026679', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02631', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-159.72', 'loss/train': '0.71084', 'examples_per_second': '32.625', 'grad_norm': '22', 'counters/examples': 5472, 'counters/updates': 171}
skipping logging after 5504 examples to avoid logging too frequently
train stats after 5536 examples: {'rewards_train/chosen': '-0.020962', 'rewards_train/rejected': '-0.005704', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015258', 'logps_train/rejected': '-94.901', 'logps_train/chosen': '-90.483', 'loss/train': '0.7028', 'examples_per_second': '31.627', 'grad_norm': '17', 'counters/examples': 5536, 'counters/updates': 173}
train stats after 5568 examples: {'rewards_train/chosen': '-0.036758', 'rewards_train/rejected': '-0.0061424', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.030615', 'logps_train/rejected': '-111.33', 'logps_train/chosen': '-137.98', 'loss/train': '0.71085', 'examples_per_second': '30.628', 'grad_norm': '20.625', 'counters/examples': 5568, 'counters/updates': 174}
skipping logging after 5600 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '-0.026424', 'rewards_train/rejected': '-0.054311', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027887', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-104.1', 'loss/train': '0.68238', 'examples_per_second': '35.968', 'grad_norm': '19.375', 'counters/examples': 5632, 'counters/updates': 176}
train stats after 5664 examples: {'rewards_train/chosen': '0.0070479', 'rewards_train/rejected': '-0.010712', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01776', 'logps_train/rejected': '-164.15', 'logps_train/chosen': '-126.3', 'loss/train': '0.68599', 'examples_per_second': '30.886', 'grad_norm': '20.875', 'counters/examples': 5664, 'counters/updates': 177}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5728 examples: {'rewards_train/chosen': '-0.042221', 'rewards_train/rejected': '-0.020624', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021598', 'logps_train/rejected': '-88.906', 'logps_train/chosen': '-153.94', 'loss/train': '0.70689', 'examples_per_second': '32.807', 'grad_norm': '21.125', 'counters/examples': 5728, 'counters/updates': 179}
skipping logging after 5760 examples to avoid logging too frequently
train stats after 5792 examples: {'rewards_train/chosen': '-0.04504', 'rewards_train/rejected': '-0.011605', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033435', 'logps_train/rejected': '-136.3', 'logps_train/chosen': '-174.18', 'loss/train': '0.71278', 'examples_per_second': '31.689', 'grad_norm': '24.25', 'counters/examples': 5792, 'counters/updates': 181}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5856 examples: {'rewards_train/chosen': '-0.015819', 'rewards_train/rejected': '0.016736', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.032555', 'logps_train/rejected': '-108.23', 'logps_train/chosen': '-131.38', 'loss/train': '0.7118', 'examples_per_second': '31.433', 'grad_norm': '21.125', 'counters/examples': 5856, 'counters/updates': 183}
train stats after 5888 examples: {'rewards_train/chosen': '-0.0011622', 'rewards_train/rejected': '-0.02802', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.026857', 'logps_train/rejected': '-97.042', 'logps_train/chosen': '-133.45', 'loss/train': '0.68189', 'examples_per_second': '31.598', 'grad_norm': '19.25', 'counters/examples': 5888, 'counters/updates': 184}
skipping logging after 5920 examples to avoid logging too frequently
train stats after 5952 examples: {'rewards_train/chosen': '-0.0094971', 'rewards_train/rejected': '-0.032806', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023309', 'logps_train/rejected': '-102.14', 'logps_train/chosen': '-112.76', 'loss/train': '0.68349', 'examples_per_second': '32.303', 'grad_norm': '17.25', 'counters/examples': 5952, 'counters/updates': 186}
train stats after 5984 examples: {'rewards_train/chosen': '0.016228', 'rewards_train/rejected': '0.021012', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0047841', 'logps_train/rejected': '-113.01', 'logps_train/chosen': '-97.467', 'loss/train': '0.69725', 'examples_per_second': '30.846', 'grad_norm': '19.375', 'counters/examples': 5984, 'counters/updates': 187}
train stats after 6016 examples: {'rewards_train/chosen': '-0.0095175', 'rewards_train/rejected': '0.012149', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.021666', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-161.17', 'loss/train': '0.70636', 'examples_per_second': '31.31', 'grad_norm': '21.625', 'counters/examples': 6016, 'counters/updates': 188}
train stats after 6048 examples: {'rewards_train/chosen': '-0.019042', 'rewards_train/rejected': '0.020794', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.039836', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-158.88', 'loss/train': '0.71691', 'examples_per_second': '31.724', 'grad_norm': '22.375', 'counters/examples': 6048, 'counters/updates': 189}
train stats after 6080 examples: {'rewards_train/chosen': '0.011926', 'rewards_train/rejected': '-0.0090814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021007', 'logps_train/rejected': '-143.71', 'logps_train/chosen': '-142.65', 'loss/train': '0.68603', 'examples_per_second': '31.7', 'grad_norm': '21.375', 'counters/examples': 6080, 'counters/updates': 190}
skipping logging after 6112 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '-0.005681', 'rewards_train/rejected': '-0.018922', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013241', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-119.86', 'loss/train': '0.68983', 'examples_per_second': '31.661', 'grad_norm': '20', 'counters/examples': 6144, 'counters/updates': 192}
skipping logging after 6176 examples to avoid logging too frequently
train stats after 6208 examples: {'rewards_train/chosen': '-0.036619', 'rewards_train/rejected': '-0.03266', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0039591', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-155.49', 'loss/train': '0.69697', 'examples_per_second': '32.084', 'grad_norm': '21.25', 'counters/examples': 6208, 'counters/updates': 194}
skipping logging after 6240 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '0.0024347', 'rewards_train/rejected': '-0.0097617', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012196', 'logps_train/rejected': '-81.343', 'logps_train/chosen': '-140.56', 'loss/train': '0.68975', 'examples_per_second': '31.6', 'grad_norm': '19.875', 'counters/examples': 6272, 'counters/updates': 196}
train stats after 6304 examples: {'rewards_train/chosen': '-0.015429', 'rewards_train/rejected': '0.010791', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02622', 'logps_train/rejected': '-130.18', 'logps_train/chosen': '-155.71', 'loss/train': '0.70843', 'examples_per_second': '31.687', 'grad_norm': '22.125', 'counters/examples': 6304, 'counters/updates': 197}
train stats after 6336 examples: {'rewards_train/chosen': '-0.026724', 'rewards_train/rejected': '-0.0080616', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018662', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-136.83', 'loss/train': '0.70474', 'examples_per_second': '31.772', 'grad_norm': '20.25', 'counters/examples': 6336, 'counters/updates': 198}
train stats after 6368 examples: {'rewards_train/chosen': '-0.0056557', 'rewards_train/rejected': '0.017961', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023616', 'logps_train/rejected': '-113.15', 'logps_train/chosen': '-148.55', 'loss/train': '0.70798', 'examples_per_second': '31.161', 'grad_norm': '21.375', 'counters/examples': 6368, 'counters/updates': 199}
train stats after 6400 examples: {'rewards_train/chosen': '-0.0074294', 'rewards_train/rejected': '0.010968', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018397', 'logps_train/rejected': '-129.2', 'logps_train/chosen': '-148.96', 'loss/train': '0.70384', 'examples_per_second': '32.566', 'grad_norm': '22.125', 'counters/examples': 6400, 'counters/updates': 200}
train stats after 6432 examples: {'rewards_train/chosen': '-0.010939', 'rewards_train/rejected': '-0.027211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016272', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-149.2', 'loss/train': '0.68732', 'examples_per_second': '31.256', 'grad_norm': '20.375', 'counters/examples': 6432, 'counters/updates': 201}
skipping logging after 6464 examples to avoid logging too frequently
train stats after 6496 examples: {'rewards_train/chosen': '-0.024527', 'rewards_train/rejected': '-0.026661', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0021342', 'logps_train/rejected': '-112.89', 'logps_train/chosen': '-132.13', 'loss/train': '0.69407', 'examples_per_second': '30.145', 'grad_norm': '20.125', 'counters/examples': 6496, 'counters/updates': 203}
train stats after 6528 examples: {'rewards_train/chosen': '-0.024148', 'rewards_train/rejected': '-0.0064451', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017703', 'logps_train/rejected': '-102.64', 'logps_train/chosen': '-100.89', 'loss/train': '0.70496', 'examples_per_second': '31.717', 'grad_norm': '18.375', 'counters/examples': 6528, 'counters/updates': 204}
train stats after 6560 examples: {'rewards_train/chosen': '0.0017319', 'rewards_train/rejected': '0.0033884', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0016564', 'logps_train/rejected': '-110.66', 'logps_train/chosen': '-114.54', 'loss/train': '0.69799', 'examples_per_second': '21.154', 'grad_norm': '20.5', 'counters/examples': 6560, 'counters/updates': 205}
train stats after 6592 examples: {'rewards_train/chosen': '-0.017659', 'rewards_train/rejected': '-0.042009', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024351', 'logps_train/rejected': '-179.52', 'logps_train/chosen': '-153.31', 'loss/train': '0.68476', 'examples_per_second': '31.656', 'grad_norm': '22.875', 'counters/examples': 6592, 'counters/updates': 206}
skipping logging after 6624 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '-0.041898', 'rewards_train/rejected': '-0.01057', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.031329', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-130.43', 'loss/train': '0.71084', 'examples_per_second': '21.698', 'grad_norm': '20', 'counters/examples': 6656, 'counters/updates': 208}
train stats after 6688 examples: {'rewards_train/chosen': '-0.0067669', 'rewards_train/rejected': '-0.022791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016024', 'logps_train/rejected': '-132.89', 'logps_train/chosen': '-153.78', 'loss/train': '0.68764', 'examples_per_second': '31.289', 'grad_norm': '21.125', 'counters/examples': 6688, 'counters/updates': 209}
skipping logging after 6720 examples to avoid logging too frequently
train stats after 6752 examples: {'rewards_train/chosen': '-0.035442', 'rewards_train/rejected': '-0.004079', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031363', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-140.59', 'loss/train': '0.71341', 'examples_per_second': '30.917', 'grad_norm': '23', 'counters/examples': 6752, 'counters/updates': 211}
train stats after 6784 examples: {'rewards_train/chosen': '-0.0069882', 'rewards_train/rejected': '-0.021686', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014698', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-145.97', 'loss/train': '0.688', 'examples_per_second': '30.972', 'grad_norm': '21.125', 'counters/examples': 6784, 'counters/updates': 212}
train stats after 6816 examples: {'rewards_train/chosen': '-0.0063218', 'rewards_train/rejected': '-0.01505', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0087282', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-140.69', 'loss/train': '0.69146', 'examples_per_second': '31.713', 'grad_norm': '21', 'counters/examples': 6816, 'counters/updates': 213}
train stats after 6848 examples: {'rewards_train/chosen': '0.017803', 'rewards_train/rejected': '0.011429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0063744', 'logps_train/rejected': '-150.32', 'logps_train/chosen': '-149.27', 'loss/train': '0.69254', 'examples_per_second': '31.618', 'grad_norm': '22.625', 'counters/examples': 6848, 'counters/updates': 214}
train stats after 6880 examples: {'rewards_train/chosen': '-0.03225', 'rewards_train/rejected': '-0.030678', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0015725', 'logps_train/rejected': '-144.97', 'logps_train/chosen': '-164.43', 'loss/train': '0.69644', 'examples_per_second': '30.372', 'grad_norm': '21.75', 'counters/examples': 6880, 'counters/updates': 215}
train stats after 6912 examples: {'rewards_train/chosen': '-0.00071906', 'rewards_train/rejected': '-0.017851', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017132', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-131.77', 'loss/train': '0.68684', 'examples_per_second': '31.359', 'grad_norm': '20.5', 'counters/examples': 6912, 'counters/updates': 216}
skipping logging after 6944 examples to avoid logging too frequently
train stats after 6976 examples: {'rewards_train/chosen': '-0.015895', 'rewards_train/rejected': '0.010567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.026462', 'logps_train/rejected': '-125.72', 'logps_train/chosen': '-149.82', 'loss/train': '0.70924', 'examples_per_second': '30.711', 'grad_norm': '23.25', 'counters/examples': 6976, 'counters/updates': 218}
train stats after 7008 examples: {'rewards_train/chosen': '0.044245', 'rewards_train/rejected': '0.0091664', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035078', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-146.81', 'loss/train': '0.67792', 'examples_per_second': '31.794', 'grad_norm': '21.5', 'counters/examples': 7008, 'counters/updates': 219}
train stats after 7040 examples: {'rewards_train/chosen': '-0.039553', 'rewards_train/rejected': '-0.016024', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.023529', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-143.95', 'loss/train': '0.70722', 'examples_per_second': '31.667', 'grad_norm': '20.5', 'counters/examples': 7040, 'counters/updates': 220}
train stats after 7072 examples: {'rewards_train/chosen': '-0.0045833', 'rewards_train/rejected': '0.01021', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014794', 'logps_train/rejected': '-100.95', 'logps_train/chosen': '-139.18', 'loss/train': '0.70256', 'examples_per_second': '30.831', 'grad_norm': '20.75', 'counters/examples': 7072, 'counters/updates': 221}
train stats after 7104 examples: {'rewards_train/chosen': '-0.010225', 'rewards_train/rejected': '0.013577', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023802', 'logps_train/rejected': '-107.74', 'logps_train/chosen': '-116.18', 'loss/train': '0.70627', 'examples_per_second': '31.101', 'grad_norm': '19.625', 'counters/examples': 7104, 'counters/updates': 222}
train stats after 7136 examples: {'rewards_train/chosen': '-0.040488', 'rewards_train/rejected': '-0.019784', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020704', 'logps_train/rejected': '-133.95', 'logps_train/chosen': '-167.85', 'loss/train': '0.70557', 'examples_per_second': '24.732', 'grad_norm': '22', 'counters/examples': 7136, 'counters/updates': 223}
train stats after 7168 examples: {'rewards_train/chosen': '0.016534', 'rewards_train/rejected': '-0.016518', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033052', 'logps_train/rejected': '-103.27', 'logps_train/chosen': '-137.4', 'loss/train': '0.67864', 'examples_per_second': '30.411', 'grad_norm': '19.25', 'counters/examples': 7168, 'counters/updates': 224}
train stats after 7200 examples: {'rewards_train/chosen': '-0.050918', 'rewards_train/rejected': '-0.0036139', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.047304', 'logps_train/rejected': '-88.202', 'logps_train/chosen': '-141.68', 'loss/train': '0.71801', 'examples_per_second': '32.806', 'grad_norm': '20', 'counters/examples': 7200, 'counters/updates': 225}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7264 examples: {'rewards_train/chosen': '-0.033659', 'rewards_train/rejected': '0.036291', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.069949', 'logps_train/rejected': '-109.69', 'logps_train/chosen': '-152.79', 'loss/train': '0.73489', 'examples_per_second': '31.64', 'grad_norm': '22.375', 'counters/examples': 7264, 'counters/updates': 227}
train stats after 7296 examples: {'rewards_train/chosen': '-0.0015887', 'rewards_train/rejected': '-0.0034561', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0018674', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-153.98', 'loss/train': '0.69458', 'examples_per_second': '32.101', 'grad_norm': '21.125', 'counters/examples': 7296, 'counters/updates': 228}
skipping logging after 7328 examples to avoid logging too frequently
train stats after 7360 examples: {'rewards_train/chosen': '-0.014093', 'rewards_train/rejected': '-0.044943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030851', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-107.58', 'loss/train': '0.68013', 'examples_per_second': '32.042', 'grad_norm': '17.875', 'counters/examples': 7360, 'counters/updates': 230}
skipping logging after 7392 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '0.0041718', 'rewards_train/rejected': '-0.040617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044789', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-131.21', 'loss/train': '0.67443', 'examples_per_second': '31.635', 'grad_norm': '20.125', 'counters/examples': 7424, 'counters/updates': 232}
train stats after 7456 examples: {'rewards_train/chosen': '-0.010398', 'rewards_train/rejected': '-0.035865', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025467', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-148.67', 'loss/train': '0.68309', 'examples_per_second': '33.099', 'grad_norm': '21.25', 'counters/examples': 7456, 'counters/updates': 233}
train stats after 7488 examples: {'rewards_train/chosen': '-0.029126', 'rewards_train/rejected': '-0.012892', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.016234', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-166.45', 'loss/train': '0.70554', 'examples_per_second': '30.291', 'grad_norm': '22.125', 'counters/examples': 7488, 'counters/updates': 234}
train stats after 7520 examples: {'rewards_train/chosen': '-0.00093396', 'rewards_train/rejected': '-0.010699', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0097646', 'logps_train/rejected': '-112.01', 'logps_train/chosen': '-103.14', 'loss/train': '0.68981', 'examples_per_second': '30.734', 'grad_norm': '18.375', 'counters/examples': 7520, 'counters/updates': 235}
train stats after 7552 examples: {'rewards_train/chosen': '-0.010219', 'rewards_train/rejected': '0.00013617', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010355', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-145.16', 'loss/train': '0.70034', 'examples_per_second': '32.573', 'grad_norm': '20.25', 'counters/examples': 7552, 'counters/updates': 236}
train stats after 7584 examples: {'rewards_train/chosen': '0.0039277', 'rewards_train/rejected': '-0.032612', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036539', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-126.87', 'loss/train': '0.67693', 'examples_per_second': '31.733', 'grad_norm': '20.5', 'counters/examples': 7584, 'counters/updates': 237}
train stats after 7616 examples: {'rewards_train/chosen': '-0.030694', 'rewards_train/rejected': '-0.025831', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0048631', 'logps_train/rejected': '-90.512', 'logps_train/chosen': '-119.48', 'loss/train': '0.69764', 'examples_per_second': '32.747', 'grad_norm': '19.375', 'counters/examples': 7616, 'counters/updates': 238}
train stats after 7648 examples: {'rewards_train/chosen': '-0.024015', 'rewards_train/rejected': '-0.04651', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022496', 'logps_train/rejected': '-130.8', 'logps_train/chosen': '-141.13', 'loss/train': '0.68542', 'examples_per_second': '32.69', 'grad_norm': '20.375', 'counters/examples': 7648, 'counters/updates': 239}
skipping logging after 7680 examples to avoid logging too frequently
train stats after 7712 examples: {'rewards_train/chosen': '-0.0020914', 'rewards_train/rejected': '-5.8967e-05', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0020324', 'logps_train/rejected': '-84.842', 'logps_train/chosen': '-133.8', 'loss/train': '0.69659', 'examples_per_second': '34.527', 'grad_norm': '19.625', 'counters/examples': 7712, 'counters/updates': 241}
train stats after 7744 examples: {'rewards_train/chosen': '0.020331', 'rewards_train/rejected': '-0.018487', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038818', 'logps_train/rejected': '-116.9', 'logps_train/chosen': '-167.34', 'loss/train': '0.67612', 'examples_per_second': '30.837', 'grad_norm': '21.125', 'counters/examples': 7744, 'counters/updates': 242}
skipping logging after 7776 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '-0.054332', 'rewards_train/rejected': '-0.015318', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.039014', 'logps_train/rejected': '-98.779', 'logps_train/chosen': '-165.47', 'loss/train': '0.7189', 'examples_per_second': '32', 'grad_norm': '23.75', 'counters/examples': 7808, 'counters/updates': 244}
train stats after 7840 examples: {'rewards_train/chosen': '-0.017351', 'rewards_train/rejected': '0.0066469', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023998', 'logps_train/rejected': '-150.65', 'logps_train/chosen': '-152.34', 'loss/train': '0.70839', 'examples_per_second': '31.777', 'grad_norm': '23', 'counters/examples': 7840, 'counters/updates': 245}
train stats after 7872 examples: {'rewards_train/chosen': '-0.0076539', 'rewards_train/rejected': '0.016689', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024343', 'logps_train/rejected': '-108.68', 'logps_train/chosen': '-118.89', 'loss/train': '0.70882', 'examples_per_second': '30.766', 'grad_norm': '20.625', 'counters/examples': 7872, 'counters/updates': 246}
train stats after 7904 examples: {'rewards_train/chosen': '0.004908', 'rewards_train/rejected': '0.015042', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.010134', 'logps_train/rejected': '-103.33', 'logps_train/chosen': '-98.037', 'loss/train': '0.69988', 'examples_per_second': '30.405', 'grad_norm': '18.125', 'counters/examples': 7904, 'counters/updates': 247}
skipping logging after 7936 examples to avoid logging too frequently
train stats after 7968 examples: {'rewards_train/chosen': '0.013454', 'rewards_train/rejected': '0.018958', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0055032', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-138.62', 'loss/train': '0.69913', 'examples_per_second': '33.423', 'grad_norm': '21.125', 'counters/examples': 7968, 'counters/updates': 249}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8032 examples: {'rewards_train/chosen': '-0.023916', 'rewards_train/rejected': '-0.029687', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0057711', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-144.36', 'loss/train': '0.69265', 'examples_per_second': '31.418', 'grad_norm': '21.125', 'counters/examples': 8032, 'counters/updates': 251}
train stats after 8064 examples: {'rewards_train/chosen': '-0.00084341', 'rewards_train/rejected': '-0.024252', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.023409', 'logps_train/rejected': '-129.06', 'logps_train/chosen': '-156.47', 'loss/train': '0.68415', 'examples_per_second': '30.651', 'grad_norm': '22.75', 'counters/examples': 8064, 'counters/updates': 252}
skipping logging after 8096 examples to avoid logging too frequently
train stats after 8128 examples: {'rewards_train/chosen': '-0.021915', 'rewards_train/rejected': '0.005712', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027627', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-144.73', 'loss/train': '0.71053', 'examples_per_second': '31.684', 'grad_norm': '22.125', 'counters/examples': 8128, 'counters/updates': 254}
train stats after 8160 examples: {'rewards_train/chosen': '-0.0093311', 'rewards_train/rejected': '0.0013345', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010666', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-148.56', 'loss/train': '0.70202', 'examples_per_second': '30.402', 'grad_norm': '20.375', 'counters/examples': 8160, 'counters/updates': 255}
train stats after 8192 examples: {'rewards_train/chosen': '-0.010684', 'rewards_train/rejected': '-0.020781', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010096', 'logps_train/rejected': '-125', 'logps_train/chosen': '-184.74', 'loss/train': '0.69207', 'examples_per_second': '30.449', 'grad_norm': '23.25', 'counters/examples': 8192, 'counters/updates': 256}
train stats after 8224 examples: {'rewards_train/chosen': '0.0086434', 'rewards_train/rejected': '-0.012693', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021336', 'logps_train/rejected': '-112.34', 'logps_train/chosen': '-136.09', 'loss/train': '0.68532', 'examples_per_second': '32.485', 'grad_norm': '19.375', 'counters/examples': 8224, 'counters/updates': 257}
train stats after 8256 examples: {'rewards_train/chosen': '-0.0097129', 'rewards_train/rejected': '-0.019187', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.009474', 'logps_train/rejected': '-151.25', 'logps_train/chosen': '-148.35', 'loss/train': '0.693', 'examples_per_second': '30.945', 'grad_norm': '22', 'counters/examples': 8256, 'counters/updates': 258}
train stats after 8288 examples: {'rewards_train/chosen': '0.0011954', 'rewards_train/rejected': '0.033716', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032521', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-129.36', 'loss/train': '0.7119', 'examples_per_second': '32.282', 'grad_norm': '21.125', 'counters/examples': 8288, 'counters/updates': 259}
train stats after 8320 examples: {'rewards_train/chosen': '-0.025558', 'rewards_train/rejected': '0.0011082', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026666', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-129.86', 'loss/train': '0.70926', 'examples_per_second': '32.198', 'grad_norm': '20.25', 'counters/examples': 8320, 'counters/updates': 260}
skipping logging after 8352 examples to avoid logging too frequently
train stats after 8384 examples: {'rewards_train/chosen': '0.038552', 'rewards_train/rejected': '0.011672', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02688', 'logps_train/rejected': '-117.19', 'logps_train/chosen': '-137.13', 'loss/train': '0.68343', 'examples_per_second': '31.105', 'grad_norm': '20.25', 'counters/examples': 8384, 'counters/updates': 262}
train stats after 8416 examples: {'rewards_train/chosen': '-0.005227', 'rewards_train/rejected': '0.0045325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0097595', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-116.69', 'loss/train': '0.70113', 'examples_per_second': '31.716', 'grad_norm': '20.125', 'counters/examples': 8416, 'counters/updates': 263}
train stats after 8448 examples: {'rewards_train/chosen': '0.0033728', 'rewards_train/rejected': '-0.040248', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043621', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-173.15', 'loss/train': '0.67592', 'examples_per_second': '30.243', 'grad_norm': '20.75', 'counters/examples': 8448, 'counters/updates': 264}
train stats after 8480 examples: {'rewards_train/chosen': '0.035086', 'rewards_train/rejected': '-0.013719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048805', 'logps_train/rejected': '-134.62', 'logps_train/chosen': '-145.21', 'loss/train': '0.67339', 'examples_per_second': '31.746', 'grad_norm': '20.5', 'counters/examples': 8480, 'counters/updates': 265}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8544 examples: {'rewards_train/chosen': '0.036603', 'rewards_train/rejected': '-0.00047673', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037079', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-156.65', 'loss/train': '0.67823', 'examples_per_second': '30.742', 'grad_norm': '21.625', 'counters/examples': 8544, 'counters/updates': 267}
train stats after 8576 examples: {'rewards_train/chosen': '-0.023292', 'rewards_train/rejected': '0.01212', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035412', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-142.86', 'loss/train': '0.71427', 'examples_per_second': '31.765', 'grad_norm': '20', 'counters/examples': 8576, 'counters/updates': 268}
train stats after 8608 examples: {'rewards_train/chosen': '-0.010607', 'rewards_train/rejected': '-0.045862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035255', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-124.49', 'loss/train': '0.67797', 'examples_per_second': '32.213', 'grad_norm': '19.125', 'counters/examples': 8608, 'counters/updates': 269}
train stats after 8640 examples: {'rewards_train/chosen': '-0.017098', 'rewards_train/rejected': '0.022489', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.039587', 'logps_train/rejected': '-87.606', 'logps_train/chosen': '-124.87', 'loss/train': '0.71553', 'examples_per_second': '31.514', 'grad_norm': '19.875', 'counters/examples': 8640, 'counters/updates': 270}
train stats after 8672 examples: {'rewards_train/chosen': '-0.050998', 'rewards_train/rejected': '-0.029834', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021164', 'logps_train/rejected': '-124.25', 'logps_train/chosen': '-141.06', 'loss/train': '0.70599', 'examples_per_second': '31.664', 'grad_norm': '22', 'counters/examples': 8672, 'counters/updates': 271}
train stats after 8704 examples: {'rewards_train/chosen': '0.024692', 'rewards_train/rejected': '0.020778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0039142', 'logps_train/rejected': '-100.38', 'logps_train/chosen': '-106.7', 'loss/train': '0.69295', 'examples_per_second': '32.06', 'grad_norm': '17.625', 'counters/examples': 8704, 'counters/updates': 272}
skipping logging after 8736 examples to avoid logging too frequently
train stats after 8768 examples: {'rewards_train/chosen': '0.0039341', 'rewards_train/rejected': '-0.03111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035045', 'logps_train/rejected': '-137.63', 'logps_train/chosen': '-128.9', 'loss/train': '0.6786', 'examples_per_second': '30.765', 'grad_norm': '20', 'counters/examples': 8768, 'counters/updates': 274}
train stats after 8800 examples: {'rewards_train/chosen': '0.0085611', 'rewards_train/rejected': '-0.018692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027253', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-134.51', 'loss/train': '0.68162', 'examples_per_second': '31.712', 'grad_norm': '19.75', 'counters/examples': 8800, 'counters/updates': 275}
train stats after 8832 examples: {'rewards_train/chosen': '-0.01901', 'rewards_train/rejected': '-0.016106', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0029045', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-155.37', 'loss/train': '0.69832', 'examples_per_second': '32.896', 'grad_norm': '22', 'counters/examples': 8832, 'counters/updates': 276}
skipping logging after 8864 examples to avoid logging too frequently
train stats after 8896 examples: {'rewards_train/chosen': '-0.01116', 'rewards_train/rejected': '0.0084382', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019598', 'logps_train/rejected': '-96.986', 'logps_train/chosen': '-147.27', 'loss/train': '0.70602', 'examples_per_second': '31.204', 'grad_norm': '20.75', 'counters/examples': 8896, 'counters/updates': 278}
train stats after 8928 examples: {'rewards_train/chosen': '-0.039515', 'rewards_train/rejected': '0.0049584', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.044473', 'logps_train/rejected': '-108.45', 'logps_train/chosen': '-134.95', 'loss/train': '0.71933', 'examples_per_second': '32.375', 'grad_norm': '20.125', 'counters/examples': 8928, 'counters/updates': 279}
train stats after 8960 examples: {'rewards_train/chosen': '-0.011042', 'rewards_train/rejected': '-0.015102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00406', 'logps_train/rejected': '-95.578', 'logps_train/chosen': '-118.21', 'loss/train': '0.69329', 'examples_per_second': '30.219', 'grad_norm': '17.875', 'counters/examples': 8960, 'counters/updates': 280}
train stats after 8992 examples: {'rewards_train/chosen': '-0.018956', 'rewards_train/rejected': '-0.0089114', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010045', 'logps_train/rejected': '-115.05', 'logps_train/chosen': '-138.2', 'loss/train': '0.70212', 'examples_per_second': '29.863', 'grad_norm': '22', 'counters/examples': 8992, 'counters/updates': 281}
train stats after 9024 examples: {'rewards_train/chosen': '-0.0058598', 'rewards_train/rejected': '-0.044902', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039042', 'logps_train/rejected': '-115.81', 'logps_train/chosen': '-149.72', 'loss/train': '0.67653', 'examples_per_second': '30.483', 'grad_norm': '20', 'counters/examples': 9024, 'counters/updates': 282}
skipping logging after 9056 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '0.0068088', 'rewards_train/rejected': '0.019847', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013038', 'logps_train/rejected': '-123', 'logps_train/chosen': '-120.41', 'loss/train': '0.70195', 'examples_per_second': '31.05', 'grad_norm': '20.625', 'counters/examples': 9088, 'counters/updates': 284}
train stats after 9120 examples: {'rewards_train/chosen': '0.0017136', 'rewards_train/rejected': '0.0072954', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0055818', 'logps_train/rejected': '-100.18', 'logps_train/chosen': '-142.67', 'loss/train': '0.69847', 'examples_per_second': '31.063', 'grad_norm': '20.125', 'counters/examples': 9120, 'counters/updates': 285}
train stats after 9152 examples: {'rewards_train/chosen': '-0.0092856', 'rewards_train/rejected': '-0.0020013', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0072843', 'logps_train/rejected': '-117.88', 'logps_train/chosen': '-126.32', 'loss/train': '0.70022', 'examples_per_second': '30.61', 'grad_norm': '20.5', 'counters/examples': 9152, 'counters/updates': 286}
train stats after 9184 examples: {'rewards_train/chosen': '0.028682', 'rewards_train/rejected': '-0.017421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046102', 'logps_train/rejected': '-135.34', 'logps_train/chosen': '-140.94', 'loss/train': '0.67358', 'examples_per_second': '31.324', 'grad_norm': '20.875', 'counters/examples': 9184, 'counters/updates': 287}
train stats after 9216 examples: {'rewards_train/chosen': '-0.006722', 'rewards_train/rejected': '-0.018966', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012244', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-151.47', 'loss/train': '0.69023', 'examples_per_second': '31.177', 'grad_norm': '21.375', 'counters/examples': 9216, 'counters/updates': 288}
train stats after 9248 examples: {'rewards_train/chosen': '0.013559', 'rewards_train/rejected': '-0.0032231', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016783', 'logps_train/rejected': '-106.67', 'logps_train/chosen': '-176.62', 'loss/train': '0.68947', 'examples_per_second': '30.968', 'grad_norm': '22.125', 'counters/examples': 9248, 'counters/updates': 289}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9312 examples: {'rewards_train/chosen': '-0.010191', 'rewards_train/rejected': '-0.034117', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023926', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-129.53', 'loss/train': '0.68342', 'examples_per_second': '30.95', 'grad_norm': '21', 'counters/examples': 9312, 'counters/updates': 291}
train stats after 9344 examples: {'rewards_train/chosen': '-0.013929', 'rewards_train/rejected': '-0.00032787', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013601', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-150.13', 'loss/train': '0.70249', 'examples_per_second': '31.287', 'grad_norm': '21.75', 'counters/examples': 9344, 'counters/updates': 292}
skipping logging after 9376 examples to avoid logging too frequently
train stats after 9408 examples: {'rewards_train/chosen': '-0.0058325', 'rewards_train/rejected': '-0.013941', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0081089', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-122.07', 'loss/train': '0.69185', 'examples_per_second': '31.291', 'grad_norm': '20.25', 'counters/examples': 9408, 'counters/updates': 294}
train stats after 9440 examples: {'rewards_train/chosen': '0.010622', 'rewards_train/rejected': '-0.0069651', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017587', 'logps_train/rejected': '-100.71', 'logps_train/chosen': '-122', 'loss/train': '0.6866', 'examples_per_second': '30.46', 'grad_norm': '18.375', 'counters/examples': 9440, 'counters/updates': 295}
train stats after 9472 examples: {'rewards_train/chosen': '-0.0073553', 'rewards_train/rejected': '-0.016818', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0094624', 'logps_train/rejected': '-110.64', 'logps_train/chosen': '-101.8', 'loss/train': '0.69042', 'examples_per_second': '31.198', 'grad_norm': '19.125', 'counters/examples': 9472, 'counters/updates': 296}
skipping logging after 9504 examples to avoid logging too frequently
train stats after 9536 examples: {'rewards_train/chosen': '0.030089', 'rewards_train/rejected': '0.049069', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01898', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-125.26', 'loss/train': '0.70628', 'examples_per_second': '31.185', 'grad_norm': '20.125', 'counters/examples': 9536, 'counters/updates': 298}
train stats after 9568 examples: {'rewards_train/chosen': '-0.0043295', 'rewards_train/rejected': '-0.0087137', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0043842', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-108.49', 'loss/train': '0.69279', 'examples_per_second': '31.197', 'grad_norm': '18.875', 'counters/examples': 9568, 'counters/updates': 299}
train stats after 9600 examples: {'rewards_train/chosen': '-0.038581', 'rewards_train/rejected': '0.010365', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.048945', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-166.4', 'loss/train': '0.72212', 'examples_per_second': '31.071', 'grad_norm': '22.875', 'counters/examples': 9600, 'counters/updates': 300}
train stats after 9632 examples: {'rewards_train/chosen': '0.018956', 'rewards_train/rejected': '0.00066388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018292', 'logps_train/rejected': '-124.58', 'logps_train/chosen': '-182.54', 'loss/train': '0.68677', 'examples_per_second': '31.266', 'grad_norm': '22.125', 'counters/examples': 9632, 'counters/updates': 301}
train stats after 9664 examples: {'rewards_train/chosen': '-0.018708', 'rewards_train/rejected': '-0.0013358', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017372', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-169.18', 'loss/train': '0.70479', 'examples_per_second': '30.61', 'grad_norm': '22.5', 'counters/examples': 9664, 'counters/updates': 302}
train stats after 9696 examples: {'rewards_train/chosen': '0.029858', 'rewards_train/rejected': '-0.0018775', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.031735', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-135.75', 'loss/train': '0.67979', 'examples_per_second': '29.763', 'grad_norm': '20.125', 'counters/examples': 9696, 'counters/updates': 303}
train stats after 9728 examples: {'rewards_train/chosen': '0.034116', 'rewards_train/rejected': '0.032518', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0015983', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-141.54', 'loss/train': '0.69358', 'examples_per_second': '31.402', 'grad_norm': '20.5', 'counters/examples': 9728, 'counters/updates': 304}
train stats after 9760 examples: {'rewards_train/chosen': '0.021399', 'rewards_train/rejected': '-0.047278', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068677', 'logps_train/rejected': '-198.28', 'logps_train/chosen': '-166.82', 'loss/train': '0.66371', 'examples_per_second': '31.269', 'grad_norm': '23.75', 'counters/examples': 9760, 'counters/updates': 305}
train stats after 9792 examples: {'rewards_train/chosen': '-0.011232', 'rewards_train/rejected': '0.0050056', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016238', 'logps_train/rejected': '-120.57', 'logps_train/chosen': '-158.17', 'loss/train': '0.70319', 'examples_per_second': '30.682', 'grad_norm': '21.375', 'counters/examples': 9792, 'counters/updates': 306}
train stats after 9824 examples: {'rewards_train/chosen': '-6.375e-06', 'rewards_train/rejected': '0.010663', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01067', 'logps_train/rejected': '-86.698', 'logps_train/chosen': '-116.24', 'loss/train': '0.69945', 'examples_per_second': '32.08', 'grad_norm': '17.625', 'counters/examples': 9824, 'counters/updates': 307}
train stats after 9856 examples: {'rewards_train/chosen': '0.01398', 'rewards_train/rejected': '0.0097791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0042005', 'logps_train/rejected': '-90.171', 'logps_train/chosen': '-103.1', 'loss/train': '0.69225', 'examples_per_second': '31.676', 'grad_norm': '17.375', 'counters/examples': 9856, 'counters/updates': 308}
train stats after 9888 examples: {'rewards_train/chosen': '-0.026452', 'rewards_train/rejected': '-0.017098', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.009354', 'logps_train/rejected': '-112.36', 'logps_train/chosen': '-123.92', 'loss/train': '0.70103', 'examples_per_second': '30.54', 'grad_norm': '20.125', 'counters/examples': 9888, 'counters/updates': 309}
train stats after 9920 examples: {'rewards_train/chosen': '0.011739', 'rewards_train/rejected': '-0.0056138', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017353', 'logps_train/rejected': '-111.9', 'logps_train/chosen': '-99.004', 'loss/train': '0.68625', 'examples_per_second': '31.741', 'grad_norm': '17.5', 'counters/examples': 9920, 'counters/updates': 310}
train stats after 9952 examples: {'rewards_train/chosen': '-0.0093186', 'rewards_train/rejected': '-0.00054727', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0087714', 'logps_train/rejected': '-119.41', 'logps_train/chosen': '-153.52', 'loss/train': '0.70057', 'examples_per_second': '31.137', 'grad_norm': '21.5', 'counters/examples': 9952, 'counters/updates': 311}
train stats after 9984 examples: {'rewards_train/chosen': '0.0021228', 'rewards_train/rejected': '-0.0046858', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0068086', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-113.08', 'loss/train': '0.69199', 'examples_per_second': '30.541', 'grad_norm': '19.5', 'counters/examples': 9984, 'counters/updates': 312}
train stats after 10016 examples: {'rewards_train/chosen': '0.015835', 'rewards_train/rejected': '0.030178', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014343', 'logps_train/rejected': '-98.608', 'logps_train/chosen': '-122.84', 'loss/train': '0.70192', 'examples_per_second': '31.696', 'grad_norm': '18', 'counters/examples': 10016, 'counters/updates': 313}
train stats after 10048 examples: {'rewards_train/chosen': '-0.0039561', 'rewards_train/rejected': '-0.013359', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0094032', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-133.73', 'loss/train': '0.69121', 'examples_per_second': '29.889', 'grad_norm': '20.375', 'counters/examples': 10048, 'counters/updates': 314}
train stats after 10080 examples: {'rewards_train/chosen': '-0.063016', 'rewards_train/rejected': '-0.023847', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.039169', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-165.43', 'loss/train': '0.71452', 'examples_per_second': '31.191', 'grad_norm': '21.875', 'counters/examples': 10080, 'counters/updates': 315}
train stats after 10112 examples: {'rewards_train/chosen': '-0.028636', 'rewards_train/rejected': '-0.012549', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016087', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-169.49', 'loss/train': '0.70405', 'examples_per_second': '31.112', 'grad_norm': '23.125', 'counters/examples': 10112, 'counters/updates': 316}
train stats after 10144 examples: {'rewards_train/chosen': '0.012966', 'rewards_train/rejected': '-0.0047861', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017752', 'logps_train/rejected': '-109.87', 'logps_train/chosen': '-156.54', 'loss/train': '0.68647', 'examples_per_second': '30.542', 'grad_norm': '19.875', 'counters/examples': 10144, 'counters/updates': 317}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10208 examples: {'rewards_train/chosen': '-0.015612', 'rewards_train/rejected': '0.0039104', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019522', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-166.88', 'loss/train': '0.70526', 'examples_per_second': '30.223', 'grad_norm': '22.375', 'counters/examples': 10208, 'counters/updates': 319}
train stats after 10240 examples: {'rewards_train/chosen': '0.016683', 'rewards_train/rejected': '-0.020436', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037119', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-151.6', 'loss/train': '0.6764', 'examples_per_second': '31.219', 'grad_norm': '21', 'counters/examples': 10240, 'counters/updates': 320}
train stats after 10272 examples: {'rewards_train/chosen': '-0.010669', 'rewards_train/rejected': '0.00051454', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011184', 'logps_train/rejected': '-138.29', 'logps_train/chosen': '-172.79', 'loss/train': '0.70183', 'examples_per_second': '29.782', 'grad_norm': '22.25', 'counters/examples': 10272, 'counters/updates': 321}
train stats after 10304 examples: {'rewards_train/chosen': '0.0010959', 'rewards_train/rejected': '-0.013794', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01489', 'logps_train/rejected': '-115.67', 'logps_train/chosen': '-116.44', 'loss/train': '0.6875', 'examples_per_second': '31.238', 'grad_norm': '18.375', 'counters/examples': 10304, 'counters/updates': 322}
train stats after 10336 examples: {'rewards_train/chosen': '-0.02432', 'rewards_train/rejected': '0.019272', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.043592', 'logps_train/rejected': '-121.32', 'logps_train/chosen': '-163.6', 'loss/train': '0.72166', 'examples_per_second': '31.187', 'grad_norm': '24.125', 'counters/examples': 10336, 'counters/updates': 323}
train stats after 10368 examples: {'rewards_train/chosen': '0.018298', 'rewards_train/rejected': '0.0123', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0059975', 'logps_train/rejected': '-90.516', 'logps_train/chosen': '-120.54', 'loss/train': '0.69228', 'examples_per_second': '32.063', 'grad_norm': '19', 'counters/examples': 10368, 'counters/updates': 324}
train stats after 10400 examples: {'rewards_train/chosen': '-0.0076622', 'rewards_train/rejected': '-0.019237', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.011575', 'logps_train/rejected': '-121.25', 'logps_train/chosen': '-186.69', 'loss/train': '0.69055', 'examples_per_second': '32.977', 'grad_norm': '22.25', 'counters/examples': 10400, 'counters/updates': 325}
train stats after 10432 examples: {'rewards_train/chosen': '-0.010228', 'rewards_train/rejected': '0.013886', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024114', 'logps_train/rejected': '-148.23', 'logps_train/chosen': '-153.98', 'loss/train': '0.71155', 'examples_per_second': '31.093', 'grad_norm': '23.75', 'counters/examples': 10432, 'counters/updates': 326}
train stats after 10464 examples: {'rewards_train/chosen': '0.012371', 'rewards_train/rejected': '0.016319', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0039475', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-110.86', 'loss/train': '0.69791', 'examples_per_second': '31.23', 'grad_norm': '20.375', 'counters/examples': 10464, 'counters/updates': 327}
skipping logging after 10496 examples to avoid logging too frequently
train stats after 10528 examples: {'rewards_train/chosen': '0.053669', 'rewards_train/rejected': '0.0069504', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046718', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-148.05', 'loss/train': '0.67292', 'examples_per_second': '30.35', 'grad_norm': '20.875', 'counters/examples': 10528, 'counters/updates': 329}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10592 examples: {'rewards_train/chosen': '0.0078547', 'rewards_train/rejected': '0.0010217', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.006833', 'logps_train/rejected': '-109.39', 'logps_train/chosen': '-135.5', 'loss/train': '0.69157', 'examples_per_second': '30.428', 'grad_norm': '20.5', 'counters/examples': 10592, 'counters/updates': 331}
train stats after 10624 examples: {'rewards_train/chosen': '-0.013626', 'rewards_train/rejected': '-0.0046403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0089857', 'logps_train/rejected': '-92.149', 'logps_train/chosen': '-98.949', 'loss/train': '0.69977', 'examples_per_second': '30.889', 'grad_norm': '18.25', 'counters/examples': 10624, 'counters/updates': 332}
train stats after 10656 examples: {'rewards_train/chosen': '-0.0082675', 'rewards_train/rejected': '-0.026503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018235', 'logps_train/rejected': '-88.845', 'logps_train/chosen': '-115.85', 'loss/train': '0.68745', 'examples_per_second': '29.814', 'grad_norm': '18.5', 'counters/examples': 10656, 'counters/updates': 333}
train stats after 10688 examples: {'rewards_train/chosen': '-0.03626', 'rewards_train/rejected': '0.02296', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.05922', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-139.53', 'loss/train': '0.72588', 'examples_per_second': '31.067', 'grad_norm': '21.375', 'counters/examples': 10688, 'counters/updates': 334}
train stats after 10720 examples: {'rewards_train/chosen': '0.01838', 'rewards_train/rejected': '-0.015973', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034354', 'logps_train/rejected': '-129', 'logps_train/chosen': '-141.01', 'loss/train': '0.67788', 'examples_per_second': '29.799', 'grad_norm': '20.5', 'counters/examples': 10720, 'counters/updates': 335}
skipping logging after 10752 examples to avoid logging too frequently
train stats after 10784 examples: {'rewards_train/chosen': '-0.016463', 'rewards_train/rejected': '-0.025312', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.008849', 'logps_train/rejected': '-140.78', 'logps_train/chosen': '-169.7', 'loss/train': '0.69265', 'examples_per_second': '31.281', 'grad_norm': '24.5', 'counters/examples': 10784, 'counters/updates': 337}
train stats after 10816 examples: {'rewards_train/chosen': '0.013747', 'rewards_train/rejected': '-0.043239', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056986', 'logps_train/rejected': '-186.71', 'logps_train/chosen': '-146.59', 'loss/train': '0.66938', 'examples_per_second': '30.709', 'grad_norm': '21.5', 'counters/examples': 10816, 'counters/updates': 338}
train stats after 10848 examples: {'rewards_train/chosen': '0.0085145', 'rewards_train/rejected': '-0.023212', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031726', 'logps_train/rejected': '-164.4', 'logps_train/chosen': '-132.81', 'loss/train': '0.68132', 'examples_per_second': '31.209', 'grad_norm': '22.125', 'counters/examples': 10848, 'counters/updates': 339}
train stats after 10880 examples: {'rewards_train/chosen': '-0.031715', 'rewards_train/rejected': '0.0024491', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.034164', 'logps_train/rejected': '-166.96', 'logps_train/chosen': '-182.12', 'loss/train': '0.71406', 'examples_per_second': '30.355', 'grad_norm': '24.25', 'counters/examples': 10880, 'counters/updates': 340}
train stats after 10912 examples: {'rewards_train/chosen': '-0.019083', 'rewards_train/rejected': '0.028649', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047732', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-148.84', 'loss/train': '0.72073', 'examples_per_second': '31.223', 'grad_norm': '21.875', 'counters/examples': 10912, 'counters/updates': 341}
train stats after 10944 examples: {'rewards_train/chosen': '-0.0053969', 'rewards_train/rejected': '-0.017405', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012008', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-130.51', 'loss/train': '0.68945', 'examples_per_second': '31.253', 'grad_norm': '19.5', 'counters/examples': 10944, 'counters/updates': 342}
train stats after 10976 examples: {'rewards_train/chosen': '0.034751', 'rewards_train/rejected': '0.037812', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0030618', 'logps_train/rejected': '-125', 'logps_train/chosen': '-161.7', 'loss/train': '0.69722', 'examples_per_second': '30.262', 'grad_norm': '22', 'counters/examples': 10976, 'counters/updates': 343}
skipping logging after 11008 examples to avoid logging too frequently
train stats after 11040 examples: {'rewards_train/chosen': '0.030444', 'rewards_train/rejected': '0.017652', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012791', 'logps_train/rejected': '-97.431', 'logps_train/chosen': '-137.64', 'loss/train': '0.6885', 'examples_per_second': '30.676', 'grad_norm': '19.625', 'counters/examples': 11040, 'counters/updates': 345}
train stats after 11072 examples: {'rewards_train/chosen': '0.029088', 'rewards_train/rejected': '-0.015617', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044705', 'logps_train/rejected': '-153.01', 'logps_train/chosen': '-133.87', 'loss/train': '0.67388', 'examples_per_second': '30.966', 'grad_norm': '20.375', 'counters/examples': 11072, 'counters/updates': 346}
train stats after 11104 examples: {'rewards_train/chosen': '0.028445', 'rewards_train/rejected': '-0.0053767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.033822', 'logps_train/rejected': '-137.35', 'logps_train/chosen': '-143.18', 'loss/train': '0.68016', 'examples_per_second': '31.276', 'grad_norm': '21.75', 'counters/examples': 11104, 'counters/updates': 347}
train stats after 11136 examples: {'rewards_train/chosen': '0.021946', 'rewards_train/rejected': '-0.0010355', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022982', 'logps_train/rejected': '-152.58', 'logps_train/chosen': '-175.08', 'loss/train': '0.68658', 'examples_per_second': '30.478', 'grad_norm': '23.25', 'counters/examples': 11136, 'counters/updates': 348}
train stats after 11168 examples: {'rewards_train/chosen': '-0.0025762', 'rewards_train/rejected': '-0.034936', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.03236', 'logps_train/rejected': '-108.14', 'logps_train/chosen': '-126.22', 'loss/train': '0.6789', 'examples_per_second': '31.161', 'grad_norm': '19.125', 'counters/examples': 11168, 'counters/updates': 349}
train stats after 11200 examples: {'rewards_train/chosen': '-0.001801', 'rewards_train/rejected': '-0.034084', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032283', 'logps_train/rejected': '-144.8', 'logps_train/chosen': '-152.75', 'loss/train': '0.68087', 'examples_per_second': '32.133', 'grad_norm': '21.125', 'counters/examples': 11200, 'counters/updates': 350}
train stats after 11232 examples: {'rewards_train/chosen': '0.012343', 'rewards_train/rejected': '0.028098', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.015754', 'logps_train/rejected': '-157.87', 'logps_train/chosen': '-135.48', 'loss/train': '0.70364', 'examples_per_second': '31.251', 'grad_norm': '21.375', 'counters/examples': 11232, 'counters/updates': 351}
train stats after 11264 examples: {'rewards_train/chosen': '0.0086401', 'rewards_train/rejected': '-0.0085721', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017212', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-136', 'loss/train': '0.68637', 'examples_per_second': '29.755', 'grad_norm': '20.875', 'counters/examples': 11264, 'counters/updates': 352}
skipping logging after 11296 examples to avoid logging too frequently
train stats after 11328 examples: {'rewards_train/chosen': '0.039509', 'rewards_train/rejected': '-0.0077653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047274', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-144.56', 'loss/train': '0.67152', 'examples_per_second': '30.458', 'grad_norm': '19.375', 'counters/examples': 11328, 'counters/updates': 354}
train stats after 11360 examples: {'rewards_train/chosen': '0.013986', 'rewards_train/rejected': '0.0064276', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0075579', 'logps_train/rejected': '-134.03', 'logps_train/chosen': '-114.46', 'loss/train': '0.69248', 'examples_per_second': '30.289', 'grad_norm': '20.5', 'counters/examples': 11360, 'counters/updates': 355}
train stats after 11392 examples: {'rewards_train/chosen': '0.014188', 'rewards_train/rejected': '-0.0046749', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018863', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-157.44', 'loss/train': '0.68715', 'examples_per_second': '31.828', 'grad_norm': '20.5', 'counters/examples': 11392, 'counters/updates': 356}
train stats after 11424 examples: {'rewards_train/chosen': '0.018157', 'rewards_train/rejected': '-0.022983', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04114', 'logps_train/rejected': '-123.44', 'logps_train/chosen': '-147.92', 'loss/train': '0.67497', 'examples_per_second': '29.912', 'grad_norm': '20.875', 'counters/examples': 11424, 'counters/updates': 357}
train stats after 11456 examples: {'rewards_train/chosen': '-0.032026', 'rewards_train/rejected': '-0.015685', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016341', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-121.6', 'loss/train': '0.7031', 'examples_per_second': '31.24', 'grad_norm': '20.875', 'counters/examples': 11456, 'counters/updates': 358}
train stats after 11488 examples: {'rewards_train/chosen': '-0.0078201', 'rewards_train/rejected': '-0.036299', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028479', 'logps_train/rejected': '-100.32', 'logps_train/chosen': '-140.55', 'loss/train': '0.68226', 'examples_per_second': '32.44', 'grad_norm': '20.125', 'counters/examples': 11488, 'counters/updates': 359}
train stats after 11520 examples: {'rewards_train/chosen': '0.0063644', 'rewards_train/rejected': '-0.023216', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029581', 'logps_train/rejected': '-119.41', 'logps_train/chosen': '-170.91', 'loss/train': '0.68249', 'examples_per_second': '31.191', 'grad_norm': '21.25', 'counters/examples': 11520, 'counters/updates': 360}
train stats after 11552 examples: {'rewards_train/chosen': '-0.0038804', 'rewards_train/rejected': '-0.012777', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0088963', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-128.64', 'loss/train': '0.69045', 'examples_per_second': '30.826', 'grad_norm': '20', 'counters/examples': 11552, 'counters/updates': 361}
train stats after 11584 examples: {'rewards_train/chosen': '0.017186', 'rewards_train/rejected': '0.032019', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014833', 'logps_train/rejected': '-97.668', 'logps_train/chosen': '-131.47', 'loss/train': '0.70255', 'examples_per_second': '31.206', 'grad_norm': '20.25', 'counters/examples': 11584, 'counters/updates': 362}
skipping logging after 11616 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '0.020247', 'rewards_train/rejected': '-0.021543', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04179', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-118.14', 'loss/train': '0.67447', 'examples_per_second': '30.153', 'grad_norm': '19', 'counters/examples': 11648, 'counters/updates': 364}
train stats after 11680 examples: {'rewards_train/chosen': '0.019143', 'rewards_train/rejected': '-0.040203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059346', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-136.88', 'loss/train': '0.66703', 'examples_per_second': '32.827', 'grad_norm': '18.25', 'counters/examples': 11680, 'counters/updates': 365}
train stats after 11712 examples: {'rewards_train/chosen': '-0.0015865', 'rewards_train/rejected': '-0.0042775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.002691', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-121.82', 'loss/train': '0.69571', 'examples_per_second': '31.202', 'grad_norm': '19.75', 'counters/examples': 11712, 'counters/updates': 366}
train stats after 11744 examples: {'rewards_train/chosen': '-0.041031', 'rewards_train/rejected': '-0.024384', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016646', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-183.48', 'loss/train': '0.70419', 'examples_per_second': '30.428', 'grad_norm': '22.625', 'counters/examples': 11744, 'counters/updates': 367}
train stats after 11776 examples: {'rewards_train/chosen': '-0.021024', 'rewards_train/rejected': '-0.018779', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0022449', 'logps_train/rejected': '-99.199', 'logps_train/chosen': '-153.49', 'loss/train': '0.69762', 'examples_per_second': '31.184', 'grad_norm': '21.25', 'counters/examples': 11776, 'counters/updates': 368}
train stats after 11808 examples: {'rewards_train/chosen': '-0.00073784', 'rewards_train/rejected': '0.006031', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0067689', 'logps_train/rejected': '-117.24', 'logps_train/chosen': '-116.07', 'loss/train': '0.69906', 'examples_per_second': '31.061', 'grad_norm': '21.75', 'counters/examples': 11808, 'counters/updates': 369}
train stats after 11840 examples: {'rewards_train/chosen': '-0.01343', 'rewards_train/rejected': '-0.0064803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0069498', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-143.85', 'loss/train': '0.6996', 'examples_per_second': '30.903', 'grad_norm': '20.625', 'counters/examples': 11840, 'counters/updates': 370}
train stats after 11872 examples: {'rewards_train/chosen': '0.032333', 'rewards_train/rejected': '-0.016542', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048875', 'logps_train/rejected': '-164.56', 'logps_train/chosen': '-148.48', 'loss/train': '0.6733', 'examples_per_second': '31.121', 'grad_norm': '24', 'counters/examples': 11872, 'counters/updates': 371}
train stats after 11904 examples: {'rewards_train/chosen': '0.028248', 'rewards_train/rejected': '-0.003361', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031609', 'logps_train/rejected': '-140', 'logps_train/chosen': '-145.07', 'loss/train': '0.68027', 'examples_per_second': '29.83', 'grad_norm': '20', 'counters/examples': 11904, 'counters/updates': 372}
train stats after 11936 examples: {'rewards_train/chosen': '0.022182', 'rewards_train/rejected': '0.011857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010325', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-137.62', 'loss/train': '0.6899', 'examples_per_second': '31.498', 'grad_norm': '21.5', 'counters/examples': 11936, 'counters/updates': 373}
train stats after 11968 examples: {'rewards_train/chosen': '0.026408', 'rewards_train/rejected': '0.037171', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.010764', 'logps_train/rejected': '-140.67', 'logps_train/chosen': '-148.57', 'loss/train': '0.70345', 'examples_per_second': '31.205', 'grad_norm': '22.375', 'counters/examples': 11968, 'counters/updates': 374}
train stats after 12000 examples: {'rewards_train/chosen': '0.021783', 'rewards_train/rejected': '0.011003', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01078', 'logps_train/rejected': '-103.39', 'logps_train/chosen': '-115.03', 'loss/train': '0.68976', 'examples_per_second': '31.194', 'grad_norm': '19.25', 'counters/examples': 12000, 'counters/updates': 375}
Running evaluation after 12000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.05it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.72it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.84it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.75it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.88it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.84it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.88it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.81it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.81it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.82it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.74it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.80it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.71it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.80it/s]
eval after 12000: {'rewards_eval/chosen': '-0.0046972', 'rewards_eval/rejected': '-0.0098446', 'rewards_eval/accuracies': '0.50781', 'rewards_eval/margins': '0.0051474', 'logps_eval/rejected': '-115.49', 'logps_eval/chosen': '-135.59', 'loss/eval': '0.69291'}
skipping save for non epoch
train stats after 12032 examples: {'rewards_train/chosen': '-0.032753', 'rewards_train/rejected': '0.0069363', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03969', 'logps_train/rejected': '-106.74', 'logps_train/chosen': '-164.11', 'loss/train': '0.71626', 'examples_per_second': '31.417', 'grad_norm': '21.625', 'counters/examples': 12032, 'counters/updates': 376}
skipping logging after 12064 examples to avoid logging too frequently
train stats after 12096 examples: {'rewards_train/chosen': '0.023949', 'rewards_train/rejected': '0.0033562', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020593', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-116', 'loss/train': '0.68622', 'examples_per_second': '29.674', 'grad_norm': '20', 'counters/examples': 12096, 'counters/updates': 378}
skipping logging after 12128 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.0061378', 'rewards_train/rejected': '-0.064417', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058279', 'logps_train/rejected': '-124.91', 'logps_train/chosen': '-157.93', 'loss/train': '0.66966', 'examples_per_second': '32.073', 'grad_norm': '20.25', 'counters/examples': 12160, 'counters/updates': 380}
train stats after 12192 examples: {'rewards_train/chosen': '0.0052204', 'rewards_train/rejected': '-0.014308', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019529', 'logps_train/rejected': '-135.8', 'logps_train/chosen': '-149.42', 'loss/train': '0.68663', 'examples_per_second': '30.053', 'grad_norm': '21.25', 'counters/examples': 12192, 'counters/updates': 381}
train stats after 12224 examples: {'rewards_train/chosen': '-0.00023426', 'rewards_train/rejected': '-0.02778', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027546', 'logps_train/rejected': '-93.587', 'logps_train/chosen': '-107.84', 'loss/train': '0.68205', 'examples_per_second': '32.543', 'grad_norm': '17.75', 'counters/examples': 12224, 'counters/updates': 382}
train stats after 12256 examples: {'rewards_train/chosen': '0.036815', 'rewards_train/rejected': '0.0075178', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029297', 'logps_train/rejected': '-110.96', 'logps_train/chosen': '-128.85', 'loss/train': '0.68008', 'examples_per_second': '31.026', 'grad_norm': '19.5', 'counters/examples': 12256, 'counters/updates': 383}
train stats after 12288 examples: {'rewards_train/chosen': '-0.0064806', 'rewards_train/rejected': '-0.016273', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0097925', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-149.09', 'loss/train': '0.69146', 'examples_per_second': '30.688', 'grad_norm': '21.375', 'counters/examples': 12288, 'counters/updates': 384}
train stats after 12320 examples: {'rewards_train/chosen': '0.0085514', 'rewards_train/rejected': '0.0024631', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0060883', 'logps_train/rejected': '-128.71', 'logps_train/chosen': '-144.49', 'loss/train': '0.69385', 'examples_per_second': '32.819', 'grad_norm': '21', 'counters/examples': 12320, 'counters/updates': 385}
skipping logging after 12352 examples to avoid logging too frequently
train stats after 12384 examples: {'rewards_train/chosen': '0.0055996', 'rewards_train/rejected': '0.00065082', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0049487', 'logps_train/rejected': '-94.172', 'logps_train/chosen': '-117.02', 'loss/train': '0.69223', 'examples_per_second': '31.259', 'grad_norm': '17.75', 'counters/examples': 12384, 'counters/updates': 387}
train stats after 12416 examples: {'rewards_train/chosen': '-0.010513', 'rewards_train/rejected': '-0.019', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0084874', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-104.1', 'loss/train': '0.69151', 'examples_per_second': '31.125', 'grad_norm': '19.875', 'counters/examples': 12416, 'counters/updates': 388}
train stats after 12448 examples: {'rewards_train/chosen': '0.0064805', 'rewards_train/rejected': '0.00049456', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0059859', 'logps_train/rejected': '-156.72', 'logps_train/chosen': '-176.85', 'loss/train': '0.69511', 'examples_per_second': '29.686', 'grad_norm': '23.875', 'counters/examples': 12448, 'counters/updates': 389}
train stats after 12480 examples: {'rewards_train/chosen': '-0.021698', 'rewards_train/rejected': '-0.011397', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010301', 'logps_train/rejected': '-124.84', 'logps_train/chosen': '-172.02', 'loss/train': '0.70168', 'examples_per_second': '30.413', 'grad_norm': '23.125', 'counters/examples': 12480, 'counters/updates': 390}
train stats after 12512 examples: {'rewards_train/chosen': '0.028781', 'rewards_train/rejected': '-0.0095744', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038356', 'logps_train/rejected': '-128.37', 'logps_train/chosen': '-143.27', 'loss/train': '0.67659', 'examples_per_second': '30.487', 'grad_norm': '19.875', 'counters/examples': 12512, 'counters/updates': 391}
train stats after 12544 examples: {'rewards_train/chosen': '0.042951', 'rewards_train/rejected': '0.020419', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022532', 'logps_train/rejected': '-88.109', 'logps_train/chosen': '-155.12', 'loss/train': '0.68449', 'examples_per_second': '31.208', 'grad_norm': '19.25', 'counters/examples': 12544, 'counters/updates': 392}
train stats after 12576 examples: {'rewards_train/chosen': '0.0036871', 'rewards_train/rejected': '-0.013511', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017198', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-120.17', 'loss/train': '0.68614', 'examples_per_second': '31.479', 'grad_norm': '19.875', 'counters/examples': 12576, 'counters/updates': 393}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12640 examples: {'rewards_train/chosen': '0.00037987', 'rewards_train/rejected': '-0.0098762', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010256', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-139.58', 'loss/train': '0.69112', 'examples_per_second': '29.742', 'grad_norm': '20.625', 'counters/examples': 12640, 'counters/updates': 395}
train stats after 12672 examples: {'rewards_train/chosen': '-0.00355', 'rewards_train/rejected': '0.0029307', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0064807', 'logps_train/rejected': '-128.55', 'logps_train/chosen': '-147.74', 'loss/train': '0.69845', 'examples_per_second': '30.081', 'grad_norm': '21.75', 'counters/examples': 12672, 'counters/updates': 396}
train stats after 12704 examples: {'rewards_train/chosen': '0.012181', 'rewards_train/rejected': '0.018964', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0067826', 'logps_train/rejected': '-101.75', 'logps_train/chosen': '-124.16', 'loss/train': '0.69835', 'examples_per_second': '32.839', 'grad_norm': '19.25', 'counters/examples': 12704, 'counters/updates': 397}
train stats after 12736 examples: {'rewards_train/chosen': '0.0088232', 'rewards_train/rejected': '-0.0054697', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014293', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-138.26', 'loss/train': '0.68866', 'examples_per_second': '24.962', 'grad_norm': '21', 'counters/examples': 12736, 'counters/updates': 398}
train stats after 12768 examples: {'rewards_train/chosen': '0.046215', 'rewards_train/rejected': '0.010652', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035564', 'logps_train/rejected': '-110.49', 'logps_train/chosen': '-136.59', 'loss/train': '0.6775', 'examples_per_second': '30.617', 'grad_norm': '20.25', 'counters/examples': 12768, 'counters/updates': 399}
train stats after 12800 examples: {'rewards_train/chosen': '0.020024', 'rewards_train/rejected': '0.013806', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0062176', 'logps_train/rejected': '-109.7', 'logps_train/chosen': '-129.6', 'loss/train': '0.69196', 'examples_per_second': '31.165', 'grad_norm': '20.5', 'counters/examples': 12800, 'counters/updates': 400}
skipping logging after 12832 examples to avoid logging too frequently
train stats after 12864 examples: {'rewards_train/chosen': '0.020955', 'rewards_train/rejected': '0.005898', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015057', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-148.59', 'loss/train': '0.68783', 'examples_per_second': '31.18', 'grad_norm': '21.75', 'counters/examples': 12864, 'counters/updates': 402}
train stats after 12896 examples: {'rewards_train/chosen': '0.0073011', 'rewards_train/rejected': '0.01325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0059486', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-124.33', 'loss/train': '0.69876', 'examples_per_second': '31.157', 'grad_norm': '19.875', 'counters/examples': 12896, 'counters/updates': 403}
skipping logging after 12928 examples to avoid logging too frequently
train stats after 12960 examples: {'rewards_train/chosen': '-0.010395', 'rewards_train/rejected': '-0.0052589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0051361', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-142.28', 'loss/train': '0.69812', 'examples_per_second': '33.268', 'grad_norm': '20.125', 'counters/examples': 12960, 'counters/updates': 405}
train stats after 12992 examples: {'rewards_train/chosen': '-0.0026047', 'rewards_train/rejected': '0.00025606', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0028608', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-126.01', 'loss/train': '0.69901', 'examples_per_second': '30.637', 'grad_norm': '22.875', 'counters/examples': 12992, 'counters/updates': 406}
train stats after 13024 examples: {'rewards_train/chosen': '0.0095882', 'rewards_train/rejected': '0.035531', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.025943', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-119.05', 'loss/train': '0.70783', 'examples_per_second': '30.578', 'grad_norm': '19.25', 'counters/examples': 13024, 'counters/updates': 407}
train stats after 13056 examples: {'rewards_train/chosen': '0.038072', 'rewards_train/rejected': '0.0027776', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035295', 'logps_train/rejected': '-101.74', 'logps_train/chosen': '-108.16', 'loss/train': '0.67758', 'examples_per_second': '31.745', 'grad_norm': '17.25', 'counters/examples': 13056, 'counters/updates': 408}
train stats after 13088 examples: {'rewards_train/chosen': '0.051325', 'rewards_train/rejected': '0.024317', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027008', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-131.76', 'loss/train': '0.68101', 'examples_per_second': '32.781', 'grad_norm': '20.375', 'counters/examples': 13088, 'counters/updates': 409}
train stats after 13120 examples: {'rewards_train/chosen': '-0.0064161', 'rewards_train/rejected': '-0.030965', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024549', 'logps_train/rejected': '-124.78', 'logps_train/chosen': '-169.16', 'loss/train': '0.68303', 'examples_per_second': '31.01', 'grad_norm': '22.5', 'counters/examples': 13120, 'counters/updates': 410}
train stats after 13152 examples: {'rewards_train/chosen': '0.041807', 'rewards_train/rejected': '0.012469', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029338', 'logps_train/rejected': '-107.56', 'logps_train/chosen': '-140.69', 'loss/train': '0.67965', 'examples_per_second': '30.729', 'grad_norm': '19.5', 'counters/examples': 13152, 'counters/updates': 411}
train stats after 13184 examples: {'rewards_train/chosen': '0.029185', 'rewards_train/rejected': '-0.01294', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.042125', 'logps_train/rejected': '-126.55', 'logps_train/chosen': '-135.03', 'loss/train': '0.67331', 'examples_per_second': '30.249', 'grad_norm': '19.75', 'counters/examples': 13184, 'counters/updates': 412}
skipping logging after 13216 examples to avoid logging too frequently
train stats after 13248 examples: {'rewards_train/chosen': '0.025397', 'rewards_train/rejected': '0.0014476', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023949', 'logps_train/rejected': '-111.32', 'logps_train/chosen': '-150.39', 'loss/train': '0.68405', 'examples_per_second': '33.672', 'grad_norm': '20.375', 'counters/examples': 13248, 'counters/updates': 414}
train stats after 13280 examples: {'rewards_train/chosen': '-0.0087541', 'rewards_train/rejected': '0.011138', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019893', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-145.5', 'loss/train': '0.7057', 'examples_per_second': '31.08', 'grad_norm': '21.75', 'counters/examples': 13280, 'counters/updates': 415}
skipping logging after 13312 examples to avoid logging too frequently
train stats after 13344 examples: {'rewards_train/chosen': '-0.014991', 'rewards_train/rejected': '0.009667', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024658', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-142.7', 'loss/train': '0.70804', 'examples_per_second': '31.073', 'grad_norm': '21.75', 'counters/examples': 13344, 'counters/updates': 417}
train stats after 13376 examples: {'rewards_train/chosen': '-0.021131', 'rewards_train/rejected': '-0.019477', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0016545', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-134.02', 'loss/train': '0.69697', 'examples_per_second': '29.682', 'grad_norm': '20.875', 'counters/examples': 13376, 'counters/updates': 418}
train stats after 13408 examples: {'rewards_train/chosen': '0.070313', 'rewards_train/rejected': '-0.0015218', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.071835', 'logps_train/rejected': '-126.08', 'logps_train/chosen': '-146.92', 'loss/train': '0.65968', 'examples_per_second': '31.057', 'grad_norm': '20.5', 'counters/examples': 13408, 'counters/updates': 419}
train stats after 13440 examples: {'rewards_train/chosen': '0.026391', 'rewards_train/rejected': '0.014119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012272', 'logps_train/rejected': '-126.48', 'logps_train/chosen': '-130.34', 'loss/train': '0.68906', 'examples_per_second': '31.067', 'grad_norm': '20.125', 'counters/examples': 13440, 'counters/updates': 420}
train stats after 13472 examples: {'rewards_train/chosen': '-0.0039347', 'rewards_train/rejected': '0.014274', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018209', 'logps_train/rejected': '-86.189', 'logps_train/chosen': '-130.43', 'loss/train': '0.70429', 'examples_per_second': '24.84', 'grad_norm': '19.25', 'counters/examples': 13472, 'counters/updates': 421}
train stats after 13504 examples: {'rewards_train/chosen': '-0.024265', 'rewards_train/rejected': '-0.0057199', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.018545', 'logps_train/rejected': '-142.12', 'logps_train/chosen': '-157.04', 'loss/train': '0.70486', 'examples_per_second': '31.126', 'grad_norm': '22.5', 'counters/examples': 13504, 'counters/updates': 422}
skipping logging after 13536 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '0.046251', 'rewards_train/rejected': '0.0069083', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.039343', 'logps_train/rejected': '-109.58', 'logps_train/chosen': '-137.45', 'loss/train': '0.67625', 'examples_per_second': '23.703', 'grad_norm': '20', 'counters/examples': 13568, 'counters/updates': 424}
train stats after 13600 examples: {'rewards_train/chosen': '0.021266', 'rewards_train/rejected': '-0.0068093', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028076', 'logps_train/rejected': '-132.89', 'logps_train/chosen': '-148.2', 'loss/train': '0.68209', 'examples_per_second': '30.675', 'grad_norm': '20.875', 'counters/examples': 13600, 'counters/updates': 425}
train stats after 13632 examples: {'rewards_train/chosen': '-0.012546', 'rewards_train/rejected': '0.0043863', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.016932', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-118.2', 'loss/train': '0.70496', 'examples_per_second': '30.593', 'grad_norm': '19.375', 'counters/examples': 13632, 'counters/updates': 426}
train stats after 13664 examples: {'rewards_train/chosen': '-0.00018791', 'rewards_train/rejected': '0.0026292', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0028172', 'logps_train/rejected': '-121.69', 'logps_train/chosen': '-129.39', 'loss/train': '0.69673', 'examples_per_second': '33.016', 'grad_norm': '19.5', 'counters/examples': 13664, 'counters/updates': 427}
train stats after 13696 examples: {'rewards_train/chosen': '-0.0052826', 'rewards_train/rejected': '-0.0080285', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0027459', 'logps_train/rejected': '-140.78', 'logps_train/chosen': '-149.45', 'loss/train': '0.69444', 'examples_per_second': '31.56', 'grad_norm': '21.75', 'counters/examples': 13696, 'counters/updates': 428}
train stats after 13728 examples: {'rewards_train/chosen': '0.047217', 'rewards_train/rejected': '-0.003016', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050233', 'logps_train/rejected': '-114.42', 'logps_train/chosen': '-160.04', 'loss/train': '0.67266', 'examples_per_second': '30.14', 'grad_norm': '20.5', 'counters/examples': 13728, 'counters/updates': 429}
train stats after 13760 examples: {'rewards_train/chosen': '-0.0036355', 'rewards_train/rejected': '-0.0070102', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0033747', 'logps_train/rejected': '-111.53', 'logps_train/chosen': '-160.78', 'loss/train': '0.69399', 'examples_per_second': '29.935', 'grad_norm': '21.75', 'counters/examples': 13760, 'counters/updates': 430}
train stats after 13792 examples: {'rewards_train/chosen': '-0.016762', 'rewards_train/rejected': '0.0039413', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020704', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-153.16', 'loss/train': '0.70601', 'examples_per_second': '30.459', 'grad_norm': '22.375', 'counters/examples': 13792, 'counters/updates': 431}
train stats after 13824 examples: {'rewards_train/chosen': '0.0057979', 'rewards_train/rejected': '0.0073619', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.001564', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-124.57', 'loss/train': '0.69619', 'examples_per_second': '31.137', 'grad_norm': '19.375', 'counters/examples': 13824, 'counters/updates': 432}
train stats after 13856 examples: {'rewards_train/chosen': '0.0129', 'rewards_train/rejected': '0.033677', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020777', 'logps_train/rejected': '-152.28', 'logps_train/chosen': '-144.61', 'loss/train': '0.70833', 'examples_per_second': '32.408', 'grad_norm': '23.125', 'counters/examples': 13856, 'counters/updates': 433}
train stats after 13888 examples: {'rewards_train/chosen': '0.039569', 'rewards_train/rejected': '0.0086265', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030942', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-126.41', 'loss/train': '0.67991', 'examples_per_second': '31.147', 'grad_norm': '20.625', 'counters/examples': 13888, 'counters/updates': 434}
skipping logging after 13920 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '0.0087975', 'rewards_train/rejected': '0.0015142', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0072833', 'logps_train/rejected': '-96.742', 'logps_train/chosen': '-122.52', 'loss/train': '0.69184', 'examples_per_second': '32.651', 'grad_norm': '18.75', 'counters/examples': 13952, 'counters/updates': 436}
train stats after 13984 examples: {'rewards_train/chosen': '0.010226', 'rewards_train/rejected': '-0.0048407', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.015066', 'logps_train/rejected': '-93.675', 'logps_train/chosen': '-121.02', 'loss/train': '0.68836', 'examples_per_second': '32.07', 'grad_norm': '19.625', 'counters/examples': 13984, 'counters/updates': 437}
train stats after 14016 examples: {'rewards_train/chosen': '0.014742', 'rewards_train/rejected': '-0.041949', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056691', 'logps_train/rejected': '-131.71', 'logps_train/chosen': '-153.64', 'loss/train': '0.6674', 'examples_per_second': '31.895', 'grad_norm': '21.25', 'counters/examples': 14016, 'counters/updates': 438}
train stats after 14048 examples: {'rewards_train/chosen': '0.033955', 'rewards_train/rejected': '0.028376', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0055792', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-113.42', 'loss/train': '0.69321', 'examples_per_second': '31.646', 'grad_norm': '19.625', 'counters/examples': 14048, 'counters/updates': 439}
train stats after 14080 examples: {'rewards_train/chosen': '0.0097752', 'rewards_train/rejected': '0.023131', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013356', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-123.55', 'loss/train': '0.7019', 'examples_per_second': '31.001', 'grad_norm': '19.625', 'counters/examples': 14080, 'counters/updates': 440}
train stats after 14112 examples: {'rewards_train/chosen': '-0.002201', 'rewards_train/rejected': '0.010458', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012659', 'logps_train/rejected': '-123.52', 'logps_train/chosen': '-146.06', 'loss/train': '0.70288', 'examples_per_second': '32.355', 'grad_norm': '21.375', 'counters/examples': 14112, 'counters/updates': 441}
train stats after 14144 examples: {'rewards_train/chosen': '0.010757', 'rewards_train/rejected': '-0.019419', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030176', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-123.48', 'loss/train': '0.68017', 'examples_per_second': '31.09', 'grad_norm': '18.875', 'counters/examples': 14144, 'counters/updates': 442}
train stats after 14176 examples: {'rewards_train/chosen': '-0.03867', 'rewards_train/rejected': '-0.023496', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015175', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-154.97', 'loss/train': '0.70648', 'examples_per_second': '32.433', 'grad_norm': '22.5', 'counters/examples': 14176, 'counters/updates': 443}
skipping logging after 14208 examples to avoid logging too frequently
train stats after 14240 examples: {'rewards_train/chosen': '-0.0043802', 'rewards_train/rejected': '0.039223', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043603', 'logps_train/rejected': '-101.3', 'logps_train/chosen': '-133.74', 'loss/train': '0.71684', 'examples_per_second': '31.201', 'grad_norm': '20.25', 'counters/examples': 14240, 'counters/updates': 445}
train stats after 14272 examples: {'rewards_train/chosen': '0.0036969', 'rewards_train/rejected': '-0.018235', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021932', 'logps_train/rejected': '-98.271', 'logps_train/chosen': '-110.39', 'loss/train': '0.68466', 'examples_per_second': '31.753', 'grad_norm': '18.25', 'counters/examples': 14272, 'counters/updates': 446}
skipping logging after 14304 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.042999', 'rewards_train/rejected': '0.0018867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041113', 'logps_train/rejected': '-140.78', 'logps_train/chosen': '-160.54', 'loss/train': '0.67547', 'examples_per_second': '30.178', 'grad_norm': '21.5', 'counters/examples': 14336, 'counters/updates': 448}
train stats after 14368 examples: {'rewards_train/chosen': '-0.021131', 'rewards_train/rejected': '0.015999', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03713', 'logps_train/rejected': '-94.105', 'logps_train/chosen': '-135.04', 'loss/train': '0.71451', 'examples_per_second': '29.65', 'grad_norm': '19.625', 'counters/examples': 14368, 'counters/updates': 449}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14432 examples: {'rewards_train/chosen': '-0.012076', 'rewards_train/rejected': '-0.012322', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.00024616', 'logps_train/rejected': '-136.39', 'logps_train/chosen': '-165.95', 'loss/train': '0.69593', 'examples_per_second': '31.232', 'grad_norm': '24', 'counters/examples': 14432, 'counters/updates': 451}
train stats after 14464 examples: {'rewards_train/chosen': '-0.0052726', 'rewards_train/rejected': '-0.0068671', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0015945', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-128.3', 'loss/train': '0.69563', 'examples_per_second': '31.243', 'grad_norm': '20', 'counters/examples': 14464, 'counters/updates': 452}
train stats after 14496 examples: {'rewards_train/chosen': '0.010468', 'rewards_train/rejected': '-0.018367', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028834', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-150.66', 'loss/train': '0.68268', 'examples_per_second': '30.24', 'grad_norm': '20.75', 'counters/examples': 14496, 'counters/updates': 453}
train stats after 14528 examples: {'rewards_train/chosen': '0.040619', 'rewards_train/rejected': '-0.0072095', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047828', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-112.06', 'loss/train': '0.67272', 'examples_per_second': '31.873', 'grad_norm': '19.25', 'counters/examples': 14528, 'counters/updates': 454}
skipping logging after 14560 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '0.029144', 'rewards_train/rejected': '-0.014837', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043982', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-130.46', 'loss/train': '0.6766', 'examples_per_second': '30.96', 'grad_norm': '19.5', 'counters/examples': 14592, 'counters/updates': 456}
train stats after 14624 examples: {'rewards_train/chosen': '0.0041218', 'rewards_train/rejected': '0.014981', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010859', 'logps_train/rejected': '-81.591', 'logps_train/chosen': '-106.62', 'loss/train': '0.70072', 'examples_per_second': '31.104', 'grad_norm': '18.25', 'counters/examples': 14624, 'counters/updates': 457}
train stats after 14656 examples: {'rewards_train/chosen': '0.031052', 'rewards_train/rejected': '-0.0059714', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037023', 'logps_train/rejected': '-107.68', 'logps_train/chosen': '-124.92', 'loss/train': '0.67833', 'examples_per_second': '31.101', 'grad_norm': '20.25', 'counters/examples': 14656, 'counters/updates': 458}
skipping logging after 14688 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '0.022234', 'rewards_train/rejected': '0.0096454', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012589', 'logps_train/rejected': '-149.94', 'logps_train/chosen': '-173.39', 'loss/train': '0.68958', 'examples_per_second': '31.229', 'grad_norm': '23.25', 'counters/examples': 14720, 'counters/updates': 460}
train stats after 14752 examples: {'rewards_train/chosen': '0.037431', 'rewards_train/rejected': '-0.038551', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075982', 'logps_train/rejected': '-154.77', 'logps_train/chosen': '-158.62', 'loss/train': '0.65876', 'examples_per_second': '31.036', 'grad_norm': '20.625', 'counters/examples': 14752, 'counters/updates': 461}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14816 examples: {'rewards_train/chosen': '-0.035279', 'rewards_train/rejected': '0.010725', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046004', 'logps_train/rejected': '-140.81', 'logps_train/chosen': '-122.15', 'loss/train': '0.71969', 'examples_per_second': '31.046', 'grad_norm': '22', 'counters/examples': 14816, 'counters/updates': 463}
skipping logging after 14848 examples to avoid logging too frequently
train stats after 14880 examples: {'rewards_train/chosen': '0.0014857', 'rewards_train/rejected': '0.004008', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0025223', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-135.14', 'loss/train': '0.69721', 'examples_per_second': '32.017', 'grad_norm': '20.625', 'counters/examples': 14880, 'counters/updates': 465}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14944 examples: {'rewards_train/chosen': '0.0462', 'rewards_train/rejected': '0.016242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029958', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-126.23', 'loss/train': '0.68161', 'examples_per_second': '31.049', 'grad_norm': '19.625', 'counters/examples': 14944, 'counters/updates': 467}
train stats after 14976 examples: {'rewards_train/chosen': '-0.019649', 'rewards_train/rejected': '-0.0029585', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01669', 'logps_train/rejected': '-119.19', 'logps_train/chosen': '-115.27', 'loss/train': '0.70454', 'examples_per_second': '32.208', 'grad_norm': '19.5', 'counters/examples': 14976, 'counters/updates': 468}
train stats after 15008 examples: {'rewards_train/chosen': '0.0062928', 'rewards_train/rejected': '0.0051197', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0011731', 'logps_train/rejected': '-94.938', 'logps_train/chosen': '-94.966', 'loss/train': '0.69536', 'examples_per_second': '30.421', 'grad_norm': '17.375', 'counters/examples': 15008, 'counters/updates': 469}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15072 examples: {'rewards_train/chosen': '-0.0040083', 'rewards_train/rejected': '-0.01321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0092015', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-142.9', 'loss/train': '0.69161', 'examples_per_second': '29.878', 'grad_norm': '21.375', 'counters/examples': 15072, 'counters/updates': 471}
train stats after 15104 examples: {'rewards_train/chosen': '0.011065', 'rewards_train/rejected': '0.022707', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011641', 'logps_train/rejected': '-124.98', 'logps_train/chosen': '-129.93', 'loss/train': '0.70141', 'examples_per_second': '30.37', 'grad_norm': '21.375', 'counters/examples': 15104, 'counters/updates': 472}
train stats after 15136 examples: {'rewards_train/chosen': '0.035828', 'rewards_train/rejected': '-0.0078284', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043656', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-139.68', 'loss/train': '0.67365', 'examples_per_second': '32.133', 'grad_norm': '20.125', 'counters/examples': 15136, 'counters/updates': 473}
train stats after 15168 examples: {'rewards_train/chosen': '0.050946', 'rewards_train/rejected': '0.054587', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0036412', 'logps_train/rejected': '-123.87', 'logps_train/chosen': '-115.79', 'loss/train': '0.69707', 'examples_per_second': '31.018', 'grad_norm': '19.625', 'counters/examples': 15168, 'counters/updates': 474}
train stats after 15200 examples: {'rewards_train/chosen': '-0.014488', 'rewards_train/rejected': '0.003058', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.017546', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-164.93', 'loss/train': '0.7064', 'examples_per_second': '32.5', 'grad_norm': '20.875', 'counters/examples': 15200, 'counters/updates': 475}
train stats after 15232 examples: {'rewards_train/chosen': '0.018994', 'rewards_train/rejected': '0.0041065', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014887', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-109.73', 'loss/train': '0.68909', 'examples_per_second': '30.789', 'grad_norm': '18.25', 'counters/examples': 15232, 'counters/updates': 476}
train stats after 15264 examples: {'rewards_train/chosen': '0.015889', 'rewards_train/rejected': '-0.013226', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.029116', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-150.5', 'loss/train': '0.68169', 'examples_per_second': '30.414', 'grad_norm': '21.375', 'counters/examples': 15264, 'counters/updates': 477}
train stats after 15296 examples: {'rewards_train/chosen': '0.054557', 'rewards_train/rejected': '0.0041796', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.050377', 'logps_train/rejected': '-96.536', 'logps_train/chosen': '-155.52', 'loss/train': '0.67164', 'examples_per_second': '29.726', 'grad_norm': '19.75', 'counters/examples': 15296, 'counters/updates': 478}
skipping logging after 15328 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '0.0069139', 'rewards_train/rejected': '0.024932', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018018', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-148.68', 'loss/train': '0.70475', 'examples_per_second': '29.816', 'grad_norm': '20.375', 'counters/examples': 15360, 'counters/updates': 480}
train stats after 15392 examples: {'rewards_train/chosen': '-0.0020839', 'rewards_train/rejected': '-0.022094', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02001', 'logps_train/rejected': '-105', 'logps_train/chosen': '-134.94', 'loss/train': '0.68609', 'examples_per_second': '31.459', 'grad_norm': '19.875', 'counters/examples': 15392, 'counters/updates': 481}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15456 examples: {'rewards_train/chosen': '-0.00029962', 'rewards_train/rejected': '0.040524', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040824', 'logps_train/rejected': '-96.115', 'logps_train/chosen': '-126.88', 'loss/train': '0.71525', 'examples_per_second': '31.035', 'grad_norm': '19.125', 'counters/examples': 15456, 'counters/updates': 483}
train stats after 15488 examples: {'rewards_train/chosen': '-0.012369', 'rewards_train/rejected': '-0.012301', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-6.836e-05', 'logps_train/rejected': '-93.199', 'logps_train/chosen': '-115.03', 'loss/train': '0.69604', 'examples_per_second': '31.233', 'grad_norm': '18.375', 'counters/examples': 15488, 'counters/updates': 484}
train stats after 15520 examples: {'rewards_train/chosen': '0.033144', 'rewards_train/rejected': '0.017569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015576', 'logps_train/rejected': '-171.01', 'logps_train/chosen': '-173.75', 'loss/train': '0.69077', 'examples_per_second': '32.16', 'grad_norm': '23.25', 'counters/examples': 15520, 'counters/updates': 485}
train stats after 15552 examples: {'rewards_train/chosen': '0.017177', 'rewards_train/rejected': '0.039119', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.021942', 'logps_train/rejected': '-107.91', 'logps_train/chosen': '-132.06', 'loss/train': '0.70639', 'examples_per_second': '32.548', 'grad_norm': '21', 'counters/examples': 15552, 'counters/updates': 486}
train stats after 15584 examples: {'rewards_train/chosen': '0.063546', 'rewards_train/rejected': '-0.018637', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.082183', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-148.4', 'loss/train': '0.65518', 'examples_per_second': '29.854', 'grad_norm': '19.5', 'counters/examples': 15584, 'counters/updates': 487}
train stats after 15616 examples: {'rewards_train/chosen': '-0.026664', 'rewards_train/rejected': '0.0028687', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029533', 'logps_train/rejected': '-114.96', 'logps_train/chosen': '-138.72', 'loss/train': '0.71204', 'examples_per_second': '31.022', 'grad_norm': '20.625', 'counters/examples': 15616, 'counters/updates': 488}
skipping logging after 15648 examples to avoid logging too frequently
train stats after 15680 examples: {'rewards_train/chosen': '0.015741', 'rewards_train/rejected': '0.0073957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0083456', 'logps_train/rejected': '-118.7', 'logps_train/chosen': '-112.02', 'loss/train': '0.691', 'examples_per_second': '31.803', 'grad_norm': '18.875', 'counters/examples': 15680, 'counters/updates': 490}
train stats after 15712 examples: {'rewards_train/chosen': '-0.0082834', 'rewards_train/rejected': '0.0028066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01109', 'logps_train/rejected': '-138.26', 'logps_train/chosen': '-136.29', 'loss/train': '0.70191', 'examples_per_second': '32.066', 'grad_norm': '22', 'counters/examples': 15712, 'counters/updates': 491}
train stats after 15744 examples: {'rewards_train/chosen': '0.0090833', 'rewards_train/rejected': '0.0039287', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0051545', 'logps_train/rejected': '-110.56', 'logps_train/chosen': '-112.65', 'loss/train': '0.6942', 'examples_per_second': '30.899', 'grad_norm': '18.75', 'counters/examples': 15744, 'counters/updates': 492}
train stats after 15776 examples: {'rewards_train/chosen': '0.076875', 'rewards_train/rejected': '0.021681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055193', 'logps_train/rejected': '-116.49', 'logps_train/chosen': '-137.07', 'loss/train': '0.66793', 'examples_per_second': '31.154', 'grad_norm': '19.5', 'counters/examples': 15776, 'counters/updates': 493}
train stats after 15808 examples: {'rewards_train/chosen': '-0.012391', 'rewards_train/rejected': '0.01701', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029401', 'logps_train/rejected': '-142.74', 'logps_train/chosen': '-117', 'loss/train': '0.71049', 'examples_per_second': '31.007', 'grad_norm': '21.125', 'counters/examples': 15808, 'counters/updates': 494}
train stats after 15840 examples: {'rewards_train/chosen': '-0.0081875', 'rewards_train/rejected': '0.054018', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.062206', 'logps_train/rejected': '-133.2', 'logps_train/chosen': '-114.83', 'loss/train': '0.72874', 'examples_per_second': '31.242', 'grad_norm': '21.25', 'counters/examples': 15840, 'counters/updates': 495}
train stats after 15872 examples: {'rewards_train/chosen': '0.021836', 'rewards_train/rejected': '0.010766', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01107', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-137.44', 'loss/train': '0.68982', 'examples_per_second': '31.047', 'grad_norm': '21.125', 'counters/examples': 15872, 'counters/updates': 496}
skipping logging after 15904 examples to avoid logging too frequently
train stats after 15936 examples: {'rewards_train/chosen': '0.018524', 'rewards_train/rejected': '-0.0082866', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026811', 'logps_train/rejected': '-163.62', 'logps_train/chosen': '-114.92', 'loss/train': '0.68388', 'examples_per_second': '34.835', 'grad_norm': '20.375', 'counters/examples': 15936, 'counters/updates': 498}
train stats after 15968 examples: {'rewards_train/chosen': '0.046399', 'rewards_train/rejected': '0.029233', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017166', 'logps_train/rejected': '-148.5', 'logps_train/chosen': '-143.78', 'loss/train': '0.68833', 'examples_per_second': '30.259', 'grad_norm': '21.625', 'counters/examples': 15968, 'counters/updates': 499}
train stats after 16000 examples: {'rewards_train/chosen': '0.018689', 'rewards_train/rejected': '0.00025895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01843', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-149.53', 'loss/train': '0.68583', 'examples_per_second': '31.015', 'grad_norm': '19.75', 'counters/examples': 16000, 'counters/updates': 500}
train stats after 16032 examples: {'rewards_train/chosen': '-0.0038663', 'rewards_train/rejected': '-0.0077087', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0038423', 'logps_train/rejected': '-100.51', 'logps_train/chosen': '-157.76', 'loss/train': '0.69336', 'examples_per_second': '31.068', 'grad_norm': '20.875', 'counters/examples': 16032, 'counters/updates': 501}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16096 examples: {'rewards_train/chosen': '0.0048228', 'rewards_train/rejected': '0.00307', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0017529', 'logps_train/rejected': '-164.84', 'logps_train/chosen': '-157.8', 'loss/train': '0.69525', 'examples_per_second': '31.184', 'grad_norm': '23.625', 'counters/examples': 16096, 'counters/updates': 503}
train stats after 16128 examples: {'rewards_train/chosen': '0.058211', 'rewards_train/rejected': '-0.0043053', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062516', 'logps_train/rejected': '-115.27', 'logps_train/chosen': '-148.37', 'loss/train': '0.66602', 'examples_per_second': '31.504', 'grad_norm': '20', 'counters/examples': 16128, 'counters/updates': 504}
train stats after 16160 examples: {'rewards_train/chosen': '0.02707', 'rewards_train/rejected': '0.026727', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00034347', 'logps_train/rejected': '-112.57', 'logps_train/chosen': '-119.55', 'loss/train': '0.69687', 'examples_per_second': '31.563', 'grad_norm': '19.5', 'counters/examples': 16160, 'counters/updates': 505}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16224 examples: {'rewards_train/chosen': '-0.0096079', 'rewards_train/rejected': '0.043545', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053152', 'logps_train/rejected': '-118.85', 'logps_train/chosen': '-132.79', 'loss/train': '0.72289', 'examples_per_second': '31.634', 'grad_norm': '20.25', 'counters/examples': 16224, 'counters/updates': 507}
skipping logging after 16256 examples to avoid logging too frequently
train stats after 16288 examples: {'rewards_train/chosen': '0.030137', 'rewards_train/rejected': '-0.0098012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039939', 'logps_train/rejected': '-86.957', 'logps_train/chosen': '-116.82', 'loss/train': '0.67583', 'examples_per_second': '30.681', 'grad_norm': '17.875', 'counters/examples': 16288, 'counters/updates': 509}
skipping logging after 16320 examples to avoid logging too frequently
train stats after 16352 examples: {'rewards_train/chosen': '0.048223', 'rewards_train/rejected': '0.083621', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035398', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-129.51', 'loss/train': '0.71351', 'examples_per_second': '30.262', 'grad_norm': '21', 'counters/examples': 16352, 'counters/updates': 511}
train stats after 16384 examples: {'rewards_train/chosen': '-0.012223', 'rewards_train/rejected': '-0.014973', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0027498', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-155.12', 'loss/train': '0.69433', 'examples_per_second': '31.526', 'grad_norm': '21.875', 'counters/examples': 16384, 'counters/updates': 512}
skipping logging after 16416 examples to avoid logging too frequently
train stats after 16448 examples: {'rewards_train/chosen': '0.04813', 'rewards_train/rejected': '-0.015463', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063593', 'logps_train/rejected': '-134.69', 'logps_train/chosen': '-152.41', 'loss/train': '0.66561', 'examples_per_second': '31.523', 'grad_norm': '21.875', 'counters/examples': 16448, 'counters/updates': 514}
train stats after 16480 examples: {'rewards_train/chosen': '0.0074046', 'rewards_train/rejected': '-0.0094696', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016874', 'logps_train/rejected': '-92.794', 'logps_train/chosen': '-118.41', 'loss/train': '0.68712', 'examples_per_second': '31.533', 'grad_norm': '18.125', 'counters/examples': 16480, 'counters/updates': 515}
skipping logging after 16512 examples to avoid logging too frequently
train stats after 16544 examples: {'rewards_train/chosen': '-0.0031774', 'rewards_train/rejected': '0.020199', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023377', 'logps_train/rejected': '-104.38', 'logps_train/chosen': '-168.35', 'loss/train': '0.70764', 'examples_per_second': '31.83', 'grad_norm': '21', 'counters/examples': 16544, 'counters/updates': 517}
train stats after 16576 examples: {'rewards_train/chosen': '0.014623', 'rewards_train/rejected': '0.030481', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015859', 'logps_train/rejected': '-112.95', 'logps_train/chosen': '-104.36', 'loss/train': '0.70231', 'examples_per_second': '30.734', 'grad_norm': '18.5', 'counters/examples': 16576, 'counters/updates': 518}
train stats after 16608 examples: {'rewards_train/chosen': '0.014186', 'rewards_train/rejected': '0.031089', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016903', 'logps_train/rejected': '-100.47', 'logps_train/chosen': '-112.57', 'loss/train': '0.70388', 'examples_per_second': '32.744', 'grad_norm': '18.875', 'counters/examples': 16608, 'counters/updates': 519}
train stats after 16640 examples: {'rewards_train/chosen': '0.0023124', 'rewards_train/rejected': '-0.0015462', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0038586', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-145.48', 'loss/train': '0.69464', 'examples_per_second': '30.057', 'grad_norm': '21.375', 'counters/examples': 16640, 'counters/updates': 520}
train stats after 16672 examples: {'rewards_train/chosen': '0.017876', 'rewards_train/rejected': '-0.0082495', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026126', 'logps_train/rejected': '-124.36', 'logps_train/chosen': '-141.8', 'loss/train': '0.68263', 'examples_per_second': '30.701', 'grad_norm': '20.5', 'counters/examples': 16672, 'counters/updates': 521}
train stats after 16704 examples: {'rewards_train/chosen': '0.028721', 'rewards_train/rejected': '-0.0049032', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033624', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-121.3', 'loss/train': '0.67814', 'examples_per_second': '30.761', 'grad_norm': '20', 'counters/examples': 16704, 'counters/updates': 522}
train stats after 16736 examples: {'rewards_train/chosen': '-0.026333', 'rewards_train/rejected': '0.017556', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.043889', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-113.81', 'loss/train': '0.71732', 'examples_per_second': '31.492', 'grad_norm': '19.875', 'counters/examples': 16736, 'counters/updates': 523}
train stats after 16768 examples: {'rewards_train/chosen': '0.057501', 'rewards_train/rejected': '0.027566', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029934', 'logps_train/rejected': '-110.93', 'logps_train/chosen': '-140.46', 'loss/train': '0.68178', 'examples_per_second': '31.07', 'grad_norm': '20.25', 'counters/examples': 16768, 'counters/updates': 524}
train stats after 16800 examples: {'rewards_train/chosen': '0.00075309', 'rewards_train/rejected': '0.028161', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027408', 'logps_train/rejected': '-172.09', 'logps_train/chosen': '-157.44', 'loss/train': '0.70991', 'examples_per_second': '31.513', 'grad_norm': '23.375', 'counters/examples': 16800, 'counters/updates': 525}
train stats after 16832 examples: {'rewards_train/chosen': '-0.0052944', 'rewards_train/rejected': '-0.0067872', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0014929', 'logps_train/rejected': '-151.44', 'logps_train/chosen': '-117.5', 'loss/train': '0.69647', 'examples_per_second': '30.463', 'grad_norm': '21', 'counters/examples': 16832, 'counters/updates': 526}
train stats after 16864 examples: {'rewards_train/chosen': '0.039739', 'rewards_train/rejected': '0.02635', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.013389', 'logps_train/rejected': '-178.66', 'logps_train/chosen': '-130.55', 'loss/train': '0.69179', 'examples_per_second': '31.551', 'grad_norm': '23.5', 'counters/examples': 16864, 'counters/updates': 527}
train stats after 16896 examples: {'rewards_train/chosen': '0.0482', 'rewards_train/rejected': '-0.012366', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060565', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-165.68', 'loss/train': '0.66606', 'examples_per_second': '31.541', 'grad_norm': '20.875', 'counters/examples': 16896, 'counters/updates': 528}
train stats after 16928 examples: {'rewards_train/chosen': '-0.0021669', 'rewards_train/rejected': '0.0060072', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0081741', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-150.54', 'loss/train': '0.7007', 'examples_per_second': '29.975', 'grad_norm': '22.5', 'counters/examples': 16928, 'counters/updates': 529}
train stats after 16960 examples: {'rewards_train/chosen': '0.045595', 'rewards_train/rejected': '0.011117', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034478', 'logps_train/rejected': '-73.881', 'logps_train/chosen': '-111.14', 'loss/train': '0.67885', 'examples_per_second': '30.883', 'grad_norm': '16', 'counters/examples': 16960, 'counters/updates': 530}
train stats after 16992 examples: {'rewards_train/chosen': '-0.015576', 'rewards_train/rejected': '-0.0032819', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012295', 'logps_train/rejected': '-186.44', 'logps_train/chosen': '-153.7', 'loss/train': '0.70351', 'examples_per_second': '30.024', 'grad_norm': '23.625', 'counters/examples': 16992, 'counters/updates': 531}
train stats after 17024 examples: {'rewards_train/chosen': '0.021106', 'rewards_train/rejected': '0.021197', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-9.1073e-05', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-115.77', 'loss/train': '0.695', 'examples_per_second': '31.526', 'grad_norm': '19.25', 'counters/examples': 17024, 'counters/updates': 532}
train stats after 17056 examples: {'rewards_train/chosen': '0.028038', 'rewards_train/rejected': '-0.0056594', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033698', 'logps_train/rejected': '-138.88', 'logps_train/chosen': '-183.06', 'loss/train': '0.6818', 'examples_per_second': '31.688', 'grad_norm': '23.375', 'counters/examples': 17056, 'counters/updates': 533}
train stats after 17088 examples: {'rewards_train/chosen': '0.003042', 'rewards_train/rejected': '0.0031967', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00015474', 'logps_train/rejected': '-106.6', 'logps_train/chosen': '-158.68', 'loss/train': '0.69563', 'examples_per_second': '30.261', 'grad_norm': '21.5', 'counters/examples': 17088, 'counters/updates': 534}
train stats after 17120 examples: {'rewards_train/chosen': '-0.018691', 'rewards_train/rejected': '-0.010853', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0078376', 'logps_train/rejected': '-109.75', 'logps_train/chosen': '-132.04', 'loss/train': '0.70005', 'examples_per_second': '31.247', 'grad_norm': '20.125', 'counters/examples': 17120, 'counters/updates': 535}
train stats after 17152 examples: {'rewards_train/chosen': '0.025747', 'rewards_train/rejected': '-0.0032435', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028991', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-145.19', 'loss/train': '0.68381', 'examples_per_second': '30.12', 'grad_norm': '21.25', 'counters/examples': 17152, 'counters/updates': 536}
skipping logging after 17184 examples to avoid logging too frequently
train stats after 17216 examples: {'rewards_train/chosen': '0.027778', 'rewards_train/rejected': '0.061495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033717', 'logps_train/rejected': '-106.25', 'logps_train/chosen': '-125.65', 'loss/train': '0.71487', 'examples_per_second': '30.484', 'grad_norm': '20.125', 'counters/examples': 17216, 'counters/updates': 538}
train stats after 17248 examples: {'rewards_train/chosen': '-0.01118', 'rewards_train/rejected': '-0.0027676', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0084128', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-155.39', 'loss/train': '0.6991', 'examples_per_second': '33.048', 'grad_norm': '21.25', 'counters/examples': 17248, 'counters/updates': 539}
train stats after 17280 examples: {'rewards_train/chosen': '0.0079675', 'rewards_train/rejected': '-0.053137', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.061104', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-115.93', 'loss/train': '0.66526', 'examples_per_second': '31.627', 'grad_norm': '19', 'counters/examples': 17280, 'counters/updates': 540}
train stats after 17312 examples: {'rewards_train/chosen': '-0.031097', 'rewards_train/rejected': '0.019083', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05018', 'logps_train/rejected': '-71.508', 'logps_train/chosen': '-145.57', 'loss/train': '0.72125', 'examples_per_second': '32.844', 'grad_norm': '20.125', 'counters/examples': 17312, 'counters/updates': 541}
train stats after 17344 examples: {'rewards_train/chosen': '0.013582', 'rewards_train/rejected': '0.020201', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0066188', 'logps_train/rejected': '-102.43', 'logps_train/chosen': '-124.3', 'loss/train': '0.69936', 'examples_per_second': '32.851', 'grad_norm': '19.125', 'counters/examples': 17344, 'counters/updates': 542}
skipping logging after 17376 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '-0.018956', 'rewards_train/rejected': '0.0030958', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022052', 'logps_train/rejected': '-116.17', 'logps_train/chosen': '-143', 'loss/train': '0.70624', 'examples_per_second': '29.978', 'grad_norm': '21.25', 'counters/examples': 17408, 'counters/updates': 544}
train stats after 17440 examples: {'rewards_train/chosen': '-0.0047585', 'rewards_train/rejected': '0.021409', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.026168', 'logps_train/rejected': '-106.83', 'logps_train/chosen': '-126.54', 'loss/train': '0.70724', 'examples_per_second': '32.062', 'grad_norm': '21', 'counters/examples': 17440, 'counters/updates': 545}
train stats after 17472 examples: {'rewards_train/chosen': '0.03404', 'rewards_train/rejected': '0.030704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0033362', 'logps_train/rejected': '-106.25', 'logps_train/chosen': '-119.87', 'loss/train': '0.69374', 'examples_per_second': '30.312', 'grad_norm': '19.25', 'counters/examples': 17472, 'counters/updates': 546}
skipping logging after 17504 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '0.019642', 'rewards_train/rejected': '0.039018', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019377', 'logps_train/rejected': '-104.83', 'logps_train/chosen': '-134.25', 'loss/train': '0.70664', 'examples_per_second': '32.687', 'grad_norm': '20.75', 'counters/examples': 17536, 'counters/updates': 548}
train stats after 17568 examples: {'rewards_train/chosen': '0.035114', 'rewards_train/rejected': '-0.012829', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047944', 'logps_train/rejected': '-147.01', 'logps_train/chosen': '-144.28', 'loss/train': '0.67263', 'examples_per_second': '30.556', 'grad_norm': '20.5', 'counters/examples': 17568, 'counters/updates': 549}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17632 examples: {'rewards_train/chosen': '0.035443', 'rewards_train/rejected': '-0.032347', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06779', 'logps_train/rejected': '-140.25', 'logps_train/chosen': '-107.15', 'loss/train': '0.66375', 'examples_per_second': '33.681', 'grad_norm': '20', 'counters/examples': 17632, 'counters/updates': 551}
train stats after 17664 examples: {'rewards_train/chosen': '0.012387', 'rewards_train/rejected': '0.0063951', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.005992', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-132.62', 'loss/train': '0.69193', 'examples_per_second': '30.528', 'grad_norm': '20', 'counters/examples': 17664, 'counters/updates': 552}
skipping logging after 17696 examples to avoid logging too frequently
train stats after 17728 examples: {'rewards_train/chosen': '0.032765', 'rewards_train/rejected': '-0.045197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077962', 'logps_train/rejected': '-131.5', 'logps_train/chosen': '-139.72', 'loss/train': '0.65809', 'examples_per_second': '32.226', 'grad_norm': '21.25', 'counters/examples': 17728, 'counters/updates': 554}
train stats after 17760 examples: {'rewards_train/chosen': '0.053137', 'rewards_train/rejected': '-0.015335', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.068471', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-104.47', 'loss/train': '0.66174', 'examples_per_second': '32.413', 'grad_norm': '18.5', 'counters/examples': 17760, 'counters/updates': 555}
train stats after 17792 examples: {'rewards_train/chosen': '0.033015', 'rewards_train/rejected': '0.010337', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022678', 'logps_train/rejected': '-90.569', 'logps_train/chosen': '-101.16', 'loss/train': '0.68366', 'examples_per_second': '32.629', 'grad_norm': '16.25', 'counters/examples': 17792, 'counters/updates': 556}
train stats after 17824 examples: {'rewards_train/chosen': '0.02155', 'rewards_train/rejected': '-0.011127', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032677', 'logps_train/rejected': '-90.775', 'logps_train/chosen': '-134.27', 'loss/train': '0.67898', 'examples_per_second': '31.397', 'grad_norm': '19.25', 'counters/examples': 17824, 'counters/updates': 557}
train stats after 17856 examples: {'rewards_train/chosen': '0.017243', 'rewards_train/rejected': '-0.019737', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03698', 'logps_train/rejected': '-134.29', 'logps_train/chosen': '-187.33', 'loss/train': '0.67716', 'examples_per_second': '31.548', 'grad_norm': '22.125', 'counters/examples': 17856, 'counters/updates': 558}
skipping logging after 17888 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '0.017307', 'rewards_train/rejected': '-0.0041136', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02142', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-112.9', 'loss/train': '0.6855', 'examples_per_second': '34.341', 'grad_norm': '18.5', 'counters/examples': 17920, 'counters/updates': 560}
skipping logging after 17952 examples to avoid logging too frequently
train stats after 17984 examples: {'rewards_train/chosen': '0.043062', 'rewards_train/rejected': '0.016496', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026566', 'logps_train/rejected': '-112.04', 'logps_train/chosen': '-134.47', 'loss/train': '0.68344', 'examples_per_second': '33.323', 'grad_norm': '19.75', 'counters/examples': 17984, 'counters/updates': 562}
train stats after 18016 examples: {'rewards_train/chosen': '0.020257', 'rewards_train/rejected': '0.0076442', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012613', 'logps_train/rejected': '-99.762', 'logps_train/chosen': '-134.68', 'loss/train': '0.68972', 'examples_per_second': '32.684', 'grad_norm': '19', 'counters/examples': 18016, 'counters/updates': 563}
train stats after 18048 examples: {'rewards_train/chosen': '0.0039698', 'rewards_train/rejected': '0.013503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0095329', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-141.91', 'loss/train': '0.70176', 'examples_per_second': '31.472', 'grad_norm': '21.125', 'counters/examples': 18048, 'counters/updates': 564}
train stats after 18080 examples: {'rewards_train/chosen': '0.0079296', 'rewards_train/rejected': '0.0087015', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0007719', 'logps_train/rejected': '-149.57', 'logps_train/chosen': '-135.36', 'loss/train': '0.69599', 'examples_per_second': '30.92', 'grad_norm': '22.5', 'counters/examples': 18080, 'counters/updates': 565}
train stats after 18112 examples: {'rewards_train/chosen': '-0.0013458', 'rewards_train/rejected': '-0.033168', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.031823', 'logps_train/rejected': '-100.26', 'logps_train/chosen': '-108.01', 'loss/train': '0.68065', 'examples_per_second': '32.756', 'grad_norm': '17.875', 'counters/examples': 18112, 'counters/updates': 566}
train stats after 18144 examples: {'rewards_train/chosen': '0.008702', 'rewards_train/rejected': '0.0088189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.00011684', 'logps_train/rejected': '-97.795', 'logps_train/chosen': '-108.83', 'loss/train': '0.69522', 'examples_per_second': '30.093', 'grad_norm': '18.25', 'counters/examples': 18144, 'counters/updates': 567}
skipping logging after 18176 examples to avoid logging too frequently
train stats after 18208 examples: {'rewards_train/chosen': '-0.0061288', 'rewards_train/rejected': '-0.0051996', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.00092917', 'logps_train/rejected': '-112.57', 'logps_train/chosen': '-128.84', 'loss/train': '0.69587', 'examples_per_second': '37.323', 'grad_norm': '19.125', 'counters/examples': 18208, 'counters/updates': 569}
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18272 examples: {'rewards_train/chosen': '-0.0045468', 'rewards_train/rejected': '0.009742', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.014289', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-160.48', 'loss/train': '0.70412', 'examples_per_second': '30.839', 'grad_norm': '22.375', 'counters/examples': 18272, 'counters/updates': 571}
train stats after 18304 examples: {'rewards_train/chosen': '0.056304', 'rewards_train/rejected': '-0.0022291', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058533', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-167.65', 'loss/train': '0.66716', 'examples_per_second': '16.478', 'grad_norm': '21.5', 'counters/examples': 18304, 'counters/updates': 572}
train stats after 18336 examples: {'rewards_train/chosen': '0.02381', 'rewards_train/rejected': '0.0014079', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022402', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-136.78', 'loss/train': '0.68426', 'examples_per_second': '23.162', 'grad_norm': '19.5', 'counters/examples': 18336, 'counters/updates': 573}
train stats after 18368 examples: {'rewards_train/chosen': '0.067815', 'rewards_train/rejected': '0.01461', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053204', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-129.78', 'loss/train': '0.66974', 'examples_per_second': '25.952', 'grad_norm': '19', 'counters/examples': 18368, 'counters/updates': 574}
skipping logging after 18400 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '0.023917', 'rewards_train/rejected': '0.0039514', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019966', 'logps_train/rejected': '-108.75', 'logps_train/chosen': '-139.71', 'loss/train': '0.68601', 'examples_per_second': '30.536', 'grad_norm': '20.25', 'counters/examples': 18432, 'counters/updates': 576}
skipping logging after 18464 examples to avoid logging too frequently
train stats after 18496 examples: {'rewards_train/chosen': '0.031228', 'rewards_train/rejected': '-0.0070307', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038259', 'logps_train/rejected': '-135.89', 'logps_train/chosen': '-119.6', 'loss/train': '0.67648', 'examples_per_second': '31.603', 'grad_norm': '20.375', 'counters/examples': 18496, 'counters/updates': 578}
train stats after 18528 examples: {'rewards_train/chosen': '-0.018823', 'rewards_train/rejected': '0.022726', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041549', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-200.09', 'loss/train': '0.71816', 'examples_per_second': '31.476', 'grad_norm': '24.625', 'counters/examples': 18528, 'counters/updates': 579}
train stats after 18560 examples: {'rewards_train/chosen': '-0.016815', 'rewards_train/rejected': '0.0069391', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023754', 'logps_train/rejected': '-110.96', 'logps_train/chosen': '-117.89', 'loss/train': '0.70799', 'examples_per_second': '30.737', 'grad_norm': '19.5', 'counters/examples': 18560, 'counters/updates': 580}
train stats after 18592 examples: {'rewards_train/chosen': '0.0089499', 'rewards_train/rejected': '0.023608', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014658', 'logps_train/rejected': '-75.644', 'logps_train/chosen': '-116.28', 'loss/train': '0.70491', 'examples_per_second': '32.548', 'grad_norm': '17.5', 'counters/examples': 18592, 'counters/updates': 581}
train stats after 18624 examples: {'rewards_train/chosen': '0.024721', 'rewards_train/rejected': '0.0096337', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015087', 'logps_train/rejected': '-111.58', 'logps_train/chosen': '-168.38', 'loss/train': '0.6885', 'examples_per_second': '31.528', 'grad_norm': '22.75', 'counters/examples': 18624, 'counters/updates': 582}
skipping logging after 18656 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '0.033217', 'rewards_train/rejected': '0.0078398', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025378', 'logps_train/rejected': '-126', 'logps_train/chosen': '-141.39', 'loss/train': '0.68449', 'examples_per_second': '31.713', 'grad_norm': '20.5', 'counters/examples': 18688, 'counters/updates': 584}
skipping logging after 18720 examples to avoid logging too frequently
train stats after 18752 examples: {'rewards_train/chosen': '0.028671', 'rewards_train/rejected': '-0.0065474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035219', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-149.43', 'loss/train': '0.67819', 'examples_per_second': '30.571', 'grad_norm': '20.875', 'counters/examples': 18752, 'counters/updates': 586}
train stats after 18784 examples: {'rewards_train/chosen': '0.0068328', 'rewards_train/rejected': '0.019557', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012724', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-138.99', 'loss/train': '0.70224', 'examples_per_second': '31.491', 'grad_norm': '21.375', 'counters/examples': 18784, 'counters/updates': 587}
train stats after 18816 examples: {'rewards_train/chosen': '0.039044', 'rewards_train/rejected': '0.037884', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0011609', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-146.24', 'loss/train': '0.6959', 'examples_per_second': '30.048', 'grad_norm': '20.875', 'counters/examples': 18816, 'counters/updates': 588}
skipping logging after 18848 examples to avoid logging too frequently
train stats after 18880 examples: {'rewards_train/chosen': '0.0019514', 'rewards_train/rejected': '0.0016402', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00031123', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-170.73', 'loss/train': '0.69561', 'examples_per_second': '32.829', 'grad_norm': '21.5', 'counters/examples': 18880, 'counters/updates': 590}
train stats after 18912 examples: {'rewards_train/chosen': '0.05543', 'rewards_train/rejected': '-0.024604', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080034', 'logps_train/rejected': '-93.434', 'logps_train/chosen': '-127.91', 'loss/train': '0.65699', 'examples_per_second': '30.114', 'grad_norm': '18.125', 'counters/examples': 18912, 'counters/updates': 591}
train stats after 18944 examples: {'rewards_train/chosen': '0.033974', 'rewards_train/rejected': '0.0041486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029825', 'logps_train/rejected': '-138.76', 'logps_train/chosen': '-133.62', 'loss/train': '0.68113', 'examples_per_second': '31.542', 'grad_norm': '20.75', 'counters/examples': 18944, 'counters/updates': 592}
train stats after 18976 examples: {'rewards_train/chosen': '0.07518', 'rewards_train/rejected': '0.010463', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064717', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-138.74', 'loss/train': '0.66526', 'examples_per_second': '32.155', 'grad_norm': '19.375', 'counters/examples': 18976, 'counters/updates': 593}
train stats after 19008 examples: {'rewards_train/chosen': '0.029022', 'rewards_train/rejected': '-0.051954', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080975', 'logps_train/rejected': '-132.41', 'logps_train/chosen': '-156.83', 'loss/train': '0.65685', 'examples_per_second': '31.541', 'grad_norm': '20', 'counters/examples': 19008, 'counters/updates': 594}
skipping logging after 19040 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '0.035856', 'rewards_train/rejected': '0.028068', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0077878', 'logps_train/rejected': '-133.53', 'logps_train/chosen': '-163.23', 'loss/train': '0.6951', 'examples_per_second': '23.59', 'grad_norm': '22.5', 'counters/examples': 19072, 'counters/updates': 596}
skipping logging after 19104 examples to avoid logging too frequently
train stats after 19136 examples: {'rewards_train/chosen': '0.035574', 'rewards_train/rejected': '-0.0117', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047274', 'logps_train/rejected': '-108.72', 'logps_train/chosen': '-127.9', 'loss/train': '0.67232', 'examples_per_second': '32.115', 'grad_norm': '19.25', 'counters/examples': 19136, 'counters/updates': 598}
train stats after 19168 examples: {'rewards_train/chosen': '-0.00061289', 'rewards_train/rejected': '0.0054134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0060263', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-147', 'loss/train': '0.69952', 'examples_per_second': '23.821', 'grad_norm': '22.375', 'counters/examples': 19168, 'counters/updates': 599}
train stats after 19200 examples: {'rewards_train/chosen': '-0.012894', 'rewards_train/rejected': '0.03354', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046434', 'logps_train/rejected': '-118.25', 'logps_train/chosen': '-164.48', 'loss/train': '0.721', 'examples_per_second': '30.037', 'grad_norm': '23.5', 'counters/examples': 19200, 'counters/updates': 600}
skipping logging after 19232 examples to avoid logging too frequently
train stats after 19264 examples: {'rewards_train/chosen': '0.027137', 'rewards_train/rejected': '0.0096609', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017476', 'logps_train/rejected': '-128.42', 'logps_train/chosen': '-141.44', 'loss/train': '0.68787', 'examples_per_second': '33.086', 'grad_norm': '20.75', 'counters/examples': 19264, 'counters/updates': 602}
train stats after 19296 examples: {'rewards_train/chosen': '0.00092238', 'rewards_train/rejected': '-0.0042254', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0051478', 'logps_train/rejected': '-131.89', 'logps_train/chosen': '-160.43', 'loss/train': '0.69461', 'examples_per_second': '31.593', 'grad_norm': '22.5', 'counters/examples': 19296, 'counters/updates': 603}
train stats after 19328 examples: {'rewards_train/chosen': '0.015066', 'rewards_train/rejected': '-0.00011427', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01518', 'logps_train/rejected': '-109.69', 'logps_train/chosen': '-143.08', 'loss/train': '0.68885', 'examples_per_second': '32.3', 'grad_norm': '19.5', 'counters/examples': 19328, 'counters/updates': 604}
train stats after 19360 examples: {'rewards_train/chosen': '0.040855', 'rewards_train/rejected': '0.017171', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023684', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-127.52', 'loss/train': '0.68313', 'examples_per_second': '30.158', 'grad_norm': '19', 'counters/examples': 19360, 'counters/updates': 605}
train stats after 19392 examples: {'rewards_train/chosen': '0.030187', 'rewards_train/rejected': '0.013556', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01663', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-133.12', 'loss/train': '0.6885', 'examples_per_second': '31.453', 'grad_norm': '20.75', 'counters/examples': 19392, 'counters/updates': 606}
train stats after 19424 examples: {'rewards_train/chosen': '0.028261', 'rewards_train/rejected': '-0.0028532', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031114', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-131.34', 'loss/train': '0.68009', 'examples_per_second': '30.783', 'grad_norm': '21', 'counters/examples': 19424, 'counters/updates': 607}
skipping logging after 19456 examples to avoid logging too frequently
train stats after 19488 examples: {'rewards_train/chosen': '0.015536', 'rewards_train/rejected': '0.021668', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0061317', 'logps_train/rejected': '-158.38', 'logps_train/chosen': '-149.72', 'loss/train': '0.70137', 'examples_per_second': '33.198', 'grad_norm': '22.75', 'counters/examples': 19488, 'counters/updates': 609}
train stats after 19520 examples: {'rewards_train/chosen': '0.06708', 'rewards_train/rejected': '0.073801', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0067209', 'logps_train/rejected': '-108.12', 'logps_train/chosen': '-129.16', 'loss/train': '0.69899', 'examples_per_second': '30.988', 'grad_norm': '19.375', 'counters/examples': 19520, 'counters/updates': 610}
train stats after 19552 examples: {'rewards_train/chosen': '0.052187', 'rewards_train/rejected': '0.024945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027242', 'logps_train/rejected': '-129.8', 'logps_train/chosen': '-153.41', 'loss/train': '0.68199', 'examples_per_second': '32.554', 'grad_norm': '21.625', 'counters/examples': 19552, 'counters/updates': 611}
train stats after 19584 examples: {'rewards_train/chosen': '0.0087118', 'rewards_train/rejected': '-0.016597', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025309', 'logps_train/rejected': '-156.34', 'logps_train/chosen': '-149.99', 'loss/train': '0.68316', 'examples_per_second': '31.655', 'grad_norm': '21.5', 'counters/examples': 19584, 'counters/updates': 612}
train stats after 19616 examples: {'rewards_train/chosen': '0.038574', 'rewards_train/rejected': '0.01635', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022224', 'logps_train/rejected': '-74.448', 'logps_train/chosen': '-107.51', 'loss/train': '0.68383', 'examples_per_second': '31.751', 'grad_norm': '16.5', 'counters/examples': 19616, 'counters/updates': 613}
train stats after 19648 examples: {'rewards_train/chosen': '-0.0099422', 'rewards_train/rejected': '0.023213', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.033155', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-144.75', 'loss/train': '0.71266', 'examples_per_second': '30.576', 'grad_norm': '21.125', 'counters/examples': 19648, 'counters/updates': 614}
train stats after 19680 examples: {'rewards_train/chosen': '0.058544', 'rewards_train/rejected': '0.015065', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04348', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-156.26', 'loss/train': '0.67346', 'examples_per_second': '31.299', 'grad_norm': '21.125', 'counters/examples': 19680, 'counters/updates': 615}
train stats after 19712 examples: {'rewards_train/chosen': '-0.0027084', 'rewards_train/rejected': '0.015102', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01781', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-112.65', 'loss/train': '0.70529', 'examples_per_second': '31.421', 'grad_norm': '21', 'counters/examples': 19712, 'counters/updates': 616}
train stats after 19744 examples: {'rewards_train/chosen': '-0.012051', 'rewards_train/rejected': '-0.014611', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0025597', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-122.15', 'loss/train': '0.69372', 'examples_per_second': '31.804', 'grad_norm': '19.625', 'counters/examples': 19744, 'counters/updates': 617}
train stats after 19776 examples: {'rewards_train/chosen': '0.011868', 'rewards_train/rejected': '0.0079709', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0038972', 'logps_train/rejected': '-119.97', 'logps_train/chosen': '-134.61', 'loss/train': '0.69345', 'examples_per_second': '30.811', 'grad_norm': '20.375', 'counters/examples': 19776, 'counters/updates': 618}
train stats after 19808 examples: {'rewards_train/chosen': '0.041028', 'rewards_train/rejected': '0.036677', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0043507', 'logps_train/rejected': '-126.17', 'logps_train/chosen': '-131.21', 'loss/train': '0.69354', 'examples_per_second': '31.598', 'grad_norm': '20.25', 'counters/examples': 19808, 'counters/updates': 619}
train stats after 19840 examples: {'rewards_train/chosen': '-0.0060047', 'rewards_train/rejected': '0.045818', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.051823', 'logps_train/rejected': '-131.15', 'logps_train/chosen': '-158.2', 'loss/train': '0.723', 'examples_per_second': '32.651', 'grad_norm': '22.625', 'counters/examples': 19840, 'counters/updates': 620}
skipping logging after 19872 examples to avoid logging too frequently
train stats after 19904 examples: {'rewards_train/chosen': '0.086366', 'rewards_train/rejected': '0.023642', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062725', 'logps_train/rejected': '-154.89', 'logps_train/chosen': '-156.65', 'loss/train': '0.66765', 'examples_per_second': '30.055', 'grad_norm': '21.125', 'counters/examples': 19904, 'counters/updates': 622}
train stats after 19936 examples: {'rewards_train/chosen': '0.001752', 'rewards_train/rejected': '0.015832', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01408', 'logps_train/rejected': '-92.428', 'logps_train/chosen': '-162.92', 'loss/train': '0.70301', 'examples_per_second': '32.022', 'grad_norm': '20.875', 'counters/examples': 19936, 'counters/updates': 623}
skipping logging after 19968 examples to avoid logging too frequently
train stats after 20000 examples: {'rewards_train/chosen': '0.056325', 'rewards_train/rejected': '0.0105', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045825', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-181.35', 'loss/train': '0.67355', 'examples_per_second': '30.596', 'grad_norm': '21.875', 'counters/examples': 20000, 'counters/updates': 625}
train stats after 20032 examples: {'rewards_train/chosen': '0.043498', 'rewards_train/rejected': '0.036902', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0065969', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-142.98', 'loss/train': '0.69479', 'examples_per_second': '30.269', 'grad_norm': '20.375', 'counters/examples': 20032, 'counters/updates': 626}
skipping logging after 20064 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '0.023708', 'rewards_train/rejected': '0.026963', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0032544', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-163.13', 'loss/train': '0.69852', 'examples_per_second': '34.645', 'grad_norm': '21.625', 'counters/examples': 20096, 'counters/updates': 628}
train stats after 20128 examples: {'rewards_train/chosen': '0.026952', 'rewards_train/rejected': '0.067281', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.040329', 'logps_train/rejected': '-150.14', 'logps_train/chosen': '-129.66', 'loss/train': '0.71793', 'examples_per_second': '31.519', 'grad_norm': '21.25', 'counters/examples': 20128, 'counters/updates': 629}
train stats after 20160 examples: {'rewards_train/chosen': '0.0068604', 'rewards_train/rejected': '-0.01021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017071', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-151.59', 'loss/train': '0.68803', 'examples_per_second': '31.663', 'grad_norm': '21.125', 'counters/examples': 20160, 'counters/updates': 630}
train stats after 20192 examples: {'rewards_train/chosen': '0.036928', 'rewards_train/rejected': '-0.0012704', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038198', 'logps_train/rejected': '-125.05', 'logps_train/chosen': '-124.82', 'loss/train': '0.67656', 'examples_per_second': '30.938', 'grad_norm': '19.375', 'counters/examples': 20192, 'counters/updates': 631}
train stats after 20224 examples: {'rewards_train/chosen': '-0.0099698', 'rewards_train/rejected': '0.0097343', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019704', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-194.3', 'loss/train': '0.70733', 'examples_per_second': '31.562', 'grad_norm': '24', 'counters/examples': 20224, 'counters/updates': 632}
train stats after 20256 examples: {'rewards_train/chosen': '0.021526', 'rewards_train/rejected': '0.026439', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0049135', 'logps_train/rejected': '-159.42', 'logps_train/chosen': '-184.51', 'loss/train': '0.69911', 'examples_per_second': '31.261', 'grad_norm': '24.25', 'counters/examples': 20256, 'counters/updates': 633}
train stats after 20288 examples: {'rewards_train/chosen': '0.016067', 'rewards_train/rejected': '0.012778', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0032883', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-130.34', 'loss/train': '0.69419', 'examples_per_second': '30.087', 'grad_norm': '19.5', 'counters/examples': 20288, 'counters/updates': 634}
train stats after 20320 examples: {'rewards_train/chosen': '0.047079', 'rewards_train/rejected': '-0.0071912', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05427', 'logps_train/rejected': '-89.306', 'logps_train/chosen': '-117.11', 'loss/train': '0.67022', 'examples_per_second': '30.969', 'grad_norm': '18.25', 'counters/examples': 20320, 'counters/updates': 635}
train stats after 20352 examples: {'rewards_train/chosen': '0.022383', 'rewards_train/rejected': '0.038557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016175', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-151.29', 'loss/train': '0.70584', 'examples_per_second': '31.361', 'grad_norm': '22.75', 'counters/examples': 20352, 'counters/updates': 636}
train stats after 20384 examples: {'rewards_train/chosen': '0.014462', 'rewards_train/rejected': '0.011095', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0033665', 'logps_train/rejected': '-156.77', 'logps_train/chosen': '-149.09', 'loss/train': '0.69489', 'examples_per_second': '30.924', 'grad_norm': '22.625', 'counters/examples': 20384, 'counters/updates': 637}
train stats after 20416 examples: {'rewards_train/chosen': '0.025672', 'rewards_train/rejected': '0.044447', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.018775', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-147.63', 'loss/train': '0.70551', 'examples_per_second': '31.053', 'grad_norm': '20.125', 'counters/examples': 20416, 'counters/updates': 638}
train stats after 20448 examples: {'rewards_train/chosen': '-0.0075929', 'rewards_train/rejected': '0.0044197', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012013', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-135.4', 'loss/train': '0.70373', 'examples_per_second': '31.493', 'grad_norm': '21.125', 'counters/examples': 20448, 'counters/updates': 639}
train stats after 20480 examples: {'rewards_train/chosen': '-0.013513', 'rewards_train/rejected': '-0.016789', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0032756', 'logps_train/rejected': '-178.29', 'logps_train/chosen': '-151.44', 'loss/train': '0.69538', 'examples_per_second': '31.522', 'grad_norm': '22.5', 'counters/examples': 20480, 'counters/updates': 640}
train stats after 20512 examples: {'rewards_train/chosen': '0.03586', 'rewards_train/rejected': '0.0043573', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.031502', 'logps_train/rejected': '-96.404', 'logps_train/chosen': '-141.13', 'loss/train': '0.68119', 'examples_per_second': '31.539', 'grad_norm': '18.625', 'counters/examples': 20512, 'counters/updates': 641}
train stats after 20544 examples: {'rewards_train/chosen': '-0.0026097', 'rewards_train/rejected': '-0.00018224', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0024274', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-136.83', 'loss/train': '0.69782', 'examples_per_second': '30.905', 'grad_norm': '20.375', 'counters/examples': 20544, 'counters/updates': 642}
train stats after 20576 examples: {'rewards_train/chosen': '0.018517', 'rewards_train/rejected': '0.052931', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034414', 'logps_train/rejected': '-146.8', 'logps_train/chosen': '-174.84', 'loss/train': '0.71274', 'examples_per_second': '31.359', 'grad_norm': '24.5', 'counters/examples': 20576, 'counters/updates': 643}
skipping logging after 20608 examples to avoid logging too frequently
train stats after 20640 examples: {'rewards_train/chosen': '0.031869', 'rewards_train/rejected': '0.028666', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0032026', 'logps_train/rejected': '-89.293', 'logps_train/chosen': '-145.34', 'loss/train': '0.6943', 'examples_per_second': '33.224', 'grad_norm': '20', 'counters/examples': 20640, 'counters/updates': 645}
train stats after 20672 examples: {'rewards_train/chosen': '0.011495', 'rewards_train/rejected': '0.044672', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033177', 'logps_train/rejected': '-95.64', 'logps_train/chosen': '-129.33', 'loss/train': '0.71265', 'examples_per_second': '30.314', 'grad_norm': '19.25', 'counters/examples': 20672, 'counters/updates': 646}
train stats after 20704 examples: {'rewards_train/chosen': '0.02253', 'rewards_train/rejected': '0.0027016', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019828', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-148.15', 'loss/train': '0.68478', 'examples_per_second': '31.544', 'grad_norm': '20.125', 'counters/examples': 20704, 'counters/updates': 647}
train stats after 20736 examples: {'rewards_train/chosen': '0.026336', 'rewards_train/rejected': '0.044008', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.017672', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-191.26', 'loss/train': '0.70523', 'examples_per_second': '30.216', 'grad_norm': '23.875', 'counters/examples': 20736, 'counters/updates': 648}
train stats after 20768 examples: {'rewards_train/chosen': '0.012112', 'rewards_train/rejected': '0.022625', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010513', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-120.93', 'loss/train': '0.70094', 'examples_per_second': '30.901', 'grad_norm': '19.5', 'counters/examples': 20768, 'counters/updates': 649}
train stats after 20800 examples: {'rewards_train/chosen': '0.021342', 'rewards_train/rejected': '0.0029341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018408', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-110.29', 'loss/train': '0.68567', 'examples_per_second': '31.034', 'grad_norm': '18.5', 'counters/examples': 20800, 'counters/updates': 650}
train stats after 20832 examples: {'rewards_train/chosen': '0.041984', 'rewards_train/rejected': '0.012604', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02938', 'logps_train/rejected': '-101.03', 'logps_train/chosen': '-124.23', 'loss/train': '0.68091', 'examples_per_second': '29.899', 'grad_norm': '18.625', 'counters/examples': 20832, 'counters/updates': 651}
train stats after 20864 examples: {'rewards_train/chosen': '0.025117', 'rewards_train/rejected': '0.033364', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0082472', 'logps_train/rejected': '-119.93', 'logps_train/chosen': '-153.68', 'loss/train': '0.69964', 'examples_per_second': '32.924', 'grad_norm': '20', 'counters/examples': 20864, 'counters/updates': 652}
train stats after 20896 examples: {'rewards_train/chosen': '0.014168', 'rewards_train/rejected': '0.016377', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0022093', 'logps_train/rejected': '-98.299', 'logps_train/chosen': '-157.96', 'loss/train': '0.69775', 'examples_per_second': '32.262', 'grad_norm': '21.625', 'counters/examples': 20896, 'counters/updates': 653}
train stats after 20928 examples: {'rewards_train/chosen': '-0.0017904', 'rewards_train/rejected': '-0.012171', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01038', 'logps_train/rejected': '-123.71', 'logps_train/chosen': '-131.65', 'loss/train': '0.69084', 'examples_per_second': '31.317', 'grad_norm': '21.25', 'counters/examples': 20928, 'counters/updates': 654}
train stats after 20960 examples: {'rewards_train/chosen': '0.034118', 'rewards_train/rejected': '0.054196', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020078', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-138.45', 'loss/train': '0.70553', 'examples_per_second': '31.175', 'grad_norm': '21.5', 'counters/examples': 20960, 'counters/updates': 655}
train stats after 20992 examples: {'rewards_train/chosen': '0.015755', 'rewards_train/rejected': '-0.003909', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019664', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-151.02', 'loss/train': '0.68652', 'examples_per_second': '31.549', 'grad_norm': '21.25', 'counters/examples': 20992, 'counters/updates': 656}
train stats after 21024 examples: {'rewards_train/chosen': '0.019943', 'rewards_train/rejected': '-0.017505', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037449', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-115.93', 'loss/train': '0.67697', 'examples_per_second': '31.078', 'grad_norm': '18.75', 'counters/examples': 21024, 'counters/updates': 657}
train stats after 21056 examples: {'rewards_train/chosen': '0.0021203', 'rewards_train/rejected': '0.0017492', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00037108', 'logps_train/rejected': '-130.73', 'logps_train/chosen': '-166.55', 'loss/train': '0.69589', 'examples_per_second': '31.477', 'grad_norm': '23.25', 'counters/examples': 21056, 'counters/updates': 658}
train stats after 21088 examples: {'rewards_train/chosen': '0.014325', 'rewards_train/rejected': '-0.017735', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032061', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-115.93', 'loss/train': '0.67927', 'examples_per_second': '31.311', 'grad_norm': '19', 'counters/examples': 21088, 'counters/updates': 659}
train stats after 21120 examples: {'rewards_train/chosen': '0.010171', 'rewards_train/rejected': '0.032902', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022731', 'logps_train/rejected': '-131.01', 'logps_train/chosen': '-139.56', 'loss/train': '0.70839', 'examples_per_second': '31.572', 'grad_norm': '22', 'counters/examples': 21120, 'counters/updates': 660}
train stats after 21152 examples: {'rewards_train/chosen': '0.0072335', 'rewards_train/rejected': '-0.0041605', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011394', 'logps_train/rejected': '-108.4', 'logps_train/chosen': '-131.13', 'loss/train': '0.69029', 'examples_per_second': '31.651', 'grad_norm': '20.375', 'counters/examples': 21152, 'counters/updates': 661}
train stats after 21184 examples: {'rewards_train/chosen': '-0.022848', 'rewards_train/rejected': '-0.022185', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00066351', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-129.94', 'loss/train': '0.69583', 'examples_per_second': '30.732', 'grad_norm': '19.75', 'counters/examples': 21184, 'counters/updates': 662}
skipping logging after 21216 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '0.065154', 'rewards_train/rejected': '0.020569', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044585', 'logps_train/rejected': '-106.68', 'logps_train/chosen': '-111.3', 'loss/train': '0.67347', 'examples_per_second': '32.023', 'grad_norm': '17.5', 'counters/examples': 21248, 'counters/updates': 664}
train stats after 21280 examples: {'rewards_train/chosen': '0.033629', 'rewards_train/rejected': '0.00071267', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032916', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-142.16', 'loss/train': '0.67886', 'examples_per_second': '30.735', 'grad_norm': '19.875', 'counters/examples': 21280, 'counters/updates': 665}
train stats after 21312 examples: {'rewards_train/chosen': '0.021741', 'rewards_train/rejected': '-0.0040055', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025746', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-136.41', 'loss/train': '0.68183', 'examples_per_second': '32.408', 'grad_norm': '21', 'counters/examples': 21312, 'counters/updates': 666}
train stats after 21344 examples: {'rewards_train/chosen': '0.05787', 'rewards_train/rejected': '0.0075693', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0503', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-135.77', 'loss/train': '0.67213', 'examples_per_second': '29.955', 'grad_norm': '20.375', 'counters/examples': 21344, 'counters/updates': 667}
train stats after 21376 examples: {'rewards_train/chosen': '0.077124', 'rewards_train/rejected': '0.076058', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0010659', 'logps_train/rejected': '-159.19', 'logps_train/chosen': '-129.71', 'loss/train': '0.6965', 'examples_per_second': '31.206', 'grad_norm': '22', 'counters/examples': 21376, 'counters/updates': 668}
train stats after 21408 examples: {'rewards_train/chosen': '0.028443', 'rewards_train/rejected': '0.011442', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017001', 'logps_train/rejected': '-91.717', 'logps_train/chosen': '-140.19', 'loss/train': '0.68694', 'examples_per_second': '30.731', 'grad_norm': '19.875', 'counters/examples': 21408, 'counters/updates': 669}
train stats after 21440 examples: {'rewards_train/chosen': '0.0664', 'rewards_train/rejected': '0.037278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029123', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-163.02', 'loss/train': '0.68283', 'examples_per_second': '31.048', 'grad_norm': '21.5', 'counters/examples': 21440, 'counters/updates': 670}
train stats after 21472 examples: {'rewards_train/chosen': '0.0030342', 'rewards_train/rejected': '0.0091617', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0061275', 'logps_train/rejected': '-112.36', 'logps_train/chosen': '-143.3', 'loss/train': '0.70007', 'examples_per_second': '31.084', 'grad_norm': '21.25', 'counters/examples': 21472, 'counters/updates': 671}
train stats after 21504 examples: {'rewards_train/chosen': '0.034995', 'rewards_train/rejected': '0.019908', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015087', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-162.06', 'loss/train': '0.68879', 'examples_per_second': '31.005', 'grad_norm': '22.5', 'counters/examples': 21504, 'counters/updates': 672}
train stats after 21536 examples: {'rewards_train/chosen': '0.043859', 'rewards_train/rejected': '0.062582', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.018723', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-156.52', 'loss/train': '0.70529', 'examples_per_second': '30.891', 'grad_norm': '21.125', 'counters/examples': 21536, 'counters/updates': 673}
train stats after 21568 examples: {'rewards_train/chosen': '0.033912', 'rewards_train/rejected': '0.0022653', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.031646', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-140.36', 'loss/train': '0.68036', 'examples_per_second': '29.626', 'grad_norm': '19.875', 'counters/examples': 21568, 'counters/updates': 674}
train stats after 21600 examples: {'rewards_train/chosen': '-0.0020172', 'rewards_train/rejected': '0.0010684', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0030856', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-151.27', 'loss/train': '0.69876', 'examples_per_second': '31.06', 'grad_norm': '22.375', 'counters/examples': 21600, 'counters/updates': 675}
train stats after 21632 examples: {'rewards_train/chosen': '0.054176', 'rewards_train/rejected': '0.023105', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031072', 'logps_train/rejected': '-92.748', 'logps_train/chosen': '-109.03', 'loss/train': '0.68034', 'examples_per_second': '30.976', 'grad_norm': '18.25', 'counters/examples': 21632, 'counters/updates': 676}
train stats after 21664 examples: {'rewards_train/chosen': '0.045814', 'rewards_train/rejected': '-0.0020585', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047872', 'logps_train/rejected': '-124.89', 'logps_train/chosen': '-179.74', 'loss/train': '0.67244', 'examples_per_second': '31.057', 'grad_norm': '21.75', 'counters/examples': 21664, 'counters/updates': 677}
train stats after 21696 examples: {'rewards_train/chosen': '0.013617', 'rewards_train/rejected': '0.018882', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0052658', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-136.99', 'loss/train': '0.69956', 'examples_per_second': '30.102', 'grad_norm': '20.375', 'counters/examples': 21696, 'counters/updates': 678}
train stats after 21728 examples: {'rewards_train/chosen': '0.063979', 'rewards_train/rejected': '0.058888', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0050913', 'logps_train/rejected': '-152.03', 'logps_train/chosen': '-157.87', 'loss/train': '0.69221', 'examples_per_second': '31.317', 'grad_norm': '22.625', 'counters/examples': 21728, 'counters/updates': 679}
skipping logging after 21760 examples to avoid logging too frequently
train stats after 21792 examples: {'rewards_train/chosen': '0.052682', 'rewards_train/rejected': '0.062172', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0094895', 'logps_train/rejected': '-114.96', 'logps_train/chosen': '-162.82', 'loss/train': '0.69955', 'examples_per_second': '29.976', 'grad_norm': '21.625', 'counters/examples': 21792, 'counters/updates': 681}
train stats after 21824 examples: {'rewards_train/chosen': '0.065661', 'rewards_train/rejected': '0.0063036', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059357', 'logps_train/rejected': '-114.43', 'logps_train/chosen': '-141.64', 'loss/train': '0.66688', 'examples_per_second': '30.362', 'grad_norm': '20', 'counters/examples': 21824, 'counters/updates': 682}
train stats after 21856 examples: {'rewards_train/chosen': '0.0095152', 'rewards_train/rejected': '0.008456', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0010592', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-120.78', 'loss/train': '0.69445', 'examples_per_second': '31.117', 'grad_norm': '20', 'counters/examples': 21856, 'counters/updates': 683}
train stats after 21888 examples: {'rewards_train/chosen': '0.024796', 'rewards_train/rejected': '0.017129', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0076668', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-129.23', 'loss/train': '0.69257', 'examples_per_second': '32.834', 'grad_norm': '20.625', 'counters/examples': 21888, 'counters/updates': 684}
train stats after 21920 examples: {'rewards_train/chosen': '0.0044783', 'rewards_train/rejected': '0.018894', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014416', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-146.96', 'loss/train': '0.70359', 'examples_per_second': '30.5', 'grad_norm': '21', 'counters/examples': 21920, 'counters/updates': 685}
train stats after 21952 examples: {'rewards_train/chosen': '0.025862', 'rewards_train/rejected': '0.012191', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013671', 'logps_train/rejected': '-122.89', 'logps_train/chosen': '-145.65', 'loss/train': '0.69126', 'examples_per_second': '30.428', 'grad_norm': '21.375', 'counters/examples': 21952, 'counters/updates': 686}
train stats after 21984 examples: {'rewards_train/chosen': '0.0035007', 'rewards_train/rejected': '-0.0015912', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0050919', 'logps_train/rejected': '-146.19', 'logps_train/chosen': '-156.72', 'loss/train': '0.69541', 'examples_per_second': '30.947', 'grad_norm': '22.5', 'counters/examples': 21984, 'counters/updates': 687}
train stats after 22016 examples: {'rewards_train/chosen': '0.026083', 'rewards_train/rejected': '0.015805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010279', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-104.63', 'loss/train': '0.68981', 'examples_per_second': '30.461', 'grad_norm': '20.125', 'counters/examples': 22016, 'counters/updates': 688}
train stats after 22048 examples: {'rewards_train/chosen': '0.049692', 'rewards_train/rejected': '0.008766', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040926', 'logps_train/rejected': '-95.055', 'logps_train/chosen': '-123.02', 'loss/train': '0.67519', 'examples_per_second': '30.17', 'grad_norm': '18.125', 'counters/examples': 22048, 'counters/updates': 689}
train stats after 22080 examples: {'rewards_train/chosen': '0.014433', 'rewards_train/rejected': '-0.0037458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018179', 'logps_train/rejected': '-169.37', 'logps_train/chosen': '-168.25', 'loss/train': '0.68732', 'examples_per_second': '30.988', 'grad_norm': '23.125', 'counters/examples': 22080, 'counters/updates': 690}
skipping logging after 22112 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '0.0040797', 'rewards_train/rejected': '0.0059384', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0018587', 'logps_train/rejected': '-92.788', 'logps_train/chosen': '-112.95', 'loss/train': '0.69608', 'examples_per_second': '29.878', 'grad_norm': '18.125', 'counters/examples': 22144, 'counters/updates': 692}
train stats after 22176 examples: {'rewards_train/chosen': '0.0047697', 'rewards_train/rejected': '0.0091627', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.004393', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-99.908', 'loss/train': '0.70026', 'examples_per_second': '31.001', 'grad_norm': '18.25', 'counters/examples': 22176, 'counters/updates': 693}
train stats after 22208 examples: {'rewards_train/chosen': '0.019058', 'rewards_train/rejected': '0.010779', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0082792', 'logps_train/rejected': '-109.99', 'logps_train/chosen': '-150.22', 'loss/train': '0.69317', 'examples_per_second': '31.367', 'grad_norm': '20.75', 'counters/examples': 22208, 'counters/updates': 694}
train stats after 22240 examples: {'rewards_train/chosen': '0.065271', 'rewards_train/rejected': '0.049901', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01537', 'logps_train/rejected': '-104.66', 'logps_train/chosen': '-147.37', 'loss/train': '0.68899', 'examples_per_second': '31.345', 'grad_norm': '20.875', 'counters/examples': 22240, 'counters/updates': 695}
train stats after 22272 examples: {'rewards_train/chosen': '-0.017417', 'rewards_train/rejected': '-0.0060593', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011358', 'logps_train/rejected': '-104.63', 'logps_train/chosen': '-102.36', 'loss/train': '0.70214', 'examples_per_second': '29.6', 'grad_norm': '19.375', 'counters/examples': 22272, 'counters/updates': 696}
train stats after 22304 examples: {'rewards_train/chosen': '0.073504', 'rewards_train/rejected': '0.013088', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060415', 'logps_train/rejected': '-144.26', 'logps_train/chosen': '-180.88', 'loss/train': '0.66565', 'examples_per_second': '30.958', 'grad_norm': '22.5', 'counters/examples': 22304, 'counters/updates': 697}
train stats after 22336 examples: {'rewards_train/chosen': '0.050432', 'rewards_train/rejected': '0.03137', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019062', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-163.88', 'loss/train': '0.68582', 'examples_per_second': '30.889', 'grad_norm': '21.5', 'counters/examples': 22336, 'counters/updates': 698}
train stats after 22368 examples: {'rewards_train/chosen': '0.023022', 'rewards_train/rejected': '0.034804', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011782', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-165.21', 'loss/train': '0.70376', 'examples_per_second': '32.114', 'grad_norm': '21.375', 'counters/examples': 22368, 'counters/updates': 699}
train stats after 22400 examples: {'rewards_train/chosen': '0.0092323', 'rewards_train/rejected': '-0.02007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029302', 'logps_train/rejected': '-96.796', 'logps_train/chosen': '-134.36', 'loss/train': '0.6808', 'examples_per_second': '31.094', 'grad_norm': '19.25', 'counters/examples': 22400, 'counters/updates': 700}
train stats after 22432 examples: {'rewards_train/chosen': '0.040851', 'rewards_train/rejected': '0.04028', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00057176', 'logps_train/rejected': '-145.36', 'logps_train/chosen': '-147.2', 'loss/train': '0.69882', 'examples_per_second': '31.01', 'grad_norm': '22.625', 'counters/examples': 22432, 'counters/updates': 701}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22496 examples: {'rewards_train/chosen': '0.017304', 'rewards_train/rejected': '0.0056844', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01162', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-173.66', 'loss/train': '0.69113', 'examples_per_second': '31.062', 'grad_norm': '21.75', 'counters/examples': 22496, 'counters/updates': 703}
train stats after 22528 examples: {'rewards_train/chosen': '0.028402', 'rewards_train/rejected': '0.045011', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01661', 'logps_train/rejected': '-112.8', 'logps_train/chosen': '-130.21', 'loss/train': '0.70361', 'examples_per_second': '30.964', 'grad_norm': '19.625', 'counters/examples': 22528, 'counters/updates': 704}
skipping logging after 22560 examples to avoid logging too frequently
train stats after 22592 examples: {'rewards_train/chosen': '0.016675', 'rewards_train/rejected': '0.022077', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0054021', 'logps_train/rejected': '-144.63', 'logps_train/chosen': '-141.1', 'loss/train': '0.69843', 'examples_per_second': '29.947', 'grad_norm': '21.5', 'counters/examples': 22592, 'counters/updates': 706}
train stats after 22624 examples: {'rewards_train/chosen': '0.051458', 'rewards_train/rejected': '0.059324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0078664', 'logps_train/rejected': '-128.78', 'logps_train/chosen': '-160.99', 'loss/train': '0.69935', 'examples_per_second': '30.978', 'grad_norm': '22.375', 'counters/examples': 22624, 'counters/updates': 707}
train stats after 22656 examples: {'rewards_train/chosen': '-0.030842', 'rewards_train/rejected': '-0.016355', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014487', 'logps_train/rejected': '-104.61', 'logps_train/chosen': '-128.53', 'loss/train': '0.70311', 'examples_per_second': '31.018', 'grad_norm': '19.25', 'counters/examples': 22656, 'counters/updates': 708}
train stats after 22688 examples: {'rewards_train/chosen': '0.096373', 'rewards_train/rejected': '0.00022009', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096153', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-148.96', 'loss/train': '0.64976', 'examples_per_second': '31.365', 'grad_norm': '21', 'counters/examples': 22688, 'counters/updates': 709}
train stats after 22720 examples: {'rewards_train/chosen': '0.050868', 'rewards_train/rejected': '0.016739', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034128', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-118.55', 'loss/train': '0.67821', 'examples_per_second': '31.044', 'grad_norm': '20', 'counters/examples': 22720, 'counters/updates': 710}
skipping logging after 22752 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '0.031333', 'rewards_train/rejected': '0.025214', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0061192', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-150.96', 'loss/train': '0.69365', 'examples_per_second': '31.735', 'grad_norm': '21.625', 'counters/examples': 22784, 'counters/updates': 712}
train stats after 22816 examples: {'rewards_train/chosen': '0.06492', 'rewards_train/rejected': '0.02367', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.04125', 'logps_train/rejected': '-121.6', 'logps_train/chosen': '-94.95', 'loss/train': '0.67435', 'examples_per_second': '30.231', 'grad_norm': '18.25', 'counters/examples': 22816, 'counters/updates': 713}
train stats after 22848 examples: {'rewards_train/chosen': '0.028816', 'rewards_train/rejected': '0.028275', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00054078', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-126.4', 'loss/train': '0.69618', 'examples_per_second': '29.575', 'grad_norm': '20.125', 'counters/examples': 22848, 'counters/updates': 714}
skipping logging after 22880 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '0.040003', 'rewards_train/rejected': '0.043289', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0032853', 'logps_train/rejected': '-113.5', 'logps_train/chosen': '-149.64', 'loss/train': '0.69749', 'examples_per_second': '33.164', 'grad_norm': '21.375', 'counters/examples': 22912, 'counters/updates': 716}
skipping logging after 22944 examples to avoid logging too frequently
train stats after 22976 examples: {'rewards_train/chosen': '0.0104', 'rewards_train/rejected': '-0.0040329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014433', 'logps_train/rejected': '-110.22', 'logps_train/chosen': '-124.01', 'loss/train': '0.68733', 'examples_per_second': '32.811', 'grad_norm': '19.25', 'counters/examples': 22976, 'counters/updates': 718}
train stats after 23008 examples: {'rewards_train/chosen': '0.01389', 'rewards_train/rejected': '0.023168', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0092774', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-111.61', 'loss/train': '0.70007', 'examples_per_second': '31.034', 'grad_norm': '19.375', 'counters/examples': 23008, 'counters/updates': 719}
train stats after 23040 examples: {'rewards_train/chosen': '0.051143', 'rewards_train/rejected': '0.01496', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.036183', 'logps_train/rejected': '-105.23', 'logps_train/chosen': '-129.67', 'loss/train': '0.67721', 'examples_per_second': '30.118', 'grad_norm': '18.875', 'counters/examples': 23040, 'counters/updates': 720}
skipping logging after 23072 examples to avoid logging too frequently
train stats after 23104 examples: {'rewards_train/chosen': '0.031116', 'rewards_train/rejected': '-0.031642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062758', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-137.29', 'loss/train': '0.66509', 'examples_per_second': '33.005', 'grad_norm': '20.375', 'counters/examples': 23104, 'counters/updates': 722}
skipping logging after 23136 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '0.039887', 'rewards_train/rejected': '-0.0003727', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040259', 'logps_train/rejected': '-140.4', 'logps_train/chosen': '-177.62', 'loss/train': '0.67532', 'examples_per_second': '31.116', 'grad_norm': '21.875', 'counters/examples': 23168, 'counters/updates': 724}
train stats after 23200 examples: {'rewards_train/chosen': '0.0085235', 'rewards_train/rejected': '0.021934', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013411', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-154.26', 'loss/train': '0.70291', 'examples_per_second': '31.041', 'grad_norm': '21.625', 'counters/examples': 23200, 'counters/updates': 725}
train stats after 23232 examples: {'rewards_train/chosen': '-0.01216', 'rewards_train/rejected': '0.011118', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023278', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-119.06', 'loss/train': '0.70816', 'examples_per_second': '31.054', 'grad_norm': '20.375', 'counters/examples': 23232, 'counters/updates': 726}
train stats after 23264 examples: {'rewards_train/chosen': '0.036884', 'rewards_train/rejected': '-0.021672', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058556', 'logps_train/rejected': '-151.13', 'logps_train/chosen': '-141.86', 'loss/train': '0.66818', 'examples_per_second': '31.005', 'grad_norm': '21.125', 'counters/examples': 23264, 'counters/updates': 727}
train stats after 23296 examples: {'rewards_train/chosen': '-0.0014324', 'rewards_train/rejected': '-0.00688', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0054476', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-163.37', 'loss/train': '0.69257', 'examples_per_second': '30.37', 'grad_norm': '21.375', 'counters/examples': 23296, 'counters/updates': 728}
train stats after 23328 examples: {'rewards_train/chosen': '0.027353', 'rewards_train/rejected': '0.028503', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0011499', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-164.73', 'loss/train': '0.69702', 'examples_per_second': '30.973', 'grad_norm': '22', 'counters/examples': 23328, 'counters/updates': 729}
train stats after 23360 examples: {'rewards_train/chosen': '0.031153', 'rewards_train/rejected': '0.042687', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011534', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-132.28', 'loss/train': '0.70198', 'examples_per_second': '31.338', 'grad_norm': '21.125', 'counters/examples': 23360, 'counters/updates': 730}
train stats after 23392 examples: {'rewards_train/chosen': '0.045246', 'rewards_train/rejected': '-0.018286', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063532', 'logps_train/rejected': '-122.77', 'logps_train/chosen': '-127.44', 'loss/train': '0.66364', 'examples_per_second': '31.027', 'grad_norm': '20.5', 'counters/examples': 23392, 'counters/updates': 731}
train stats after 23424 examples: {'rewards_train/chosen': '0.036814', 'rewards_train/rejected': '0.042768', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0059538', 'logps_train/rejected': '-105.57', 'logps_train/chosen': '-128.43', 'loss/train': '0.69788', 'examples_per_second': '31.767', 'grad_norm': '19', 'counters/examples': 23424, 'counters/updates': 732}
train stats after 23456 examples: {'rewards_train/chosen': '0.059539', 'rewards_train/rejected': '0.023825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035714', 'logps_train/rejected': '-148.46', 'logps_train/chosen': '-146.13', 'loss/train': '0.67915', 'examples_per_second': '30.496', 'grad_norm': '21.125', 'counters/examples': 23456, 'counters/updates': 733}
train stats after 23488 examples: {'rewards_train/chosen': '0.049075', 'rewards_train/rejected': '0.0092129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039863', 'logps_train/rejected': '-157.65', 'logps_train/chosen': '-137.66', 'loss/train': '0.67671', 'examples_per_second': '31.494', 'grad_norm': '22.5', 'counters/examples': 23488, 'counters/updates': 734}
train stats after 23520 examples: {'rewards_train/chosen': '-0.00030311', 'rewards_train/rejected': '0.05085', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.051153', 'logps_train/rejected': '-160.67', 'logps_train/chosen': '-124.66', 'loss/train': '0.72267', 'examples_per_second': '31.548', 'grad_norm': '22.5', 'counters/examples': 23520, 'counters/updates': 735}
skipping logging after 23552 examples to avoid logging too frequently
train stats after 23584 examples: {'rewards_train/chosen': '0.059113', 'rewards_train/rejected': '0.032632', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026481', 'logps_train/rejected': '-114.14', 'logps_train/chosen': '-167.13', 'loss/train': '0.68308', 'examples_per_second': '30.053', 'grad_norm': '21.75', 'counters/examples': 23584, 'counters/updates': 737}
train stats after 23616 examples: {'rewards_train/chosen': '0.039827', 'rewards_train/rejected': '0.026385', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013442', 'logps_train/rejected': '-161.59', 'logps_train/chosen': '-152.94', 'loss/train': '0.68914', 'examples_per_second': '29.893', 'grad_norm': '23.5', 'counters/examples': 23616, 'counters/updates': 738}
train stats after 23648 examples: {'rewards_train/chosen': '-0.00766', 'rewards_train/rejected': '0.029304', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036964', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-127.07', 'loss/train': '0.71542', 'examples_per_second': '32.305', 'grad_norm': '19.875', 'counters/examples': 23648, 'counters/updates': 739}
train stats after 23680 examples: {'rewards_train/chosen': '0.033415', 'rewards_train/rejected': '0.039667', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0062526', 'logps_train/rejected': '-93.353', 'logps_train/chosen': '-129.48', 'loss/train': '0.69855', 'examples_per_second': '30.041', 'grad_norm': '19.5', 'counters/examples': 23680, 'counters/updates': 740}
train stats after 23712 examples: {'rewards_train/chosen': '0.050235', 'rewards_train/rejected': '0.01726', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032974', 'logps_train/rejected': '-148.73', 'logps_train/chosen': '-146.89', 'loss/train': '0.67884', 'examples_per_second': '31.583', 'grad_norm': '20.625', 'counters/examples': 23712, 'counters/updates': 741}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23776 examples: {'rewards_train/chosen': '0.044332', 'rewards_train/rejected': '0.025869', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018463', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-126.82', 'loss/train': '0.68604', 'examples_per_second': '31.141', 'grad_norm': '21.375', 'counters/examples': 23776, 'counters/updates': 743}
train stats after 23808 examples: {'rewards_train/chosen': '0.031046', 'rewards_train/rejected': '0.035601', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.004555', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-146.84', 'loss/train': '0.69931', 'examples_per_second': '32.376', 'grad_norm': '21.25', 'counters/examples': 23808, 'counters/updates': 744}
skipping logging after 23840 examples to avoid logging too frequently
train stats after 23872 examples: {'rewards_train/chosen': '0.016448', 'rewards_train/rejected': '-0.015112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03156', 'logps_train/rejected': '-93.939', 'logps_train/chosen': '-119.45', 'loss/train': '0.67949', 'examples_per_second': '24.119', 'grad_norm': '18.25', 'counters/examples': 23872, 'counters/updates': 746}
train stats after 23904 examples: {'rewards_train/chosen': '0.056673', 'rewards_train/rejected': '0.037719', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018954', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-143.1', 'loss/train': '0.68536', 'examples_per_second': '31.393', 'grad_norm': '20.375', 'counters/examples': 23904, 'counters/updates': 747}
skipping logging after 23936 examples to avoid logging too frequently
train stats after 23968 examples: {'rewards_train/chosen': '0.01907', 'rewards_train/rejected': '0.024151', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.005081', 'logps_train/rejected': '-107.45', 'logps_train/chosen': '-121.15', 'loss/train': '0.69775', 'examples_per_second': '34.294', 'grad_norm': '19.875', 'counters/examples': 23968, 'counters/updates': 749}
train stats after 24000 examples: {'rewards_train/chosen': '0.04188', 'rewards_train/rejected': '0.006882', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034998', 'logps_train/rejected': '-109.19', 'logps_train/chosen': '-143.54', 'loss/train': '0.67741', 'examples_per_second': '31.519', 'grad_norm': '19.625', 'counters/examples': 24000, 'counters/updates': 750}
Running evaluation after 24000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.18it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.81it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.90it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.90it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.81it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 24000: {'rewards_eval/chosen': '0.024572', 'rewards_eval/rejected': '0.017513', 'rewards_eval/accuracies': '0.53125', 'rewards_eval/margins': '0.0070583', 'logps_eval/rejected': '-115.22', 'logps_eval/chosen': '-135.3', 'loss/eval': '0.69301'}
skipping save for non epoch
train stats after 24032 examples: {'rewards_train/chosen': '0.054391', 'rewards_train/rejected': '-0.010599', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06499', 'logps_train/rejected': '-113.96', 'logps_train/chosen': '-123.43', 'loss/train': '0.66375', 'examples_per_second': '32.145', 'grad_norm': '19.375', 'counters/examples': 24032, 'counters/updates': 751}
train stats after 24064 examples: {'rewards_train/chosen': '0.033353', 'rewards_train/rejected': '0.017959', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015394', 'logps_train/rejected': '-97.418', 'logps_train/chosen': '-125.18', 'loss/train': '0.68748', 'examples_per_second': '30.126', 'grad_norm': '19.375', 'counters/examples': 24064, 'counters/updates': 752}
train stats after 24096 examples: {'rewards_train/chosen': '0.068485', 'rewards_train/rejected': '0.029338', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039147', 'logps_train/rejected': '-103.52', 'logps_train/chosen': '-156.68', 'loss/train': '0.67713', 'examples_per_second': '31.763', 'grad_norm': '20.625', 'counters/examples': 24096, 'counters/updates': 753}
train stats after 24128 examples: {'rewards_train/chosen': '0.0174', 'rewards_train/rejected': '0.04051', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02311', 'logps_train/rejected': '-110.84', 'logps_train/chosen': '-125.59', 'loss/train': '0.70771', 'examples_per_second': '31.509', 'grad_norm': '20.625', 'counters/examples': 24128, 'counters/updates': 754}
train stats after 24160 examples: {'rewards_train/chosen': '0.05428', 'rewards_train/rejected': '0.0051007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04918', 'logps_train/rejected': '-103.06', 'logps_train/chosen': '-127.27', 'loss/train': '0.67155', 'examples_per_second': '32.328', 'grad_norm': '18', 'counters/examples': 24160, 'counters/updates': 755}
train stats after 24192 examples: {'rewards_train/chosen': '0.043804', 'rewards_train/rejected': '0.024467', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019337', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-127.47', 'loss/train': '0.68638', 'examples_per_second': '30.751', 'grad_norm': '19.5', 'counters/examples': 24192, 'counters/updates': 756}
train stats after 24224 examples: {'rewards_train/chosen': '0.031133', 'rewards_train/rejected': '0.0040797', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027053', 'logps_train/rejected': '-99.837', 'logps_train/chosen': '-175.96', 'loss/train': '0.68246', 'examples_per_second': '30.768', 'grad_norm': '21.25', 'counters/examples': 24224, 'counters/updates': 757}
train stats after 24256 examples: {'rewards_train/chosen': '0.031257', 'rewards_train/rejected': '0.088461', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057204', 'logps_train/rejected': '-125.64', 'logps_train/chosen': '-159.49', 'loss/train': '0.72682', 'examples_per_second': '31.409', 'grad_norm': '23.125', 'counters/examples': 24256, 'counters/updates': 758}
train stats after 24288 examples: {'rewards_train/chosen': '0.017723', 'rewards_train/rejected': '0.027752', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.010028', 'logps_train/rejected': '-106.45', 'logps_train/chosen': '-138.13', 'loss/train': '0.69996', 'examples_per_second': '32.674', 'grad_norm': '19.875', 'counters/examples': 24288, 'counters/updates': 759}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24352 examples: {'rewards_train/chosen': '0.035044', 'rewards_train/rejected': '0.023434', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01161', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-136.14', 'loss/train': '0.69006', 'examples_per_second': '30.975', 'grad_norm': '21.25', 'counters/examples': 24352, 'counters/updates': 761}
train stats after 24384 examples: {'rewards_train/chosen': '0.035779', 'rewards_train/rejected': '0.011814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023965', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-146.66', 'loss/train': '0.68363', 'examples_per_second': '30.956', 'grad_norm': '21.25', 'counters/examples': 24384, 'counters/updates': 762}
train stats after 24416 examples: {'rewards_train/chosen': '0.02181', 'rewards_train/rejected': '-0.013424', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035234', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-125.32', 'loss/train': '0.67739', 'examples_per_second': '30.06', 'grad_norm': '19.75', 'counters/examples': 24416, 'counters/updates': 763}
train stats after 24448 examples: {'rewards_train/chosen': '0.060338', 'rewards_train/rejected': '-0.023575', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.083913', 'logps_train/rejected': '-112.45', 'logps_train/chosen': '-137.73', 'loss/train': '0.6549', 'examples_per_second': '31.39', 'grad_norm': '19.375', 'counters/examples': 24448, 'counters/updates': 764}
train stats after 24480 examples: {'rewards_train/chosen': '0.040407', 'rewards_train/rejected': '0.0087229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031684', 'logps_train/rejected': '-103.86', 'logps_train/chosen': '-142.79', 'loss/train': '0.67951', 'examples_per_second': '30.039', 'grad_norm': '19', 'counters/examples': 24480, 'counters/updates': 765}
train stats after 24512 examples: {'rewards_train/chosen': '0.020555', 'rewards_train/rejected': '-0.032005', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05256', 'logps_train/rejected': '-105.4', 'logps_train/chosen': '-132.04', 'loss/train': '0.67133', 'examples_per_second': '31.845', 'grad_norm': '19.125', 'counters/examples': 24512, 'counters/updates': 766}
train stats after 24544 examples: {'rewards_train/chosen': '0.018835', 'rewards_train/rejected': '0.0028603', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015974', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-154.76', 'loss/train': '0.68869', 'examples_per_second': '24.203', 'grad_norm': '21.5', 'counters/examples': 24544, 'counters/updates': 767}
train stats after 24576 examples: {'rewards_train/chosen': '0.023895', 'rewards_train/rejected': '0.008889', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015006', 'logps_train/rejected': '-115.74', 'logps_train/chosen': '-132.45', 'loss/train': '0.69084', 'examples_per_second': '31.504', 'grad_norm': '20', 'counters/examples': 24576, 'counters/updates': 768}
train stats after 24608 examples: {'rewards_train/chosen': '0.012558', 'rewards_train/rejected': '-0.0038057', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016364', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-129.86', 'loss/train': '0.68918', 'examples_per_second': '31.996', 'grad_norm': '21.375', 'counters/examples': 24608, 'counters/updates': 769}
train stats after 24640 examples: {'rewards_train/chosen': '0.03752', 'rewards_train/rejected': '0.014861', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022659', 'logps_train/rejected': '-77.516', 'logps_train/chosen': '-156.07', 'loss/train': '0.68446', 'examples_per_second': '25.007', 'grad_norm': '20.125', 'counters/examples': 24640, 'counters/updates': 770}
skipping logging after 24672 examples to avoid logging too frequently
train stats after 24704 examples: {'rewards_train/chosen': '0.036043', 'rewards_train/rejected': '0.0038813', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032161', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-124.12', 'loss/train': '0.6788', 'examples_per_second': '32.121', 'grad_norm': '18.875', 'counters/examples': 24704, 'counters/updates': 772}
train stats after 24736 examples: {'rewards_train/chosen': '-0.0084817', 'rewards_train/rejected': '0.017175', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025657', 'logps_train/rejected': '-151.2', 'logps_train/chosen': '-132.69', 'loss/train': '0.71012', 'examples_per_second': '31.685', 'grad_norm': '22.625', 'counters/examples': 24736, 'counters/updates': 773}
train stats after 24768 examples: {'rewards_train/chosen': '0.073289', 'rewards_train/rejected': '0.027901', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045388', 'logps_train/rejected': '-119.82', 'logps_train/chosen': '-134.79', 'loss/train': '0.6735', 'examples_per_second': '32.84', 'grad_norm': '20.5', 'counters/examples': 24768, 'counters/updates': 774}
train stats after 24800 examples: {'rewards_train/chosen': '0.027257', 'rewards_train/rejected': '-0.029507', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056764', 'logps_train/rejected': '-108.1', 'logps_train/chosen': '-145.1', 'loss/train': '0.66971', 'examples_per_second': '31.254', 'grad_norm': '20.5', 'counters/examples': 24800, 'counters/updates': 775}
train stats after 24832 examples: {'rewards_train/chosen': '0.035505', 'rewards_train/rejected': '-0.02645', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061955', 'logps_train/rejected': '-93.183', 'logps_train/chosen': '-143.54', 'loss/train': '0.66494', 'examples_per_second': '32.101', 'grad_norm': '19.375', 'counters/examples': 24832, 'counters/updates': 776}
train stats after 24864 examples: {'rewards_train/chosen': '0.040264', 'rewards_train/rejected': '0.011106', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029158', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-154.51', 'loss/train': '0.68162', 'examples_per_second': '30.743', 'grad_norm': '21.25', 'counters/examples': 24864, 'counters/updates': 777}
train stats after 24896 examples: {'rewards_train/chosen': '0.014105', 'rewards_train/rejected': '0.0014341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012671', 'logps_train/rejected': '-178.01', 'logps_train/chosen': '-153.43', 'loss/train': '0.6894', 'examples_per_second': '31.165', 'grad_norm': '23.875', 'counters/examples': 24896, 'counters/updates': 778}
skipping logging after 24928 examples to avoid logging too frequently
train stats after 24960 examples: {'rewards_train/chosen': '0.0015251', 'rewards_train/rejected': '0.0015748', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-4.9679e-05', 'logps_train/rejected': '-94.518', 'logps_train/chosen': '-112.46', 'loss/train': '0.69548', 'examples_per_second': '30.57', 'grad_norm': '19.625', 'counters/examples': 24960, 'counters/updates': 780}
train stats after 24992 examples: {'rewards_train/chosen': '0.070016', 'rewards_train/rejected': '-0.0032853', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073301', 'logps_train/rejected': '-146.75', 'logps_train/chosen': '-157.27', 'loss/train': '0.66216', 'examples_per_second': '30.994', 'grad_norm': '21.25', 'counters/examples': 24992, 'counters/updates': 781}
train stats after 25024 examples: {'rewards_train/chosen': '0.054627', 'rewards_train/rejected': '0.061974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0073469', 'logps_train/rejected': '-107.84', 'logps_train/chosen': '-132.46', 'loss/train': '0.70008', 'examples_per_second': '32.361', 'grad_norm': '19.375', 'counters/examples': 25024, 'counters/updates': 782}
train stats after 25056 examples: {'rewards_train/chosen': '0.023968', 'rewards_train/rejected': '0.046639', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.022671', 'logps_train/rejected': '-150.52', 'logps_train/chosen': '-142.17', 'loss/train': '0.70637', 'examples_per_second': '31.518', 'grad_norm': '21.375', 'counters/examples': 25056, 'counters/updates': 783}
train stats after 25088 examples: {'rewards_train/chosen': '0.02279', 'rewards_train/rejected': '0.01672', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0060706', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-146.31', 'loss/train': '0.69181', 'examples_per_second': '31.577', 'grad_norm': '20.75', 'counters/examples': 25088, 'counters/updates': 784}
train stats after 25120 examples: {'rewards_train/chosen': '0.037255', 'rewards_train/rejected': '0.012089', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025166', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-131.6', 'loss/train': '0.68575', 'examples_per_second': '30.754', 'grad_norm': '20.875', 'counters/examples': 25120, 'counters/updates': 785}
train stats after 25152 examples: {'rewards_train/chosen': '0.032515', 'rewards_train/rejected': '0.01623', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016285', 'logps_train/rejected': '-111.53', 'logps_train/chosen': '-120.04', 'loss/train': '0.68676', 'examples_per_second': '31.491', 'grad_norm': '19', 'counters/examples': 25152, 'counters/updates': 786}
skipping logging after 25184 examples to avoid logging too frequently
train stats after 25216 examples: {'rewards_train/chosen': '0.067482', 'rewards_train/rejected': '0.042462', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02502', 'logps_train/rejected': '-102.7', 'logps_train/chosen': '-134.46', 'loss/train': '0.68359', 'examples_per_second': '32.89', 'grad_norm': '19.25', 'counters/examples': 25216, 'counters/updates': 788}
train stats after 25248 examples: {'rewards_train/chosen': '0.03388', 'rewards_train/rejected': '0.018167', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015713', 'logps_train/rejected': '-134.94', 'logps_train/chosen': '-119.01', 'loss/train': '0.68856', 'examples_per_second': '32.045', 'grad_norm': '20', 'counters/examples': 25248, 'counters/updates': 789}
train stats after 25280 examples: {'rewards_train/chosen': '0.032396', 'rewards_train/rejected': '-0.01479', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047186', 'logps_train/rejected': '-103.55', 'logps_train/chosen': '-120.94', 'loss/train': '0.67362', 'examples_per_second': '31.565', 'grad_norm': '18.5', 'counters/examples': 25280, 'counters/updates': 790}
skipping logging after 25312 examples to avoid logging too frequently
train stats after 25344 examples: {'rewards_train/chosen': '-0.0012003', 'rewards_train/rejected': '-0.020855', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019655', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-143.57', 'loss/train': '0.6849', 'examples_per_second': '30.624', 'grad_norm': '20.375', 'counters/examples': 25344, 'counters/updates': 792}
skipping logging after 25376 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '0.030561', 'rewards_train/rejected': '0.011513', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019048', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-141.61', 'loss/train': '0.6871', 'examples_per_second': '31.572', 'grad_norm': '20.875', 'counters/examples': 25408, 'counters/updates': 794}
train stats after 25440 examples: {'rewards_train/chosen': '0.065077', 'rewards_train/rejected': '0.014509', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050569', 'logps_train/rejected': '-101.67', 'logps_train/chosen': '-138.41', 'loss/train': '0.67034', 'examples_per_second': '31.575', 'grad_norm': '19.625', 'counters/examples': 25440, 'counters/updates': 795}
train stats after 25472 examples: {'rewards_train/chosen': '0.044681', 'rewards_train/rejected': '-0.0054146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050096', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-98.056', 'loss/train': '0.67042', 'examples_per_second': '31.638', 'grad_norm': '17.25', 'counters/examples': 25472, 'counters/updates': 796}
train stats after 25504 examples: {'rewards_train/chosen': '0.015902', 'rewards_train/rejected': '0.022515', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0066127', 'logps_train/rejected': '-109.33', 'logps_train/chosen': '-140.9', 'loss/train': '0.70123', 'examples_per_second': '31.582', 'grad_norm': '21.75', 'counters/examples': 25504, 'counters/updates': 797}
train stats after 25536 examples: {'rewards_train/chosen': '0.0039187', 'rewards_train/rejected': '0.016789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01287', 'logps_train/rejected': '-94.535', 'logps_train/chosen': '-120.89', 'loss/train': '0.70278', 'examples_per_second': '31.773', 'grad_norm': '18.875', 'counters/examples': 25536, 'counters/updates': 798}
train stats after 25568 examples: {'rewards_train/chosen': '0.0086793', 'rewards_train/rejected': '-0.0024063', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.011086', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-135.01', 'loss/train': '0.68971', 'examples_per_second': '30.99', 'grad_norm': '21.75', 'counters/examples': 25568, 'counters/updates': 799}
train stats after 25600 examples: {'rewards_train/chosen': '0.018614', 'rewards_train/rejected': '-0.012718', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031332', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-142.24', 'loss/train': '0.68098', 'examples_per_second': '31.686', 'grad_norm': '20.75', 'counters/examples': 25600, 'counters/updates': 800}
train stats after 25632 examples: {'rewards_train/chosen': '-0.0024252', 'rewards_train/rejected': '0.031245', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033671', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-126.61', 'loss/train': '0.71321', 'examples_per_second': '31.276', 'grad_norm': '20.625', 'counters/examples': 25632, 'counters/updates': 801}
train stats after 25664 examples: {'rewards_train/chosen': '0.065415', 'rewards_train/rejected': '0.014054', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051361', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-134.03', 'loss/train': '0.67218', 'examples_per_second': '30.607', 'grad_norm': '19.625', 'counters/examples': 25664, 'counters/updates': 802}
train stats after 25696 examples: {'rewards_train/chosen': '0.037411', 'rewards_train/rejected': '0.054686', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017275', 'logps_train/rejected': '-130.64', 'logps_train/chosen': '-187.21', 'loss/train': '0.70466', 'examples_per_second': '30.524', 'grad_norm': '22.875', 'counters/examples': 25696, 'counters/updates': 803}
train stats after 25728 examples: {'rewards_train/chosen': '0.081187', 'rewards_train/rejected': '0.034603', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046584', 'logps_train/rejected': '-128.73', 'logps_train/chosen': '-155.94', 'loss/train': '0.67287', 'examples_per_second': '32.372', 'grad_norm': '21', 'counters/examples': 25728, 'counters/updates': 804}
train stats after 25760 examples: {'rewards_train/chosen': '0.017912', 'rewards_train/rejected': '0.040634', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022722', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-132.84', 'loss/train': '0.70767', 'examples_per_second': '30.564', 'grad_norm': '20.875', 'counters/examples': 25760, 'counters/updates': 805}
skipping logging after 25792 examples to avoid logging too frequently
train stats after 25824 examples: {'rewards_train/chosen': '0.032226', 'rewards_train/rejected': '0.013781', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.018445', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-133.34', 'loss/train': '0.68635', 'examples_per_second': '30.182', 'grad_norm': '21.875', 'counters/examples': 25824, 'counters/updates': 807}
train stats after 25856 examples: {'rewards_train/chosen': '0.0076016', 'rewards_train/rejected': '0.037212', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02961', 'logps_train/rejected': '-99.103', 'logps_train/chosen': '-155.04', 'loss/train': '0.71215', 'examples_per_second': '31.687', 'grad_norm': '21.25', 'counters/examples': 25856, 'counters/updates': 808}
train stats after 25888 examples: {'rewards_train/chosen': '0.027732', 'rewards_train/rejected': '-0.027393', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055125', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-149.93', 'loss/train': '0.66868', 'examples_per_second': '32.008', 'grad_norm': '20.375', 'counters/examples': 25888, 'counters/updates': 809}
train stats after 25920 examples: {'rewards_train/chosen': '0.046742', 'rewards_train/rejected': '0.044538', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0022045', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-163.54', 'loss/train': '0.69527', 'examples_per_second': '31.674', 'grad_norm': '22.25', 'counters/examples': 25920, 'counters/updates': 810}
train stats after 25952 examples: {'rewards_train/chosen': '0.015178', 'rewards_train/rejected': '-0.022222', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0374', 'logps_train/rejected': '-103.69', 'logps_train/chosen': '-127.58', 'loss/train': '0.67634', 'examples_per_second': '31.638', 'grad_norm': '18.625', 'counters/examples': 25952, 'counters/updates': 811}
train stats after 25984 examples: {'rewards_train/chosen': '0.036928', 'rewards_train/rejected': '0.032991', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0039363', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-123.73', 'loss/train': '0.6929', 'examples_per_second': '32.375', 'grad_norm': '20.125', 'counters/examples': 25984, 'counters/updates': 812}
train stats after 26016 examples: {'rewards_train/chosen': '0.03282', 'rewards_train/rejected': '0.040079', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0072591', 'logps_train/rejected': '-117.58', 'logps_train/chosen': '-141.89', 'loss/train': '0.70024', 'examples_per_second': '32.399', 'grad_norm': '20.875', 'counters/examples': 26016, 'counters/updates': 813}
train stats after 26048 examples: {'rewards_train/chosen': '0.074792', 'rewards_train/rejected': '0.0060441', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068748', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-177.21', 'loss/train': '0.6635', 'examples_per_second': '30.719', 'grad_norm': '20.75', 'counters/examples': 26048, 'counters/updates': 814}
skipping logging after 26080 examples to avoid logging too frequently
train stats after 26112 examples: {'rewards_train/chosen': '0.08545', 'rewards_train/rejected': '0.028967', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056483', 'logps_train/rejected': '-132.4', 'logps_train/chosen': '-129.67', 'loss/train': '0.66824', 'examples_per_second': '36.462', 'grad_norm': '19.5', 'counters/examples': 26112, 'counters/updates': 816}
train stats after 26144 examples: {'rewards_train/chosen': '0.034954', 'rewards_train/rejected': '0.065628', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030674', 'logps_train/rejected': '-121.41', 'logps_train/chosen': '-136.88', 'loss/train': '0.7119', 'examples_per_second': '32.099', 'grad_norm': '21.625', 'counters/examples': 26144, 'counters/updates': 817}
train stats after 26176 examples: {'rewards_train/chosen': '0.016115', 'rewards_train/rejected': '-0.0061199', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022235', 'logps_train/rejected': '-98.51', 'logps_train/chosen': '-105.51', 'loss/train': '0.68372', 'examples_per_second': '30.345', 'grad_norm': '18', 'counters/examples': 26176, 'counters/updates': 818}
train stats after 26208 examples: {'rewards_train/chosen': '0.023492', 'rewards_train/rejected': '0.014635', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0088568', 'logps_train/rejected': '-136.68', 'logps_train/chosen': '-120.09', 'loss/train': '0.69123', 'examples_per_second': '30.886', 'grad_norm': '20.5', 'counters/examples': 26208, 'counters/updates': 819}
train stats after 26240 examples: {'rewards_train/chosen': '0.024655', 'rewards_train/rejected': '-0.010843', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035498', 'logps_train/rejected': '-87.439', 'logps_train/chosen': '-119.71', 'loss/train': '0.68029', 'examples_per_second': '30.838', 'grad_norm': '18.5', 'counters/examples': 26240, 'counters/updates': 820}
train stats after 26272 examples: {'rewards_train/chosen': '0.059227', 'rewards_train/rejected': '0.026752', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032475', 'logps_train/rejected': '-101.34', 'logps_train/chosen': '-142.11', 'loss/train': '0.67911', 'examples_per_second': '32.293', 'grad_norm': '19.5', 'counters/examples': 26272, 'counters/updates': 821}
train stats after 26304 examples: {'rewards_train/chosen': '0.061077', 'rewards_train/rejected': '0.056857', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0042199', 'logps_train/rejected': '-113.79', 'logps_train/chosen': '-152.78', 'loss/train': '0.69447', 'examples_per_second': '32.403', 'grad_norm': '20.5', 'counters/examples': 26304, 'counters/updates': 822}
train stats after 26336 examples: {'rewards_train/chosen': '0.014234', 'rewards_train/rejected': '0.023195', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0089607', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-158.36', 'loss/train': '0.70381', 'examples_per_second': '32.777', 'grad_norm': '21.25', 'counters/examples': 26336, 'counters/updates': 823}
train stats after 26368 examples: {'rewards_train/chosen': '-0.010529', 'rewards_train/rejected': '0.016249', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026778', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-135.47', 'loss/train': '0.71121', 'examples_per_second': '30.965', 'grad_norm': '22.375', 'counters/examples': 26368, 'counters/updates': 824}
train stats after 26400 examples: {'rewards_train/chosen': '0.064676', 'rewards_train/rejected': '-0.0071447', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071821', 'logps_train/rejected': '-170.39', 'logps_train/chosen': '-175.02', 'loss/train': '0.66363', 'examples_per_second': '30.33', 'grad_norm': '21.625', 'counters/examples': 26400, 'counters/updates': 825}
skipping logging after 26432 examples to avoid logging too frequently
train stats after 26464 examples: {'rewards_train/chosen': '0.018886', 'rewards_train/rejected': '-0.027762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046649', 'logps_train/rejected': '-122.42', 'logps_train/chosen': '-150.01', 'loss/train': '0.67149', 'examples_per_second': '30.32', 'grad_norm': '20.5', 'counters/examples': 26464, 'counters/updates': 827}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26528 examples: {'rewards_train/chosen': '0.017441', 'rewards_train/rejected': '0.0062068', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011234', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-121.99', 'loss/train': '0.69206', 'examples_per_second': '32.943', 'grad_norm': '19.75', 'counters/examples': 26528, 'counters/updates': 829}
train stats after 26560 examples: {'rewards_train/chosen': '0.024508', 'rewards_train/rejected': '0.00091448', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023594', 'logps_train/rejected': '-108.24', 'logps_train/chosen': '-112.09', 'loss/train': '0.6837', 'examples_per_second': '31.85', 'grad_norm': '17.875', 'counters/examples': 26560, 'counters/updates': 830}
skipping logging after 26592 examples to avoid logging too frequently
train stats after 26624 examples: {'rewards_train/chosen': '0.012905', 'rewards_train/rejected': '0.026191', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013286', 'logps_train/rejected': '-101.83', 'logps_train/chosen': '-128.38', 'loss/train': '0.703', 'examples_per_second': '31.926', 'grad_norm': '20.5', 'counters/examples': 26624, 'counters/updates': 832}
skipping logging after 26656 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '0.083285', 'rewards_train/rejected': '0.072595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01069', 'logps_train/rejected': '-156.91', 'logps_train/chosen': '-171.8', 'loss/train': '0.69032', 'examples_per_second': '31.678', 'grad_norm': '23.25', 'counters/examples': 26688, 'counters/updates': 834}
train stats after 26720 examples: {'rewards_train/chosen': '0.02032', 'rewards_train/rejected': '0.061949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.04163', 'logps_train/rejected': '-117.17', 'logps_train/chosen': '-126.21', 'loss/train': '0.71998', 'examples_per_second': '30.556', 'grad_norm': '20.125', 'counters/examples': 26720, 'counters/updates': 835}
train stats after 26752 examples: {'rewards_train/chosen': '0.053771', 'rewards_train/rejected': '0.00099426', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052777', 'logps_train/rejected': '-96.885', 'logps_train/chosen': '-118.36', 'loss/train': '0.66897', 'examples_per_second': '30.898', 'grad_norm': '17.75', 'counters/examples': 26752, 'counters/updates': 836}
skipping logging after 26784 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '0.033467', 'rewards_train/rejected': '-0.0016395', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035107', 'logps_train/rejected': '-108.35', 'logps_train/chosen': '-162.97', 'loss/train': '0.6799', 'examples_per_second': '31.639', 'grad_norm': '21.5', 'counters/examples': 26816, 'counters/updates': 838}
train stats after 26848 examples: {'rewards_train/chosen': '0.034425', 'rewards_train/rejected': '0.011403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023022', 'logps_train/rejected': '-103.32', 'logps_train/chosen': '-186.56', 'loss/train': '0.68735', 'examples_per_second': '31.643', 'grad_norm': '22.875', 'counters/examples': 26848, 'counters/updates': 839}
train stats after 26880 examples: {'rewards_train/chosen': '0.05671', 'rewards_train/rejected': '-0.0073891', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064099', 'logps_train/rejected': '-102.56', 'logps_train/chosen': '-157.77', 'loss/train': '0.66313', 'examples_per_second': '31.531', 'grad_norm': '21.25', 'counters/examples': 26880, 'counters/updates': 840}
train stats after 26912 examples: {'rewards_train/chosen': '-0.0022199', 'rewards_train/rejected': '-0.017589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015369', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-149.62', 'loss/train': '0.68886', 'examples_per_second': '32.544', 'grad_norm': '22', 'counters/examples': 26912, 'counters/updates': 841}
skipping logging after 26944 examples to avoid logging too frequently
train stats after 26976 examples: {'rewards_train/chosen': '0.032691', 'rewards_train/rejected': '0.010092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022599', 'logps_train/rejected': '-108.41', 'logps_train/chosen': '-118.42', 'loss/train': '0.68442', 'examples_per_second': '34.346', 'grad_norm': '18.625', 'counters/examples': 26976, 'counters/updates': 843}
skipping logging after 27008 examples to avoid logging too frequently
train stats after 27040 examples: {'rewards_train/chosen': '0.018489', 'rewards_train/rejected': '0.020978', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0024888', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-125.03', 'loss/train': '0.69759', 'examples_per_second': '31.181', 'grad_norm': '19.625', 'counters/examples': 27040, 'counters/updates': 845}
train stats after 27072 examples: {'rewards_train/chosen': '0.042845', 'rewards_train/rejected': '-0.011597', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054442', 'logps_train/rejected': '-110.68', 'logps_train/chosen': '-150.27', 'loss/train': '0.66929', 'examples_per_second': '31.275', 'grad_norm': '19.25', 'counters/examples': 27072, 'counters/updates': 846}
train stats after 27104 examples: {'rewards_train/chosen': '0.028795', 'rewards_train/rejected': '0.025113', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0036821', 'logps_train/rejected': '-138.99', 'logps_train/chosen': '-154.94', 'loss/train': '0.69572', 'examples_per_second': '31.641', 'grad_norm': '21.875', 'counters/examples': 27104, 'counters/updates': 847}
train stats after 27136 examples: {'rewards_train/chosen': '0.022411', 'rewards_train/rejected': '0.0023218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020089', 'logps_train/rejected': '-98.719', 'logps_train/chosen': '-117.15', 'loss/train': '0.68626', 'examples_per_second': '31.262', 'grad_norm': '18', 'counters/examples': 27136, 'counters/updates': 848}
skipping logging after 27168 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '-0.0015007', 'rewards_train/rejected': '-0.0048692', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0033685', 'logps_train/rejected': '-118.12', 'logps_train/chosen': '-109.6', 'loss/train': '0.69341', 'examples_per_second': '30.211', 'grad_norm': '18.5', 'counters/examples': 27200, 'counters/updates': 850}
train stats after 27232 examples: {'rewards_train/chosen': '0.016042', 'rewards_train/rejected': '-0.0073299', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.023372', 'logps_train/rejected': '-116.29', 'logps_train/chosen': '-155.44', 'loss/train': '0.68397', 'examples_per_second': '31.623', 'grad_norm': '21.5', 'counters/examples': 27232, 'counters/updates': 851}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27296 examples: {'rewards_train/chosen': '0.029889', 'rewards_train/rejected': '0.011484', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018405', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-101.7', 'loss/train': '0.68695', 'examples_per_second': '34.628', 'grad_norm': '19', 'counters/examples': 27296, 'counters/updates': 853}
train stats after 27328 examples: {'rewards_train/chosen': '0.061008', 'rewards_train/rejected': '-0.00055901', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061567', 'logps_train/rejected': '-169.14', 'logps_train/chosen': '-149.4', 'loss/train': '0.66671', 'examples_per_second': '31.657', 'grad_norm': '22.125', 'counters/examples': 27328, 'counters/updates': 854}
train stats after 27360 examples: {'rewards_train/chosen': '0.041346', 'rewards_train/rejected': '-0.013017', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.054362', 'logps_train/rejected': '-121.67', 'logps_train/chosen': '-128.86', 'loss/train': '0.6685', 'examples_per_second': '32.2', 'grad_norm': '20.125', 'counters/examples': 27360, 'counters/updates': 855}
train stats after 27392 examples: {'rewards_train/chosen': '0.045924', 'rewards_train/rejected': '0.026673', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019251', 'logps_train/rejected': '-104.61', 'logps_train/chosen': '-143', 'loss/train': '0.68675', 'examples_per_second': '31.698', 'grad_norm': '19.25', 'counters/examples': 27392, 'counters/updates': 856}
train stats after 27424 examples: {'rewards_train/chosen': '0.02124', 'rewards_train/rejected': '0.020088', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.001152', 'logps_train/rejected': '-153.93', 'logps_train/chosen': '-170.45', 'loss/train': '0.696', 'examples_per_second': '31', 'grad_norm': '22.75', 'counters/examples': 27424, 'counters/updates': 857}
train stats after 27456 examples: {'rewards_train/chosen': '-0.026306', 'rewards_train/rejected': '0.037085', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.063392', 'logps_train/rejected': '-138.65', 'logps_train/chosen': '-166.09', 'loss/train': '0.73169', 'examples_per_second': '31.642', 'grad_norm': '24.5', 'counters/examples': 27456, 'counters/updates': 858}
train stats after 27488 examples: {'rewards_train/chosen': '0.034761', 'rewards_train/rejected': '-0.00026563', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035026', 'logps_train/rejected': '-111.95', 'logps_train/chosen': '-117.95', 'loss/train': '0.67755', 'examples_per_second': '32.305', 'grad_norm': '19', 'counters/examples': 27488, 'counters/updates': 859}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27552 examples: {'rewards_train/chosen': '0.022388', 'rewards_train/rejected': '0.038181', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015793', 'logps_train/rejected': '-111.49', 'logps_train/chosen': '-146.46', 'loss/train': '0.70401', 'examples_per_second': '30.851', 'grad_norm': '21.75', 'counters/examples': 27552, 'counters/updates': 861}
train stats after 27584 examples: {'rewards_train/chosen': '0.034027', 'rewards_train/rejected': '0.051343', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017316', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-127.45', 'loss/train': '0.70441', 'examples_per_second': '30.354', 'grad_norm': '20.5', 'counters/examples': 27584, 'counters/updates': 862}
train stats after 27616 examples: {'rewards_train/chosen': '0.025214', 'rewards_train/rejected': '0.047807', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022594', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-149.74', 'loss/train': '0.70726', 'examples_per_second': '31.349', 'grad_norm': '21.625', 'counters/examples': 27616, 'counters/updates': 863}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27680 examples: {'rewards_train/chosen': '0.049513', 'rewards_train/rejected': '0.017447', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032065', 'logps_train/rejected': '-88.808', 'logps_train/chosen': '-117.16', 'loss/train': '0.67877', 'examples_per_second': '31.63', 'grad_norm': '18.25', 'counters/examples': 27680, 'counters/updates': 865}
train stats after 27712 examples: {'rewards_train/chosen': '0.013709', 'rewards_train/rejected': '0.051143', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037434', 'logps_train/rejected': '-148.15', 'logps_train/chosen': '-144.48', 'loss/train': '0.71523', 'examples_per_second': '30.613', 'grad_norm': '23', 'counters/examples': 27712, 'counters/updates': 866}
skipping logging after 27744 examples to avoid logging too frequently
train stats after 27776 examples: {'rewards_train/chosen': '0.081258', 'rewards_train/rejected': '0.062522', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018736', 'logps_train/rejected': '-128.85', 'logps_train/chosen': '-139.17', 'loss/train': '0.68645', 'examples_per_second': '37.056', 'grad_norm': '20', 'counters/examples': 27776, 'counters/updates': 868}
train stats after 27808 examples: {'rewards_train/chosen': '0.054452', 'rewards_train/rejected': '-0.0057977', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060249', 'logps_train/rejected': '-110.62', 'logps_train/chosen': '-125.06', 'loss/train': '0.66811', 'examples_per_second': '31.567', 'grad_norm': '19', 'counters/examples': 27808, 'counters/updates': 869}
train stats after 27840 examples: {'rewards_train/chosen': '0.021396', 'rewards_train/rejected': '-0.00070703', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022103', 'logps_train/rejected': '-111.8', 'logps_train/chosen': '-136.77', 'loss/train': '0.68399', 'examples_per_second': '31.877', 'grad_norm': '20.5', 'counters/examples': 27840, 'counters/updates': 870}
train stats after 27872 examples: {'rewards_train/chosen': '0.038998', 'rewards_train/rejected': '0.007414', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031584', 'logps_train/rejected': '-75.074', 'logps_train/chosen': '-129.29', 'loss/train': '0.68007', 'examples_per_second': '31.467', 'grad_norm': '18.25', 'counters/examples': 27872, 'counters/updates': 871}
train stats after 27904 examples: {'rewards_train/chosen': '0.0032164', 'rewards_train/rejected': '0.0086526', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0054362', 'logps_train/rejected': '-139.75', 'logps_train/chosen': '-146.66', 'loss/train': '0.6983', 'examples_per_second': '31.701', 'grad_norm': '21.75', 'counters/examples': 27904, 'counters/updates': 872}
train stats after 27936 examples: {'rewards_train/chosen': '0.01374', 'rewards_train/rejected': '0.016214', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0024746', 'logps_train/rejected': '-92.922', 'logps_train/chosen': '-113.41', 'loss/train': '0.69694', 'examples_per_second': '31.704', 'grad_norm': '18.25', 'counters/examples': 27936, 'counters/updates': 873}
train stats after 27968 examples: {'rewards_train/chosen': '0.030202', 'rewards_train/rejected': '0.038534', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0083319', 'logps_train/rejected': '-125.25', 'logps_train/chosen': '-133.3', 'loss/train': '0.70247', 'examples_per_second': '30.545', 'grad_norm': '21.75', 'counters/examples': 27968, 'counters/updates': 874}
train stats after 28000 examples: {'rewards_train/chosen': '0.012197', 'rewards_train/rejected': '-0.0015046', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013702', 'logps_train/rejected': '-173.77', 'logps_train/chosen': '-177.56', 'loss/train': '0.6897', 'examples_per_second': '31.752', 'grad_norm': '24.5', 'counters/examples': 28000, 'counters/updates': 875}
train stats after 28032 examples: {'rewards_train/chosen': '0.012103', 'rewards_train/rejected': '0.0058829', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0062202', 'logps_train/rejected': '-175.73', 'logps_train/chosen': '-137.73', 'loss/train': '0.69429', 'examples_per_second': '30.949', 'grad_norm': '22.875', 'counters/examples': 28032, 'counters/updates': 876}
train stats after 28064 examples: {'rewards_train/chosen': '0.025669', 'rewards_train/rejected': '0.018768', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0069007', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-179.93', 'loss/train': '0.69387', 'examples_per_second': '31.623', 'grad_norm': '22.25', 'counters/examples': 28064, 'counters/updates': 877}
train stats after 28096 examples: {'rewards_train/chosen': '0.0033092', 'rewards_train/rejected': '0.034941', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.031631', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-143.59', 'loss/train': '0.71162', 'examples_per_second': '31.597', 'grad_norm': '21', 'counters/examples': 28096, 'counters/updates': 878}
skipping logging after 28128 examples to avoid logging too frequently
train stats after 28160 examples: {'rewards_train/chosen': '0.029302', 'rewards_train/rejected': '0.036349', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0070467', 'logps_train/rejected': '-126.56', 'logps_train/chosen': '-129.2', 'loss/train': '0.70131', 'examples_per_second': '30.182', 'grad_norm': '19.875', 'counters/examples': 28160, 'counters/updates': 880}
train stats after 28192 examples: {'rewards_train/chosen': '0.063284', 'rewards_train/rejected': '0.02562', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037664', 'logps_train/rejected': '-100.28', 'logps_train/chosen': '-144.08', 'loss/train': '0.67946', 'examples_per_second': '30.216', 'grad_norm': '19.5', 'counters/examples': 28192, 'counters/updates': 881}
skipping logging after 28224 examples to avoid logging too frequently
train stats after 28256 examples: {'rewards_train/chosen': '0.024327', 'rewards_train/rejected': '0.03776', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013433', 'logps_train/rejected': '-143.61', 'logps_train/chosen': '-132.14', 'loss/train': '0.70352', 'examples_per_second': '31.699', 'grad_norm': '22', 'counters/examples': 28256, 'counters/updates': 883}
train stats after 28288 examples: {'rewards_train/chosen': '0.055633', 'rewards_train/rejected': '0.0008141', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054819', 'logps_train/rejected': '-96.431', 'logps_train/chosen': '-119.77', 'loss/train': '0.66884', 'examples_per_second': '31.702', 'grad_norm': '20.375', 'counters/examples': 28288, 'counters/updates': 884}
train stats after 28320 examples: {'rewards_train/chosen': '0.081151', 'rewards_train/rejected': '0.021175', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.059976', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-137.68', 'loss/train': '0.66531', 'examples_per_second': '32.758', 'grad_norm': '21.625', 'counters/examples': 28320, 'counters/updates': 885}
train stats after 28352 examples: {'rewards_train/chosen': '0.064228', 'rewards_train/rejected': '0.0037972', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060431', 'logps_train/rejected': '-113.14', 'logps_train/chosen': '-114.31', 'loss/train': '0.66852', 'examples_per_second': '32.935', 'grad_norm': '18.375', 'counters/examples': 28352, 'counters/updates': 886}
train stats after 28384 examples: {'rewards_train/chosen': '0.023855', 'rewards_train/rejected': '-0.0093493', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033204', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-164.04', 'loss/train': '0.68015', 'examples_per_second': '31.657', 'grad_norm': '21', 'counters/examples': 28384, 'counters/updates': 887}
train stats after 28416 examples: {'rewards_train/chosen': '0.071686', 'rewards_train/rejected': '0.05471', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016976', 'logps_train/rejected': '-124.01', 'logps_train/chosen': '-144.74', 'loss/train': '0.68878', 'examples_per_second': '31.147', 'grad_norm': '21.375', 'counters/examples': 28416, 'counters/updates': 888}
train stats after 28448 examples: {'rewards_train/chosen': '0.0049374', 'rewards_train/rejected': '0.026103', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.021165', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-161.03', 'loss/train': '0.70646', 'examples_per_second': '31.476', 'grad_norm': '22', 'counters/examples': 28448, 'counters/updates': 889}
train stats after 28480 examples: {'rewards_train/chosen': '0.069858', 'rewards_train/rejected': '0.044808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02505', 'logps_train/rejected': '-102.85', 'logps_train/chosen': '-114.85', 'loss/train': '0.68478', 'examples_per_second': '30.568', 'grad_norm': '17.875', 'counters/examples': 28480, 'counters/updates': 890}
train stats after 28512 examples: {'rewards_train/chosen': '0.010643', 'rewards_train/rejected': '-0.00083755', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011481', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-120.02', 'loss/train': '0.69102', 'examples_per_second': '31.557', 'grad_norm': '19.375', 'counters/examples': 28512, 'counters/updates': 891}
train stats after 28544 examples: {'rewards_train/chosen': '0.079261', 'rewards_train/rejected': '0.00059103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07867', 'logps_train/rejected': '-126.74', 'logps_train/chosen': '-154.45', 'loss/train': '0.65803', 'examples_per_second': '30.248', 'grad_norm': '20.375', 'counters/examples': 28544, 'counters/updates': 892}
skipping logging after 28576 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '0.032174', 'rewards_train/rejected': '0.041737', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0095629', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-164.45', 'loss/train': '0.70111', 'examples_per_second': '32.582', 'grad_norm': '21.75', 'counters/examples': 28608, 'counters/updates': 894}
train stats after 28640 examples: {'rewards_train/chosen': '0.035834', 'rewards_train/rejected': '0.076603', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.040769', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-139.6', 'loss/train': '0.71626', 'examples_per_second': '32.476', 'grad_norm': '22.125', 'counters/examples': 28640, 'counters/updates': 895}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28704 examples: {'rewards_train/chosen': '0.057754', 'rewards_train/rejected': '-0.016615', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.074369', 'logps_train/rejected': '-100.7', 'logps_train/chosen': '-163.91', 'loss/train': '0.65856', 'examples_per_second': '31.657', 'grad_norm': '20.375', 'counters/examples': 28704, 'counters/updates': 897}
train stats after 28736 examples: {'rewards_train/chosen': '0.046833', 'rewards_train/rejected': '0.069482', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022649', 'logps_train/rejected': '-129.73', 'logps_train/chosen': '-144.31', 'loss/train': '0.70758', 'examples_per_second': '30.683', 'grad_norm': '22.25', 'counters/examples': 28736, 'counters/updates': 898}
train stats after 28768 examples: {'rewards_train/chosen': '0.036332', 'rewards_train/rejected': '0.033471', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0028605', 'logps_train/rejected': '-119.37', 'logps_train/chosen': '-174.72', 'loss/train': '0.69418', 'examples_per_second': '30.34', 'grad_norm': '22.875', 'counters/examples': 28768, 'counters/updates': 899}
train stats after 28800 examples: {'rewards_train/chosen': '0.078596', 'rewards_train/rejected': '0.050481', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028115', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-125.31', 'loss/train': '0.68404', 'examples_per_second': '31.681', 'grad_norm': '17.75', 'counters/examples': 28800, 'counters/updates': 900}
train stats after 28832 examples: {'rewards_train/chosen': '0.076146', 'rewards_train/rejected': '-0.0041416', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080287', 'logps_train/rejected': '-90.956', 'logps_train/chosen': '-118.63', 'loss/train': '0.65877', 'examples_per_second': '31.058', 'grad_norm': '19.5', 'counters/examples': 28832, 'counters/updates': 901}
train stats after 28864 examples: {'rewards_train/chosen': '0.072691', 'rewards_train/rejected': '0.025803', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046888', 'logps_train/rejected': '-109.26', 'logps_train/chosen': '-115.18', 'loss/train': '0.67243', 'examples_per_second': '32.825', 'grad_norm': '18.875', 'counters/examples': 28864, 'counters/updates': 902}
train stats after 28896 examples: {'rewards_train/chosen': '-0.01089', 'rewards_train/rejected': '0.022943', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033833', 'logps_train/rejected': '-91.98', 'logps_train/chosen': '-128.24', 'loss/train': '0.71232', 'examples_per_second': '30.757', 'grad_norm': '18.75', 'counters/examples': 28896, 'counters/updates': 903}
train stats after 28928 examples: {'rewards_train/chosen': '0.045429', 'rewards_train/rejected': '0.014172', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031257', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-127.81', 'loss/train': '0.67977', 'examples_per_second': '30.679', 'grad_norm': '20.375', 'counters/examples': 28928, 'counters/updates': 904}
skipping logging after 28960 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '0.041574', 'rewards_train/rejected': '0.013164', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02841', 'logps_train/rejected': '-97.001', 'logps_train/chosen': '-110.79', 'loss/train': '0.68065', 'examples_per_second': '32.438', 'grad_norm': '17.625', 'counters/examples': 28992, 'counters/updates': 906}
train stats after 29024 examples: {'rewards_train/chosen': '0.025162', 'rewards_train/rejected': '0.013382', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01178', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-151.47', 'loss/train': '0.68892', 'examples_per_second': '30.666', 'grad_norm': '21', 'counters/examples': 29024, 'counters/updates': 907}
train stats after 29056 examples: {'rewards_train/chosen': '0.041972', 'rewards_train/rejected': '0.025327', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016646', 'logps_train/rejected': '-100.63', 'logps_train/chosen': '-143.46', 'loss/train': '0.6876', 'examples_per_second': '31.358', 'grad_norm': '20.875', 'counters/examples': 29056, 'counters/updates': 908}
train stats after 29088 examples: {'rewards_train/chosen': '0.029173', 'rewards_train/rejected': '0.037172', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0079997', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-149.62', 'loss/train': '0.69922', 'examples_per_second': '31.327', 'grad_norm': '20.75', 'counters/examples': 29088, 'counters/updates': 909}
train stats after 29120 examples: {'rewards_train/chosen': '0.031661', 'rewards_train/rejected': '0.023729', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0079318', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-157.45', 'loss/train': '0.69183', 'examples_per_second': '30.161', 'grad_norm': '19.75', 'counters/examples': 29120, 'counters/updates': 910}
train stats after 29152 examples: {'rewards_train/chosen': '0.056111', 'rewards_train/rejected': '0.055712', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.00039858', 'logps_train/rejected': '-129.2', 'logps_train/chosen': '-143.76', 'loss/train': '0.69532', 'examples_per_second': '31.727', 'grad_norm': '21.125', 'counters/examples': 29152, 'counters/updates': 911}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29216 examples: {'rewards_train/chosen': '0.045145', 'rewards_train/rejected': '0.011718', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033427', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-138.36', 'loss/train': '0.67866', 'examples_per_second': '30.316', 'grad_norm': '20.625', 'counters/examples': 29216, 'counters/updates': 913}
train stats after 29248 examples: {'rewards_train/chosen': '0.035128', 'rewards_train/rejected': '0.03529', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00016209', 'logps_train/rejected': '-111.6', 'logps_train/chosen': '-152.29', 'loss/train': '0.6965', 'examples_per_second': '31.658', 'grad_norm': '20.75', 'counters/examples': 29248, 'counters/updates': 914}
train stats after 29280 examples: {'rewards_train/chosen': '-0.0069595', 'rewards_train/rejected': '0.012561', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019521', 'logps_train/rejected': '-108.88', 'logps_train/chosen': '-140.76', 'loss/train': '0.70674', 'examples_per_second': '31.71', 'grad_norm': '21.125', 'counters/examples': 29280, 'counters/updates': 915}
train stats after 29312 examples: {'rewards_train/chosen': '0.07982', 'rewards_train/rejected': '-0.00072539', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080546', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-139.55', 'loss/train': '0.65789', 'examples_per_second': '30.569', 'grad_norm': '19.625', 'counters/examples': 29312, 'counters/updates': 916}
train stats after 29344 examples: {'rewards_train/chosen': '0.018007', 'rewards_train/rejected': '0.036354', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018347', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-172.36', 'loss/train': '0.70607', 'examples_per_second': '30.663', 'grad_norm': '23', 'counters/examples': 29344, 'counters/updates': 917}
train stats after 29376 examples: {'rewards_train/chosen': '0.0601', 'rewards_train/rejected': '0.030604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029496', 'logps_train/rejected': '-111.11', 'logps_train/chosen': '-133.21', 'loss/train': '0.68054', 'examples_per_second': '30.438', 'grad_norm': '19.125', 'counters/examples': 29376, 'counters/updates': 918}
train stats after 29408 examples: {'rewards_train/chosen': '0.025123', 'rewards_train/rejected': '-0.0013895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026512', 'logps_train/rejected': '-120.53', 'logps_train/chosen': '-139.94', 'loss/train': '0.68584', 'examples_per_second': '32.49', 'grad_norm': '21.75', 'counters/examples': 29408, 'counters/updates': 919}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29472 examples: {'rewards_train/chosen': '0.060541', 'rewards_train/rejected': '-0.031105', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091646', 'logps_train/rejected': '-93.699', 'logps_train/chosen': '-139.56', 'loss/train': '0.65167', 'examples_per_second': '32.499', 'grad_norm': '18.25', 'counters/examples': 29472, 'counters/updates': 921}
train stats after 29504 examples: {'rewards_train/chosen': '0.046847', 'rewards_train/rejected': '0.028846', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018', 'logps_train/rejected': '-84.607', 'logps_train/chosen': '-99.945', 'loss/train': '0.68685', 'examples_per_second': '31.363', 'grad_norm': '17.375', 'counters/examples': 29504, 'counters/updates': 922}
train stats after 29536 examples: {'rewards_train/chosen': '0.021689', 'rewards_train/rejected': '0.0085886', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0131', 'logps_train/rejected': '-135.19', 'logps_train/chosen': '-151.29', 'loss/train': '0.69086', 'examples_per_second': '30.695', 'grad_norm': '21.25', 'counters/examples': 29536, 'counters/updates': 923}
train stats after 29568 examples: {'rewards_train/chosen': '0.0013211', 'rewards_train/rejected': '0.0027704', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0014493', 'logps_train/rejected': '-109.69', 'logps_train/chosen': '-122.73', 'loss/train': '0.69791', 'examples_per_second': '31.387', 'grad_norm': '19.375', 'counters/examples': 29568, 'counters/updates': 924}
skipping logging after 29600 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '0.045064', 'rewards_train/rejected': '0.029256', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015807', 'logps_train/rejected': '-108.33', 'logps_train/chosen': '-118.94', 'loss/train': '0.688', 'examples_per_second': '31.643', 'grad_norm': '20.5', 'counters/examples': 29632, 'counters/updates': 926}
train stats after 29664 examples: {'rewards_train/chosen': '0.057626', 'rewards_train/rejected': '0.030491', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027135', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-125.89', 'loss/train': '0.68217', 'examples_per_second': '31.971', 'grad_norm': '20', 'counters/examples': 29664, 'counters/updates': 927}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29728 examples: {'rewards_train/chosen': '0.055089', 'rewards_train/rejected': '0.074198', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019109', 'logps_train/rejected': '-144.66', 'logps_train/chosen': '-141.79', 'loss/train': '0.70618', 'examples_per_second': '31.488', 'grad_norm': '22', 'counters/examples': 29728, 'counters/updates': 929}
train stats after 29760 examples: {'rewards_train/chosen': '0.0038551', 'rewards_train/rejected': '0.038875', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.03502', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-127.92', 'loss/train': '0.71483', 'examples_per_second': '30.866', 'grad_norm': '21.625', 'counters/examples': 29760, 'counters/updates': 930}
train stats after 29792 examples: {'rewards_train/chosen': '0.029368', 'rewards_train/rejected': '-0.014746', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044114', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-140.04', 'loss/train': '0.67471', 'examples_per_second': '30.552', 'grad_norm': '20.75', 'counters/examples': 29792, 'counters/updates': 931}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29856 examples: {'rewards_train/chosen': '0.074416', 'rewards_train/rejected': '0.045349', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029067', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-124.32', 'loss/train': '0.68192', 'examples_per_second': '34.783', 'grad_norm': '19', 'counters/examples': 29856, 'counters/updates': 933}
skipping logging after 29888 examples to avoid logging too frequently
train stats after 29920 examples: {'rewards_train/chosen': '0.037648', 'rewards_train/rejected': '0.026447', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011201', 'logps_train/rejected': '-116.89', 'logps_train/chosen': '-107.67', 'loss/train': '0.69256', 'examples_per_second': '31.662', 'grad_norm': '19.5', 'counters/examples': 29920, 'counters/updates': 935}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 29984 examples: {'rewards_train/chosen': '0.018631', 'rewards_train/rejected': '0.0067983', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011833', 'logps_train/rejected': '-94.766', 'logps_train/chosen': '-107.59', 'loss/train': '0.69021', 'examples_per_second': '35.992', 'grad_norm': '18.25', 'counters/examples': 29984, 'counters/updates': 937}
train stats after 30016 examples: {'rewards_train/chosen': '0.032258', 'rewards_train/rejected': '0.053636', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021377', 'logps_train/rejected': '-125.39', 'logps_train/chosen': '-133.48', 'loss/train': '0.70696', 'examples_per_second': '26', 'grad_norm': '20.25', 'counters/examples': 30016, 'counters/updates': 938}
skipping logging after 30048 examples to avoid logging too frequently
train stats after 30080 examples: {'rewards_train/chosen': '0.021749', 'rewards_train/rejected': '-0.0099466', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031696', 'logps_train/rejected': '-122.05', 'logps_train/chosen': '-144.32', 'loss/train': '0.67912', 'examples_per_second': '30.731', 'grad_norm': '21', 'counters/examples': 30080, 'counters/updates': 940}
train stats after 30112 examples: {'rewards_train/chosen': '0.03923', 'rewards_train/rejected': '0.029338', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0098922', 'logps_train/rejected': '-125.91', 'logps_train/chosen': '-128.98', 'loss/train': '0.69071', 'examples_per_second': '24.167', 'grad_norm': '20.125', 'counters/examples': 30112, 'counters/updates': 941}
train stats after 30144 examples: {'rewards_train/chosen': '0.039203', 'rewards_train/rejected': '-0.029397', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068601', 'logps_train/rejected': '-115.39', 'logps_train/chosen': '-126.82', 'loss/train': '0.66179', 'examples_per_second': '32.241', 'grad_norm': '18.75', 'counters/examples': 30144, 'counters/updates': 942}
train stats after 30176 examples: {'rewards_train/chosen': '0.030225', 'rewards_train/rejected': '-0.037995', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.068219', 'logps_train/rejected': '-106.92', 'logps_train/chosen': '-109.67', 'loss/train': '0.66192', 'examples_per_second': '32.796', 'grad_norm': '19.625', 'counters/examples': 30176, 'counters/updates': 943}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30240 examples: {'rewards_train/chosen': '0.10902', 'rewards_train/rejected': '0.063251', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045765', 'logps_train/rejected': '-155.28', 'logps_train/chosen': '-145.82', 'loss/train': '0.67366', 'examples_per_second': '33.392', 'grad_norm': '21.375', 'counters/examples': 30240, 'counters/updates': 945}
train stats after 30272 examples: {'rewards_train/chosen': '0.070764', 'rewards_train/rejected': '0.027019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043745', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-136.42', 'loss/train': '0.67385', 'examples_per_second': '31.699', 'grad_norm': '20.25', 'counters/examples': 30272, 'counters/updates': 946}
train stats after 30304 examples: {'rewards_train/chosen': '0.019457', 'rewards_train/rejected': '0.008921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010536', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-133.14', 'loss/train': '0.69054', 'examples_per_second': '31.164', 'grad_norm': '21.25', 'counters/examples': 30304, 'counters/updates': 947}
train stats after 30336 examples: {'rewards_train/chosen': '0.047812', 'rewards_train/rejected': '0.02876', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019052', 'logps_train/rejected': '-141.81', 'logps_train/chosen': '-149.25', 'loss/train': '0.68688', 'examples_per_second': '31.674', 'grad_norm': '22.75', 'counters/examples': 30336, 'counters/updates': 948}
train stats after 30368 examples: {'rewards_train/chosen': '0.044928', 'rewards_train/rejected': '0.0084354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036492', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-137.09', 'loss/train': '0.67853', 'examples_per_second': '32.214', 'grad_norm': '19.875', 'counters/examples': 30368, 'counters/updates': 949}
train stats after 30400 examples: {'rewards_train/chosen': '0.011179', 'rewards_train/rejected': '-0.034832', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046011', 'logps_train/rejected': '-147.6', 'logps_train/chosen': '-139.71', 'loss/train': '0.67503', 'examples_per_second': '31.504', 'grad_norm': '21.25', 'counters/examples': 30400, 'counters/updates': 950}
skipping logging after 30432 examples to avoid logging too frequently
train stats after 30464 examples: {'rewards_train/chosen': '0.087009', 'rewards_train/rejected': '0.027279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05973', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-134.24', 'loss/train': '0.6674', 'examples_per_second': '30.233', 'grad_norm': '20.25', 'counters/examples': 30464, 'counters/updates': 952}
train stats after 30496 examples: {'rewards_train/chosen': '0.058681', 'rewards_train/rejected': '0.023572', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.035109', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-128.65', 'loss/train': '0.67882', 'examples_per_second': '31.732', 'grad_norm': '20.625', 'counters/examples': 30496, 'counters/updates': 953}
skipping logging after 30528 examples to avoid logging too frequently
train stats after 30560 examples: {'rewards_train/chosen': '0.056379', 'rewards_train/rejected': '0.021787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034592', 'logps_train/rejected': '-108.6', 'logps_train/chosen': '-117.1', 'loss/train': '0.67931', 'examples_per_second': '30.764', 'grad_norm': '18.875', 'counters/examples': 30560, 'counters/updates': 955}
train stats after 30592 examples: {'rewards_train/chosen': '0.083887', 'rewards_train/rejected': '0.012267', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071621', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-144.58', 'loss/train': '0.66239', 'examples_per_second': '30.896', 'grad_norm': '18.875', 'counters/examples': 30592, 'counters/updates': 956}
train stats after 30624 examples: {'rewards_train/chosen': '0.067363', 'rewards_train/rejected': '0.046554', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020808', 'logps_train/rejected': '-149.18', 'logps_train/chosen': '-138.1', 'loss/train': '0.68602', 'examples_per_second': '32.439', 'grad_norm': '22.125', 'counters/examples': 30624, 'counters/updates': 957}
train stats after 30656 examples: {'rewards_train/chosen': '0.033171', 'rewards_train/rejected': '0.022952', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010219', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-134.61', 'loss/train': '0.69152', 'examples_per_second': '30.45', 'grad_norm': '20.125', 'counters/examples': 30656, 'counters/updates': 958}
skipping logging after 30688 examples to avoid logging too frequently
train stats after 30720 examples: {'rewards_train/chosen': '0.070314', 'rewards_train/rejected': '0.037816', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032497', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-118.24', 'loss/train': '0.67951', 'examples_per_second': '36.119', 'grad_norm': '19', 'counters/examples': 30720, 'counters/updates': 960}
train stats after 30752 examples: {'rewards_train/chosen': '0.054902', 'rewards_train/rejected': '-0.0091162', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064018', 'logps_train/rejected': '-100.05', 'logps_train/chosen': '-133.54', 'loss/train': '0.66679', 'examples_per_second': '25.755', 'grad_norm': '18.875', 'counters/examples': 30752, 'counters/updates': 961}
train stats after 30784 examples: {'rewards_train/chosen': '0.034391', 'rewards_train/rejected': '-0.0067867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041178', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-113.28', 'loss/train': '0.67539', 'examples_per_second': '31.651', 'grad_norm': '19.125', 'counters/examples': 30784, 'counters/updates': 962}
train stats after 30816 examples: {'rewards_train/chosen': '0.057666', 'rewards_train/rejected': '-0.012659', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.070325', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-160.88', 'loss/train': '0.66236', 'examples_per_second': '31.311', 'grad_norm': '21.625', 'counters/examples': 30816, 'counters/updates': 963}
train stats after 30848 examples: {'rewards_train/chosen': '0.039335', 'rewards_train/rejected': '0.072871', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033535', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-157.69', 'loss/train': '0.71497', 'examples_per_second': '31.518', 'grad_norm': '22.5', 'counters/examples': 30848, 'counters/updates': 964}
train stats after 30880 examples: {'rewards_train/chosen': '0.032958', 'rewards_train/rejected': '-0.0022728', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035231', 'logps_train/rejected': '-92.476', 'logps_train/chosen': '-138.5', 'loss/train': '0.6788', 'examples_per_second': '31.666', 'grad_norm': '20.125', 'counters/examples': 30880, 'counters/updates': 965}
train stats after 30912 examples: {'rewards_train/chosen': '0.059699', 'rewards_train/rejected': '0.055147', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0045519', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-159.65', 'loss/train': '0.69357', 'examples_per_second': '30.382', 'grad_norm': '21.625', 'counters/examples': 30912, 'counters/updates': 966}
train stats after 30944 examples: {'rewards_train/chosen': '0.031196', 'rewards_train/rejected': '0.0022794', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028916', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-125.18', 'loss/train': '0.68451', 'examples_per_second': '31.715', 'grad_norm': '20.375', 'counters/examples': 30944, 'counters/updates': 967}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31008 examples: {'rewards_train/chosen': '0.05486', 'rewards_train/rejected': '0.039087', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015773', 'logps_train/rejected': '-112.18', 'logps_train/chosen': '-141.22', 'loss/train': '0.68809', 'examples_per_second': '34.531', 'grad_norm': '19.125', 'counters/examples': 31008, 'counters/updates': 969}
train stats after 31040 examples: {'rewards_train/chosen': '0.021674', 'rewards_train/rejected': '0.064362', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.042688', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-98.076', 'loss/train': '0.71841', 'examples_per_second': '31.374', 'grad_norm': '19.375', 'counters/examples': 31040, 'counters/updates': 970}
skipping logging after 31072 examples to avoid logging too frequently
train stats after 31104 examples: {'rewards_train/chosen': '0.082879', 'rewards_train/rejected': '-0.014377', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097257', 'logps_train/rejected': '-150.55', 'logps_train/chosen': '-116.69', 'loss/train': '0.64865', 'examples_per_second': '32.957', 'grad_norm': '18.875', 'counters/examples': 31104, 'counters/updates': 972}
train stats after 31136 examples: {'rewards_train/chosen': '0.044703', 'rewards_train/rejected': '0.037048', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.007655', 'logps_train/rejected': '-114.99', 'logps_train/chosen': '-144.91', 'loss/train': '0.69206', 'examples_per_second': '30.754', 'grad_norm': '20.5', 'counters/examples': 31136, 'counters/updates': 973}
train stats after 31168 examples: {'rewards_train/chosen': '0.025704', 'rewards_train/rejected': '0.002256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023448', 'logps_train/rejected': '-162.9', 'logps_train/chosen': '-134.64', 'loss/train': '0.68506', 'examples_per_second': '31.738', 'grad_norm': '23.125', 'counters/examples': 31168, 'counters/updates': 974}
train stats after 31200 examples: {'rewards_train/chosen': '0.041669', 'rewards_train/rejected': '0.031785', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0098847', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-158.85', 'loss/train': '0.69155', 'examples_per_second': '32.828', 'grad_norm': '21.25', 'counters/examples': 31200, 'counters/updates': 975}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31264 examples: {'rewards_train/chosen': '0.05506', 'rewards_train/rejected': '0.011218', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043842', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-160.35', 'loss/train': '0.67353', 'examples_per_second': '31.45', 'grad_norm': '20.875', 'counters/examples': 31264, 'counters/updates': 977}
train stats after 31296 examples: {'rewards_train/chosen': '0.045719', 'rewards_train/rejected': '-0.0042323', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049951', 'logps_train/rejected': '-99.971', 'logps_train/chosen': '-124.16', 'loss/train': '0.67161', 'examples_per_second': '30.734', 'grad_norm': '19.125', 'counters/examples': 31296, 'counters/updates': 978}
train stats after 31328 examples: {'rewards_train/chosen': '0.091756', 'rewards_train/rejected': '0.05706', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034696', 'logps_train/rejected': '-88.586', 'logps_train/chosen': '-137.49', 'loss/train': '0.67961', 'examples_per_second': '30.325', 'grad_norm': '19.875', 'counters/examples': 31328, 'counters/updates': 979}
train stats after 31360 examples: {'rewards_train/chosen': '0.027319', 'rewards_train/rejected': '0.012154', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015165', 'logps_train/rejected': '-159.38', 'logps_train/chosen': '-157.7', 'loss/train': '0.68922', 'examples_per_second': '31.773', 'grad_norm': '22.375', 'counters/examples': 31360, 'counters/updates': 980}
train stats after 31392 examples: {'rewards_train/chosen': '0.078458', 'rewards_train/rejected': '0.056807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021651', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-156.6', 'loss/train': '0.68589', 'examples_per_second': '30.896', 'grad_norm': '19.875', 'counters/examples': 31392, 'counters/updates': 981}
train stats after 31424 examples: {'rewards_train/chosen': '0.044692', 'rewards_train/rejected': '-0.0097966', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.054489', 'logps_train/rejected': '-104.92', 'logps_train/chosen': '-134.99', 'loss/train': '0.66909', 'examples_per_second': '30.58', 'grad_norm': '20.125', 'counters/examples': 31424, 'counters/updates': 982}
train stats after 31456 examples: {'rewards_train/chosen': '0.065245', 'rewards_train/rejected': '0.04454', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020705', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-159.98', 'loss/train': '0.68498', 'examples_per_second': '31.647', 'grad_norm': '21.375', 'counters/examples': 31456, 'counters/updates': 983}
train stats after 31488 examples: {'rewards_train/chosen': '0.059274', 'rewards_train/rejected': '-0.0056725', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064946', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-146.72', 'loss/train': '0.66399', 'examples_per_second': '31.732', 'grad_norm': '20.375', 'counters/examples': 31488, 'counters/updates': 984}
skipping logging after 31520 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '0.019379', 'rewards_train/rejected': '-0.0058318', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025211', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-144.51', 'loss/train': '0.68394', 'examples_per_second': '31.656', 'grad_norm': '20.125', 'counters/examples': 31552, 'counters/updates': 986}
train stats after 31584 examples: {'rewards_train/chosen': '0.049592', 'rewards_train/rejected': '0.024443', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025149', 'logps_train/rejected': '-137.59', 'logps_train/chosen': '-137.78', 'loss/train': '0.68421', 'examples_per_second': '31.77', 'grad_norm': '22.125', 'counters/examples': 31584, 'counters/updates': 987}
train stats after 31616 examples: {'rewards_train/chosen': '-0.0049073', 'rewards_train/rejected': '0.02207', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026977', 'logps_train/rejected': '-114.76', 'logps_train/chosen': '-154.45', 'loss/train': '0.71002', 'examples_per_second': '30.968', 'grad_norm': '21.5', 'counters/examples': 31616, 'counters/updates': 988}
train stats after 31648 examples: {'rewards_train/chosen': '0.039832', 'rewards_train/rejected': '0.03377', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.006062', 'logps_train/rejected': '-130.2', 'logps_train/chosen': '-136.2', 'loss/train': '0.69364', 'examples_per_second': '31.166', 'grad_norm': '20.75', 'counters/examples': 31648, 'counters/updates': 989}
train stats after 31680 examples: {'rewards_train/chosen': '0.040953', 'rewards_train/rejected': '0.03605', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0049032', 'logps_train/rejected': '-113.07', 'logps_train/chosen': '-156.03', 'loss/train': '0.69373', 'examples_per_second': '31.563', 'grad_norm': '20.75', 'counters/examples': 31680, 'counters/updates': 990}
train stats after 31712 examples: {'rewards_train/chosen': '0.093835', 'rewards_train/rejected': '0.046582', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047253', 'logps_train/rejected': '-107.69', 'logps_train/chosen': '-140.08', 'loss/train': '0.67307', 'examples_per_second': '31.06', 'grad_norm': '20.25', 'counters/examples': 31712, 'counters/updates': 991}
train stats after 31744 examples: {'rewards_train/chosen': '0.063702', 'rewards_train/rejected': '-0.017098', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0808', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-161.03', 'loss/train': '0.65805', 'examples_per_second': '31.33', 'grad_norm': '22.625', 'counters/examples': 31744, 'counters/updates': 992}
skipping logging after 31776 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '0.035', 'rewards_train/rejected': '0.033738', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.001262', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-171.26', 'loss/train': '0.69679', 'examples_per_second': '31.689', 'grad_norm': '22.625', 'counters/examples': 31808, 'counters/updates': 994}
train stats after 31840 examples: {'rewards_train/chosen': '0.02103', 'rewards_train/rejected': '0.0092124', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011818', 'logps_train/rejected': '-88.171', 'logps_train/chosen': '-100.12', 'loss/train': '0.68854', 'examples_per_second': '31.838', 'grad_norm': '16.5', 'counters/examples': 31840, 'counters/updates': 995}
train stats after 31872 examples: {'rewards_train/chosen': '0.039284', 'rewards_train/rejected': '0.027324', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01196', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-135.46', 'loss/train': '0.69032', 'examples_per_second': '32.234', 'grad_norm': '19.625', 'counters/examples': 31872, 'counters/updates': 996}
train stats after 31904 examples: {'rewards_train/chosen': '0.040056', 'rewards_train/rejected': '0.0057122', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034344', 'logps_train/rejected': '-158.24', 'logps_train/chosen': '-120.24', 'loss/train': '0.68032', 'examples_per_second': '30.711', 'grad_norm': '21', 'counters/examples': 31904, 'counters/updates': 997}
skipping logging after 31936 examples to avoid logging too frequently
train stats after 31968 examples: {'rewards_train/chosen': '0.082557', 'rewards_train/rejected': '0.070359', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012197', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-146.27', 'loss/train': '0.68976', 'examples_per_second': '32.168', 'grad_norm': '21.375', 'counters/examples': 31968, 'counters/updates': 999}
train stats after 32000 examples: {'rewards_train/chosen': '0.040207', 'rewards_train/rejected': '0.002749', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.037458', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-142.94', 'loss/train': '0.6785', 'examples_per_second': '31.195', 'grad_norm': '21', 'counters/examples': 32000, 'counters/updates': 1000}
train stats after 32032 examples: {'rewards_train/chosen': '0.095898', 'rewards_train/rejected': '0.033724', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.062174', 'logps_train/rejected': '-121.11', 'logps_train/chosen': '-157.19', 'loss/train': '0.66473', 'examples_per_second': '31.647', 'grad_norm': '20.75', 'counters/examples': 32032, 'counters/updates': 1001}
train stats after 32064 examples: {'rewards_train/chosen': '0.031755', 'rewards_train/rejected': '0.025503', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0062526', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-130.86', 'loss/train': '0.69335', 'examples_per_second': '31.688', 'grad_norm': '21', 'counters/examples': 32064, 'counters/updates': 1002}
train stats after 32096 examples: {'rewards_train/chosen': '0.020011', 'rewards_train/rejected': '0.028743', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0087327', 'logps_train/rejected': '-123.08', 'logps_train/chosen': '-97.784', 'loss/train': '0.70037', 'examples_per_second': '31.896', 'grad_norm': '18.625', 'counters/examples': 32096, 'counters/updates': 1003}
train stats after 32128 examples: {'rewards_train/chosen': '0.04187', 'rewards_train/rejected': '0.0066126', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035258', 'logps_train/rejected': '-97.481', 'logps_train/chosen': '-115.74', 'loss/train': '0.67782', 'examples_per_second': '31.746', 'grad_norm': '18.25', 'counters/examples': 32128, 'counters/updates': 1004}
skipping logging after 32160 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '0.012137', 'rewards_train/rejected': '0.050548', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.038411', 'logps_train/rejected': '-146.12', 'logps_train/chosen': '-151.36', 'loss/train': '0.71563', 'examples_per_second': '30.359', 'grad_norm': '23.75', 'counters/examples': 32192, 'counters/updates': 1006}
train stats after 32224 examples: {'rewards_train/chosen': '0.018253', 'rewards_train/rejected': '0.013417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.004836', 'logps_train/rejected': '-109.83', 'logps_train/chosen': '-115.3', 'loss/train': '0.69399', 'examples_per_second': '30.516', 'grad_norm': '18.625', 'counters/examples': 32224, 'counters/updates': 1007}
train stats after 32256 examples: {'rewards_train/chosen': '0.079347', 'rewards_train/rejected': '0.054423', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024924', 'logps_train/rejected': '-96.444', 'logps_train/chosen': '-121.59', 'loss/train': '0.68337', 'examples_per_second': '30.787', 'grad_norm': '18.5', 'counters/examples': 32256, 'counters/updates': 1008}
skipping logging after 32288 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '0.051028', 'rewards_train/rejected': '-0.0051691', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.056198', 'logps_train/rejected': '-97.769', 'logps_train/chosen': '-144.3', 'loss/train': '0.66828', 'examples_per_second': '32.958', 'grad_norm': '19.25', 'counters/examples': 32320, 'counters/updates': 1010}
train stats after 32352 examples: {'rewards_train/chosen': '0.023669', 'rewards_train/rejected': '-0.024574', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048242', 'logps_train/rejected': '-124.5', 'logps_train/chosen': '-133.62', 'loss/train': '0.67248', 'examples_per_second': '31.244', 'grad_norm': '20.125', 'counters/examples': 32352, 'counters/updates': 1011}
skipping logging after 32384 examples to avoid logging too frequently
train stats after 32416 examples: {'rewards_train/chosen': '0.037576', 'rewards_train/rejected': '0.018066', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01951', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-127.74', 'loss/train': '0.68572', 'examples_per_second': '31.682', 'grad_norm': '20.125', 'counters/examples': 32416, 'counters/updates': 1013}
train stats after 32448 examples: {'rewards_train/chosen': '0.074152', 'rewards_train/rejected': '0.02209', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052063', 'logps_train/rejected': '-142.22', 'logps_train/chosen': '-161.93', 'loss/train': '0.67032', 'examples_per_second': '31.615', 'grad_norm': '20.75', 'counters/examples': 32448, 'counters/updates': 1014}
train stats after 32480 examples: {'rewards_train/chosen': '0.074484', 'rewards_train/rejected': '0.055164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01932', 'logps_train/rejected': '-104.59', 'logps_train/chosen': '-106.57', 'loss/train': '0.68682', 'examples_per_second': '31.638', 'grad_norm': '18.375', 'counters/examples': 32480, 'counters/updates': 1015}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32544 examples: {'rewards_train/chosen': '0.067736', 'rewards_train/rejected': '0.022329', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045407', 'logps_train/rejected': '-108.19', 'logps_train/chosen': '-136.32', 'loss/train': '0.6725', 'examples_per_second': '33.703', 'grad_norm': '19.375', 'counters/examples': 32544, 'counters/updates': 1017}
train stats after 32576 examples: {'rewards_train/chosen': '0.042238', 'rewards_train/rejected': '0.017209', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02503', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-129.92', 'loss/train': '0.68383', 'examples_per_second': '31.674', 'grad_norm': '19.375', 'counters/examples': 32576, 'counters/updates': 1018}
train stats after 32608 examples: {'rewards_train/chosen': '0.048946', 'rewards_train/rejected': '0.052262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0033163', 'logps_train/rejected': '-100.51', 'logps_train/chosen': '-124.83', 'loss/train': '0.69777', 'examples_per_second': '32.739', 'grad_norm': '19', 'counters/examples': 32608, 'counters/updates': 1019}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32672 examples: {'rewards_train/chosen': '0.028224', 'rewards_train/rejected': '0.0094413', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018782', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-153.12', 'loss/train': '0.68678', 'examples_per_second': '31.616', 'grad_norm': '20.75', 'counters/examples': 32672, 'counters/updates': 1021}
skipping logging after 32704 examples to avoid logging too frequently
train stats after 32736 examples: {'rewards_train/chosen': '0.060802', 'rewards_train/rejected': '0.025296', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035505', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-156.95', 'loss/train': '0.6793', 'examples_per_second': '30.124', 'grad_norm': '21.625', 'counters/examples': 32736, 'counters/updates': 1023}
train stats after 32768 examples: {'rewards_train/chosen': '0.096334', 'rewards_train/rejected': '0.030183', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066151', 'logps_train/rejected': '-160.85', 'logps_train/chosen': '-158.43', 'loss/train': '0.66606', 'examples_per_second': '31.654', 'grad_norm': '22.75', 'counters/examples': 32768, 'counters/updates': 1024}
skipping logging after 32800 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '0.046452', 'rewards_train/rejected': '-0.014806', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061258', 'logps_train/rejected': '-125.44', 'logps_train/chosen': '-119.24', 'loss/train': '0.66813', 'examples_per_second': '31.645', 'grad_norm': '19.125', 'counters/examples': 32832, 'counters/updates': 1026}
train stats after 32864 examples: {'rewards_train/chosen': '0.090529', 'rewards_train/rejected': '0.051261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039268', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-148.17', 'loss/train': '0.67889', 'examples_per_second': '31.164', 'grad_norm': '21.375', 'counters/examples': 32864, 'counters/updates': 1027}
train stats after 32896 examples: {'rewards_train/chosen': '0.063868', 'rewards_train/rejected': '0.0089028', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.054965', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-127.9', 'loss/train': '0.66844', 'examples_per_second': '31.773', 'grad_norm': '20.625', 'counters/examples': 32896, 'counters/updates': 1028}
skipping logging after 32928 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '0.04633', 'rewards_train/rejected': '-0.0022876', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048617', 'logps_train/rejected': '-151.76', 'logps_train/chosen': '-139.17', 'loss/train': '0.67262', 'examples_per_second': '30.604', 'grad_norm': '23', 'counters/examples': 32960, 'counters/updates': 1030}
train stats after 32992 examples: {'rewards_train/chosen': '0.043895', 'rewards_train/rejected': '0.043683', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.00021243', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-120.27', 'loss/train': '0.69667', 'examples_per_second': '31.346', 'grad_norm': '19.875', 'counters/examples': 32992, 'counters/updates': 1031}
train stats after 33024 examples: {'rewards_train/chosen': '0.034492', 'rewards_train/rejected': '0.018861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015631', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-107.05', 'loss/train': '0.68907', 'examples_per_second': '31.653', 'grad_norm': '18.375', 'counters/examples': 33024, 'counters/updates': 1032}
train stats after 33056 examples: {'rewards_train/chosen': '0.068491', 'rewards_train/rejected': '0.038347', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030144', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-147.49', 'loss/train': '0.67968', 'examples_per_second': '31.799', 'grad_norm': '20.375', 'counters/examples': 33056, 'counters/updates': 1033}
train stats after 33088 examples: {'rewards_train/chosen': '0.066867', 'rewards_train/rejected': '0.0098459', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057021', 'logps_train/rejected': '-183.43', 'logps_train/chosen': '-165.02', 'loss/train': '0.67019', 'examples_per_second': '31.463', 'grad_norm': '23.625', 'counters/examples': 33088, 'counters/updates': 1034}
skipping logging after 33120 examples to avoid logging too frequently
train stats after 33152 examples: {'rewards_train/chosen': '0.029581', 'rewards_train/rejected': '0.0069849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022596', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-136.18', 'loss/train': '0.68376', 'examples_per_second': '30.37', 'grad_norm': '20.5', 'counters/examples': 33152, 'counters/updates': 1036}
train stats after 33184 examples: {'rewards_train/chosen': '0.025669', 'rewards_train/rejected': '0.016786', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0088831', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-133.28', 'loss/train': '0.69081', 'examples_per_second': '31.645', 'grad_norm': '21', 'counters/examples': 33184, 'counters/updates': 1037}
train stats after 33216 examples: {'rewards_train/chosen': '0.051284', 'rewards_train/rejected': '0.051837', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0005534', 'logps_train/rejected': '-119.87', 'logps_train/chosen': '-130.72', 'loss/train': '0.69776', 'examples_per_second': '30.725', 'grad_norm': '20.375', 'counters/examples': 33216, 'counters/updates': 1038}
train stats after 33248 examples: {'rewards_train/chosen': '0.049234', 'rewards_train/rejected': '0.0083638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04087', 'logps_train/rejected': '-170.38', 'logps_train/chosen': '-170.96', 'loss/train': '0.67586', 'examples_per_second': '30.269', 'grad_norm': '23.375', 'counters/examples': 33248, 'counters/updates': 1039}
train stats after 33280 examples: {'rewards_train/chosen': '0.068198', 'rewards_train/rejected': '0.059217', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0089814', 'logps_train/rejected': '-150.47', 'logps_train/chosen': '-129.39', 'loss/train': '0.6935', 'examples_per_second': '32.551', 'grad_norm': '21', 'counters/examples': 33280, 'counters/updates': 1040}
train stats after 33312 examples: {'rewards_train/chosen': '0.073918', 'rewards_train/rejected': '0.039137', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.034781', 'logps_train/rejected': '-132.63', 'logps_train/chosen': '-151.72', 'loss/train': '0.67877', 'examples_per_second': '31.892', 'grad_norm': '20.875', 'counters/examples': 33312, 'counters/updates': 1041}
train stats after 33344 examples: {'rewards_train/chosen': '0.04526', 'rewards_train/rejected': '0.0184', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026859', 'logps_train/rejected': '-151.36', 'logps_train/chosen': '-144.94', 'loss/train': '0.68446', 'examples_per_second': '33.104', 'grad_norm': '21.625', 'counters/examples': 33344, 'counters/updates': 1042}
train stats after 33376 examples: {'rewards_train/chosen': '0.075129', 'rewards_train/rejected': '0.035307', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039823', 'logps_train/rejected': '-125.6', 'logps_train/chosen': '-164.76', 'loss/train': '0.67608', 'examples_per_second': '32.509', 'grad_norm': '21.625', 'counters/examples': 33376, 'counters/updates': 1043}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33440 examples: {'rewards_train/chosen': '0.061288', 'rewards_train/rejected': '0.0088918', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052397', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-156.67', 'loss/train': '0.6695', 'examples_per_second': '33.353', 'grad_norm': '20.375', 'counters/examples': 33440, 'counters/updates': 1045}
train stats after 33472 examples: {'rewards_train/chosen': '0.055732', 'rewards_train/rejected': '0.014076', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041657', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-143.45', 'loss/train': '0.67659', 'examples_per_second': '31.66', 'grad_norm': '20.5', 'counters/examples': 33472, 'counters/updates': 1046}
skipping logging after 33504 examples to avoid logging too frequently
train stats after 33536 examples: {'rewards_train/chosen': '0.02659', 'rewards_train/rejected': '0.002607', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023983', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-148.98', 'loss/train': '0.68348', 'examples_per_second': '33.11', 'grad_norm': '20.25', 'counters/examples': 33536, 'counters/updates': 1048}
train stats after 33568 examples: {'rewards_train/chosen': '0.049224', 'rewards_train/rejected': '0.042206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0070182', 'logps_train/rejected': '-104.19', 'logps_train/chosen': '-128.88', 'loss/train': '0.69275', 'examples_per_second': '31.429', 'grad_norm': '19.375', 'counters/examples': 33568, 'counters/updates': 1049}
train stats after 33600 examples: {'rewards_train/chosen': '0.0038463', 'rewards_train/rejected': '0.0080406', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0041943', 'logps_train/rejected': '-107.71', 'logps_train/chosen': '-175.08', 'loss/train': '0.69889', 'examples_per_second': '31.587', 'grad_norm': '23.375', 'counters/examples': 33600, 'counters/updates': 1050}
train stats after 33632 examples: {'rewards_train/chosen': '0.040116', 'rewards_train/rejected': '0.027989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012127', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-149.79', 'loss/train': '0.68929', 'examples_per_second': '31.638', 'grad_norm': '20.625', 'counters/examples': 33632, 'counters/updates': 1051}
train stats after 33664 examples: {'rewards_train/chosen': '0.072256', 'rewards_train/rejected': '0.062866', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0093892', 'logps_train/rejected': '-126.25', 'logps_train/chosen': '-123.52', 'loss/train': '0.69209', 'examples_per_second': '30.178', 'grad_norm': '20', 'counters/examples': 33664, 'counters/updates': 1052}
train stats after 33696 examples: {'rewards_train/chosen': '0.039501', 'rewards_train/rejected': '-0.00059869', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0401', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-148.53', 'loss/train': '0.67554', 'examples_per_second': '30.164', 'grad_norm': '20.5', 'counters/examples': 33696, 'counters/updates': 1053}
train stats after 33728 examples: {'rewards_train/chosen': '0.039566', 'rewards_train/rejected': '0.030771', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0087947', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-129.38', 'loss/train': '0.69118', 'examples_per_second': '30.501', 'grad_norm': '19.5', 'counters/examples': 33728, 'counters/updates': 1054}
train stats after 33760 examples: {'rewards_train/chosen': '0.021762', 'rewards_train/rejected': '0.035487', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013724', 'logps_train/rejected': '-102.7', 'logps_train/chosen': '-151.37', 'loss/train': '0.70382', 'examples_per_second': '30.495', 'grad_norm': '20.625', 'counters/examples': 33760, 'counters/updates': 1055}
train stats after 33792 examples: {'rewards_train/chosen': '0.092609', 'rewards_train/rejected': '0.050424', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042185', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-144.01', 'loss/train': '0.67708', 'examples_per_second': '31.773', 'grad_norm': '19.75', 'counters/examples': 33792, 'counters/updates': 1056}
train stats after 33824 examples: {'rewards_train/chosen': '0.071905', 'rewards_train/rejected': '0.014973', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056933', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-152.33', 'loss/train': '0.66881', 'examples_per_second': '30.341', 'grad_norm': '19.5', 'counters/examples': 33824, 'counters/updates': 1057}
train stats after 33856 examples: {'rewards_train/chosen': '-0.02113', 'rewards_train/rejected': '0.035955', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.057085', 'logps_train/rejected': '-117.07', 'logps_train/chosen': '-121.12', 'loss/train': '0.72497', 'examples_per_second': '31.49', 'grad_norm': '20.375', 'counters/examples': 33856, 'counters/updates': 1058}
train stats after 33888 examples: {'rewards_train/chosen': '0.043069', 'rewards_train/rejected': '-0.0097654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052834', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-147.86', 'loss/train': '0.66936', 'examples_per_second': '31.589', 'grad_norm': '19.625', 'counters/examples': 33888, 'counters/updates': 1059}
train stats after 33920 examples: {'rewards_train/chosen': '0.057931', 'rewards_train/rejected': '0.045382', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012549', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-143.41', 'loss/train': '0.69', 'examples_per_second': '31.749', 'grad_norm': '20', 'counters/examples': 33920, 'counters/updates': 1060}
train stats after 33952 examples: {'rewards_train/chosen': '0.04997', 'rewards_train/rejected': '0.038604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011367', 'logps_train/rejected': '-123.99', 'logps_train/chosen': '-126.37', 'loss/train': '0.69187', 'examples_per_second': '32.249', 'grad_norm': '20.125', 'counters/examples': 33952, 'counters/updates': 1061}
train stats after 33984 examples: {'rewards_train/chosen': '0.075555', 'rewards_train/rejected': '0.01668', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058875', 'logps_train/rejected': '-92.52', 'logps_train/chosen': '-156.6', 'loss/train': '0.66899', 'examples_per_second': '31.667', 'grad_norm': '22', 'counters/examples': 33984, 'counters/updates': 1062}
train stats after 34016 examples: {'rewards_train/chosen': '0.087885', 'rewards_train/rejected': '0.078711', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0091736', 'logps_train/rejected': '-107.05', 'logps_train/chosen': '-130.98', 'loss/train': '0.6917', 'examples_per_second': '30.675', 'grad_norm': '19.625', 'counters/examples': 34016, 'counters/updates': 1063}
train stats after 34048 examples: {'rewards_train/chosen': '0.063767', 'rewards_train/rejected': '-0.0035466', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067314', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-135.53', 'loss/train': '0.66469', 'examples_per_second': '30.48', 'grad_norm': '19', 'counters/examples': 34048, 'counters/updates': 1064}
train stats after 34080 examples: {'rewards_train/chosen': '0.039393', 'rewards_train/rejected': '0.016886', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022506', 'logps_train/rejected': '-85.599', 'logps_train/chosen': '-152.57', 'loss/train': '0.68442', 'examples_per_second': '30.269', 'grad_norm': '19.125', 'counters/examples': 34080, 'counters/updates': 1065}
skipping logging after 34112 examples to avoid logging too frequently
train stats after 34144 examples: {'rewards_train/chosen': '0.055546', 'rewards_train/rejected': '0.015892', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039654', 'logps_train/rejected': '-129.56', 'logps_train/chosen': '-117.85', 'loss/train': '0.67552', 'examples_per_second': '34.115', 'grad_norm': '21.375', 'counters/examples': 34144, 'counters/updates': 1067}
train stats after 34176 examples: {'rewards_train/chosen': '0.052494', 'rewards_train/rejected': '0.038952', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013541', 'logps_train/rejected': '-129.84', 'logps_train/chosen': '-158.91', 'loss/train': '0.69077', 'examples_per_second': '31.662', 'grad_norm': '23.875', 'counters/examples': 34176, 'counters/updates': 1068}
train stats after 34208 examples: {'rewards_train/chosen': '0.024302', 'rewards_train/rejected': '0.01224', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012062', 'logps_train/rejected': '-93.821', 'logps_train/chosen': '-126.45', 'loss/train': '0.68922', 'examples_per_second': '32.774', 'grad_norm': '19.125', 'counters/examples': 34208, 'counters/updates': 1069}
train stats after 34240 examples: {'rewards_train/chosen': '0.036921', 'rewards_train/rejected': '0.052476', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015555', 'logps_train/rejected': '-132.19', 'logps_train/chosen': '-145.69', 'loss/train': '0.70573', 'examples_per_second': '31.76', 'grad_norm': '21.25', 'counters/examples': 34240, 'counters/updates': 1070}
train stats after 34272 examples: {'rewards_train/chosen': '0.049948', 'rewards_train/rejected': '-0.027517', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.077465', 'logps_train/rejected': '-134.83', 'logps_train/chosen': '-178.34', 'loss/train': '0.65886', 'examples_per_second': '31.444', 'grad_norm': '22.25', 'counters/examples': 34272, 'counters/updates': 1071}
train stats after 34304 examples: {'rewards_train/chosen': '0.039739', 'rewards_train/rejected': '0.048743', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0090048', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-144.24', 'loss/train': '0.70146', 'examples_per_second': '31.643', 'grad_norm': '21.375', 'counters/examples': 34304, 'counters/updates': 1072}
train stats after 34336 examples: {'rewards_train/chosen': '0.039045', 'rewards_train/rejected': '-0.037254', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076299', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-92.258', 'loss/train': '0.65799', 'examples_per_second': '31.513', 'grad_norm': '18.5', 'counters/examples': 34336, 'counters/updates': 1073}
skipping logging after 34368 examples to avoid logging too frequently
train stats after 34400 examples: {'rewards_train/chosen': '0.067087', 'rewards_train/rejected': '0.0093189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057768', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-166.36', 'loss/train': '0.66845', 'examples_per_second': '30.46', 'grad_norm': '20.5', 'counters/examples': 34400, 'counters/updates': 1075}
train stats after 34432 examples: {'rewards_train/chosen': '0.031451', 'rewards_train/rejected': '0.018497', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012954', 'logps_train/rejected': '-109.8', 'logps_train/chosen': '-138.38', 'loss/train': '0.68975', 'examples_per_second': '31.093', 'grad_norm': '20.5', 'counters/examples': 34432, 'counters/updates': 1076}
train stats after 34464 examples: {'rewards_train/chosen': '-0.0055555', 'rewards_train/rejected': '-0.012921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0073657', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-127.62', 'loss/train': '0.69499', 'examples_per_second': '31.51', 'grad_norm': '22.75', 'counters/examples': 34464, 'counters/updates': 1077}
train stats after 34496 examples: {'rewards_train/chosen': '0.079528', 'rewards_train/rejected': '0.020711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058816', 'logps_train/rejected': '-100.34', 'logps_train/chosen': '-135.29', 'loss/train': '0.66878', 'examples_per_second': '31.47', 'grad_norm': '18.875', 'counters/examples': 34496, 'counters/updates': 1078}
train stats after 34528 examples: {'rewards_train/chosen': '0.044558', 'rewards_train/rejected': '0.046368', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0018096', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-110.74', 'loss/train': '0.69618', 'examples_per_second': '32.805', 'grad_norm': '19.375', 'counters/examples': 34528, 'counters/updates': 1079}
train stats after 34560 examples: {'rewards_train/chosen': '0.10705', 'rewards_train/rejected': '0.045439', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061606', 'logps_train/rejected': '-117.19', 'logps_train/chosen': '-141.2', 'loss/train': '0.6691', 'examples_per_second': '30.507', 'grad_norm': '19.5', 'counters/examples': 34560, 'counters/updates': 1080}
train stats after 34592 examples: {'rewards_train/chosen': '0.096647', 'rewards_train/rejected': '0.034036', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062611', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-149.84', 'loss/train': '0.6657', 'examples_per_second': '31.129', 'grad_norm': '23.25', 'counters/examples': 34592, 'counters/updates': 1081}
skipping logging after 34624 examples to avoid logging too frequently
train stats after 34656 examples: {'rewards_train/chosen': '0.10276', 'rewards_train/rejected': '0.011342', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.091416', 'logps_train/rejected': '-94.154', 'logps_train/chosen': '-146.17', 'loss/train': '0.65167', 'examples_per_second': '31.619', 'grad_norm': '18.75', 'counters/examples': 34656, 'counters/updates': 1083}
train stats after 34688 examples: {'rewards_train/chosen': '0.058002', 'rewards_train/rejected': '0.019615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038387', 'logps_train/rejected': '-95.015', 'logps_train/chosen': '-112.05', 'loss/train': '0.67861', 'examples_per_second': '30.831', 'grad_norm': '18.25', 'counters/examples': 34688, 'counters/updates': 1084}
skipping logging after 34720 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '0.085926', 'rewards_train/rejected': '0.050455', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035471', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-116.05', 'loss/train': '0.67994', 'examples_per_second': '31.768', 'grad_norm': '20.125', 'counters/examples': 34752, 'counters/updates': 1086}
train stats after 34784 examples: {'rewards_train/chosen': '0.029687', 'rewards_train/rejected': '0.030992', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.001305', 'logps_train/rejected': '-123.95', 'logps_train/chosen': '-119.13', 'loss/train': '0.69551', 'examples_per_second': '30.264', 'grad_norm': '19.875', 'counters/examples': 34784, 'counters/updates': 1087}
train stats after 34816 examples: {'rewards_train/chosen': '0.058148', 'rewards_train/rejected': '0.023847', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0343', 'logps_train/rejected': '-152.6', 'logps_train/chosen': '-103.35', 'loss/train': '0.68124', 'examples_per_second': '30.999', 'grad_norm': '19.375', 'counters/examples': 34816, 'counters/updates': 1088}
train stats after 34848 examples: {'rewards_train/chosen': '0.067535', 'rewards_train/rejected': '0.033425', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03411', 'logps_train/rejected': '-108.03', 'logps_train/chosen': '-149.66', 'loss/train': '0.67982', 'examples_per_second': '31.606', 'grad_norm': '20', 'counters/examples': 34848, 'counters/updates': 1089}
skipping logging after 34880 examples to avoid logging too frequently
train stats after 34912 examples: {'rewards_train/chosen': '0.046408', 'rewards_train/rejected': '0.050524', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0041158', 'logps_train/rejected': '-136.97', 'logps_train/chosen': '-152.57', 'loss/train': '0.70177', 'examples_per_second': '30.624', 'grad_norm': '23.5', 'counters/examples': 34912, 'counters/updates': 1091}
train stats after 34944 examples: {'rewards_train/chosen': '0.039826', 'rewards_train/rejected': '0.036452', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0033739', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-142.48', 'loss/train': '0.69475', 'examples_per_second': '30.644', 'grad_norm': '20.75', 'counters/examples': 34944, 'counters/updates': 1092}
train stats after 34976 examples: {'rewards_train/chosen': '0.064209', 'rewards_train/rejected': '0.013774', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050435', 'logps_train/rejected': '-98.804', 'logps_train/chosen': '-145.12', 'loss/train': '0.67328', 'examples_per_second': '32.048', 'grad_norm': '19.375', 'counters/examples': 34976, 'counters/updates': 1093}
train stats after 35008 examples: {'rewards_train/chosen': '0.029645', 'rewards_train/rejected': '0.015445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014201', 'logps_train/rejected': '-123.27', 'logps_train/chosen': '-125.19', 'loss/train': '0.69125', 'examples_per_second': '31.782', 'grad_norm': '19', 'counters/examples': 35008, 'counters/updates': 1094}
train stats after 35040 examples: {'rewards_train/chosen': '0.024605', 'rewards_train/rejected': '0.04125', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016645', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-124.79', 'loss/train': '0.70643', 'examples_per_second': '31.643', 'grad_norm': '22', 'counters/examples': 35040, 'counters/updates': 1095}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35104 examples: {'rewards_train/chosen': '0.023621', 'rewards_train/rejected': '0.0069499', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016671', 'logps_train/rejected': '-98.048', 'logps_train/chosen': '-138.96', 'loss/train': '0.68903', 'examples_per_second': '31.613', 'grad_norm': '20', 'counters/examples': 35104, 'counters/updates': 1097}
train stats after 35136 examples: {'rewards_train/chosen': '0.071009', 'rewards_train/rejected': '0.048554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022455', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-113.61', 'loss/train': '0.68601', 'examples_per_second': '32.495', 'grad_norm': '19.625', 'counters/examples': 35136, 'counters/updates': 1098}
train stats after 35168 examples: {'rewards_train/chosen': '0.044158', 'rewards_train/rejected': '0.057496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013337', 'logps_train/rejected': '-153.79', 'logps_train/chosen': '-135.05', 'loss/train': '0.70431', 'examples_per_second': '30.642', 'grad_norm': '22.25', 'counters/examples': 35168, 'counters/updates': 1099}
train stats after 35200 examples: {'rewards_train/chosen': '0.013795', 'rewards_train/rejected': '0.038396', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0246', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-155.97', 'loss/train': '0.70908', 'examples_per_second': '31.612', 'grad_norm': '21.375', 'counters/examples': 35200, 'counters/updates': 1100}
train stats after 35232 examples: {'rewards_train/chosen': '0.029279', 'rewards_train/rejected': '-0.0069099', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036189', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-153.1', 'loss/train': '0.6787', 'examples_per_second': '31.509', 'grad_norm': '20.375', 'counters/examples': 35232, 'counters/updates': 1101}
train stats after 35264 examples: {'rewards_train/chosen': '0.027126', 'rewards_train/rejected': '-0.014758', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041883', 'logps_train/rejected': '-103.9', 'logps_train/chosen': '-105.02', 'loss/train': '0.67477', 'examples_per_second': '31.265', 'grad_norm': '18.25', 'counters/examples': 35264, 'counters/updates': 1102}
train stats after 35296 examples: {'rewards_train/chosen': '0.080757', 'rewards_train/rejected': '-0.0035143', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084271', 'logps_train/rejected': '-92.99', 'logps_train/chosen': '-114.99', 'loss/train': '0.6542', 'examples_per_second': '31.775', 'grad_norm': '17.375', 'counters/examples': 35296, 'counters/updates': 1103}
train stats after 35328 examples: {'rewards_train/chosen': '0.059657', 'rewards_train/rejected': '0.046934', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012723', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-132.16', 'loss/train': '0.68909', 'examples_per_second': '30.453', 'grad_norm': '19.375', 'counters/examples': 35328, 'counters/updates': 1104}
train stats after 35360 examples: {'rewards_train/chosen': '0.042432', 'rewards_train/rejected': '0.034346', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0080862', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-149.96', 'loss/train': '0.69305', 'examples_per_second': '31.561', 'grad_norm': '21.625', 'counters/examples': 35360, 'counters/updates': 1105}
train stats after 35392 examples: {'rewards_train/chosen': '0.030236', 'rewards_train/rejected': '0.022535', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0077007', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-148.16', 'loss/train': '0.69335', 'examples_per_second': '30.108', 'grad_norm': '20.875', 'counters/examples': 35392, 'counters/updates': 1106}
train stats after 35424 examples: {'rewards_train/chosen': '0.033912', 'rewards_train/rejected': '0.05664', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022728', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-111.85', 'loss/train': '0.70928', 'examples_per_second': '31.597', 'grad_norm': '21.25', 'counters/examples': 35424, 'counters/updates': 1107}
train stats after 35456 examples: {'rewards_train/chosen': '0.081622', 'rewards_train/rejected': '0.067128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014495', 'logps_train/rejected': '-140.13', 'logps_train/chosen': '-129.68', 'loss/train': '0.68834', 'examples_per_second': '31.044', 'grad_norm': '20.75', 'counters/examples': 35456, 'counters/updates': 1108}
train stats after 35488 examples: {'rewards_train/chosen': '0.050794', 'rewards_train/rejected': '0.00068741', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050106', 'logps_train/rejected': '-102.28', 'logps_train/chosen': '-123.38', 'loss/train': '0.67139', 'examples_per_second': '24.13', 'grad_norm': '18.375', 'counters/examples': 35488, 'counters/updates': 1109}
train stats after 35520 examples: {'rewards_train/chosen': '0.10905', 'rewards_train/rejected': '0.018059', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090991', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-170.06', 'loss/train': '0.65348', 'examples_per_second': '31.478', 'grad_norm': '21.375', 'counters/examples': 35520, 'counters/updates': 1110}
train stats after 35552 examples: {'rewards_train/chosen': '0.014492', 'rewards_train/rejected': '-0.0021935', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016686', 'logps_train/rejected': '-140.95', 'logps_train/chosen': '-120.53', 'loss/train': '0.68791', 'examples_per_second': '31.603', 'grad_norm': '20.625', 'counters/examples': 35552, 'counters/updates': 1111}
train stats after 35584 examples: {'rewards_train/chosen': '0.043969', 'rewards_train/rejected': '0.070299', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026329', 'logps_train/rejected': '-109.98', 'logps_train/chosen': '-121.16', 'loss/train': '0.7088', 'examples_per_second': '24.604', 'grad_norm': '19.125', 'counters/examples': 35584, 'counters/updates': 1112}
skipping logging after 35616 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '0.068749', 'rewards_train/rejected': '0.015411', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053338', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-149.43', 'loss/train': '0.67066', 'examples_per_second': '31.576', 'grad_norm': '21.75', 'counters/examples': 35648, 'counters/updates': 1114}
train stats after 35680 examples: {'rewards_train/chosen': '0.039745', 'rewards_train/rejected': '0.02937', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.010375', 'logps_train/rejected': '-129.62', 'logps_train/chosen': '-145.34', 'loss/train': '0.69254', 'examples_per_second': '32.023', 'grad_norm': '21', 'counters/examples': 35680, 'counters/updates': 1115}
train stats after 35712 examples: {'rewards_train/chosen': '0.044295', 'rewards_train/rejected': '0.019474', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024821', 'logps_train/rejected': '-146.08', 'logps_train/chosen': '-121.09', 'loss/train': '0.68411', 'examples_per_second': '30.784', 'grad_norm': '21.375', 'counters/examples': 35712, 'counters/updates': 1116}
train stats after 35744 examples: {'rewards_train/chosen': '0.07978', 'rewards_train/rejected': '0.054888', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024892', 'logps_train/rejected': '-164.43', 'logps_train/chosen': '-147.45', 'loss/train': '0.68509', 'examples_per_second': '32.391', 'grad_norm': '21.875', 'counters/examples': 35744, 'counters/updates': 1117}
train stats after 35776 examples: {'rewards_train/chosen': '0.071889', 'rewards_train/rejected': '0.072299', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00041065', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-136.78', 'loss/train': '0.69609', 'examples_per_second': '30.676', 'grad_norm': '21.125', 'counters/examples': 35776, 'counters/updates': 1118}
train stats after 35808 examples: {'rewards_train/chosen': '0.055482', 'rewards_train/rejected': '0.037799', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017683', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-156.24', 'loss/train': '0.6895', 'examples_per_second': '31.668', 'grad_norm': '21.875', 'counters/examples': 35808, 'counters/updates': 1119}
train stats after 35840 examples: {'rewards_train/chosen': '0.036215', 'rewards_train/rejected': '-0.0073387', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043554', 'logps_train/rejected': '-138.39', 'logps_train/chosen': '-120.87', 'loss/train': '0.67414', 'examples_per_second': '31.406', 'grad_norm': '19.875', 'counters/examples': 35840, 'counters/updates': 1120}
train stats after 35872 examples: {'rewards_train/chosen': '0.080706', 'rewards_train/rejected': '0.038013', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042693', 'logps_train/rejected': '-143.38', 'logps_train/chosen': '-138.97', 'loss/train': '0.67662', 'examples_per_second': '33.113', 'grad_norm': '22.375', 'counters/examples': 35872, 'counters/updates': 1121}
train stats after 35904 examples: {'rewards_train/chosen': '0.017653', 'rewards_train/rejected': '0.0094754', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0081781', 'logps_train/rejected': '-101.28', 'logps_train/chosen': '-142.3', 'loss/train': '0.69436', 'examples_per_second': '32.05', 'grad_norm': '20.375', 'counters/examples': 35904, 'counters/updates': 1122}
train stats after 35936 examples: {'rewards_train/chosen': '0.069277', 'rewards_train/rejected': '0.049705', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019571', 'logps_train/rejected': '-99.236', 'logps_train/chosen': '-125.56', 'loss/train': '0.68688', 'examples_per_second': '30.226', 'grad_norm': '18.875', 'counters/examples': 35936, 'counters/updates': 1123}
train stats after 35968 examples: {'rewards_train/chosen': '0.070506', 'rewards_train/rejected': '0.026008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044498', 'logps_train/rejected': '-95.264', 'logps_train/chosen': '-113.86', 'loss/train': '0.6737', 'examples_per_second': '30.896', 'grad_norm': '18.875', 'counters/examples': 35968, 'counters/updates': 1124}
train stats after 36000 examples: {'rewards_train/chosen': '0.040743', 'rewards_train/rejected': '0.034143', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0066003', 'logps_train/rejected': '-155.2', 'logps_train/chosen': '-147.28', 'loss/train': '0.69316', 'examples_per_second': '31.548', 'grad_norm': '22.25', 'counters/examples': 36000, 'counters/updates': 1125}
Running evaluation after 36000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.81it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.88it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.73it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.71it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.78it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.73it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.85it/s]
eval after 36000: {'rewards_eval/chosen': '0.049964', 'rewards_eval/rejected': '0.025973', 'rewards_eval/accuracies': '0.55859', 'rewards_eval/margins': '0.023991', 'logps_eval/rejected': '-115.13', 'logps_eval/chosen': '-135.05', 'loss/eval': '0.68521'}
skipping save for non epoch
train stats after 36032 examples: {'rewards_train/chosen': '0.045065', 'rewards_train/rejected': '-0.002656', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047721', 'logps_train/rejected': '-96.986', 'logps_train/chosen': '-159.71', 'loss/train': '0.67191', 'examples_per_second': '33.309', 'grad_norm': '19.375', 'counters/examples': 36032, 'counters/updates': 1126}
train stats after 36064 examples: {'rewards_train/chosen': '0.052799', 'rewards_train/rejected': '0.085781', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032982', 'logps_train/rejected': '-157.05', 'logps_train/chosen': '-131.81', 'loss/train': '0.71188', 'examples_per_second': '31.598', 'grad_norm': '22.5', 'counters/examples': 36064, 'counters/updates': 1127}
train stats after 36096 examples: {'rewards_train/chosen': '0.039972', 'rewards_train/rejected': '0.033858', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0061142', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-114.22', 'loss/train': '0.69285', 'examples_per_second': '31.559', 'grad_norm': '19.5', 'counters/examples': 36096, 'counters/updates': 1128}
train stats after 36128 examples: {'rewards_train/chosen': '0.054099', 'rewards_train/rejected': '0.044903', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0091963', 'logps_train/rejected': '-109.83', 'logps_train/chosen': '-139.57', 'loss/train': '0.69219', 'examples_per_second': '30.808', 'grad_norm': '20.25', 'counters/examples': 36128, 'counters/updates': 1129}
train stats after 36160 examples: {'rewards_train/chosen': '0.052086', 'rewards_train/rejected': '0.028516', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02357', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-133.32', 'loss/train': '0.68509', 'examples_per_second': '30.845', 'grad_norm': '19.5', 'counters/examples': 36160, 'counters/updates': 1130}
train stats after 36192 examples: {'rewards_train/chosen': '0.057625', 'rewards_train/rejected': '0.030732', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026894', 'logps_train/rejected': '-135.68', 'logps_train/chosen': '-143.31', 'loss/train': '0.68234', 'examples_per_second': '32.1', 'grad_norm': '21.625', 'counters/examples': 36192, 'counters/updates': 1131}
train stats after 36224 examples: {'rewards_train/chosen': '0.05529', 'rewards_train/rejected': '0.056229', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00093884', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-142.35', 'loss/train': '0.69636', 'examples_per_second': '31.252', 'grad_norm': '20.625', 'counters/examples': 36224, 'counters/updates': 1132}
train stats after 36256 examples: {'rewards_train/chosen': '0.048096', 'rewards_train/rejected': '0.044998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0030985', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-157.15', 'loss/train': '0.69555', 'examples_per_second': '31.305', 'grad_norm': '21.125', 'counters/examples': 36256, 'counters/updates': 1133}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36320 examples: {'rewards_train/chosen': '0.017505', 'rewards_train/rejected': '-0.0082924', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025798', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-166.69', 'loss/train': '0.68369', 'examples_per_second': '32.412', 'grad_norm': '23.125', 'counters/examples': 36320, 'counters/updates': 1135}
train stats after 36352 examples: {'rewards_train/chosen': '0.030947', 'rewards_train/rejected': '0.030302', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00064571', 'logps_train/rejected': '-90.241', 'logps_train/chosen': '-127.39', 'loss/train': '0.69482', 'examples_per_second': '25.41', 'grad_norm': '18.625', 'counters/examples': 36352, 'counters/updates': 1136}
skipping logging after 36384 examples to avoid logging too frequently
train stats after 36416 examples: {'rewards_train/chosen': '0.047795', 'rewards_train/rejected': '0.023819', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023976', 'logps_train/rejected': '-99.273', 'logps_train/chosen': '-133.79', 'loss/train': '0.68585', 'examples_per_second': '29.914', 'grad_norm': '19.5', 'counters/examples': 36416, 'counters/updates': 1138}
train stats after 36448 examples: {'rewards_train/chosen': '0.066166', 'rewards_train/rejected': '0.0019922', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064174', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-147.51', 'loss/train': '0.66551', 'examples_per_second': '32.492', 'grad_norm': '20.25', 'counters/examples': 36448, 'counters/updates': 1139}
train stats after 36480 examples: {'rewards_train/chosen': '0.072017', 'rewards_train/rejected': '0.035005', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037012', 'logps_train/rejected': '-90.855', 'logps_train/chosen': '-123.59', 'loss/train': '0.67638', 'examples_per_second': '32.079', 'grad_norm': '18', 'counters/examples': 36480, 'counters/updates': 1140}
train stats after 36512 examples: {'rewards_train/chosen': '0.075825', 'rewards_train/rejected': '0.044857', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.030969', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-117.78', 'loss/train': '0.68197', 'examples_per_second': '32.735', 'grad_norm': '17.875', 'counters/examples': 36512, 'counters/updates': 1141}
train stats after 36544 examples: {'rewards_train/chosen': '0.062283', 'rewards_train/rejected': '0.060558', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0017256', 'logps_train/rejected': '-142.01', 'logps_train/chosen': '-164.76', 'loss/train': '0.69675', 'examples_per_second': '30.529', 'grad_norm': '23', 'counters/examples': 36544, 'counters/updates': 1142}
skipping logging after 36576 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '0.025397', 'rewards_train/rejected': '0.012518', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012879', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-114.88', 'loss/train': '0.68905', 'examples_per_second': '29.975', 'grad_norm': '19.625', 'counters/examples': 36608, 'counters/updates': 1144}
train stats after 36640 examples: {'rewards_train/chosen': '0.017524', 'rewards_train/rejected': '0.01758', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-5.611e-05', 'logps_train/rejected': '-137', 'logps_train/chosen': '-114.42', 'loss/train': '0.69716', 'examples_per_second': '31.444', 'grad_norm': '21.875', 'counters/examples': 36640, 'counters/updates': 1145}
train stats after 36672 examples: {'rewards_train/chosen': '0.038175', 'rewards_train/rejected': '0.046196', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0080212', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-157.23', 'loss/train': '0.70086', 'examples_per_second': '31.488', 'grad_norm': '22', 'counters/examples': 36672, 'counters/updates': 1146}
train stats after 36704 examples: {'rewards_train/chosen': '0.028566', 'rewards_train/rejected': '0.0068984', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021668', 'logps_train/rejected': '-70.813', 'logps_train/chosen': '-155.57', 'loss/train': '0.68478', 'examples_per_second': '30.576', 'grad_norm': '19.25', 'counters/examples': 36704, 'counters/updates': 1147}
train stats after 36736 examples: {'rewards_train/chosen': '0.050817', 'rewards_train/rejected': '0.035352', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015465', 'logps_train/rejected': '-114.46', 'logps_train/chosen': '-137.12', 'loss/train': '0.6883', 'examples_per_second': '31.526', 'grad_norm': '19.75', 'counters/examples': 36736, 'counters/updates': 1148}
skipping logging after 36768 examples to avoid logging too frequently
train stats after 36800 examples: {'rewards_train/chosen': '0.063656', 'rewards_train/rejected': '0.035278', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028378', 'logps_train/rejected': '-88.513', 'logps_train/chosen': '-150.04', 'loss/train': '0.68191', 'examples_per_second': '30.907', 'grad_norm': '20.875', 'counters/examples': 36800, 'counters/updates': 1150}
skipping logging after 36832 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '0.053697', 'rewards_train/rejected': '0.06468', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010983', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-158.42', 'loss/train': '0.70072', 'examples_per_second': '30.416', 'grad_norm': '23', 'counters/examples': 36864, 'counters/updates': 1152}
train stats after 36896 examples: {'rewards_train/chosen': '0.062834', 'rewards_train/rejected': '0.034927', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027907', 'logps_train/rejected': '-133.37', 'logps_train/chosen': '-125.54', 'loss/train': '0.68181', 'examples_per_second': '30.807', 'grad_norm': '20.5', 'counters/examples': 36896, 'counters/updates': 1153}
train stats after 36928 examples: {'rewards_train/chosen': '0.065083', 'rewards_train/rejected': '0.05304', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012043', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-137.77', 'loss/train': '0.6919', 'examples_per_second': '31.177', 'grad_norm': '20.625', 'counters/examples': 36928, 'counters/updates': 1154}
train stats after 36960 examples: {'rewards_train/chosen': '0.096862', 'rewards_train/rejected': '0.071559', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025303', 'logps_train/rejected': '-143.01', 'logps_train/chosen': '-107.34', 'loss/train': '0.68306', 'examples_per_second': '31.857', 'grad_norm': '21.75', 'counters/examples': 36960, 'counters/updates': 1155}
train stats after 36992 examples: {'rewards_train/chosen': '0.05852', 'rewards_train/rejected': '0.035311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.023209', 'logps_train/rejected': '-133.89', 'logps_train/chosen': '-151.3', 'loss/train': '0.68612', 'examples_per_second': '31.809', 'grad_norm': '21.25', 'counters/examples': 36992, 'counters/updates': 1156}
train stats after 37024 examples: {'rewards_train/chosen': '0.069104', 'rewards_train/rejected': '0.068145', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00095905', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-163.26', 'loss/train': '0.69695', 'examples_per_second': '31.795', 'grad_norm': '24.875', 'counters/examples': 37024, 'counters/updates': 1157}
train stats after 37056 examples: {'rewards_train/chosen': '0.045097', 'rewards_train/rejected': '0.035315', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0097819', 'logps_train/rejected': '-121.29', 'logps_train/chosen': '-135.63', 'loss/train': '0.69228', 'examples_per_second': '31.558', 'grad_norm': '21.375', 'counters/examples': 37056, 'counters/updates': 1158}
skipping logging after 37088 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '0.071913', 'rewards_train/rejected': '0.00035579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071557', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-155.48', 'loss/train': '0.661', 'examples_per_second': '36.073', 'grad_norm': '20.125', 'counters/examples': 37120, 'counters/updates': 1160}
skipping logging after 37152 examples to avoid logging too frequently
train stats after 37184 examples: {'rewards_train/chosen': '0.072774', 'rewards_train/rejected': '0.040454', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03232', 'logps_train/rejected': '-96.088', 'logps_train/chosen': '-167.6', 'loss/train': '0.67994', 'examples_per_second': '31.198', 'grad_norm': '21', 'counters/examples': 37184, 'counters/updates': 1162}
train stats after 37216 examples: {'rewards_train/chosen': '-0.011257', 'rewards_train/rejected': '0.0034663', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.014724', 'logps_train/rejected': '-80.908', 'logps_train/chosen': '-102.82', 'loss/train': '0.70292', 'examples_per_second': '31.503', 'grad_norm': '18.625', 'counters/examples': 37216, 'counters/updates': 1163}
skipping logging after 37248 examples to avoid logging too frequently
train stats after 37280 examples: {'rewards_train/chosen': '0.036232', 'rewards_train/rejected': '0.019421', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016811', 'logps_train/rejected': '-90.187', 'logps_train/chosen': '-161.65', 'loss/train': '0.68726', 'examples_per_second': '30.778', 'grad_norm': '19.875', 'counters/examples': 37280, 'counters/updates': 1165}
train stats after 37312 examples: {'rewards_train/chosen': '0.075976', 'rewards_train/rejected': '0.055235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020741', 'logps_train/rejected': '-167.52', 'logps_train/chosen': '-163.23', 'loss/train': '0.68662', 'examples_per_second': '31.559', 'grad_norm': '22.875', 'counters/examples': 37312, 'counters/updates': 1166}
skipping logging after 37344 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '0.069857', 'rewards_train/rejected': '-0.00069903', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070556', 'logps_train/rejected': '-101.55', 'logps_train/chosen': '-148.52', 'loss/train': '0.6628', 'examples_per_second': '30.973', 'grad_norm': '19', 'counters/examples': 37376, 'counters/updates': 1168}
train stats after 37408 examples: {'rewards_train/chosen': '0.035535', 'rewards_train/rejected': '0.029236', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0062989', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-157.99', 'loss/train': '0.69376', 'examples_per_second': '30.014', 'grad_norm': '20.75', 'counters/examples': 37408, 'counters/updates': 1169}
train stats after 37440 examples: {'rewards_train/chosen': '0.083208', 'rewards_train/rejected': '0.03884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044368', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-166.43', 'loss/train': '0.67488', 'examples_per_second': '31.289', 'grad_norm': '21.625', 'counters/examples': 37440, 'counters/updates': 1170}
train stats after 37472 examples: {'rewards_train/chosen': '0.055162', 'rewards_train/rejected': '0.046577', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0085853', 'logps_train/rejected': '-136.51', 'logps_train/chosen': '-135.52', 'loss/train': '0.69307', 'examples_per_second': '30.399', 'grad_norm': '21.75', 'counters/examples': 37472, 'counters/updates': 1171}
skipping logging after 37504 examples to avoid logging too frequently
train stats after 37536 examples: {'rewards_train/chosen': '0.025194', 'rewards_train/rejected': '0.05024', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025046', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-139.99', 'loss/train': '0.71002', 'examples_per_second': '32.439', 'grad_norm': '23.625', 'counters/examples': 37536, 'counters/updates': 1173}
train stats after 37568 examples: {'rewards_train/chosen': '0.069817', 'rewards_train/rejected': '0.064339', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0054787', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-103.7', 'loss/train': '0.69468', 'examples_per_second': '30.407', 'grad_norm': '19.5', 'counters/examples': 37568, 'counters/updates': 1174}
train stats after 37600 examples: {'rewards_train/chosen': '0.074193', 'rewards_train/rejected': '0.039721', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034472', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-170.3', 'loss/train': '0.67991', 'examples_per_second': '30.245', 'grad_norm': '21.75', 'counters/examples': 37600, 'counters/updates': 1175}
train stats after 37632 examples: {'rewards_train/chosen': '0.018973', 'rewards_train/rejected': '0.024269', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0052967', 'logps_train/rejected': '-107.26', 'logps_train/chosen': '-110.78', 'loss/train': '0.69917', 'examples_per_second': '30.465', 'grad_norm': '19', 'counters/examples': 37632, 'counters/updates': 1176}
skipping logging after 37664 examples to avoid logging too frequently
train stats after 37696 examples: {'rewards_train/chosen': '0.031994', 'rewards_train/rejected': '0.0047812', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.027212', 'logps_train/rejected': '-116.89', 'logps_train/chosen': '-161.99', 'loss/train': '0.68544', 'examples_per_second': '34.24', 'grad_norm': '21.125', 'counters/examples': 37696, 'counters/updates': 1178}
skipping logging after 37728 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '0.035311', 'rewards_train/rejected': '0.010299', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025012', 'logps_train/rejected': '-106.97', 'logps_train/chosen': '-181.36', 'loss/train': '0.68424', 'examples_per_second': '31.618', 'grad_norm': '22.25', 'counters/examples': 37760, 'counters/updates': 1180}
train stats after 37792 examples: {'rewards_train/chosen': '0.019103', 'rewards_train/rejected': '0.011691', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.007412', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-116.08', 'loss/train': '0.69233', 'examples_per_second': '31.595', 'grad_norm': '22.125', 'counters/examples': 37792, 'counters/updates': 1181}
train stats after 37824 examples: {'rewards_train/chosen': '0.074161', 'rewards_train/rejected': '-0.012971', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087132', 'logps_train/rejected': '-158.02', 'logps_train/chosen': '-159.57', 'loss/train': '0.65684', 'examples_per_second': '31.638', 'grad_norm': '22', 'counters/examples': 37824, 'counters/updates': 1182}
train stats after 37856 examples: {'rewards_train/chosen': '0.10564', 'rewards_train/rejected': '0.029974', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.075669', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-178.95', 'loss/train': '0.65859', 'examples_per_second': '31.6', 'grad_norm': '21.875', 'counters/examples': 37856, 'counters/updates': 1183}
skipping logging after 37888 examples to avoid logging too frequently
train stats after 37920 examples: {'rewards_train/chosen': '0.075011', 'rewards_train/rejected': '0.024377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050633', 'logps_train/rejected': '-143.05', 'logps_train/chosen': '-139.15', 'loss/train': '0.67452', 'examples_per_second': '32.205', 'grad_norm': '20.125', 'counters/examples': 37920, 'counters/updates': 1185}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 37984 examples: {'rewards_train/chosen': '0.012022', 'rewards_train/rejected': '0.028319', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016297', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-139.82', 'loss/train': '0.70379', 'examples_per_second': '31.327', 'grad_norm': '21.875', 'counters/examples': 37984, 'counters/updates': 1187}
train stats after 38016 examples: {'rewards_train/chosen': '0.074396', 'rewards_train/rejected': '0.0089342', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065462', 'logps_train/rejected': '-116.07', 'logps_train/chosen': '-166.67', 'loss/train': '0.66467', 'examples_per_second': '31.888', 'grad_norm': '21.125', 'counters/examples': 38016, 'counters/updates': 1188}
train stats after 38048 examples: {'rewards_train/chosen': '0.054994', 'rewards_train/rejected': '0.040582', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014412', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-124.57', 'loss/train': '0.69144', 'examples_per_second': '31.623', 'grad_norm': '20.625', 'counters/examples': 38048, 'counters/updates': 1189}
train stats after 38080 examples: {'rewards_train/chosen': '0.1011', 'rewards_train/rejected': '0.042825', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058276', 'logps_train/rejected': '-85.017', 'logps_train/chosen': '-150.57', 'loss/train': '0.66727', 'examples_per_second': '30.223', 'grad_norm': '19.75', 'counters/examples': 38080, 'counters/updates': 1190}
train stats after 38112 examples: {'rewards_train/chosen': '0.044468', 'rewards_train/rejected': '0.017188', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02728', 'logps_train/rejected': '-129.8', 'logps_train/chosen': '-142.88', 'loss/train': '0.68261', 'examples_per_second': '32.145', 'grad_norm': '20.25', 'counters/examples': 38112, 'counters/updates': 1191}
train stats after 38144 examples: {'rewards_train/chosen': '0.031908', 'rewards_train/rejected': '0.046579', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014671', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-118.72', 'loss/train': '0.7034', 'examples_per_second': '31.681', 'grad_norm': '20.375', 'counters/examples': 38144, 'counters/updates': 1192}
train stats after 38176 examples: {'rewards_train/chosen': '0.038325', 'rewards_train/rejected': '0.013016', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025309', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-119.36', 'loss/train': '0.68322', 'examples_per_second': '31.956', 'grad_norm': '18.375', 'counters/examples': 38176, 'counters/updates': 1193}
train stats after 38208 examples: {'rewards_train/chosen': '0.065756', 'rewards_train/rejected': '0.029964', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035792', 'logps_train/rejected': '-101.54', 'logps_train/chosen': '-169.95', 'loss/train': '0.68055', 'examples_per_second': '31.885', 'grad_norm': '20.625', 'counters/examples': 38208, 'counters/updates': 1194}
train stats after 38240 examples: {'rewards_train/chosen': '0.074949', 'rewards_train/rejected': '0.051019', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02393', 'logps_train/rejected': '-103.6', 'logps_train/chosen': '-130.79', 'loss/train': '0.68607', 'examples_per_second': '30.317', 'grad_norm': '18.875', 'counters/examples': 38240, 'counters/updates': 1195}
train stats after 38272 examples: {'rewards_train/chosen': '0.04178', 'rewards_train/rejected': '0.07861', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.03683', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-150.55', 'loss/train': '0.71502', 'examples_per_second': '30.668', 'grad_norm': '21.75', 'counters/examples': 38272, 'counters/updates': 1196}
train stats after 38304 examples: {'rewards_train/chosen': '0.068518', 'rewards_train/rejected': '0.035109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033409', 'logps_train/rejected': '-155.42', 'logps_train/chosen': '-148.15', 'loss/train': '0.68388', 'examples_per_second': '31.576', 'grad_norm': '21.375', 'counters/examples': 38304, 'counters/updates': 1197}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38368 examples: {'rewards_train/chosen': '0.067925', 'rewards_train/rejected': '0.042998', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024927', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-152.36', 'loss/train': '0.68569', 'examples_per_second': '31.67', 'grad_norm': '22.25', 'counters/examples': 38368, 'counters/updates': 1199}
train stats after 38400 examples: {'rewards_train/chosen': '0.05361', 'rewards_train/rejected': '0.011324', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042286', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-138.68', 'loss/train': '0.67594', 'examples_per_second': '32.085', 'grad_norm': '19.75', 'counters/examples': 38400, 'counters/updates': 1200}
train stats after 38432 examples: {'rewards_train/chosen': '0.067927', 'rewards_train/rejected': '0.028284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039643', 'logps_train/rejected': '-138.26', 'logps_train/chosen': '-150.08', 'loss/train': '0.67735', 'examples_per_second': '31.609', 'grad_norm': '20.875', 'counters/examples': 38432, 'counters/updates': 1201}
train stats after 38464 examples: {'rewards_train/chosen': '0.062889', 'rewards_train/rejected': '0.036151', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026738', 'logps_train/rejected': '-97.083', 'logps_train/chosen': '-166.99', 'loss/train': '0.68426', 'examples_per_second': '31.653', 'grad_norm': '21.125', 'counters/examples': 38464, 'counters/updates': 1202}
train stats after 38496 examples: {'rewards_train/chosen': '0.092805', 'rewards_train/rejected': '0.029022', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063782', 'logps_train/rejected': '-156.62', 'logps_train/chosen': '-159.84', 'loss/train': '0.66821', 'examples_per_second': '30.665', 'grad_norm': '22.5', 'counters/examples': 38496, 'counters/updates': 1203}
train stats after 38528 examples: {'rewards_train/chosen': '0.065821', 'rewards_train/rejected': '0.030405', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035416', 'logps_train/rejected': '-101.21', 'logps_train/chosen': '-130.73', 'loss/train': '0.67769', 'examples_per_second': '31.102', 'grad_norm': '19.375', 'counters/examples': 38528, 'counters/updates': 1204}
train stats after 38560 examples: {'rewards_train/chosen': '0.065816', 'rewards_train/rejected': '0.026179', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039638', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-109.24', 'loss/train': '0.67605', 'examples_per_second': '31.145', 'grad_norm': '18.375', 'counters/examples': 38560, 'counters/updates': 1205}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38624 examples: {'rewards_train/chosen': '0.10133', 'rewards_train/rejected': '0.071281', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.030048', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-134.32', 'loss/train': '0.68079', 'examples_per_second': '30.325', 'grad_norm': '20.625', 'counters/examples': 38624, 'counters/updates': 1207}
train stats after 38656 examples: {'rewards_train/chosen': '0.048945', 'rewards_train/rejected': '0.035254', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013691', 'logps_train/rejected': '-98.927', 'logps_train/chosen': '-150.62', 'loss/train': '0.69126', 'examples_per_second': '31.897', 'grad_norm': '20.5', 'counters/examples': 38656, 'counters/updates': 1208}
train stats after 38688 examples: {'rewards_train/chosen': '0.056843', 'rewards_train/rejected': '0.070571', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013728', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-91.959', 'loss/train': '0.7051', 'examples_per_second': '32.967', 'grad_norm': '18.625', 'counters/examples': 38688, 'counters/updates': 1209}
train stats after 38720 examples: {'rewards_train/chosen': '0.053919', 'rewards_train/rejected': '0.027345', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026574', 'logps_train/rejected': '-138.84', 'logps_train/chosen': '-155.43', 'loss/train': '0.68462', 'examples_per_second': '30.816', 'grad_norm': '22.875', 'counters/examples': 38720, 'counters/updates': 1210}
train stats after 38752 examples: {'rewards_train/chosen': '0.022914', 'rewards_train/rejected': '-0.044829', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067743', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-149.14', 'loss/train': '0.66339', 'examples_per_second': '30.435', 'grad_norm': '20.5', 'counters/examples': 38752, 'counters/updates': 1211}
train stats after 38784 examples: {'rewards_train/chosen': '0.056386', 'rewards_train/rejected': '0.026199', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030187', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-147.27', 'loss/train': '0.68111', 'examples_per_second': '30.164', 'grad_norm': '21.875', 'counters/examples': 38784, 'counters/updates': 1212}
train stats after 38816 examples: {'rewards_train/chosen': '0.055311', 'rewards_train/rejected': '0.0099394', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045372', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-167.14', 'loss/train': '0.6735', 'examples_per_second': '31.592', 'grad_norm': '21.375', 'counters/examples': 38816, 'counters/updates': 1213}
train stats after 38848 examples: {'rewards_train/chosen': '0.091801', 'rewards_train/rejected': '0.031091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06071', 'logps_train/rejected': '-147.3', 'logps_train/chosen': '-165.76', 'loss/train': '0.6691', 'examples_per_second': '30.653', 'grad_norm': '23.875', 'counters/examples': 38848, 'counters/updates': 1214}
train stats after 38880 examples: {'rewards_train/chosen': '0.11606', 'rewards_train/rejected': '0.043127', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072928', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-154.4', 'loss/train': '0.66301', 'examples_per_second': '31.68', 'grad_norm': '20.5', 'counters/examples': 38880, 'counters/updates': 1215}
train stats after 38912 examples: {'rewards_train/chosen': '0.052536', 'rewards_train/rejected': '-0.018159', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.070695', 'logps_train/rejected': '-111.49', 'logps_train/chosen': '-118.17', 'loss/train': '0.66178', 'examples_per_second': '31.484', 'grad_norm': '18.75', 'counters/examples': 38912, 'counters/updates': 1216}
train stats after 38944 examples: {'rewards_train/chosen': '0.065141', 'rewards_train/rejected': '0.031246', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033896', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-153.12', 'loss/train': '0.6798', 'examples_per_second': '30.898', 'grad_norm': '20.25', 'counters/examples': 38944, 'counters/updates': 1217}
train stats after 38976 examples: {'rewards_train/chosen': '0.10733', 'rewards_train/rejected': '0.054767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052562', 'logps_train/rejected': '-122.79', 'logps_train/chosen': '-175.9', 'loss/train': '0.67195', 'examples_per_second': '31.514', 'grad_norm': '22.25', 'counters/examples': 38976, 'counters/updates': 1218}
train stats after 39008 examples: {'rewards_train/chosen': '0.056784', 'rewards_train/rejected': '0.04247', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014314', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-132.8', 'loss/train': '0.68916', 'examples_per_second': '32.725', 'grad_norm': '21', 'counters/examples': 39008, 'counters/updates': 1219}
train stats after 39040 examples: {'rewards_train/chosen': '0.03639', 'rewards_train/rejected': '0.014131', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022259', 'logps_train/rejected': '-112.3', 'logps_train/chosen': '-124.29', 'loss/train': '0.68551', 'examples_per_second': '31.636', 'grad_norm': '20.25', 'counters/examples': 39040, 'counters/updates': 1220}
skipping logging after 39072 examples to avoid logging too frequently
train stats after 39104 examples: {'rewards_train/chosen': '0.054373', 'rewards_train/rejected': '-0.0032581', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057631', 'logps_train/rejected': '-90.152', 'logps_train/chosen': '-195.71', 'loss/train': '0.66815', 'examples_per_second': '32.703', 'grad_norm': '22.125', 'counters/examples': 39104, 'counters/updates': 1222}
train stats after 39136 examples: {'rewards_train/chosen': '0.030102', 'rewards_train/rejected': '0.02701', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0030913', 'logps_train/rejected': '-120.63', 'logps_train/chosen': '-149.26', 'loss/train': '0.69528', 'examples_per_second': '31.147', 'grad_norm': '21', 'counters/examples': 39136, 'counters/updates': 1223}
skipping logging after 39168 examples to avoid logging too frequently
train stats after 39200 examples: {'rewards_train/chosen': '0.05245', 'rewards_train/rejected': '0.061552', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0091027', 'logps_train/rejected': '-137.15', 'logps_train/chosen': '-130.97', 'loss/train': '0.7006', 'examples_per_second': '36.083', 'grad_norm': '21.25', 'counters/examples': 39200, 'counters/updates': 1225}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39264 examples: {'rewards_train/chosen': '0.065747', 'rewards_train/rejected': '0.011741', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054005', 'logps_train/rejected': '-111.14', 'logps_train/chosen': '-135.18', 'loss/train': '0.67135', 'examples_per_second': '31.861', 'grad_norm': '19.375', 'counters/examples': 39264, 'counters/updates': 1227}
skipping logging after 39296 examples to avoid logging too frequently
train stats after 39328 examples: {'rewards_train/chosen': '0.079458', 'rewards_train/rejected': '0.013575', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.065883', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-143.97', 'loss/train': '0.66347', 'examples_per_second': '32.233', 'grad_norm': '19.875', 'counters/examples': 39328, 'counters/updates': 1229}
train stats after 39360 examples: {'rewards_train/chosen': '0.05879', 'rewards_train/rejected': '0.019', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039791', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-124.06', 'loss/train': '0.67547', 'examples_per_second': '32.967', 'grad_norm': '19.375', 'counters/examples': 39360, 'counters/updates': 1230}
skipping logging after 39392 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '0.04293', 'rewards_train/rejected': '0.038147', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0047837', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-148.3', 'loss/train': '0.69327', 'examples_per_second': '31.606', 'grad_norm': '21.125', 'counters/examples': 39424, 'counters/updates': 1232}
train stats after 39456 examples: {'rewards_train/chosen': '0.10139', 'rewards_train/rejected': '0.036072', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065322', 'logps_train/rejected': '-86.173', 'logps_train/chosen': '-133.77', 'loss/train': '0.66563', 'examples_per_second': '31.627', 'grad_norm': '17.75', 'counters/examples': 39456, 'counters/updates': 1233}
train stats after 39488 examples: {'rewards_train/chosen': '0.066653', 'rewards_train/rejected': '0.047853', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0188', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-108.64', 'loss/train': '0.687', 'examples_per_second': '30.103', 'grad_norm': '20.25', 'counters/examples': 39488, 'counters/updates': 1234}
train stats after 39520 examples: {'rewards_train/chosen': '0.08666', 'rewards_train/rejected': '0.033811', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.052849', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-141.46', 'loss/train': '0.67208', 'examples_per_second': '30.132', 'grad_norm': '22.75', 'counters/examples': 39520, 'counters/updates': 1235}
train stats after 39552 examples: {'rewards_train/chosen': '0.041864', 'rewards_train/rejected': '0.018448', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023416', 'logps_train/rejected': '-112.83', 'logps_train/chosen': '-105.2', 'loss/train': '0.68438', 'examples_per_second': '30.974', 'grad_norm': '17.875', 'counters/examples': 39552, 'counters/updates': 1236}
train stats after 39584 examples: {'rewards_train/chosen': '0.044644', 'rewards_train/rejected': '-0.0010969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045741', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-130.06', 'loss/train': '0.67321', 'examples_per_second': '31.53', 'grad_norm': '20.125', 'counters/examples': 39584, 'counters/updates': 1237}
train stats after 39616 examples: {'rewards_train/chosen': '0.078909', 'rewards_train/rejected': '0.06644', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012468', 'logps_train/rejected': '-119.93', 'logps_train/chosen': '-135.64', 'loss/train': '0.69314', 'examples_per_second': '33.164', 'grad_norm': '20.125', 'counters/examples': 39616, 'counters/updates': 1238}
train stats after 39648 examples: {'rewards_train/chosen': '0.087283', 'rewards_train/rejected': '0.035962', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051321', 'logps_train/rejected': '-107.88', 'logps_train/chosen': '-133.31', 'loss/train': '0.67255', 'examples_per_second': '31.678', 'grad_norm': '19', 'counters/examples': 39648, 'counters/updates': 1239}
train stats after 39680 examples: {'rewards_train/chosen': '0.070217', 'rewards_train/rejected': '-0.0018023', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072019', 'logps_train/rejected': '-111.92', 'logps_train/chosen': '-147.67', 'loss/train': '0.66059', 'examples_per_second': '32.505', 'grad_norm': '19.375', 'counters/examples': 39680, 'counters/updates': 1240}
train stats after 39712 examples: {'rewards_train/chosen': '0.027754', 'rewards_train/rejected': '0.038653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.010899', 'logps_train/rejected': '-131.43', 'logps_train/chosen': '-150.64', 'loss/train': '0.70122', 'examples_per_second': '32.438', 'grad_norm': '22.625', 'counters/examples': 39712, 'counters/updates': 1241}
train stats after 39744 examples: {'rewards_train/chosen': '0.10463', 'rewards_train/rejected': '0.019221', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085413', 'logps_train/rejected': '-105.04', 'logps_train/chosen': '-163.79', 'loss/train': '0.65439', 'examples_per_second': '31.873', 'grad_norm': '19.25', 'counters/examples': 39744, 'counters/updates': 1242}
train stats after 39776 examples: {'rewards_train/chosen': '0.027379', 'rewards_train/rejected': '0.052996', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.025618', 'logps_train/rejected': '-146.6', 'logps_train/chosen': '-149.73', 'loss/train': '0.71198', 'examples_per_second': '32.209', 'grad_norm': '22.875', 'counters/examples': 39776, 'counters/updates': 1243}
train stats after 39808 examples: {'rewards_train/chosen': '0.087532', 'rewards_train/rejected': '0.072449', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015083', 'logps_train/rejected': '-97.146', 'logps_train/chosen': '-121.59', 'loss/train': '0.69006', 'examples_per_second': '32.561', 'grad_norm': '18.875', 'counters/examples': 39808, 'counters/updates': 1244}
train stats after 39840 examples: {'rewards_train/chosen': '0.093597', 'rewards_train/rejected': '0.065881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027716', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-112.36', 'loss/train': '0.68188', 'examples_per_second': '31.791', 'grad_norm': '19.375', 'counters/examples': 39840, 'counters/updates': 1245}
train stats after 39872 examples: {'rewards_train/chosen': '0.055246', 'rewards_train/rejected': '0.035792', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019454', 'logps_train/rejected': '-103.49', 'logps_train/chosen': '-109.79', 'loss/train': '0.68611', 'examples_per_second': '30.389', 'grad_norm': '17.75', 'counters/examples': 39872, 'counters/updates': 1246}
train stats after 39904 examples: {'rewards_train/chosen': '0.096588', 'rewards_train/rejected': '0.020148', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076439', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-169.76', 'loss/train': '0.66184', 'examples_per_second': '31.785', 'grad_norm': '21.625', 'counters/examples': 39904, 'counters/updates': 1247}
train stats after 39936 examples: {'rewards_train/chosen': '0.046821', 'rewards_train/rejected': '0.038007', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0088145', 'logps_train/rejected': '-92.268', 'logps_train/chosen': '-141.04', 'loss/train': '0.69036', 'examples_per_second': '30.359', 'grad_norm': '19', 'counters/examples': 39936, 'counters/updates': 1248}
train stats after 39968 examples: {'rewards_train/chosen': '0.031156', 'rewards_train/rejected': '0.036241', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.005085', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-108.39', 'loss/train': '0.69892', 'examples_per_second': '30.737', 'grad_norm': '19.75', 'counters/examples': 39968, 'counters/updates': 1249}
train stats after 40000 examples: {'rewards_train/chosen': '0.083764', 'rewards_train/rejected': '0.053012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030752', 'logps_train/rejected': '-139.26', 'logps_train/chosen': '-122.78', 'loss/train': '0.68126', 'examples_per_second': '30.567', 'grad_norm': '20.25', 'counters/examples': 40000, 'counters/updates': 1250}
train stats after 40032 examples: {'rewards_train/chosen': '0.06562', 'rewards_train/rejected': '-0.020961', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.086581', 'logps_train/rejected': '-107.6', 'logps_train/chosen': '-144.64', 'loss/train': '0.65451', 'examples_per_second': '32.77', 'grad_norm': '20', 'counters/examples': 40032, 'counters/updates': 1251}
train stats after 40064 examples: {'rewards_train/chosen': '0.10127', 'rewards_train/rejected': '0.076776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024497', 'logps_train/rejected': '-104.35', 'logps_train/chosen': '-142.64', 'loss/train': '0.68458', 'examples_per_second': '30.309', 'grad_norm': '20', 'counters/examples': 40064, 'counters/updates': 1252}
skipping logging after 40096 examples to avoid logging too frequently
train stats after 40128 examples: {'rewards_train/chosen': '0.067732', 'rewards_train/rejected': '0.036468', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031264', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-155.07', 'loss/train': '0.68149', 'examples_per_second': '31.611', 'grad_norm': '21.625', 'counters/examples': 40128, 'counters/updates': 1254}
skipping logging after 40160 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '0.060717', 'rewards_train/rejected': '0.039259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.021458', 'logps_train/rejected': '-111.3', 'logps_train/chosen': '-153.06', 'loss/train': '0.68702', 'examples_per_second': '32.579', 'grad_norm': '21', 'counters/examples': 40192, 'counters/updates': 1256}
train stats after 40224 examples: {'rewards_train/chosen': '0.05581', 'rewards_train/rejected': '0.042913', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012898', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-148.88', 'loss/train': '0.69089', 'examples_per_second': '31.654', 'grad_norm': '22', 'counters/examples': 40224, 'counters/updates': 1257}
train stats after 40256 examples: {'rewards_train/chosen': '0.10113', 'rewards_train/rejected': '0.028658', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.072477', 'logps_train/rejected': '-132.19', 'logps_train/chosen': '-156.13', 'loss/train': '0.66106', 'examples_per_second': '30.776', 'grad_norm': '22.125', 'counters/examples': 40256, 'counters/updates': 1258}
train stats after 40288 examples: {'rewards_train/chosen': '0.062941', 'rewards_train/rejected': '0.020156', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042785', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-134.55', 'loss/train': '0.67617', 'examples_per_second': '31.66', 'grad_norm': '21.25', 'counters/examples': 40288, 'counters/updates': 1259}
train stats after 40320 examples: {'rewards_train/chosen': '0.070084', 'rewards_train/rejected': '0.053484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0166', 'logps_train/rejected': '-107.16', 'logps_train/chosen': '-127.86', 'loss/train': '0.6882', 'examples_per_second': '32.002', 'grad_norm': '19.375', 'counters/examples': 40320, 'counters/updates': 1260}
skipping logging after 40352 examples to avoid logging too frequently
train stats after 40384 examples: {'rewards_train/chosen': '0.034369', 'rewards_train/rejected': '0.00937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024999', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-124.38', 'loss/train': '0.68331', 'examples_per_second': '32.816', 'grad_norm': '18.875', 'counters/examples': 40384, 'counters/updates': 1262}
train stats after 40416 examples: {'rewards_train/chosen': '0.040985', 'rewards_train/rejected': '0.05855', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017565', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-124.57', 'loss/train': '0.7074', 'examples_per_second': '30.729', 'grad_norm': '21.875', 'counters/examples': 40416, 'counters/updates': 1263}
train stats after 40448 examples: {'rewards_train/chosen': '0.062105', 'rewards_train/rejected': '0.042524', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.019581', 'logps_train/rejected': '-160.1', 'logps_train/chosen': '-131.47', 'loss/train': '0.68959', 'examples_per_second': '32.467', 'grad_norm': '22.5', 'counters/examples': 40448, 'counters/updates': 1264}
train stats after 40480 examples: {'rewards_train/chosen': '0.081021', 'rewards_train/rejected': '0.054227', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026794', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-134.76', 'loss/train': '0.68348', 'examples_per_second': '31.717', 'grad_norm': '20.375', 'counters/examples': 40480, 'counters/updates': 1265}
train stats after 40512 examples: {'rewards_train/chosen': '0.078009', 'rewards_train/rejected': '0.10526', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.02725', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-131.32', 'loss/train': '0.7112', 'examples_per_second': '32.383', 'grad_norm': '20', 'counters/examples': 40512, 'counters/updates': 1266}
train stats after 40544 examples: {'rewards_train/chosen': '0.019203', 'rewards_train/rejected': '0.0067413', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012462', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-144.26', 'loss/train': '0.68906', 'examples_per_second': '31.702', 'grad_norm': '21', 'counters/examples': 40544, 'counters/updates': 1267}
train stats after 40576 examples: {'rewards_train/chosen': '0.074503', 'rewards_train/rejected': '0.018847', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055656', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-139.19', 'loss/train': '0.6703', 'examples_per_second': '30.358', 'grad_norm': '20', 'counters/examples': 40576, 'counters/updates': 1268}
train stats after 40608 examples: {'rewards_train/chosen': '0.04466', 'rewards_train/rejected': '0.073697', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.029037', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-171.89', 'loss/train': '0.71131', 'examples_per_second': '30.161', 'grad_norm': '23', 'counters/examples': 40608, 'counters/updates': 1269}
train stats after 40640 examples: {'rewards_train/chosen': '0.052834', 'rewards_train/rejected': '0.033475', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019359', 'logps_train/rejected': '-95.234', 'logps_train/chosen': '-106.25', 'loss/train': '0.68934', 'examples_per_second': '32.128', 'grad_norm': '16.625', 'counters/examples': 40640, 'counters/updates': 1270}
train stats after 40672 examples: {'rewards_train/chosen': '0.10958', 'rewards_train/rejected': '0.069725', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039859', 'logps_train/rejected': '-143.74', 'logps_train/chosen': '-126.7', 'loss/train': '0.67599', 'examples_per_second': '31.661', 'grad_norm': '20.75', 'counters/examples': 40672, 'counters/updates': 1271}
train stats after 40704 examples: {'rewards_train/chosen': '0.027842', 'rewards_train/rejected': '0.076671', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048829', 'logps_train/rejected': '-93.443', 'logps_train/chosen': '-126.29', 'loss/train': '0.7198', 'examples_per_second': '32.903', 'grad_norm': '19.875', 'counters/examples': 40704, 'counters/updates': 1272}
skipping logging after 40736 examples to avoid logging too frequently
train stats after 40768 examples: {'rewards_train/chosen': '0.038702', 'rewards_train/rejected': '0.014098', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024603', 'logps_train/rejected': '-118.78', 'logps_train/chosen': '-151.01', 'loss/train': '0.68748', 'examples_per_second': '33.347', 'grad_norm': '20.625', 'counters/examples': 40768, 'counters/updates': 1274}
skipping logging after 40800 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '0.021302', 'rewards_train/rejected': '0.02676', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0054585', 'logps_train/rejected': '-84.715', 'logps_train/chosen': '-115.72', 'loss/train': '0.69804', 'examples_per_second': '35.453', 'grad_norm': '18', 'counters/examples': 40832, 'counters/updates': 1276}
skipping logging after 40864 examples to avoid logging too frequently
train stats after 40896 examples: {'rewards_train/chosen': '0.029416', 'rewards_train/rejected': '0.027113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0023037', 'logps_train/rejected': '-135.68', 'logps_train/chosen': '-153.53', 'loss/train': '0.69905', 'examples_per_second': '31.651', 'grad_norm': '22.625', 'counters/examples': 40896, 'counters/updates': 1278}
train stats after 40928 examples: {'rewards_train/chosen': '0.050801', 'rewards_train/rejected': '0.048415', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0023864', 'logps_train/rejected': '-134.73', 'logps_train/chosen': '-118', 'loss/train': '0.69675', 'examples_per_second': '31.989', 'grad_norm': '21.25', 'counters/examples': 40928, 'counters/updates': 1279}
train stats after 40960 examples: {'rewards_train/chosen': '0.056947', 'rewards_train/rejected': '0.030824', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026122', 'logps_train/rejected': '-116.4', 'logps_train/chosen': '-116.74', 'loss/train': '0.6833', 'examples_per_second': '24.395', 'grad_norm': '19.5', 'counters/examples': 40960, 'counters/updates': 1280}
train stats after 40992 examples: {'rewards_train/chosen': '0.062777', 'rewards_train/rejected': '-0.005167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067944', 'logps_train/rejected': '-97.374', 'logps_train/chosen': '-116.78', 'loss/train': '0.66148', 'examples_per_second': '31.634', 'grad_norm': '17.125', 'counters/examples': 40992, 'counters/updates': 1281}
train stats after 41024 examples: {'rewards_train/chosen': '0.043326', 'rewards_train/rejected': '0.054851', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011525', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-132.33', 'loss/train': '0.70381', 'examples_per_second': '31.518', 'grad_norm': '20.125', 'counters/examples': 41024, 'counters/updates': 1282}
train stats after 41056 examples: {'rewards_train/chosen': '0.080116', 'rewards_train/rejected': '0.026918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053198', 'logps_train/rejected': '-131', 'logps_train/chosen': '-170.07', 'loss/train': '0.67263', 'examples_per_second': '24.518', 'grad_norm': '21.75', 'counters/examples': 41056, 'counters/updates': 1283}
train stats after 41088 examples: {'rewards_train/chosen': '0.017922', 'rewards_train/rejected': '0.038606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020683', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-137.49', 'loss/train': '0.70837', 'examples_per_second': '30.561', 'grad_norm': '22.375', 'counters/examples': 41088, 'counters/updates': 1284}
skipping logging after 41120 examples to avoid logging too frequently
train stats after 41152 examples: {'rewards_train/chosen': '0.068385', 'rewards_train/rejected': '0.041027', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027359', 'logps_train/rejected': '-98.701', 'logps_train/chosen': '-135.59', 'loss/train': '0.68251', 'examples_per_second': '34.835', 'grad_norm': '18.875', 'counters/examples': 41152, 'counters/updates': 1286}
skipping logging after 41184 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '0.051227', 'rewards_train/rejected': '0.07072', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019494', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-117.54', 'loss/train': '0.70619', 'examples_per_second': '37.107', 'grad_norm': '20.375', 'counters/examples': 41216, 'counters/updates': 1288}
skipping logging after 41248 examples to avoid logging too frequently
train stats after 41280 examples: {'rewards_train/chosen': '0.018161', 'rewards_train/rejected': '0.008259', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0099016', 'logps_train/rejected': '-113.75', 'logps_train/chosen': '-137.03', 'loss/train': '0.69259', 'examples_per_second': '33.253', 'grad_norm': '19.625', 'counters/examples': 41280, 'counters/updates': 1290}
train stats after 41312 examples: {'rewards_train/chosen': '0.1303', 'rewards_train/rejected': '0.065988', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064315', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-138.88', 'loss/train': '0.6658', 'examples_per_second': '32.541', 'grad_norm': '19.625', 'counters/examples': 41312, 'counters/updates': 1291}
skipping logging after 41344 examples to avoid logging too frequently
train stats after 41376 examples: {'rewards_train/chosen': '0.049984', 'rewards_train/rejected': '0.050793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0008089', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-102.51', 'loss/train': '0.69653', 'examples_per_second': '31.155', 'grad_norm': '19.5', 'counters/examples': 41376, 'counters/updates': 1293}
train stats after 41408 examples: {'rewards_train/chosen': '0.081661', 'rewards_train/rejected': '-0.002027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083688', 'logps_train/rejected': '-107.05', 'logps_train/chosen': '-124.69', 'loss/train': '0.65632', 'examples_per_second': '32.173', 'grad_norm': '18.625', 'counters/examples': 41408, 'counters/updates': 1294}
train stats after 41440 examples: {'rewards_train/chosen': '0.041055', 'rewards_train/rejected': '0.013441', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027614', 'logps_train/rejected': '-121.54', 'logps_train/chosen': '-132.09', 'loss/train': '0.68474', 'examples_per_second': '31.63', 'grad_norm': '19.625', 'counters/examples': 41440, 'counters/updates': 1295}
train stats after 41472 examples: {'rewards_train/chosen': '0.076029', 'rewards_train/rejected': '0.065553', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010475', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-170.56', 'loss/train': '0.69143', 'examples_per_second': '30.905', 'grad_norm': '22.5', 'counters/examples': 41472, 'counters/updates': 1296}
skipping logging after 41504 examples to avoid logging too frequently
train stats after 41536 examples: {'rewards_train/chosen': '0.095119', 'rewards_train/rejected': '0.034083', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061036', 'logps_train/rejected': '-151.45', 'logps_train/chosen': '-143.14', 'loss/train': '0.6686', 'examples_per_second': '32.659', 'grad_norm': '20.25', 'counters/examples': 41536, 'counters/updates': 1298}
train stats after 41568 examples: {'rewards_train/chosen': '0.060261', 'rewards_train/rejected': '0.036949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023312', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-118.41', 'loss/train': '0.6849', 'examples_per_second': '30.562', 'grad_norm': '20.125', 'counters/examples': 41568, 'counters/updates': 1299}
train stats after 41600 examples: {'rewards_train/chosen': '0.080342', 'rewards_train/rejected': '0.016862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06348', 'logps_train/rejected': '-139.74', 'logps_train/chosen': '-128.21', 'loss/train': '0.6661', 'examples_per_second': '31.185', 'grad_norm': '20.375', 'counters/examples': 41600, 'counters/updates': 1300}
train stats after 41632 examples: {'rewards_train/chosen': '0.022888', 'rewards_train/rejected': '0.01812', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0047676', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-138.22', 'loss/train': '0.6957', 'examples_per_second': '30.606', 'grad_norm': '20.25', 'counters/examples': 41632, 'counters/updates': 1301}
train stats after 41664 examples: {'rewards_train/chosen': '0.057008', 'rewards_train/rejected': '0.030793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026214', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-120.1', 'loss/train': '0.68262', 'examples_per_second': '31.712', 'grad_norm': '20.625', 'counters/examples': 41664, 'counters/updates': 1302}
train stats after 41696 examples: {'rewards_train/chosen': '0.052032', 'rewards_train/rejected': '-0.0053341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057366', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-151.58', 'loss/train': '0.67152', 'examples_per_second': '31.67', 'grad_norm': '19.75', 'counters/examples': 41696, 'counters/updates': 1303}
train stats after 41728 examples: {'rewards_train/chosen': '0.031007', 'rewards_train/rejected': '0.030258', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00074844', 'logps_train/rejected': '-103.35', 'logps_train/chosen': '-126.85', 'loss/train': '0.69547', 'examples_per_second': '30.371', 'grad_norm': '19.125', 'counters/examples': 41728, 'counters/updates': 1304}
skipping logging after 41760 examples to avoid logging too frequently
train stats after 41792 examples: {'rewards_train/chosen': '0.0047984', 'rewards_train/rejected': '0.030297', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025499', 'logps_train/rejected': '-154.07', 'logps_train/chosen': '-132.45', 'loss/train': '0.71222', 'examples_per_second': '31.631', 'grad_norm': '21.75', 'counters/examples': 41792, 'counters/updates': 1306}
train stats after 41824 examples: {'rewards_train/chosen': '0.063806', 'rewards_train/rejected': '-0.014412', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078218', 'logps_train/rejected': '-100.73', 'logps_train/chosen': '-122.04', 'loss/train': '0.65872', 'examples_per_second': '32.734', 'grad_norm': '18.75', 'counters/examples': 41824, 'counters/updates': 1307}
train stats after 41856 examples: {'rewards_train/chosen': '0.0039048', 'rewards_train/rejected': '-0.0044903', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0083951', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-145.87', 'loss/train': '0.69392', 'examples_per_second': '31.668', 'grad_norm': '21', 'counters/examples': 41856, 'counters/updates': 1308}
train stats after 41888 examples: {'rewards_train/chosen': '0.0085563', 'rewards_train/rejected': '-0.0056434', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0142', 'logps_train/rejected': '-89.52', 'logps_train/chosen': '-120.41', 'loss/train': '0.68911', 'examples_per_second': '32.19', 'grad_norm': '17.625', 'counters/examples': 41888, 'counters/updates': 1309}
train stats after 41920 examples: {'rewards_train/chosen': '0.12022', 'rewards_train/rejected': '0.088234', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031983', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-134.72', 'loss/train': '0.68256', 'examples_per_second': '24.34', 'grad_norm': '21.625', 'counters/examples': 41920, 'counters/updates': 1310}
train stats after 41952 examples: {'rewards_train/chosen': '0.023896', 'rewards_train/rejected': '0.060639', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036743', 'logps_train/rejected': '-103.52', 'logps_train/chosen': '-142.01', 'loss/train': '0.71464', 'examples_per_second': '31.284', 'grad_norm': '21.375', 'counters/examples': 41952, 'counters/updates': 1311}
skipping logging after 41984 examples to avoid logging too frequently
train stats after 42016 examples: {'rewards_train/chosen': '0.056376', 'rewards_train/rejected': '0.011952', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.044424', 'logps_train/rejected': '-103.41', 'logps_train/chosen': '-128.04', 'loss/train': '0.67443', 'examples_per_second': '30.525', 'grad_norm': '19.625', 'counters/examples': 42016, 'counters/updates': 1313}
train stats after 42048 examples: {'rewards_train/chosen': '0.061661', 'rewards_train/rejected': '-0.0024362', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064097', 'logps_train/rejected': '-132.14', 'logps_train/chosen': '-135.49', 'loss/train': '0.66715', 'examples_per_second': '33.119', 'grad_norm': '20.875', 'counters/examples': 42048, 'counters/updates': 1314}
train stats after 42080 examples: {'rewards_train/chosen': '0.014714', 'rewards_train/rejected': '0.013357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0013573', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-109.89', 'loss/train': '0.69757', 'examples_per_second': '31.687', 'grad_norm': '20.25', 'counters/examples': 42080, 'counters/updates': 1315}
train stats after 42112 examples: {'rewards_train/chosen': '0.059475', 'rewards_train/rejected': '0.010485', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048991', 'logps_train/rejected': '-105.86', 'logps_train/chosen': '-122.63', 'loss/train': '0.67327', 'examples_per_second': '31.652', 'grad_norm': '18.75', 'counters/examples': 42112, 'counters/updates': 1316}
train stats after 42144 examples: {'rewards_train/chosen': '0.098587', 'rewards_train/rejected': '0.016315', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082272', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-144.23', 'loss/train': '0.6601', 'examples_per_second': '31.631', 'grad_norm': '20.125', 'counters/examples': 42144, 'counters/updates': 1317}
train stats after 42176 examples: {'rewards_train/chosen': '0.081303', 'rewards_train/rejected': '-0.010295', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091598', 'logps_train/rejected': '-167.38', 'logps_train/chosen': '-161.97', 'loss/train': '0.65356', 'examples_per_second': '31.699', 'grad_norm': '21.625', 'counters/examples': 42176, 'counters/updates': 1318}
train stats after 42208 examples: {'rewards_train/chosen': '0.090922', 'rewards_train/rejected': '0.042294', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048628', 'logps_train/rejected': '-92.817', 'logps_train/chosen': '-147.94', 'loss/train': '0.67481', 'examples_per_second': '30.248', 'grad_norm': '19.625', 'counters/examples': 42208, 'counters/updates': 1319}
train stats after 42240 examples: {'rewards_train/chosen': '0.11443', 'rewards_train/rejected': '0.032977', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.081451', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-147.61', 'loss/train': '0.65839', 'examples_per_second': '30.602', 'grad_norm': '20.125', 'counters/examples': 42240, 'counters/updates': 1320}
train stats after 42272 examples: {'rewards_train/chosen': '0.089453', 'rewards_train/rejected': '0.069423', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02003', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-126.15', 'loss/train': '0.68781', 'examples_per_second': '30.98', 'grad_norm': '20.125', 'counters/examples': 42272, 'counters/updates': 1321}
train stats after 42304 examples: {'rewards_train/chosen': '0.040555', 'rewards_train/rejected': '0.026001', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014554', 'logps_train/rejected': '-135.05', 'logps_train/chosen': '-121.34', 'loss/train': '0.68831', 'examples_per_second': '32.715', 'grad_norm': '19.25', 'counters/examples': 42304, 'counters/updates': 1322}
skipping logging after 42336 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '0.018286', 'rewards_train/rejected': '0.042906', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02462', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-126.4', 'loss/train': '0.70959', 'examples_per_second': '36.227', 'grad_norm': '20.875', 'counters/examples': 42368, 'counters/updates': 1324}
train stats after 42400 examples: {'rewards_train/chosen': '0.076397', 'rewards_train/rejected': '0.01742', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058977', 'logps_train/rejected': '-134.26', 'logps_train/chosen': '-136.37', 'loss/train': '0.66783', 'examples_per_second': '32.174', 'grad_norm': '19.875', 'counters/examples': 42400, 'counters/updates': 1325}
train stats after 42432 examples: {'rewards_train/chosen': '0.091474', 'rewards_train/rejected': '0.03008', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.061394', 'logps_train/rejected': '-84.837', 'logps_train/chosen': '-111.07', 'loss/train': '0.66711', 'examples_per_second': '32.48', 'grad_norm': '16.875', 'counters/examples': 42432, 'counters/updates': 1326}
train stats after 42464 examples: {'rewards_train/chosen': '0.054419', 'rewards_train/rejected': '0.0084345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045984', 'logps_train/rejected': '-102.11', 'logps_train/chosen': '-142.74', 'loss/train': '0.67392', 'examples_per_second': '31.463', 'grad_norm': '19.625', 'counters/examples': 42464, 'counters/updates': 1327}
train stats after 42496 examples: {'rewards_train/chosen': '0.080623', 'rewards_train/rejected': '0.035884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044739', 'logps_train/rejected': '-95.661', 'logps_train/chosen': '-123.36', 'loss/train': '0.673', 'examples_per_second': '31.457', 'grad_norm': '17.625', 'counters/examples': 42496, 'counters/updates': 1328}
train stats after 42528 examples: {'rewards_train/chosen': '0.078088', 'rewards_train/rejected': '0.042292', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035796', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-141.43', 'loss/train': '0.67913', 'examples_per_second': '32.18', 'grad_norm': '21', 'counters/examples': 42528, 'counters/updates': 1329}
train stats after 42560 examples: {'rewards_train/chosen': '0.06865', 'rewards_train/rejected': '0.052784', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015865', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-126.3', 'loss/train': '0.68957', 'examples_per_second': '31.669', 'grad_norm': '21.375', 'counters/examples': 42560, 'counters/updates': 1330}
skipping logging after 42592 examples to avoid logging too frequently
train stats after 42624 examples: {'rewards_train/chosen': '0.085424', 'rewards_train/rejected': '-0.0020771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087501', 'logps_train/rejected': '-102.4', 'logps_train/chosen': '-132.66', 'loss/train': '0.65468', 'examples_per_second': '30.666', 'grad_norm': '18.875', 'counters/examples': 42624, 'counters/updates': 1332}
skipping logging after 42656 examples to avoid logging too frequently
train stats after 42688 examples: {'rewards_train/chosen': '0.091856', 'rewards_train/rejected': '0.056681', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035175', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-139.64', 'loss/train': '0.67901', 'examples_per_second': '33.328', 'grad_norm': '20.625', 'counters/examples': 42688, 'counters/updates': 1334}
skipping logging after 42720 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '0.072507', 'rewards_train/rejected': '0.046898', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025609', 'logps_train/rejected': '-118.83', 'logps_train/chosen': '-156.99', 'loss/train': '0.6844', 'examples_per_second': '31.645', 'grad_norm': '19.75', 'counters/examples': 42752, 'counters/updates': 1336}
train stats after 42784 examples: {'rewards_train/chosen': '0.032168', 'rewards_train/rejected': '0.024109', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0080587', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-116.57', 'loss/train': '0.691', 'examples_per_second': '30.149', 'grad_norm': '19.625', 'counters/examples': 42784, 'counters/updates': 1337}
train stats after 42816 examples: {'rewards_train/chosen': '0.056004', 'rewards_train/rejected': '0.036693', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019311', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-155.64', 'loss/train': '0.68619', 'examples_per_second': '32.594', 'grad_norm': '22.75', 'counters/examples': 42816, 'counters/updates': 1338}
train stats after 42848 examples: {'rewards_train/chosen': '0.044152', 'rewards_train/rejected': '0.028439', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015713', 'logps_train/rejected': '-111.3', 'logps_train/chosen': '-98.654', 'loss/train': '0.68895', 'examples_per_second': '32.594', 'grad_norm': '18', 'counters/examples': 42848, 'counters/updates': 1339}
skipping logging after 42880 examples to avoid logging too frequently
train stats after 42912 examples: {'rewards_train/chosen': '0.094114', 'rewards_train/rejected': '0.0031677', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090946', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-126.99', 'loss/train': '0.65219', 'examples_per_second': '31.954', 'grad_norm': '20.375', 'counters/examples': 42912, 'counters/updates': 1341}
train stats after 42944 examples: {'rewards_train/chosen': '0.072662', 'rewards_train/rejected': '0.050072', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02259', 'logps_train/rejected': '-113.61', 'logps_train/chosen': '-131.7', 'loss/train': '0.68483', 'examples_per_second': '31.55', 'grad_norm': '19.125', 'counters/examples': 42944, 'counters/updates': 1342}
train stats after 42976 examples: {'rewards_train/chosen': '0.088037', 'rewards_train/rejected': '0.049112', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038925', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-147.32', 'loss/train': '0.6776', 'examples_per_second': '31.581', 'grad_norm': '19.375', 'counters/examples': 42976, 'counters/updates': 1343}
train stats after 43008 examples: {'rewards_train/chosen': '0.12888', 'rewards_train/rejected': '0.033674', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095207', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-166.36', 'loss/train': '0.65364', 'examples_per_second': '31.724', 'grad_norm': '22.25', 'counters/examples': 43008, 'counters/updates': 1344}
train stats after 43040 examples: {'rewards_train/chosen': '0.080182', 'rewards_train/rejected': '0.019346', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060836', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-141.01', 'loss/train': '0.66795', 'examples_per_second': '31.678', 'grad_norm': '21', 'counters/examples': 43040, 'counters/updates': 1345}
train stats after 43072 examples: {'rewards_train/chosen': '0.10487', 'rewards_train/rejected': '0.050842', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054032', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-150.22', 'loss/train': '0.66913', 'examples_per_second': '32.142', 'grad_norm': '20.25', 'counters/examples': 43072, 'counters/updates': 1346}
train stats after 43104 examples: {'rewards_train/chosen': '0.013124', 'rewards_train/rejected': '0.033122', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019998', 'logps_train/rejected': '-126.36', 'logps_train/chosen': '-117.13', 'loss/train': '0.70514', 'examples_per_second': '31.138', 'grad_norm': '20.875', 'counters/examples': 43104, 'counters/updates': 1347}
train stats after 43136 examples: {'rewards_train/chosen': '0.08514', 'rewards_train/rejected': '0.049294', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035846', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-148.05', 'loss/train': '0.68067', 'examples_per_second': '32.867', 'grad_norm': '21', 'counters/examples': 43136, 'counters/updates': 1348}
skipping logging after 43168 examples to avoid logging too frequently
train stats after 43200 examples: {'rewards_train/chosen': '0.082716', 'rewards_train/rejected': '0.022913', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059804', 'logps_train/rejected': '-121.67', 'logps_train/chosen': '-140.57', 'loss/train': '0.66773', 'examples_per_second': '31.685', 'grad_norm': '20.75', 'counters/examples': 43200, 'counters/updates': 1350}
train stats after 43232 examples: {'rewards_train/chosen': '0.096421', 'rewards_train/rejected': '0.057122', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039299', 'logps_train/rejected': '-85.474', 'logps_train/chosen': '-133.01', 'loss/train': '0.67814', 'examples_per_second': '31.075', 'grad_norm': '18.625', 'counters/examples': 43232, 'counters/updates': 1351}
train stats after 43264 examples: {'rewards_train/chosen': '0.0562', 'rewards_train/rejected': '0.016851', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039349', 'logps_train/rejected': '-146.45', 'logps_train/chosen': '-152.55', 'loss/train': '0.67638', 'examples_per_second': '30.7', 'grad_norm': '21.625', 'counters/examples': 43264, 'counters/updates': 1352}
skipping logging after 43296 examples to avoid logging too frequently
train stats after 43328 examples: {'rewards_train/chosen': '0.088325', 'rewards_train/rejected': '0.034295', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05403', 'logps_train/rejected': '-133.91', 'logps_train/chosen': '-161.43', 'loss/train': '0.67219', 'examples_per_second': '30.645', 'grad_norm': '21.625', 'counters/examples': 43328, 'counters/updates': 1354}
skipping logging after 43360 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '0.065944', 'rewards_train/rejected': '0.036442', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029502', 'logps_train/rejected': '-109.4', 'logps_train/chosen': '-148.45', 'loss/train': '0.68092', 'examples_per_second': '30.622', 'grad_norm': '20.125', 'counters/examples': 43392, 'counters/updates': 1356}
train stats after 43424 examples: {'rewards_train/chosen': '0.096905', 'rewards_train/rejected': '0.027197', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.069707', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-123.47', 'loss/train': '0.6617', 'examples_per_second': '32.053', 'grad_norm': '19.75', 'counters/examples': 43424, 'counters/updates': 1357}
train stats after 43456 examples: {'rewards_train/chosen': '0.079896', 'rewards_train/rejected': '0.023041', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.056855', 'logps_train/rejected': '-142.74', 'logps_train/chosen': '-123.71', 'loss/train': '0.66838', 'examples_per_second': '31.703', 'grad_norm': '20.75', 'counters/examples': 43456, 'counters/updates': 1358}
train stats after 43488 examples: {'rewards_train/chosen': '0.083661', 'rewards_train/rejected': '0.026139', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057522', 'logps_train/rejected': '-109.78', 'logps_train/chosen': '-111.19', 'loss/train': '0.6686', 'examples_per_second': '30.253', 'grad_norm': '17.875', 'counters/examples': 43488, 'counters/updates': 1359}
train stats after 43520 examples: {'rewards_train/chosen': '0.083102', 'rewards_train/rejected': '0.01184', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071261', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-149.19', 'loss/train': '0.66386', 'examples_per_second': '31.468', 'grad_norm': '20.375', 'counters/examples': 43520, 'counters/updates': 1360}
train stats after 43552 examples: {'rewards_train/chosen': '0.031566', 'rewards_train/rejected': '0.046685', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015119', 'logps_train/rejected': '-150.96', 'logps_train/chosen': '-114.28', 'loss/train': '0.7042', 'examples_per_second': '31.66', 'grad_norm': '21.875', 'counters/examples': 43552, 'counters/updates': 1361}
train stats after 43584 examples: {'rewards_train/chosen': '0.09959', 'rewards_train/rejected': '0.016902', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.082689', 'logps_train/rejected': '-119', 'logps_train/chosen': '-142.94', 'loss/train': '0.65544', 'examples_per_second': '31.678', 'grad_norm': '20.5', 'counters/examples': 43584, 'counters/updates': 1362}
train stats after 43616 examples: {'rewards_train/chosen': '0.062397', 'rewards_train/rejected': '0.012235', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050162', 'logps_train/rejected': '-72.78', 'logps_train/chosen': '-96.728', 'loss/train': '0.6729', 'examples_per_second': '30.991', 'grad_norm': '15.375', 'counters/examples': 43616, 'counters/updates': 1363}
skipping logging after 43648 examples to avoid logging too frequently
train stats after 43680 examples: {'rewards_train/chosen': '0.084178', 'rewards_train/rejected': '0.02497', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059208', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-116.32', 'loss/train': '0.6693', 'examples_per_second': '35.876', 'grad_norm': '18.375', 'counters/examples': 43680, 'counters/updates': 1365}
train stats after 43712 examples: {'rewards_train/chosen': '0.097374', 'rewards_train/rejected': '0.052468', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044906', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-161.87', 'loss/train': '0.67447', 'examples_per_second': '31.649', 'grad_norm': '21.5', 'counters/examples': 43712, 'counters/updates': 1366}
train stats after 43744 examples: {'rewards_train/chosen': '0.10712', 'rewards_train/rejected': '0.020543', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.086579', 'logps_train/rejected': '-88.934', 'logps_train/chosen': '-130.86', 'loss/train': '0.65701', 'examples_per_second': '31.643', 'grad_norm': '18.5', 'counters/examples': 43744, 'counters/updates': 1367}
skipping logging after 43776 examples to avoid logging too frequently
train stats after 43808 examples: {'rewards_train/chosen': '0.055885', 'rewards_train/rejected': '0.0031041', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052781', 'logps_train/rejected': '-81.652', 'logps_train/chosen': '-104.88', 'loss/train': '0.6696', 'examples_per_second': '33.929', 'grad_norm': '16.625', 'counters/examples': 43808, 'counters/updates': 1369}
train stats after 43840 examples: {'rewards_train/chosen': '0.085268', 'rewards_train/rejected': '0.010899', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074369', 'logps_train/rejected': '-154.76', 'logps_train/chosen': '-149.56', 'loss/train': '0.66143', 'examples_per_second': '31.358', 'grad_norm': '22', 'counters/examples': 43840, 'counters/updates': 1370}
skipping logging after 43872 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '0.083523', 'rewards_train/rejected': '0.028235', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055288', 'logps_train/rejected': '-91.451', 'logps_train/chosen': '-110.57', 'loss/train': '0.67037', 'examples_per_second': '34.313', 'grad_norm': '17.625', 'counters/examples': 43904, 'counters/updates': 1372}
train stats after 43936 examples: {'rewards_train/chosen': '0.074577', 'rewards_train/rejected': '0.019724', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054853', 'logps_train/rejected': '-150.09', 'logps_train/chosen': '-180.84', 'loss/train': '0.66903', 'examples_per_second': '31.035', 'grad_norm': '22.875', 'counters/examples': 43936, 'counters/updates': 1373}
train stats after 43968 examples: {'rewards_train/chosen': '0.04859', 'rewards_train/rejected': '0.020126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028465', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-121', 'loss/train': '0.68214', 'examples_per_second': '33.163', 'grad_norm': '20', 'counters/examples': 43968, 'counters/updates': 1374}
train stats after 44000 examples: {'rewards_train/chosen': '0.043899', 'rewards_train/rejected': '0.0087101', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035188', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-160.82', 'loss/train': '0.67929', 'examples_per_second': '31.476', 'grad_norm': '22', 'counters/examples': 44000, 'counters/updates': 1375}
train stats after 44032 examples: {'rewards_train/chosen': '0.069966', 'rewards_train/rejected': '0.032817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037149', 'logps_train/rejected': '-119.88', 'logps_train/chosen': '-131.71', 'loss/train': '0.67895', 'examples_per_second': '31.813', 'grad_norm': '19.75', 'counters/examples': 44032, 'counters/updates': 1376}
skipping logging after 44064 examples to avoid logging too frequently
train stats after 44096 examples: {'rewards_train/chosen': '0.045977', 'rewards_train/rejected': '0.038253', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.007724', 'logps_train/rejected': '-93.317', 'logps_train/chosen': '-99.431', 'loss/train': '0.69169', 'examples_per_second': '31.076', 'grad_norm': '17.625', 'counters/examples': 44096, 'counters/updates': 1378}
train stats after 44128 examples: {'rewards_train/chosen': '0.02525', 'rewards_train/rejected': '0.061586', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036336', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-183.44', 'loss/train': '0.71491', 'examples_per_second': '31.681', 'grad_norm': '24.125', 'counters/examples': 44128, 'counters/updates': 1379}
train stats after 44160 examples: {'rewards_train/chosen': '0.038784', 'rewards_train/rejected': '0.0034858', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035298', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-117.86', 'loss/train': '0.68083', 'examples_per_second': '31.656', 'grad_norm': '18.25', 'counters/examples': 44160, 'counters/updates': 1380}
train stats after 44192 examples: {'rewards_train/chosen': '0.099314', 'rewards_train/rejected': '0.0029434', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.096371', 'logps_train/rejected': '-84.041', 'logps_train/chosen': '-139.88', 'loss/train': '0.65029', 'examples_per_second': '31.528', 'grad_norm': '19.625', 'counters/examples': 44192, 'counters/updates': 1381}
train stats after 44224 examples: {'rewards_train/chosen': '0.076945', 'rewards_train/rejected': '0.037986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038959', 'logps_train/rejected': '-97.742', 'logps_train/chosen': '-163.04', 'loss/train': '0.67702', 'examples_per_second': '31.714', 'grad_norm': '19.75', 'counters/examples': 44224, 'counters/updates': 1382}
train stats after 44256 examples: {'rewards_train/chosen': '0.068717', 'rewards_train/rejected': '0.067182', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0015351', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-139.05', 'loss/train': '0.69688', 'examples_per_second': '32.745', 'grad_norm': '20.75', 'counters/examples': 44256, 'counters/updates': 1383}
train stats after 44288 examples: {'rewards_train/chosen': '0.059785', 'rewards_train/rejected': '0.04816', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011625', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-115.08', 'loss/train': '0.68964', 'examples_per_second': '32.048', 'grad_norm': '19.75', 'counters/examples': 44288, 'counters/updates': 1384}
skipping logging after 44320 examples to avoid logging too frequently
train stats after 44352 examples: {'rewards_train/chosen': '0.068532', 'rewards_train/rejected': '-0.010136', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078668', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-122.11', 'loss/train': '0.65842', 'examples_per_second': '30.206', 'grad_norm': '19.25', 'counters/examples': 44352, 'counters/updates': 1386}
train stats after 44384 examples: {'rewards_train/chosen': '0.057702', 'rewards_train/rejected': '0.028127', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029575', 'logps_train/rejected': '-115.53', 'logps_train/chosen': '-125.77', 'loss/train': '0.68396', 'examples_per_second': '31.614', 'grad_norm': '19.5', 'counters/examples': 44384, 'counters/updates': 1387}
train stats after 44416 examples: {'rewards_train/chosen': '0.075215', 'rewards_train/rejected': '0.057167', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018047', 'logps_train/rejected': '-109.74', 'logps_train/chosen': '-143.76', 'loss/train': '0.6885', 'examples_per_second': '30.212', 'grad_norm': '19.875', 'counters/examples': 44416, 'counters/updates': 1388}
train stats after 44448 examples: {'rewards_train/chosen': '0.088539', 'rewards_train/rejected': '-0.020108', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10865', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-139.35', 'loss/train': '0.64561', 'examples_per_second': '32.411', 'grad_norm': '19.75', 'counters/examples': 44448, 'counters/updates': 1389}
train stats after 44480 examples: {'rewards_train/chosen': '0.04219', 'rewards_train/rejected': '0.0058627', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036328', 'logps_train/rejected': '-123.24', 'logps_train/chosen': '-135.51', 'loss/train': '0.67859', 'examples_per_second': '32.998', 'grad_norm': '20.625', 'counters/examples': 44480, 'counters/updates': 1390}
skipping logging after 44512 examples to avoid logging too frequently
train stats after 44544 examples: {'rewards_train/chosen': '0.052921', 'rewards_train/rejected': '0.017282', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035639', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-115.92', 'loss/train': '0.68019', 'examples_per_second': '31.84', 'grad_norm': '20.5', 'counters/examples': 44544, 'counters/updates': 1392}
train stats after 44576 examples: {'rewards_train/chosen': '0.074386', 'rewards_train/rejected': '0.065735', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0086514', 'logps_train/rejected': '-125.36', 'logps_train/chosen': '-162.44', 'loss/train': '0.69543', 'examples_per_second': '31.561', 'grad_norm': '22.625', 'counters/examples': 44576, 'counters/updates': 1393}
train stats after 44608 examples: {'rewards_train/chosen': '0.056324', 'rewards_train/rejected': '0.039094', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01723', 'logps_train/rejected': '-101.49', 'logps_train/chosen': '-164.81', 'loss/train': '0.68865', 'examples_per_second': '30.858', 'grad_norm': '21.75', 'counters/examples': 44608, 'counters/updates': 1394}
skipping logging after 44640 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '0.0371', 'rewards_train/rejected': '0.025549', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011551', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-136.66', 'loss/train': '0.69112', 'examples_per_second': '31.675', 'grad_norm': '20.75', 'counters/examples': 44672, 'counters/updates': 1396}
skipping logging after 44704 examples to avoid logging too frequently
train stats after 44736 examples: {'rewards_train/chosen': '0.078077', 'rewards_train/rejected': '0.067876', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0102', 'logps_train/rejected': '-113.03', 'logps_train/chosen': '-137.49', 'loss/train': '0.69249', 'examples_per_second': '31.117', 'grad_norm': '19.5', 'counters/examples': 44736, 'counters/updates': 1398}
train stats after 44768 examples: {'rewards_train/chosen': '0.074674', 'rewards_train/rejected': '0.022158', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052517', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-128.4', 'loss/train': '0.66989', 'examples_per_second': '31.7', 'grad_norm': '20.125', 'counters/examples': 44768, 'counters/updates': 1399}
train stats after 44800 examples: {'rewards_train/chosen': '0.084355', 'rewards_train/rejected': '0.043445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04091', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-183.27', 'loss/train': '0.67702', 'examples_per_second': '31.647', 'grad_norm': '22.375', 'counters/examples': 44800, 'counters/updates': 1400}
train stats after 44832 examples: {'rewards_train/chosen': '0.096152', 'rewards_train/rejected': '0.040741', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055412', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-132.5', 'loss/train': '0.66955', 'examples_per_second': '30.318', 'grad_norm': '21.125', 'counters/examples': 44832, 'counters/updates': 1401}
train stats after 44864 examples: {'rewards_train/chosen': '0.014676', 'rewards_train/rejected': '0.024213', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.009537', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-139.84', 'loss/train': '0.70314', 'examples_per_second': '32.192', 'grad_norm': '21.875', 'counters/examples': 44864, 'counters/updates': 1402}
train stats after 44896 examples: {'rewards_train/chosen': '0.054767', 'rewards_train/rejected': '0.073466', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018699', 'logps_train/rejected': '-98.518', 'logps_train/chosen': '-136.31', 'loss/train': '0.70765', 'examples_per_second': '31.734', 'grad_norm': '21.25', 'counters/examples': 44896, 'counters/updates': 1403}
train stats after 44928 examples: {'rewards_train/chosen': '0.039334', 'rewards_train/rejected': '0.03478', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0045543', 'logps_train/rejected': '-173.71', 'logps_train/chosen': '-150.97', 'loss/train': '0.69399', 'examples_per_second': '31.329', 'grad_norm': '22.75', 'counters/examples': 44928, 'counters/updates': 1404}
train stats after 44960 examples: {'rewards_train/chosen': '0.073089', 'rewards_train/rejected': '0.05291', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.020179', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-127.84', 'loss/train': '0.68618', 'examples_per_second': '31.772', 'grad_norm': '20.625', 'counters/examples': 44960, 'counters/updates': 1405}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45024 examples: {'rewards_train/chosen': '0.063627', 'rewards_train/rejected': '0.060186', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0034412', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-153.83', 'loss/train': '0.69583', 'examples_per_second': '31.674', 'grad_norm': '24', 'counters/examples': 45024, 'counters/updates': 1407}
train stats after 45056 examples: {'rewards_train/chosen': '0.094782', 'rewards_train/rejected': '0.064354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030428', 'logps_train/rejected': '-124.22', 'logps_train/chosen': '-123.56', 'loss/train': '0.68226', 'examples_per_second': '32.34', 'grad_norm': '21.125', 'counters/examples': 45056, 'counters/updates': 1408}
train stats after 45088 examples: {'rewards_train/chosen': '0.069391', 'rewards_train/rejected': '-0.0095554', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078946', 'logps_train/rejected': '-101.08', 'logps_train/chosen': '-129.04', 'loss/train': '0.65855', 'examples_per_second': '30.199', 'grad_norm': '19.25', 'counters/examples': 45088, 'counters/updates': 1409}
train stats after 45120 examples: {'rewards_train/chosen': '0.064249', 'rewards_train/rejected': '0.038079', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02617', 'logps_train/rejected': '-96.271', 'logps_train/chosen': '-113.48', 'loss/train': '0.68353', 'examples_per_second': '32.984', 'grad_norm': '17.75', 'counters/examples': 45120, 'counters/updates': 1410}
train stats after 45152 examples: {'rewards_train/chosen': '0.12544', 'rewards_train/rejected': '0.017181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10826', 'logps_train/rejected': '-106.78', 'logps_train/chosen': '-111.84', 'loss/train': '0.64514', 'examples_per_second': '32.587', 'grad_norm': '17.375', 'counters/examples': 45152, 'counters/updates': 1411}
train stats after 45184 examples: {'rewards_train/chosen': '0.062762', 'rewards_train/rejected': '0.0020389', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060723', 'logps_train/rejected': '-101.73', 'logps_train/chosen': '-133.99', 'loss/train': '0.66728', 'examples_per_second': '30.924', 'grad_norm': '18.875', 'counters/examples': 45184, 'counters/updates': 1412}
train stats after 45216 examples: {'rewards_train/chosen': '0.034968', 'rewards_train/rejected': '0.075524', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.040555', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-127.67', 'loss/train': '0.7176', 'examples_per_second': '30.659', 'grad_norm': '20.5', 'counters/examples': 45216, 'counters/updates': 1413}
train stats after 45248 examples: {'rewards_train/chosen': '0.098973', 'rewards_train/rejected': '0.081304', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017669', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-110.53', 'loss/train': '0.68809', 'examples_per_second': '32.551', 'grad_norm': '21.125', 'counters/examples': 45248, 'counters/updates': 1414}
train stats after 45280 examples: {'rewards_train/chosen': '0.11118', 'rewards_train/rejected': '0.035844', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075338', 'logps_train/rejected': '-95.875', 'logps_train/chosen': '-118.49', 'loss/train': '0.65971', 'examples_per_second': '31.167', 'grad_norm': '17.75', 'counters/examples': 45280, 'counters/updates': 1415}
train stats after 45312 examples: {'rewards_train/chosen': '0.080116', 'rewards_train/rejected': '-0.0093771', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089493', 'logps_train/rejected': '-138.36', 'logps_train/chosen': '-155.98', 'loss/train': '0.65339', 'examples_per_second': '32.113', 'grad_norm': '20.625', 'counters/examples': 45312, 'counters/updates': 1416}
skipping logging after 45344 examples to avoid logging too frequently
train stats after 45376 examples: {'rewards_train/chosen': '0.04374', 'rewards_train/rejected': '0.063719', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019979', 'logps_train/rejected': '-97.681', 'logps_train/chosen': '-128.6', 'loss/train': '0.70571', 'examples_per_second': '31.699', 'grad_norm': '20.375', 'counters/examples': 45376, 'counters/updates': 1418}
train stats after 45408 examples: {'rewards_train/chosen': '0.033579', 'rewards_train/rejected': '0.016888', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016691', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-127.23', 'loss/train': '0.68863', 'examples_per_second': '32.373', 'grad_norm': '20.875', 'counters/examples': 45408, 'counters/updates': 1419}
train stats after 45440 examples: {'rewards_train/chosen': '0.079727', 'rewards_train/rejected': '0.067819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011908', 'logps_train/rejected': '-101', 'logps_train/chosen': '-109.39', 'loss/train': '0.69388', 'examples_per_second': '31.21', 'grad_norm': '18.375', 'counters/examples': 45440, 'counters/updates': 1420}
train stats after 45472 examples: {'rewards_train/chosen': '0.05351', 'rewards_train/rejected': '0.040532', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012979', 'logps_train/rejected': '-101.2', 'logps_train/chosen': '-106.7', 'loss/train': '0.69119', 'examples_per_second': '30.651', 'grad_norm': '18.25', 'counters/examples': 45472, 'counters/updates': 1421}
train stats after 45504 examples: {'rewards_train/chosen': '0.058812', 'rewards_train/rejected': '-0.014059', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072871', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-181.89', 'loss/train': '0.66457', 'examples_per_second': '32.788', 'grad_norm': '21.625', 'counters/examples': 45504, 'counters/updates': 1422}
skipping logging after 45536 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '0.049362', 'rewards_train/rejected': '0.02206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027301', 'logps_train/rejected': '-113.25', 'logps_train/chosen': '-108.74', 'loss/train': '0.68384', 'examples_per_second': '33.154', 'grad_norm': '18.875', 'counters/examples': 45568, 'counters/updates': 1424}
train stats after 45600 examples: {'rewards_train/chosen': '0.063085', 'rewards_train/rejected': '0.030684', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032402', 'logps_train/rejected': '-96.454', 'logps_train/chosen': '-135.52', 'loss/train': '0.68048', 'examples_per_second': '31.644', 'grad_norm': '19.375', 'counters/examples': 45600, 'counters/updates': 1425}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45664 examples: {'rewards_train/chosen': '0.014113', 'rewards_train/rejected': '0.038973', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.02486', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-131.55', 'loss/train': '0.70865', 'examples_per_second': '31.689', 'grad_norm': '19.75', 'counters/examples': 45664, 'counters/updates': 1427}
train stats after 45696 examples: {'rewards_train/chosen': '0.068345', 'rewards_train/rejected': '-0.0039531', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072298', 'logps_train/rejected': '-117.15', 'logps_train/chosen': '-119.66', 'loss/train': '0.66056', 'examples_per_second': '32.758', 'grad_norm': '19.75', 'counters/examples': 45696, 'counters/updates': 1428}
train stats after 45728 examples: {'rewards_train/chosen': '0.044137', 'rewards_train/rejected': '0.03916', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0049772', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-164.69', 'loss/train': '0.69539', 'examples_per_second': '31.952', 'grad_norm': '22', 'counters/examples': 45728, 'counters/updates': 1429}
train stats after 45760 examples: {'rewards_train/chosen': '0.083088', 'rewards_train/rejected': '0.067101', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015986', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-151.09', 'loss/train': '0.69117', 'examples_per_second': '30.722', 'grad_norm': '20.5', 'counters/examples': 45760, 'counters/updates': 1430}
train stats after 45792 examples: {'rewards_train/chosen': '0.088036', 'rewards_train/rejected': '0.024251', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063785', 'logps_train/rejected': '-82.04', 'logps_train/chosen': '-119.71', 'loss/train': '0.66605', 'examples_per_second': '31.49', 'grad_norm': '18.75', 'counters/examples': 45792, 'counters/updates': 1431}
train stats after 45824 examples: {'rewards_train/chosen': '0.088834', 'rewards_train/rejected': '0.042878', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045956', 'logps_train/rejected': '-120.15', 'logps_train/chosen': '-133.59', 'loss/train': '0.67463', 'examples_per_second': '31.669', 'grad_norm': '19', 'counters/examples': 45824, 'counters/updates': 1432}
train stats after 45856 examples: {'rewards_train/chosen': '0.060864', 'rewards_train/rejected': '0.031903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028961', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-154.47', 'loss/train': '0.68513', 'examples_per_second': '31.682', 'grad_norm': '21.5', 'counters/examples': 45856, 'counters/updates': 1433}
train stats after 45888 examples: {'rewards_train/chosen': '0.090567', 'rewards_train/rejected': '0.0013485', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089219', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-142.39', 'loss/train': '0.65377', 'examples_per_second': '30.289', 'grad_norm': '21.125', 'counters/examples': 45888, 'counters/updates': 1434}
train stats after 45920 examples: {'rewards_train/chosen': '0.028987', 'rewards_train/rejected': '0.049337', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02035', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-171.83', 'loss/train': '0.71024', 'examples_per_second': '32.377', 'grad_norm': '23.125', 'counters/examples': 45920, 'counters/updates': 1435}
train stats after 45952 examples: {'rewards_train/chosen': '0.087902', 'rewards_train/rejected': '0.046239', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041663', 'logps_train/rejected': '-89.156', 'logps_train/chosen': '-103.18', 'loss/train': '0.67542', 'examples_per_second': '32.939', 'grad_norm': '17.375', 'counters/examples': 45952, 'counters/updates': 1436}
train stats after 45984 examples: {'rewards_train/chosen': '0.078771', 'rewards_train/rejected': '0.054448', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024323', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-173.15', 'loss/train': '0.68552', 'examples_per_second': '31.652', 'grad_norm': '22.75', 'counters/examples': 45984, 'counters/updates': 1437}
train stats after 46016 examples: {'rewards_train/chosen': '0.076203', 'rewards_train/rejected': '0.050339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025863', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-146.92', 'loss/train': '0.68528', 'examples_per_second': '31.637', 'grad_norm': '22.75', 'counters/examples': 46016, 'counters/updates': 1438}
train stats after 46048 examples: {'rewards_train/chosen': '0.068736', 'rewards_train/rejected': '0.023976', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04476', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-119.66', 'loss/train': '0.67609', 'examples_per_second': '30.271', 'grad_norm': '20.375', 'counters/examples': 46048, 'counters/updates': 1439}
train stats after 46080 examples: {'rewards_train/chosen': '0.081494', 'rewards_train/rejected': '0.057206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024288', 'logps_train/rejected': '-139.35', 'logps_train/chosen': '-137.98', 'loss/train': '0.68635', 'examples_per_second': '32.765', 'grad_norm': '20', 'counters/examples': 46080, 'counters/updates': 1440}
skipping logging after 46112 examples to avoid logging too frequently
train stats after 46144 examples: {'rewards_train/chosen': '0.09537', 'rewards_train/rejected': '0.077893', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017476', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-175.44', 'loss/train': '0.68964', 'examples_per_second': '31.656', 'grad_norm': '22', 'counters/examples': 46144, 'counters/updates': 1442}
train stats after 46176 examples: {'rewards_train/chosen': '0.080415', 'rewards_train/rejected': '0.0016469', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.078768', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-147.45', 'loss/train': '0.65922', 'examples_per_second': '32.677', 'grad_norm': '20.875', 'counters/examples': 46176, 'counters/updates': 1443}
train stats after 46208 examples: {'rewards_train/chosen': '0.0777', 'rewards_train/rejected': '0.041695', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036005', 'logps_train/rejected': '-108.38', 'logps_train/chosen': '-150.42', 'loss/train': '0.67899', 'examples_per_second': '31.656', 'grad_norm': '19.75', 'counters/examples': 46208, 'counters/updates': 1444}
train stats after 46240 examples: {'rewards_train/chosen': '0.03741', 'rewards_train/rejected': '0.019402', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018008', 'logps_train/rejected': '-132.26', 'logps_train/chosen': '-116.6', 'loss/train': '0.68859', 'examples_per_second': '31.526', 'grad_norm': '19.875', 'counters/examples': 46240, 'counters/updates': 1445}
train stats after 46272 examples: {'rewards_train/chosen': '0.030311', 'rewards_train/rejected': '0.072973', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042662', 'logps_train/rejected': '-136.68', 'logps_train/chosen': '-150.85', 'loss/train': '0.7168', 'examples_per_second': '31.689', 'grad_norm': '23.5', 'counters/examples': 46272, 'counters/updates': 1446}
skipping logging after 46304 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '0.080905', 'rewards_train/rejected': '0.031063', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049841', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-143.45', 'loss/train': '0.67208', 'examples_per_second': '31.809', 'grad_norm': '19.75', 'counters/examples': 46336, 'counters/updates': 1448}
skipping logging after 46368 examples to avoid logging too frequently
train stats after 46400 examples: {'rewards_train/chosen': '0.069135', 'rewards_train/rejected': '0.033454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035681', 'logps_train/rejected': '-179.04', 'logps_train/chosen': '-145.9', 'loss/train': '0.67927', 'examples_per_second': '33.118', 'grad_norm': '23.25', 'counters/examples': 46400, 'counters/updates': 1450}
train stats after 46432 examples: {'rewards_train/chosen': '0.029686', 'rewards_train/rejected': '0.034085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0043986', 'logps_train/rejected': '-138.49', 'logps_train/chosen': '-115.98', 'loss/train': '0.69866', 'examples_per_second': '24.366', 'grad_norm': '21.25', 'counters/examples': 46432, 'counters/updates': 1451}
train stats after 46464 examples: {'rewards_train/chosen': '0.055847', 'rewards_train/rejected': '0.044449', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011398', 'logps_train/rejected': '-100.27', 'logps_train/chosen': '-108.46', 'loss/train': '0.69049', 'examples_per_second': '32.24', 'grad_norm': '19', 'counters/examples': 46464, 'counters/updates': 1452}
train stats after 46496 examples: {'rewards_train/chosen': '0.13138', 'rewards_train/rejected': '0.050451', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080933', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-159.72', 'loss/train': '0.65638', 'examples_per_second': '31.589', 'grad_norm': '21.25', 'counters/examples': 46496, 'counters/updates': 1453}
train stats after 46528 examples: {'rewards_train/chosen': '0.042123', 'rewards_train/rejected': '0.023037', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019085', 'logps_train/rejected': '-89.78', 'logps_train/chosen': '-137.53', 'loss/train': '0.68937', 'examples_per_second': '27.844', 'grad_norm': '19.5', 'counters/examples': 46528, 'counters/updates': 1454}
train stats after 46560 examples: {'rewards_train/chosen': '0.10002', 'rewards_train/rejected': '0.026134', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073891', 'logps_train/rejected': '-139.88', 'logps_train/chosen': '-117.8', 'loss/train': '0.66116', 'examples_per_second': '30.256', 'grad_norm': '19.75', 'counters/examples': 46560, 'counters/updates': 1455}
train stats after 46592 examples: {'rewards_train/chosen': '0.062962', 'rewards_train/rejected': '0.023968', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038994', 'logps_train/rejected': '-107.62', 'logps_train/chosen': '-141', 'loss/train': '0.67664', 'examples_per_second': '31.701', 'grad_norm': '19.875', 'counters/examples': 46592, 'counters/updates': 1456}
train stats after 46624 examples: {'rewards_train/chosen': '0.12318', 'rewards_train/rejected': '0.026266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096917', 'logps_train/rejected': '-102.27', 'logps_train/chosen': '-147.09', 'loss/train': '0.64999', 'examples_per_second': '31.734', 'grad_norm': '19', 'counters/examples': 46624, 'counters/updates': 1457}
train stats after 46656 examples: {'rewards_train/chosen': '0.078147', 'rewards_train/rejected': '0.036876', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.041271', 'logps_train/rejected': '-88.802', 'logps_train/chosen': '-114.26', 'loss/train': '0.67885', 'examples_per_second': '31.314', 'grad_norm': '18.375', 'counters/examples': 46656, 'counters/updates': 1458}
skipping logging after 46688 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '0.11756', 'rewards_train/rejected': '0.065849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051711', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-180.82', 'loss/train': '0.67285', 'examples_per_second': '30.671', 'grad_norm': '22', 'counters/examples': 46720, 'counters/updates': 1460}
train stats after 46752 examples: {'rewards_train/chosen': '0.11066', 'rewards_train/rejected': '0.015173', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.095484', 'logps_train/rejected': '-99.452', 'logps_train/chosen': '-147.96', 'loss/train': '0.6516', 'examples_per_second': '31.622', 'grad_norm': '21.125', 'counters/examples': 46752, 'counters/updates': 1461}
train stats after 46784 examples: {'rewards_train/chosen': '0.070151', 'rewards_train/rejected': '0.030818', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039334', 'logps_train/rejected': '-156.45', 'logps_train/chosen': '-145.66', 'loss/train': '0.67979', 'examples_per_second': '32.579', 'grad_norm': '22.25', 'counters/examples': 46784, 'counters/updates': 1462}
train stats after 46816 examples: {'rewards_train/chosen': '0.10502', 'rewards_train/rejected': '0.051457', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05356', 'logps_train/rejected': '-93.599', 'logps_train/chosen': '-143.59', 'loss/train': '0.67011', 'examples_per_second': '30.163', 'grad_norm': '19.875', 'counters/examples': 46816, 'counters/updates': 1463}
skipping logging after 46848 examples to avoid logging too frequently
train stats after 46880 examples: {'rewards_train/chosen': '0.064733', 'rewards_train/rejected': '0.051849', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012884', 'logps_train/rejected': '-120.42', 'logps_train/chosen': '-142.29', 'loss/train': '0.68905', 'examples_per_second': '30.298', 'grad_norm': '21.375', 'counters/examples': 46880, 'counters/updates': 1465}
train stats after 46912 examples: {'rewards_train/chosen': '0.072012', 'rewards_train/rejected': '0.021031', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050981', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-174.92', 'loss/train': '0.67105', 'examples_per_second': '31.505', 'grad_norm': '23.125', 'counters/examples': 46912, 'counters/updates': 1466}
train stats after 46944 examples: {'rewards_train/chosen': '0.076673', 'rewards_train/rejected': '-0.026894', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10357', 'logps_train/rejected': '-98.668', 'logps_train/chosen': '-135.97', 'loss/train': '0.64679', 'examples_per_second': '30.733', 'grad_norm': '18.5', 'counters/examples': 46944, 'counters/updates': 1467}
train stats after 46976 examples: {'rewards_train/chosen': '0.11296', 'rewards_train/rejected': '0.052296', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060662', 'logps_train/rejected': '-108.14', 'logps_train/chosen': '-143.31', 'loss/train': '0.66851', 'examples_per_second': '32.787', 'grad_norm': '19.125', 'counters/examples': 46976, 'counters/updates': 1468}
train stats after 47008 examples: {'rewards_train/chosen': '0.056065', 'rewards_train/rejected': '0.036652', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019414', 'logps_train/rejected': '-149.89', 'logps_train/chosen': '-155.52', 'loss/train': '0.68959', 'examples_per_second': '31.907', 'grad_norm': '22.375', 'counters/examples': 47008, 'counters/updates': 1469}
train stats after 47040 examples: {'rewards_train/chosen': '0.13855', 'rewards_train/rejected': '0.054008', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084537', 'logps_train/rejected': '-109.2', 'logps_train/chosen': '-131.21', 'loss/train': '0.65881', 'examples_per_second': '30.222', 'grad_norm': '18', 'counters/examples': 47040, 'counters/updates': 1470}
train stats after 47072 examples: {'rewards_train/chosen': '0.12036', 'rewards_train/rejected': '0.064174', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056182', 'logps_train/rejected': '-111.87', 'logps_train/chosen': '-138.69', 'loss/train': '0.6698', 'examples_per_second': '32.051', 'grad_norm': '20.25', 'counters/examples': 47072, 'counters/updates': 1471}
train stats after 47104 examples: {'rewards_train/chosen': '0.070211', 'rewards_train/rejected': '0.02155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048661', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-151.52', 'loss/train': '0.67341', 'examples_per_second': '31.688', 'grad_norm': '21.625', 'counters/examples': 47104, 'counters/updates': 1472}
train stats after 47136 examples: {'rewards_train/chosen': '0.078971', 'rewards_train/rejected': '0.079772', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0008002', 'logps_train/rejected': '-107.31', 'logps_train/chosen': '-139.67', 'loss/train': '0.69862', 'examples_per_second': '30.983', 'grad_norm': '20.375', 'counters/examples': 47136, 'counters/updates': 1473}
train stats after 47168 examples: {'rewards_train/chosen': '0.045379', 'rewards_train/rejected': '0.062124', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016744', 'logps_train/rejected': '-126.33', 'logps_train/chosen': '-126.2', 'loss/train': '0.70802', 'examples_per_second': '32.972', 'grad_norm': '21.75', 'counters/examples': 47168, 'counters/updates': 1474}
train stats after 47200 examples: {'rewards_train/chosen': '0.060197', 'rewards_train/rejected': '-0.017175', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077372', 'logps_train/rejected': '-96.226', 'logps_train/chosen': '-102.18', 'loss/train': '0.65767', 'examples_per_second': '31.732', 'grad_norm': '16.5', 'counters/examples': 47200, 'counters/updates': 1475}
skipping logging after 47232 examples to avoid logging too frequently
train stats after 47264 examples: {'rewards_train/chosen': '0.025973', 'rewards_train/rejected': '0.0029782', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022995', 'logps_train/rejected': '-135.28', 'logps_train/chosen': '-159.5', 'loss/train': '0.68413', 'examples_per_second': '30.739', 'grad_norm': '21.5', 'counters/examples': 47264, 'counters/updates': 1477}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47328 examples: {'rewards_train/chosen': '0.11988', 'rewards_train/rejected': '0.0034004', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11648', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-163.36', 'loss/train': '0.64055', 'examples_per_second': '31.783', 'grad_norm': '21.875', 'counters/examples': 47328, 'counters/updates': 1479}
skipping logging after 47360 examples to avoid logging too frequently
train stats after 47392 examples: {'rewards_train/chosen': '0.11644', 'rewards_train/rejected': '0.016236', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10021', 'logps_train/rejected': '-106.27', 'logps_train/chosen': '-144.13', 'loss/train': '0.64924', 'examples_per_second': '31.269', 'grad_norm': '19', 'counters/examples': 47392, 'counters/updates': 1481}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47456 examples: {'rewards_train/chosen': '0.11624', 'rewards_train/rejected': '0.031167', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085069', 'logps_train/rejected': '-120.47', 'logps_train/chosen': '-137.26', 'loss/train': '0.65719', 'examples_per_second': '31.67', 'grad_norm': '19.75', 'counters/examples': 47456, 'counters/updates': 1483}
train stats after 47488 examples: {'rewards_train/chosen': '0.045328', 'rewards_train/rejected': '0.031516', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013812', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-161.49', 'loss/train': '0.68927', 'examples_per_second': '24.838', 'grad_norm': '22.125', 'counters/examples': 47488, 'counters/updates': 1484}
train stats after 47520 examples: {'rewards_train/chosen': '0.070921', 'rewards_train/rejected': '0.051526', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019396', 'logps_train/rejected': '-103.7', 'logps_train/chosen': '-127.7', 'loss/train': '0.68691', 'examples_per_second': '31.782', 'grad_norm': '18.625', 'counters/examples': 47520, 'counters/updates': 1485}
train stats after 47552 examples: {'rewards_train/chosen': '0.088435', 'rewards_train/rejected': '0.029265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05917', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-139.33', 'loss/train': '0.6703', 'examples_per_second': '31.717', 'grad_norm': '20.625', 'counters/examples': 47552, 'counters/updates': 1486}
train stats after 47584 examples: {'rewards_train/chosen': '0.022877', 'rewards_train/rejected': '0.014873', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.008004', 'logps_train/rejected': '-132.4', 'logps_train/chosen': '-144.06', 'loss/train': '0.69457', 'examples_per_second': '30.156', 'grad_norm': '23.25', 'counters/examples': 47584, 'counters/updates': 1487}
skipping logging after 47616 examples to avoid logging too frequently
train stats after 47648 examples: {'rewards_train/chosen': '0.083845', 'rewards_train/rejected': '0.0088604', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.074985', 'logps_train/rejected': '-95.694', 'logps_train/chosen': '-108.22', 'loss/train': '0.65935', 'examples_per_second': '32.949', 'grad_norm': '17.375', 'counters/examples': 47648, 'counters/updates': 1489}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47712 examples: {'rewards_train/chosen': '0.095947', 'rewards_train/rejected': '0.064545', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031401', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-130.49', 'loss/train': '0.68', 'examples_per_second': '38.306', 'grad_norm': '19.75', 'counters/examples': 47712, 'counters/updates': 1491}
train stats after 47744 examples: {'rewards_train/chosen': '0.1057', 'rewards_train/rejected': '0.049923', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055774', 'logps_train/rejected': '-116.89', 'logps_train/chosen': '-117.65', 'loss/train': '0.66925', 'examples_per_second': '31.593', 'grad_norm': '19.875', 'counters/examples': 47744, 'counters/updates': 1492}
skipping logging after 47776 examples to avoid logging too frequently
train stats after 47808 examples: {'rewards_train/chosen': '0.020703', 'rewards_train/rejected': '0.01925', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0014537', 'logps_train/rejected': '-159.88', 'logps_train/chosen': '-141.07', 'loss/train': '0.69868', 'examples_per_second': '30.319', 'grad_norm': '22.375', 'counters/examples': 47808, 'counters/updates': 1494}
train stats after 47840 examples: {'rewards_train/chosen': '0.035934', 'rewards_train/rejected': '0.083788', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.047854', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-112.07', 'loss/train': '0.72072', 'examples_per_second': '30.139', 'grad_norm': '22.5', 'counters/examples': 47840, 'counters/updates': 1495}
skipping logging after 47872 examples to avoid logging too frequently
train stats after 47904 examples: {'rewards_train/chosen': '0.080284', 'rewards_train/rejected': '0.045475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034808', 'logps_train/rejected': '-141.69', 'logps_train/chosen': '-156.18', 'loss/train': '0.68164', 'examples_per_second': '30.352', 'grad_norm': '22.625', 'counters/examples': 47904, 'counters/updates': 1497}
train stats after 47936 examples: {'rewards_train/chosen': '0.093703', 'rewards_train/rejected': '0.040362', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053341', 'logps_train/rejected': '-106.31', 'logps_train/chosen': '-129.87', 'loss/train': '0.67089', 'examples_per_second': '31.659', 'grad_norm': '19.25', 'counters/examples': 47936, 'counters/updates': 1498}
train stats after 47968 examples: {'rewards_train/chosen': '0.035272', 'rewards_train/rejected': '-0.025193', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.060465', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-152.96', 'loss/train': '0.66574', 'examples_per_second': '30.516', 'grad_norm': '20.625', 'counters/examples': 47968, 'counters/updates': 1499}
skipping logging after 48000 examples to avoid logging too frequently
Running evaluation after 48000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.21it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.91it/s]
eval after 48000: {'rewards_eval/chosen': '0.086002', 'rewards_eval/rejected': '0.036703', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.049299', 'logps_eval/rejected': '-115.03', 'logps_eval/chosen': '-134.69', 'loss/eval': '0.67403'}
skipping save for non epoch
train stats after 48032 examples: {'rewards_train/chosen': '0.07767', 'rewards_train/rejected': '0.0095301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06814', 'logps_train/rejected': '-123.65', 'logps_train/chosen': '-122.41', 'loss/train': '0.66214', 'examples_per_second': '38.039', 'grad_norm': '20.625', 'counters/examples': 48032, 'counters/updates': 1501}
train stats after 48064 examples: {'rewards_train/chosen': '0.08059', 'rewards_train/rejected': '0.034524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046066', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-161.04', 'loss/train': '0.67483', 'examples_per_second': '32.277', 'grad_norm': '22', 'counters/examples': 48064, 'counters/updates': 1502}
skipping logging after 48096 examples to avoid logging too frequently
train stats after 48128 examples: {'rewards_train/chosen': '0.042815', 'rewards_train/rejected': '-0.0084464', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051262', 'logps_train/rejected': '-95.662', 'logps_train/chosen': '-124.45', 'loss/train': '0.67213', 'examples_per_second': '31.481', 'grad_norm': '18.5', 'counters/examples': 48128, 'counters/updates': 1504}
train stats after 48160 examples: {'rewards_train/chosen': '0.059081', 'rewards_train/rejected': '-0.011676', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070757', 'logps_train/rejected': '-99.464', 'logps_train/chosen': '-129.43', 'loss/train': '0.6613', 'examples_per_second': '31.049', 'grad_norm': '18', 'counters/examples': 48160, 'counters/updates': 1505}
train stats after 48192 examples: {'rewards_train/chosen': '0.045273', 'rewards_train/rejected': '0.067692', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022419', 'logps_train/rejected': '-95.715', 'logps_train/chosen': '-107.15', 'loss/train': '0.70901', 'examples_per_second': '31.372', 'grad_norm': '18.625', 'counters/examples': 48192, 'counters/updates': 1506}
train stats after 48224 examples: {'rewards_train/chosen': '0.11228', 'rewards_train/rejected': '0.05503', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057246', 'logps_train/rejected': '-122.22', 'logps_train/chosen': '-106.4', 'loss/train': '0.66788', 'examples_per_second': '31.147', 'grad_norm': '18.125', 'counters/examples': 48224, 'counters/updates': 1507}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48288 examples: {'rewards_train/chosen': '0.01971', 'rewards_train/rejected': '0.055145', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.035435', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-123.15', 'loss/train': '0.71821', 'examples_per_second': '31.293', 'grad_norm': '21.375', 'counters/examples': 48288, 'counters/updates': 1509}
train stats after 48320 examples: {'rewards_train/chosen': '0.029335', 'rewards_train/rejected': '0.015599', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013735', 'logps_train/rejected': '-117.33', 'logps_train/chosen': '-127.14', 'loss/train': '0.6918', 'examples_per_second': '30.694', 'grad_norm': '19.375', 'counters/examples': 48320, 'counters/updates': 1510}
train stats after 48352 examples: {'rewards_train/chosen': '0.037478', 'rewards_train/rejected': '-0.024327', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061805', 'logps_train/rejected': '-72.58', 'logps_train/chosen': '-130.54', 'loss/train': '0.66656', 'examples_per_second': '31.267', 'grad_norm': '17.875', 'counters/examples': 48352, 'counters/updates': 1511}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48416 examples: {'rewards_train/chosen': '0.034413', 'rewards_train/rejected': '0.023365', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011048', 'logps_train/rejected': '-117.6', 'logps_train/chosen': '-119.15', 'loss/train': '0.6935', 'examples_per_second': '32.515', 'grad_norm': '20.25', 'counters/examples': 48416, 'counters/updates': 1513}
train stats after 48448 examples: {'rewards_train/chosen': '0.05214', 'rewards_train/rejected': '0.044782', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0073572', 'logps_train/rejected': '-112.84', 'logps_train/chosen': '-148.2', 'loss/train': '0.69455', 'examples_per_second': '31.645', 'grad_norm': '21.625', 'counters/examples': 48448, 'counters/updates': 1514}
train stats after 48480 examples: {'rewards_train/chosen': '0.062102', 'rewards_train/rejected': '0.033384', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028718', 'logps_train/rejected': '-107.07', 'logps_train/chosen': '-159.1', 'loss/train': '0.68234', 'examples_per_second': '31.649', 'grad_norm': '20.625', 'counters/examples': 48480, 'counters/updates': 1515}
train stats after 48512 examples: {'rewards_train/chosen': '0.11729', 'rewards_train/rejected': '0.05799', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059302', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-120.65', 'loss/train': '0.6688', 'examples_per_second': '31.333', 'grad_norm': '19.375', 'counters/examples': 48512, 'counters/updates': 1516}
skipping logging after 48544 examples to avoid logging too frequently
train stats after 48576 examples: {'rewards_train/chosen': '0.11192', 'rewards_train/rejected': '0.053814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058104', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-145.77', 'loss/train': '0.67118', 'examples_per_second': '31.698', 'grad_norm': '21.375', 'counters/examples': 48576, 'counters/updates': 1518}
skipping logging after 48608 examples to avoid logging too frequently
train stats after 48640 examples: {'rewards_train/chosen': '0.077565', 'rewards_train/rejected': '0.063754', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013811', 'logps_train/rejected': '-133.19', 'logps_train/chosen': '-117.56', 'loss/train': '0.69191', 'examples_per_second': '32.102', 'grad_norm': '20.5', 'counters/examples': 48640, 'counters/updates': 1520}
train stats after 48672 examples: {'rewards_train/chosen': '0.12891', 'rewards_train/rejected': '0.026361', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10255', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-133.49', 'loss/train': '0.64894', 'examples_per_second': '31.67', 'grad_norm': '20.875', 'counters/examples': 48672, 'counters/updates': 1521}
train stats after 48704 examples: {'rewards_train/chosen': '0.067639', 'rewards_train/rejected': '0.010913', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056726', 'logps_train/rejected': '-87.43', 'logps_train/chosen': '-123.48', 'loss/train': '0.66837', 'examples_per_second': '31.651', 'grad_norm': '17.25', 'counters/examples': 48704, 'counters/updates': 1522}
skipping logging after 48736 examples to avoid logging too frequently
train stats after 48768 examples: {'rewards_train/chosen': '0.09555', 'rewards_train/rejected': '0.068157', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027393', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-157.52', 'loss/train': '0.68346', 'examples_per_second': '32.339', 'grad_norm': '22.75', 'counters/examples': 48768, 'counters/updates': 1524}
train stats after 48800 examples: {'rewards_train/chosen': '0.035189', 'rewards_train/rejected': '0.054402', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019213', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-163.38', 'loss/train': '0.71045', 'examples_per_second': '31.647', 'grad_norm': '23', 'counters/examples': 48800, 'counters/updates': 1525}
train stats after 48832 examples: {'rewards_train/chosen': '0.066715', 'rewards_train/rejected': '0.069425', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0027107', 'logps_train/rejected': '-144.03', 'logps_train/chosen': '-154.83', 'loss/train': '0.6979', 'examples_per_second': '31.552', 'grad_norm': '21.875', 'counters/examples': 48832, 'counters/updates': 1526}
train stats after 48864 examples: {'rewards_train/chosen': '0.072022', 'rewards_train/rejected': '0.04739', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024632', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-132.38', 'loss/train': '0.68509', 'examples_per_second': '31.719', 'grad_norm': '19.125', 'counters/examples': 48864, 'counters/updates': 1527}
train stats after 48896 examples: {'rewards_train/chosen': '0.036504', 'rewards_train/rejected': '0.029852', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0066521', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-147.95', 'loss/train': '0.69461', 'examples_per_second': '31.651', 'grad_norm': '22.125', 'counters/examples': 48896, 'counters/updates': 1528}
train stats after 48928 examples: {'rewards_train/chosen': '0.057658', 'rewards_train/rejected': '0.045385', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012273', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-172.63', 'loss/train': '0.69308', 'examples_per_second': '31.662', 'grad_norm': '22.5', 'counters/examples': 48928, 'counters/updates': 1529}
train stats after 48960 examples: {'rewards_train/chosen': '0.03812', 'rewards_train/rejected': '0.02632', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0118', 'logps_train/rejected': '-94.411', 'logps_train/chosen': '-147.51', 'loss/train': '0.6936', 'examples_per_second': '32.741', 'grad_norm': '19.75', 'counters/examples': 48960, 'counters/updates': 1530}
train stats after 48992 examples: {'rewards_train/chosen': '0.089831', 'rewards_train/rejected': '0.069325', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.020506', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-102.08', 'loss/train': '0.68895', 'examples_per_second': '31.48', 'grad_norm': '19.875', 'counters/examples': 48992, 'counters/updates': 1531}
train stats after 49024 examples: {'rewards_train/chosen': '0.097696', 'rewards_train/rejected': '-0.0104', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1081', 'logps_train/rejected': '-103.48', 'logps_train/chosen': '-123.7', 'loss/train': '0.64488', 'examples_per_second': '31.985', 'grad_norm': '18.125', 'counters/examples': 49024, 'counters/updates': 1532}
skipping logging after 49056 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '0.10718', 'rewards_train/rejected': '0.0065152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10066', 'logps_train/rejected': '-104.89', 'logps_train/chosen': '-122.75', 'loss/train': '0.6494', 'examples_per_second': '31.409', 'grad_norm': '18.625', 'counters/examples': 49088, 'counters/updates': 1534}
train stats after 49120 examples: {'rewards_train/chosen': '0.028923', 'rewards_train/rejected': '0.013507', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015417', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-137.52', 'loss/train': '0.69202', 'examples_per_second': '32.025', 'grad_norm': '20.375', 'counters/examples': 49120, 'counters/updates': 1535}
train stats after 49152 examples: {'rewards_train/chosen': '0.079009', 'rewards_train/rejected': '0.016342', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.062667', 'logps_train/rejected': '-107', 'logps_train/chosen': '-151.9', 'loss/train': '0.66644', 'examples_per_second': '31.547', 'grad_norm': '20.5', 'counters/examples': 49152, 'counters/updates': 1536}
train stats after 49184 examples: {'rewards_train/chosen': '0.047495', 'rewards_train/rejected': '0.039352', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0081429', 'logps_train/rejected': '-153.68', 'logps_train/chosen': '-172.13', 'loss/train': '0.69331', 'examples_per_second': '31.594', 'grad_norm': '23.375', 'counters/examples': 49184, 'counters/updates': 1537}
train stats after 49216 examples: {'rewards_train/chosen': '0.035151', 'rewards_train/rejected': '-0.0010004', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036152', 'logps_train/rejected': '-129.3', 'logps_train/chosen': '-170.03', 'loss/train': '0.67914', 'examples_per_second': '31.659', 'grad_norm': '23', 'counters/examples': 49216, 'counters/updates': 1538}
skipping logging after 49248 examples to avoid logging too frequently
train stats after 49280 examples: {'rewards_train/chosen': '0.10021', 'rewards_train/rejected': '0.029862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070347', 'logps_train/rejected': '-95.686', 'logps_train/chosen': '-123.25', 'loss/train': '0.6635', 'examples_per_second': '31.212', 'grad_norm': '18.25', 'counters/examples': 49280, 'counters/updates': 1540}
train stats after 49312 examples: {'rewards_train/chosen': '0.032555', 'rewards_train/rejected': '0.00046577', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032089', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-136.95', 'loss/train': '0.67918', 'examples_per_second': '31.664', 'grad_norm': '20.375', 'counters/examples': 49312, 'counters/updates': 1541}
train stats after 49344 examples: {'rewards_train/chosen': '0.082785', 'rewards_train/rejected': '0.026824', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055961', 'logps_train/rejected': '-100.76', 'logps_train/chosen': '-147', 'loss/train': '0.67199', 'examples_per_second': '30.545', 'grad_norm': '20.25', 'counters/examples': 49344, 'counters/updates': 1542}
train stats after 49376 examples: {'rewards_train/chosen': '0.09292', 'rewards_train/rejected': '0.09218', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00073979', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-163.31', 'loss/train': '0.69691', 'examples_per_second': '32.615', 'grad_norm': '23.5', 'counters/examples': 49376, 'counters/updates': 1543}
train stats after 49408 examples: {'rewards_train/chosen': '0.12456', 'rewards_train/rejected': '0.026467', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098094', 'logps_train/rejected': '-102.98', 'logps_train/chosen': '-146.03', 'loss/train': '0.64906', 'examples_per_second': '31.645', 'grad_norm': '18.875', 'counters/examples': 49408, 'counters/updates': 1544}
train stats after 49440 examples: {'rewards_train/chosen': '0.071143', 'rewards_train/rejected': '0.026734', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.04441', 'logps_train/rejected': '-99.306', 'logps_train/chosen': '-132.05', 'loss/train': '0.67336', 'examples_per_second': '32.42', 'grad_norm': '19.375', 'counters/examples': 49440, 'counters/updates': 1545}
skipping logging after 49472 examples to avoid logging too frequently
train stats after 49504 examples: {'rewards_train/chosen': '0.0074832', 'rewards_train/rejected': '0.0065972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00088601', 'logps_train/rejected': '-172.37', 'logps_train/chosen': '-124.1', 'loss/train': '0.69834', 'examples_per_second': '31.572', 'grad_norm': '23.5', 'counters/examples': 49504, 'counters/updates': 1547}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49568 examples: {'rewards_train/chosen': '0.10361', 'rewards_train/rejected': '0.079062', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024547', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-149.58', 'loss/train': '0.68542', 'examples_per_second': '31.647', 'grad_norm': '21.875', 'counters/examples': 49568, 'counters/updates': 1549}
train stats after 49600 examples: {'rewards_train/chosen': '0.068573', 'rewards_train/rejected': '0.040232', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02834', 'logps_train/rejected': '-100.29', 'logps_train/chosen': '-131.76', 'loss/train': '0.68299', 'examples_per_second': '30.28', 'grad_norm': '19.625', 'counters/examples': 49600, 'counters/updates': 1550}
skipping logging after 49632 examples to avoid logging too frequently
train stats after 49664 examples: {'rewards_train/chosen': '0.060395', 'rewards_train/rejected': '-0.020351', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080746', 'logps_train/rejected': '-101.72', 'logps_train/chosen': '-123.46', 'loss/train': '0.65756', 'examples_per_second': '34.446', 'grad_norm': '18.375', 'counters/examples': 49664, 'counters/updates': 1552}
skipping logging after 49696 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '0.084748', 'rewards_train/rejected': '0.068388', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01636', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-123.78', 'loss/train': '0.68938', 'examples_per_second': '31.669', 'grad_norm': '20.875', 'counters/examples': 49728, 'counters/updates': 1554}
skipping logging after 49760 examples to avoid logging too frequently
train stats after 49792 examples: {'rewards_train/chosen': '0.12185', 'rewards_train/rejected': '0.060302', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061552', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-163.16', 'loss/train': '0.66982', 'examples_per_second': '31.506', 'grad_norm': '20.5', 'counters/examples': 49792, 'counters/updates': 1556}
train stats after 49824 examples: {'rewards_train/chosen': '0.066598', 'rewards_train/rejected': '0.061197', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0054013', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-129.33', 'loss/train': '0.69376', 'examples_per_second': '31.202', 'grad_norm': '19.75', 'counters/examples': 49824, 'counters/updates': 1557}
skipping logging after 49856 examples to avoid logging too frequently
train stats after 49888 examples: {'rewards_train/chosen': '0.05973', 'rewards_train/rejected': '0.033529', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026201', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-130', 'loss/train': '0.68388', 'examples_per_second': '32.12', 'grad_norm': '20', 'counters/examples': 49888, 'counters/updates': 1559}
train stats after 49920 examples: {'rewards_train/chosen': '0.053504', 'rewards_train/rejected': '0.0088059', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044698', 'logps_train/rejected': '-101.56', 'logps_train/chosen': '-147.63', 'loss/train': '0.67416', 'examples_per_second': '30.026', 'grad_norm': '20.25', 'counters/examples': 49920, 'counters/updates': 1560}
train stats after 49952 examples: {'rewards_train/chosen': '0.10153', 'rewards_train/rejected': '0.054698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046834', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-126.86', 'loss/train': '0.67565', 'examples_per_second': '31.759', 'grad_norm': '20.625', 'counters/examples': 49952, 'counters/updates': 1561}
train stats after 49984 examples: {'rewards_train/chosen': '0.047852', 'rewards_train/rejected': '0.041426', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.006426', 'logps_train/rejected': '-93.743', 'logps_train/chosen': '-128.91', 'loss/train': '0.69487', 'examples_per_second': '31.453', 'grad_norm': '20.5', 'counters/examples': 49984, 'counters/updates': 1562}
train stats after 50016 examples: {'rewards_train/chosen': '0.089814', 'rewards_train/rejected': '0.095336', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.005522', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-105.08', 'loss/train': '0.69939', 'examples_per_second': '31.628', 'grad_norm': '19.625', 'counters/examples': 50016, 'counters/updates': 1563}
train stats after 50048 examples: {'rewards_train/chosen': '0.053621', 'rewards_train/rejected': '0.058063', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.004442', 'logps_train/rejected': '-131.61', 'logps_train/chosen': '-139.19', 'loss/train': '0.69822', 'examples_per_second': '31.626', 'grad_norm': '21.625', 'counters/examples': 50048, 'counters/updates': 1564}
train stats after 50080 examples: {'rewards_train/chosen': '0.1189', 'rewards_train/rejected': '0.069377', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049528', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-144.25', 'loss/train': '0.67355', 'examples_per_second': '31.58', 'grad_norm': '20.625', 'counters/examples': 50080, 'counters/updates': 1565}
train stats after 50112 examples: {'rewards_train/chosen': '0.064711', 'rewards_train/rejected': '0.098521', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033811', 'logps_train/rejected': '-135.69', 'logps_train/chosen': '-161', 'loss/train': '0.71684', 'examples_per_second': '30.169', 'grad_norm': '22.75', 'counters/examples': 50112, 'counters/updates': 1566}
train stats after 50144 examples: {'rewards_train/chosen': '0.064854', 'rewards_train/rejected': '0.015715', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049139', 'logps_train/rejected': '-131.13', 'logps_train/chosen': '-115.99', 'loss/train': '0.67309', 'examples_per_second': '31.679', 'grad_norm': '19.375', 'counters/examples': 50144, 'counters/updates': 1567}
train stats after 50176 examples: {'rewards_train/chosen': '0.054817', 'rewards_train/rejected': '0.033191', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021626', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-123.86', 'loss/train': '0.68671', 'examples_per_second': '32.743', 'grad_norm': '19.75', 'counters/examples': 50176, 'counters/updates': 1568}
skipping logging after 50208 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '0.071857', 'rewards_train/rejected': '0.06927', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0025873', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-121.7', 'loss/train': '0.69498', 'examples_per_second': '31.738', 'grad_norm': '19.5', 'counters/examples': 50240, 'counters/updates': 1570}
skipping logging after 50272 examples to avoid logging too frequently
train stats after 50304 examples: {'rewards_train/chosen': '0.072412', 'rewards_train/rejected': '0.036917', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035496', 'logps_train/rejected': '-101.78', 'logps_train/chosen': '-149.67', 'loss/train': '0.67904', 'examples_per_second': '31.319', 'grad_norm': '20', 'counters/examples': 50304, 'counters/updates': 1572}
train stats after 50336 examples: {'rewards_train/chosen': '0.10028', 'rewards_train/rejected': '0.009884', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090399', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-161.71', 'loss/train': '0.65311', 'examples_per_second': '31.321', 'grad_norm': '20.75', 'counters/examples': 50336, 'counters/updates': 1573}
skipping logging after 50368 examples to avoid logging too frequently
train stats after 50400 examples: {'rewards_train/chosen': '0.023526', 'rewards_train/rejected': '0.017678', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0058481', 'logps_train/rejected': '-100.83', 'logps_train/chosen': '-114.71', 'loss/train': '0.69379', 'examples_per_second': '31.276', 'grad_norm': '18.75', 'counters/examples': 50400, 'counters/updates': 1575}
train stats after 50432 examples: {'rewards_train/chosen': '0.086898', 'rewards_train/rejected': '0.042545', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.044353', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-145.52', 'loss/train': '0.67563', 'examples_per_second': '31.503', 'grad_norm': '21', 'counters/examples': 50432, 'counters/updates': 1576}
train stats after 50464 examples: {'rewards_train/chosen': '0.077141', 'rewards_train/rejected': '0.065793', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011348', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-130.15', 'loss/train': '0.69001', 'examples_per_second': '31.301', 'grad_norm': '20.875', 'counters/examples': 50464, 'counters/updates': 1577}
train stats after 50496 examples: {'rewards_train/chosen': '0.066089', 'rewards_train/rejected': '0.05771', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0083785', 'logps_train/rejected': '-157.56', 'logps_train/chosen': '-163.88', 'loss/train': '0.69396', 'examples_per_second': '31.357', 'grad_norm': '23.25', 'counters/examples': 50496, 'counters/updates': 1578}
train stats after 50528 examples: {'rewards_train/chosen': '0.066848', 'rewards_train/rejected': '0.042725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024123', 'logps_train/rejected': '-130.12', 'logps_train/chosen': '-127.63', 'loss/train': '0.68486', 'examples_per_second': '32.781', 'grad_norm': '20.625', 'counters/examples': 50528, 'counters/updates': 1579}
train stats after 50560 examples: {'rewards_train/chosen': '0.065291', 'rewards_train/rejected': '0.024257', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041034', 'logps_train/rejected': '-95.298', 'logps_train/chosen': '-102.89', 'loss/train': '0.6795', 'examples_per_second': '32.731', 'grad_norm': '17.25', 'counters/examples': 50560, 'counters/updates': 1580}
skipping logging after 50592 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '0.050065', 'rewards_train/rejected': '0.0039451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04612', 'logps_train/rejected': '-111.35', 'logps_train/chosen': '-116.61', 'loss/train': '0.67329', 'examples_per_second': '32.57', 'grad_norm': '19', 'counters/examples': 50624, 'counters/updates': 1582}
train stats after 50656 examples: {'rewards_train/chosen': '0.061183', 'rewards_train/rejected': '0.059663', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0015198', 'logps_train/rejected': '-117.15', 'logps_train/chosen': '-124.23', 'loss/train': '0.69615', 'examples_per_second': '30.602', 'grad_norm': '19.5', 'counters/examples': 50656, 'counters/updates': 1583}
train stats after 50688 examples: {'rewards_train/chosen': '0.066505', 'rewards_train/rejected': '0.023152', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043353', 'logps_train/rejected': '-118.05', 'logps_train/chosen': '-135.06', 'loss/train': '0.67691', 'examples_per_second': '31.615', 'grad_norm': '20.625', 'counters/examples': 50688, 'counters/updates': 1584}
train stats after 50720 examples: {'rewards_train/chosen': '0.12572', 'rewards_train/rejected': '0.061801', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063924', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-140.73', 'loss/train': '0.67117', 'examples_per_second': '31.834', 'grad_norm': '20.875', 'counters/examples': 50720, 'counters/updates': 1585}
train stats after 50752 examples: {'rewards_train/chosen': '0.082568', 'rewards_train/rejected': '0.062097', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02047', 'logps_train/rejected': '-128.6', 'logps_train/chosen': '-120.72', 'loss/train': '0.68841', 'examples_per_second': '31.652', 'grad_norm': '19.875', 'counters/examples': 50752, 'counters/updates': 1586}
train stats after 50784 examples: {'rewards_train/chosen': '0.071411', 'rewards_train/rejected': '0.037413', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033998', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-129.5', 'loss/train': '0.6808', 'examples_per_second': '31.238', 'grad_norm': '20.125', 'counters/examples': 50784, 'counters/updates': 1587}
train stats after 50816 examples: {'rewards_train/chosen': '0.054739', 'rewards_train/rejected': '0.029007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025732', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-146.94', 'loss/train': '0.68497', 'examples_per_second': '31.222', 'grad_norm': '19.875', 'counters/examples': 50816, 'counters/updates': 1588}
train stats after 50848 examples: {'rewards_train/chosen': '0.14883', 'rewards_train/rejected': '0.018497', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13033', 'logps_train/rejected': '-114.98', 'logps_train/chosen': '-158.88', 'loss/train': '0.63706', 'examples_per_second': '31.637', 'grad_norm': '19.875', 'counters/examples': 50848, 'counters/updates': 1589}
train stats after 50880 examples: {'rewards_train/chosen': '0.1068', 'rewards_train/rejected': '0.028795', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.078004', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-135.93', 'loss/train': '0.65733', 'examples_per_second': '31.083', 'grad_norm': '20.375', 'counters/examples': 50880, 'counters/updates': 1590}
train stats after 50912 examples: {'rewards_train/chosen': '0.11152', 'rewards_train/rejected': '0.031364', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080156', 'logps_train/rejected': '-133.05', 'logps_train/chosen': '-128.39', 'loss/train': '0.66053', 'examples_per_second': '30.506', 'grad_norm': '20.375', 'counters/examples': 50912, 'counters/updates': 1591}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 50976 examples: {'rewards_train/chosen': '0.0871', 'rewards_train/rejected': '0.027001', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060099', 'logps_train/rejected': '-108.59', 'logps_train/chosen': '-181.22', 'loss/train': '0.6675', 'examples_per_second': '31.671', 'grad_norm': '20.125', 'counters/examples': 50976, 'counters/updates': 1593}
skipping logging after 51008 examples to avoid logging too frequently
train stats after 51040 examples: {'rewards_train/chosen': '0.068028', 'rewards_train/rejected': '0.016043', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051986', 'logps_train/rejected': '-110.27', 'logps_train/chosen': '-148.02', 'loss/train': '0.67053', 'examples_per_second': '31.606', 'grad_norm': '20', 'counters/examples': 51040, 'counters/updates': 1595}
train stats after 51072 examples: {'rewards_train/chosen': '0.086121', 'rewards_train/rejected': '0.043849', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042272', 'logps_train/rejected': '-139.13', 'logps_train/chosen': '-143.5', 'loss/train': '0.67681', 'examples_per_second': '31.149', 'grad_norm': '21', 'counters/examples': 51072, 'counters/updates': 1596}
train stats after 51104 examples: {'rewards_train/chosen': '0.12923', 'rewards_train/rejected': '0.097933', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031293', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-179.94', 'loss/train': '0.68393', 'examples_per_second': '30.723', 'grad_norm': '22.625', 'counters/examples': 51104, 'counters/updates': 1597}
train stats after 51136 examples: {'rewards_train/chosen': '0.11344', 'rewards_train/rejected': '0.019441', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094', 'logps_train/rejected': '-86.995', 'logps_train/chosen': '-108.68', 'loss/train': '0.65239', 'examples_per_second': '31.621', 'grad_norm': '16.875', 'counters/examples': 51136, 'counters/updates': 1598}
train stats after 51168 examples: {'rewards_train/chosen': '0.052595', 'rewards_train/rejected': '0.044767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0078278', 'logps_train/rejected': '-134.73', 'logps_train/chosen': '-150.09', 'loss/train': '0.69283', 'examples_per_second': '31.068', 'grad_norm': '23.25', 'counters/examples': 51168, 'counters/updates': 1599}
train stats after 51200 examples: {'rewards_train/chosen': '0.081675', 'rewards_train/rejected': '0.021307', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060368', 'logps_train/rejected': '-96.02', 'logps_train/chosen': '-134.67', 'loss/train': '0.66716', 'examples_per_second': '30.491', 'grad_norm': '19.75', 'counters/examples': 51200, 'counters/updates': 1600}
train stats after 51232 examples: {'rewards_train/chosen': '0.062245', 'rewards_train/rejected': '0.014985', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04726', 'logps_train/rejected': '-110.47', 'logps_train/chosen': '-146.06', 'loss/train': '0.67683', 'examples_per_second': '31.611', 'grad_norm': '20.25', 'counters/examples': 51232, 'counters/updates': 1601}
skipping logging after 51264 examples to avoid logging too frequently
train stats after 51296 examples: {'rewards_train/chosen': '0.070774', 'rewards_train/rejected': '0.0024874', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068286', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-134.53', 'loss/train': '0.66411', 'examples_per_second': '31.536', 'grad_norm': '21', 'counters/examples': 51296, 'counters/updates': 1603}
train stats after 51328 examples: {'rewards_train/chosen': '0.10025', 'rewards_train/rejected': '0.025199', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075051', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-137.49', 'loss/train': '0.66307', 'examples_per_second': '30.231', 'grad_norm': '20.25', 'counters/examples': 51328, 'counters/updates': 1604}
train stats after 51360 examples: {'rewards_train/chosen': '0.091376', 'rewards_train/rejected': '-0.035304', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12668', 'logps_train/rejected': '-129.36', 'logps_train/chosen': '-164.46', 'loss/train': '0.6363', 'examples_per_second': '30.127', 'grad_norm': '20.375', 'counters/examples': 51360, 'counters/updates': 1605}
train stats after 51392 examples: {'rewards_train/chosen': '0.085671', 'rewards_train/rejected': '0.02255', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063121', 'logps_train/rejected': '-94.961', 'logps_train/chosen': '-127.03', 'loss/train': '0.66738', 'examples_per_second': '30.148', 'grad_norm': '18.5', 'counters/examples': 51392, 'counters/updates': 1606}
train stats after 51424 examples: {'rewards_train/chosen': '0.1003', 'rewards_train/rejected': '0.065789', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034511', 'logps_train/rejected': '-117.05', 'logps_train/chosen': '-105.01', 'loss/train': '0.68073', 'examples_per_second': '32.51', 'grad_norm': '19.5', 'counters/examples': 51424, 'counters/updates': 1607}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51488 examples: {'rewards_train/chosen': '0.083168', 'rewards_train/rejected': '0.038687', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04448', 'logps_train/rejected': '-76.932', 'logps_train/chosen': '-154.58', 'loss/train': '0.67709', 'examples_per_second': '30.475', 'grad_norm': '19.875', 'counters/examples': 51488, 'counters/updates': 1609}
train stats after 51520 examples: {'rewards_train/chosen': '0.091106', 'rewards_train/rejected': '0.067104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024003', 'logps_train/rejected': '-151.63', 'logps_train/chosen': '-128.16', 'loss/train': '0.68415', 'examples_per_second': '31.523', 'grad_norm': '20.875', 'counters/examples': 51520, 'counters/updates': 1610}
train stats after 51552 examples: {'rewards_train/chosen': '0.078174', 'rewards_train/rejected': '0.022889', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055285', 'logps_train/rejected': '-126.79', 'logps_train/chosen': '-117.1', 'loss/train': '0.66883', 'examples_per_second': '30.345', 'grad_norm': '20', 'counters/examples': 51552, 'counters/updates': 1611}
train stats after 51584 examples: {'rewards_train/chosen': '0.061141', 'rewards_train/rejected': '0.038', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023141', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-131.69', 'loss/train': '0.68594', 'examples_per_second': '31.808', 'grad_norm': '18.875', 'counters/examples': 51584, 'counters/updates': 1612}
train stats after 51616 examples: {'rewards_train/chosen': '0.10441', 'rewards_train/rejected': '0.046128', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058286', 'logps_train/rejected': '-99.934', 'logps_train/chosen': '-138.33', 'loss/train': '0.66685', 'examples_per_second': '32.039', 'grad_norm': '19.25', 'counters/examples': 51616, 'counters/updates': 1613}
train stats after 51648 examples: {'rewards_train/chosen': '0.11825', 'rewards_train/rejected': '0.057631', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060618', 'logps_train/rejected': '-118.46', 'logps_train/chosen': '-143.04', 'loss/train': '0.66891', 'examples_per_second': '31.475', 'grad_norm': '20.625', 'counters/examples': 51648, 'counters/updates': 1614}
train stats after 51680 examples: {'rewards_train/chosen': '0.059761', 'rewards_train/rejected': '0.039732', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020029', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-151.7', 'loss/train': '0.68836', 'examples_per_second': '30.986', 'grad_norm': '21.375', 'counters/examples': 51680, 'counters/updates': 1615}
train stats after 51712 examples: {'rewards_train/chosen': '0.094212', 'rewards_train/rejected': '0.013724', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.080488', 'logps_train/rejected': '-110.77', 'logps_train/chosen': '-169.1', 'loss/train': '0.65753', 'examples_per_second': '31.19', 'grad_norm': '20.875', 'counters/examples': 51712, 'counters/updates': 1616}
train stats after 51744 examples: {'rewards_train/chosen': '0.099297', 'rewards_train/rejected': '0.068897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0304', 'logps_train/rejected': '-111.67', 'logps_train/chosen': '-121.39', 'loss/train': '0.68226', 'examples_per_second': '31.425', 'grad_norm': '19.375', 'counters/examples': 51744, 'counters/updates': 1617}
train stats after 51776 examples: {'rewards_train/chosen': '0.086718', 'rewards_train/rejected': '0.11858', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.031862', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-117.52', 'loss/train': '0.71512', 'examples_per_second': '30.846', 'grad_norm': '20.75', 'counters/examples': 51776, 'counters/updates': 1618}
train stats after 51808 examples: {'rewards_train/chosen': '0.090554', 'rewards_train/rejected': '0.08966', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00089391', 'logps_train/rejected': '-181.88', 'logps_train/chosen': '-167.46', 'loss/train': '0.70004', 'examples_per_second': '30.46', 'grad_norm': '24.125', 'counters/examples': 51808, 'counters/updates': 1619}
train stats after 51840 examples: {'rewards_train/chosen': '0.057333', 'rewards_train/rejected': '0.056127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0012067', 'logps_train/rejected': '-94.291', 'logps_train/chosen': '-134.98', 'loss/train': '0.69683', 'examples_per_second': '31.522', 'grad_norm': '20.375', 'counters/examples': 51840, 'counters/updates': 1620}
train stats after 51872 examples: {'rewards_train/chosen': '0.10665', 'rewards_train/rejected': '0.063862', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042791', 'logps_train/rejected': '-114.41', 'logps_train/chosen': '-142.93', 'loss/train': '0.6766', 'examples_per_second': '32.332', 'grad_norm': '19.125', 'counters/examples': 51872, 'counters/updates': 1621}
train stats after 51904 examples: {'rewards_train/chosen': '0.10028', 'rewards_train/rejected': '0.022853', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07743', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-121.74', 'loss/train': '0.66188', 'examples_per_second': '25.87', 'grad_norm': '19', 'counters/examples': 51904, 'counters/updates': 1622}
train stats after 51936 examples: {'rewards_train/chosen': '0.091412', 'rewards_train/rejected': '0.050312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0411', 'logps_train/rejected': '-102.68', 'logps_train/chosen': '-123.22', 'loss/train': '0.67667', 'examples_per_second': '30.515', 'grad_norm': '19.75', 'counters/examples': 51936, 'counters/updates': 1623}
train stats after 51968 examples: {'rewards_train/chosen': '0.054313', 'rewards_train/rejected': '0.020628', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.033686', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-149.25', 'loss/train': '0.68018', 'examples_per_second': '30.186', 'grad_norm': '21', 'counters/examples': 51968, 'counters/updates': 1624}
train stats after 52000 examples: {'rewards_train/chosen': '0.029558', 'rewards_train/rejected': '0.03336', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0038019', 'logps_train/rejected': '-116.57', 'logps_train/chosen': '-147.53', 'loss/train': '0.70018', 'examples_per_second': '25.498', 'grad_norm': '20.5', 'counters/examples': 52000, 'counters/updates': 1625}
skipping logging after 52032 examples to avoid logging too frequently
train stats after 52064 examples: {'rewards_train/chosen': '0.096533', 'rewards_train/rejected': '0.049078', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047455', 'logps_train/rejected': '-102.08', 'logps_train/chosen': '-119.98', 'loss/train': '0.67353', 'examples_per_second': '30.802', 'grad_norm': '19.25', 'counters/examples': 52064, 'counters/updates': 1627}
train stats after 52096 examples: {'rewards_train/chosen': '0.057894', 'rewards_train/rejected': '0.042877', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015018', 'logps_train/rejected': '-114.98', 'logps_train/chosen': '-168.03', 'loss/train': '0.69057', 'examples_per_second': '31.281', 'grad_norm': '21.75', 'counters/examples': 52096, 'counters/updates': 1628}
train stats after 52128 examples: {'rewards_train/chosen': '0.091017', 'rewards_train/rejected': '0.047509', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043507', 'logps_train/rejected': '-144.37', 'logps_train/chosen': '-142.41', 'loss/train': '0.67605', 'examples_per_second': '31.289', 'grad_norm': '21.125', 'counters/examples': 52128, 'counters/updates': 1629}
train stats after 52160 examples: {'rewards_train/chosen': '0.085053', 'rewards_train/rejected': '0.0081381', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076915', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-134.33', 'loss/train': '0.65874', 'examples_per_second': '33.028', 'grad_norm': '20.625', 'counters/examples': 52160, 'counters/updates': 1630}
train stats after 52192 examples: {'rewards_train/chosen': '0.062945', 'rewards_train/rejected': '0.028902', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034043', 'logps_train/rejected': '-85.493', 'logps_train/chosen': '-112.88', 'loss/train': '0.67928', 'examples_per_second': '30.942', 'grad_norm': '17.625', 'counters/examples': 52192, 'counters/updates': 1631}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52256 examples: {'rewards_train/chosen': '0.13747', 'rewards_train/rejected': '0.0424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09507', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-163.66', 'loss/train': '0.65201', 'examples_per_second': '33.357', 'grad_norm': '20.25', 'counters/examples': 52256, 'counters/updates': 1633}
train stats after 52288 examples: {'rewards_train/chosen': '0.033201', 'rewards_train/rejected': '0.034257', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0010559', 'logps_train/rejected': '-160.34', 'logps_train/chosen': '-167.82', 'loss/train': '0.69836', 'examples_per_second': '31.438', 'grad_norm': '24.25', 'counters/examples': 52288, 'counters/updates': 1634}
train stats after 52320 examples: {'rewards_train/chosen': '0.1216', 'rewards_train/rejected': '0.043514', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.078085', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-127.19', 'loss/train': '0.6589', 'examples_per_second': '30.944', 'grad_norm': '20.125', 'counters/examples': 52320, 'counters/updates': 1635}
train stats after 52352 examples: {'rewards_train/chosen': '0.080252', 'rewards_train/rejected': '0.039169', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041083', 'logps_train/rejected': '-107.72', 'logps_train/chosen': '-106.5', 'loss/train': '0.67692', 'examples_per_second': '31.519', 'grad_norm': '18.875', 'counters/examples': 52352, 'counters/updates': 1636}
train stats after 52384 examples: {'rewards_train/chosen': '0.048275', 'rewards_train/rejected': '0.045081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0031938', 'logps_train/rejected': '-92.926', 'logps_train/chosen': '-114.87', 'loss/train': '0.69519', 'examples_per_second': '31.562', 'grad_norm': '19.625', 'counters/examples': 52384, 'counters/updates': 1637}
skipping logging after 52416 examples to avoid logging too frequently
train stats after 52448 examples: {'rewards_train/chosen': '0.048212', 'rewards_train/rejected': '0.053554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0053419', 'logps_train/rejected': '-102.89', 'logps_train/chosen': '-134.22', 'loss/train': '0.70144', 'examples_per_second': '31.045', 'grad_norm': '21', 'counters/examples': 52448, 'counters/updates': 1639}
train stats after 52480 examples: {'rewards_train/chosen': '0.12923', 'rewards_train/rejected': '0.03965', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08958', 'logps_train/rejected': '-135.57', 'logps_train/chosen': '-194.15', 'loss/train': '0.6529', 'examples_per_second': '31.456', 'grad_norm': '22.25', 'counters/examples': 52480, 'counters/updates': 1640}
skipping logging after 52512 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '0.050775', 'rewards_train/rejected': '-0.025686', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076461', 'logps_train/rejected': '-103.13', 'logps_train/chosen': '-83.73', 'loss/train': '0.66012', 'examples_per_second': '30.287', 'grad_norm': '17', 'counters/examples': 52544, 'counters/updates': 1642}
train stats after 52576 examples: {'rewards_train/chosen': '0.058921', 'rewards_train/rejected': '0.034539', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024382', 'logps_train/rejected': '-81.057', 'logps_train/chosen': '-110.14', 'loss/train': '0.6848', 'examples_per_second': '32.246', 'grad_norm': '17.625', 'counters/examples': 52576, 'counters/updates': 1643}
train stats after 52608 examples: {'rewards_train/chosen': '0.06722', 'rewards_train/rejected': '0.034194', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033026', 'logps_train/rejected': '-153.67', 'logps_train/chosen': '-138.35', 'loss/train': '0.68064', 'examples_per_second': '31.614', 'grad_norm': '21.625', 'counters/examples': 52608, 'counters/updates': 1644}
skipping logging after 52640 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '0.13131', 'rewards_train/rejected': '0.09105', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04026', 'logps_train/rejected': '-121.33', 'logps_train/chosen': '-140.75', 'loss/train': '0.67678', 'examples_per_second': '30.543', 'grad_norm': '20', 'counters/examples': 52672, 'counters/updates': 1646}
train stats after 52704 examples: {'rewards_train/chosen': '0.070684', 'rewards_train/rejected': '0.04697', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023713', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-106.29', 'loss/train': '0.68511', 'examples_per_second': '31.501', 'grad_norm': '18.5', 'counters/examples': 52704, 'counters/updates': 1647}
train stats after 52736 examples: {'rewards_train/chosen': '0.10618', 'rewards_train/rejected': '0.026024', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.080156', 'logps_train/rejected': '-107.47', 'logps_train/chosen': '-120.68', 'loss/train': '0.65777', 'examples_per_second': '30.064', 'grad_norm': '18.5', 'counters/examples': 52736, 'counters/updates': 1648}
train stats after 52768 examples: {'rewards_train/chosen': '0.12441', 'rewards_train/rejected': '0.071193', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053216', 'logps_train/rejected': '-119.34', 'logps_train/chosen': '-145.93', 'loss/train': '0.67066', 'examples_per_second': '29.853', 'grad_norm': '20.375', 'counters/examples': 52768, 'counters/updates': 1649}
train stats after 52800 examples: {'rewards_train/chosen': '0.12347', 'rewards_train/rejected': '0.053569', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069902', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-155.6', 'loss/train': '0.66375', 'examples_per_second': '31.493', 'grad_norm': '21', 'counters/examples': 52800, 'counters/updates': 1650}
train stats after 52832 examples: {'rewards_train/chosen': '0.064235', 'rewards_train/rejected': '0.060625', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0036104', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-158.95', 'loss/train': '0.69934', 'examples_per_second': '30.062', 'grad_norm': '22.25', 'counters/examples': 52832, 'counters/updates': 1651}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52896 examples: {'rewards_train/chosen': '0.055964', 'rewards_train/rejected': '0.027922', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028042', 'logps_train/rejected': '-150.05', 'logps_train/chosen': '-164.77', 'loss/train': '0.68217', 'examples_per_second': '31.248', 'grad_norm': '22.875', 'counters/examples': 52896, 'counters/updates': 1653}
train stats after 52928 examples: {'rewards_train/chosen': '0.063386', 'rewards_train/rejected': '-0.00094173', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064328', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-119.83', 'loss/train': '0.66457', 'examples_per_second': '30.03', 'grad_norm': '22.125', 'counters/examples': 52928, 'counters/updates': 1654}
train stats after 52960 examples: {'rewards_train/chosen': '0.12809', 'rewards_train/rejected': '0.048478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079612', 'logps_train/rejected': '-144.18', 'logps_train/chosen': '-116.64', 'loss/train': '0.66008', 'examples_per_second': '31.471', 'grad_norm': '21.25', 'counters/examples': 52960, 'counters/updates': 1655}
train stats after 52992 examples: {'rewards_train/chosen': '0.076379', 'rewards_train/rejected': '0.034669', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04171', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-215.38', 'loss/train': '0.67705', 'examples_per_second': '31.414', 'grad_norm': '24.875', 'counters/examples': 52992, 'counters/updates': 1656}
train stats after 53024 examples: {'rewards_train/chosen': '0.062291', 'rewards_train/rejected': '0.0095165', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052775', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-134.78', 'loss/train': '0.67207', 'examples_per_second': '30.687', 'grad_norm': '20.875', 'counters/examples': 53024, 'counters/updates': 1657}
train stats after 53056 examples: {'rewards_train/chosen': '0.092097', 'rewards_train/rejected': '0.028286', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063811', 'logps_train/rejected': '-91.25', 'logps_train/chosen': '-125.09', 'loss/train': '0.66625', 'examples_per_second': '24.847', 'grad_norm': '17.875', 'counters/examples': 53056, 'counters/updates': 1658}
train stats after 53088 examples: {'rewards_train/chosen': '0.040807', 'rewards_train/rejected': '-0.0056112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046419', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-110.6', 'loss/train': '0.67272', 'examples_per_second': '32.636', 'grad_norm': '19.875', 'counters/examples': 53088, 'counters/updates': 1659}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53152 examples: {'rewards_train/chosen': '0.091052', 'rewards_train/rejected': '0.067046', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024006', 'logps_train/rejected': '-87.832', 'logps_train/chosen': '-106.88', 'loss/train': '0.68384', 'examples_per_second': '30.336', 'grad_norm': '17.75', 'counters/examples': 53152, 'counters/updates': 1661}
train stats after 53184 examples: {'rewards_train/chosen': '0.048267', 'rewards_train/rejected': '0.0091313', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039136', 'logps_train/rejected': '-111.9', 'logps_train/chosen': '-111.38', 'loss/train': '0.67749', 'examples_per_second': '31.526', 'grad_norm': '18.625', 'counters/examples': 53184, 'counters/updates': 1662}
skipping logging after 53216 examples to avoid logging too frequently
train stats after 53248 examples: {'rewards_train/chosen': '0.11929', 'rewards_train/rejected': '0.073922', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045367', 'logps_train/rejected': '-130.23', 'logps_train/chosen': '-137.7', 'loss/train': '0.67453', 'examples_per_second': '31.142', 'grad_norm': '20.125', 'counters/examples': 53248, 'counters/updates': 1664}
train stats after 53280 examples: {'rewards_train/chosen': '0.050884', 'rewards_train/rejected': '0.035972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014912', 'logps_train/rejected': '-155.35', 'logps_train/chosen': '-136.25', 'loss/train': '0.69195', 'examples_per_second': '31.571', 'grad_norm': '21.25', 'counters/examples': 53280, 'counters/updates': 1665}
skipping logging after 53312 examples to avoid logging too frequently
train stats after 53344 examples: {'rewards_train/chosen': '0.073164', 'rewards_train/rejected': '0.04335', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029814', 'logps_train/rejected': '-159.2', 'logps_train/chosen': '-134.8', 'loss/train': '0.68422', 'examples_per_second': '31.521', 'grad_norm': '22', 'counters/examples': 53344, 'counters/updates': 1667}
train stats after 53376 examples: {'rewards_train/chosen': '0.070481', 'rewards_train/rejected': '0.024981', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0455', 'logps_train/rejected': '-106.78', 'logps_train/chosen': '-117.83', 'loss/train': '0.6735', 'examples_per_second': '32.45', 'grad_norm': '18.75', 'counters/examples': 53376, 'counters/updates': 1668}
train stats after 53408 examples: {'rewards_train/chosen': '0.093247', 'rewards_train/rejected': '-0.014352', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1076', 'logps_train/rejected': '-88.849', 'logps_train/chosen': '-124.37', 'loss/train': '0.64653', 'examples_per_second': '32.632', 'grad_norm': '18.125', 'counters/examples': 53408, 'counters/updates': 1669}
train stats after 53440 examples: {'rewards_train/chosen': '0.078146', 'rewards_train/rejected': '-0.0076953', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.085841', 'logps_train/rejected': '-131.76', 'logps_train/chosen': '-126.74', 'loss/train': '0.65558', 'examples_per_second': '31.533', 'grad_norm': '19.625', 'counters/examples': 53440, 'counters/updates': 1670}
train stats after 53472 examples: {'rewards_train/chosen': '0.094325', 'rewards_train/rejected': '0.043082', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051243', 'logps_train/rejected': '-129.25', 'logps_train/chosen': '-155.01', 'loss/train': '0.67222', 'examples_per_second': '30.49', 'grad_norm': '20.875', 'counters/examples': 53472, 'counters/updates': 1671}
train stats after 53504 examples: {'rewards_train/chosen': '0.090726', 'rewards_train/rejected': '0.034527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056199', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-117.16', 'loss/train': '0.66964', 'examples_per_second': '30.913', 'grad_norm': '18.375', 'counters/examples': 53504, 'counters/updates': 1672}
skipping logging after 53536 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '0.060934', 'rewards_train/rejected': '0.028418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032516', 'logps_train/rejected': '-120.15', 'logps_train/chosen': '-125.21', 'loss/train': '0.68107', 'examples_per_second': '34.045', 'grad_norm': '20.375', 'counters/examples': 53568, 'counters/updates': 1674}
train stats after 53600 examples: {'rewards_train/chosen': '0.057497', 'rewards_train/rejected': '0.012351', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045147', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-132.88', 'loss/train': '0.67692', 'examples_per_second': '32.189', 'grad_norm': '20.5', 'counters/examples': 53600, 'counters/updates': 1675}
train stats after 53632 examples: {'rewards_train/chosen': '0.079375', 'rewards_train/rejected': '0.01256', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066814', 'logps_train/rejected': '-110.94', 'logps_train/chosen': '-110.01', 'loss/train': '0.66443', 'examples_per_second': '32.497', 'grad_norm': '18.375', 'counters/examples': 53632, 'counters/updates': 1676}
train stats after 53664 examples: {'rewards_train/chosen': '0.10158', 'rewards_train/rejected': '0.033117', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068461', 'logps_train/rejected': '-114.94', 'logps_train/chosen': '-162.11', 'loss/train': '0.66673', 'examples_per_second': '31.009', 'grad_norm': '19.375', 'counters/examples': 53664, 'counters/updates': 1677}
train stats after 53696 examples: {'rewards_train/chosen': '0.036878', 'rewards_train/rejected': '0.04304', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0061612', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-129.1', 'loss/train': '0.70351', 'examples_per_second': '30.801', 'grad_norm': '19.75', 'counters/examples': 53696, 'counters/updates': 1678}
train stats after 53728 examples: {'rewards_train/chosen': '0.039743', 'rewards_train/rejected': '0.021851', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017892', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-127.5', 'loss/train': '0.68832', 'examples_per_second': '30.671', 'grad_norm': '21', 'counters/examples': 53728, 'counters/updates': 1679}
train stats after 53760 examples: {'rewards_train/chosen': '0.036555', 'rewards_train/rejected': '0.0040739', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032482', 'logps_train/rejected': '-94.218', 'logps_train/chosen': '-161.92', 'loss/train': '0.68058', 'examples_per_second': '31.485', 'grad_norm': '21.625', 'counters/examples': 53760, 'counters/updates': 1680}
train stats after 53792 examples: {'rewards_train/chosen': '0.17614', 'rewards_train/rejected': '0.060282', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11586', 'logps_train/rejected': '-166.62', 'logps_train/chosen': '-174.82', 'loss/train': '0.64401', 'examples_per_second': '32.352', 'grad_norm': '23.25', 'counters/examples': 53792, 'counters/updates': 1681}
train stats after 53824 examples: {'rewards_train/chosen': '0.081112', 'rewards_train/rejected': '0.044907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036205', 'logps_train/rejected': '-148', 'logps_train/chosen': '-186.68', 'loss/train': '0.68163', 'examples_per_second': '31.483', 'grad_norm': '23.5', 'counters/examples': 53824, 'counters/updates': 1682}
train stats after 53856 examples: {'rewards_train/chosen': '0.057133', 'rewards_train/rejected': '0.028153', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02898', 'logps_train/rejected': '-75.431', 'logps_train/chosen': '-149.19', 'loss/train': '0.6815', 'examples_per_second': '31.51', 'grad_norm': '19.625', 'counters/examples': 53856, 'counters/updates': 1683}
train stats after 53888 examples: {'rewards_train/chosen': '0.073988', 'rewards_train/rejected': '0.067735', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0062527', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-143.07', 'loss/train': '0.6962', 'examples_per_second': '29.714', 'grad_norm': '20.5', 'counters/examples': 53888, 'counters/updates': 1684}
skipping logging after 53920 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '0.046949', 'rewards_train/rejected': '0.049511', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0025624', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-167.94', 'loss/train': '0.69964', 'examples_per_second': '31.496', 'grad_norm': '23.625', 'counters/examples': 53952, 'counters/updates': 1686}
train stats after 53984 examples: {'rewards_train/chosen': '0.076195', 'rewards_train/rejected': '0.068446', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0077488', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-147.33', 'loss/train': '0.69777', 'examples_per_second': '30.932', 'grad_norm': '20.875', 'counters/examples': 53984, 'counters/updates': 1687}
train stats after 54016 examples: {'rewards_train/chosen': '0.072138', 'rewards_train/rejected': '0.035021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037117', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-123.32', 'loss/train': '0.6816', 'examples_per_second': '32.992', 'grad_norm': '18.375', 'counters/examples': 54016, 'counters/updates': 1688}
train stats after 54048 examples: {'rewards_train/chosen': '0.08971', 'rewards_train/rejected': '0.034761', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.054949', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-130.1', 'loss/train': '0.67053', 'examples_per_second': '31.587', 'grad_norm': '19.75', 'counters/examples': 54048, 'counters/updates': 1689}
train stats after 54080 examples: {'rewards_train/chosen': '0.10657', 'rewards_train/rejected': '0.033013', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073559', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-153.63', 'loss/train': '0.66048', 'examples_per_second': '31.384', 'grad_norm': '21.25', 'counters/examples': 54080, 'counters/updates': 1690}
skipping logging after 54112 examples to avoid logging too frequently
train stats after 54144 examples: {'rewards_train/chosen': '0.10781', 'rewards_train/rejected': '0.052638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055168', 'logps_train/rejected': '-92.324', 'logps_train/chosen': '-152.37', 'loss/train': '0.67172', 'examples_per_second': '30.654', 'grad_norm': '20.5', 'counters/examples': 54144, 'counters/updates': 1692}
train stats after 54176 examples: {'rewards_train/chosen': '0.05134', 'rewards_train/rejected': '0.06264', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011299', 'logps_train/rejected': '-140.02', 'logps_train/chosen': '-121.11', 'loss/train': '0.70352', 'examples_per_second': '31.442', 'grad_norm': '21.5', 'counters/examples': 54176, 'counters/updates': 1693}
train stats after 54208 examples: {'rewards_train/chosen': '0.043735', 'rewards_train/rejected': '0.058727', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014991', 'logps_train/rejected': '-133.27', 'logps_train/chosen': '-120.69', 'loss/train': '0.70991', 'examples_per_second': '32.442', 'grad_norm': '20.875', 'counters/examples': 54208, 'counters/updates': 1694}
skipping logging after 54240 examples to avoid logging too frequently
train stats after 54272 examples: {'rewards_train/chosen': '0.14154', 'rewards_train/rejected': '0.053292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088246', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-162.27', 'loss/train': '0.65629', 'examples_per_second': '34.143', 'grad_norm': '21.375', 'counters/examples': 54272, 'counters/updates': 1696}
train stats after 54304 examples: {'rewards_train/chosen': '0.021963', 'rewards_train/rejected': '0.080136', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.058173', 'logps_train/rejected': '-122.06', 'logps_train/chosen': '-135.51', 'loss/train': '0.72564', 'examples_per_second': '31.489', 'grad_norm': '22.125', 'counters/examples': 54304, 'counters/updates': 1697}
train stats after 54336 examples: {'rewards_train/chosen': '0.088584', 'rewards_train/rejected': '0.02872', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.059864', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-144.35', 'loss/train': '0.6685', 'examples_per_second': '31.517', 'grad_norm': '20.25', 'counters/examples': 54336, 'counters/updates': 1698}
train stats after 54368 examples: {'rewards_train/chosen': '0.10419', 'rewards_train/rejected': '0.054154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050034', 'logps_train/rejected': '-96.505', 'logps_train/chosen': '-172.15', 'loss/train': '0.67113', 'examples_per_second': '30.933', 'grad_norm': '20.875', 'counters/examples': 54368, 'counters/updates': 1699}
train stats after 54400 examples: {'rewards_train/chosen': '0.086257', 'rewards_train/rejected': '0.044651', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041606', 'logps_train/rejected': '-126.85', 'logps_train/chosen': '-135.58', 'loss/train': '0.67784', 'examples_per_second': '31.401', 'grad_norm': '20.625', 'counters/examples': 54400, 'counters/updates': 1700}
train stats after 54432 examples: {'rewards_train/chosen': '0.045935', 'rewards_train/rejected': '0.046391', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00045671', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-180.17', 'loss/train': '0.69857', 'examples_per_second': '31.409', 'grad_norm': '21.5', 'counters/examples': 54432, 'counters/updates': 1701}
train stats after 54464 examples: {'rewards_train/chosen': '0.11867', 'rewards_train/rejected': '0.031757', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086914', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-164.62', 'loss/train': '0.65494', 'examples_per_second': '31.5', 'grad_norm': '21.5', 'counters/examples': 54464, 'counters/updates': 1702}
train stats after 54496 examples: {'rewards_train/chosen': '0.037098', 'rewards_train/rejected': '-0.015808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052906', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-149.37', 'loss/train': '0.67363', 'examples_per_second': '30.091', 'grad_norm': '20.75', 'counters/examples': 54496, 'counters/updates': 1703}
train stats after 54528 examples: {'rewards_train/chosen': '0.042174', 'rewards_train/rejected': '0.03536', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0068138', 'logps_train/rejected': '-156.23', 'logps_train/chosen': '-121.09', 'loss/train': '0.6943', 'examples_per_second': '29.998', 'grad_norm': '21.25', 'counters/examples': 54528, 'counters/updates': 1704}
train stats after 54560 examples: {'rewards_train/chosen': '0.12649', 'rewards_train/rejected': '0.0095297', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11696', 'logps_train/rejected': '-99.859', 'logps_train/chosen': '-193.67', 'loss/train': '0.64058', 'examples_per_second': '31.448', 'grad_norm': '22.375', 'counters/examples': 54560, 'counters/updates': 1705}
train stats after 54592 examples: {'rewards_train/chosen': '0.10284', 'rewards_train/rejected': '0.026662', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076182', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-155.58', 'loss/train': '0.66337', 'examples_per_second': '30.407', 'grad_norm': '21.375', 'counters/examples': 54592, 'counters/updates': 1706}
train stats after 54624 examples: {'rewards_train/chosen': '0.096765', 'rewards_train/rejected': '0.076785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01998', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-137.07', 'loss/train': '0.68752', 'examples_per_second': '31.492', 'grad_norm': '20.5', 'counters/examples': 54624, 'counters/updates': 1707}
train stats after 54656 examples: {'rewards_train/chosen': '0.057071', 'rewards_train/rejected': '0.030609', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026461', 'logps_train/rejected': '-110.83', 'logps_train/chosen': '-131.64', 'loss/train': '0.68245', 'examples_per_second': '31.586', 'grad_norm': '20.25', 'counters/examples': 54656, 'counters/updates': 1708}
train stats after 54688 examples: {'rewards_train/chosen': '0.05323', 'rewards_train/rejected': '0.0046014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048629', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-163.34', 'loss/train': '0.67466', 'examples_per_second': '32.235', 'grad_norm': '22.5', 'counters/examples': 54688, 'counters/updates': 1709}
train stats after 54720 examples: {'rewards_train/chosen': '0.077976', 'rewards_train/rejected': '-0.0083546', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086331', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-114.06', 'loss/train': '0.65554', 'examples_per_second': '30.086', 'grad_norm': '18.75', 'counters/examples': 54720, 'counters/updates': 1710}
skipping logging after 54752 examples to avoid logging too frequently
train stats after 54784 examples: {'rewards_train/chosen': '0.045952', 'rewards_train/rejected': '0.031859', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014093', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-142.45', 'loss/train': '0.69109', 'examples_per_second': '32.253', 'grad_norm': '20.75', 'counters/examples': 54784, 'counters/updates': 1712}
train stats after 54816 examples: {'rewards_train/chosen': '0.072954', 'rewards_train/rejected': '0.035532', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037421', 'logps_train/rejected': '-166.43', 'logps_train/chosen': '-116.01', 'loss/train': '0.68136', 'examples_per_second': '32.06', 'grad_norm': '21.375', 'counters/examples': 54816, 'counters/updates': 1713}
train stats after 54848 examples: {'rewards_train/chosen': '0.1052', 'rewards_train/rejected': '0.069903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035299', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-156.77', 'loss/train': '0.68259', 'examples_per_second': '30.737', 'grad_norm': '20.875', 'counters/examples': 54848, 'counters/updates': 1714}
skipping logging after 54880 examples to avoid logging too frequently
train stats after 54912 examples: {'rewards_train/chosen': '0.098448', 'rewards_train/rejected': '0.031335', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067113', 'logps_train/rejected': '-103', 'logps_train/chosen': '-149.77', 'loss/train': '0.66554', 'examples_per_second': '33.127', 'grad_norm': '21', 'counters/examples': 54912, 'counters/updates': 1716}
train stats after 54944 examples: {'rewards_train/chosen': '0.086226', 'rewards_train/rejected': '0.048971', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037255', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-128.57', 'loss/train': '0.67974', 'examples_per_second': '32.294', 'grad_norm': '20.5', 'counters/examples': 54944, 'counters/updates': 1717}
train stats after 54976 examples: {'rewards_train/chosen': '0.10499', 'rewards_train/rejected': '0.056624', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04837', 'logps_train/rejected': '-116.42', 'logps_train/chosen': '-141.5', 'loss/train': '0.67471', 'examples_per_second': '31.529', 'grad_norm': '19.75', 'counters/examples': 54976, 'counters/updates': 1718}
skipping logging after 55008 examples to avoid logging too frequently
train stats after 55040 examples: {'rewards_train/chosen': '0.090053', 'rewards_train/rejected': '0.054721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035332', 'logps_train/rejected': '-131.07', 'logps_train/chosen': '-152.41', 'loss/train': '0.67993', 'examples_per_second': '32.785', 'grad_norm': '23.25', 'counters/examples': 55040, 'counters/updates': 1720}
train stats after 55072 examples: {'rewards_train/chosen': '0.077691', 'rewards_train/rejected': '0.035155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042536', 'logps_train/rejected': '-83.116', 'logps_train/chosen': '-102.2', 'loss/train': '0.67588', 'examples_per_second': '31.478', 'grad_norm': '17', 'counters/examples': 55072, 'counters/updates': 1721}
train stats after 55104 examples: {'rewards_train/chosen': '0.11609', 'rewards_train/rejected': '0.031696', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.084392', 'logps_train/rejected': '-102.36', 'logps_train/chosen': '-105.99', 'loss/train': '0.65612', 'examples_per_second': '32.623', 'grad_norm': '18', 'counters/examples': 55104, 'counters/updates': 1722}
train stats after 55136 examples: {'rewards_train/chosen': '0.096838', 'rewards_train/rejected': '0.068024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028813', 'logps_train/rejected': '-116.41', 'logps_train/chosen': '-137.82', 'loss/train': '0.68367', 'examples_per_second': '31.508', 'grad_norm': '20.125', 'counters/examples': 55136, 'counters/updates': 1723}
train stats after 55168 examples: {'rewards_train/chosen': '0.073972', 'rewards_train/rejected': '0.093539', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019568', 'logps_train/rejected': '-128.51', 'logps_train/chosen': '-150.85', 'loss/train': '0.70787', 'examples_per_second': '30.277', 'grad_norm': '22.125', 'counters/examples': 55168, 'counters/updates': 1724}
train stats after 55200 examples: {'rewards_train/chosen': '0.078284', 'rewards_train/rejected': '0.056667', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021617', 'logps_train/rejected': '-89.578', 'logps_train/chosen': '-161.66', 'loss/train': '0.6878', 'examples_per_second': '31.502', 'grad_norm': '20.625', 'counters/examples': 55200, 'counters/updates': 1725}
train stats after 55232 examples: {'rewards_train/chosen': '0.063735', 'rewards_train/rejected': '0.053239', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010495', 'logps_train/rejected': '-165.28', 'logps_train/chosen': '-157.74', 'loss/train': '0.6936', 'examples_per_second': '32.33', 'grad_norm': '22.5', 'counters/examples': 55232, 'counters/updates': 1726}
train stats after 55264 examples: {'rewards_train/chosen': '0.11814', 'rewards_train/rejected': '0.028384', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.089757', 'logps_train/rejected': '-104.71', 'logps_train/chosen': '-138.37', 'loss/train': '0.65396', 'examples_per_second': '31.681', 'grad_norm': '19.375', 'counters/examples': 55264, 'counters/updates': 1727}
train stats after 55296 examples: {'rewards_train/chosen': '0.069014', 'rewards_train/rejected': '0.012849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056165', 'logps_train/rejected': '-96.36', 'logps_train/chosen': '-133.57', 'loss/train': '0.672', 'examples_per_second': '31.771', 'grad_norm': '18.875', 'counters/examples': 55296, 'counters/updates': 1728}
skipping logging after 55328 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '0.069039', 'rewards_train/rejected': '0.0027893', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.06625', 'logps_train/rejected': '-141.46', 'logps_train/chosen': '-139.09', 'loss/train': '0.66482', 'examples_per_second': '30.083', 'grad_norm': '25', 'counters/examples': 55360, 'counters/updates': 1730}
train stats after 55392 examples: {'rewards_train/chosen': '0.024004', 'rewards_train/rejected': '-0.0083727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032377', 'logps_train/rejected': '-98.039', 'logps_train/chosen': '-116.12', 'loss/train': '0.6811', 'examples_per_second': '30.172', 'grad_norm': '18.5', 'counters/examples': 55392, 'counters/updates': 1731}
train stats after 55424 examples: {'rewards_train/chosen': '0.049403', 'rewards_train/rejected': '0.096475', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.047072', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-128.12', 'loss/train': '0.72248', 'examples_per_second': '32.707', 'grad_norm': '20.125', 'counters/examples': 55424, 'counters/updates': 1732}
train stats after 55456 examples: {'rewards_train/chosen': '0.059232', 'rewards_train/rejected': '0.032021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027211', 'logps_train/rejected': '-167.27', 'logps_train/chosen': '-198.88', 'loss/train': '0.68531', 'examples_per_second': '30.479', 'grad_norm': '24.125', 'counters/examples': 55456, 'counters/updates': 1733}
train stats after 55488 examples: {'rewards_train/chosen': '0.086191', 'rewards_train/rejected': '0.034431', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05176', 'logps_train/rejected': '-126.07', 'logps_train/chosen': '-148.46', 'loss/train': '0.67411', 'examples_per_second': '30.688', 'grad_norm': '21.75', 'counters/examples': 55488, 'counters/updates': 1734}
skipping logging after 55520 examples to avoid logging too frequently
train stats after 55552 examples: {'rewards_train/chosen': '0.045894', 'rewards_train/rejected': '0.018528', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027366', 'logps_train/rejected': '-153.05', 'logps_train/chosen': '-167.95', 'loss/train': '0.68473', 'examples_per_second': '31.393', 'grad_norm': '22.75', 'counters/examples': 55552, 'counters/updates': 1736}
skipping logging after 55584 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '0.069481', 'rewards_train/rejected': '-0.013232', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.082714', 'logps_train/rejected': '-122.89', 'logps_train/chosen': '-143.36', 'loss/train': '0.65678', 'examples_per_second': '30.11', 'grad_norm': '20.875', 'counters/examples': 55616, 'counters/updates': 1738}
skipping logging after 55648 examples to avoid logging too frequently
train stats after 55680 examples: {'rewards_train/chosen': '0.095629', 'rewards_train/rejected': '0.008746', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086883', 'logps_train/rejected': '-145.77', 'logps_train/chosen': '-122.14', 'loss/train': '0.65473', 'examples_per_second': '31.382', 'grad_norm': '20', 'counters/examples': 55680, 'counters/updates': 1740}
train stats after 55712 examples: {'rewards_train/chosen': '0.049472', 'rewards_train/rejected': '0.03645', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013022', 'logps_train/rejected': '-115.53', 'logps_train/chosen': '-156.46', 'loss/train': '0.69221', 'examples_per_second': '30.491', 'grad_norm': '21.75', 'counters/examples': 55712, 'counters/updates': 1741}
train stats after 55744 examples: {'rewards_train/chosen': '0.061141', 'rewards_train/rejected': '0.018844', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042297', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-123.83', 'loss/train': '0.67825', 'examples_per_second': '31.574', 'grad_norm': '19', 'counters/examples': 55744, 'counters/updates': 1742}
train stats after 55776 examples: {'rewards_train/chosen': '0.064982', 'rewards_train/rejected': '0.068173', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0031904', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-149.25', 'loss/train': '0.70227', 'examples_per_second': '31.499', 'grad_norm': '21.25', 'counters/examples': 55776, 'counters/updates': 1743}
train stats after 55808 examples: {'rewards_train/chosen': '0.059636', 'rewards_train/rejected': '0.017318', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042318', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-142.29', 'loss/train': '0.67486', 'examples_per_second': '31.439', 'grad_norm': '20.625', 'counters/examples': 55808, 'counters/updates': 1744}
skipping logging after 55840 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '0.018287', 'rewards_train/rejected': '-0.0071938', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02548', 'logps_train/rejected': '-142.87', 'logps_train/chosen': '-136.88', 'loss/train': '0.68501', 'examples_per_second': '30.022', 'grad_norm': '20.875', 'counters/examples': 55872, 'counters/updates': 1746}
train stats after 55904 examples: {'rewards_train/chosen': '0.10913', 'rewards_train/rejected': '-0.010795', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11992', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-163.21', 'loss/train': '0.63871', 'examples_per_second': '31.751', 'grad_norm': '19.875', 'counters/examples': 55904, 'counters/updates': 1747}
train stats after 55936 examples: {'rewards_train/chosen': '0.13581', 'rewards_train/rejected': '0.097226', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038582', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-118.72', 'loss/train': '0.67854', 'examples_per_second': '30.976', 'grad_norm': '20.875', 'counters/examples': 55936, 'counters/updates': 1748}
train stats after 55968 examples: {'rewards_train/chosen': '0.060114', 'rewards_train/rejected': '0.055085', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0050291', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-143.45', 'loss/train': '0.69522', 'examples_per_second': '31.033', 'grad_norm': '20.375', 'counters/examples': 55968, 'counters/updates': 1749}
train stats after 56000 examples: {'rewards_train/chosen': '0.08708', 'rewards_train/rejected': '0.064409', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022671', 'logps_train/rejected': '-107.14', 'logps_train/chosen': '-153.94', 'loss/train': '0.68689', 'examples_per_second': '31.222', 'grad_norm': '22.125', 'counters/examples': 56000, 'counters/updates': 1750}
train stats after 56032 examples: {'rewards_train/chosen': '0.061803', 'rewards_train/rejected': '0.041554', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020249', 'logps_train/rejected': '-83.482', 'logps_train/chosen': '-110.3', 'loss/train': '0.68688', 'examples_per_second': '32.502', 'grad_norm': '17.5', 'counters/examples': 56032, 'counters/updates': 1751}
train stats after 56064 examples: {'rewards_train/chosen': '0.084471', 'rewards_train/rejected': '0.01394', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070531', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-156.42', 'loss/train': '0.66534', 'examples_per_second': '30.194', 'grad_norm': '20.75', 'counters/examples': 56064, 'counters/updates': 1752}
train stats after 56096 examples: {'rewards_train/chosen': '0.13356', 'rewards_train/rejected': '0.082638', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050922', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-158.9', 'loss/train': '0.67546', 'examples_per_second': '30.542', 'grad_norm': '22.125', 'counters/examples': 56096, 'counters/updates': 1753}
train stats after 56128 examples: {'rewards_train/chosen': '0.054784', 'rewards_train/rejected': '0.031148', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023636', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-137.1', 'loss/train': '0.68712', 'examples_per_second': '31.232', 'grad_norm': '20.625', 'counters/examples': 56128, 'counters/updates': 1754}
train stats after 56160 examples: {'rewards_train/chosen': '0.088605', 'rewards_train/rejected': '0.0077005', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080904', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-135.68', 'loss/train': '0.65873', 'examples_per_second': '31.892', 'grad_norm': '19.25', 'counters/examples': 56160, 'counters/updates': 1755}
train stats after 56192 examples: {'rewards_train/chosen': '0.11517', 'rewards_train/rejected': '0.068915', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046254', 'logps_train/rejected': '-132.33', 'logps_train/chosen': '-161.24', 'loss/train': '0.67612', 'examples_per_second': '31.257', 'grad_norm': '21.875', 'counters/examples': 56192, 'counters/updates': 1756}
skipping logging after 56224 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '0.062834', 'rewards_train/rejected': '0.048929', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013905', 'logps_train/rejected': '-165.98', 'logps_train/chosen': '-136.5', 'loss/train': '0.69068', 'examples_per_second': '30.782', 'grad_norm': '22', 'counters/examples': 56256, 'counters/updates': 1758}
train stats after 56288 examples: {'rewards_train/chosen': '0.10617', 'rewards_train/rejected': '0.0021188', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10406', 'logps_train/rejected': '-114.93', 'logps_train/chosen': '-125.1', 'loss/train': '0.64741', 'examples_per_second': '32.962', 'grad_norm': '17.75', 'counters/examples': 56288, 'counters/updates': 1759}
train stats after 56320 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '0.040866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094915', 'logps_train/rejected': '-98.939', 'logps_train/chosen': '-132.71', 'loss/train': '0.65095', 'examples_per_second': '31.069', 'grad_norm': '18.5', 'counters/examples': 56320, 'counters/updates': 1760}
train stats after 56352 examples: {'rewards_train/chosen': '0.094785', 'rewards_train/rejected': '0.058209', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.036576', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-140.27', 'loss/train': '0.68008', 'examples_per_second': '31.538', 'grad_norm': '20.375', 'counters/examples': 56352, 'counters/updates': 1761}
train stats after 56384 examples: {'rewards_train/chosen': '0.085647', 'rewards_train/rejected': '0.021227', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064421', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-163.02', 'loss/train': '0.66837', 'examples_per_second': '31.383', 'grad_norm': '21.625', 'counters/examples': 56384, 'counters/updates': 1762}
skipping logging after 56416 examples to avoid logging too frequently
train stats after 56448 examples: {'rewards_train/chosen': '0.058151', 'rewards_train/rejected': '0.050074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.008077', 'logps_train/rejected': '-93.463', 'logps_train/chosen': '-130.83', 'loss/train': '0.6933', 'examples_per_second': '33.453', 'grad_norm': '19.125', 'counters/examples': 56448, 'counters/updates': 1764}
train stats after 56480 examples: {'rewards_train/chosen': '0.12665', 'rewards_train/rejected': '0.019016', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10763', 'logps_train/rejected': '-100.96', 'logps_train/chosen': '-135.16', 'loss/train': '0.64715', 'examples_per_second': '30.461', 'grad_norm': '18.75', 'counters/examples': 56480, 'counters/updates': 1765}
skipping logging after 56512 examples to avoid logging too frequently
train stats after 56544 examples: {'rewards_train/chosen': '0.090309', 'rewards_train/rejected': '0.05035', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039959', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-145.61', 'loss/train': '0.67697', 'examples_per_second': '32.015', 'grad_norm': '21.5', 'counters/examples': 56544, 'counters/updates': 1767}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56608 examples: {'rewards_train/chosen': '0.061261', 'rewards_train/rejected': '0.0048212', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.05644', 'logps_train/rejected': '-101.14', 'logps_train/chosen': '-122.79', 'loss/train': '0.67027', 'examples_per_second': '33.175', 'grad_norm': '18.125', 'counters/examples': 56608, 'counters/updates': 1769}
skipping logging after 56640 examples to avoid logging too frequently
train stats after 56672 examples: {'rewards_train/chosen': '0.095505', 'rewards_train/rejected': '0.043355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052151', 'logps_train/rejected': '-94.014', 'logps_train/chosen': '-120.82', 'loss/train': '0.67053', 'examples_per_second': '30.164', 'grad_norm': '18.625', 'counters/examples': 56672, 'counters/updates': 1771}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56736 examples: {'rewards_train/chosen': '0.096804', 'rewards_train/rejected': '0.052126', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044678', 'logps_train/rejected': '-96.62', 'logps_train/chosen': '-124.74', 'loss/train': '0.67693', 'examples_per_second': '31.929', 'grad_norm': '18.375', 'counters/examples': 56736, 'counters/updates': 1773}
train stats after 56768 examples: {'rewards_train/chosen': '0.13616', 'rewards_train/rejected': '0.078261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057902', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-152.25', 'loss/train': '0.67034', 'examples_per_second': '31.536', 'grad_norm': '20.75', 'counters/examples': 56768, 'counters/updates': 1774}
skipping logging after 56800 examples to avoid logging too frequently
train stats after 56832 examples: {'rewards_train/chosen': '0.083158', 'rewards_train/rejected': '0.067012', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016146', 'logps_train/rejected': '-82.046', 'logps_train/chosen': '-104.28', 'loss/train': '0.68719', 'examples_per_second': '32.043', 'grad_norm': '17.375', 'counters/examples': 56832, 'counters/updates': 1776}
skipping logging after 56864 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '0.079524', 'rewards_train/rejected': '-0.012653', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092177', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-133.46', 'loss/train': '0.65342', 'examples_per_second': '34.081', 'grad_norm': '19.375', 'counters/examples': 56896, 'counters/updates': 1778}
train stats after 56928 examples: {'rewards_train/chosen': '0.14533', 'rewards_train/rejected': '0.065723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.079604', 'logps_train/rejected': '-118.72', 'logps_train/chosen': '-134.56', 'loss/train': '0.65927', 'examples_per_second': '31.363', 'grad_norm': '19.375', 'counters/examples': 56928, 'counters/updates': 1779}
train stats after 56960 examples: {'rewards_train/chosen': '0.073868', 'rewards_train/rejected': '0.035241', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038627', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-135.1', 'loss/train': '0.67822', 'examples_per_second': '31.506', 'grad_norm': '19.875', 'counters/examples': 56960, 'counters/updates': 1780}
train stats after 56992 examples: {'rewards_train/chosen': '0.099415', 'rewards_train/rejected': '0.026254', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073161', 'logps_train/rejected': '-132.08', 'logps_train/chosen': '-147.96', 'loss/train': '0.66337', 'examples_per_second': '30.617', 'grad_norm': '20.375', 'counters/examples': 56992, 'counters/updates': 1781}
train stats after 57024 examples: {'rewards_train/chosen': '0.066316', 'rewards_train/rejected': '0.063294', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.003022', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-121.36', 'loss/train': '0.69658', 'examples_per_second': '32.312', 'grad_norm': '19.875', 'counters/examples': 57024, 'counters/updates': 1782}
skipping logging after 57056 examples to avoid logging too frequently
train stats after 57088 examples: {'rewards_train/chosen': '0.075568', 'rewards_train/rejected': '0.033943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041624', 'logps_train/rejected': '-101.36', 'logps_train/chosen': '-168.19', 'loss/train': '0.67801', 'examples_per_second': '31.489', 'grad_norm': '20.625', 'counters/examples': 57088, 'counters/updates': 1784}
train stats after 57120 examples: {'rewards_train/chosen': '0.098658', 'rewards_train/rejected': '0.016692', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081966', 'logps_train/rejected': '-162.12', 'logps_train/chosen': '-145.04', 'loss/train': '0.65815', 'examples_per_second': '31.361', 'grad_norm': '22.875', 'counters/examples': 57120, 'counters/updates': 1785}
train stats after 57152 examples: {'rewards_train/chosen': '0.060058', 'rewards_train/rejected': '0.05252', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0075384', 'logps_train/rejected': '-109.02', 'logps_train/chosen': '-102.12', 'loss/train': '0.69337', 'examples_per_second': '30.495', 'grad_norm': '18.875', 'counters/examples': 57152, 'counters/updates': 1786}
train stats after 57184 examples: {'rewards_train/chosen': '0.058749', 'rewards_train/rejected': '-0.057522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11627', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-119.81', 'loss/train': '0.64216', 'examples_per_second': '30.135', 'grad_norm': '18.125', 'counters/examples': 57184, 'counters/updates': 1787}
train stats after 57216 examples: {'rewards_train/chosen': '0.096533', 'rewards_train/rejected': '0.056193', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04034', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-172.97', 'loss/train': '0.67996', 'examples_per_second': '31.496', 'grad_norm': '22.5', 'counters/examples': 57216, 'counters/updates': 1788}
skipping logging after 57248 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '0.043229', 'rewards_train/rejected': '0.018545', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024683', 'logps_train/rejected': '-101.65', 'logps_train/chosen': '-124.18', 'loss/train': '0.68323', 'examples_per_second': '31.484', 'grad_norm': '19.75', 'counters/examples': 57280, 'counters/updates': 1790}
train stats after 57312 examples: {'rewards_train/chosen': '0.11893', 'rewards_train/rejected': '0.051689', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067243', 'logps_train/rejected': '-135.67', 'logps_train/chosen': '-113.27', 'loss/train': '0.66573', 'examples_per_second': '31.585', 'grad_norm': '19.5', 'counters/examples': 57312, 'counters/updates': 1791}
train stats after 57344 examples: {'rewards_train/chosen': '0.069174', 'rewards_train/rejected': '-0.0060549', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075229', 'logps_train/rejected': '-89.62', 'logps_train/chosen': '-122', 'loss/train': '0.65857', 'examples_per_second': '30.337', 'grad_norm': '17.75', 'counters/examples': 57344, 'counters/updates': 1792}
train stats after 57376 examples: {'rewards_train/chosen': '0.070754', 'rewards_train/rejected': '0.05065', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020105', 'logps_train/rejected': '-103.78', 'logps_train/chosen': '-148.61', 'loss/train': '0.68722', 'examples_per_second': '24.439', 'grad_norm': '20.75', 'counters/examples': 57376, 'counters/updates': 1793}
train stats after 57408 examples: {'rewards_train/chosen': '0.12049', 'rewards_train/rejected': '0.049803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070689', 'logps_train/rejected': '-176.59', 'logps_train/chosen': '-159.03', 'loss/train': '0.66375', 'examples_per_second': '31.195', 'grad_norm': '22.875', 'counters/examples': 57408, 'counters/updates': 1794}
skipping logging after 57440 examples to avoid logging too frequently
train stats after 57472 examples: {'rewards_train/chosen': '0.098052', 'rewards_train/rejected': '0.026427', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071625', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-111.81', 'loss/train': '0.66245', 'examples_per_second': '24.413', 'grad_norm': '19.25', 'counters/examples': 57472, 'counters/updates': 1796}
train stats after 57504 examples: {'rewards_train/chosen': '0.04023', 'rewards_train/rejected': '0.042302', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0020721', 'logps_train/rejected': '-143.01', 'logps_train/chosen': '-148.04', 'loss/train': '0.69983', 'examples_per_second': '31.654', 'grad_norm': '23', 'counters/examples': 57504, 'counters/updates': 1797}
train stats after 57536 examples: {'rewards_train/chosen': '0.091392', 'rewards_train/rejected': '0.024895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066497', 'logps_train/rejected': '-100.68', 'logps_train/chosen': '-118.2', 'loss/train': '0.66442', 'examples_per_second': '32.209', 'grad_norm': '18.25', 'counters/examples': 57536, 'counters/updates': 1798}
train stats after 57568 examples: {'rewards_train/chosen': '0.10276', 'rewards_train/rejected': '0.062833', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039928', 'logps_train/rejected': '-119.79', 'logps_train/chosen': '-152.92', 'loss/train': '0.67613', 'examples_per_second': '31.166', 'grad_norm': '20.375', 'counters/examples': 57568, 'counters/updates': 1799}
train stats after 57600 examples: {'rewards_train/chosen': '0.04722', 'rewards_train/rejected': '0.047719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00049914', 'logps_train/rejected': '-119.78', 'logps_train/chosen': '-147.48', 'loss/train': '0.69907', 'examples_per_second': '32.32', 'grad_norm': '20.5', 'counters/examples': 57600, 'counters/updates': 1800}
train stats after 57632 examples: {'rewards_train/chosen': '0.092366', 'rewards_train/rejected': '0.047229', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045137', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-136.69', 'loss/train': '0.67536', 'examples_per_second': '32.116', 'grad_norm': '20.125', 'counters/examples': 57632, 'counters/updates': 1801}
train stats after 57664 examples: {'rewards_train/chosen': '0.093734', 'rewards_train/rejected': '0.065046', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.028688', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-121.92', 'loss/train': '0.68424', 'examples_per_second': '30.531', 'grad_norm': '20.875', 'counters/examples': 57664, 'counters/updates': 1802}
train stats after 57696 examples: {'rewards_train/chosen': '0.13375', 'rewards_train/rejected': '0.066159', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067594', 'logps_train/rejected': '-102.37', 'logps_train/chosen': '-160.96', 'loss/train': '0.66461', 'examples_per_second': '30.493', 'grad_norm': '20.625', 'counters/examples': 57696, 'counters/updates': 1803}
train stats after 57728 examples: {'rewards_train/chosen': '0.06527', 'rewards_train/rejected': '0.013142', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052128', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-135.26', 'loss/train': '0.67289', 'examples_per_second': '31.376', 'grad_norm': '20', 'counters/examples': 57728, 'counters/updates': 1804}
train stats after 57760 examples: {'rewards_train/chosen': '0.071248', 'rewards_train/rejected': '0.056113', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015135', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-148.67', 'loss/train': '0.69274', 'examples_per_second': '30.023', 'grad_norm': '21.5', 'counters/examples': 57760, 'counters/updates': 1805}
train stats after 57792 examples: {'rewards_train/chosen': '0.10332', 'rewards_train/rejected': '0.079482', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023838', 'logps_train/rejected': '-152.44', 'logps_train/chosen': '-175.2', 'loss/train': '0.68801', 'examples_per_second': '31.589', 'grad_norm': '22.75', 'counters/examples': 57792, 'counters/updates': 1806}
train stats after 57824 examples: {'rewards_train/chosen': '0.060892', 'rewards_train/rejected': '0.031007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029885', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-176.23', 'loss/train': '0.68052', 'examples_per_second': '30.258', 'grad_norm': '22.125', 'counters/examples': 57824, 'counters/updates': 1807}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57888 examples: {'rewards_train/chosen': '0.096606', 'rewards_train/rejected': '-0.013174', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10978', 'logps_train/rejected': '-94.71', 'logps_train/chosen': '-129.26', 'loss/train': '0.64488', 'examples_per_second': '31.197', 'grad_norm': '19.5', 'counters/examples': 57888, 'counters/updates': 1809}
skipping logging after 57920 examples to avoid logging too frequently
train stats after 57952 examples: {'rewards_train/chosen': '0.09214', 'rewards_train/rejected': '0.05819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03395', 'logps_train/rejected': '-105.89', 'logps_train/chosen': '-122.03', 'loss/train': '0.68067', 'examples_per_second': '35.812', 'grad_norm': '18.625', 'counters/examples': 57952, 'counters/updates': 1811}
train stats after 57984 examples: {'rewards_train/chosen': '0.031426', 'rewards_train/rejected': '0.025366', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0060601', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-126.59', 'loss/train': '0.69631', 'examples_per_second': '31.191', 'grad_norm': '20', 'counters/examples': 57984, 'counters/updates': 1812}
skipping logging after 58016 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '0.069778', 'rewards_train/rejected': '0.015974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053804', 'logps_train/rejected': '-97.638', 'logps_train/chosen': '-120.06', 'loss/train': '0.67118', 'examples_per_second': '36.494', 'grad_norm': '18.125', 'counters/examples': 58048, 'counters/updates': 1814}
train stats after 58080 examples: {'rewards_train/chosen': '0.090661', 'rewards_train/rejected': '0.040687', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049973', 'logps_train/rejected': '-132', 'logps_train/chosen': '-158.35', 'loss/train': '0.67449', 'examples_per_second': '30.011', 'grad_norm': '21', 'counters/examples': 58080, 'counters/updates': 1815}
train stats after 58112 examples: {'rewards_train/chosen': '0.10353', 'rewards_train/rejected': '0.080483', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023042', 'logps_train/rejected': '-99.513', 'logps_train/chosen': '-111.78', 'loss/train': '0.68538', 'examples_per_second': '31.555', 'grad_norm': '17.75', 'counters/examples': 58112, 'counters/updates': 1816}
train stats after 58144 examples: {'rewards_train/chosen': '0.10816', 'rewards_train/rejected': '0.038184', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069975', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-140.76', 'loss/train': '0.66248', 'examples_per_second': '30.195', 'grad_norm': '20.75', 'counters/examples': 58144, 'counters/updates': 1817}
skipping logging after 58176 examples to avoid logging too frequently
train stats after 58208 examples: {'rewards_train/chosen': '0.094482', 'rewards_train/rejected': '0.033816', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060666', 'logps_train/rejected': '-94.726', 'logps_train/chosen': '-148', 'loss/train': '0.66818', 'examples_per_second': '34.841', 'grad_norm': '19.5', 'counters/examples': 58208, 'counters/updates': 1819}
train stats after 58240 examples: {'rewards_train/chosen': '0.093455', 'rewards_train/rejected': '0.039236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054219', 'logps_train/rejected': '-95.494', 'logps_train/chosen': '-132.82', 'loss/train': '0.67276', 'examples_per_second': '31.444', 'grad_norm': '19.25', 'counters/examples': 58240, 'counters/updates': 1820}
train stats after 58272 examples: {'rewards_train/chosen': '0.11', 'rewards_train/rejected': '-0.0060555', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11605', 'logps_train/rejected': '-97.83', 'logps_train/chosen': '-145.02', 'loss/train': '0.64064', 'examples_per_second': '31.821', 'grad_norm': '19.25', 'counters/examples': 58272, 'counters/updates': 1821}
train stats after 58304 examples: {'rewards_train/chosen': '0.20153', 'rewards_train/rejected': '0.045697', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15583', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-131.48', 'loss/train': '0.62648', 'examples_per_second': '32.327', 'grad_norm': '19.375', 'counters/examples': 58304, 'counters/updates': 1822}
train stats after 58336 examples: {'rewards_train/chosen': '0.0472', 'rewards_train/rejected': '0.017047', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030153', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-162.56', 'loss/train': '0.68114', 'examples_per_second': '31.467', 'grad_norm': '21.25', 'counters/examples': 58336, 'counters/updates': 1823}
train stats after 58368 examples: {'rewards_train/chosen': '0.1088', 'rewards_train/rejected': '-0.030335', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13914', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-134.92', 'loss/train': '0.6315', 'examples_per_second': '31.348', 'grad_norm': '19.375', 'counters/examples': 58368, 'counters/updates': 1824}
skipping logging after 58400 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '0.11012', 'rewards_train/rejected': '0.032585', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077534', 'logps_train/rejected': '-96.148', 'logps_train/chosen': '-120.73', 'loss/train': '0.65958', 'examples_per_second': '33.021', 'grad_norm': '18.5', 'counters/examples': 58432, 'counters/updates': 1826}
skipping logging after 58464 examples to avoid logging too frequently
train stats after 58496 examples: {'rewards_train/chosen': '0.12141', 'rewards_train/rejected': '0.0080517', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11335', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-138.7', 'loss/train': '0.64271', 'examples_per_second': '30.741', 'grad_norm': '19.75', 'counters/examples': 58496, 'counters/updates': 1828}
train stats after 58528 examples: {'rewards_train/chosen': '0.07686', 'rewards_train/rejected': '0.073019', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0038411', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-139.63', 'loss/train': '0.69785', 'examples_per_second': '30.914', 'grad_norm': '20.875', 'counters/examples': 58528, 'counters/updates': 1829}
train stats after 58560 examples: {'rewards_train/chosen': '0.076798', 'rewards_train/rejected': '0.00097667', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075821', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-158.24', 'loss/train': '0.66171', 'examples_per_second': '31.604', 'grad_norm': '21.75', 'counters/examples': 58560, 'counters/updates': 1830}
train stats after 58592 examples: {'rewards_train/chosen': '0.13346', 'rewards_train/rejected': '0.038814', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094649', 'logps_train/rejected': '-125.6', 'logps_train/chosen': '-133.74', 'loss/train': '0.65345', 'examples_per_second': '31.516', 'grad_norm': '20.625', 'counters/examples': 58592, 'counters/updates': 1831}
train stats after 58624 examples: {'rewards_train/chosen': '0.11696', 'rewards_train/rejected': '0.044172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072785', 'logps_train/rejected': '-114.07', 'logps_train/chosen': '-137.82', 'loss/train': '0.66329', 'examples_per_second': '25.966', 'grad_norm': '19.25', 'counters/examples': 58624, 'counters/updates': 1832}
train stats after 58656 examples: {'rewards_train/chosen': '0.06496', 'rewards_train/rejected': '0.048801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01616', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-127.47', 'loss/train': '0.68969', 'examples_per_second': '30.061', 'grad_norm': '20.625', 'counters/examples': 58656, 'counters/updates': 1833}
train stats after 58688 examples: {'rewards_train/chosen': '0.086346', 'rewards_train/rejected': '0.018147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068199', 'logps_train/rejected': '-136.79', 'logps_train/chosen': '-121.84', 'loss/train': '0.66551', 'examples_per_second': '29.901', 'grad_norm': '20.25', 'counters/examples': 58688, 'counters/updates': 1834}
train stats after 58720 examples: {'rewards_train/chosen': '0.075654', 'rewards_train/rejected': '-0.0021409', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077795', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-132.37', 'loss/train': '0.66018', 'examples_per_second': '32.038', 'grad_norm': '18.625', 'counters/examples': 58720, 'counters/updates': 1835}
train stats after 58752 examples: {'rewards_train/chosen': '0.11686', 'rewards_train/rejected': '0.008543', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10832', 'logps_train/rejected': '-110.57', 'logps_train/chosen': '-121.62', 'loss/train': '0.64404', 'examples_per_second': '30.429', 'grad_norm': '18.875', 'counters/examples': 58752, 'counters/updates': 1836}
train stats after 58784 examples: {'rewards_train/chosen': '0.06491', 'rewards_train/rejected': '0.054784', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010126', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-125.1', 'loss/train': '0.69295', 'examples_per_second': '30.481', 'grad_norm': '18.875', 'counters/examples': 58784, 'counters/updates': 1837}
train stats after 58816 examples: {'rewards_train/chosen': '0.12003', 'rewards_train/rejected': '0.086508', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033527', 'logps_train/rejected': '-169.42', 'logps_train/chosen': '-169.19', 'loss/train': '0.68034', 'examples_per_second': '31.491', 'grad_norm': '23.25', 'counters/examples': 58816, 'counters/updates': 1838}
train stats after 58848 examples: {'rewards_train/chosen': '0.037496', 'rewards_train/rejected': '-0.003384', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.04088', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-109.94', 'loss/train': '0.67887', 'examples_per_second': '30.154', 'grad_norm': '19.875', 'counters/examples': 58848, 'counters/updates': 1839}
train stats after 58880 examples: {'rewards_train/chosen': '0.076486', 'rewards_train/rejected': '0.046012', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030474', 'logps_train/rejected': '-113.88', 'logps_train/chosen': '-138.91', 'loss/train': '0.68415', 'examples_per_second': '30.088', 'grad_norm': '19.75', 'counters/examples': 58880, 'counters/updates': 1840}
skipping logging after 58912 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '0.086613', 'rewards_train/rejected': '0.072361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014252', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-138.73', 'loss/train': '0.69113', 'examples_per_second': '31.647', 'grad_norm': '20.625', 'counters/examples': 58944, 'counters/updates': 1842}
train stats after 58976 examples: {'rewards_train/chosen': '0.094294', 'rewards_train/rejected': '0.036925', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057369', 'logps_train/rejected': '-162.93', 'logps_train/chosen': '-176.81', 'loss/train': '0.66793', 'examples_per_second': '31.22', 'grad_norm': '23', 'counters/examples': 58976, 'counters/updates': 1843}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59040 examples: {'rewards_train/chosen': '0.14294', 'rewards_train/rejected': '0.057566', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08537', 'logps_train/rejected': '-164.91', 'logps_train/chosen': '-123.65', 'loss/train': '0.65726', 'examples_per_second': '31.017', 'grad_norm': '21.125', 'counters/examples': 59040, 'counters/updates': 1845}
train stats after 59072 examples: {'rewards_train/chosen': '0.11172', 'rewards_train/rejected': '0.019551', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092166', 'logps_train/rejected': '-100.83', 'logps_train/chosen': '-126.11', 'loss/train': '0.65621', 'examples_per_second': '30.807', 'grad_norm': '18.375', 'counters/examples': 59072, 'counters/updates': 1846}
skipping logging after 59104 examples to avoid logging too frequently
train stats after 59136 examples: {'rewards_train/chosen': '0.093794', 'rewards_train/rejected': '0.010425', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083369', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-158.93', 'loss/train': '0.65861', 'examples_per_second': '31.822', 'grad_norm': '20.875', 'counters/examples': 59136, 'counters/updates': 1848}
train stats after 59168 examples: {'rewards_train/chosen': '0.048145', 'rewards_train/rejected': '0.02968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018465', 'logps_train/rejected': '-144.94', 'logps_train/chosen': '-138.41', 'loss/train': '0.68955', 'examples_per_second': '31.449', 'grad_norm': '22.75', 'counters/examples': 59168, 'counters/updates': 1849}
train stats after 59200 examples: {'rewards_train/chosen': '0.1243', 'rewards_train/rejected': '0.078908', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045395', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-125.04', 'loss/train': '0.67484', 'examples_per_second': '30.988', 'grad_norm': '19.625', 'counters/examples': 59200, 'counters/updates': 1850}
train stats after 59232 examples: {'rewards_train/chosen': '0.11284', 'rewards_train/rejected': '0.085192', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027646', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-116.47', 'loss/train': '0.68436', 'examples_per_second': '30.52', 'grad_norm': '20.125', 'counters/examples': 59232, 'counters/updates': 1851}
train stats after 59264 examples: {'rewards_train/chosen': '0.1079', 'rewards_train/rejected': '0.027864', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080033', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-160.24', 'loss/train': '0.65963', 'examples_per_second': '30.555', 'grad_norm': '20.125', 'counters/examples': 59264, 'counters/updates': 1852}
skipping logging after 59296 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '0.057324', 'rewards_train/rejected': '0.046005', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011318', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-124.9', 'loss/train': '0.69416', 'examples_per_second': '30.507', 'grad_norm': '20.25', 'counters/examples': 59328, 'counters/updates': 1854}
train stats after 59360 examples: {'rewards_train/chosen': '0.083875', 'rewards_train/rejected': '0.044514', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039361', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-142.65', 'loss/train': '0.67998', 'examples_per_second': '31.545', 'grad_norm': '19.5', 'counters/examples': 59360, 'counters/updates': 1855}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59424 examples: {'rewards_train/chosen': '0.15668', 'rewards_train/rejected': '0.05945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097233', 'logps_train/rejected': '-111.41', 'logps_train/chosen': '-150.83', 'loss/train': '0.64906', 'examples_per_second': '31.498', 'grad_norm': '20.375', 'counters/examples': 59424, 'counters/updates': 1857}
train stats after 59456 examples: {'rewards_train/chosen': '0.049614', 'rewards_train/rejected': '0.040186', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0094287', 'logps_train/rejected': '-161.17', 'logps_train/chosen': '-153.88', 'loss/train': '0.69831', 'examples_per_second': '30.814', 'grad_norm': '22.25', 'counters/examples': 59456, 'counters/updates': 1858}
train stats after 59488 examples: {'rewards_train/chosen': '0.11487', 'rewards_train/rejected': '0.080394', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034479', 'logps_train/rejected': '-200.91', 'logps_train/chosen': '-161.36', 'loss/train': '0.6833', 'examples_per_second': '31.305', 'grad_norm': '24.625', 'counters/examples': 59488, 'counters/updates': 1859}
train stats after 59520 examples: {'rewards_train/chosen': '0.092442', 'rewards_train/rejected': '0.028804', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063638', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-127.87', 'loss/train': '0.66766', 'examples_per_second': '31.267', 'grad_norm': '19', 'counters/examples': 59520, 'counters/updates': 1860}
train stats after 59552 examples: {'rewards_train/chosen': '0.034202', 'rewards_train/rejected': '-0.0177', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051902', 'logps_train/rejected': '-109.09', 'logps_train/chosen': '-108.19', 'loss/train': '0.6731', 'examples_per_second': '31.796', 'grad_norm': '18', 'counters/examples': 59552, 'counters/updates': 1861}
train stats after 59584 examples: {'rewards_train/chosen': '0.13382', 'rewards_train/rejected': '0.08566', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04816', 'logps_train/rejected': '-152.41', 'logps_train/chosen': '-151.76', 'loss/train': '0.67502', 'examples_per_second': '31.473', 'grad_norm': '21.75', 'counters/examples': 59584, 'counters/updates': 1862}
train stats after 59616 examples: {'rewards_train/chosen': '0.069389', 'rewards_train/rejected': '-0.00086172', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070251', 'logps_train/rejected': '-75.874', 'logps_train/chosen': '-135.78', 'loss/train': '0.66285', 'examples_per_second': '31.867', 'grad_norm': '19.25', 'counters/examples': 59616, 'counters/updates': 1863}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59680 examples: {'rewards_train/chosen': '0.081839', 'rewards_train/rejected': '0.039679', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04216', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-149.29', 'loss/train': '0.676', 'examples_per_second': '31.459', 'grad_norm': '20.875', 'counters/examples': 59680, 'counters/updates': 1865}
skipping logging after 59712 examples to avoid logging too frequently
train stats after 59744 examples: {'rewards_train/chosen': '0.090273', 'rewards_train/rejected': '0.042305', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047968', 'logps_train/rejected': '-104.75', 'logps_train/chosen': '-119.11', 'loss/train': '0.67872', 'examples_per_second': '30.714', 'grad_norm': '19', 'counters/examples': 59744, 'counters/updates': 1867}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59808 examples: {'rewards_train/chosen': '0.090277', 'rewards_train/rejected': '0.04546', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044817', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-150.65', 'loss/train': '0.67384', 'examples_per_second': '33.696', 'grad_norm': '21.75', 'counters/examples': 59808, 'counters/updates': 1869}
train stats after 59840 examples: {'rewards_train/chosen': '0.056742', 'rewards_train/rejected': '0.037889', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018852', 'logps_train/rejected': '-104.81', 'logps_train/chosen': '-114.34', 'loss/train': '0.688', 'examples_per_second': '32.368', 'grad_norm': '18.875', 'counters/examples': 59840, 'counters/updates': 1870}
train stats after 59872 examples: {'rewards_train/chosen': '0.057033', 'rewards_train/rejected': '0.040282', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01675', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-121.54', 'loss/train': '0.68875', 'examples_per_second': '30.083', 'grad_norm': '19.75', 'counters/examples': 59872, 'counters/updates': 1871}
train stats after 59904 examples: {'rewards_train/chosen': '0.1181', 'rewards_train/rejected': '0.059035', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059061', 'logps_train/rejected': '-108.46', 'logps_train/chosen': '-133.63', 'loss/train': '0.67203', 'examples_per_second': '30.07', 'grad_norm': '18.75', 'counters/examples': 59904, 'counters/updates': 1872}
train stats after 59936 examples: {'rewards_train/chosen': '0.1021', 'rewards_train/rejected': '0.037327', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064772', 'logps_train/rejected': '-108.27', 'logps_train/chosen': '-136.21', 'loss/train': '0.66628', 'examples_per_second': '31.59', 'grad_norm': '19.5', 'counters/examples': 59936, 'counters/updates': 1873}
train stats after 59968 examples: {'rewards_train/chosen': '0.060548', 'rewards_train/rejected': '0.0031144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057433', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-125.6', 'loss/train': '0.67037', 'examples_per_second': '30.185', 'grad_norm': '19.25', 'counters/examples': 59968, 'counters/updates': 1874}
train stats after 60000 examples: {'rewards_train/chosen': '0.1037', 'rewards_train/rejected': '0.014774', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088929', 'logps_train/rejected': '-142.84', 'logps_train/chosen': '-154.14', 'loss/train': '0.65271', 'examples_per_second': '30.282', 'grad_norm': '21.125', 'counters/examples': 60000, 'counters/updates': 1875}
Running evaluation after 60000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.19it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.81it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.90it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.96it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.92it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.88it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 60000: {'rewards_eval/chosen': '0.091218', 'rewards_eval/rejected': '0.039672', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.051546', 'logps_eval/rejected': '-115', 'logps_eval/chosen': '-134.64', 'loss/eval': '0.67299'}
skipping save for non epoch
train stats after 60032 examples: {'rewards_train/chosen': '0.11164', 'rewards_train/rejected': '0.021526', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090119', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-162.84', 'loss/train': '0.65354', 'examples_per_second': '31.265', 'grad_norm': '21', 'counters/examples': 60032, 'counters/updates': 1876}
train stats after 60064 examples: {'rewards_train/chosen': '0.082713', 'rewards_train/rejected': '0.08451', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0017973', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-166.95', 'loss/train': '0.69793', 'examples_per_second': '32.521', 'grad_norm': '22.25', 'counters/examples': 60064, 'counters/updates': 1877}
train stats after 60096 examples: {'rewards_train/chosen': '0.12537', 'rewards_train/rejected': '0.031291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094076', 'logps_train/rejected': '-108.41', 'logps_train/chosen': '-138.2', 'loss/train': '0.65469', 'examples_per_second': '31.571', 'grad_norm': '20.125', 'counters/examples': 60096, 'counters/updates': 1878}
train stats after 60128 examples: {'rewards_train/chosen': '0.1071', 'rewards_train/rejected': '0.028903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078193', 'logps_train/rejected': '-118.64', 'logps_train/chosen': '-144.49', 'loss/train': '0.6608', 'examples_per_second': '31.336', 'grad_norm': '20.5', 'counters/examples': 60128, 'counters/updates': 1879}
train stats after 60160 examples: {'rewards_train/chosen': '0.12043', 'rewards_train/rejected': '0.058156', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06227', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-156.67', 'loss/train': '0.66596', 'examples_per_second': '32.719', 'grad_norm': '20.25', 'counters/examples': 60160, 'counters/updates': 1880}
train stats after 60192 examples: {'rewards_train/chosen': '0.10062', 'rewards_train/rejected': '0.075298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025325', 'logps_train/rejected': '-121.88', 'logps_train/chosen': '-112.08', 'loss/train': '0.68543', 'examples_per_second': '32.536', 'grad_norm': '19.875', 'counters/examples': 60192, 'counters/updates': 1881}
train stats after 60224 examples: {'rewards_train/chosen': '0.090074', 'rewards_train/rejected': '0.039628', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050447', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-178.18', 'loss/train': '0.67194', 'examples_per_second': '31.821', 'grad_norm': '21.375', 'counters/examples': 60224, 'counters/updates': 1882}
train stats after 60256 examples: {'rewards_train/chosen': '0.18202', 'rewards_train/rejected': '0.053762', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12826', 'logps_train/rejected': '-96.628', 'logps_train/chosen': '-110.28', 'loss/train': '0.63821', 'examples_per_second': '30.768', 'grad_norm': '17.5', 'counters/examples': 60256, 'counters/updates': 1883}
train stats after 60288 examples: {'rewards_train/chosen': '0.077108', 'rewards_train/rejected': '0.051542', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025567', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-133.57', 'loss/train': '0.68482', 'examples_per_second': '30.043', 'grad_norm': '20.125', 'counters/examples': 60288, 'counters/updates': 1884}
train stats after 60320 examples: {'rewards_train/chosen': '0.1447', 'rewards_train/rejected': '0.082015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062685', 'logps_train/rejected': '-137.34', 'logps_train/chosen': '-122.74', 'loss/train': '0.66946', 'examples_per_second': '30.238', 'grad_norm': '18.875', 'counters/examples': 60320, 'counters/updates': 1885}
train stats after 60352 examples: {'rewards_train/chosen': '0.083406', 'rewards_train/rejected': '0.016016', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067389', 'logps_train/rejected': '-100.8', 'logps_train/chosen': '-125.5', 'loss/train': '0.66479', 'examples_per_second': '30.541', 'grad_norm': '18.375', 'counters/examples': 60352, 'counters/updates': 1886}
train stats after 60384 examples: {'rewards_train/chosen': '0.082292', 'rewards_train/rejected': '0.032728', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049564', 'logps_train/rejected': '-103.49', 'logps_train/chosen': '-138.41', 'loss/train': '0.67469', 'examples_per_second': '31.607', 'grad_norm': '19.125', 'counters/examples': 60384, 'counters/updates': 1887}
train stats after 60416 examples: {'rewards_train/chosen': '0.10172', 'rewards_train/rejected': '0.051384', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.050331', 'logps_train/rejected': '-129.79', 'logps_train/chosen': '-145.74', 'loss/train': '0.67363', 'examples_per_second': '31.43', 'grad_norm': '21.5', 'counters/examples': 60416, 'counters/updates': 1888}
train stats after 60448 examples: {'rewards_train/chosen': '0.056572', 'rewards_train/rejected': '0.04564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010932', 'logps_train/rejected': '-125.41', 'logps_train/chosen': '-138.95', 'loss/train': '0.69225', 'examples_per_second': '31.219', 'grad_norm': '21.75', 'counters/examples': 60448, 'counters/updates': 1889}
train stats after 60480 examples: {'rewards_train/chosen': '0.037577', 'rewards_train/rejected': '-0.0010125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03859', 'logps_train/rejected': '-91.058', 'logps_train/chosen': '-104.56', 'loss/train': '0.67791', 'examples_per_second': '32.023', 'grad_norm': '17.375', 'counters/examples': 60480, 'counters/updates': 1890}
train stats after 60512 examples: {'rewards_train/chosen': '0.070969', 'rewards_train/rejected': '-0.00023732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071206', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-134.62', 'loss/train': '0.66328', 'examples_per_second': '31.976', 'grad_norm': '20.25', 'counters/examples': 60512, 'counters/updates': 1891}
train stats after 60544 examples: {'rewards_train/chosen': '0.067531', 'rewards_train/rejected': '0.0747', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0071696', 'logps_train/rejected': '-99.018', 'logps_train/chosen': '-109.5', 'loss/train': '0.70358', 'examples_per_second': '32.783', 'grad_norm': '18', 'counters/examples': 60544, 'counters/updates': 1892}
train stats after 60576 examples: {'rewards_train/chosen': '0.071293', 'rewards_train/rejected': '0.034562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036731', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-133.05', 'loss/train': '0.68238', 'examples_per_second': '31.277', 'grad_norm': '19.625', 'counters/examples': 60576, 'counters/updates': 1893}
train stats after 60608 examples: {'rewards_train/chosen': '0.11448', 'rewards_train/rejected': '0.041408', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073075', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-135.84', 'loss/train': '0.66154', 'examples_per_second': '30.974', 'grad_norm': '19.375', 'counters/examples': 60608, 'counters/updates': 1894}
train stats after 60640 examples: {'rewards_train/chosen': '0.1052', 'rewards_train/rejected': '0.070575', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034629', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-130.84', 'loss/train': '0.67963', 'examples_per_second': '30.43', 'grad_norm': '19.625', 'counters/examples': 60640, 'counters/updates': 1895}
train stats after 60672 examples: {'rewards_train/chosen': '0.12908', 'rewards_train/rejected': '0.031774', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097304', 'logps_train/rejected': '-75.158', 'logps_train/chosen': '-144.51', 'loss/train': '0.64936', 'examples_per_second': '32.32', 'grad_norm': '18.75', 'counters/examples': 60672, 'counters/updates': 1896}
train stats after 60704 examples: {'rewards_train/chosen': '0.09941', 'rewards_train/rejected': '0.042365', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057045', 'logps_train/rejected': '-151.19', 'logps_train/chosen': '-172.59', 'loss/train': '0.6697', 'examples_per_second': '30.256', 'grad_norm': '22.875', 'counters/examples': 60704, 'counters/updates': 1897}
train stats after 60736 examples: {'rewards_train/chosen': '0.04241', 'rewards_train/rejected': '0.046217', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.003807', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-117.05', 'loss/train': '0.70089', 'examples_per_second': '30.626', 'grad_norm': '19.5', 'counters/examples': 60736, 'counters/updates': 1898}
train stats after 60768 examples: {'rewards_train/chosen': '0.057612', 'rewards_train/rejected': '0.036815', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020797', 'logps_train/rejected': '-108.23', 'logps_train/chosen': '-129.77', 'loss/train': '0.68867', 'examples_per_second': '30.211', 'grad_norm': '20.75', 'counters/examples': 60768, 'counters/updates': 1899}
train stats after 60800 examples: {'rewards_train/chosen': '0.096395', 'rewards_train/rejected': '0.055113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041282', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-114.47', 'loss/train': '0.67914', 'examples_per_second': '32.265', 'grad_norm': '19.125', 'counters/examples': 60800, 'counters/updates': 1900}
train stats after 60832 examples: {'rewards_train/chosen': '0.047098', 'rewards_train/rejected': '0.020189', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026909', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-103.04', 'loss/train': '0.68486', 'examples_per_second': '31.361', 'grad_norm': '20', 'counters/examples': 60832, 'counters/updates': 1901}
train stats after 60864 examples: {'rewards_train/chosen': '0.079904', 'rewards_train/rejected': '0.056809', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023095', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-144.12', 'loss/train': '0.68956', 'examples_per_second': '30.759', 'grad_norm': '21.375', 'counters/examples': 60864, 'counters/updates': 1902}
train stats after 60896 examples: {'rewards_train/chosen': '0.098434', 'rewards_train/rejected': '0.058792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039642', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-140.1', 'loss/train': '0.67896', 'examples_per_second': '31.579', 'grad_norm': '19.875', 'counters/examples': 60896, 'counters/updates': 1903}
train stats after 60928 examples: {'rewards_train/chosen': '0.058234', 'rewards_train/rejected': '0.047767', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.010468', 'logps_train/rejected': '-128.88', 'logps_train/chosen': '-144.41', 'loss/train': '0.69275', 'examples_per_second': '30.244', 'grad_norm': '21.5', 'counters/examples': 60928, 'counters/updates': 1904}
train stats after 60960 examples: {'rewards_train/chosen': '0.094518', 'rewards_train/rejected': '0.070042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024476', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-131.56', 'loss/train': '0.68699', 'examples_per_second': '30.617', 'grad_norm': '20', 'counters/examples': 60960, 'counters/updates': 1905}
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61024 examples: {'rewards_train/chosen': '0.10613', 'rewards_train/rejected': '0.021992', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084135', 'logps_train/rejected': '-113.05', 'logps_train/chosen': '-157.99', 'loss/train': '0.65609', 'examples_per_second': '37.373', 'grad_norm': '20.5', 'counters/examples': 61024, 'counters/updates': 1907}
skipping logging after 61056 examples to avoid logging too frequently
train stats after 61088 examples: {'rewards_train/chosen': '0.074482', 'rewards_train/rejected': '0.046955', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027527', 'logps_train/rejected': '-146.29', 'logps_train/chosen': '-154.73', 'loss/train': '0.68379', 'examples_per_second': '30.713', 'grad_norm': '21.875', 'counters/examples': 61088, 'counters/updates': 1909}
train stats after 61120 examples: {'rewards_train/chosen': '0.07582', 'rewards_train/rejected': '0.020135', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055685', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-133.01', 'loss/train': '0.6713', 'examples_per_second': '32.443', 'grad_norm': '20.5', 'counters/examples': 61120, 'counters/updates': 1910}
train stats after 61152 examples: {'rewards_train/chosen': '0.066149', 'rewards_train/rejected': '0.045794', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020355', 'logps_train/rejected': '-129.62', 'logps_train/chosen': '-161.9', 'loss/train': '0.68727', 'examples_per_second': '30.156', 'grad_norm': '21.625', 'counters/examples': 61152, 'counters/updates': 1911}
train stats after 61184 examples: {'rewards_train/chosen': '0.071165', 'rewards_train/rejected': '-0.013024', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084188', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-135.83', 'loss/train': '0.65876', 'examples_per_second': '30.129', 'grad_norm': '20.125', 'counters/examples': 61184, 'counters/updates': 1912}
train stats after 61216 examples: {'rewards_train/chosen': '0.11375', 'rewards_train/rejected': '0.047815', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06593', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-148.95', 'loss/train': '0.66635', 'examples_per_second': '30.821', 'grad_norm': '20.25', 'counters/examples': 61216, 'counters/updates': 1913}
train stats after 61248 examples: {'rewards_train/chosen': '0.087853', 'rewards_train/rejected': '0.047952', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039901', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-128.18', 'loss/train': '0.67931', 'examples_per_second': '32.591', 'grad_norm': '19.5', 'counters/examples': 61248, 'counters/updates': 1914}
skipping logging after 61280 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '0.12884', 'rewards_train/rejected': '0.037156', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.091689', 'logps_train/rejected': '-105.83', 'logps_train/chosen': '-148.94', 'loss/train': '0.65128', 'examples_per_second': '30.644', 'grad_norm': '21.125', 'counters/examples': 61312, 'counters/updates': 1916}
train stats after 61344 examples: {'rewards_train/chosen': '0.083863', 'rewards_train/rejected': '0.051342', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032521', 'logps_train/rejected': '-103.1', 'logps_train/chosen': '-128.12', 'loss/train': '0.68373', 'examples_per_second': '30.843', 'grad_norm': '20.25', 'counters/examples': 61344, 'counters/updates': 1917}
train stats after 61376 examples: {'rewards_train/chosen': '0.055229', 'rewards_train/rejected': '0.032707', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022522', 'logps_train/rejected': '-132.65', 'logps_train/chosen': '-136.79', 'loss/train': '0.68787', 'examples_per_second': '30.205', 'grad_norm': '21.375', 'counters/examples': 61376, 'counters/updates': 1918}
skipping logging after 61408 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '0.13369', 'rewards_train/rejected': '0.098811', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034881', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-127.45', 'loss/train': '0.68035', 'examples_per_second': '31.662', 'grad_norm': '21.75', 'counters/examples': 61440, 'counters/updates': 1920}
train stats after 61472 examples: {'rewards_train/chosen': '0.11158', 'rewards_train/rejected': '0.02859', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082992', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-178.79', 'loss/train': '0.65805', 'examples_per_second': '30.145', 'grad_norm': '21.25', 'counters/examples': 61472, 'counters/updates': 1921}
train stats after 61504 examples: {'rewards_train/chosen': '0.071218', 'rewards_train/rejected': '0.089713', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018495', 'logps_train/rejected': '-131.06', 'logps_train/chosen': '-105.52', 'loss/train': '0.70848', 'examples_per_second': '30.562', 'grad_norm': '19.75', 'counters/examples': 61504, 'counters/updates': 1922}
train stats after 61536 examples: {'rewards_train/chosen': '0.10725', 'rewards_train/rejected': '0.032885', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.074367', 'logps_train/rejected': '-146.97', 'logps_train/chosen': '-179.3', 'loss/train': '0.6618', 'examples_per_second': '30.052', 'grad_norm': '22.625', 'counters/examples': 61536, 'counters/updates': 1923}
train stats after 61568 examples: {'rewards_train/chosen': '0.079102', 'rewards_train/rejected': '0.044234', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034868', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-152.42', 'loss/train': '0.68143', 'examples_per_second': '31.562', 'grad_norm': '21.375', 'counters/examples': 61568, 'counters/updates': 1924}
train stats after 61600 examples: {'rewards_train/chosen': '0.078951', 'rewards_train/rejected': '0.017524', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061427', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-136.59', 'loss/train': '0.67051', 'examples_per_second': '31.455', 'grad_norm': '22.125', 'counters/examples': 61600, 'counters/updates': 1925}
train stats after 61632 examples: {'rewards_train/chosen': '0.052704', 'rewards_train/rejected': '-0.011795', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064499', 'logps_train/rejected': '-113.41', 'logps_train/chosen': '-124.84', 'loss/train': '0.66353', 'examples_per_second': '32.34', 'grad_norm': '19.375', 'counters/examples': 61632, 'counters/updates': 1926}
train stats after 61664 examples: {'rewards_train/chosen': '0.093153', 'rewards_train/rejected': '0.054049', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039104', 'logps_train/rejected': '-88.36', 'logps_train/chosen': '-104.39', 'loss/train': '0.67763', 'examples_per_second': '31.61', 'grad_norm': '17.5', 'counters/examples': 61664, 'counters/updates': 1927}
train stats after 61696 examples: {'rewards_train/chosen': '0.12722', 'rewards_train/rejected': '0.067503', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059719', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-101.56', 'loss/train': '0.66747', 'examples_per_second': '32.233', 'grad_norm': '19.5', 'counters/examples': 61696, 'counters/updates': 1928}
train stats after 61728 examples: {'rewards_train/chosen': '0.15427', 'rewards_train/rejected': '0.070733', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.083538', 'logps_train/rejected': '-143.43', 'logps_train/chosen': '-165.59', 'loss/train': '0.65696', 'examples_per_second': '31.949', 'grad_norm': '22.125', 'counters/examples': 61728, 'counters/updates': 1929}
train stats after 61760 examples: {'rewards_train/chosen': '0.079986', 'rewards_train/rejected': '0.056756', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02323', 'logps_train/rejected': '-125.51', 'logps_train/chosen': '-116.62', 'loss/train': '0.68541', 'examples_per_second': '33.116', 'grad_norm': '19.625', 'counters/examples': 61760, 'counters/updates': 1930}
train stats after 61792 examples: {'rewards_train/chosen': '0.10787', 'rewards_train/rejected': '-0.0030842', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11095', 'logps_train/rejected': '-87.945', 'logps_train/chosen': '-140.07', 'loss/train': '0.64434', 'examples_per_second': '32.189', 'grad_norm': '18.375', 'counters/examples': 61792, 'counters/updates': 1931}
train stats after 61824 examples: {'rewards_train/chosen': '0.1016', 'rewards_train/rejected': '0.022187', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.079416', 'logps_train/rejected': '-119.87', 'logps_train/chosen': '-151.86', 'loss/train': '0.65571', 'examples_per_second': '31.422', 'grad_norm': '20', 'counters/examples': 61824, 'counters/updates': 1932}
train stats after 61856 examples: {'rewards_train/chosen': '0.13097', 'rewards_train/rejected': '0.071282', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059692', 'logps_train/rejected': '-128.88', 'logps_train/chosen': '-138.18', 'loss/train': '0.66703', 'examples_per_second': '31.38', 'grad_norm': '21', 'counters/examples': 61856, 'counters/updates': 1933}
train stats after 61888 examples: {'rewards_train/chosen': '0.050743', 'rewards_train/rejected': '0.018244', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032499', 'logps_train/rejected': '-93.47', 'logps_train/chosen': '-110.53', 'loss/train': '0.68223', 'examples_per_second': '31.618', 'grad_norm': '17.625', 'counters/examples': 61888, 'counters/updates': 1934}
train stats after 61920 examples: {'rewards_train/chosen': '0.07143', 'rewards_train/rejected': '0.0096238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061806', 'logps_train/rejected': '-96.936', 'logps_train/chosen': '-136.14', 'loss/train': '0.66582', 'examples_per_second': '31.761', 'grad_norm': '18.25', 'counters/examples': 61920, 'counters/updates': 1935}
train stats after 61952 examples: {'rewards_train/chosen': '0.06478', 'rewards_train/rejected': '0.038487', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026294', 'logps_train/rejected': '-134.68', 'logps_train/chosen': '-161.84', 'loss/train': '0.68474', 'examples_per_second': '31.007', 'grad_norm': '22.25', 'counters/examples': 61952, 'counters/updates': 1936}
train stats after 61984 examples: {'rewards_train/chosen': '0.10461', 'rewards_train/rejected': '0.031173', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073439', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-173.46', 'loss/train': '0.66307', 'examples_per_second': '32.075', 'grad_norm': '22.125', 'counters/examples': 61984, 'counters/updates': 1937}
train stats after 62016 examples: {'rewards_train/chosen': '0.10606', 'rewards_train/rejected': '0.036099', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069957', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-113.17', 'loss/train': '0.66373', 'examples_per_second': '31.622', 'grad_norm': '19', 'counters/examples': 62016, 'counters/updates': 1938}
train stats after 62048 examples: {'rewards_train/chosen': '0.083909', 'rewards_train/rejected': '0.028372', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055537', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-177.69', 'loss/train': '0.67147', 'examples_per_second': '31.052', 'grad_norm': '24.125', 'counters/examples': 62048, 'counters/updates': 1939}
train stats after 62080 examples: {'rewards_train/chosen': '0.11845', 'rewards_train/rejected': '0.054521', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063928', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-142.3', 'loss/train': '0.6698', 'examples_per_second': '30.253', 'grad_norm': '21.125', 'counters/examples': 62080, 'counters/updates': 1940}
skipping logging after 62112 examples to avoid logging too frequently
train stats after 62144 examples: {'rewards_train/chosen': '0.11475', 'rewards_train/rejected': '0.031991', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082757', 'logps_train/rejected': '-91.79', 'logps_train/chosen': '-143.95', 'loss/train': '0.65757', 'examples_per_second': '31.665', 'grad_norm': '18.875', 'counters/examples': 62144, 'counters/updates': 1942}
train stats after 62176 examples: {'rewards_train/chosen': '0.094928', 'rewards_train/rejected': '0.063436', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031492', 'logps_train/rejected': '-144.47', 'logps_train/chosen': '-127.52', 'loss/train': '0.68238', 'examples_per_second': '30.197', 'grad_norm': '20.875', 'counters/examples': 62176, 'counters/updates': 1943}
train stats after 62208 examples: {'rewards_train/chosen': '0.082352', 'rewards_train/rejected': '0.030598', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051754', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-143.15', 'loss/train': '0.67167', 'examples_per_second': '31.384', 'grad_norm': '20.5', 'counters/examples': 62208, 'counters/updates': 1944}
train stats after 62240 examples: {'rewards_train/chosen': '0.085628', 'rewards_train/rejected': '0.053986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031642', 'logps_train/rejected': '-142.2', 'logps_train/chosen': '-143.31', 'loss/train': '0.68142', 'examples_per_second': '31.647', 'grad_norm': '22', 'counters/examples': 62240, 'counters/updates': 1945}
train stats after 62272 examples: {'rewards_train/chosen': '0.1586', 'rewards_train/rejected': '0.097415', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061189', 'logps_train/rejected': '-114.24', 'logps_train/chosen': '-130.26', 'loss/train': '0.67167', 'examples_per_second': '31.893', 'grad_norm': '18.75', 'counters/examples': 62272, 'counters/updates': 1946}
train stats after 62304 examples: {'rewards_train/chosen': '0.061906', 'rewards_train/rejected': '0.089615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.027709', 'logps_train/rejected': '-131.79', 'logps_train/chosen': '-149.07', 'loss/train': '0.71709', 'examples_per_second': '31.553', 'grad_norm': '22.375', 'counters/examples': 62304, 'counters/updates': 1947}
train stats after 62336 examples: {'rewards_train/chosen': '0.10324', 'rewards_train/rejected': '0.047659', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055585', 'logps_train/rejected': '-138.45', 'logps_train/chosen': '-113.59', 'loss/train': '0.67532', 'examples_per_second': '31.207', 'grad_norm': '20.5', 'counters/examples': 62336, 'counters/updates': 1948}
skipping logging after 62368 examples to avoid logging too frequently
train stats after 62400 examples: {'rewards_train/chosen': '0.088524', 'rewards_train/rejected': '0.038929', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049595', 'logps_train/rejected': '-114.07', 'logps_train/chosen': '-141.66', 'loss/train': '0.67102', 'examples_per_second': '32.406', 'grad_norm': '20.75', 'counters/examples': 62400, 'counters/updates': 1950}
train stats after 62432 examples: {'rewards_train/chosen': '0.07376', 'rewards_train/rejected': '0.0329', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040859', 'logps_train/rejected': '-141.69', 'logps_train/chosen': '-151.49', 'loss/train': '0.6767', 'examples_per_second': '31.68', 'grad_norm': '21.5', 'counters/examples': 62432, 'counters/updates': 1951}
train stats after 62464 examples: {'rewards_train/chosen': '0.11643', 'rewards_train/rejected': '0.066746', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.049689', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-154.69', 'loss/train': '0.67159', 'examples_per_second': '30.35', 'grad_norm': '20.75', 'counters/examples': 62464, 'counters/updates': 1952}
train stats after 62496 examples: {'rewards_train/chosen': '0.1076', 'rewards_train/rejected': '0.020034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087571', 'logps_train/rejected': '-110', 'logps_train/chosen': '-129.69', 'loss/train': '0.65548', 'examples_per_second': '32.975', 'grad_norm': '18.25', 'counters/examples': 62496, 'counters/updates': 1953}
train stats after 62528 examples: {'rewards_train/chosen': '0.13127', 'rewards_train/rejected': '0.056367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074908', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-146.33', 'loss/train': '0.66278', 'examples_per_second': '32.141', 'grad_norm': '20', 'counters/examples': 62528, 'counters/updates': 1954}
skipping logging after 62560 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '0.11432', 'rewards_train/rejected': '0.04114', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07318', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-156.08', 'loss/train': '0.66377', 'examples_per_second': '31.628', 'grad_norm': '20.25', 'counters/examples': 62592, 'counters/updates': 1956}
train stats after 62624 examples: {'rewards_train/chosen': '0.077919', 'rewards_train/rejected': '0.044416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033503', 'logps_train/rejected': '-144.37', 'logps_train/chosen': '-150.88', 'loss/train': '0.68375', 'examples_per_second': '32.818', 'grad_norm': '22', 'counters/examples': 62624, 'counters/updates': 1957}
train stats after 62656 examples: {'rewards_train/chosen': '0.10413', 'rewards_train/rejected': '0.062104', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042026', 'logps_train/rejected': '-133.78', 'logps_train/chosen': '-149.47', 'loss/train': '0.67765', 'examples_per_second': '33.033', 'grad_norm': '21', 'counters/examples': 62656, 'counters/updates': 1958}
train stats after 62688 examples: {'rewards_train/chosen': '0.098339', 'rewards_train/rejected': '0.017132', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081207', 'logps_train/rejected': '-128', 'logps_train/chosen': '-115.77', 'loss/train': '0.65974', 'examples_per_second': '31.102', 'grad_norm': '18.25', 'counters/examples': 62688, 'counters/updates': 1959}
train stats after 62720 examples: {'rewards_train/chosen': '0.094926', 'rewards_train/rejected': '0.053177', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04175', 'logps_train/rejected': '-104.22', 'logps_train/chosen': '-137.15', 'loss/train': '0.6809', 'examples_per_second': '32.59', 'grad_norm': '20.375', 'counters/examples': 62720, 'counters/updates': 1960}
train stats after 62752 examples: {'rewards_train/chosen': '0.073787', 'rewards_train/rejected': '0.02296', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050826', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-123.49', 'loss/train': '0.67163', 'examples_per_second': '31.646', 'grad_norm': '18.5', 'counters/examples': 62752, 'counters/updates': 1961}
train stats after 62784 examples: {'rewards_train/chosen': '0.11591', 'rewards_train/rejected': '0.0020409', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11387', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-128.64', 'loss/train': '0.64518', 'examples_per_second': '32.466', 'grad_norm': '18.875', 'counters/examples': 62784, 'counters/updates': 1962}
train stats after 62816 examples: {'rewards_train/chosen': '0.088356', 'rewards_train/rejected': '0.045122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043234', 'logps_train/rejected': '-162', 'logps_train/chosen': '-125.42', 'loss/train': '0.67876', 'examples_per_second': '31.175', 'grad_norm': '21.75', 'counters/examples': 62816, 'counters/updates': 1963}
train stats after 62848 examples: {'rewards_train/chosen': '0.10871', 'rewards_train/rejected': '0.028116', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080598', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-93.603', 'loss/train': '0.66083', 'examples_per_second': '21.94', 'grad_norm': '18.25', 'counters/examples': 62848, 'counters/updates': 1964}
skipping logging after 62880 examples to avoid logging too frequently
train stats after 62912 examples: {'rewards_train/chosen': '0.10254', 'rewards_train/rejected': '0.042627', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059914', 'logps_train/rejected': '-103.38', 'logps_train/chosen': '-129.16', 'loss/train': '0.66978', 'examples_per_second': '32.298', 'grad_norm': '19.625', 'counters/examples': 62912, 'counters/updates': 1966}
train stats after 62944 examples: {'rewards_train/chosen': '0.11654', 'rewards_train/rejected': '0.017047', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099492', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-140.88', 'loss/train': '0.65214', 'examples_per_second': '24.092', 'grad_norm': '20.125', 'counters/examples': 62944, 'counters/updates': 1967}
train stats after 62976 examples: {'rewards_train/chosen': '0.082794', 'rewards_train/rejected': '0.031844', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05095', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-161.11', 'loss/train': '0.67092', 'examples_per_second': '31.664', 'grad_norm': '21.375', 'counters/examples': 62976, 'counters/updates': 1968}
skipping logging after 63008 examples to avoid logging too frequently
train stats after 63040 examples: {'rewards_train/chosen': '0.080135', 'rewards_train/rejected': '0.017064', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063071', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-144.78', 'loss/train': '0.66877', 'examples_per_second': '32.651', 'grad_norm': '21', 'counters/examples': 63040, 'counters/updates': 1970}
train stats after 63072 examples: {'rewards_train/chosen': '0.11718', 'rewards_train/rejected': '-0.0036301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12081', 'logps_train/rejected': '-112.81', 'logps_train/chosen': '-133.47', 'loss/train': '0.64021', 'examples_per_second': '31.174', 'grad_norm': '19.25', 'counters/examples': 63072, 'counters/updates': 1971}
train stats after 63104 examples: {'rewards_train/chosen': '0.096106', 'rewards_train/rejected': '0.080104', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016002', 'logps_train/rejected': '-136.51', 'logps_train/chosen': '-128.2', 'loss/train': '0.69177', 'examples_per_second': '30.864', 'grad_norm': '21.625', 'counters/examples': 63104, 'counters/updates': 1972}
train stats after 63136 examples: {'rewards_train/chosen': '0.084846', 'rewards_train/rejected': '0.074832', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.010014', 'logps_train/rejected': '-102.45', 'logps_train/chosen': '-128.83', 'loss/train': '0.69434', 'examples_per_second': '30.152', 'grad_norm': '20', 'counters/examples': 63136, 'counters/updates': 1973}
train stats after 63168 examples: {'rewards_train/chosen': '0.079798', 'rewards_train/rejected': '0.039296', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040502', 'logps_train/rejected': '-109.4', 'logps_train/chosen': '-111.79', 'loss/train': '0.67903', 'examples_per_second': '31.332', 'grad_norm': '18.25', 'counters/examples': 63168, 'counters/updates': 1974}
train stats after 63200 examples: {'rewards_train/chosen': '0.11254', 'rewards_train/rejected': '0.073701', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038835', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-131.54', 'loss/train': '0.67935', 'examples_per_second': '30.962', 'grad_norm': '19.625', 'counters/examples': 63200, 'counters/updates': 1975}
skipping logging after 63232 examples to avoid logging too frequently
train stats after 63264 examples: {'rewards_train/chosen': '0.037327', 'rewards_train/rejected': '-0.0037628', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04109', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-135.66', 'loss/train': '0.68168', 'examples_per_second': '31.633', 'grad_norm': '20.875', 'counters/examples': 63264, 'counters/updates': 1977}
train stats after 63296 examples: {'rewards_train/chosen': '0.13788', 'rewards_train/rejected': '0.025401', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11248', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-149.86', 'loss/train': '0.64451', 'examples_per_second': '32.7', 'grad_norm': '19.75', 'counters/examples': 63296, 'counters/updates': 1978}
train stats after 63328 examples: {'rewards_train/chosen': '0.088154', 'rewards_train/rejected': '0.013968', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074186', 'logps_train/rejected': '-126.74', 'logps_train/chosen': '-125.56', 'loss/train': '0.66186', 'examples_per_second': '31.79', 'grad_norm': '19.875', 'counters/examples': 63328, 'counters/updates': 1979}
skipping logging after 63360 examples to avoid logging too frequently
train stats after 63392 examples: {'rewards_train/chosen': '0.15036', 'rewards_train/rejected': '0.060231', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090124', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-164.38', 'loss/train': '0.65645', 'examples_per_second': '34.315', 'grad_norm': '21', 'counters/examples': 63392, 'counters/updates': 1981}
train stats after 63424 examples: {'rewards_train/chosen': '0.15036', 'rewards_train/rejected': '0.047611', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10275', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-130.9', 'loss/train': '0.64899', 'examples_per_second': '32.074', 'grad_norm': '17.75', 'counters/examples': 63424, 'counters/updates': 1982}
train stats after 63456 examples: {'rewards_train/chosen': '0.073309', 'rewards_train/rejected': '0.036179', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03713', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-114.35', 'loss/train': '0.68022', 'examples_per_second': '31.621', 'grad_norm': '20.125', 'counters/examples': 63456, 'counters/updates': 1983}
train stats after 63488 examples: {'rewards_train/chosen': '0.040015', 'rewards_train/rejected': '0.037015', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0029997', 'logps_train/rejected': '-113.5', 'logps_train/chosen': '-135.22', 'loss/train': '0.69517', 'examples_per_second': '30.307', 'grad_norm': '19.875', 'counters/examples': 63488, 'counters/updates': 1984}
train stats after 63520 examples: {'rewards_train/chosen': '0.069634', 'rewards_train/rejected': '0.12006', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.050423', 'logps_train/rejected': '-131', 'logps_train/chosen': '-126.89', 'loss/train': '0.72599', 'examples_per_second': '32.941', 'grad_norm': '21.5', 'counters/examples': 63520, 'counters/updates': 1985}
train stats after 63552 examples: {'rewards_train/chosen': '0.12627', 'rewards_train/rejected': '0.075719', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050549', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-127.98', 'loss/train': '0.67404', 'examples_per_second': '31.358', 'grad_norm': '19.875', 'counters/examples': 63552, 'counters/updates': 1986}
train stats after 63584 examples: {'rewards_train/chosen': '0.074569', 'rewards_train/rejected': '0.019303', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055266', 'logps_train/rejected': '-115.39', 'logps_train/chosen': '-125.27', 'loss/train': '0.66984', 'examples_per_second': '32.008', 'grad_norm': '19.5', 'counters/examples': 63584, 'counters/updates': 1987}
train stats after 63616 examples: {'rewards_train/chosen': '0.13564', 'rewards_train/rejected': '0.035054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10058', 'logps_train/rejected': '-115.69', 'logps_train/chosen': '-178.94', 'loss/train': '0.6486', 'examples_per_second': '31.492', 'grad_norm': '21.625', 'counters/examples': 63616, 'counters/updates': 1988}
skipping logging after 63648 examples to avoid logging too frequently
train stats after 63680 examples: {'rewards_train/chosen': '0.093356', 'rewards_train/rejected': '0.11222', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018866', 'logps_train/rejected': '-128.26', 'logps_train/chosen': '-127.3', 'loss/train': '0.70864', 'examples_per_second': '30.915', 'grad_norm': '20.875', 'counters/examples': 63680, 'counters/updates': 1990}
train stats after 63712 examples: {'rewards_train/chosen': '0.091576', 'rewards_train/rejected': '0.020361', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071215', 'logps_train/rejected': '-114.56', 'logps_train/chosen': '-146.06', 'loss/train': '0.6667', 'examples_per_second': '32.119', 'grad_norm': '21', 'counters/examples': 63712, 'counters/updates': 1991}
train stats after 63744 examples: {'rewards_train/chosen': '0.14141', 'rewards_train/rejected': '0.041828', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099581', 'logps_train/rejected': '-127', 'logps_train/chosen': '-190.57', 'loss/train': '0.6507', 'examples_per_second': '33.149', 'grad_norm': '22', 'counters/examples': 63744, 'counters/updates': 1992}
train stats after 63776 examples: {'rewards_train/chosen': '0.04444', 'rewards_train/rejected': '0.056306', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011867', 'logps_train/rejected': '-147.39', 'logps_train/chosen': '-149.85', 'loss/train': '0.70467', 'examples_per_second': '31.842', 'grad_norm': '22.25', 'counters/examples': 63776, 'counters/updates': 1993}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63840 examples: {'rewards_train/chosen': '0.088438', 'rewards_train/rejected': '0.056367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032071', 'logps_train/rejected': '-112.83', 'logps_train/chosen': '-139.72', 'loss/train': '0.68076', 'examples_per_second': '31.268', 'grad_norm': '20.125', 'counters/examples': 63840, 'counters/updates': 1995}
train stats after 63872 examples: {'rewards_train/chosen': '0.02765', 'rewards_train/rejected': '0.0010256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026625', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-126.41', 'loss/train': '0.68218', 'examples_per_second': '32.834', 'grad_norm': '21.625', 'counters/examples': 63872, 'counters/updates': 1996}
train stats after 63904 examples: {'rewards_train/chosen': '0.04422', 'rewards_train/rejected': '0.039602', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0046182', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-111.68', 'loss/train': '0.69643', 'examples_per_second': '31.334', 'grad_norm': '20.125', 'counters/examples': 63904, 'counters/updates': 1997}
train stats after 63936 examples: {'rewards_train/chosen': '0.10573', 'rewards_train/rejected': '0.035727', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-146.63', 'loss/train': '0.66658', 'examples_per_second': '31.052', 'grad_norm': '20.625', 'counters/examples': 63936, 'counters/updates': 1998}
train stats after 63968 examples: {'rewards_train/chosen': '0.13355', 'rewards_train/rejected': '0.057699', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07585', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-152.86', 'loss/train': '0.66219', 'examples_per_second': '31.655', 'grad_norm': '20.625', 'counters/examples': 63968, 'counters/updates': 1999}
train stats after 64000 examples: {'rewards_train/chosen': '0.14417', 'rewards_train/rejected': '0.073246', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.070927', 'logps_train/rejected': '-117.03', 'logps_train/chosen': '-156.38', 'loss/train': '0.66253', 'examples_per_second': '32.193', 'grad_norm': '21.375', 'counters/examples': 64000, 'counters/updates': 2000}
train stats after 64032 examples: {'rewards_train/chosen': '0.09776', 'rewards_train/rejected': '0.024915', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072844', 'logps_train/rejected': '-95.358', 'logps_train/chosen': '-139', 'loss/train': '0.66183', 'examples_per_second': '31.937', 'grad_norm': '20.375', 'counters/examples': 64032, 'counters/updates': 2001}
train stats after 64064 examples: {'rewards_train/chosen': '0.080193', 'rewards_train/rejected': '0.05693', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023263', 'logps_train/rejected': '-143.95', 'logps_train/chosen': '-155.94', 'loss/train': '0.68896', 'examples_per_second': '31.686', 'grad_norm': '21', 'counters/examples': 64064, 'counters/updates': 2002}
skipping logging after 64096 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '0.051593', 'rewards_train/rejected': '0.063416', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011824', 'logps_train/rejected': '-107.14', 'logps_train/chosen': '-120.68', 'loss/train': '0.70402', 'examples_per_second': '31.558', 'grad_norm': '20.75', 'counters/examples': 64128, 'counters/updates': 2004}
train stats after 64160 examples: {'rewards_train/chosen': '0.084417', 'rewards_train/rejected': '0.052202', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032214', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-127.26', 'loss/train': '0.6818', 'examples_per_second': '32.416', 'grad_norm': '20.5', 'counters/examples': 64160, 'counters/updates': 2005}
train stats after 64192 examples: {'rewards_train/chosen': '0.067259', 'rewards_train/rejected': '0.0085251', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058733', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-138.26', 'loss/train': '0.67182', 'examples_per_second': '25.075', 'grad_norm': '21', 'counters/examples': 64192, 'counters/updates': 2006}
train stats after 64224 examples: {'rewards_train/chosen': '0.13529', 'rewards_train/rejected': '0.058373', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076919', 'logps_train/rejected': '-124.81', 'logps_train/chosen': '-130.87', 'loss/train': '0.66082', 'examples_per_second': '31.699', 'grad_norm': '19.75', 'counters/examples': 64224, 'counters/updates': 2007}
skipping logging after 64256 examples to avoid logging too frequently
train stats after 64288 examples: {'rewards_train/chosen': '0.14554', 'rewards_train/rejected': '0.028119', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11743', 'logps_train/rejected': '-100.78', 'logps_train/chosen': '-149.13', 'loss/train': '0.64022', 'examples_per_second': '31.542', 'grad_norm': '19.25', 'counters/examples': 64288, 'counters/updates': 2009}
train stats after 64320 examples: {'rewards_train/chosen': '0.069762', 'rewards_train/rejected': '0.033003', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036759', 'logps_train/rejected': '-113', 'logps_train/chosen': '-118.02', 'loss/train': '0.67885', 'examples_per_second': '32.779', 'grad_norm': '20.375', 'counters/examples': 64320, 'counters/updates': 2010}
train stats after 64352 examples: {'rewards_train/chosen': '0.13273', 'rewards_train/rejected': '0.083881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048854', 'logps_train/rejected': '-140.6', 'logps_train/chosen': '-145.2', 'loss/train': '0.67219', 'examples_per_second': '30.378', 'grad_norm': '21.5', 'counters/examples': 64352, 'counters/updates': 2011}
train stats after 64384 examples: {'rewards_train/chosen': '0.08429', 'rewards_train/rejected': '0.085815', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0015255', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-141.08', 'loss/train': '0.70066', 'examples_per_second': '31.418', 'grad_norm': '21.625', 'counters/examples': 64384, 'counters/updates': 2012}
skipping logging after 64416 examples to avoid logging too frequently
train stats after 64448 examples: {'rewards_train/chosen': '0.098588', 'rewards_train/rejected': '0.12299', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0244', 'logps_train/rejected': '-174.28', 'logps_train/chosen': '-189.08', 'loss/train': '0.70975', 'examples_per_second': '32.128', 'grad_norm': '24.5', 'counters/examples': 64448, 'counters/updates': 2014}
train stats after 64480 examples: {'rewards_train/chosen': '0.079936', 'rewards_train/rejected': '0.00077164', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.079165', 'logps_train/rejected': '-113.78', 'logps_train/chosen': '-108.97', 'loss/train': '0.65958', 'examples_per_second': '31.679', 'grad_norm': '17.875', 'counters/examples': 64480, 'counters/updates': 2015}
skipping logging after 64512 examples to avoid logging too frequently
train stats after 64544 examples: {'rewards_train/chosen': '0.13586', 'rewards_train/rejected': '0.073114', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062746', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-132.38', 'loss/train': '0.66822', 'examples_per_second': '32.596', 'grad_norm': '19.625', 'counters/examples': 64544, 'counters/updates': 2017}
train stats after 64576 examples: {'rewards_train/chosen': '0.15121', 'rewards_train/rejected': '0.015919', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13529', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-159.38', 'loss/train': '0.63516', 'examples_per_second': '31.601', 'grad_norm': '21.375', 'counters/examples': 64576, 'counters/updates': 2018}
skipping logging after 64608 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '0.11485', 'rewards_train/rejected': '0.06058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054268', 'logps_train/rejected': '-117.74', 'logps_train/chosen': '-120.71', 'loss/train': '0.67165', 'examples_per_second': '31.157', 'grad_norm': '18.625', 'counters/examples': 64640, 'counters/updates': 2020}
skipping logging after 64672 examples to avoid logging too frequently
train stats after 64704 examples: {'rewards_train/chosen': '0.11866', 'rewards_train/rejected': '0.050187', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068472', 'logps_train/rejected': '-88.98', 'logps_train/chosen': '-97.392', 'loss/train': '0.66264', 'examples_per_second': '35.926', 'grad_norm': '16.875', 'counters/examples': 64704, 'counters/updates': 2022}
train stats after 64736 examples: {'rewards_train/chosen': '0.059693', 'rewards_train/rejected': '0.035821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023872', 'logps_train/rejected': '-119.43', 'logps_train/chosen': '-131.71', 'loss/train': '0.68748', 'examples_per_second': '30.627', 'grad_norm': '20.625', 'counters/examples': 64736, 'counters/updates': 2023}
skipping logging after 64768 examples to avoid logging too frequently
train stats after 64800 examples: {'rewards_train/chosen': '0.092387', 'rewards_train/rejected': '0.039621', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052766', 'logps_train/rejected': '-135.18', 'logps_train/chosen': '-114.96', 'loss/train': '0.67119', 'examples_per_second': '32.537', 'grad_norm': '18.625', 'counters/examples': 64800, 'counters/updates': 2025}
train stats after 64832 examples: {'rewards_train/chosen': '0.19731', 'rewards_train/rejected': '0.037127', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16018', 'logps_train/rejected': '-104.86', 'logps_train/chosen': '-144.27', 'loss/train': '0.62213', 'examples_per_second': '30.596', 'grad_norm': '18.125', 'counters/examples': 64832, 'counters/updates': 2026}
train stats after 64864 examples: {'rewards_train/chosen': '0.054637', 'rewards_train/rejected': '0.05', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0046364', 'logps_train/rejected': '-133.25', 'logps_train/chosen': '-156.37', 'loss/train': '0.69477', 'examples_per_second': '31.886', 'grad_norm': '22.875', 'counters/examples': 64864, 'counters/updates': 2027}
train stats after 64896 examples: {'rewards_train/chosen': '0.081297', 'rewards_train/rejected': '0.07289', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0084074', 'logps_train/rejected': '-172.89', 'logps_train/chosen': '-178.5', 'loss/train': '0.69321', 'examples_per_second': '30.99', 'grad_norm': '25', 'counters/examples': 64896, 'counters/updates': 2028}
skipping logging after 64928 examples to avoid logging too frequently
train stats after 64960 examples: {'rewards_train/chosen': '0.062063', 'rewards_train/rejected': '0.034796', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027267', 'logps_train/rejected': '-103.14', 'logps_train/chosen': '-109.34', 'loss/train': '0.68235', 'examples_per_second': '31.606', 'grad_norm': '18.625', 'counters/examples': 64960, 'counters/updates': 2030}
skipping logging after 64992 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '0.077888', 'rewards_train/rejected': '0.041321', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.036567', 'logps_train/rejected': '-107.4', 'logps_train/chosen': '-118.36', 'loss/train': '0.67875', 'examples_per_second': '30.319', 'grad_norm': '18.625', 'counters/examples': 65024, 'counters/updates': 2032}
train stats after 65056 examples: {'rewards_train/chosen': '0.094832', 'rewards_train/rejected': '0.013777', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.081055', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-146.23', 'loss/train': '0.65809', 'examples_per_second': '30.861', 'grad_norm': '20.125', 'counters/examples': 65056, 'counters/updates': 2033}
train stats after 65088 examples: {'rewards_train/chosen': '0.11205', 'rewards_train/rejected': '0.066002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046052', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-136.05', 'loss/train': '0.67634', 'examples_per_second': '31.036', 'grad_norm': '21', 'counters/examples': 65088, 'counters/updates': 2034}
train stats after 65120 examples: {'rewards_train/chosen': '0.11997', 'rewards_train/rejected': '-0.011172', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13115', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-155.43', 'loss/train': '0.63506', 'examples_per_second': '31.653', 'grad_norm': '20.25', 'counters/examples': 65120, 'counters/updates': 2035}
train stats after 65152 examples: {'rewards_train/chosen': '0.10263', 'rewards_train/rejected': '-0.00094786', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10358', 'logps_train/rejected': '-95.285', 'logps_train/chosen': '-127.78', 'loss/train': '0.64601', 'examples_per_second': '31.362', 'grad_norm': '18.25', 'counters/examples': 65152, 'counters/updates': 2036}
train stats after 65184 examples: {'rewards_train/chosen': '0.14319', 'rewards_train/rejected': '0.049186', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094001', 'logps_train/rejected': '-102.02', 'logps_train/chosen': '-149.25', 'loss/train': '0.65388', 'examples_per_second': '31.589', 'grad_norm': '21.125', 'counters/examples': 65184, 'counters/updates': 2037}
train stats after 65216 examples: {'rewards_train/chosen': '0.11606', 'rewards_train/rejected': '0.061211', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054847', 'logps_train/rejected': '-113.35', 'logps_train/chosen': '-152.62', 'loss/train': '0.66975', 'examples_per_second': '31.044', 'grad_norm': '21.75', 'counters/examples': 65216, 'counters/updates': 2038}
train stats after 65248 examples: {'rewards_train/chosen': '0.091037', 'rewards_train/rejected': '0.01039', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.080647', 'logps_train/rejected': '-111.44', 'logps_train/chosen': '-140.59', 'loss/train': '0.66026', 'examples_per_second': '32.199', 'grad_norm': '20.75', 'counters/examples': 65248, 'counters/updates': 2039}
train stats after 65280 examples: {'rewards_train/chosen': '0.095603', 'rewards_train/rejected': '0.0071314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088472', 'logps_train/rejected': '-102.98', 'logps_train/chosen': '-122.21', 'loss/train': '0.663', 'examples_per_second': '31.647', 'grad_norm': '19.75', 'counters/examples': 65280, 'counters/updates': 2040}
skipping logging after 65312 examples to avoid logging too frequently
train stats after 65344 examples: {'rewards_train/chosen': '0.047375', 'rewards_train/rejected': '-0.0033656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050741', 'logps_train/rejected': '-105', 'logps_train/chosen': '-161.06', 'loss/train': '0.67121', 'examples_per_second': '30.212', 'grad_norm': '20.625', 'counters/examples': 65344, 'counters/updates': 2042}
train stats after 65376 examples: {'rewards_train/chosen': '0.13048', 'rewards_train/rejected': '-0.042441', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17292', 'logps_train/rejected': '-95.767', 'logps_train/chosen': '-151.56', 'loss/train': '0.61676', 'examples_per_second': '30.219', 'grad_norm': '19.5', 'counters/examples': 65376, 'counters/updates': 2043}
train stats after 65408 examples: {'rewards_train/chosen': '0.1138', 'rewards_train/rejected': '0.024345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089459', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-179.73', 'loss/train': '0.65419', 'examples_per_second': '31.671', 'grad_norm': '22.125', 'counters/examples': 65408, 'counters/updates': 2044}
train stats after 65440 examples: {'rewards_train/chosen': '0.061036', 'rewards_train/rejected': '0.027717', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033319', 'logps_train/rejected': '-131.35', 'logps_train/chosen': '-142.93', 'loss/train': '0.68438', 'examples_per_second': '31.446', 'grad_norm': '21.75', 'counters/examples': 65440, 'counters/updates': 2045}
train stats after 65472 examples: {'rewards_train/chosen': '0.084745', 'rewards_train/rejected': '0.058458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026287', 'logps_train/rejected': '-111.86', 'logps_train/chosen': '-165.71', 'loss/train': '0.68515', 'examples_per_second': '30.428', 'grad_norm': '21', 'counters/examples': 65472, 'counters/updates': 2046}
skipping logging after 65504 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '0.10189', 'rewards_train/rejected': '0.07257', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02932', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-117.61', 'loss/train': '0.6864', 'examples_per_second': '32.387', 'grad_norm': '19.25', 'counters/examples': 65536, 'counters/updates': 2048}
train stats after 65568 examples: {'rewards_train/chosen': '0.084779', 'rewards_train/rejected': '0.055037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029742', 'logps_train/rejected': '-113.87', 'logps_train/chosen': '-145.82', 'loss/train': '0.68472', 'examples_per_second': '30.824', 'grad_norm': '20.5', 'counters/examples': 65568, 'counters/updates': 2049}
train stats after 65600 examples: {'rewards_train/chosen': '0.091267', 'rewards_train/rejected': '0.057639', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033629', 'logps_train/rejected': '-102.79', 'logps_train/chosen': '-144.13', 'loss/train': '0.68263', 'examples_per_second': '31.695', 'grad_norm': '20.375', 'counters/examples': 65600, 'counters/updates': 2050}
train stats after 65632 examples: {'rewards_train/chosen': '0.11333', 'rewards_train/rejected': '0.015496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097829', 'logps_train/rejected': '-87.208', 'logps_train/chosen': '-126.08', 'loss/train': '0.65046', 'examples_per_second': '33.015', 'grad_norm': '17.375', 'counters/examples': 65632, 'counters/updates': 2051}
train stats after 65664 examples: {'rewards_train/chosen': '0.19684', 'rewards_train/rejected': '0.067983', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12885', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-177.1', 'loss/train': '0.64492', 'examples_per_second': '32.068', 'grad_norm': '21.375', 'counters/examples': 65664, 'counters/updates': 2052}
train stats after 65696 examples: {'rewards_train/chosen': '0.035655', 'rewards_train/rejected': '0.038826', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0031715', 'logps_train/rejected': '-86.935', 'logps_train/chosen': '-110.82', 'loss/train': '0.69963', 'examples_per_second': '30.832', 'grad_norm': '17.625', 'counters/examples': 65696, 'counters/updates': 2053}
skipping logging after 65728 examples to avoid logging too frequently
train stats after 65760 examples: {'rewards_train/chosen': '0.10481', 'rewards_train/rejected': '0.068005', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036808', 'logps_train/rejected': '-134.8', 'logps_train/chosen': '-105.02', 'loss/train': '0.68063', 'examples_per_second': '31.218', 'grad_norm': '19.375', 'counters/examples': 65760, 'counters/updates': 2055}
train stats after 65792 examples: {'rewards_train/chosen': '0.076071', 'rewards_train/rejected': '0.03553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040541', 'logps_train/rejected': '-99.924', 'logps_train/chosen': '-118.07', 'loss/train': '0.6775', 'examples_per_second': '33.052', 'grad_norm': '18', 'counters/examples': 65792, 'counters/updates': 2056}
train stats after 65824 examples: {'rewards_train/chosen': '0.075584', 'rewards_train/rejected': '-0.0032721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078856', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-170.51', 'loss/train': '0.66351', 'examples_per_second': '31.411', 'grad_norm': '21', 'counters/examples': 65824, 'counters/updates': 2057}
train stats after 65856 examples: {'rewards_train/chosen': '0.056717', 'rewards_train/rejected': '0.017537', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039179', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-137.61', 'loss/train': '0.67713', 'examples_per_second': '30.697', 'grad_norm': '20.875', 'counters/examples': 65856, 'counters/updates': 2058}
train stats after 65888 examples: {'rewards_train/chosen': '0.074273', 'rewards_train/rejected': '0.022675', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051598', 'logps_train/rejected': '-78.374', 'logps_train/chosen': '-115.26', 'loss/train': '0.6742', 'examples_per_second': '31.508', 'grad_norm': '17.25', 'counters/examples': 65888, 'counters/updates': 2059}
train stats after 65920 examples: {'rewards_train/chosen': '0.12728', 'rewards_train/rejected': '0.024349', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10293', 'logps_train/rejected': '-151.6', 'logps_train/chosen': '-147.86', 'loss/train': '0.65313', 'examples_per_second': '31.551', 'grad_norm': '22.625', 'counters/examples': 65920, 'counters/updates': 2060}
train stats after 65952 examples: {'rewards_train/chosen': '0.17206', 'rewards_train/rejected': '0.037277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13478', 'logps_train/rejected': '-79.396', 'logps_train/chosen': '-130.48', 'loss/train': '0.63341', 'examples_per_second': '31.554', 'grad_norm': '18.125', 'counters/examples': 65952, 'counters/updates': 2061}
train stats after 65984 examples: {'rewards_train/chosen': '0.11161', 'rewards_train/rejected': '-0.0054653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11707', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-137.22', 'loss/train': '0.64517', 'examples_per_second': '30.266', 'grad_norm': '21.375', 'counters/examples': 65984, 'counters/updates': 2062}
train stats after 66016 examples: {'rewards_train/chosen': '0.063081', 'rewards_train/rejected': '0.076554', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013472', 'logps_train/rejected': '-103.72', 'logps_train/chosen': '-139.47', 'loss/train': '0.70672', 'examples_per_second': '31.21', 'grad_norm': '21.75', 'counters/examples': 66016, 'counters/updates': 2063}
train stats after 66048 examples: {'rewards_train/chosen': '0.066546', 'rewards_train/rejected': '0.019142', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047405', 'logps_train/rejected': '-134.42', 'logps_train/chosen': '-155.86', 'loss/train': '0.6759', 'examples_per_second': '31.669', 'grad_norm': '21.75', 'counters/examples': 66048, 'counters/updates': 2064}
train stats after 66080 examples: {'rewards_train/chosen': '0.074885', 'rewards_train/rejected': '0.033529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041356', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-120.62', 'loss/train': '0.67471', 'examples_per_second': '30.829', 'grad_norm': '20.5', 'counters/examples': 66080, 'counters/updates': 2065}
train stats after 66112 examples: {'rewards_train/chosen': '0.076995', 'rewards_train/rejected': '-0.035297', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11229', 'logps_train/rejected': '-113.96', 'logps_train/chosen': '-118.54', 'loss/train': '0.64162', 'examples_per_second': '31.183', 'grad_norm': '18.375', 'counters/examples': 66112, 'counters/updates': 2066}
train stats after 66144 examples: {'rewards_train/chosen': '0.094512', 'rewards_train/rejected': '0.081171', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.013341', 'logps_train/rejected': '-124.69', 'logps_train/chosen': '-126.4', 'loss/train': '0.69311', 'examples_per_second': '31.103', 'grad_norm': '20', 'counters/examples': 66144, 'counters/updates': 2067}
skipping logging after 66176 examples to avoid logging too frequently
train stats after 66208 examples: {'rewards_train/chosen': '0.098291', 'rewards_train/rejected': '0.045091', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0532', 'logps_train/rejected': '-105.62', 'logps_train/chosen': '-121.86', 'loss/train': '0.66926', 'examples_per_second': '32.722', 'grad_norm': '18.5', 'counters/examples': 66208, 'counters/updates': 2069}
train stats after 66240 examples: {'rewards_train/chosen': '0.13977', 'rewards_train/rejected': '-0.014782', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15455', 'logps_train/rejected': '-133.58', 'logps_train/chosen': '-142.05', 'loss/train': '0.62374', 'examples_per_second': '32.04', 'grad_norm': '19.625', 'counters/examples': 66240, 'counters/updates': 2070}
train stats after 66272 examples: {'rewards_train/chosen': '0.1433', 'rewards_train/rejected': '0.014626', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12868', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-155.73', 'loss/train': '0.63739', 'examples_per_second': '30.35', 'grad_norm': '20.25', 'counters/examples': 66272, 'counters/updates': 2071}
train stats after 66304 examples: {'rewards_train/chosen': '0.073506', 'rewards_train/rejected': '0.022306', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051199', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-122.62', 'loss/train': '0.67306', 'examples_per_second': '30.21', 'grad_norm': '19.125', 'counters/examples': 66304, 'counters/updates': 2072}
skipping logging after 66336 examples to avoid logging too frequently
train stats after 66368 examples: {'rewards_train/chosen': '0.10399', 'rewards_train/rejected': '0.031159', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072834', 'logps_train/rejected': '-137.41', 'logps_train/chosen': '-179.84', 'loss/train': '0.66473', 'examples_per_second': '32.769', 'grad_norm': '22.875', 'counters/examples': 66368, 'counters/updates': 2074}
train stats after 66400 examples: {'rewards_train/chosen': '0.11231', 'rewards_train/rejected': '0.10061', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011692', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-138.57', 'loss/train': '0.69156', 'examples_per_second': '30.686', 'grad_norm': '21.375', 'counters/examples': 66400, 'counters/updates': 2075}
train stats after 66432 examples: {'rewards_train/chosen': '0.083099', 'rewards_train/rejected': '0.035864', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047235', 'logps_train/rejected': '-112.02', 'logps_train/chosen': '-128.82', 'loss/train': '0.67681', 'examples_per_second': '31.791', 'grad_norm': '20.375', 'counters/examples': 66432, 'counters/updates': 2076}
train stats after 66464 examples: {'rewards_train/chosen': '0.07504', 'rewards_train/rejected': '0.042936', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032104', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-165.27', 'loss/train': '0.68349', 'examples_per_second': '31.278', 'grad_norm': '21.5', 'counters/examples': 66464, 'counters/updates': 2077}
train stats after 66496 examples: {'rewards_train/chosen': '0.08948', 'rewards_train/rejected': '0.062294', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027185', 'logps_train/rejected': '-101.19', 'logps_train/chosen': '-128.86', 'loss/train': '0.68585', 'examples_per_second': '31.712', 'grad_norm': '19.875', 'counters/examples': 66496, 'counters/updates': 2078}
train stats after 66528 examples: {'rewards_train/chosen': '0.081091', 'rewards_train/rejected': '0.045081', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03601', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-162.25', 'loss/train': '0.68192', 'examples_per_second': '31.872', 'grad_norm': '21.375', 'counters/examples': 66528, 'counters/updates': 2079}
train stats after 66560 examples: {'rewards_train/chosen': '0.12188', 'rewards_train/rejected': '0.03427', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087613', 'logps_train/rejected': '-103.83', 'logps_train/chosen': '-124.82', 'loss/train': '0.6586', 'examples_per_second': '30.635', 'grad_norm': '18.5', 'counters/examples': 66560, 'counters/updates': 2080}
train stats after 66592 examples: {'rewards_train/chosen': '0.094457', 'rewards_train/rejected': '0.023788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07067', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-115.21', 'loss/train': '0.66361', 'examples_per_second': '31.846', 'grad_norm': '18.375', 'counters/examples': 66592, 'counters/updates': 2081}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66656 examples: {'rewards_train/chosen': '0.12109', 'rewards_train/rejected': '-0.018389', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13947', 'logps_train/rejected': '-80.401', 'logps_train/chosen': '-94.338', 'loss/train': '0.62986', 'examples_per_second': '32.891', 'grad_norm': '15', 'counters/examples': 66656, 'counters/updates': 2083}
skipping logging after 66688 examples to avoid logging too frequently
train stats after 66720 examples: {'rewards_train/chosen': '0.085655', 'rewards_train/rejected': '0.03596', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049696', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-142.5', 'loss/train': '0.67536', 'examples_per_second': '31.394', 'grad_norm': '20.625', 'counters/examples': 66720, 'counters/updates': 2085}
train stats after 66752 examples: {'rewards_train/chosen': '0.10469', 'rewards_train/rejected': '0.091098', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.01359', 'logps_train/rejected': '-156.68', 'logps_train/chosen': '-163.07', 'loss/train': '0.69226', 'examples_per_second': '31.65', 'grad_norm': '24.625', 'counters/examples': 66752, 'counters/updates': 2086}
skipping logging after 66784 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '0.18437', 'rewards_train/rejected': '0.040945', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14343', 'logps_train/rejected': '-148.83', 'logps_train/chosen': '-147.65', 'loss/train': '0.63078', 'examples_per_second': '31.318', 'grad_norm': '22.875', 'counters/examples': 66816, 'counters/updates': 2088}
skipping logging after 66848 examples to avoid logging too frequently
train stats after 66880 examples: {'rewards_train/chosen': '0.088016', 'rewards_train/rejected': '0.093', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0049839', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-127.74', 'loss/train': '0.69877', 'examples_per_second': '31.633', 'grad_norm': '20.125', 'counters/examples': 66880, 'counters/updates': 2090}
skipping logging after 66912 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '0.077688', 'rewards_train/rejected': '0.084694', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0070064', 'logps_train/rejected': '-104.16', 'logps_train/chosen': '-132.48', 'loss/train': '0.70246', 'examples_per_second': '32.705', 'grad_norm': '20.25', 'counters/examples': 66944, 'counters/updates': 2092}
train stats after 66976 examples: {'rewards_train/chosen': '0.10273', 'rewards_train/rejected': '0.076542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026188', 'logps_train/rejected': '-100.78', 'logps_train/chosen': '-130.43', 'loss/train': '0.68365', 'examples_per_second': '32.808', 'grad_norm': '20.375', 'counters/examples': 66976, 'counters/updates': 2093}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67040 examples: {'rewards_train/chosen': '0.050417', 'rewards_train/rejected': '0.04026', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010156', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-136.4', 'loss/train': '0.69484', 'examples_per_second': '31.404', 'grad_norm': '21.5', 'counters/examples': 67040, 'counters/updates': 2095}
train stats after 67072 examples: {'rewards_train/chosen': '0.037444', 'rewards_train/rejected': '0.014399', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023046', 'logps_train/rejected': '-108.31', 'logps_train/chosen': '-107.42', 'loss/train': '0.68424', 'examples_per_second': '31.416', 'grad_norm': '18.375', 'counters/examples': 67072, 'counters/updates': 2096}
train stats after 67104 examples: {'rewards_train/chosen': '0.097007', 'rewards_train/rejected': '0.019855', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077151', 'logps_train/rejected': '-106.39', 'logps_train/chosen': '-130.73', 'loss/train': '0.66147', 'examples_per_second': '31.795', 'grad_norm': '18.125', 'counters/examples': 67104, 'counters/updates': 2097}
train stats after 67136 examples: {'rewards_train/chosen': '0.12263', 'rewards_train/rejected': '0.062066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060564', 'logps_train/rejected': '-127.81', 'logps_train/chosen': '-145.48', 'loss/train': '0.66939', 'examples_per_second': '30.67', 'grad_norm': '21.625', 'counters/examples': 67136, 'counters/updates': 2098}
train stats after 67168 examples: {'rewards_train/chosen': '0.083878', 'rewards_train/rejected': '0.058388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02549', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-114.43', 'loss/train': '0.68446', 'examples_per_second': '31.583', 'grad_norm': '19.25', 'counters/examples': 67168, 'counters/updates': 2099}
train stats after 67200 examples: {'rewards_train/chosen': '0.02845', 'rewards_train/rejected': '-0.0074379', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035888', 'logps_train/rejected': '-99.662', 'logps_train/chosen': '-105.79', 'loss/train': '0.67786', 'examples_per_second': '30.616', 'grad_norm': '18.75', 'counters/examples': 67200, 'counters/updates': 2100}
skipping logging after 67232 examples to avoid logging too frequently
train stats after 67264 examples: {'rewards_train/chosen': '0.10661', 'rewards_train/rejected': '0.086166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020444', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-126.94', 'loss/train': '0.68922', 'examples_per_second': '31.665', 'grad_norm': '19.5', 'counters/examples': 67264, 'counters/updates': 2102}
skipping logging after 67296 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '0.06238', 'rewards_train/rejected': '0.059126', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0032542', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-116.12', 'loss/train': '0.69571', 'examples_per_second': '32.032', 'grad_norm': '20.125', 'counters/examples': 67328, 'counters/updates': 2104}
skipping logging after 67360 examples to avoid logging too frequently
train stats after 67392 examples: {'rewards_train/chosen': '0.07227', 'rewards_train/rejected': '0.004368', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067902', 'logps_train/rejected': '-95.253', 'logps_train/chosen': '-116.92', 'loss/train': '0.66352', 'examples_per_second': '35.954', 'grad_norm': '17.25', 'counters/examples': 67392, 'counters/updates': 2106}
train stats after 67424 examples: {'rewards_train/chosen': '0.13054', 'rewards_train/rejected': '0.0067895', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12375', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-137.07', 'loss/train': '0.63928', 'examples_per_second': '31.675', 'grad_norm': '20.25', 'counters/examples': 67424, 'counters/updates': 2107}
skipping logging after 67456 examples to avoid logging too frequently
train stats after 67488 examples: {'rewards_train/chosen': '0.085444', 'rewards_train/rejected': '0.084146', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0012979', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-147.84', 'loss/train': '0.69617', 'examples_per_second': '30.981', 'grad_norm': '21.625', 'counters/examples': 67488, 'counters/updates': 2109}
train stats after 67520 examples: {'rewards_train/chosen': '0.046991', 'rewards_train/rejected': '0.090888', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043897', 'logps_train/rejected': '-135.45', 'logps_train/chosen': '-124.29', 'loss/train': '0.7199', 'examples_per_second': '31.464', 'grad_norm': '21.625', 'counters/examples': 67520, 'counters/updates': 2110}
train stats after 67552 examples: {'rewards_train/chosen': '0.0934', 'rewards_train/rejected': '0.037927', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055473', 'logps_train/rejected': '-87.369', 'logps_train/chosen': '-145.01', 'loss/train': '0.67133', 'examples_per_second': '31.14', 'grad_norm': '19.875', 'counters/examples': 67552, 'counters/updates': 2111}
train stats after 67584 examples: {'rewards_train/chosen': '0.06723', 'rewards_train/rejected': '0.039014', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028216', 'logps_train/rejected': '-114.56', 'logps_train/chosen': '-119.81', 'loss/train': '0.68392', 'examples_per_second': '32.801', 'grad_norm': '19.375', 'counters/examples': 67584, 'counters/updates': 2112}
train stats after 67616 examples: {'rewards_train/chosen': '0.097686', 'rewards_train/rejected': '0.13677', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039087', 'logps_train/rejected': '-159.92', 'logps_train/chosen': '-140.23', 'loss/train': '0.71906', 'examples_per_second': '31.707', 'grad_norm': '22.75', 'counters/examples': 67616, 'counters/updates': 2113}
train stats after 67648 examples: {'rewards_train/chosen': '0.12485', 'rewards_train/rejected': '0.030207', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094646', 'logps_train/rejected': '-84.561', 'logps_train/chosen': '-148.03', 'loss/train': '0.65121', 'examples_per_second': '32.231', 'grad_norm': '19.375', 'counters/examples': 67648, 'counters/updates': 2114}
train stats after 67680 examples: {'rewards_train/chosen': '0.15979', 'rewards_train/rejected': '0.083072', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.076713', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-133.56', 'loss/train': '0.66047', 'examples_per_second': '30.663', 'grad_norm': '19.875', 'counters/examples': 67680, 'counters/updates': 2115}
train stats after 67712 examples: {'rewards_train/chosen': '0.11901', 'rewards_train/rejected': '0.049743', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.069269', 'logps_train/rejected': '-154.89', 'logps_train/chosen': '-157.36', 'loss/train': '0.66707', 'examples_per_second': '31.433', 'grad_norm': '22.25', 'counters/examples': 67712, 'counters/updates': 2116}
train stats after 67744 examples: {'rewards_train/chosen': '0.080909', 'rewards_train/rejected': '0.025606', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055302', 'logps_train/rejected': '-107.21', 'logps_train/chosen': '-129.51', 'loss/train': '0.6701', 'examples_per_second': '31.679', 'grad_norm': '19.25', 'counters/examples': 67744, 'counters/updates': 2117}
train stats after 67776 examples: {'rewards_train/chosen': '0.10982', 'rewards_train/rejected': '0.083542', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026278', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-122.97', 'loss/train': '0.68684', 'examples_per_second': '31.393', 'grad_norm': '20', 'counters/examples': 67776, 'counters/updates': 2118}
train stats after 67808 examples: {'rewards_train/chosen': '0.086689', 'rewards_train/rejected': '0.044627', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042062', 'logps_train/rejected': '-156.12', 'logps_train/chosen': '-153.5', 'loss/train': '0.67797', 'examples_per_second': '31.705', 'grad_norm': '22.25', 'counters/examples': 67808, 'counters/updates': 2119}
train stats after 67840 examples: {'rewards_train/chosen': '0.081023', 'rewards_train/rejected': '0.094176', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.013153', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-105.41', 'loss/train': '0.70686', 'examples_per_second': '31.731', 'grad_norm': '20', 'counters/examples': 67840, 'counters/updates': 2120}
skipping logging after 67872 examples to avoid logging too frequently
train stats after 67904 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '0.047207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067615', 'logps_train/rejected': '-83.329', 'logps_train/chosen': '-110.07', 'loss/train': '0.66292', 'examples_per_second': '34.529', 'grad_norm': '18.125', 'counters/examples': 67904, 'counters/updates': 2122}
train stats after 67936 examples: {'rewards_train/chosen': '0.042695', 'rewards_train/rejected': '0.054009', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011313', 'logps_train/rejected': '-120.24', 'logps_train/chosen': '-125.59', 'loss/train': '0.7042', 'examples_per_second': '32.778', 'grad_norm': '21.125', 'counters/examples': 67936, 'counters/updates': 2123}
train stats after 67968 examples: {'rewards_train/chosen': '0.10039', 'rewards_train/rejected': '0.024583', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075803', 'logps_train/rejected': '-102.03', 'logps_train/chosen': '-206.44', 'loss/train': '0.66165', 'examples_per_second': '31.361', 'grad_norm': '21.625', 'counters/examples': 67968, 'counters/updates': 2124}
skipping logging after 68000 examples to avoid logging too frequently
train stats after 68032 examples: {'rewards_train/chosen': '0.11787', 'rewards_train/rejected': '0.034425', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083447', 'logps_train/rejected': '-121.43', 'logps_train/chosen': '-140.33', 'loss/train': '0.65827', 'examples_per_second': '31.662', 'grad_norm': '21', 'counters/examples': 68032, 'counters/updates': 2126}
train stats after 68064 examples: {'rewards_train/chosen': '0.085786', 'rewards_train/rejected': '0.049267', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036519', 'logps_train/rejected': '-132.84', 'logps_train/chosen': '-170.55', 'loss/train': '0.67939', 'examples_per_second': '30.714', 'grad_norm': '22.625', 'counters/examples': 68064, 'counters/updates': 2127}
train stats after 68096 examples: {'rewards_train/chosen': '0.063756', 'rewards_train/rejected': '0.054708', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0090485', 'logps_train/rejected': '-118.48', 'logps_train/chosen': '-139.13', 'loss/train': '0.69312', 'examples_per_second': '30.044', 'grad_norm': '21.5', 'counters/examples': 68096, 'counters/updates': 2128}
skipping logging after 68128 examples to avoid logging too frequently
train stats after 68160 examples: {'rewards_train/chosen': '0.084744', 'rewards_train/rejected': '-0.022462', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10721', 'logps_train/rejected': '-120.35', 'logps_train/chosen': '-154.55', 'loss/train': '0.64611', 'examples_per_second': '32.657', 'grad_norm': '20', 'counters/examples': 68160, 'counters/updates': 2130}
train stats after 68192 examples: {'rewards_train/chosen': '0.089838', 'rewards_train/rejected': '0.019967', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069872', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-139.4', 'loss/train': '0.66426', 'examples_per_second': '30.237', 'grad_norm': '20.875', 'counters/examples': 68192, 'counters/updates': 2131}
train stats after 68224 examples: {'rewards_train/chosen': '0.088068', 'rewards_train/rejected': '-0.013359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10143', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-129.49', 'loss/train': '0.6502', 'examples_per_second': '30.545', 'grad_norm': '18.375', 'counters/examples': 68224, 'counters/updates': 2132}
train stats after 68256 examples: {'rewards_train/chosen': '0.14345', 'rewards_train/rejected': '0.11747', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.025985', 'logps_train/rejected': '-128.58', 'logps_train/chosen': '-130.9', 'loss/train': '0.6895', 'examples_per_second': '30.671', 'grad_norm': '20.375', 'counters/examples': 68256, 'counters/updates': 2133}
train stats after 68288 examples: {'rewards_train/chosen': '0.13425', 'rewards_train/rejected': '0.075637', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058612', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-137.01', 'loss/train': '0.67146', 'examples_per_second': '31.82', 'grad_norm': '20.25', 'counters/examples': 68288, 'counters/updates': 2134}
train stats after 68320 examples: {'rewards_train/chosen': '0.069132', 'rewards_train/rejected': '0.079968', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010836', 'logps_train/rejected': '-163.26', 'logps_train/chosen': '-152.43', 'loss/train': '0.70273', 'examples_per_second': '24.457', 'grad_norm': '23.875', 'counters/examples': 68320, 'counters/updates': 2135}
train stats after 68352 examples: {'rewards_train/chosen': '0.11481', 'rewards_train/rejected': '0.045585', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069225', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-142.25', 'loss/train': '0.66524', 'examples_per_second': '31.657', 'grad_norm': '21.625', 'counters/examples': 68352, 'counters/updates': 2136}
train stats after 68384 examples: {'rewards_train/chosen': '0.041364', 'rewards_train/rejected': '0.078609', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.037244', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-158.68', 'loss/train': '0.71567', 'examples_per_second': '33.016', 'grad_norm': '22.25', 'counters/examples': 68384, 'counters/updates': 2137}
train stats after 68416 examples: {'rewards_train/chosen': '0.073592', 'rewards_train/rejected': '0.10668', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.033093', 'logps_train/rejected': '-132.29', 'logps_train/chosen': '-174.54', 'loss/train': '0.71389', 'examples_per_second': '24.532', 'grad_norm': '23.375', 'counters/examples': 68416, 'counters/updates': 2138}
train stats after 68448 examples: {'rewards_train/chosen': '0.10854', 'rewards_train/rejected': '0.10149', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0070447', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-96.574', 'loss/train': '0.69508', 'examples_per_second': '32.133', 'grad_norm': '18.625', 'counters/examples': 68448, 'counters/updates': 2139}
skipping logging after 68480 examples to avoid logging too frequently
train stats after 68512 examples: {'rewards_train/chosen': '0.073551', 'rewards_train/rejected': '0.074392', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00084139', 'logps_train/rejected': '-140.07', 'logps_train/chosen': '-142.93', 'loss/train': '0.69934', 'examples_per_second': '30.189', 'grad_norm': '21.125', 'counters/examples': 68512, 'counters/updates': 2141}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68576 examples: {'rewards_train/chosen': '0.089345', 'rewards_train/rejected': '0.064751', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024594', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-137.75', 'loss/train': '0.68923', 'examples_per_second': '30.659', 'grad_norm': '20.5', 'counters/examples': 68576, 'counters/updates': 2143}
train stats after 68608 examples: {'rewards_train/chosen': '0.078062', 'rewards_train/rejected': '0.058925', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019137', 'logps_train/rejected': '-115.55', 'logps_train/chosen': '-151.65', 'loss/train': '0.68975', 'examples_per_second': '31.713', 'grad_norm': '22', 'counters/examples': 68608, 'counters/updates': 2144}
skipping logging after 68640 examples to avoid logging too frequently
train stats after 68672 examples: {'rewards_train/chosen': '0.11702', 'rewards_train/rejected': '0.0095995', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10742', 'logps_train/rejected': '-107.5', 'logps_train/chosen': '-117.6', 'loss/train': '0.64608', 'examples_per_second': '30.619', 'grad_norm': '18.75', 'counters/examples': 68672, 'counters/updates': 2146}
skipping logging after 68704 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '0.1517', 'rewards_train/rejected': '0.057772', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093929', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-168.18', 'loss/train': '0.65289', 'examples_per_second': '31.635', 'grad_norm': '21.125', 'counters/examples': 68736, 'counters/updates': 2148}
skipping logging after 68768 examples to avoid logging too frequently
train stats after 68800 examples: {'rewards_train/chosen': '0.038338', 'rewards_train/rejected': '0.026433', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011905', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-131.32', 'loss/train': '0.69363', 'examples_per_second': '32.769', 'grad_norm': '21.5', 'counters/examples': 68800, 'counters/updates': 2150}
skipping logging after 68832 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '0.07563', 'rewards_train/rejected': '0.098324', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022694', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-142.76', 'loss/train': '0.70905', 'examples_per_second': '30.474', 'grad_norm': '21.5', 'counters/examples': 68864, 'counters/updates': 2152}
train stats after 68896 examples: {'rewards_train/chosen': '0.10848', 'rewards_train/rejected': '0.03288', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075605', 'logps_train/rejected': '-107.3', 'logps_train/chosen': '-147.66', 'loss/train': '0.66307', 'examples_per_second': '30.999', 'grad_norm': '20.75', 'counters/examples': 68896, 'counters/updates': 2153}
train stats after 68928 examples: {'rewards_train/chosen': '0.090835', 'rewards_train/rejected': '0.0587', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032135', 'logps_train/rejected': '-119.14', 'logps_train/chosen': '-165.13', 'loss/train': '0.68006', 'examples_per_second': '31.674', 'grad_norm': '21.375', 'counters/examples': 68928, 'counters/updates': 2154}
skipping logging after 68960 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '0.1214', 'rewards_train/rejected': '0.11819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0032076', 'logps_train/rejected': '-148.19', 'logps_train/chosen': '-151.51', 'loss/train': '0.69613', 'examples_per_second': '31.708', 'grad_norm': '22.5', 'counters/examples': 68992, 'counters/updates': 2156}
train stats after 69024 examples: {'rewards_train/chosen': '0.11942', 'rewards_train/rejected': '0.014625', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1048', 'logps_train/rejected': '-169.03', 'logps_train/chosen': '-162.52', 'loss/train': '0.64932', 'examples_per_second': '30.112', 'grad_norm': '21.75', 'counters/examples': 69024, 'counters/updates': 2157}
train stats after 69056 examples: {'rewards_train/chosen': '0.10322', 'rewards_train/rejected': '0.042217', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.061004', 'logps_train/rejected': '-100.86', 'logps_train/chosen': '-120', 'loss/train': '0.66753', 'examples_per_second': '32.777', 'grad_norm': '19.375', 'counters/examples': 69056, 'counters/updates': 2158}
train stats after 69088 examples: {'rewards_train/chosen': '0.099798', 'rewards_train/rejected': '0.075847', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023952', 'logps_train/rejected': '-161.54', 'logps_train/chosen': '-140.85', 'loss/train': '0.68633', 'examples_per_second': '30.187', 'grad_norm': '22.125', 'counters/examples': 69088, 'counters/updates': 2159}
train stats after 69120 examples: {'rewards_train/chosen': '0.090162', 'rewards_train/rejected': '0.041125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049037', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-118.89', 'loss/train': '0.67592', 'examples_per_second': '30.726', 'grad_norm': '18.875', 'counters/examples': 69120, 'counters/updates': 2160}
skipping logging after 69152 examples to avoid logging too frequently
train stats after 69184 examples: {'rewards_train/chosen': '0.090288', 'rewards_train/rejected': '0.011632', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078656', 'logps_train/rejected': '-170.02', 'logps_train/chosen': '-190.92', 'loss/train': '0.66414', 'examples_per_second': '31.667', 'grad_norm': '23.875', 'counters/examples': 69184, 'counters/updates': 2162}
skipping logging after 69216 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '0.083057', 'rewards_train/rejected': '0.074094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0089631', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-98.484', 'loss/train': '0.69492', 'examples_per_second': '30.75', 'grad_norm': '19.875', 'counters/examples': 69248, 'counters/updates': 2164}
train stats after 69280 examples: {'rewards_train/chosen': '0.087839', 'rewards_train/rejected': '0.00023201', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.087607', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-123.11', 'loss/train': '0.65634', 'examples_per_second': '31.912', 'grad_norm': '20.25', 'counters/examples': 69280, 'counters/updates': 2165}
train stats after 69312 examples: {'rewards_train/chosen': '0.1618', 'rewards_train/rejected': '0.069192', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092605', 'logps_train/rejected': '-172.02', 'logps_train/chosen': '-176.03', 'loss/train': '0.65824', 'examples_per_second': '31.642', 'grad_norm': '22.625', 'counters/examples': 69312, 'counters/updates': 2166}
train stats after 69344 examples: {'rewards_train/chosen': '0.10882', 'rewards_train/rejected': '0.033726', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075097', 'logps_train/rejected': '-121.55', 'logps_train/chosen': '-146.99', 'loss/train': '0.66154', 'examples_per_second': '30.149', 'grad_norm': '19.625', 'counters/examples': 69344, 'counters/updates': 2167}
train stats after 69376 examples: {'rewards_train/chosen': '0.14156', 'rewards_train/rejected': '0.016864', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12469', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-105.92', 'loss/train': '0.64083', 'examples_per_second': '32.722', 'grad_norm': '17.375', 'counters/examples': 69376, 'counters/updates': 2168}
train stats after 69408 examples: {'rewards_train/chosen': '0.06044', 'rewards_train/rejected': '0.062767', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0023276', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-145.66', 'loss/train': '0.69936', 'examples_per_second': '30.827', 'grad_norm': '20.375', 'counters/examples': 69408, 'counters/updates': 2169}
train stats after 69440 examples: {'rewards_train/chosen': '0.088374', 'rewards_train/rejected': '0.055862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032512', 'logps_train/rejected': '-130.77', 'logps_train/chosen': '-125.78', 'loss/train': '0.68474', 'examples_per_second': '30.898', 'grad_norm': '21.125', 'counters/examples': 69440, 'counters/updates': 2170}
train stats after 69472 examples: {'rewards_train/chosen': '0.11042', 'rewards_train/rejected': '0.071029', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03939', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-174.84', 'loss/train': '0.6817', 'examples_per_second': '31.015', 'grad_norm': '23.125', 'counters/examples': 69472, 'counters/updates': 2171}
skipping logging after 69504 examples to avoid logging too frequently
train stats after 69536 examples: {'rewards_train/chosen': '0.069548', 'rewards_train/rejected': '-0.045144', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11469', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-167.9', 'loss/train': '0.64468', 'examples_per_second': '30.656', 'grad_norm': '21.25', 'counters/examples': 69536, 'counters/updates': 2173}
train stats after 69568 examples: {'rewards_train/chosen': '0.061466', 'rewards_train/rejected': '0.024221', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037245', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-143.72', 'loss/train': '0.67897', 'examples_per_second': '31.401', 'grad_norm': '22', 'counters/examples': 69568, 'counters/updates': 2174}
train stats after 69600 examples: {'rewards_train/chosen': '0.14682', 'rewards_train/rejected': '0.10034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046481', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-139.42', 'loss/train': '0.67448', 'examples_per_second': '32.188', 'grad_norm': '19.875', 'counters/examples': 69600, 'counters/updates': 2175}
skipping logging after 69632 examples to avoid logging too frequently
train stats after 69664 examples: {'rewards_train/chosen': '0.096555', 'rewards_train/rejected': '0.027787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068768', 'logps_train/rejected': '-74.113', 'logps_train/chosen': '-129.47', 'loss/train': '0.66518', 'examples_per_second': '31.501', 'grad_norm': '17.875', 'counters/examples': 69664, 'counters/updates': 2177}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69728 examples: {'rewards_train/chosen': '0.1265', 'rewards_train/rejected': '0.088554', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037947', 'logps_train/rejected': '-157', 'logps_train/chosen': '-158.23', 'loss/train': '0.6817', 'examples_per_second': '32', 'grad_norm': '22.75', 'counters/examples': 69728, 'counters/updates': 2179}
train stats after 69760 examples: {'rewards_train/chosen': '0.10984', 'rewards_train/rejected': '0.044935', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064908', 'logps_train/rejected': '-121.37', 'logps_train/chosen': '-102.87', 'loss/train': '0.67036', 'examples_per_second': '25.817', 'grad_norm': '19.5', 'counters/examples': 69760, 'counters/updates': 2180}
train stats after 69792 examples: {'rewards_train/chosen': '0.10169', 'rewards_train/rejected': '0.049939', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051748', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-120.34', 'loss/train': '0.67312', 'examples_per_second': '32.242', 'grad_norm': '19.125', 'counters/examples': 69792, 'counters/updates': 2181}
train stats after 69824 examples: {'rewards_train/chosen': '0.097321', 'rewards_train/rejected': '0.014127', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083194', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-142.65', 'loss/train': '0.65607', 'examples_per_second': '31.603', 'grad_norm': '20', 'counters/examples': 69824, 'counters/updates': 2182}
train stats after 69856 examples: {'rewards_train/chosen': '0.12801', 'rewards_train/rejected': '0.010348', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11766', 'logps_train/rejected': '-110.83', 'logps_train/chosen': '-132.78', 'loss/train': '0.64113', 'examples_per_second': '31.353', 'grad_norm': '18.625', 'counters/examples': 69856, 'counters/updates': 2183}
skipping logging after 69888 examples to avoid logging too frequently
train stats after 69920 examples: {'rewards_train/chosen': '0.086436', 'rewards_train/rejected': '0.028968', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.057468', 'logps_train/rejected': '-89.65', 'logps_train/chosen': '-146.07', 'loss/train': '0.6696', 'examples_per_second': '31.91', 'grad_norm': '19.25', 'counters/examples': 69920, 'counters/updates': 2185}
train stats after 69952 examples: {'rewards_train/chosen': '0.13978', 'rewards_train/rejected': '0.03987', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.099913', 'logps_train/rejected': '-117.02', 'logps_train/chosen': '-142.38', 'loss/train': '0.65326', 'examples_per_second': '30.11', 'grad_norm': '20.75', 'counters/examples': 69952, 'counters/updates': 2186}
skipping logging after 69984 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '0.11288', 'rewards_train/rejected': '0.055736', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057145', 'logps_train/rejected': '-84.948', 'logps_train/chosen': '-105.48', 'loss/train': '0.66939', 'examples_per_second': '32.736', 'grad_norm': '16.375', 'counters/examples': 70016, 'counters/updates': 2188}
train stats after 70048 examples: {'rewards_train/chosen': '0.13793', 'rewards_train/rejected': '0.057293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080634', 'logps_train/rejected': '-142.8', 'logps_train/chosen': '-139.67', 'loss/train': '0.65668', 'examples_per_second': '30.234', 'grad_norm': '20.75', 'counters/examples': 70048, 'counters/updates': 2189}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70112 examples: {'rewards_train/chosen': '0.06333', 'rewards_train/rejected': '0.031854', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031476', 'logps_train/rejected': '-96.94', 'logps_train/chosen': '-100.58', 'loss/train': '0.68329', 'examples_per_second': '38.36', 'grad_norm': '18.25', 'counters/examples': 70112, 'counters/updates': 2191}
train stats after 70144 examples: {'rewards_train/chosen': '0.11231', 'rewards_train/rejected': '0.021438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090875', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-178.96', 'loss/train': '0.65162', 'examples_per_second': '30.055', 'grad_norm': '21.75', 'counters/examples': 70144, 'counters/updates': 2192}
train stats after 70176 examples: {'rewards_train/chosen': '0.077623', 'rewards_train/rejected': '0.027964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049659', 'logps_train/rejected': '-98.174', 'logps_train/chosen': '-150.47', 'loss/train': '0.6733', 'examples_per_second': '32.506', 'grad_norm': '20', 'counters/examples': 70176, 'counters/updates': 2193}
train stats after 70208 examples: {'rewards_train/chosen': '0.12929', 'rewards_train/rejected': '0.028604', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10069', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-117.25', 'loss/train': '0.64738', 'examples_per_second': '31.145', 'grad_norm': '19.375', 'counters/examples': 70208, 'counters/updates': 2194}
train stats after 70240 examples: {'rewards_train/chosen': '0.11668', 'rewards_train/rejected': '0.072317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044365', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-132.98', 'loss/train': '0.67845', 'examples_per_second': '30.204', 'grad_norm': '19.5', 'counters/examples': 70240, 'counters/updates': 2195}
train stats after 70272 examples: {'rewards_train/chosen': '0.11497', 'rewards_train/rejected': '0.11264', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0023356', 'logps_train/rejected': '-122.73', 'logps_train/chosen': '-126.7', 'loss/train': '0.6954', 'examples_per_second': '32.457', 'grad_norm': '20.25', 'counters/examples': 70272, 'counters/updates': 2196}
skipping logging after 70304 examples to avoid logging too frequently
train stats after 70336 examples: {'rewards_train/chosen': '0.17075', 'rewards_train/rejected': '0.021534', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14922', 'logps_train/rejected': '-126.17', 'logps_train/chosen': '-149.59', 'loss/train': '0.62969', 'examples_per_second': '35.975', 'grad_norm': '19.75', 'counters/examples': 70336, 'counters/updates': 2198}
skipping logging after 70368 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '0.12122', 'rewards_train/rejected': '0.054911', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066305', 'logps_train/rejected': '-109.6', 'logps_train/chosen': '-145.06', 'loss/train': '0.66948', 'examples_per_second': '30.35', 'grad_norm': '20.625', 'counters/examples': 70400, 'counters/updates': 2200}
skipping logging after 70432 examples to avoid logging too frequently
train stats after 70464 examples: {'rewards_train/chosen': '0.086629', 'rewards_train/rejected': '0.057736', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028892', 'logps_train/rejected': '-118.03', 'logps_train/chosen': '-143.85', 'loss/train': '0.68246', 'examples_per_second': '30.174', 'grad_norm': '21.125', 'counters/examples': 70464, 'counters/updates': 2202}
train stats after 70496 examples: {'rewards_train/chosen': '0.10365', 'rewards_train/rejected': '-0.0081176', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11176', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-166.53', 'loss/train': '0.64731', 'examples_per_second': '30.185', 'grad_norm': '20.875', 'counters/examples': 70496, 'counters/updates': 2203}
train stats after 70528 examples: {'rewards_train/chosen': '0.10191', 'rewards_train/rejected': '0.10092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00099555', 'logps_train/rejected': '-127.11', 'logps_train/chosen': '-144.57', 'loss/train': '0.69857', 'examples_per_second': '30.96', 'grad_norm': '21.125', 'counters/examples': 70528, 'counters/updates': 2204}
train stats after 70560 examples: {'rewards_train/chosen': '0.13205', 'rewards_train/rejected': '0.067912', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06414', 'logps_train/rejected': '-111.59', 'logps_train/chosen': '-152.14', 'loss/train': '0.66782', 'examples_per_second': '29.936', 'grad_norm': '20.625', 'counters/examples': 70560, 'counters/updates': 2205}
train stats after 70592 examples: {'rewards_train/chosen': '0.083033', 'rewards_train/rejected': '0.069491', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013542', 'logps_train/rejected': '-121.21', 'logps_train/chosen': '-133.65', 'loss/train': '0.69188', 'examples_per_second': '32.301', 'grad_norm': '20.5', 'counters/examples': 70592, 'counters/updates': 2206}
train stats after 70624 examples: {'rewards_train/chosen': '0.081164', 'rewards_train/rejected': '0.049192', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031972', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-149.65', 'loss/train': '0.68764', 'examples_per_second': '31.677', 'grad_norm': '23.125', 'counters/examples': 70624, 'counters/updates': 2207}
train stats after 70656 examples: {'rewards_train/chosen': '0.11656', 'rewards_train/rejected': '0.0093212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10724', 'logps_train/rejected': '-120', 'logps_train/chosen': '-158.53', 'loss/train': '0.64749', 'examples_per_second': '31.414', 'grad_norm': '20.5', 'counters/examples': 70656, 'counters/updates': 2208}
skipping logging after 70688 examples to avoid logging too frequently
train stats after 70720 examples: {'rewards_train/chosen': '0.096119', 'rewards_train/rejected': '0.066865', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029254', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-144.89', 'loss/train': '0.68463', 'examples_per_second': '32.111', 'grad_norm': '20.625', 'counters/examples': 70720, 'counters/updates': 2210}
train stats after 70752 examples: {'rewards_train/chosen': '0.094534', 'rewards_train/rejected': '0.010322', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084212', 'logps_train/rejected': '-99.625', 'logps_train/chosen': '-130.05', 'loss/train': '0.65727', 'examples_per_second': '31.585', 'grad_norm': '19.75', 'counters/examples': 70752, 'counters/updates': 2211}
train stats after 70784 examples: {'rewards_train/chosen': '0.12317', 'rewards_train/rejected': '0.040264', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.082907', 'logps_train/rejected': '-104.85', 'logps_train/chosen': '-156.25', 'loss/train': '0.65847', 'examples_per_second': '31.055', 'grad_norm': '20.25', 'counters/examples': 70784, 'counters/updates': 2212}
skipping logging after 70816 examples to avoid logging too frequently
train stats after 70848 examples: {'rewards_train/chosen': '0.13255', 'rewards_train/rejected': '0.070952', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061596', 'logps_train/rejected': '-121.55', 'logps_train/chosen': '-140.16', 'loss/train': '0.66978', 'examples_per_second': '31.13', 'grad_norm': '21.125', 'counters/examples': 70848, 'counters/updates': 2214}
train stats after 70880 examples: {'rewards_train/chosen': '0.082272', 'rewards_train/rejected': '0.060698', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.021574', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-164.34', 'loss/train': '0.68961', 'examples_per_second': '31.487', 'grad_norm': '21.625', 'counters/examples': 70880, 'counters/updates': 2215}
train stats after 70912 examples: {'rewards_train/chosen': '0.11774', 'rewards_train/rejected': '0.06191', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055826', 'logps_train/rejected': '-111.87', 'logps_train/chosen': '-146.01', 'loss/train': '0.67282', 'examples_per_second': '32.56', 'grad_norm': '20', 'counters/examples': 70912, 'counters/updates': 2216}
skipping logging after 70944 examples to avoid logging too frequently
train stats after 70976 examples: {'rewards_train/chosen': '0.042907', 'rewards_train/rejected': '0.1012', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.058296', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-136.5', 'loss/train': '0.72611', 'examples_per_second': '31.61', 'grad_norm': '21.625', 'counters/examples': 70976, 'counters/updates': 2218}
skipping logging after 71008 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '0.1233', 'rewards_train/rejected': '0.047315', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075987', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-139.47', 'loss/train': '0.66588', 'examples_per_second': '31.636', 'grad_norm': '19.625', 'counters/examples': 71040, 'counters/updates': 2220}
skipping logging after 71072 examples to avoid logging too frequently
train stats after 71104 examples: {'rewards_train/chosen': '0.1108', 'rewards_train/rejected': '0.030721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080081', 'logps_train/rejected': '-104.87', 'logps_train/chosen': '-115.22', 'loss/train': '0.65835', 'examples_per_second': '31.854', 'grad_norm': '18.125', 'counters/examples': 71104, 'counters/updates': 2222}
skipping logging after 71136 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '0.16129', 'rewards_train/rejected': '0.063936', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097354', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-126.81', 'loss/train': '0.65206', 'examples_per_second': '34.323', 'grad_norm': '19.875', 'counters/examples': 71168, 'counters/updates': 2224}
train stats after 71200 examples: {'rewards_train/chosen': '0.11127', 'rewards_train/rejected': '0.090201', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021069', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-158.53', 'loss/train': '0.69092', 'examples_per_second': '30.89', 'grad_norm': '22', 'counters/examples': 71200, 'counters/updates': 2225}
train stats after 71232 examples: {'rewards_train/chosen': '0.12825', 'rewards_train/rejected': '-0.00049841', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12875', 'logps_train/rejected': '-78.235', 'logps_train/chosen': '-135.11', 'loss/train': '0.636', 'examples_per_second': '32.67', 'grad_norm': '18.625', 'counters/examples': 71232, 'counters/updates': 2226}
train stats after 71264 examples: {'rewards_train/chosen': '0.080103', 'rewards_train/rejected': '0.0049586', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075144', 'logps_train/rejected': '-107.25', 'logps_train/chosen': '-165.99', 'loss/train': '0.66371', 'examples_per_second': '31.332', 'grad_norm': '20.875', 'counters/examples': 71264, 'counters/updates': 2227}
train stats after 71296 examples: {'rewards_train/chosen': '0.091881', 'rewards_train/rejected': '0.051418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040463', 'logps_train/rejected': '-103.63', 'logps_train/chosen': '-143.12', 'loss/train': '0.676', 'examples_per_second': '30.875', 'grad_norm': '18.875', 'counters/examples': 71296, 'counters/updates': 2228}
train stats after 71328 examples: {'rewards_train/chosen': '0.1351', 'rewards_train/rejected': '0.096941', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038156', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-151.94', 'loss/train': '0.67921', 'examples_per_second': '32.933', 'grad_norm': '21.5', 'counters/examples': 71328, 'counters/updates': 2229}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71392 examples: {'rewards_train/chosen': '0.14487', 'rewards_train/rejected': '0.11818', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026693', 'logps_train/rejected': '-163.08', 'logps_train/chosen': '-127.59', 'loss/train': '0.69052', 'examples_per_second': '30.19', 'grad_norm': '22.25', 'counters/examples': 71392, 'counters/updates': 2231}
train stats after 71424 examples: {'rewards_train/chosen': '0.11548', 'rewards_train/rejected': '0.034855', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080622', 'logps_train/rejected': '-101.49', 'logps_train/chosen': '-97.939', 'loss/train': '0.6598', 'examples_per_second': '32.563', 'grad_norm': '18.375', 'counters/examples': 71424, 'counters/updates': 2232}
train stats after 71456 examples: {'rewards_train/chosen': '0.12517', 'rewards_train/rejected': '0.037619', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087556', 'logps_train/rejected': '-115.9', 'logps_train/chosen': '-139.62', 'loss/train': '0.65338', 'examples_per_second': '32.11', 'grad_norm': '19.25', 'counters/examples': 71456, 'counters/updates': 2233}
train stats after 71488 examples: {'rewards_train/chosen': '0.1484', 'rewards_train/rejected': '0.10555', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042852', 'logps_train/rejected': '-139.48', 'logps_train/chosen': '-172.14', 'loss/train': '0.6785', 'examples_per_second': '31.026', 'grad_norm': '24.125', 'counters/examples': 71488, 'counters/updates': 2234}
train stats after 71520 examples: {'rewards_train/chosen': '0.10337', 'rewards_train/rejected': '-0.0036798', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10705', 'logps_train/rejected': '-112.28', 'logps_train/chosen': '-119.86', 'loss/train': '0.64572', 'examples_per_second': '30.754', 'grad_norm': '18.625', 'counters/examples': 71520, 'counters/updates': 2235}
train stats after 71552 examples: {'rewards_train/chosen': '0.063862', 'rewards_train/rejected': '0.039044', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024819', 'logps_train/rejected': '-106.43', 'logps_train/chosen': '-120.06', 'loss/train': '0.68798', 'examples_per_second': '32.496', 'grad_norm': '18.625', 'counters/examples': 71552, 'counters/updates': 2236}
train stats after 71584 examples: {'rewards_train/chosen': '0.098282', 'rewards_train/rejected': '0.025807', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.072476', 'logps_train/rejected': '-129.84', 'logps_train/chosen': '-154.46', 'loss/train': '0.66132', 'examples_per_second': '30.613', 'grad_norm': '20.75', 'counters/examples': 71584, 'counters/updates': 2237}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71648 examples: {'rewards_train/chosen': '0.09165', 'rewards_train/rejected': '0.071889', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019761', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-145.76', 'loss/train': '0.68832', 'examples_per_second': '31.642', 'grad_norm': '21.75', 'counters/examples': 71648, 'counters/updates': 2239}
train stats after 71680 examples: {'rewards_train/chosen': '0.037067', 'rewards_train/rejected': '0.055474', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018407', 'logps_train/rejected': '-110.59', 'logps_train/chosen': '-140.95', 'loss/train': '0.71113', 'examples_per_second': '31.645', 'grad_norm': '21.25', 'counters/examples': 71680, 'counters/updates': 2240}
train stats after 71712 examples: {'rewards_train/chosen': '0.032284', 'rewards_train/rejected': '0.0048481', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027436', 'logps_train/rejected': '-125.37', 'logps_train/chosen': '-121.32', 'loss/train': '0.68339', 'examples_per_second': '32.515', 'grad_norm': '21.5', 'counters/examples': 71712, 'counters/updates': 2241}
train stats after 71744 examples: {'rewards_train/chosen': '0.097064', 'rewards_train/rejected': '0.078994', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01807', 'logps_train/rejected': '-126.65', 'logps_train/chosen': '-179.17', 'loss/train': '0.69007', 'examples_per_second': '32.946', 'grad_norm': '23.75', 'counters/examples': 71744, 'counters/updates': 2242}
train stats after 71776 examples: {'rewards_train/chosen': '0.096052', 'rewards_train/rejected': '0.068799', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027253', 'logps_train/rejected': '-92.881', 'logps_train/chosen': '-131.18', 'loss/train': '0.68233', 'examples_per_second': '30.812', 'grad_norm': '19.625', 'counters/examples': 71776, 'counters/updates': 2243}
skipping logging after 71808 examples to avoid logging too frequently
train stats after 71840 examples: {'rewards_train/chosen': '0.10843', 'rewards_train/rejected': '0.066212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042216', 'logps_train/rejected': '-99.232', 'logps_train/chosen': '-192.31', 'loss/train': '0.67859', 'examples_per_second': '31.856', 'grad_norm': '22.625', 'counters/examples': 71840, 'counters/updates': 2245}
train stats after 71872 examples: {'rewards_train/chosen': '0.07018', 'rewards_train/rejected': '0.069159', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0010209', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-149.05', 'loss/train': '0.6987', 'examples_per_second': '31.312', 'grad_norm': '21.25', 'counters/examples': 71872, 'counters/updates': 2246}
train stats after 71904 examples: {'rewards_train/chosen': '0.078874', 'rewards_train/rejected': '0.039713', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039161', 'logps_train/rejected': '-105.94', 'logps_train/chosen': '-126.18', 'loss/train': '0.67926', 'examples_per_second': '32.548', 'grad_norm': '19.375', 'counters/examples': 71904, 'counters/updates': 2247}
train stats after 71936 examples: {'rewards_train/chosen': '0.035591', 'rewards_train/rejected': '-0.013205', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048796', 'logps_train/rejected': '-110.29', 'logps_train/chosen': '-160.04', 'loss/train': '0.67366', 'examples_per_second': '31.619', 'grad_norm': '21.375', 'counters/examples': 71936, 'counters/updates': 2248}
skipping logging after 71968 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '0.13234', 'rewards_train/rejected': '0.0035688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12877', 'logps_train/rejected': '-138.9', 'logps_train/chosen': '-138.1', 'loss/train': '0.63906', 'examples_per_second': '32.465', 'grad_norm': '21.375', 'counters/examples': 72000, 'counters/updates': 2250}
Running evaluation after 72000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.12it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.80it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.89it/s]
eval after 72000: {'rewards_eval/chosen': '0.10576', 'rewards_eval/rejected': '0.049079', 'rewards_eval/accuracies': '0.55078', 'rewards_eval/margins': '0.056681', 'logps_eval/rejected': '-114.9', 'logps_eval/chosen': '-134.49', 'loss/eval': '0.67259'}
skipping save for non epoch
train stats after 72032 examples: {'rewards_train/chosen': '0.098495', 'rewards_train/rejected': '0.045114', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053381', 'logps_train/rejected': '-161.54', 'logps_train/chosen': '-150.22', 'loss/train': '0.67454', 'examples_per_second': '30.657', 'grad_norm': '22.5', 'counters/examples': 72032, 'counters/updates': 2251}
train stats after 72064 examples: {'rewards_train/chosen': '0.12704', 'rewards_train/rejected': '0.038843', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088199', 'logps_train/rejected': '-87.616', 'logps_train/chosen': '-129.98', 'loss/train': '0.65457', 'examples_per_second': '30.133', 'grad_norm': '17.625', 'counters/examples': 72064, 'counters/updates': 2252}
train stats after 72096 examples: {'rewards_train/chosen': '0.11089', 'rewards_train/rejected': '0.066773', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044121', 'logps_train/rejected': '-130.85', 'logps_train/chosen': '-178.55', 'loss/train': '0.67646', 'examples_per_second': '32.824', 'grad_norm': '22.875', 'counters/examples': 72096, 'counters/updates': 2253}
skipping logging after 72128 examples to avoid logging too frequently
train stats after 72160 examples: {'rewards_train/chosen': '0.096158', 'rewards_train/rejected': '0.052788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043371', 'logps_train/rejected': '-103.21', 'logps_train/chosen': '-110.95', 'loss/train': '0.67941', 'examples_per_second': '31.896', 'grad_norm': '17.625', 'counters/examples': 72160, 'counters/updates': 2255}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72224 examples: {'rewards_train/chosen': '0.15292', 'rewards_train/rejected': '0.036282', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11663', 'logps_train/rejected': '-95.906', 'logps_train/chosen': '-159.93', 'loss/train': '0.64231', 'examples_per_second': '31.612', 'grad_norm': '19.625', 'counters/examples': 72224, 'counters/updates': 2257}
skipping logging after 72256 examples to avoid logging too frequently
train stats after 72288 examples: {'rewards_train/chosen': '0.066174', 'rewards_train/rejected': '0.041236', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024938', 'logps_train/rejected': '-111.53', 'logps_train/chosen': '-150.2', 'loss/train': '0.68537', 'examples_per_second': '31.561', 'grad_norm': '25.375', 'counters/examples': 72288, 'counters/updates': 2259}
train stats after 72320 examples: {'rewards_train/chosen': '0.18339', 'rewards_train/rejected': '0.056797', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1266', 'logps_train/rejected': '-120.28', 'logps_train/chosen': '-164.11', 'loss/train': '0.63781', 'examples_per_second': '32.359', 'grad_norm': '20.375', 'counters/examples': 72320, 'counters/updates': 2260}
skipping logging after 72352 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '0.08968', 'rewards_train/rejected': '0.027606', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062074', 'logps_train/rejected': '-150.01', 'logps_train/chosen': '-142.18', 'loss/train': '0.66895', 'examples_per_second': '32.682', 'grad_norm': '21.25', 'counters/examples': 72384, 'counters/updates': 2262}
train stats after 72416 examples: {'rewards_train/chosen': '0.097011', 'rewards_train/rejected': '0.075264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021748', 'logps_train/rejected': '-107.08', 'logps_train/chosen': '-123.5', 'loss/train': '0.68634', 'examples_per_second': '31.569', 'grad_norm': '19.125', 'counters/examples': 72416, 'counters/updates': 2263}
train stats after 72448 examples: {'rewards_train/chosen': '0.094119', 'rewards_train/rejected': '0.056583', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.037537', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-134.65', 'loss/train': '0.68018', 'examples_per_second': '32.494', 'grad_norm': '20.125', 'counters/examples': 72448, 'counters/updates': 2264}
train stats after 72480 examples: {'rewards_train/chosen': '0.10672', 'rewards_train/rejected': '0.081028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025696', 'logps_train/rejected': '-108.96', 'logps_train/chosen': '-137.04', 'loss/train': '0.68574', 'examples_per_second': '30.263', 'grad_norm': '19.375', 'counters/examples': 72480, 'counters/updates': 2265}
train stats after 72512 examples: {'rewards_train/chosen': '0.097907', 'rewards_train/rejected': '0.06282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035087', 'logps_train/rejected': '-101.28', 'logps_train/chosen': '-126.03', 'loss/train': '0.68036', 'examples_per_second': '31.621', 'grad_norm': '19.5', 'counters/examples': 72512, 'counters/updates': 2266}
skipping logging after 72544 examples to avoid logging too frequently
train stats after 72576 examples: {'rewards_train/chosen': '0.10051', 'rewards_train/rejected': '0.094525', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0059849', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-156.35', 'loss/train': '0.69868', 'examples_per_second': '31.433', 'grad_norm': '23', 'counters/examples': 72576, 'counters/updates': 2268}
train stats after 72608 examples: {'rewards_train/chosen': '0.12004', 'rewards_train/rejected': '0.052631', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067413', 'logps_train/rejected': '-105.98', 'logps_train/chosen': '-124.36', 'loss/train': '0.6643', 'examples_per_second': '32.613', 'grad_norm': '19.125', 'counters/examples': 72608, 'counters/updates': 2269}
train stats after 72640 examples: {'rewards_train/chosen': '0.090757', 'rewards_train/rejected': '0.045501', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045256', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-123.92', 'loss/train': '0.67657', 'examples_per_second': '32.181', 'grad_norm': '21.25', 'counters/examples': 72640, 'counters/updates': 2270}
train stats after 72672 examples: {'rewards_train/chosen': '0.087813', 'rewards_train/rejected': '0.064826', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022987', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-124.02', 'loss/train': '0.68555', 'examples_per_second': '32.141', 'grad_norm': '19.125', 'counters/examples': 72672, 'counters/updates': 2271}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72736 examples: {'rewards_train/chosen': '0.088159', 'rewards_train/rejected': '-0.038114', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12627', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-122.35', 'loss/train': '0.63811', 'examples_per_second': '32.301', 'grad_norm': '19.5', 'counters/examples': 72736, 'counters/updates': 2273}
train stats after 72768 examples: {'rewards_train/chosen': '0.082019', 'rewards_train/rejected': '-0.0082862', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090306', 'logps_train/rejected': '-120.29', 'logps_train/chosen': '-126.85', 'loss/train': '0.65541', 'examples_per_second': '31.516', 'grad_norm': '19', 'counters/examples': 72768, 'counters/updates': 2274}
skipping logging after 72800 examples to avoid logging too frequently
train stats after 72832 examples: {'rewards_train/chosen': '0.082164', 'rewards_train/rejected': '0.053894', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02827', 'logps_train/rejected': '-106.04', 'logps_train/chosen': '-136.47', 'loss/train': '0.68387', 'examples_per_second': '30.8', 'grad_norm': '20.625', 'counters/examples': 72832, 'counters/updates': 2276}
skipping logging after 72864 examples to avoid logging too frequently
train stats after 72896 examples: {'rewards_train/chosen': '0.11281', 'rewards_train/rejected': '0.080988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031823', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-141.27', 'loss/train': '0.68126', 'examples_per_second': '31.79', 'grad_norm': '21', 'counters/examples': 72896, 'counters/updates': 2278}
train stats after 72928 examples: {'rewards_train/chosen': '0.10204', 'rewards_train/rejected': '0.045248', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05679', 'logps_train/rejected': '-87.942', 'logps_train/chosen': '-125.7', 'loss/train': '0.67208', 'examples_per_second': '33.046', 'grad_norm': '18', 'counters/examples': 72928, 'counters/updates': 2279}
train stats after 72960 examples: {'rewards_train/chosen': '0.04873', 'rewards_train/rejected': '0.048548', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00018223', 'logps_train/rejected': '-152.65', 'logps_train/chosen': '-104.57', 'loss/train': '0.69621', 'examples_per_second': '30.711', 'grad_norm': '20.875', 'counters/examples': 72960, 'counters/updates': 2280}
train stats after 72992 examples: {'rewards_train/chosen': '0.097116', 'rewards_train/rejected': '0.098818', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0017024', 'logps_train/rejected': '-178.26', 'logps_train/chosen': '-147.07', 'loss/train': '0.69951', 'examples_per_second': '32.521', 'grad_norm': '23.25', 'counters/examples': 72992, 'counters/updates': 2281}
train stats after 73024 examples: {'rewards_train/chosen': '0.13307', 'rewards_train/rejected': '0.051265', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081808', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-133.82', 'loss/train': '0.65952', 'examples_per_second': '31.521', 'grad_norm': '18.5', 'counters/examples': 73024, 'counters/updates': 2282}
train stats after 73056 examples: {'rewards_train/chosen': '0.067181', 'rewards_train/rejected': '0.070148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0029664', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-139.32', 'loss/train': '0.70193', 'examples_per_second': '33.176', 'grad_norm': '19.75', 'counters/examples': 73056, 'counters/updates': 2283}
train stats after 73088 examples: {'rewards_train/chosen': '0.19252', 'rewards_train/rejected': '0.079045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11348', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-139.06', 'loss/train': '0.64146', 'examples_per_second': '30.57', 'grad_norm': '20.375', 'counters/examples': 73088, 'counters/updates': 2284}
train stats after 73120 examples: {'rewards_train/chosen': '0.082036', 'rewards_train/rejected': '-0.017464', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0995', 'logps_train/rejected': '-128.41', 'logps_train/chosen': '-120.36', 'loss/train': '0.65228', 'examples_per_second': '31.508', 'grad_norm': '19.5', 'counters/examples': 73120, 'counters/updates': 2285}
train stats after 73152 examples: {'rewards_train/chosen': '0.11331', 'rewards_train/rejected': '0.064476', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048837', 'logps_train/rejected': '-150.66', 'logps_train/chosen': '-175.26', 'loss/train': '0.67623', 'examples_per_second': '33.254', 'grad_norm': '24.5', 'counters/examples': 73152, 'counters/updates': 2286}
train stats after 73184 examples: {'rewards_train/chosen': '0.14786', 'rewards_train/rejected': '0.099233', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048623', 'logps_train/rejected': '-106.97', 'logps_train/chosen': '-114.02', 'loss/train': '0.67929', 'examples_per_second': '32.662', 'grad_norm': '17.75', 'counters/examples': 73184, 'counters/updates': 2287}
train stats after 73216 examples: {'rewards_train/chosen': '0.11912', 'rewards_train/rejected': '-0.0048391', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12396', 'logps_train/rejected': '-108.64', 'logps_train/chosen': '-128.34', 'loss/train': '0.63876', 'examples_per_second': '31.465', 'grad_norm': '18.125', 'counters/examples': 73216, 'counters/updates': 2288}
train stats after 73248 examples: {'rewards_train/chosen': '0.060319', 'rewards_train/rejected': '0.030549', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.02977', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-128.18', 'loss/train': '0.68251', 'examples_per_second': '31.491', 'grad_norm': '22.125', 'counters/examples': 73248, 'counters/updates': 2289}
skipping logging after 73280 examples to avoid logging too frequently
train stats after 73312 examples: {'rewards_train/chosen': '0.095978', 'rewards_train/rejected': '0.10339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0074151', 'logps_train/rejected': '-126.18', 'logps_train/chosen': '-143', 'loss/train': '0.7084', 'examples_per_second': '37.473', 'grad_norm': '26.125', 'counters/examples': 73312, 'counters/updates': 2291}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73376 examples: {'rewards_train/chosen': '0.092231', 'rewards_train/rejected': '-0.027393', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11962', 'logps_train/rejected': '-121.84', 'logps_train/chosen': '-129.95', 'loss/train': '0.64162', 'examples_per_second': '33.584', 'grad_norm': '19.25', 'counters/examples': 73376, 'counters/updates': 2293}
train stats after 73408 examples: {'rewards_train/chosen': '0.095112', 'rewards_train/rejected': '0.048374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046737', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-138.56', 'loss/train': '0.67378', 'examples_per_second': '30.827', 'grad_norm': '20.25', 'counters/examples': 73408, 'counters/updates': 2294}
skipping logging after 73440 examples to avoid logging too frequently
train stats after 73472 examples: {'rewards_train/chosen': '0.049485', 'rewards_train/rejected': '0.024432', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025053', 'logps_train/rejected': '-98.946', 'logps_train/chosen': '-114.52', 'loss/train': '0.68651', 'examples_per_second': '31.804', 'grad_norm': '18.125', 'counters/examples': 73472, 'counters/updates': 2296}
train stats after 73504 examples: {'rewards_train/chosen': '0.14633', 'rewards_train/rejected': '0.1333', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.013029', 'logps_train/rejected': '-140.95', 'logps_train/chosen': '-136.96', 'loss/train': '0.69084', 'examples_per_second': '30.524', 'grad_norm': '22.375', 'counters/examples': 73504, 'counters/updates': 2297}
train stats after 73536 examples: {'rewards_train/chosen': '0.041424', 'rewards_train/rejected': '0.013268', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028156', 'logps_train/rejected': '-84.1', 'logps_train/chosen': '-138.08', 'loss/train': '0.68499', 'examples_per_second': '32.322', 'grad_norm': '20.375', 'counters/examples': 73536, 'counters/updates': 2298}
train stats after 73568 examples: {'rewards_train/chosen': '0.11157', 'rewards_train/rejected': '0.058437', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053134', 'logps_train/rejected': '-114.91', 'logps_train/chosen': '-153.83', 'loss/train': '0.67131', 'examples_per_second': '31.567', 'grad_norm': '20.25', 'counters/examples': 73568, 'counters/updates': 2299}
train stats after 73600 examples: {'rewards_train/chosen': '0.057541', 'rewards_train/rejected': '0.076624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019084', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-157.82', 'loss/train': '0.70907', 'examples_per_second': '32.044', 'grad_norm': '22.875', 'counters/examples': 73600, 'counters/updates': 2300}
train stats after 73632 examples: {'rewards_train/chosen': '0.074209', 'rewards_train/rejected': '0.063383', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.010826', 'logps_train/rejected': '-106.04', 'logps_train/chosen': '-135.01', 'loss/train': '0.69924', 'examples_per_second': '32.374', 'grad_norm': '20.625', 'counters/examples': 73632, 'counters/updates': 2301}
train stats after 73664 examples: {'rewards_train/chosen': '0.085131', 'rewards_train/rejected': '0.053638', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031493', 'logps_train/rejected': '-168.51', 'logps_train/chosen': '-153.01', 'loss/train': '0.68261', 'examples_per_second': '31.488', 'grad_norm': '22.5', 'counters/examples': 73664, 'counters/updates': 2302}
train stats after 73696 examples: {'rewards_train/chosen': '0.059169', 'rewards_train/rejected': '0.027737', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031432', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-128.59', 'loss/train': '0.68512', 'examples_per_second': '30.569', 'grad_norm': '21', 'counters/examples': 73696, 'counters/updates': 2303}
train stats after 73728 examples: {'rewards_train/chosen': '0.089227', 'rewards_train/rejected': '0.035207', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05402', 'logps_train/rejected': '-138.18', 'logps_train/chosen': '-129.55', 'loss/train': '0.67399', 'examples_per_second': '30.92', 'grad_norm': '20.75', 'counters/examples': 73728, 'counters/updates': 2304}
train stats after 73760 examples: {'rewards_train/chosen': '0.16297', 'rewards_train/rejected': '0.091601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071364', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-137.1', 'loss/train': '0.66352', 'examples_per_second': '31.515', 'grad_norm': '20.5', 'counters/examples': 73760, 'counters/updates': 2305}
train stats after 73792 examples: {'rewards_train/chosen': '0.10787', 'rewards_train/rejected': '0.03329', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.074581', 'logps_train/rejected': '-116.9', 'logps_train/chosen': '-121.86', 'loss/train': '0.66181', 'examples_per_second': '24.217', 'grad_norm': '18.75', 'counters/examples': 73792, 'counters/updates': 2306}
train stats after 73824 examples: {'rewards_train/chosen': '0.19415', 'rewards_train/rejected': '0.090712', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10344', 'logps_train/rejected': '-142.48', 'logps_train/chosen': '-151.51', 'loss/train': '0.65095', 'examples_per_second': '31.477', 'grad_norm': '21.125', 'counters/examples': 73824, 'counters/updates': 2307}
train stats after 73856 examples: {'rewards_train/chosen': '0.061826', 'rewards_train/rejected': '-0.01619', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078016', 'logps_train/rejected': '-96.359', 'logps_train/chosen': '-126.51', 'loss/train': '0.66061', 'examples_per_second': '32.037', 'grad_norm': '19.125', 'counters/examples': 73856, 'counters/updates': 2308}
train stats after 73888 examples: {'rewards_train/chosen': '0.14065', 'rewards_train/rejected': '0.030301', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11035', 'logps_train/rejected': '-130.64', 'logps_train/chosen': '-151.42', 'loss/train': '0.64543', 'examples_per_second': '23.745', 'grad_norm': '20.125', 'counters/examples': 73888, 'counters/updates': 2309}
train stats after 73920 examples: {'rewards_train/chosen': '0.12261', 'rewards_train/rejected': '0.029056', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093551', 'logps_train/rejected': '-111.51', 'logps_train/chosen': '-132.04', 'loss/train': '0.6518', 'examples_per_second': '31.932', 'grad_norm': '19.375', 'counters/examples': 73920, 'counters/updates': 2310}
train stats after 73952 examples: {'rewards_train/chosen': '0.12892', 'rewards_train/rejected': '0.053919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-156.12', 'loss/train': '0.66189', 'examples_per_second': '30.104', 'grad_norm': '21.625', 'counters/examples': 73952, 'counters/updates': 2311}
train stats after 73984 examples: {'rewards_train/chosen': '0.1162', 'rewards_train/rejected': '0.010902', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1053', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-115.64', 'loss/train': '0.64841', 'examples_per_second': '30.314', 'grad_norm': '17.375', 'counters/examples': 73984, 'counters/updates': 2312}
skipping logging after 74016 examples to avoid logging too frequently
train stats after 74048 examples: {'rewards_train/chosen': '0.08248', 'rewards_train/rejected': '0.0007341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081746', 'logps_train/rejected': '-119.41', 'logps_train/chosen': '-131.01', 'loss/train': '0.65714', 'examples_per_second': '34.39', 'grad_norm': '19.375', 'counters/examples': 74048, 'counters/updates': 2314}
train stats after 74080 examples: {'rewards_train/chosen': '0.12826', 'rewards_train/rejected': '0.068834', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059422', 'logps_train/rejected': '-100.43', 'logps_train/chosen': '-113.93', 'loss/train': '0.67147', 'examples_per_second': '31.549', 'grad_norm': '17.875', 'counters/examples': 74080, 'counters/updates': 2315}
train stats after 74112 examples: {'rewards_train/chosen': '0.089123', 'rewards_train/rejected': '0.051928', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037195', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-141.35', 'loss/train': '0.67891', 'examples_per_second': '29.985', 'grad_norm': '21.75', 'counters/examples': 74112, 'counters/updates': 2316}
skipping logging after 74144 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': '0.15463', 'rewards_train/rejected': '0.074323', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080309', 'logps_train/rejected': '-83.888', 'logps_train/chosen': '-122.9', 'loss/train': '0.65923', 'examples_per_second': '30.829', 'grad_norm': '17.5', 'counters/examples': 74176, 'counters/updates': 2318}
train stats after 74208 examples: {'rewards_train/chosen': '0.11825', 'rewards_train/rejected': '0.041265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076983', 'logps_train/rejected': '-105.66', 'logps_train/chosen': '-125.54', 'loss/train': '0.66172', 'examples_per_second': '31.907', 'grad_norm': '18.125', 'counters/examples': 74208, 'counters/updates': 2319}
train stats after 74240 examples: {'rewards_train/chosen': '0.12717', 'rewards_train/rejected': '0.047627', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079539', 'logps_train/rejected': '-97.134', 'logps_train/chosen': '-149.87', 'loss/train': '0.65868', 'examples_per_second': '31.532', 'grad_norm': '19.25', 'counters/examples': 74240, 'counters/updates': 2320}
skipping logging after 74272 examples to avoid logging too frequently
train stats after 74304 examples: {'rewards_train/chosen': '0.12922', 'rewards_train/rejected': '0.051436', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077781', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-137.41', 'loss/train': '0.66268', 'examples_per_second': '34.7', 'grad_norm': '20.375', 'counters/examples': 74304, 'counters/updates': 2322}
train stats after 74336 examples: {'rewards_train/chosen': '0.10714', 'rewards_train/rejected': '0.052443', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054692', 'logps_train/rejected': '-101.82', 'logps_train/chosen': '-147.53', 'loss/train': '0.67189', 'examples_per_second': '32.886', 'grad_norm': '20.25', 'counters/examples': 74336, 'counters/updates': 2323}
train stats after 74368 examples: {'rewards_train/chosen': '0.14155', 'rewards_train/rejected': '0.07365', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067902', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-164.94', 'loss/train': '0.66774', 'examples_per_second': '32.117', 'grad_norm': '21.125', 'counters/examples': 74368, 'counters/updates': 2324}
train stats after 74400 examples: {'rewards_train/chosen': '0.1191', 'rewards_train/rejected': '0.056839', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062266', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-150.88', 'loss/train': '0.66864', 'examples_per_second': '30.513', 'grad_norm': '20.5', 'counters/examples': 74400, 'counters/updates': 2325}
train stats after 74432 examples: {'rewards_train/chosen': '0.087976', 'rewards_train/rejected': '0.039451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048525', 'logps_train/rejected': '-132.09', 'logps_train/chosen': '-113.07', 'loss/train': '0.67529', 'examples_per_second': '31.2', 'grad_norm': '19.375', 'counters/examples': 74432, 'counters/updates': 2326}
train stats after 74464 examples: {'rewards_train/chosen': '0.15016', 'rewards_train/rejected': '0.052247', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097914', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-153.69', 'loss/train': '0.65006', 'examples_per_second': '31.85', 'grad_norm': '20.5', 'counters/examples': 74464, 'counters/updates': 2327}
train stats after 74496 examples: {'rewards_train/chosen': '0.13423', 'rewards_train/rejected': '0.10702', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027209', 'logps_train/rejected': '-137.6', 'logps_train/chosen': '-131.91', 'loss/train': '0.68393', 'examples_per_second': '31.667', 'grad_norm': '21.125', 'counters/examples': 74496, 'counters/updates': 2328}
train stats after 74528 examples: {'rewards_train/chosen': '0.10786', 'rewards_train/rejected': '-0.017148', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12501', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-92.821', 'loss/train': '0.63637', 'examples_per_second': '30.405', 'grad_norm': '17', 'counters/examples': 74528, 'counters/updates': 2329}
train stats after 74560 examples: {'rewards_train/chosen': '0.095337', 'rewards_train/rejected': '0.031788', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063549', 'logps_train/rejected': '-93.597', 'logps_train/chosen': '-129.6', 'loss/train': '0.66615', 'examples_per_second': '31.44', 'grad_norm': '18.5', 'counters/examples': 74560, 'counters/updates': 2330}
skipping logging after 74592 examples to avoid logging too frequently
train stats after 74624 examples: {'rewards_train/chosen': '0.079134', 'rewards_train/rejected': '0.04704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032094', 'logps_train/rejected': '-107.71', 'logps_train/chosen': '-124.1', 'loss/train': '0.68014', 'examples_per_second': '30.907', 'grad_norm': '18.875', 'counters/examples': 74624, 'counters/updates': 2332}
train stats after 74656 examples: {'rewards_train/chosen': '0.08234', 'rewards_train/rejected': '0.041575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040765', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-134.52', 'loss/train': '0.67936', 'examples_per_second': '31.421', 'grad_norm': '21', 'counters/examples': 74656, 'counters/updates': 2333}
skipping logging after 74688 examples to avoid logging too frequently
train stats after 74720 examples: {'rewards_train/chosen': '0.097086', 'rewards_train/rejected': '0.046505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050581', 'logps_train/rejected': '-117.55', 'logps_train/chosen': '-151.47', 'loss/train': '0.6763', 'examples_per_second': '31.499', 'grad_norm': '20.875', 'counters/examples': 74720, 'counters/updates': 2335}
train stats after 74752 examples: {'rewards_train/chosen': '0.13107', 'rewards_train/rejected': '0.044491', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086578', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-145.06', 'loss/train': '0.65872', 'examples_per_second': '31.304', 'grad_norm': '20.75', 'counters/examples': 74752, 'counters/updates': 2336}
train stats after 74784 examples: {'rewards_train/chosen': '0.1239', 'rewards_train/rejected': '0.051693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072209', 'logps_train/rejected': '-145.66', 'logps_train/chosen': '-127.29', 'loss/train': '0.66392', 'examples_per_second': '31.276', 'grad_norm': '21.625', 'counters/examples': 74784, 'counters/updates': 2337}
train stats after 74816 examples: {'rewards_train/chosen': '0.13572', 'rewards_train/rejected': '0.066089', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069629', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-121.54', 'loss/train': '0.6672', 'examples_per_second': '31.508', 'grad_norm': '19.875', 'counters/examples': 74816, 'counters/updates': 2338}
train stats after 74848 examples: {'rewards_train/chosen': '0.1109', 'rewards_train/rejected': '0.034807', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076093', 'logps_train/rejected': '-127.74', 'logps_train/chosen': '-142.24', 'loss/train': '0.66298', 'examples_per_second': '30.784', 'grad_norm': '21.25', 'counters/examples': 74848, 'counters/updates': 2339}
train stats after 74880 examples: {'rewards_train/chosen': '0.10274', 'rewards_train/rejected': '0.035617', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067118', 'logps_train/rejected': '-73.161', 'logps_train/chosen': '-122.02', 'loss/train': '0.66572', 'examples_per_second': '31.308', 'grad_norm': '18.5', 'counters/examples': 74880, 'counters/updates': 2340}
train stats after 74912 examples: {'rewards_train/chosen': '0.12389', 'rewards_train/rejected': '0.098325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025565', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-119.22', 'loss/train': '0.69169', 'examples_per_second': '31.421', 'grad_norm': '20.375', 'counters/examples': 74912, 'counters/updates': 2341}
train stats after 74944 examples: {'rewards_train/chosen': '0.12975', 'rewards_train/rejected': '0.084956', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044797', 'logps_train/rejected': '-151.85', 'logps_train/chosen': '-132.9', 'loss/train': '0.67726', 'examples_per_second': '33.081', 'grad_norm': '21.75', 'counters/examples': 74944, 'counters/updates': 2342}
train stats after 74976 examples: {'rewards_train/chosen': '0.11833', 'rewards_train/rejected': '0.05155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066782', 'logps_train/rejected': '-119.3', 'logps_train/chosen': '-139.98', 'loss/train': '0.67036', 'examples_per_second': '32.565', 'grad_norm': '19.875', 'counters/examples': 74976, 'counters/updates': 2343}
train stats after 75008 examples: {'rewards_train/chosen': '0.14098', 'rewards_train/rejected': '0.022465', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11852', 'logps_train/rejected': '-103.87', 'logps_train/chosen': '-129.79', 'loss/train': '0.6411', 'examples_per_second': '31.455', 'grad_norm': '18.125', 'counters/examples': 75008, 'counters/updates': 2344}
skipping logging after 75040 examples to avoid logging too frequently
train stats after 75072 examples: {'rewards_train/chosen': '0.066083', 'rewards_train/rejected': '0.075158', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.0090755', 'logps_train/rejected': '-161.74', 'logps_train/chosen': '-156.63', 'loss/train': '0.71102', 'examples_per_second': '31.187', 'grad_norm': '24.25', 'counters/examples': 75072, 'counters/updates': 2346}
train stats after 75104 examples: {'rewards_train/chosen': '0.10074', 'rewards_train/rejected': '0.017928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082812', 'logps_train/rejected': '-91.084', 'logps_train/chosen': '-103.84', 'loss/train': '0.65598', 'examples_per_second': '31.314', 'grad_norm': '16.75', 'counters/examples': 75104, 'counters/updates': 2347}
train stats after 75136 examples: {'rewards_train/chosen': '0.086145', 'rewards_train/rejected': '0.086282', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0001365', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-124.19', 'loss/train': '0.69987', 'examples_per_second': '30.16', 'grad_norm': '20.75', 'counters/examples': 75136, 'counters/updates': 2348}
train stats after 75168 examples: {'rewards_train/chosen': '0.16175', 'rewards_train/rejected': '0.0069065', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15484', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-136.71', 'loss/train': '0.6269', 'examples_per_second': '30.554', 'grad_norm': '18.75', 'counters/examples': 75168, 'counters/updates': 2349}
skipping logging after 75200 examples to avoid logging too frequently
train stats after 75232 examples: {'rewards_train/chosen': '0.15499', 'rewards_train/rejected': '0.057822', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097169', 'logps_train/rejected': '-104.23', 'logps_train/chosen': '-158.8', 'loss/train': '0.65247', 'examples_per_second': '31.246', 'grad_norm': '21.5', 'counters/examples': 75232, 'counters/updates': 2351}
train stats after 75264 examples: {'rewards_train/chosen': '0.076416', 'rewards_train/rejected': '0.099207', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022791', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-135.13', 'loss/train': '0.7121', 'examples_per_second': '30.468', 'grad_norm': '20', 'counters/examples': 75264, 'counters/updates': 2352}
skipping logging after 75296 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': '0.049685', 'rewards_train/rejected': '0.079549', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029864', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-154.75', 'loss/train': '0.71422', 'examples_per_second': '24.062', 'grad_norm': '22.375', 'counters/examples': 75328, 'counters/updates': 2354}
train stats after 75360 examples: {'rewards_train/chosen': '0.11685', 'rewards_train/rejected': '0.032831', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084017', 'logps_train/rejected': '-94.055', 'logps_train/chosen': '-124.77', 'loss/train': '0.66315', 'examples_per_second': '32.827', 'grad_norm': '18.75', 'counters/examples': 75360, 'counters/updates': 2355}
train stats after 75392 examples: {'rewards_train/chosen': '0.087207', 'rewards_train/rejected': '0.080479', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0067278', 'logps_train/rejected': '-109.39', 'logps_train/chosen': '-115.15', 'loss/train': '0.69634', 'examples_per_second': '30.055', 'grad_norm': '19.75', 'counters/examples': 75392, 'counters/updates': 2356}
skipping logging after 75424 examples to avoid logging too frequently
train stats after 75456 examples: {'rewards_train/chosen': '0.10884', 'rewards_train/rejected': '0.082769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02607', 'logps_train/rejected': '-129.35', 'logps_train/chosen': '-150.07', 'loss/train': '0.68635', 'examples_per_second': '36.094', 'grad_norm': '21.625', 'counters/examples': 75456, 'counters/updates': 2358}
train stats after 75488 examples: {'rewards_train/chosen': '0.13022', 'rewards_train/rejected': '0.099776', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030446', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-144.39', 'loss/train': '0.68641', 'examples_per_second': '30.444', 'grad_norm': '21.625', 'counters/examples': 75488, 'counters/updates': 2359}
train stats after 75520 examples: {'rewards_train/chosen': '0.12885', 'rewards_train/rejected': '0.019762', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.10909', 'logps_train/rejected': '-117.6', 'logps_train/chosen': '-174.36', 'loss/train': '0.64857', 'examples_per_second': '31.442', 'grad_norm': '21', 'counters/examples': 75520, 'counters/updates': 2360}
train stats after 75552 examples: {'rewards_train/chosen': '0.10865', 'rewards_train/rejected': '0.0045628', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10409', 'logps_train/rejected': '-136.89', 'logps_train/chosen': '-115.23', 'loss/train': '0.65273', 'examples_per_second': '30.429', 'grad_norm': '18.5', 'counters/examples': 75552, 'counters/updates': 2361}
train stats after 75584 examples: {'rewards_train/chosen': '0.10226', 'rewards_train/rejected': '0.074853', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027406', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-112.95', 'loss/train': '0.68429', 'examples_per_second': '30.011', 'grad_norm': '20.125', 'counters/examples': 75584, 'counters/updates': 2362}
train stats after 75616 examples: {'rewards_train/chosen': '0.064609', 'rewards_train/rejected': '0.010498', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054111', 'logps_train/rejected': '-100.83', 'logps_train/chosen': '-119.97', 'loss/train': '0.67338', 'examples_per_second': '32.914', 'grad_norm': '18.25', 'counters/examples': 75616, 'counters/updates': 2363}
train stats after 75648 examples: {'rewards_train/chosen': '0.15844', 'rewards_train/rejected': '0.063437', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095002', 'logps_train/rejected': '-144.47', 'logps_train/chosen': '-161.19', 'loss/train': '0.65564', 'examples_per_second': '31.846', 'grad_norm': '22.25', 'counters/examples': 75648, 'counters/updates': 2364}
train stats after 75680 examples: {'rewards_train/chosen': '0.14787', 'rewards_train/rejected': '0.061009', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086864', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-116.88', 'loss/train': '0.65996', 'examples_per_second': '31.57', 'grad_norm': '19.125', 'counters/examples': 75680, 'counters/updates': 2365}
skipping logging after 75712 examples to avoid logging too frequently
train stats after 75744 examples: {'rewards_train/chosen': '0.13884', 'rewards_train/rejected': '0.053682', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085154', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-130.13', 'loss/train': '0.65694', 'examples_per_second': '34.378', 'grad_norm': '19.75', 'counters/examples': 75744, 'counters/updates': 2367}
train stats after 75776 examples: {'rewards_train/chosen': '0.096205', 'rewards_train/rejected': '0.083648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012557', 'logps_train/rejected': '-100.21', 'logps_train/chosen': '-143.96', 'loss/train': '0.69172', 'examples_per_second': '30.237', 'grad_norm': '20.125', 'counters/examples': 75776, 'counters/updates': 2368}
train stats after 75808 examples: {'rewards_train/chosen': '0.13111', 'rewards_train/rejected': '0.052338', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078767', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-170.33', 'loss/train': '0.66138', 'examples_per_second': '31.378', 'grad_norm': '20.625', 'counters/examples': 75808, 'counters/updates': 2369}
train stats after 75840 examples: {'rewards_train/chosen': '0.11752', 'rewards_train/rejected': '0.057223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060299', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-126.96', 'loss/train': '0.6676', 'examples_per_second': '31.051', 'grad_norm': '22', 'counters/examples': 75840, 'counters/updates': 2370}
train stats after 75872 examples: {'rewards_train/chosen': '0.15158', 'rewards_train/rejected': '0.070814', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080769', 'logps_train/rejected': '-91.558', 'logps_train/chosen': '-123.95', 'loss/train': '0.66091', 'examples_per_second': '31.505', 'grad_norm': '17.75', 'counters/examples': 75872, 'counters/updates': 2371}
train stats after 75904 examples: {'rewards_train/chosen': '0.038634', 'rewards_train/rejected': '0.049872', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011237', 'logps_train/rejected': '-113.75', 'logps_train/chosen': '-141.89', 'loss/train': '0.70653', 'examples_per_second': '31.493', 'grad_norm': '21.125', 'counters/examples': 75904, 'counters/updates': 2372}
train stats after 75936 examples: {'rewards_train/chosen': '0.11347', 'rewards_train/rejected': '0.058083', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05539', 'logps_train/rejected': '-108.41', 'logps_train/chosen': '-139.53', 'loss/train': '0.67366', 'examples_per_second': '31.586', 'grad_norm': '20.125', 'counters/examples': 75936, 'counters/updates': 2373}
train stats after 75968 examples: {'rewards_train/chosen': '0.081561', 'rewards_train/rejected': '0.055255', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026306', 'logps_train/rejected': '-115.18', 'logps_train/chosen': '-126.24', 'loss/train': '0.68673', 'examples_per_second': '31.568', 'grad_norm': '20.625', 'counters/examples': 75968, 'counters/updates': 2374}
train stats after 76000 examples: {'rewards_train/chosen': '0.08223', 'rewards_train/rejected': '0.034011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048219', 'logps_train/rejected': '-118.35', 'logps_train/chosen': '-108.86', 'loss/train': '0.6773', 'examples_per_second': '31.523', 'grad_norm': '20.75', 'counters/examples': 76000, 'counters/updates': 2375}
train stats after 76032 examples: {'rewards_train/chosen': '0.0714', 'rewards_train/rejected': '0.039101', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.032299', 'logps_train/rejected': '-151.38', 'logps_train/chosen': '-177.11', 'loss/train': '0.68521', 'examples_per_second': '32.927', 'grad_norm': '23.125', 'counters/examples': 76032, 'counters/updates': 2376}
train stats after 76064 examples: {'rewards_train/chosen': '0.01708', 'rewards_train/rejected': '0.060817', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043737', 'logps_train/rejected': '-144.03', 'logps_train/chosen': '-149.87', 'loss/train': '0.7243', 'examples_per_second': '30.009', 'grad_norm': '23.625', 'counters/examples': 76064, 'counters/updates': 2377}
train stats after 76096 examples: {'rewards_train/chosen': '0.13232', 'rewards_train/rejected': '0.041827', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090489', 'logps_train/rejected': '-102.61', 'logps_train/chosen': '-115.76', 'loss/train': '0.65411', 'examples_per_second': '31.467', 'grad_norm': '18.125', 'counters/examples': 76096, 'counters/updates': 2378}
skipping logging after 76128 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '0.069547', 'rewards_train/rejected': '0.052687', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01686', 'logps_train/rejected': '-100.22', 'logps_train/chosen': '-114.34', 'loss/train': '0.69213', 'examples_per_second': '34.795', 'grad_norm': '18.125', 'counters/examples': 76160, 'counters/updates': 2380}
skipping logging after 76192 examples to avoid logging too frequently
train stats after 76224 examples: {'rewards_train/chosen': '0.029243', 'rewards_train/rejected': '0.075067', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.045824', 'logps_train/rejected': '-164.88', 'logps_train/chosen': '-156.09', 'loss/train': '0.72839', 'examples_per_second': '31.464', 'grad_norm': '24.25', 'counters/examples': 76224, 'counters/updates': 2382}
skipping logging after 76256 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '0.087804', 'rewards_train/rejected': '0.027361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060443', 'logps_train/rejected': '-106.86', 'logps_train/chosen': '-115.88', 'loss/train': '0.66826', 'examples_per_second': '31.733', 'grad_norm': '19.25', 'counters/examples': 76288, 'counters/updates': 2384}
train stats after 76320 examples: {'rewards_train/chosen': '0.056372', 'rewards_train/rejected': '0.028045', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028327', 'logps_train/rejected': '-108.04', 'logps_train/chosen': '-140.8', 'loss/train': '0.68304', 'examples_per_second': '31.131', 'grad_norm': '20.5', 'counters/examples': 76320, 'counters/updates': 2385}
train stats after 76352 examples: {'rewards_train/chosen': '0.1327', 'rewards_train/rejected': '0.011143', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12156', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-176.04', 'loss/train': '0.64012', 'examples_per_second': '30.324', 'grad_norm': '22', 'counters/examples': 76352, 'counters/updates': 2386}
train stats after 76384 examples: {'rewards_train/chosen': '0.069333', 'rewards_train/rejected': '0.037956', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.031376', 'logps_train/rejected': '-116.4', 'logps_train/chosen': '-115.82', 'loss/train': '0.68426', 'examples_per_second': '32.071', 'grad_norm': '20.125', 'counters/examples': 76384, 'counters/updates': 2387}
train stats after 76416 examples: {'rewards_train/chosen': '0.15101', 'rewards_train/rejected': '0.033217', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11779', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-152.34', 'loss/train': '0.64715', 'examples_per_second': '31.325', 'grad_norm': '21.25', 'counters/examples': 76416, 'counters/updates': 2388}
train stats after 76448 examples: {'rewards_train/chosen': '0.10244', 'rewards_train/rejected': '0.04679', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055649', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-142.33', 'loss/train': '0.67008', 'examples_per_second': '31.264', 'grad_norm': '21.5', 'counters/examples': 76448, 'counters/updates': 2389}
train stats after 76480 examples: {'rewards_train/chosen': '0.10406', 'rewards_train/rejected': '0.056651', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047412', 'logps_train/rejected': '-108.48', 'logps_train/chosen': '-138.89', 'loss/train': '0.67349', 'examples_per_second': '30.189', 'grad_norm': '19.25', 'counters/examples': 76480, 'counters/updates': 2390}
train stats after 76512 examples: {'rewards_train/chosen': '0.097158', 'rewards_train/rejected': '0.063351', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.033807', 'logps_train/rejected': '-103.87', 'logps_train/chosen': '-131.24', 'loss/train': '0.68224', 'examples_per_second': '30.217', 'grad_norm': '19.75', 'counters/examples': 76512, 'counters/updates': 2391}
train stats after 76544 examples: {'rewards_train/chosen': '0.083156', 'rewards_train/rejected': '0.037408', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045748', 'logps_train/rejected': '-114.22', 'logps_train/chosen': '-130.78', 'loss/train': '0.6752', 'examples_per_second': '31.468', 'grad_norm': '19.625', 'counters/examples': 76544, 'counters/updates': 2392}
skipping logging after 76576 examples to avoid logging too frequently
train stats after 76608 examples: {'rewards_train/chosen': '0.075477', 'rewards_train/rejected': '0.059957', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015521', 'logps_train/rejected': '-137.73', 'logps_train/chosen': '-143.52', 'loss/train': '0.68997', 'examples_per_second': '35.956', 'grad_norm': '21', 'counters/examples': 76608, 'counters/updates': 2394}
train stats after 76640 examples: {'rewards_train/chosen': '0.068407', 'rewards_train/rejected': '0.01595', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052457', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-104.01', 'loss/train': '0.67171', 'examples_per_second': '29.912', 'grad_norm': '19', 'counters/examples': 76640, 'counters/updates': 2395}
train stats after 76672 examples: {'rewards_train/chosen': '0.16591', 'rewards_train/rejected': '0.03316', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13275', 'logps_train/rejected': '-139.6', 'logps_train/chosen': '-156.93', 'loss/train': '0.63576', 'examples_per_second': '31.489', 'grad_norm': '20.375', 'counters/examples': 76672, 'counters/updates': 2396}
train stats after 76704 examples: {'rewards_train/chosen': '0.052417', 'rewards_train/rejected': '0.025897', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026521', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-152', 'loss/train': '0.68548', 'examples_per_second': '31.377', 'grad_norm': '20.75', 'counters/examples': 76704, 'counters/updates': 2397}
train stats after 76736 examples: {'rewards_train/chosen': '0.10967', 'rewards_train/rejected': '0.043937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065728', 'logps_train/rejected': '-101.65', 'logps_train/chosen': '-107.79', 'loss/train': '0.66608', 'examples_per_second': '32.018', 'grad_norm': '18.125', 'counters/examples': 76736, 'counters/updates': 2398}
train stats after 76768 examples: {'rewards_train/chosen': '0.091865', 'rewards_train/rejected': '0.013999', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077866', 'logps_train/rejected': '-125.24', 'logps_train/chosen': '-137.67', 'loss/train': '0.66236', 'examples_per_second': '31.478', 'grad_norm': '20.25', 'counters/examples': 76768, 'counters/updates': 2399}
train stats after 76800 examples: {'rewards_train/chosen': '0.067629', 'rewards_train/rejected': '0.05805', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0095789', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-133.16', 'loss/train': '0.69456', 'examples_per_second': '31.475', 'grad_norm': '20.5', 'counters/examples': 76800, 'counters/updates': 2400}
skipping logging after 76832 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': '0.093078', 'rewards_train/rejected': '0.042031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051047', 'logps_train/rejected': '-117.84', 'logps_train/chosen': '-121.74', 'loss/train': '0.67389', 'examples_per_second': '31.219', 'grad_norm': '19.375', 'counters/examples': 76864, 'counters/updates': 2402}
train stats after 76896 examples: {'rewards_train/chosen': '0.089244', 'rewards_train/rejected': '0.046142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043102', 'logps_train/rejected': '-128.95', 'logps_train/chosen': '-150.4', 'loss/train': '0.67776', 'examples_per_second': '31.283', 'grad_norm': '21.375', 'counters/examples': 76896, 'counters/updates': 2403}
skipping logging after 76928 examples to avoid logging too frequently
train stats after 76960 examples: {'rewards_train/chosen': '0.13034', 'rewards_train/rejected': '0.096908', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033436', 'logps_train/rejected': '-142.41', 'logps_train/chosen': '-150.09', 'loss/train': '0.68439', 'examples_per_second': '30.112', 'grad_norm': '21.75', 'counters/examples': 76960, 'counters/updates': 2405}
train stats after 76992 examples: {'rewards_train/chosen': '0.082285', 'rewards_train/rejected': '-0.040974', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12326', 'logps_train/rejected': '-94.991', 'logps_train/chosen': '-132.23', 'loss/train': '0.63832', 'examples_per_second': '32.573', 'grad_norm': '19.125', 'counters/examples': 76992, 'counters/updates': 2406}
train stats after 77024 examples: {'rewards_train/chosen': '0.09374', 'rewards_train/rejected': '0.0090226', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.084718', 'logps_train/rejected': '-128.54', 'logps_train/chosen': '-140.74', 'loss/train': '0.65741', 'examples_per_second': '32.937', 'grad_norm': '20.375', 'counters/examples': 77024, 'counters/updates': 2407}
train stats after 77056 examples: {'rewards_train/chosen': '0.10708', 'rewards_train/rejected': '0.024788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082288', 'logps_train/rejected': '-107.99', 'logps_train/chosen': '-120.9', 'loss/train': '0.65755', 'examples_per_second': '31.188', 'grad_norm': '19.25', 'counters/examples': 77056, 'counters/updates': 2408}
train stats after 77088 examples: {'rewards_train/chosen': '0.096176', 'rewards_train/rejected': '-0.0015163', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097692', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-141.62', 'loss/train': '0.64989', 'examples_per_second': '32.592', 'grad_norm': '19.5', 'counters/examples': 77088, 'counters/updates': 2409}
skipping logging after 77120 examples to avoid logging too frequently
train stats after 77152 examples: {'rewards_train/chosen': '0.11853', 'rewards_train/rejected': '0.030147', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088384', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-145.33', 'loss/train': '0.65684', 'examples_per_second': '31.101', 'grad_norm': '19.125', 'counters/examples': 77152, 'counters/updates': 2411}
train stats after 77184 examples: {'rewards_train/chosen': '0.13363', 'rewards_train/rejected': '0.019961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11367', 'logps_train/rejected': '-102', 'logps_train/chosen': '-155', 'loss/train': '0.6469', 'examples_per_second': '30.994', 'grad_norm': '20.25', 'counters/examples': 77184, 'counters/updates': 2412}
train stats after 77216 examples: {'rewards_train/chosen': '0.13112', 'rewards_train/rejected': '0.017573', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11355', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-145.51', 'loss/train': '0.64435', 'examples_per_second': '31.437', 'grad_norm': '21', 'counters/examples': 77216, 'counters/updates': 2413}
train stats after 77248 examples: {'rewards_train/chosen': '0.15356', 'rewards_train/rejected': '0.016231', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13732', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-158.76', 'loss/train': '0.63673', 'examples_per_second': '30.019', 'grad_norm': '21.375', 'counters/examples': 77248, 'counters/updates': 2414}
train stats after 77280 examples: {'rewards_train/chosen': '0.071215', 'rewards_train/rejected': '0.067234', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0039815', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-113.24', 'loss/train': '0.69806', 'examples_per_second': '31.574', 'grad_norm': '19.75', 'counters/examples': 77280, 'counters/updates': 2415}
train stats after 77312 examples: {'rewards_train/chosen': '0.11693', 'rewards_train/rejected': '0.011251', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10568', 'logps_train/rejected': '-147.47', 'logps_train/chosen': '-147.14', 'loss/train': '0.64819', 'examples_per_second': '30.075', 'grad_norm': '21.25', 'counters/examples': 77312, 'counters/updates': 2416}
train stats after 77344 examples: {'rewards_train/chosen': '0.14196', 'rewards_train/rejected': '0.087187', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054774', 'logps_train/rejected': '-130.82', 'logps_train/chosen': '-125.94', 'loss/train': '0.67255', 'examples_per_second': '31.454', 'grad_norm': '20.5', 'counters/examples': 77344, 'counters/updates': 2417}
train stats after 77376 examples: {'rewards_train/chosen': '0.10023', 'rewards_train/rejected': '0.029945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070288', 'logps_train/rejected': '-118.14', 'logps_train/chosen': '-121.4', 'loss/train': '0.66641', 'examples_per_second': '31.401', 'grad_norm': '20', 'counters/examples': 77376, 'counters/updates': 2418}
train stats after 77408 examples: {'rewards_train/chosen': '0.11556', 'rewards_train/rejected': '0.026757', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088807', 'logps_train/rejected': '-99.493', 'logps_train/chosen': '-118.25', 'loss/train': '0.65348', 'examples_per_second': '31.633', 'grad_norm': '17.375', 'counters/examples': 77408, 'counters/updates': 2419}
skipping logging after 77440 examples to avoid logging too frequently
train stats after 77472 examples: {'rewards_train/chosen': '0.10798', 'rewards_train/rejected': '0.1127', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0047284', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-107.09', 'loss/train': '0.70002', 'examples_per_second': '32.627', 'grad_norm': '18.875', 'counters/examples': 77472, 'counters/updates': 2421}
train stats after 77504 examples: {'rewards_train/chosen': '0.099588', 'rewards_train/rejected': '0.037434', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062154', 'logps_train/rejected': '-104.61', 'logps_train/chosen': '-112.55', 'loss/train': '0.66873', 'examples_per_second': '31.287', 'grad_norm': '17.75', 'counters/examples': 77504, 'counters/updates': 2422}
skipping logging after 77536 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '0.091696', 'rewards_train/rejected': '0.050053', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041643', 'logps_train/rejected': '-116.92', 'logps_train/chosen': '-122.16', 'loss/train': '0.67935', 'examples_per_second': '30.205', 'grad_norm': '19.5', 'counters/examples': 77568, 'counters/updates': 2424}
train stats after 77600 examples: {'rewards_train/chosen': '0.12401', 'rewards_train/rejected': '0.0016308', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12238', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-142.35', 'loss/train': '0.63939', 'examples_per_second': '31.018', 'grad_norm': '20', 'counters/examples': 77600, 'counters/updates': 2425}
train stats after 77632 examples: {'rewards_train/chosen': '0.13233', 'rewards_train/rejected': '0.048617', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.083711', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-129.03', 'loss/train': '0.65742', 'examples_per_second': '30.508', 'grad_norm': '19.875', 'counters/examples': 77632, 'counters/updates': 2426}
train stats after 77664 examples: {'rewards_train/chosen': '0.091145', 'rewards_train/rejected': '0.042787', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048358', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-129.45', 'loss/train': '0.67355', 'examples_per_second': '31.889', 'grad_norm': '19', 'counters/examples': 77664, 'counters/updates': 2427}
train stats after 77696 examples: {'rewards_train/chosen': '0.097434', 'rewards_train/rejected': '0.082562', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014872', 'logps_train/rejected': '-139.79', 'logps_train/chosen': '-144.98', 'loss/train': '0.69183', 'examples_per_second': '31.513', 'grad_norm': '20.875', 'counters/examples': 77696, 'counters/updates': 2428}
train stats after 77728 examples: {'rewards_train/chosen': '0.097091', 'rewards_train/rejected': '0.027089', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070001', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-130.34', 'loss/train': '0.66281', 'examples_per_second': '32.659', 'grad_norm': '21.5', 'counters/examples': 77728, 'counters/updates': 2429}
train stats after 77760 examples: {'rewards_train/chosen': '0.068941', 'rewards_train/rejected': '0.033084', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035857', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-157.79', 'loss/train': '0.68264', 'examples_per_second': '31.557', 'grad_norm': '22', 'counters/examples': 77760, 'counters/updates': 2430}
skipping logging after 77792 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '0.097509', 'rewards_train/rejected': '0.056016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041494', 'logps_train/rejected': '-134.15', 'logps_train/chosen': '-160.73', 'loss/train': '0.67942', 'examples_per_second': '30.074', 'grad_norm': '22.625', 'counters/examples': 77824, 'counters/updates': 2432}
train stats after 77856 examples: {'rewards_train/chosen': '0.15934', 'rewards_train/rejected': '0.051112', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10822', 'logps_train/rejected': '-107.8', 'logps_train/chosen': '-130.33', 'loss/train': '0.64841', 'examples_per_second': '30.611', 'grad_norm': '18.875', 'counters/examples': 77856, 'counters/updates': 2433}
train stats after 77888 examples: {'rewards_train/chosen': '0.12863', 'rewards_train/rejected': '0.075765', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052861', 'logps_train/rejected': '-105.63', 'logps_train/chosen': '-124.74', 'loss/train': '0.67569', 'examples_per_second': '30.493', 'grad_norm': '20.5', 'counters/examples': 77888, 'counters/updates': 2434}
train stats after 77920 examples: {'rewards_train/chosen': '0.12112', 'rewards_train/rejected': '0.024279', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096839', 'logps_train/rejected': '-111.26', 'logps_train/chosen': '-148.05', 'loss/train': '0.65252', 'examples_per_second': '32.187', 'grad_norm': '20.25', 'counters/examples': 77920, 'counters/updates': 2435}
train stats after 77952 examples: {'rewards_train/chosen': '0.051265', 'rewards_train/rejected': '0.093329', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.042064', 'logps_train/rejected': '-112.34', 'logps_train/chosen': '-136.31', 'loss/train': '0.72241', 'examples_per_second': '32.445', 'grad_norm': '20.875', 'counters/examples': 77952, 'counters/updates': 2436}
train stats after 77984 examples: {'rewards_train/chosen': '0.081983', 'rewards_train/rejected': '0.0020291', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079954', 'logps_train/rejected': '-95.038', 'logps_train/chosen': '-108.72', 'loss/train': '0.66056', 'examples_per_second': '30.251', 'grad_norm': '18.375', 'counters/examples': 77984, 'counters/updates': 2437}
train stats after 78016 examples: {'rewards_train/chosen': '0.10405', 'rewards_train/rejected': '0.0752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028847', 'logps_train/rejected': '-134.06', 'logps_train/chosen': '-156.59', 'loss/train': '0.68366', 'examples_per_second': '31.098', 'grad_norm': '21.625', 'counters/examples': 78016, 'counters/updates': 2438}
train stats after 78048 examples: {'rewards_train/chosen': '0.063244', 'rewards_train/rejected': '-0.016994', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080237', 'logps_train/rejected': '-99.481', 'logps_train/chosen': '-117.88', 'loss/train': '0.65637', 'examples_per_second': '31.557', 'grad_norm': '18.625', 'counters/examples': 78048, 'counters/updates': 2439}
train stats after 78080 examples: {'rewards_train/chosen': '0.17612', 'rewards_train/rejected': '0.030557', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14556', 'logps_train/rejected': '-155.55', 'logps_train/chosen': '-140.65', 'loss/train': '0.62806', 'examples_per_second': '31.596', 'grad_norm': '21.25', 'counters/examples': 78080, 'counters/updates': 2440}
skipping logging after 78112 examples to avoid logging too frequently
train stats after 78144 examples: {'rewards_train/chosen': '0.15163', 'rewards_train/rejected': '0.037718', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11391', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-158.1', 'loss/train': '0.64368', 'examples_per_second': '36.081', 'grad_norm': '19.5', 'counters/examples': 78144, 'counters/updates': 2442}
train stats after 78176 examples: {'rewards_train/chosen': '0.10438', 'rewards_train/rejected': '0.073045', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031338', 'logps_train/rejected': '-132.63', 'logps_train/chosen': '-143.78', 'loss/train': '0.6837', 'examples_per_second': '30.2', 'grad_norm': '21.5', 'counters/examples': 78176, 'counters/updates': 2443}
train stats after 78208 examples: {'rewards_train/chosen': '0.074852', 'rewards_train/rejected': '0.059916', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014936', 'logps_train/rejected': '-142.19', 'logps_train/chosen': '-139.45', 'loss/train': '0.69449', 'examples_per_second': '31.366', 'grad_norm': '22.75', 'counters/examples': 78208, 'counters/updates': 2444}
train stats after 78240 examples: {'rewards_train/chosen': '0.11746', 'rewards_train/rejected': '0.036224', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081239', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-127.69', 'loss/train': '0.65987', 'examples_per_second': '30.165', 'grad_norm': '18.5', 'counters/examples': 78240, 'counters/updates': 2445}
train stats after 78272 examples: {'rewards_train/chosen': '0.17121', 'rewards_train/rejected': '0.031572', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13964', 'logps_train/rejected': '-94.639', 'logps_train/chosen': '-149.73', 'loss/train': '0.63649', 'examples_per_second': '32.747', 'grad_norm': '20.375', 'counters/examples': 78272, 'counters/updates': 2446}
skipping logging after 78304 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '0.069451', 'rewards_train/rejected': '0.02734', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042111', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-145.53', 'loss/train': '0.67906', 'examples_per_second': '31.673', 'grad_norm': '22.125', 'counters/examples': 78336, 'counters/updates': 2448}
train stats after 78368 examples: {'rewards_train/chosen': '0.12856', 'rewards_train/rejected': '0.059032', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069532', 'logps_train/rejected': '-160.67', 'logps_train/chosen': '-123.82', 'loss/train': '0.66614', 'examples_per_second': '31.003', 'grad_norm': '21.5', 'counters/examples': 78368, 'counters/updates': 2449}
skipping logging after 78400 examples to avoid logging too frequently
train stats after 78432 examples: {'rewards_train/chosen': '0.11996', 'rewards_train/rejected': '0.055288', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064673', 'logps_train/rejected': '-105.4', 'logps_train/chosen': '-128.08', 'loss/train': '0.66665', 'examples_per_second': '33.903', 'grad_norm': '18.875', 'counters/examples': 78432, 'counters/updates': 2451}
train stats after 78464 examples: {'rewards_train/chosen': '0.12889', 'rewards_train/rejected': '0.064186', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064708', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-108.6', 'loss/train': '0.66911', 'examples_per_second': '31.674', 'grad_norm': '18.625', 'counters/examples': 78464, 'counters/updates': 2452}
train stats after 78496 examples: {'rewards_train/chosen': '0.13963', 'rewards_train/rejected': '0.016288', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12335', 'logps_train/rejected': '-91.242', 'logps_train/chosen': '-142.6', 'loss/train': '0.64008', 'examples_per_second': '31.737', 'grad_norm': '19.125', 'counters/examples': 78496, 'counters/updates': 2453}
skipping logging after 78528 examples to avoid logging too frequently
train stats after 78560 examples: {'rewards_train/chosen': '0.11022', 'rewards_train/rejected': '0.03927', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.070949', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-102.34', 'loss/train': '0.66359', 'examples_per_second': '31.627', 'grad_norm': '18.5', 'counters/examples': 78560, 'counters/updates': 2455}
skipping logging after 78592 examples to avoid logging too frequently
train stats after 78624 examples: {'rewards_train/chosen': '0.11625', 'rewards_train/rejected': '0.011055', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1052', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-122.7', 'loss/train': '0.64683', 'examples_per_second': '31.311', 'grad_norm': '20.375', 'counters/examples': 78624, 'counters/updates': 2457}
train stats after 78656 examples: {'rewards_train/chosen': '0.16428', 'rewards_train/rejected': '-0.0047731', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16906', 'logps_train/rejected': '-113.48', 'logps_train/chosen': '-177.51', 'loss/train': '0.61805', 'examples_per_second': '31.579', 'grad_norm': '20.5', 'counters/examples': 78656, 'counters/updates': 2458}
train stats after 78688 examples: {'rewards_train/chosen': '0.052449', 'rewards_train/rejected': '0.045955', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0064936', 'logps_train/rejected': '-94.18', 'logps_train/chosen': '-122.05', 'loss/train': '0.69491', 'examples_per_second': '31.704', 'grad_norm': '19', 'counters/examples': 78688, 'counters/updates': 2459}
train stats after 78720 examples: {'rewards_train/chosen': '0.1602', 'rewards_train/rejected': '0.13574', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024455', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-170.19', 'loss/train': '0.68968', 'examples_per_second': '30.689', 'grad_norm': '22.75', 'counters/examples': 78720, 'counters/updates': 2460}
train stats after 78752 examples: {'rewards_train/chosen': '0.11858', 'rewards_train/rejected': '0.010473', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10811', 'logps_train/rejected': '-86.064', 'logps_train/chosen': '-159.93', 'loss/train': '0.64636', 'examples_per_second': '31.102', 'grad_norm': '19.625', 'counters/examples': 78752, 'counters/updates': 2461}
train stats after 78784 examples: {'rewards_train/chosen': '0.21515', 'rewards_train/rejected': '0.025938', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18921', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-127.31', 'loss/train': '0.61182', 'examples_per_second': '31.691', 'grad_norm': '18.75', 'counters/examples': 78784, 'counters/updates': 2462}
train stats after 78816 examples: {'rewards_train/chosen': '0.15008', 'rewards_train/rejected': '0.074333', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075748', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-163.89', 'loss/train': '0.66154', 'examples_per_second': '32.613', 'grad_norm': '20.25', 'counters/examples': 78816, 'counters/updates': 2463}
skipping logging after 78848 examples to avoid logging too frequently
train stats after 78880 examples: {'rewards_train/chosen': '0.1243', 'rewards_train/rejected': '0.024577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099718', 'logps_train/rejected': '-90.365', 'logps_train/chosen': '-115.59', 'loss/train': '0.65229', 'examples_per_second': '32.422', 'grad_norm': '17.625', 'counters/examples': 78880, 'counters/updates': 2465}
train stats after 78912 examples: {'rewards_train/chosen': '0.203', 'rewards_train/rejected': '0.072291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13071', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-149.11', 'loss/train': '0.64052', 'examples_per_second': '31.84', 'grad_norm': '19.875', 'counters/examples': 78912, 'counters/updates': 2466}
train stats after 78944 examples: {'rewards_train/chosen': '0.10286', 'rewards_train/rejected': '0.032667', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070192', 'logps_train/rejected': '-104.26', 'logps_train/chosen': '-138.87', 'loss/train': '0.66625', 'examples_per_second': '31.087', 'grad_norm': '21', 'counters/examples': 78944, 'counters/updates': 2467}
train stats after 78976 examples: {'rewards_train/chosen': '0.10611', 'rewards_train/rejected': '0.031904', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074204', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-128.85', 'loss/train': '0.66252', 'examples_per_second': '30.079', 'grad_norm': '19.5', 'counters/examples': 78976, 'counters/updates': 2468}
train stats after 79008 examples: {'rewards_train/chosen': '0.090117', 'rewards_train/rejected': '0.054892', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035226', 'logps_train/rejected': '-90.09', 'logps_train/chosen': '-107.63', 'loss/train': '0.68003', 'examples_per_second': '31.285', 'grad_norm': '18.25', 'counters/examples': 79008, 'counters/updates': 2469}
train stats after 79040 examples: {'rewards_train/chosen': '0.17096', 'rewards_train/rejected': '0.11459', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056369', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-146.59', 'loss/train': '0.67095', 'examples_per_second': '31.032', 'grad_norm': '20.375', 'counters/examples': 79040, 'counters/updates': 2470}
skipping logging after 79072 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '0.13188', 'rewards_train/rejected': '0.034561', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097321', 'logps_train/rejected': '-129.25', 'logps_train/chosen': '-138.77', 'loss/train': '0.65498', 'examples_per_second': '34.914', 'grad_norm': '20.5', 'counters/examples': 79104, 'counters/updates': 2472}
skipping logging after 79136 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': '0.12218', 'rewards_train/rejected': '0.079864', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042319', 'logps_train/rejected': '-98.807', 'logps_train/chosen': '-154.07', 'loss/train': '0.67733', 'examples_per_second': '31.801', 'grad_norm': '20', 'counters/examples': 79168, 'counters/updates': 2474}
train stats after 79200 examples: {'rewards_train/chosen': '0.14801', 'rewards_train/rejected': '-0.00032971', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14834', 'logps_train/rejected': '-144.61', 'logps_train/chosen': '-186.12', 'loss/train': '0.6314', 'examples_per_second': '30.492', 'grad_norm': '23.125', 'counters/examples': 79200, 'counters/updates': 2475}
skipping logging after 79232 examples to avoid logging too frequently
train stats after 79264 examples: {'rewards_train/chosen': '0.085933', 'rewards_train/rejected': '0.052519', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033414', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-133.33', 'loss/train': '0.67849', 'examples_per_second': '24.526', 'grad_norm': '19.125', 'counters/examples': 79264, 'counters/updates': 2477}
train stats after 79296 examples: {'rewards_train/chosen': '0.12207', 'rewards_train/rejected': '0.057659', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06441', 'logps_train/rejected': '-142.87', 'logps_train/chosen': '-139.51', 'loss/train': '0.66575', 'examples_per_second': '30.323', 'grad_norm': '21.25', 'counters/examples': 79296, 'counters/updates': 2478}
train stats after 79328 examples: {'rewards_train/chosen': '0.12527', 'rewards_train/rejected': '0.045383', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079888', 'logps_train/rejected': '-122.83', 'logps_train/chosen': '-133.86', 'loss/train': '0.65878', 'examples_per_second': '31.16', 'grad_norm': '20.25', 'counters/examples': 79328, 'counters/updates': 2479}
train stats after 79360 examples: {'rewards_train/chosen': '0.071153', 'rewards_train/rejected': '0.031813', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03934', 'logps_train/rejected': '-105.3', 'logps_train/chosen': '-120.75', 'loss/train': '0.67942', 'examples_per_second': '27.8', 'grad_norm': '19.375', 'counters/examples': 79360, 'counters/updates': 2480}
train stats after 79392 examples: {'rewards_train/chosen': '0.1266', 'rewards_train/rejected': '0.057957', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068646', 'logps_train/rejected': '-127.79', 'logps_train/chosen': '-126.54', 'loss/train': '0.6663', 'examples_per_second': '31.305', 'grad_norm': '19.125', 'counters/examples': 79392, 'counters/updates': 2481}
train stats after 79424 examples: {'rewards_train/chosen': '0.090504', 'rewards_train/rejected': '0.040391', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050113', 'logps_train/rejected': '-111.27', 'logps_train/chosen': '-135.23', 'loss/train': '0.67325', 'examples_per_second': '32.144', 'grad_norm': '19.25', 'counters/examples': 79424, 'counters/updates': 2482}
train stats after 79456 examples: {'rewards_train/chosen': '0.079438', 'rewards_train/rejected': '0.068154', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011284', 'logps_train/rejected': '-127.93', 'logps_train/chosen': '-135.06', 'loss/train': '0.69398', 'examples_per_second': '32.23', 'grad_norm': '21.75', 'counters/examples': 79456, 'counters/updates': 2483}
train stats after 79488 examples: {'rewards_train/chosen': '0.1967', 'rewards_train/rejected': '0.035671', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16102', 'logps_train/rejected': '-109.74', 'logps_train/chosen': '-146.42', 'loss/train': '0.62303', 'examples_per_second': '31.1', 'grad_norm': '19.125', 'counters/examples': 79488, 'counters/updates': 2484}
train stats after 79520 examples: {'rewards_train/chosen': '0.17199', 'rewards_train/rejected': '0.067144', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10485', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-129.26', 'loss/train': '0.65123', 'examples_per_second': '31.665', 'grad_norm': '20.75', 'counters/examples': 79520, 'counters/updates': 2485}
skipping logging after 79552 examples to avoid logging too frequently
train stats after 79584 examples: {'rewards_train/chosen': '0.12011', 'rewards_train/rejected': '0.017809', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1023', 'logps_train/rejected': '-124.68', 'logps_train/chosen': '-181.8', 'loss/train': '0.64719', 'examples_per_second': '31.651', 'grad_norm': '20.75', 'counters/examples': 79584, 'counters/updates': 2487}
train stats after 79616 examples: {'rewards_train/chosen': '0.076424', 'rewards_train/rejected': '0.052619', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.023805', 'logps_train/rejected': '-156.5', 'logps_train/chosen': '-161.48', 'loss/train': '0.68759', 'examples_per_second': '31.631', 'grad_norm': '23.375', 'counters/examples': 79616, 'counters/updates': 2488}
train stats after 79648 examples: {'rewards_train/chosen': '0.11601', 'rewards_train/rejected': '0.064281', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051734', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-151.89', 'loss/train': '0.67517', 'examples_per_second': '32.171', 'grad_norm': '21.125', 'counters/examples': 79648, 'counters/updates': 2489}
train stats after 79680 examples: {'rewards_train/chosen': '0.1724', 'rewards_train/rejected': '0.07874', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093662', 'logps_train/rejected': '-109.73', 'logps_train/chosen': '-137.12', 'loss/train': '0.65426', 'examples_per_second': '31.636', 'grad_norm': '19.5', 'counters/examples': 79680, 'counters/updates': 2490}
train stats after 79712 examples: {'rewards_train/chosen': '0.12103', 'rewards_train/rejected': '0.045815', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075212', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-152.58', 'loss/train': '0.66715', 'examples_per_second': '31.028', 'grad_norm': '21.625', 'counters/examples': 79712, 'counters/updates': 2491}
train stats after 79744 examples: {'rewards_train/chosen': '0.048456', 'rewards_train/rejected': '0.057982', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0095256', 'logps_train/rejected': '-123.23', 'logps_train/chosen': '-176.95', 'loss/train': '0.70487', 'examples_per_second': '30.667', 'grad_norm': '23.625', 'counters/examples': 79744, 'counters/updates': 2492}
train stats after 79776 examples: {'rewards_train/chosen': '0.028589', 'rewards_train/rejected': '0.066814', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038225', 'logps_train/rejected': '-153.58', 'logps_train/chosen': '-139.41', 'loss/train': '0.72047', 'examples_per_second': '31.696', 'grad_norm': '23.5', 'counters/examples': 79776, 'counters/updates': 2493}
train stats after 79808 examples: {'rewards_train/chosen': '0.21149', 'rewards_train/rejected': '0.15178', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059704', 'logps_train/rejected': '-139.22', 'logps_train/chosen': '-159.49', 'loss/train': '0.66932', 'examples_per_second': '32.593', 'grad_norm': '20.625', 'counters/examples': 79808, 'counters/updates': 2494}
train stats after 79840 examples: {'rewards_train/chosen': '0.13961', 'rewards_train/rejected': '0.084856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054753', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-136.06', 'loss/train': '0.67716', 'examples_per_second': '31.015', 'grad_norm': '21', 'counters/examples': 79840, 'counters/updates': 2495}
train stats after 79872 examples: {'rewards_train/chosen': '0.056232', 'rewards_train/rejected': '0.064118', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0078863', 'logps_train/rejected': '-150.9', 'logps_train/chosen': '-109.2', 'loss/train': '0.70272', 'examples_per_second': '31.547', 'grad_norm': '21.5', 'counters/examples': 79872, 'counters/updates': 2496}
train stats after 79904 examples: {'rewards_train/chosen': '0.13709', 'rewards_train/rejected': '0.027975', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10911', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-133.6', 'loss/train': '0.64798', 'examples_per_second': '30.08', 'grad_norm': '20.25', 'counters/examples': 79904, 'counters/updates': 2497}
train stats after 79936 examples: {'rewards_train/chosen': '0.18347', 'rewards_train/rejected': '0.072373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1111', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-150.23', 'loss/train': '0.64827', 'examples_per_second': '31.636', 'grad_norm': '21', 'counters/examples': 79936, 'counters/updates': 2498}
train stats after 79968 examples: {'rewards_train/chosen': '0.11677', 'rewards_train/rejected': '0.062679', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054094', 'logps_train/rejected': '-117.56', 'logps_train/chosen': '-116.24', 'loss/train': '0.67521', 'examples_per_second': '31.664', 'grad_norm': '20.25', 'counters/examples': 79968, 'counters/updates': 2499}
train stats after 80000 examples: {'rewards_train/chosen': '0.12246', 'rewards_train/rejected': '0.054967', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067492', 'logps_train/rejected': '-161.72', 'logps_train/chosen': '-142.92', 'loss/train': '0.6683', 'examples_per_second': '31.667', 'grad_norm': '22', 'counters/examples': 80000, 'counters/updates': 2500}
train stats after 80032 examples: {'rewards_train/chosen': '0.10364', 'rewards_train/rejected': '0.063081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040558', 'logps_train/rejected': '-112.67', 'logps_train/chosen': '-128.03', 'loss/train': '0.6783', 'examples_per_second': '30.591', 'grad_norm': '20.25', 'counters/examples': 80032, 'counters/updates': 2501}
skipping logging after 80064 examples to avoid logging too frequently
train stats after 80096 examples: {'rewards_train/chosen': '0.083038', 'rewards_train/rejected': '0.048233', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034805', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-145.57', 'loss/train': '0.68311', 'examples_per_second': '35.853', 'grad_norm': '20.25', 'counters/examples': 80096, 'counters/updates': 2503}
skipping logging after 80128 examples to avoid logging too frequently
train stats after 80160 examples: {'rewards_train/chosen': '0.13382', 'rewards_train/rejected': '0.048167', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085657', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-111.47', 'loss/train': '0.65522', 'examples_per_second': '34.627', 'grad_norm': '18', 'counters/examples': 80160, 'counters/updates': 2505}
train stats after 80192 examples: {'rewards_train/chosen': '0.072177', 'rewards_train/rejected': '0.015512', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056665', 'logps_train/rejected': '-96.508', 'logps_train/chosen': '-113.78', 'loss/train': '0.67003', 'examples_per_second': '31.793', 'grad_norm': '18.125', 'counters/examples': 80192, 'counters/updates': 2506}
train stats after 80224 examples: {'rewards_train/chosen': '0.13957', 'rewards_train/rejected': '0.040275', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0993', 'logps_train/rejected': '-107.56', 'logps_train/chosen': '-129.4', 'loss/train': '0.64873', 'examples_per_second': '32.701', 'grad_norm': '18.125', 'counters/examples': 80224, 'counters/updates': 2507}
train stats after 80256 examples: {'rewards_train/chosen': '0.13107', 'rewards_train/rejected': '0.080122', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050943', 'logps_train/rejected': '-130.18', 'logps_train/chosen': '-137.87', 'loss/train': '0.67783', 'examples_per_second': '31.68', 'grad_norm': '20.875', 'counters/examples': 80256, 'counters/updates': 2508}
train stats after 80288 examples: {'rewards_train/chosen': '0.10649', 'rewards_train/rejected': '0.033326', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073159', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-114.05', 'loss/train': '0.66166', 'examples_per_second': '32.571', 'grad_norm': '18.625', 'counters/examples': 80288, 'counters/updates': 2509}
train stats after 80320 examples: {'rewards_train/chosen': '0.17141', 'rewards_train/rejected': '0.07246', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098947', 'logps_train/rejected': '-158.74', 'logps_train/chosen': '-131.34', 'loss/train': '0.65103', 'examples_per_second': '30.497', 'grad_norm': '22.125', 'counters/examples': 80320, 'counters/updates': 2510}
train stats after 80352 examples: {'rewards_train/chosen': '0.1258', 'rewards_train/rejected': '0.096107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029697', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-158.11', 'loss/train': '0.68451', 'examples_per_second': '30.208', 'grad_norm': '19.75', 'counters/examples': 80352, 'counters/updates': 2511}
skipping logging after 80384 examples to avoid logging too frequently
train stats after 80416 examples: {'rewards_train/chosen': '0.098566', 'rewards_train/rejected': '0.094617', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0039497', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-118.54', 'loss/train': '0.69875', 'examples_per_second': '33.421', 'grad_norm': '20.125', 'counters/examples': 80416, 'counters/updates': 2513}
train stats after 80448 examples: {'rewards_train/chosen': '0.085198', 'rewards_train/rejected': '0.063889', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021309', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-119.74', 'loss/train': '0.68946', 'examples_per_second': '31.546', 'grad_norm': '19.75', 'counters/examples': 80448, 'counters/updates': 2514}
train stats after 80480 examples: {'rewards_train/chosen': '0.097382', 'rewards_train/rejected': '-0.019712', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.11709', 'logps_train/rejected': '-89.517', 'logps_train/chosen': '-117.42', 'loss/train': '0.63951', 'examples_per_second': '31.752', 'grad_norm': '17.25', 'counters/examples': 80480, 'counters/updates': 2515}
skipping logging after 80512 examples to avoid logging too frequently
train stats after 80544 examples: {'rewards_train/chosen': '0.16684', 'rewards_train/rejected': '0.13855', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028294', 'logps_train/rejected': '-105.94', 'logps_train/chosen': '-107', 'loss/train': '0.68347', 'examples_per_second': '30.317', 'grad_norm': '18.375', 'counters/examples': 80544, 'counters/updates': 2517}
train stats after 80576 examples: {'rewards_train/chosen': '0.041197', 'rewards_train/rejected': '7.8401e-05', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041118', 'logps_train/rejected': '-87.794', 'logps_train/chosen': '-103.24', 'loss/train': '0.67791', 'examples_per_second': '30.6', 'grad_norm': '17.75', 'counters/examples': 80576, 'counters/updates': 2518}
train stats after 80608 examples: {'rewards_train/chosen': '0.093616', 'rewards_train/rejected': '0.040945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052671', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-159.96', 'loss/train': '0.67327', 'examples_per_second': '31.441', 'grad_norm': '21.875', 'counters/examples': 80608, 'counters/updates': 2519}
train stats after 80640 examples: {'rewards_train/chosen': '0.067886', 'rewards_train/rejected': '-0.027143', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095029', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-123.63', 'loss/train': '0.65428', 'examples_per_second': '30.838', 'grad_norm': '19.5', 'counters/examples': 80640, 'counters/updates': 2520}
train stats after 80672 examples: {'rewards_train/chosen': '0.14259', 'rewards_train/rejected': '0.090196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052395', 'logps_train/rejected': '-132.26', 'logps_train/chosen': '-125.96', 'loss/train': '0.67379', 'examples_per_second': '31.081', 'grad_norm': '21', 'counters/examples': 80672, 'counters/updates': 2521}
train stats after 80704 examples: {'rewards_train/chosen': '0.14794', 'rewards_train/rejected': '0.046379', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10156', 'logps_train/rejected': '-95.996', 'logps_train/chosen': '-163.74', 'loss/train': '0.64964', 'examples_per_second': '31.513', 'grad_norm': '20.875', 'counters/examples': 80704, 'counters/updates': 2522}
train stats after 80736 examples: {'rewards_train/chosen': '0.12274', 'rewards_train/rejected': '0.06214', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060604', 'logps_train/rejected': '-97.393', 'logps_train/chosen': '-125.39', 'loss/train': '0.67075', 'examples_per_second': '32.971', 'grad_norm': '18.5', 'counters/examples': 80736, 'counters/updates': 2523}
train stats after 80768 examples: {'rewards_train/chosen': '0.14572', 'rewards_train/rejected': '0.055252', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090467', 'logps_train/rejected': '-119.41', 'logps_train/chosen': '-123.75', 'loss/train': '0.65507', 'examples_per_second': '30.74', 'grad_norm': '19.625', 'counters/examples': 80768, 'counters/updates': 2524}
train stats after 80800 examples: {'rewards_train/chosen': '0.15439', 'rewards_train/rejected': '0.055477', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098914', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-139.72', 'loss/train': '0.652', 'examples_per_second': '33.107', 'grad_norm': '19.875', 'counters/examples': 80800, 'counters/updates': 2525}
train stats after 80832 examples: {'rewards_train/chosen': '0.13665', 'rewards_train/rejected': '-0.0184', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15505', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-141.74', 'loss/train': '0.62376', 'examples_per_second': '31.674', 'grad_norm': '20.125', 'counters/examples': 80832, 'counters/updates': 2526}
train stats after 80864 examples: {'rewards_train/chosen': '0.12991', 'rewards_train/rejected': '0.11439', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015518', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-135.81', 'loss/train': '0.68956', 'examples_per_second': '31.687', 'grad_norm': '20.25', 'counters/examples': 80864, 'counters/updates': 2527}
train stats after 80896 examples: {'rewards_train/chosen': '0.059051', 'rewards_train/rejected': '0.025737', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033314', 'logps_train/rejected': '-111.64', 'logps_train/chosen': '-130.75', 'loss/train': '0.6839', 'examples_per_second': '24.779', 'grad_norm': '20.375', 'counters/examples': 80896, 'counters/updates': 2528}
train stats after 80928 examples: {'rewards_train/chosen': '0.11121', 'rewards_train/rejected': '0.067468', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043746', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-159.08', 'loss/train': '0.6769', 'examples_per_second': '31.151', 'grad_norm': '22.125', 'counters/examples': 80928, 'counters/updates': 2529}
train stats after 80960 examples: {'rewards_train/chosen': '0.089286', 'rewards_train/rejected': '0.039443', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049843', 'logps_train/rejected': '-102.94', 'logps_train/chosen': '-150.65', 'loss/train': '0.67323', 'examples_per_second': '31.669', 'grad_norm': '20.5', 'counters/examples': 80960, 'counters/updates': 2530}
skipping logging after 80992 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '0.054123', 'rewards_train/rejected': '0.049384', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0047387', 'logps_train/rejected': '-98.006', 'logps_train/chosen': '-144.77', 'loss/train': '0.6975', 'examples_per_second': '34.125', 'grad_norm': '20.75', 'counters/examples': 81024, 'counters/updates': 2532}
train stats after 81056 examples: {'rewards_train/chosen': '0.10719', 'rewards_train/rejected': '0.069693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037497', 'logps_train/rejected': '-113.15', 'logps_train/chosen': '-127.12', 'loss/train': '0.68053', 'examples_per_second': '31.109', 'grad_norm': '21', 'counters/examples': 81056, 'counters/updates': 2533}
train stats after 81088 examples: {'rewards_train/chosen': '0.11091', 'rewards_train/rejected': '0.044782', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066123', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-150.42', 'loss/train': '0.66941', 'examples_per_second': '31.56', 'grad_norm': '21.25', 'counters/examples': 81088, 'counters/updates': 2534}
train stats after 81120 examples: {'rewards_train/chosen': '0.053164', 'rewards_train/rejected': '0.0046233', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048541', 'logps_train/rejected': '-106.8', 'logps_train/chosen': '-138.88', 'loss/train': '0.67615', 'examples_per_second': '31.651', 'grad_norm': '20.625', 'counters/examples': 81120, 'counters/updates': 2535}
train stats after 81152 examples: {'rewards_train/chosen': '0.12392', 'rewards_train/rejected': '-0.017424', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14134', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-142.65', 'loss/train': '0.63033', 'examples_per_second': '30.935', 'grad_norm': '18.375', 'counters/examples': 81152, 'counters/updates': 2536}
skipping logging after 81184 examples to avoid logging too frequently
train stats after 81216 examples: {'rewards_train/chosen': '0.13681', 'rewards_train/rejected': '0.042881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09393', 'logps_train/rejected': '-102.16', 'logps_train/chosen': '-138.73', 'loss/train': '0.65222', 'examples_per_second': '32.005', 'grad_norm': '18.25', 'counters/examples': 81216, 'counters/updates': 2538}
train stats after 81248 examples: {'rewards_train/chosen': '0.13521', 'rewards_train/rejected': '0.086332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04888', 'logps_train/rejected': '-141.41', 'logps_train/chosen': '-168.44', 'loss/train': '0.67657', 'examples_per_second': '32.032', 'grad_norm': '21.75', 'counters/examples': 81248, 'counters/updates': 2539}
train stats after 81280 examples: {'rewards_train/chosen': '0.11869', 'rewards_train/rejected': '0.088267', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030419', 'logps_train/rejected': '-117.01', 'logps_train/chosen': '-110.7', 'loss/train': '0.68204', 'examples_per_second': '31.672', 'grad_norm': '19', 'counters/examples': 81280, 'counters/updates': 2540}
train stats after 81312 examples: {'rewards_train/chosen': '0.063008', 'rewards_train/rejected': '0.019507', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043502', 'logps_train/rejected': '-112.3', 'logps_train/chosen': '-129.61', 'loss/train': '0.67577', 'examples_per_second': '30.645', 'grad_norm': '20.125', 'counters/examples': 81312, 'counters/updates': 2541}
skipping logging after 81344 examples to avoid logging too frequently
train stats after 81376 examples: {'rewards_train/chosen': '0.1185', 'rewards_train/rejected': '0.042934', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075567', 'logps_train/rejected': '-99.862', 'logps_train/chosen': '-116.83', 'loss/train': '0.66386', 'examples_per_second': '31.036', 'grad_norm': '18.125', 'counters/examples': 81376, 'counters/updates': 2543}
train stats after 81408 examples: {'rewards_train/chosen': '0.12536', 'rewards_train/rejected': '0.11351', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.011843', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-142.56', 'loss/train': '0.69112', 'examples_per_second': '31.434', 'grad_norm': '21', 'counters/examples': 81408, 'counters/updates': 2544}
skipping logging after 81440 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': '0.13709', 'rewards_train/rejected': '0.05814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07895', 'logps_train/rejected': '-127.07', 'logps_train/chosen': '-167.58', 'loss/train': '0.66008', 'examples_per_second': '30.659', 'grad_norm': '21.375', 'counters/examples': 81472, 'counters/updates': 2546}
train stats after 81504 examples: {'rewards_train/chosen': '0.079159', 'rewards_train/rejected': '0.031928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047231', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-122.2', 'loss/train': '0.67745', 'examples_per_second': '31.689', 'grad_norm': '19.875', 'counters/examples': 81504, 'counters/updates': 2547}
skipping logging after 81536 examples to avoid logging too frequently
train stats after 81568 examples: {'rewards_train/chosen': '0.099787', 'rewards_train/rejected': '0.082319', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017469', 'logps_train/rejected': '-123.24', 'logps_train/chosen': '-93.535', 'loss/train': '0.6946', 'examples_per_second': '38.607', 'grad_norm': '19.875', 'counters/examples': 81568, 'counters/updates': 2549}
train stats after 81600 examples: {'rewards_train/chosen': '0.11847', 'rewards_train/rejected': '0.11006', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0084126', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-113.34', 'loss/train': '0.69767', 'examples_per_second': '30.183', 'grad_norm': '20.375', 'counters/examples': 81600, 'counters/updates': 2550}
train stats after 81632 examples: {'rewards_train/chosen': '0.13434', 'rewards_train/rejected': '-0.015929', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15027', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-147.89', 'loss/train': '0.62944', 'examples_per_second': '31.679', 'grad_norm': '19.875', 'counters/examples': 81632, 'counters/updates': 2551}
train stats after 81664 examples: {'rewards_train/chosen': '0.12029', 'rewards_train/rejected': '0.03073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089559', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-122.3', 'loss/train': '0.65526', 'examples_per_second': '30.686', 'grad_norm': '19', 'counters/examples': 81664, 'counters/updates': 2552}
train stats after 81696 examples: {'rewards_train/chosen': '0.13719', 'rewards_train/rejected': '0.045802', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091388', 'logps_train/rejected': '-101.84', 'logps_train/chosen': '-95.277', 'loss/train': '0.65478', 'examples_per_second': '33.144', 'grad_norm': '17.25', 'counters/examples': 81696, 'counters/updates': 2553}
train stats after 81728 examples: {'rewards_train/chosen': '0.17334', 'rewards_train/rejected': '0.057491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11585', 'logps_train/rejected': '-124.01', 'logps_train/chosen': '-165.12', 'loss/train': '0.64232', 'examples_per_second': '31.51', 'grad_norm': '20.375', 'counters/examples': 81728, 'counters/updates': 2554}
train stats after 81760 examples: {'rewards_train/chosen': '0.13187', 'rewards_train/rejected': '0.10089', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030983', 'logps_train/rejected': '-138.27', 'logps_train/chosen': '-166.33', 'loss/train': '0.68326', 'examples_per_second': '31.073', 'grad_norm': '22.5', 'counters/examples': 81760, 'counters/updates': 2555}
train stats after 81792 examples: {'rewards_train/chosen': '0.085826', 'rewards_train/rejected': '0.083689', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0021375', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-131.07', 'loss/train': '0.69875', 'examples_per_second': '32.002', 'grad_norm': '21.375', 'counters/examples': 81792, 'counters/updates': 2556}
train stats after 81824 examples: {'rewards_train/chosen': '0.081655', 'rewards_train/rejected': '0.0091119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072543', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-149.45', 'loss/train': '0.6731', 'examples_per_second': '31.504', 'grad_norm': '19.875', 'counters/examples': 81824, 'counters/updates': 2557}
train stats after 81856 examples: {'rewards_train/chosen': '0.092155', 'rewards_train/rejected': '0.027929', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064225', 'logps_train/rejected': '-94.984', 'logps_train/chosen': '-92.719', 'loss/train': '0.6701', 'examples_per_second': '31.739', 'grad_norm': '17.375', 'counters/examples': 81856, 'counters/updates': 2558}
train stats after 81888 examples: {'rewards_train/chosen': '0.083615', 'rewards_train/rejected': '0.022061', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061554', 'logps_train/rejected': '-147.7', 'logps_train/chosen': '-152.17', 'loss/train': '0.67062', 'examples_per_second': '31.629', 'grad_norm': '21.875', 'counters/examples': 81888, 'counters/updates': 2559}
train stats after 81920 examples: {'rewards_train/chosen': '0.10032', 'rewards_train/rejected': '0.077151', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023168', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-124.28', 'loss/train': '0.68889', 'examples_per_second': '31.181', 'grad_norm': '20.25', 'counters/examples': 81920, 'counters/updates': 2560}
train stats after 81952 examples: {'rewards_train/chosen': '0.037497', 'rewards_train/rejected': '0.031937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0055599', 'logps_train/rejected': '-94.376', 'logps_train/chosen': '-125.81', 'loss/train': '0.69518', 'examples_per_second': '32.498', 'grad_norm': '19.125', 'counters/examples': 81952, 'counters/updates': 2561}
train stats after 81984 examples: {'rewards_train/chosen': '0.076273', 'rewards_train/rejected': '0.019207', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057066', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-175.47', 'loss/train': '0.67221', 'examples_per_second': '31.637', 'grad_norm': '22.5', 'counters/examples': 81984, 'counters/updates': 2562}
train stats after 82016 examples: {'rewards_train/chosen': '0.12462', 'rewards_train/rejected': '0.03007', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094546', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-148.36', 'loss/train': '0.65255', 'examples_per_second': '31.675', 'grad_norm': '20.375', 'counters/examples': 82016, 'counters/updates': 2563}
skipping logging after 82048 examples to avoid logging too frequently
train stats after 82080 examples: {'rewards_train/chosen': '0.065667', 'rewards_train/rejected': '0.047028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018639', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-112.6', 'loss/train': '0.69266', 'examples_per_second': '32.179', 'grad_norm': '19.25', 'counters/examples': 82080, 'counters/updates': 2565}
skipping logging after 82112 examples to avoid logging too frequently
train stats after 82144 examples: {'rewards_train/chosen': '0.13952', 'rewards_train/rejected': '0.043426', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096092', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-120.11', 'loss/train': '0.65152', 'examples_per_second': '31.691', 'grad_norm': '19.375', 'counters/examples': 82144, 'counters/updates': 2567}
train stats after 82176 examples: {'rewards_train/chosen': '0.1555', 'rewards_train/rejected': '0.060075', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095422', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-144.62', 'loss/train': '0.65267', 'examples_per_second': '31.649', 'grad_norm': '20.125', 'counters/examples': 82176, 'counters/updates': 2568}
train stats after 82208 examples: {'rewards_train/chosen': '0.14985', 'rewards_train/rejected': '0.082803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067047', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-155.23', 'loss/train': '0.66578', 'examples_per_second': '32.635', 'grad_norm': '20.75', 'counters/examples': 82208, 'counters/updates': 2569}
train stats after 82240 examples: {'rewards_train/chosen': '0.10362', 'rewards_train/rejected': '0.0014937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10213', 'logps_train/rejected': '-84.138', 'logps_train/chosen': '-179.3', 'loss/train': '0.64817', 'examples_per_second': '32.531', 'grad_norm': '20.75', 'counters/examples': 82240, 'counters/updates': 2570}
train stats after 82272 examples: {'rewards_train/chosen': '0.091432', 'rewards_train/rejected': '0.0060707', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085361', 'logps_train/rejected': '-114.29', 'logps_train/chosen': '-128.73', 'loss/train': '0.6573', 'examples_per_second': '32.007', 'grad_norm': '19', 'counters/examples': 82272, 'counters/updates': 2571}
train stats after 82304 examples: {'rewards_train/chosen': '0.15703', 'rewards_train/rejected': '0.094647', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062385', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-146.41', 'loss/train': '0.6669', 'examples_per_second': '31.85', 'grad_norm': '20.25', 'counters/examples': 82304, 'counters/updates': 2572}
skipping logging after 82336 examples to avoid logging too frequently
train stats after 82368 examples: {'rewards_train/chosen': '0.090129', 'rewards_train/rejected': '0.048894', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041234', 'logps_train/rejected': '-106.51', 'logps_train/chosen': '-141.67', 'loss/train': '0.68035', 'examples_per_second': '33.571', 'grad_norm': '20.625', 'counters/examples': 82368, 'counters/updates': 2574}
train stats after 82400 examples: {'rewards_train/chosen': '0.13481', 'rewards_train/rejected': '0.052345', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082465', 'logps_train/rejected': '-163.36', 'logps_train/chosen': '-157.78', 'loss/train': '0.65865', 'examples_per_second': '31.684', 'grad_norm': '21', 'counters/examples': 82400, 'counters/updates': 2575}
train stats after 82432 examples: {'rewards_train/chosen': '0.087198', 'rewards_train/rejected': '0.058736', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028462', 'logps_train/rejected': '-145.62', 'logps_train/chosen': '-145.17', 'loss/train': '0.68529', 'examples_per_second': '32.161', 'grad_norm': '20.75', 'counters/examples': 82432, 'counters/updates': 2576}
train stats after 82464 examples: {'rewards_train/chosen': '0.13172', 'rewards_train/rejected': '0.11696', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014764', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-123.51', 'loss/train': '0.6904', 'examples_per_second': '30.789', 'grad_norm': '19.375', 'counters/examples': 82464, 'counters/updates': 2577}
train stats after 82496 examples: {'rewards_train/chosen': '0.15081', 'rewards_train/rejected': '0.037187', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11363', 'logps_train/rejected': '-156.88', 'logps_train/chosen': '-160.68', 'loss/train': '0.64642', 'examples_per_second': '31.643', 'grad_norm': '21.875', 'counters/examples': 82496, 'counters/updates': 2578}
train stats after 82528 examples: {'rewards_train/chosen': '0.090686', 'rewards_train/rejected': '0.029151', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061535', 'logps_train/rejected': '-103.64', 'logps_train/chosen': '-146.41', 'loss/train': '0.67005', 'examples_per_second': '30.482', 'grad_norm': '20.125', 'counters/examples': 82528, 'counters/updates': 2579}
train stats after 82560 examples: {'rewards_train/chosen': '0.13592', 'rewards_train/rejected': '0.057945', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07798', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-151.54', 'loss/train': '0.66198', 'examples_per_second': '30.909', 'grad_norm': '19.375', 'counters/examples': 82560, 'counters/updates': 2580}
train stats after 82592 examples: {'rewards_train/chosen': '0.18101', 'rewards_train/rejected': '0.10728', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073734', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-148.33', 'loss/train': '0.66499', 'examples_per_second': '31.694', 'grad_norm': '21.125', 'counters/examples': 82592, 'counters/updates': 2581}
skipping logging after 82624 examples to avoid logging too frequently
train stats after 82656 examples: {'rewards_train/chosen': '0.11469', 'rewards_train/rejected': '0.0031113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11157', 'logps_train/rejected': '-112.31', 'logps_train/chosen': '-113.24', 'loss/train': '0.64797', 'examples_per_second': '31.63', 'grad_norm': '17.75', 'counters/examples': 82656, 'counters/updates': 2583}
train stats after 82688 examples: {'rewards_train/chosen': '0.12311', 'rewards_train/rejected': '0.086983', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036131', 'logps_train/rejected': '-110.24', 'logps_train/chosen': '-141.72', 'loss/train': '0.68476', 'examples_per_second': '30.129', 'grad_norm': '20.875', 'counters/examples': 82688, 'counters/updates': 2584}
train stats after 82720 examples: {'rewards_train/chosen': '0.13646', 'rewards_train/rejected': '0.018063', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1184', 'logps_train/rejected': '-116.06', 'logps_train/chosen': '-119.98', 'loss/train': '0.64297', 'examples_per_second': '31.863', 'grad_norm': '17.625', 'counters/examples': 82720, 'counters/updates': 2585}
train stats after 82752 examples: {'rewards_train/chosen': '0.05673', 'rewards_train/rejected': '0.015891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040839', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-143.69', 'loss/train': '0.67799', 'examples_per_second': '30.536', 'grad_norm': '20.5', 'counters/examples': 82752, 'counters/updates': 2586}
train stats after 82784 examples: {'rewards_train/chosen': '0.05908', 'rewards_train/rejected': '0.051925', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0071546', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-130.3', 'loss/train': '0.70068', 'examples_per_second': '31.667', 'grad_norm': '20.75', 'counters/examples': 82784, 'counters/updates': 2587}
train stats after 82816 examples: {'rewards_train/chosen': '0.098816', 'rewards_train/rejected': '0.041752', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057064', 'logps_train/rejected': '-99.827', 'logps_train/chosen': '-125.45', 'loss/train': '0.67639', 'examples_per_second': '32.588', 'grad_norm': '19.125', 'counters/examples': 82816, 'counters/updates': 2588}
train stats after 82848 examples: {'rewards_train/chosen': '0.15438', 'rewards_train/rejected': '0.092412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061971', 'logps_train/rejected': '-106.17', 'logps_train/chosen': '-153.41', 'loss/train': '0.66977', 'examples_per_second': '31.753', 'grad_norm': '20.125', 'counters/examples': 82848, 'counters/updates': 2589}
train stats after 82880 examples: {'rewards_train/chosen': '0.094953', 'rewards_train/rejected': '-0.019587', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11454', 'logps_train/rejected': '-115.36', 'logps_train/chosen': '-122.81', 'loss/train': '0.64475', 'examples_per_second': '31.686', 'grad_norm': '18.875', 'counters/examples': 82880, 'counters/updates': 2590}
train stats after 82912 examples: {'rewards_train/chosen': '0.0729', 'rewards_train/rejected': '0.048398', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024502', 'logps_train/rejected': '-126.4', 'logps_train/chosen': '-144.78', 'loss/train': '0.68941', 'examples_per_second': '31.672', 'grad_norm': '21.25', 'counters/examples': 82912, 'counters/updates': 2591}
train stats after 82944 examples: {'rewards_train/chosen': '0.08184', 'rewards_train/rejected': '0.083765', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0019255', 'logps_train/rejected': '-161.55', 'logps_train/chosen': '-166.42', 'loss/train': '0.70302', 'examples_per_second': '31.672', 'grad_norm': '22.75', 'counters/examples': 82944, 'counters/updates': 2592}
train stats after 82976 examples: {'rewards_train/chosen': '0.10878', 'rewards_train/rejected': '0.063347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045429', 'logps_train/rejected': '-147.02', 'logps_train/chosen': '-166.14', 'loss/train': '0.67591', 'examples_per_second': '31.593', 'grad_norm': '22.625', 'counters/examples': 82976, 'counters/updates': 2593}
skipping logging after 83008 examples to avoid logging too frequently
train stats after 83040 examples: {'rewards_train/chosen': '0.094921', 'rewards_train/rejected': '-0.0063863', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10131', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-146.09', 'loss/train': '0.65084', 'examples_per_second': '31.612', 'grad_norm': '20.25', 'counters/examples': 83040, 'counters/updates': 2595}
train stats after 83072 examples: {'rewards_train/chosen': '0.13', 'rewards_train/rejected': '0.051885', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07811', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-137.42', 'loss/train': '0.6658', 'examples_per_second': '30.305', 'grad_norm': '21.25', 'counters/examples': 83072, 'counters/updates': 2596}
train stats after 83104 examples: {'rewards_train/chosen': '0.084703', 'rewards_train/rejected': '0.062172', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022532', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-123.41', 'loss/train': '0.69167', 'examples_per_second': '30.303', 'grad_norm': '20.375', 'counters/examples': 83104, 'counters/updates': 2597}
train stats after 83136 examples: {'rewards_train/chosen': '0.065447', 'rewards_train/rejected': '0.059056', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0063904', 'logps_train/rejected': '-134.39', 'logps_train/chosen': '-119.1', 'loss/train': '0.69527', 'examples_per_second': '31.004', 'grad_norm': '20.25', 'counters/examples': 83136, 'counters/updates': 2598}
train stats after 83168 examples: {'rewards_train/chosen': '0.077942', 'rewards_train/rejected': '0.061986', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015956', 'logps_train/rejected': '-142.81', 'logps_train/chosen': '-129.22', 'loss/train': '0.69469', 'examples_per_second': '30.351', 'grad_norm': '21.5', 'counters/examples': 83168, 'counters/updates': 2599}
skipping logging after 83200 examples to avoid logging too frequently
train stats after 83232 examples: {'rewards_train/chosen': '0.089517', 'rewards_train/rejected': '0.049291', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040226', 'logps_train/rejected': '-99.128', 'logps_train/chosen': '-105.22', 'loss/train': '0.67929', 'examples_per_second': '33.978', 'grad_norm': '17.875', 'counters/examples': 83232, 'counters/updates': 2601}
train stats after 83264 examples: {'rewards_train/chosen': '0.057971', 'rewards_train/rejected': '0.059646', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0016744', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-134.08', 'loss/train': '0.70087', 'examples_per_second': '31.509', 'grad_norm': '21.5', 'counters/examples': 83264, 'counters/updates': 2602}
train stats after 83296 examples: {'rewards_train/chosen': '0.076151', 'rewards_train/rejected': '0.060467', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015684', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-141.25', 'loss/train': '0.6902', 'examples_per_second': '32.793', 'grad_norm': '21.5', 'counters/examples': 83296, 'counters/updates': 2603}
skipping logging after 83328 examples to avoid logging too frequently
train stats after 83360 examples: {'rewards_train/chosen': '0.12613', 'rewards_train/rejected': '0.058519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06761', 'logps_train/rejected': '-81.874', 'logps_train/chosen': '-149.11', 'loss/train': '0.66734', 'examples_per_second': '31.372', 'grad_norm': '18.5', 'counters/examples': 83360, 'counters/updates': 2605}
skipping logging after 83392 examples to avoid logging too frequently
train stats after 83424 examples: {'rewards_train/chosen': '0.083629', 'rewards_train/rejected': '0.031986', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051644', 'logps_train/rejected': '-134.72', 'logps_train/chosen': '-141.69', 'loss/train': '0.67357', 'examples_per_second': '30.9', 'grad_norm': '21', 'counters/examples': 83424, 'counters/updates': 2607}
train stats after 83456 examples: {'rewards_train/chosen': '0.074728', 'rewards_train/rejected': '0.013196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061532', 'logps_train/rejected': '-109.74', 'logps_train/chosen': '-143.17', 'loss/train': '0.6691', 'examples_per_second': '31.359', 'grad_norm': '21', 'counters/examples': 83456, 'counters/updates': 2608}
skipping logging after 83488 examples to avoid logging too frequently
train stats after 83520 examples: {'rewards_train/chosen': '0.14773', 'rewards_train/rejected': '0.09204', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055686', 'logps_train/rejected': '-174.16', 'logps_train/chosen': '-152.99', 'loss/train': '0.67203', 'examples_per_second': '32.833', 'grad_norm': '23.375', 'counters/examples': 83520, 'counters/updates': 2610}
skipping logging after 83552 examples to avoid logging too frequently
train stats after 83584 examples: {'rewards_train/chosen': '0.085317', 'rewards_train/rejected': '0.016175', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069142', 'logps_train/rejected': '-96.211', 'logps_train/chosen': '-118.81', 'loss/train': '0.6665', 'examples_per_second': '34.104', 'grad_norm': '17.75', 'counters/examples': 83584, 'counters/updates': 2612}
skipping logging after 83616 examples to avoid logging too frequently
train stats after 83648 examples: {'rewards_train/chosen': '0.078037', 'rewards_train/rejected': '0.054513', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023524', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-145.34', 'loss/train': '0.69061', 'examples_per_second': '35.887', 'grad_norm': '21.25', 'counters/examples': 83648, 'counters/updates': 2614}
train stats after 83680 examples: {'rewards_train/chosen': '0.093399', 'rewards_train/rejected': '0.049665', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043734', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-116.27', 'loss/train': '0.67617', 'examples_per_second': '32.326', 'grad_norm': '19.625', 'counters/examples': 83680, 'counters/updates': 2615}
train stats after 83712 examples: {'rewards_train/chosen': '0.14259', 'rewards_train/rejected': '0.12385', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018741', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-137.67', 'loss/train': '0.69181', 'examples_per_second': '31.231', 'grad_norm': '19.625', 'counters/examples': 83712, 'counters/updates': 2616}
train stats after 83744 examples: {'rewards_train/chosen': '0.19214', 'rewards_train/rejected': '0.031369', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16077', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-155.61', 'loss/train': '0.62368', 'examples_per_second': '30.513', 'grad_norm': '19.125', 'counters/examples': 83744, 'counters/updates': 2617}
skipping logging after 83776 examples to avoid logging too frequently
train stats after 83808 examples: {'rewards_train/chosen': '0.066708', 'rewards_train/rejected': '0.033708', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033001', 'logps_train/rejected': '-83.101', 'logps_train/chosen': '-86.405', 'loss/train': '0.68047', 'examples_per_second': '33.534', 'grad_norm': '16.375', 'counters/examples': 83808, 'counters/updates': 2619}
skipping logging after 83840 examples to avoid logging too frequently
train stats after 83872 examples: {'rewards_train/chosen': '0.10608', 'rewards_train/rejected': '0.033134', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072941', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-137.38', 'loss/train': '0.66362', 'examples_per_second': '30.516', 'grad_norm': '20.125', 'counters/examples': 83872, 'counters/updates': 2621}
train stats after 83904 examples: {'rewards_train/chosen': '0.15584', 'rewards_train/rejected': '0.057873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097966', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-136.78', 'loss/train': '0.65236', 'examples_per_second': '32.128', 'grad_norm': '19.625', 'counters/examples': 83904, 'counters/updates': 2622}
train stats after 83936 examples: {'rewards_train/chosen': '0.11521', 'rewards_train/rejected': '0.061762', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053449', 'logps_train/rejected': '-131.22', 'logps_train/chosen': '-139.52', 'loss/train': '0.67295', 'examples_per_second': '32.359', 'grad_norm': '21.125', 'counters/examples': 83936, 'counters/updates': 2623}
train stats after 83968 examples: {'rewards_train/chosen': '0.21241', 'rewards_train/rejected': '0.0061419', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20627', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-146.78', 'loss/train': '0.6039', 'examples_per_second': '31.172', 'grad_norm': '18.75', 'counters/examples': 83968, 'counters/updates': 2624}
skipping logging after 84000 examples to avoid logging too frequently
Running evaluation after 84000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.91it/s]
eval after 84000: {'rewards_eval/chosen': '0.1192', 'rewards_eval/rejected': '0.051088', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.068115', 'logps_eval/rejected': '-114.88', 'logps_eval/chosen': '-134.36', 'loss/eval': '0.66851'}
skipping save for non epoch
train stats after 84032 examples: {'rewards_train/chosen': '0.18146', 'rewards_train/rejected': '0.082573', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098886', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-157.72', 'loss/train': '0.6568', 'examples_per_second': '31.453', 'grad_norm': '21', 'counters/examples': 84032, 'counters/updates': 2626}
train stats after 84064 examples: {'rewards_train/chosen': '0.069342', 'rewards_train/rejected': '0.019723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049619', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-156.77', 'loss/train': '0.67962', 'examples_per_second': '30.708', 'grad_norm': '21.75', 'counters/examples': 84064, 'counters/updates': 2627}
train stats after 84096 examples: {'rewards_train/chosen': '0.16489', 'rewards_train/rejected': '-0.019253', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18415', 'logps_train/rejected': '-150.19', 'logps_train/chosen': '-148.73', 'loss/train': '0.61546', 'examples_per_second': '32.07', 'grad_norm': '21.75', 'counters/examples': 84096, 'counters/updates': 2628}
train stats after 84128 examples: {'rewards_train/chosen': '0.10137', 'rewards_train/rejected': '0.076508', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024864', 'logps_train/rejected': '-78.336', 'logps_train/chosen': '-135.54', 'loss/train': '0.68894', 'examples_per_second': '31.601', 'grad_norm': '18.625', 'counters/examples': 84128, 'counters/updates': 2629}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84192 examples: {'rewards_train/chosen': '0.12105', 'rewards_train/rejected': '0.018084', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10296', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-99.021', 'loss/train': '0.6507', 'examples_per_second': '31.449', 'grad_norm': '19.25', 'counters/examples': 84192, 'counters/updates': 2631}
train stats after 84224 examples: {'rewards_train/chosen': '0.062792', 'rewards_train/rejected': '-0.019287', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08208', 'logps_train/rejected': '-103.33', 'logps_train/chosen': '-160.28', 'loss/train': '0.66026', 'examples_per_second': '31.672', 'grad_norm': '20.25', 'counters/examples': 84224, 'counters/updates': 2632}
train stats after 84256 examples: {'rewards_train/chosen': '0.076627', 'rewards_train/rejected': '0.085466', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0088392', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-98.146', 'loss/train': '0.70879', 'examples_per_second': '31.007', 'grad_norm': '21.125', 'counters/examples': 84256, 'counters/updates': 2633}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84320 examples: {'rewards_train/chosen': '0.11785', 'rewards_train/rejected': '0.099627', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01822', 'logps_train/rejected': '-113.89', 'logps_train/chosen': '-127.02', 'loss/train': '0.68913', 'examples_per_second': '31.714', 'grad_norm': '19.625', 'counters/examples': 84320, 'counters/updates': 2635}
train stats after 84352 examples: {'rewards_train/chosen': '0.11529', 'rewards_train/rejected': '0.041364', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07393', 'logps_train/rejected': '-105.46', 'logps_train/chosen': '-145.58', 'loss/train': '0.66144', 'examples_per_second': '30.131', 'grad_norm': '20.125', 'counters/examples': 84352, 'counters/updates': 2636}
train stats after 84384 examples: {'rewards_train/chosen': '0.15967', 'rewards_train/rejected': '0.024489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13518', 'logps_train/rejected': '-116.53', 'logps_train/chosen': '-167.65', 'loss/train': '0.63528', 'examples_per_second': '32.418', 'grad_norm': '20.25', 'counters/examples': 84384, 'counters/updates': 2637}
train stats after 84416 examples: {'rewards_train/chosen': '0.13313', 'rewards_train/rejected': '0.068748', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064381', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-142.22', 'loss/train': '0.66835', 'examples_per_second': '30.456', 'grad_norm': '21.875', 'counters/examples': 84416, 'counters/updates': 2638}
train stats after 84448 examples: {'rewards_train/chosen': '0.11234', 'rewards_train/rejected': '0.072676', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039662', 'logps_train/rejected': '-110.28', 'logps_train/chosen': '-149.73', 'loss/train': '0.68163', 'examples_per_second': '30.747', 'grad_norm': '21.5', 'counters/examples': 84448, 'counters/updates': 2639}
train stats after 84480 examples: {'rewards_train/chosen': '0.12161', 'rewards_train/rejected': '0.017376', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10424', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-131.12', 'loss/train': '0.6511', 'examples_per_second': '30.146', 'grad_norm': '20.25', 'counters/examples': 84480, 'counters/updates': 2640}
skipping logging after 84512 examples to avoid logging too frequently
train stats after 84544 examples: {'rewards_train/chosen': '0.12923', 'rewards_train/rejected': '0.10015', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029073', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-162.8', 'loss/train': '0.68377', 'examples_per_second': '31.702', 'grad_norm': '21.75', 'counters/examples': 84544, 'counters/updates': 2642}
skipping logging after 84576 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '0.13905', 'rewards_train/rejected': '0.10745', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031592', 'logps_train/rejected': '-109.61', 'logps_train/chosen': '-101.65', 'loss/train': '0.68602', 'examples_per_second': '31.646', 'grad_norm': '19.125', 'counters/examples': 84608, 'counters/updates': 2644}
train stats after 84640 examples: {'rewards_train/chosen': '0.15911', 'rewards_train/rejected': '0.011743', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14737', 'logps_train/rejected': '-105.56', 'logps_train/chosen': '-122.64', 'loss/train': '0.63035', 'examples_per_second': '31.997', 'grad_norm': '18.25', 'counters/examples': 84640, 'counters/updates': 2645}
train stats after 84672 examples: {'rewards_train/chosen': '0.15474', 'rewards_train/rejected': '0.062245', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092499', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-122.64', 'loss/train': '0.65653', 'examples_per_second': '30.482', 'grad_norm': '19.5', 'counters/examples': 84672, 'counters/updates': 2646}
skipping logging after 84704 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '0.1412', 'rewards_train/rejected': '0.045938', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095257', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-136.14', 'loss/train': '0.6572', 'examples_per_second': '22.975', 'grad_norm': '19.375', 'counters/examples': 84736, 'counters/updates': 2648}
train stats after 84768 examples: {'rewards_train/chosen': '0.1088', 'rewards_train/rejected': '0.023848', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084955', 'logps_train/rejected': '-87.632', 'logps_train/chosen': '-128.32', 'loss/train': '0.65835', 'examples_per_second': '31.08', 'grad_norm': '18.75', 'counters/examples': 84768, 'counters/updates': 2649}
train stats after 84800 examples: {'rewards_train/chosen': '0.19904', 'rewards_train/rejected': '0.11006', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088983', 'logps_train/rejected': '-125.08', 'logps_train/chosen': '-128.53', 'loss/train': '0.65644', 'examples_per_second': '31.404', 'grad_norm': '19.5', 'counters/examples': 84800, 'counters/updates': 2650}
train stats after 84832 examples: {'rewards_train/chosen': '0.13688', 'rewards_train/rejected': '0.10758', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029295', 'logps_train/rejected': '-88.882', 'logps_train/chosen': '-109.85', 'loss/train': '0.68513', 'examples_per_second': '24.17', 'grad_norm': '18.25', 'counters/examples': 84832, 'counters/updates': 2651}
train stats after 84864 examples: {'rewards_train/chosen': '0.153', 'rewards_train/rejected': '0.06897', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084029', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-135', 'loss/train': '0.65715', 'examples_per_second': '30.158', 'grad_norm': '20.25', 'counters/examples': 84864, 'counters/updates': 2652}
train stats after 84896 examples: {'rewards_train/chosen': '0.15979', 'rewards_train/rejected': '0.10055', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.059246', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-137.01', 'loss/train': '0.67333', 'examples_per_second': '31.591', 'grad_norm': '20.75', 'counters/examples': 84896, 'counters/updates': 2653}
train stats after 84928 examples: {'rewards_train/chosen': '0.15051', 'rewards_train/rejected': '-0.0099552', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16047', 'logps_train/rejected': '-94.098', 'logps_train/chosen': '-134.63', 'loss/train': '0.62639', 'examples_per_second': '31.882', 'grad_norm': '18.625', 'counters/examples': 84928, 'counters/updates': 2654}
train stats after 84960 examples: {'rewards_train/chosen': '0.098804', 'rewards_train/rejected': '0.10465', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0058492', 'logps_train/rejected': '-122.67', 'logps_train/chosen': '-127.43', 'loss/train': '0.70115', 'examples_per_second': '31.693', 'grad_norm': '20', 'counters/examples': 84960, 'counters/updates': 2655}
skipping logging after 84992 examples to avoid logging too frequently
train stats after 85024 examples: {'rewards_train/chosen': '0.11984', 'rewards_train/rejected': '0.044928', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.07491', 'logps_train/rejected': '-110.16', 'logps_train/chosen': '-127.57', 'loss/train': '0.6619', 'examples_per_second': '36.864', 'grad_norm': '18.5', 'counters/examples': 85024, 'counters/updates': 2657}
train stats after 85056 examples: {'rewards_train/chosen': '0.093909', 'rewards_train/rejected': '0.038712', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055197', 'logps_train/rejected': '-111.78', 'logps_train/chosen': '-151.12', 'loss/train': '0.6723', 'examples_per_second': '31.697', 'grad_norm': '21.375', 'counters/examples': 85056, 'counters/updates': 2658}
train stats after 85088 examples: {'rewards_train/chosen': '0.12872', 'rewards_train/rejected': '0.084297', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.044419', 'logps_train/rejected': '-112.16', 'logps_train/chosen': '-106.3', 'loss/train': '0.67807', 'examples_per_second': '30.626', 'grad_norm': '18.5', 'counters/examples': 85088, 'counters/updates': 2659}
train stats after 85120 examples: {'rewards_train/chosen': '0.19193', 'rewards_train/rejected': '0.029141', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16279', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-169.6', 'loss/train': '0.62596', 'examples_per_second': '31.62', 'grad_norm': '21.125', 'counters/examples': 85120, 'counters/updates': 2660}
train stats after 85152 examples: {'rewards_train/chosen': '0.085136', 'rewards_train/rejected': '0.053702', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031434', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-113.72', 'loss/train': '0.6854', 'examples_per_second': '31.632', 'grad_norm': '19.5', 'counters/examples': 85152, 'counters/updates': 2661}
train stats after 85184 examples: {'rewards_train/chosen': '0.087361', 'rewards_train/rejected': '0.042661', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0447', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-116.11', 'loss/train': '0.6786', 'examples_per_second': '31.825', 'grad_norm': '19.75', 'counters/examples': 85184, 'counters/updates': 2662}
train stats after 85216 examples: {'rewards_train/chosen': '0.10427', 'rewards_train/rejected': '0.055381', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048886', 'logps_train/rejected': '-113.92', 'logps_train/chosen': '-128.23', 'loss/train': '0.67486', 'examples_per_second': '31.707', 'grad_norm': '20', 'counters/examples': 85216, 'counters/updates': 2663}
train stats after 85248 examples: {'rewards_train/chosen': '0.13302', 'rewards_train/rejected': '0.024943', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10807', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-139.68', 'loss/train': '0.64859', 'examples_per_second': '32.724', 'grad_norm': '19', 'counters/examples': 85248, 'counters/updates': 2664}
train stats after 85280 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '0.058794', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096181', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-141.09', 'loss/train': '0.65519', 'examples_per_second': '30.132', 'grad_norm': '19.5', 'counters/examples': 85280, 'counters/updates': 2665}
train stats after 85312 examples: {'rewards_train/chosen': '0.17068', 'rewards_train/rejected': '0.081641', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089039', 'logps_train/rejected': '-136.57', 'logps_train/chosen': '-150.12', 'loss/train': '0.65575', 'examples_per_second': '31.981', 'grad_norm': '20.75', 'counters/examples': 85312, 'counters/updates': 2666}
skipping logging after 85344 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '0.15', 'rewards_train/rejected': '0.10395', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046049', 'logps_train/rejected': '-159.9', 'logps_train/chosen': '-139.58', 'loss/train': '0.67691', 'examples_per_second': '27.398', 'grad_norm': '21.875', 'counters/examples': 85376, 'counters/updates': 2668}
train stats after 85408 examples: {'rewards_train/chosen': '0.11344', 'rewards_train/rejected': '0.01038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10306', 'logps_train/rejected': '-95.067', 'logps_train/chosen': '-158.18', 'loss/train': '0.65005', 'examples_per_second': '31.601', 'grad_norm': '19.625', 'counters/examples': 85408, 'counters/updates': 2669}
train stats after 85440 examples: {'rewards_train/chosen': '0.11541', 'rewards_train/rejected': '0.025347', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.090067', 'logps_train/rejected': '-101.4', 'logps_train/chosen': '-134.16', 'loss/train': '0.65775', 'examples_per_second': '32.891', 'grad_norm': '20.25', 'counters/examples': 85440, 'counters/updates': 2670}
train stats after 85472 examples: {'rewards_train/chosen': '0.069024', 'rewards_train/rejected': '0.038907', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030116', 'logps_train/rejected': '-105.44', 'logps_train/chosen': '-153.33', 'loss/train': '0.68366', 'examples_per_second': '32.012', 'grad_norm': '20', 'counters/examples': 85472, 'counters/updates': 2671}
skipping logging after 85504 examples to avoid logging too frequently
train stats after 85536 examples: {'rewards_train/chosen': '0.14928', 'rewards_train/rejected': '0.085549', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063732', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-149.87', 'loss/train': '0.66886', 'examples_per_second': '32.023', 'grad_norm': '20.125', 'counters/examples': 85536, 'counters/updates': 2673}
train stats after 85568 examples: {'rewards_train/chosen': '0.078241', 'rewards_train/rejected': '0.02941', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048831', 'logps_train/rejected': '-89.869', 'logps_train/chosen': '-103.44', 'loss/train': '0.67524', 'examples_per_second': '31.146', 'grad_norm': '17.25', 'counters/examples': 85568, 'counters/updates': 2674}
train stats after 85600 examples: {'rewards_train/chosen': '0.15628', 'rewards_train/rejected': '0.040822', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11545', 'logps_train/rejected': '-112.47', 'logps_train/chosen': '-129.87', 'loss/train': '0.64637', 'examples_per_second': '31.654', 'grad_norm': '18.5', 'counters/examples': 85600, 'counters/updates': 2675}
train stats after 85632 examples: {'rewards_train/chosen': '0.097278', 'rewards_train/rejected': '0.070903', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026376', 'logps_train/rejected': '-139.85', 'logps_train/chosen': '-145.26', 'loss/train': '0.68395', 'examples_per_second': '31.683', 'grad_norm': '21', 'counters/examples': 85632, 'counters/updates': 2676}
train stats after 85664 examples: {'rewards_train/chosen': '0.2031', 'rewards_train/rejected': '0.057691', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14541', 'logps_train/rejected': '-139.97', 'logps_train/chosen': '-191.67', 'loss/train': '0.63221', 'examples_per_second': '31.653', 'grad_norm': '21.625', 'counters/examples': 85664, 'counters/updates': 2677}
train stats after 85696 examples: {'rewards_train/chosen': '0.10254', 'rewards_train/rejected': '0.03196', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.070582', 'logps_train/rejected': '-111.57', 'logps_train/chosen': '-137.07', 'loss/train': '0.66328', 'examples_per_second': '32.915', 'grad_norm': '21.125', 'counters/examples': 85696, 'counters/updates': 2678}
skipping logging after 85728 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '0.13556', 'rewards_train/rejected': '0.041221', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094342', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-132.02', 'loss/train': '0.65697', 'examples_per_second': '31.651', 'grad_norm': '20.25', 'counters/examples': 85760, 'counters/updates': 2680}
skipping logging after 85792 examples to avoid logging too frequently
train stats after 85824 examples: {'rewards_train/chosen': '0.13643', 'rewards_train/rejected': '-0.022998', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15943', 'logps_train/rejected': '-102.75', 'logps_train/chosen': '-147.35', 'loss/train': '0.62371', 'examples_per_second': '30.183', 'grad_norm': '18.125', 'counters/examples': 85824, 'counters/updates': 2682}
train stats after 85856 examples: {'rewards_train/chosen': '0.10374', 'rewards_train/rejected': '0.10234', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0014006', 'logps_train/rejected': '-140.49', 'logps_train/chosen': '-156.5', 'loss/train': '0.69868', 'examples_per_second': '30.233', 'grad_norm': '22.375', 'counters/examples': 85856, 'counters/updates': 2683}
train stats after 85888 examples: {'rewards_train/chosen': '0.11487', 'rewards_train/rejected': '0.058008', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056863', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-150.54', 'loss/train': '0.67455', 'examples_per_second': '32.77', 'grad_norm': '20.625', 'counters/examples': 85888, 'counters/updates': 2684}
train stats after 85920 examples: {'rewards_train/chosen': '0.14421', 'rewards_train/rejected': '0.038808', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1054', 'logps_train/rejected': '-115.88', 'logps_train/chosen': '-151.68', 'loss/train': '0.64927', 'examples_per_second': '30.925', 'grad_norm': '19.375', 'counters/examples': 85920, 'counters/updates': 2685}
train stats after 85952 examples: {'rewards_train/chosen': '0.097781', 'rewards_train/rejected': '0.03458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063201', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-144.02', 'loss/train': '0.66707', 'examples_per_second': '31.365', 'grad_norm': '20.625', 'counters/examples': 85952, 'counters/updates': 2686}
train stats after 85984 examples: {'rewards_train/chosen': '0.093269', 'rewards_train/rejected': '0.06187', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0314', 'logps_train/rejected': '-137.03', 'logps_train/chosen': '-119.99', 'loss/train': '0.68196', 'examples_per_second': '31.416', 'grad_norm': '20.5', 'counters/examples': 85984, 'counters/updates': 2687}
train stats after 86016 examples: {'rewards_train/chosen': '0.086144', 'rewards_train/rejected': '0.039075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047069', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-140.6', 'loss/train': '0.6781', 'examples_per_second': '31.389', 'grad_norm': '20', 'counters/examples': 86016, 'counters/updates': 2688}
train stats after 86048 examples: {'rewards_train/chosen': '0.14952', 'rewards_train/rejected': '0.047914', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10161', 'logps_train/rejected': '-145.03', 'logps_train/chosen': '-134.91', 'loss/train': '0.6502', 'examples_per_second': '31.565', 'grad_norm': '20.375', 'counters/examples': 86048, 'counters/updates': 2689}
train stats after 86080 examples: {'rewards_train/chosen': '0.10864', 'rewards_train/rejected': '0.056408', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052228', 'logps_train/rejected': '-169.36', 'logps_train/chosen': '-185.74', 'loss/train': '0.67457', 'examples_per_second': '30.246', 'grad_norm': '23.5', 'counters/examples': 86080, 'counters/updates': 2690}
train stats after 86112 examples: {'rewards_train/chosen': '0.13085', 'rewards_train/rejected': '0.12624', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0046109', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-118.14', 'loss/train': '0.6988', 'examples_per_second': '31.401', 'grad_norm': '20', 'counters/examples': 86112, 'counters/updates': 2691}
skipping logging after 86144 examples to avoid logging too frequently
train stats after 86176 examples: {'rewards_train/chosen': '0.13684', 'rewards_train/rejected': '0.025046', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1118', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-132.04', 'loss/train': '0.64825', 'examples_per_second': '32.381', 'grad_norm': '20.375', 'counters/examples': 86176, 'counters/updates': 2693}
train stats after 86208 examples: {'rewards_train/chosen': '0.1269', 'rewards_train/rejected': '0.062221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064682', 'logps_train/rejected': '-96.073', 'logps_train/chosen': '-118.92', 'loss/train': '0.6677', 'examples_per_second': '32.532', 'grad_norm': '18.75', 'counters/examples': 86208, 'counters/updates': 2694}
train stats after 86240 examples: {'rewards_train/chosen': '0.051983', 'rewards_train/rejected': '0.045998', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.005985', 'logps_train/rejected': '-107.55', 'logps_train/chosen': '-111.67', 'loss/train': '0.6984', 'examples_per_second': '31.731', 'grad_norm': '20', 'counters/examples': 86240, 'counters/updates': 2695}
skipping logging after 86272 examples to avoid logging too frequently
train stats after 86304 examples: {'rewards_train/chosen': '0.054208', 'rewards_train/rejected': '0.065342', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011134', 'logps_train/rejected': '-122.69', 'logps_train/chosen': '-105.93', 'loss/train': '0.70936', 'examples_per_second': '31.037', 'grad_norm': '20.375', 'counters/examples': 86304, 'counters/updates': 2697}
train stats after 86336 examples: {'rewards_train/chosen': '0.11705', 'rewards_train/rejected': '0.1107', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0063512', 'logps_train/rejected': '-146.83', 'logps_train/chosen': '-106.59', 'loss/train': '0.70023', 'examples_per_second': '32.836', 'grad_norm': '19.875', 'counters/examples': 86336, 'counters/updates': 2698}
train stats after 86368 examples: {'rewards_train/chosen': '0.16386', 'rewards_train/rejected': '0.12209', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041764', 'logps_train/rejected': '-146.01', 'logps_train/chosen': '-160.63', 'loss/train': '0.67661', 'examples_per_second': '31.627', 'grad_norm': '22.75', 'counters/examples': 86368, 'counters/updates': 2699}
train stats after 86400 examples: {'rewards_train/chosen': '0.046279', 'rewards_train/rejected': '0.047813', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0015338', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-118.34', 'loss/train': '0.69681', 'examples_per_second': '30.794', 'grad_norm': '19.375', 'counters/examples': 86400, 'counters/updates': 2700}
train stats after 86432 examples: {'rewards_train/chosen': '0.14336', 'rewards_train/rejected': '0.0090159', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13435', 'logps_train/rejected': '-135.69', 'logps_train/chosen': '-129.9', 'loss/train': '0.63317', 'examples_per_second': '31.39', 'grad_norm': '18.625', 'counters/examples': 86432, 'counters/updates': 2701}
train stats after 86464 examples: {'rewards_train/chosen': '0.098528', 'rewards_train/rejected': '0.057721', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040807', 'logps_train/rejected': '-97.631', 'logps_train/chosen': '-155.98', 'loss/train': '0.67837', 'examples_per_second': '24.501', 'grad_norm': '20.25', 'counters/examples': 86464, 'counters/updates': 2702}
train stats after 86496 examples: {'rewards_train/chosen': '0.10978', 'rewards_train/rejected': '0.013516', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096264', 'logps_train/rejected': '-119.96', 'logps_train/chosen': '-151.12', 'loss/train': '0.65972', 'examples_per_second': '31.461', 'grad_norm': '20.25', 'counters/examples': 86496, 'counters/updates': 2703}
train stats after 86528 examples: {'rewards_train/chosen': '0.12125', 'rewards_train/rejected': '0.068344', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052911', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-119.76', 'loss/train': '0.67239', 'examples_per_second': '31.594', 'grad_norm': '19.25', 'counters/examples': 86528, 'counters/updates': 2704}
train stats after 86560 examples: {'rewards_train/chosen': '0.13889', 'rewards_train/rejected': '0.071477', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067409', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-139.13', 'loss/train': '0.66523', 'examples_per_second': '31.502', 'grad_norm': '20.875', 'counters/examples': 86560, 'counters/updates': 2705}
skipping logging after 86592 examples to avoid logging too frequently
train stats after 86624 examples: {'rewards_train/chosen': '0.14222', 'rewards_train/rejected': '0.069932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072289', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-162.22', 'loss/train': '0.6643', 'examples_per_second': '31.541', 'grad_norm': '21.25', 'counters/examples': 86624, 'counters/updates': 2707}
train stats after 86656 examples: {'rewards_train/chosen': '0.087152', 'rewards_train/rejected': '0.099382', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01223', 'logps_train/rejected': '-167.38', 'logps_train/chosen': '-169.74', 'loss/train': '0.70419', 'examples_per_second': '31.34', 'grad_norm': '24.875', 'counters/examples': 86656, 'counters/updates': 2708}
train stats after 86688 examples: {'rewards_train/chosen': '0.10528', 'rewards_train/rejected': '0.079497', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025785', 'logps_train/rejected': '-101.98', 'logps_train/chosen': '-109.67', 'loss/train': '0.68504', 'examples_per_second': '31.034', 'grad_norm': '19.5', 'counters/examples': 86688, 'counters/updates': 2709}
train stats after 86720 examples: {'rewards_train/chosen': '0.058069', 'rewards_train/rejected': '0.058569', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.00049947', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-113.29', 'loss/train': '0.70145', 'examples_per_second': '30.136', 'grad_norm': '18.875', 'counters/examples': 86720, 'counters/updates': 2710}
train stats after 86752 examples: {'rewards_train/chosen': '0.11816', 'rewards_train/rejected': '0.010041', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10812', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-122.08', 'loss/train': '0.64697', 'examples_per_second': '30.275', 'grad_norm': '19.125', 'counters/examples': 86752, 'counters/updates': 2711}
train stats after 86784 examples: {'rewards_train/chosen': '0.13088', 'rewards_train/rejected': '0.1224', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0084739', 'logps_train/rejected': '-143.17', 'logps_train/chosen': '-112.61', 'loss/train': '0.69921', 'examples_per_second': '30.15', 'grad_norm': '20.625', 'counters/examples': 86784, 'counters/updates': 2712}
train stats after 86816 examples: {'rewards_train/chosen': '0.12879', 'rewards_train/rejected': '0.017865', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11092', 'logps_train/rejected': '-84.6', 'logps_train/chosen': '-145.55', 'loss/train': '0.64515', 'examples_per_second': '31.633', 'grad_norm': '18.25', 'counters/examples': 86816, 'counters/updates': 2713}
train stats after 86848 examples: {'rewards_train/chosen': '0.11672', 'rewards_train/rejected': '0.023372', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093346', 'logps_train/rejected': '-122.7', 'logps_train/chosen': '-129.17', 'loss/train': '0.65895', 'examples_per_second': '32.532', 'grad_norm': '19.25', 'counters/examples': 86848, 'counters/updates': 2714}
train stats after 86880 examples: {'rewards_train/chosen': '0.083682', 'rewards_train/rejected': '0.02471', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058972', 'logps_train/rejected': '-156.19', 'logps_train/chosen': '-191.91', 'loss/train': '0.68057', 'examples_per_second': '31.603', 'grad_norm': '25.5', 'counters/examples': 86880, 'counters/updates': 2715}
train stats after 86912 examples: {'rewards_train/chosen': '0.13886', 'rewards_train/rejected': '0.025328', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11353', 'logps_train/rejected': '-132.11', 'logps_train/chosen': '-111.57', 'loss/train': '0.64632', 'examples_per_second': '32.928', 'grad_norm': '18.5', 'counters/examples': 86912, 'counters/updates': 2716}
train stats after 86944 examples: {'rewards_train/chosen': '0.095829', 'rewards_train/rejected': '0.037167', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058662', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-141.63', 'loss/train': '0.66926', 'examples_per_second': '31.38', 'grad_norm': '20.875', 'counters/examples': 86944, 'counters/updates': 2717}
skipping logging after 86976 examples to avoid logging too frequently
train stats after 87008 examples: {'rewards_train/chosen': '0.15356', 'rewards_train/rejected': '0.051631', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10193', 'logps_train/rejected': '-167.18', 'logps_train/chosen': '-141.93', 'loss/train': '0.65128', 'examples_per_second': '31.579', 'grad_norm': '21.75', 'counters/examples': 87008, 'counters/updates': 2719}
skipping logging after 87040 examples to avoid logging too frequently
train stats after 87072 examples: {'rewards_train/chosen': '0.10202', 'rewards_train/rejected': '0.032891', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069125', 'logps_train/rejected': '-137', 'logps_train/chosen': '-99.231', 'loss/train': '0.66632', 'examples_per_second': '31.582', 'grad_norm': '19.5', 'counters/examples': 87072, 'counters/updates': 2721}
train stats after 87104 examples: {'rewards_train/chosen': '0.095235', 'rewards_train/rejected': '-0.00022087', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095456', 'logps_train/rejected': '-117.3', 'logps_train/chosen': '-152.08', 'loss/train': '0.65861', 'examples_per_second': '31.565', 'grad_norm': '21.5', 'counters/examples': 87104, 'counters/updates': 2722}
train stats after 87136 examples: {'rewards_train/chosen': '0.15981', 'rewards_train/rejected': '0.14896', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01085', 'logps_train/rejected': '-144.57', 'logps_train/chosen': '-122.71', 'loss/train': '0.69247', 'examples_per_second': '30.111', 'grad_norm': '21.625', 'counters/examples': 87136, 'counters/updates': 2723}
train stats after 87168 examples: {'rewards_train/chosen': '0.10359', 'rewards_train/rejected': '0.012371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091217', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-123.41', 'loss/train': '0.65814', 'examples_per_second': '30.969', 'grad_norm': '21.125', 'counters/examples': 87168, 'counters/updates': 2724}
train stats after 87200 examples: {'rewards_train/chosen': '0.11961', 'rewards_train/rejected': '0.083213', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036398', 'logps_train/rejected': '-126.45', 'logps_train/chosen': '-134.17', 'loss/train': '0.68575', 'examples_per_second': '32.024', 'grad_norm': '20.25', 'counters/examples': 87200, 'counters/updates': 2725}
skipping logging after 87232 examples to avoid logging too frequently
train stats after 87264 examples: {'rewards_train/chosen': '0.058507', 'rewards_train/rejected': '0.086263', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027756', 'logps_train/rejected': '-145.47', 'logps_train/chosen': '-161.89', 'loss/train': '0.71256', 'examples_per_second': '31.604', 'grad_norm': '21.875', 'counters/examples': 87264, 'counters/updates': 2727}
skipping logging after 87296 examples to avoid logging too frequently
train stats after 87328 examples: {'rewards_train/chosen': '0.13043', 'rewards_train/rejected': '0.023233', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1072', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-117.87', 'loss/train': '0.64774', 'examples_per_second': '32.383', 'grad_norm': '19.375', 'counters/examples': 87328, 'counters/updates': 2729}
train stats after 87360 examples: {'rewards_train/chosen': '0.12085', 'rewards_train/rejected': '-0.039499', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16035', 'logps_train/rejected': '-81.914', 'logps_train/chosen': '-129.55', 'loss/train': '0.62663', 'examples_per_second': '30.272', 'grad_norm': '17.25', 'counters/examples': 87360, 'counters/updates': 2730}
train stats after 87392 examples: {'rewards_train/chosen': '0.16362', 'rewards_train/rejected': '0.035836', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12778', 'logps_train/rejected': '-102.95', 'logps_train/chosen': '-140.47', 'loss/train': '0.64034', 'examples_per_second': '32.131', 'grad_norm': '19.375', 'counters/examples': 87392, 'counters/updates': 2731}
train stats after 87424 examples: {'rewards_train/chosen': '0.064925', 'rewards_train/rejected': '0.081696', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016772', 'logps_train/rejected': '-140.9', 'logps_train/chosen': '-107.42', 'loss/train': '0.70882', 'examples_per_second': '32.175', 'grad_norm': '21.875', 'counters/examples': 87424, 'counters/updates': 2732}
train stats after 87456 examples: {'rewards_train/chosen': '0.12455', 'rewards_train/rejected': '0.07662', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047926', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-156.12', 'loss/train': '0.67969', 'examples_per_second': '32.578', 'grad_norm': '20.75', 'counters/examples': 87456, 'counters/updates': 2733}
skipping logging after 87488 examples to avoid logging too frequently
train stats after 87520 examples: {'rewards_train/chosen': '0.14148', 'rewards_train/rejected': '0.044173', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097312', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-137.95', 'loss/train': '0.65286', 'examples_per_second': '31.616', 'grad_norm': '20.25', 'counters/examples': 87520, 'counters/updates': 2735}
train stats after 87552 examples: {'rewards_train/chosen': '0.14271', 'rewards_train/rejected': '0.023808', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1189', 'logps_train/rejected': '-124.97', 'logps_train/chosen': '-135.02', 'loss/train': '0.64174', 'examples_per_second': '32.235', 'grad_norm': '20.375', 'counters/examples': 87552, 'counters/updates': 2736}
train stats after 87584 examples: {'rewards_train/chosen': '0.10414', 'rewards_train/rejected': '0.061095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043041', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-137.57', 'loss/train': '0.67932', 'examples_per_second': '31.362', 'grad_norm': '21.625', 'counters/examples': 87584, 'counters/updates': 2737}
train stats after 87616 examples: {'rewards_train/chosen': '0.15216', 'rewards_train/rejected': '0.010264', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1419', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-120.28', 'loss/train': '0.63169', 'examples_per_second': '30.679', 'grad_norm': '18.75', 'counters/examples': 87616, 'counters/updates': 2738}
train stats after 87648 examples: {'rewards_train/chosen': '0.12814', 'rewards_train/rejected': '0.038093', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090049', 'logps_train/rejected': '-89.656', 'logps_train/chosen': '-117.77', 'loss/train': '0.65949', 'examples_per_second': '31.664', 'grad_norm': '17.875', 'counters/examples': 87648, 'counters/updates': 2739}
train stats after 87680 examples: {'rewards_train/chosen': '0.14443', 'rewards_train/rejected': '0.067171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077259', 'logps_train/rejected': '-127.63', 'logps_train/chosen': '-127.53', 'loss/train': '0.6606', 'examples_per_second': '31.569', 'grad_norm': '20.25', 'counters/examples': 87680, 'counters/updates': 2740}
skipping logging after 87712 examples to avoid logging too frequently
train stats after 87744 examples: {'rewards_train/chosen': '0.15862', 'rewards_train/rejected': '0.030691', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12793', 'logps_train/rejected': '-96.411', 'logps_train/chosen': '-165.23', 'loss/train': '0.64209', 'examples_per_second': '32.73', 'grad_norm': '20.75', 'counters/examples': 87744, 'counters/updates': 2742}
train stats after 87776 examples: {'rewards_train/chosen': '0.082067', 'rewards_train/rejected': '0.070531', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011536', 'logps_train/rejected': '-108.88', 'logps_train/chosen': '-116.08', 'loss/train': '0.69391', 'examples_per_second': '31.764', 'grad_norm': '19.625', 'counters/examples': 87776, 'counters/updates': 2743}
skipping logging after 87808 examples to avoid logging too frequently
train stats after 87840 examples: {'rewards_train/chosen': '0.12202', 'rewards_train/rejected': '0.03914', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08288', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-126.43', 'loss/train': '0.65878', 'examples_per_second': '39.326', 'grad_norm': '20.5', 'counters/examples': 87840, 'counters/updates': 2745}
skipping logging after 87872 examples to avoid logging too frequently
train stats after 87904 examples: {'rewards_train/chosen': '0.17103', 'rewards_train/rejected': '0.047946', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12308', 'logps_train/rejected': '-101.92', 'logps_train/chosen': '-111.41', 'loss/train': '0.64228', 'examples_per_second': '31.313', 'grad_norm': '18.75', 'counters/examples': 87904, 'counters/updates': 2747}
train stats after 87936 examples: {'rewards_train/chosen': '0.16124', 'rewards_train/rejected': '0.06731', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.093927', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-122.92', 'loss/train': '0.65218', 'examples_per_second': '30.127', 'grad_norm': '19.125', 'counters/examples': 87936, 'counters/updates': 2748}
train stats after 87968 examples: {'rewards_train/chosen': '0.15807', 'rewards_train/rejected': '0.050937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10713', 'logps_train/rejected': '-109.23', 'logps_train/chosen': '-127.49', 'loss/train': '0.64661', 'examples_per_second': '30.651', 'grad_norm': '19', 'counters/examples': 87968, 'counters/updates': 2749}
skipping logging after 88000 examples to avoid logging too frequently
train stats after 88032 examples: {'rewards_train/chosen': '0.10577', 'rewards_train/rejected': '0.016211', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.089561', 'logps_train/rejected': '-105.04', 'logps_train/chosen': '-140.26', 'loss/train': '0.6538', 'examples_per_second': '31.439', 'grad_norm': '19.875', 'counters/examples': 88032, 'counters/updates': 2751}
train stats after 88064 examples: {'rewards_train/chosen': '0.15983', 'rewards_train/rejected': '0.033944', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12589', 'logps_train/rejected': '-111.84', 'logps_train/chosen': '-154.92', 'loss/train': '0.64215', 'examples_per_second': '32.376', 'grad_norm': '19', 'counters/examples': 88064, 'counters/updates': 2752}
train stats after 88096 examples: {'rewards_train/chosen': '0.043316', 'rewards_train/rejected': '0.018625', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024691', 'logps_train/rejected': '-91.949', 'logps_train/chosen': '-119.21', 'loss/train': '0.68483', 'examples_per_second': '32.686', 'grad_norm': '18.375', 'counters/examples': 88096, 'counters/updates': 2753}
train stats after 88128 examples: {'rewards_train/chosen': '0.07136', 'rewards_train/rejected': '0.0384', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03296', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-122.49', 'loss/train': '0.68215', 'examples_per_second': '31.674', 'grad_norm': '19.375', 'counters/examples': 88128, 'counters/updates': 2754}
train stats after 88160 examples: {'rewards_train/chosen': '0.13929', 'rewards_train/rejected': '0.062011', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.077281', 'logps_train/rejected': '-117.85', 'logps_train/chosen': '-170.03', 'loss/train': '0.66905', 'examples_per_second': '31.595', 'grad_norm': '22.25', 'counters/examples': 88160, 'counters/updates': 2755}
train stats after 88192 examples: {'rewards_train/chosen': '0.10626', 'rewards_train/rejected': '0.023667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082589', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-108.28', 'loss/train': '0.65927', 'examples_per_second': '30.172', 'grad_norm': '18.625', 'counters/examples': 88192, 'counters/updates': 2756}
train stats after 88224 examples: {'rewards_train/chosen': '0.09447', 'rewards_train/rejected': '0.059509', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034961', 'logps_train/rejected': '-90.357', 'logps_train/chosen': '-119.23', 'loss/train': '0.68109', 'examples_per_second': '31.646', 'grad_norm': '19.375', 'counters/examples': 88224, 'counters/updates': 2757}
train stats after 88256 examples: {'rewards_train/chosen': '0.16939', 'rewards_train/rejected': '0.020553', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14884', 'logps_train/rejected': '-98.567', 'logps_train/chosen': '-132.92', 'loss/train': '0.62825', 'examples_per_second': '32.057', 'grad_norm': '17.625', 'counters/examples': 88256, 'counters/updates': 2758}
train stats after 88288 examples: {'rewards_train/chosen': '0.16552', 'rewards_train/rejected': '0.054682', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11084', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-147.27', 'loss/train': '0.64893', 'examples_per_second': '31.095', 'grad_norm': '19.125', 'counters/examples': 88288, 'counters/updates': 2759}
train stats after 88320 examples: {'rewards_train/chosen': '0.12856', 'rewards_train/rejected': '0.098906', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029658', 'logps_train/rejected': '-137.37', 'logps_train/chosen': '-121.21', 'loss/train': '0.6885', 'examples_per_second': '31.093', 'grad_norm': '20.875', 'counters/examples': 88320, 'counters/updates': 2760}
train stats after 88352 examples: {'rewards_train/chosen': '0.1334', 'rewards_train/rejected': '0.074262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059137', 'logps_train/rejected': '-106.71', 'logps_train/chosen': '-89.795', 'loss/train': '0.67307', 'examples_per_second': '31.264', 'grad_norm': '17.5', 'counters/examples': 88352, 'counters/updates': 2761}
train stats after 88384 examples: {'rewards_train/chosen': '0.092017', 'rewards_train/rejected': '0.017511', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074507', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-122.19', 'loss/train': '0.66453', 'examples_per_second': '30.044', 'grad_norm': '17.75', 'counters/examples': 88384, 'counters/updates': 2762}
train stats after 88416 examples: {'rewards_train/chosen': '0.1091', 'rewards_train/rejected': '0.014314', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094786', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-163.18', 'loss/train': '0.65274', 'examples_per_second': '31.594', 'grad_norm': '21.375', 'counters/examples': 88416, 'counters/updates': 2763}
train stats after 88448 examples: {'rewards_train/chosen': '0.068773', 'rewards_train/rejected': '-0.021224', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089997', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-135.39', 'loss/train': '0.65525', 'examples_per_second': '32.916', 'grad_norm': '19.5', 'counters/examples': 88448, 'counters/updates': 2764}
train stats after 88480 examples: {'rewards_train/chosen': '0.12393', 'rewards_train/rejected': '0.0048399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11909', 'logps_train/rejected': '-121.28', 'logps_train/chosen': '-155.09', 'loss/train': '0.64395', 'examples_per_second': '31.061', 'grad_norm': '21.125', 'counters/examples': 88480, 'counters/updates': 2765}
train stats after 88512 examples: {'rewards_train/chosen': '0.048973', 'rewards_train/rejected': '0.036807', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012166', 'logps_train/rejected': '-142.92', 'logps_train/chosen': '-160.15', 'loss/train': '0.69314', 'examples_per_second': '30.003', 'grad_norm': '23.375', 'counters/examples': 88512, 'counters/updates': 2766}
train stats after 88544 examples: {'rewards_train/chosen': '0.12413', 'rewards_train/rejected': '-0.0042347', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12836', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-130.52', 'loss/train': '0.63847', 'examples_per_second': '31.595', 'grad_norm': '19', 'counters/examples': 88544, 'counters/updates': 2767}
train stats after 88576 examples: {'rewards_train/chosen': '0.099448', 'rewards_train/rejected': '0.014094', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085354', 'logps_train/rejected': '-108.68', 'logps_train/chosen': '-152.78', 'loss/train': '0.65855', 'examples_per_second': '31.686', 'grad_norm': '21.25', 'counters/examples': 88576, 'counters/updates': 2768}
train stats after 88608 examples: {'rewards_train/chosen': '0.13706', 'rewards_train/rejected': '0.067193', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069864', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-134.47', 'loss/train': '0.66802', 'examples_per_second': '33.013', 'grad_norm': '19.25', 'counters/examples': 88608, 'counters/updates': 2769}
train stats after 88640 examples: {'rewards_train/chosen': '0.1417', 'rewards_train/rejected': '0.14939', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0076847', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-137.57', 'loss/train': '0.70186', 'examples_per_second': '30.53', 'grad_norm': '20', 'counters/examples': 88640, 'counters/updates': 2770}
skipping logging after 88672 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '0.12193', 'rewards_train/rejected': '0.027634', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094293', 'logps_train/rejected': '-106.74', 'logps_train/chosen': '-123.3', 'loss/train': '0.6538', 'examples_per_second': '34.277', 'grad_norm': '18.875', 'counters/examples': 88704, 'counters/updates': 2772}
skipping logging after 88736 examples to avoid logging too frequently
train stats after 88768 examples: {'rewards_train/chosen': '0.086155', 'rewards_train/rejected': '0.026875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059279', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-123.95', 'loss/train': '0.67166', 'examples_per_second': '31.347', 'grad_norm': '20.375', 'counters/examples': 88768, 'counters/updates': 2774}
train stats after 88800 examples: {'rewards_train/chosen': '0.067261', 'rewards_train/rejected': '0.051031', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.01623', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-124.26', 'loss/train': '0.69043', 'examples_per_second': '30.347', 'grad_norm': '21.5', 'counters/examples': 88800, 'counters/updates': 2775}
train stats after 88832 examples: {'rewards_train/chosen': '0.092547', 'rewards_train/rejected': '0.0087837', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083763', 'logps_train/rejected': '-89.905', 'logps_train/chosen': '-139.19', 'loss/train': '0.6615', 'examples_per_second': '32.754', 'grad_norm': '19', 'counters/examples': 88832, 'counters/updates': 2776}
train stats after 88864 examples: {'rewards_train/chosen': '0.14115', 'rewards_train/rejected': '0.056565', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084585', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-143.51', 'loss/train': '0.65938', 'examples_per_second': '31.574', 'grad_norm': '22.125', 'counters/examples': 88864, 'counters/updates': 2777}
train stats after 88896 examples: {'rewards_train/chosen': '0.12895', 'rewards_train/rejected': '0.010191', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11876', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-128.7', 'loss/train': '0.64811', 'examples_per_second': '31.204', 'grad_norm': '19.625', 'counters/examples': 88896, 'counters/updates': 2778}
train stats after 88928 examples: {'rewards_train/chosen': '0.12632', 'rewards_train/rejected': '0.084174', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.042144', 'logps_train/rejected': '-143.34', 'logps_train/chosen': '-145.46', 'loss/train': '0.67961', 'examples_per_second': '30.606', 'grad_norm': '21.125', 'counters/examples': 88928, 'counters/updates': 2779}
skipping logging after 88960 examples to avoid logging too frequently
train stats after 88992 examples: {'rewards_train/chosen': '0.11067', 'rewards_train/rejected': '0.0073787', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1033', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-130.21', 'loss/train': '0.64961', 'examples_per_second': '31.254', 'grad_norm': '20', 'counters/examples': 88992, 'counters/updates': 2781}
train stats after 89024 examples: {'rewards_train/chosen': '0.11032', 'rewards_train/rejected': '0.062453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047871', 'logps_train/rejected': '-114.76', 'logps_train/chosen': '-120.24', 'loss/train': '0.67666', 'examples_per_second': '32.855', 'grad_norm': '18.375', 'counters/examples': 89024, 'counters/updates': 2782}
train stats after 89056 examples: {'rewards_train/chosen': '0.085071', 'rewards_train/rejected': '-0.001129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0862', 'logps_train/rejected': '-110.63', 'logps_train/chosen': '-127.58', 'loss/train': '0.66634', 'examples_per_second': '31.361', 'grad_norm': '18.75', 'counters/examples': 89056, 'counters/updates': 2783}
train stats after 89088 examples: {'rewards_train/chosen': '0.20898', 'rewards_train/rejected': '0.031357', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17763', 'logps_train/rejected': '-125.03', 'logps_train/chosen': '-158.32', 'loss/train': '0.61565', 'examples_per_second': '31.973', 'grad_norm': '19.75', 'counters/examples': 89088, 'counters/updates': 2784}
train stats after 89120 examples: {'rewards_train/chosen': '0.068656', 'rewards_train/rejected': '0.048175', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020481', 'logps_train/rejected': '-103.38', 'logps_train/chosen': '-110.58', 'loss/train': '0.68951', 'examples_per_second': '32.012', 'grad_norm': '18.375', 'counters/examples': 89120, 'counters/updates': 2785}
train stats after 89152 examples: {'rewards_train/chosen': '0.12878', 'rewards_train/rejected': '0.09013', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038655', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-163.76', 'loss/train': '0.68015', 'examples_per_second': '30.457', 'grad_norm': '21.5', 'counters/examples': 89152, 'counters/updates': 2786}
train stats after 89184 examples: {'rewards_train/chosen': '0.1833', 'rewards_train/rejected': '0.073735', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10957', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-185.68', 'loss/train': '0.64965', 'examples_per_second': '30.075', 'grad_norm': '21.875', 'counters/examples': 89184, 'counters/updates': 2787}
train stats after 89216 examples: {'rewards_train/chosen': '0.12825', 'rewards_train/rejected': '0.043317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084932', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-148.38', 'loss/train': '0.65794', 'examples_per_second': '31.525', 'grad_norm': '19.375', 'counters/examples': 89216, 'counters/updates': 2788}
train stats after 89248 examples: {'rewards_train/chosen': '0.078627', 'rewards_train/rejected': '0.042022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036604', 'logps_train/rejected': '-79.845', 'logps_train/chosen': '-103.65', 'loss/train': '0.67802', 'examples_per_second': '33.123', 'grad_norm': '16.375', 'counters/examples': 89248, 'counters/updates': 2789}
train stats after 89280 examples: {'rewards_train/chosen': '0.18527', 'rewards_train/rejected': '0.07489', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11038', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-137.42', 'loss/train': '0.64583', 'examples_per_second': '31.414', 'grad_norm': '19.25', 'counters/examples': 89280, 'counters/updates': 2790}
train stats after 89312 examples: {'rewards_train/chosen': '0.079618', 'rewards_train/rejected': '0.090214', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010597', 'logps_train/rejected': '-130.11', 'logps_train/chosen': '-128.69', 'loss/train': '0.70185', 'examples_per_second': '30.889', 'grad_norm': '21', 'counters/examples': 89312, 'counters/updates': 2791}
train stats after 89344 examples: {'rewards_train/chosen': '0.082989', 'rewards_train/rejected': '0.061352', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.021637', 'logps_train/rejected': '-133.59', 'logps_train/chosen': '-175.63', 'loss/train': '0.68945', 'examples_per_second': '31.449', 'grad_norm': '24.25', 'counters/examples': 89344, 'counters/updates': 2792}
train stats after 89376 examples: {'rewards_train/chosen': '0.14247', 'rewards_train/rejected': '0.044464', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098007', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-180.38', 'loss/train': '0.65141', 'examples_per_second': '31.608', 'grad_norm': '21.375', 'counters/examples': 89376, 'counters/updates': 2793}
train stats after 89408 examples: {'rewards_train/chosen': '0.099069', 'rewards_train/rejected': '0.054805', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044264', 'logps_train/rejected': '-116.4', 'logps_train/chosen': '-123.49', 'loss/train': '0.67647', 'examples_per_second': '32.046', 'grad_norm': '19.75', 'counters/examples': 89408, 'counters/updates': 2794}
train stats after 89440 examples: {'rewards_train/chosen': '0.15239', 'rewards_train/rejected': '0.054909', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097479', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-144.05', 'loss/train': '0.65506', 'examples_per_second': '32.643', 'grad_norm': '21.375', 'counters/examples': 89440, 'counters/updates': 2795}
train stats after 89472 examples: {'rewards_train/chosen': '0.10586', 'rewards_train/rejected': '0.088111', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017748', 'logps_train/rejected': '-107.32', 'logps_train/chosen': '-130.81', 'loss/train': '0.69285', 'examples_per_second': '32.536', 'grad_norm': '21.875', 'counters/examples': 89472, 'counters/updates': 2796}
train stats after 89504 examples: {'rewards_train/chosen': '0.19161', 'rewards_train/rejected': '0.076324', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11529', 'logps_train/rejected': '-143.11', 'logps_train/chosen': '-151.62', 'loss/train': '0.644', 'examples_per_second': '31.473', 'grad_norm': '20.5', 'counters/examples': 89504, 'counters/updates': 2797}
train stats after 89536 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '0.093207', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06177', 'logps_train/rejected': '-116.57', 'logps_train/chosen': '-147.24', 'loss/train': '0.67383', 'examples_per_second': '31.401', 'grad_norm': '21', 'counters/examples': 89536, 'counters/updates': 2798}
train stats after 89568 examples: {'rewards_train/chosen': '0.13357', 'rewards_train/rejected': '0.062211', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071363', 'logps_train/rejected': '-84.22', 'logps_train/chosen': '-133.04', 'loss/train': '0.66476', 'examples_per_second': '30.794', 'grad_norm': '18.75', 'counters/examples': 89568, 'counters/updates': 2799}
train stats after 89600 examples: {'rewards_train/chosen': '0.059845', 'rewards_train/rejected': '0.037814', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022031', 'logps_train/rejected': '-147.32', 'logps_train/chosen': '-120.59', 'loss/train': '0.68782', 'examples_per_second': '31.041', 'grad_norm': '21.75', 'counters/examples': 89600, 'counters/updates': 2800}
skipping logging after 89632 examples to avoid logging too frequently
train stats after 89664 examples: {'rewards_train/chosen': '0.12274', 'rewards_train/rejected': '0.093011', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029727', 'logps_train/rejected': '-124.68', 'logps_train/chosen': '-142.36', 'loss/train': '0.68401', 'examples_per_second': '32.315', 'grad_norm': '20.25', 'counters/examples': 89664, 'counters/updates': 2802}
skipping logging after 89696 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '0.15485', 'rewards_train/rejected': '0.056861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097985', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-156.97', 'loss/train': '0.65345', 'examples_per_second': '31.583', 'grad_norm': '21.625', 'counters/examples': 89728, 'counters/updates': 2804}
skipping logging after 89760 examples to avoid logging too frequently
train stats after 89792 examples: {'rewards_train/chosen': '0.15353', 'rewards_train/rejected': '0.094492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059033', 'logps_train/rejected': '-163.01', 'logps_train/chosen': '-118.58', 'loss/train': '0.67276', 'examples_per_second': '31.624', 'grad_norm': '21.375', 'counters/examples': 89792, 'counters/updates': 2806}
train stats after 89824 examples: {'rewards_train/chosen': '0.13729', 'rewards_train/rejected': '0.063646', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07364', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-123.03', 'loss/train': '0.6639', 'examples_per_second': '30.144', 'grad_norm': '20.375', 'counters/examples': 89824, 'counters/updates': 2807}
train stats after 89856 examples: {'rewards_train/chosen': '0.085147', 'rewards_train/rejected': '-0.038798', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12395', 'logps_train/rejected': '-103.71', 'logps_train/chosen': '-149.34', 'loss/train': '0.6408', 'examples_per_second': '31.68', 'grad_norm': '20.25', 'counters/examples': 89856, 'counters/updates': 2808}
train stats after 89888 examples: {'rewards_train/chosen': '0.12626', 'rewards_train/rejected': '0.039643', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086619', 'logps_train/rejected': '-142.16', 'logps_train/chosen': '-123.37', 'loss/train': '0.6613', 'examples_per_second': '30.698', 'grad_norm': '22.625', 'counters/examples': 89888, 'counters/updates': 2809}
train stats after 89920 examples: {'rewards_train/chosen': '0.14943', 'rewards_train/rejected': '0.06618', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083245', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-142.87', 'loss/train': '0.6587', 'examples_per_second': '30.417', 'grad_norm': '20', 'counters/examples': 89920, 'counters/updates': 2810}
train stats after 89952 examples: {'rewards_train/chosen': '0.13924', 'rewards_train/rejected': '0.031313', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10793', 'logps_train/rejected': '-124.59', 'logps_train/chosen': '-159.33', 'loss/train': '0.65311', 'examples_per_second': '31.636', 'grad_norm': '21.25', 'counters/examples': 89952, 'counters/updates': 2811}
train stats after 89984 examples: {'rewards_train/chosen': '0.12053', 'rewards_train/rejected': '0.058349', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062183', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-174.6', 'loss/train': '0.6697', 'examples_per_second': '32.131', 'grad_norm': '22.125', 'counters/examples': 89984, 'counters/updates': 2812}
train stats after 90016 examples: {'rewards_train/chosen': '0.11376', 'rewards_train/rejected': '0.026445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087316', 'logps_train/rejected': '-114.95', 'logps_train/chosen': '-143.03', 'loss/train': '0.6565', 'examples_per_second': '31.373', 'grad_norm': '19.25', 'counters/examples': 90016, 'counters/updates': 2813}
train stats after 90048 examples: {'rewards_train/chosen': '0.093745', 'rewards_train/rejected': '0.06232', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031424', 'logps_train/rejected': '-154.09', 'logps_train/chosen': '-131.16', 'loss/train': '0.68465', 'examples_per_second': '30.666', 'grad_norm': '22.75', 'counters/examples': 90048, 'counters/updates': 2814}
train stats after 90080 examples: {'rewards_train/chosen': '0.12795', 'rewards_train/rejected': '0.095308', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032642', 'logps_train/rejected': '-150.68', 'logps_train/chosen': '-152.03', 'loss/train': '0.68347', 'examples_per_second': '33.052', 'grad_norm': '21.375', 'counters/examples': 90080, 'counters/updates': 2815}
train stats after 90112 examples: {'rewards_train/chosen': '0.10834', 'rewards_train/rejected': '-0.040796', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14913', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-147.71', 'loss/train': '0.62936', 'examples_per_second': '31.513', 'grad_norm': '19.5', 'counters/examples': 90112, 'counters/updates': 2816}
train stats after 90144 examples: {'rewards_train/chosen': '0.1071', 'rewards_train/rejected': '0.047617', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059481', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-137.64', 'loss/train': '0.6741', 'examples_per_second': '33.255', 'grad_norm': '22', 'counters/examples': 90144, 'counters/updates': 2817}
train stats after 90176 examples: {'rewards_train/chosen': '0.15157', 'rewards_train/rejected': '-0.0080325', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1596', 'logps_train/rejected': '-116.81', 'logps_train/chosen': '-142.79', 'loss/train': '0.62414', 'examples_per_second': '30.823', 'grad_norm': '19.5', 'counters/examples': 90176, 'counters/updates': 2818}
train stats after 90208 examples: {'rewards_train/chosen': '0.13841', 'rewards_train/rejected': '0.066013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072394', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-147.14', 'loss/train': '0.67439', 'examples_per_second': '25.054', 'grad_norm': '20.125', 'counters/examples': 90208, 'counters/updates': 2819}
train stats after 90240 examples: {'rewards_train/chosen': '0.097155', 'rewards_train/rejected': '-0.023035', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12019', 'logps_train/rejected': '-100.41', 'logps_train/chosen': '-174.05', 'loss/train': '0.65673', 'examples_per_second': '30.65', 'grad_norm': '21.25', 'counters/examples': 90240, 'counters/updates': 2820}
train stats after 90272 examples: {'rewards_train/chosen': '0.11455', 'rewards_train/rejected': '0.090782', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023768', 'logps_train/rejected': '-148.3', 'logps_train/chosen': '-124.22', 'loss/train': '0.69073', 'examples_per_second': '31.132', 'grad_norm': '21.5', 'counters/examples': 90272, 'counters/updates': 2821}
train stats after 90304 examples: {'rewards_train/chosen': '0.08459', 'rewards_train/rejected': '0.020349', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064241', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-136.75', 'loss/train': '0.66831', 'examples_per_second': '23.968', 'grad_norm': '21.25', 'counters/examples': 90304, 'counters/updates': 2822}
train stats after 90336 examples: {'rewards_train/chosen': '0.20639', 'rewards_train/rejected': '0.040175', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16622', 'logps_train/rejected': '-125.2', 'logps_train/chosen': '-132.29', 'loss/train': '0.63308', 'examples_per_second': '31.11', 'grad_norm': '21.625', 'counters/examples': 90336, 'counters/updates': 2823}
train stats after 90368 examples: {'rewards_train/chosen': '0.12732', 'rewards_train/rejected': '0.044213', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083108', 'logps_train/rejected': '-98.278', 'logps_train/chosen': '-110.82', 'loss/train': '0.6578', 'examples_per_second': '32.009', 'grad_norm': '17.625', 'counters/examples': 90368, 'counters/updates': 2824}
train stats after 90400 examples: {'rewards_train/chosen': '0.058488', 'rewards_train/rejected': '0.027268', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03122', 'logps_train/rejected': '-102', 'logps_train/chosen': '-122.73', 'loss/train': '0.68515', 'examples_per_second': '32.646', 'grad_norm': '18.875', 'counters/examples': 90400, 'counters/updates': 2825}
skipping logging after 90432 examples to avoid logging too frequently
train stats after 90464 examples: {'rewards_train/chosen': '0.097014', 'rewards_train/rejected': '-0.017696', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11471', 'logps_train/rejected': '-102.16', 'logps_train/chosen': '-126.36', 'loss/train': '0.64212', 'examples_per_second': '30.375', 'grad_norm': '18.625', 'counters/examples': 90464, 'counters/updates': 2827}
train stats after 90496 examples: {'rewards_train/chosen': '0.15074', 'rewards_train/rejected': '0.068925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081819', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-113.15', 'loss/train': '0.66323', 'examples_per_second': '30.265', 'grad_norm': '18.375', 'counters/examples': 90496, 'counters/updates': 2828}
train stats after 90528 examples: {'rewards_train/chosen': '0.094775', 'rewards_train/rejected': '0.066171', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028604', 'logps_train/rejected': '-138.93', 'logps_train/chosen': '-144.92', 'loss/train': '0.68514', 'examples_per_second': '30.164', 'grad_norm': '24.125', 'counters/examples': 90528, 'counters/updates': 2829}
train stats after 90560 examples: {'rewards_train/chosen': '0.091498', 'rewards_train/rejected': '0.046389', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045109', 'logps_train/rejected': '-110.88', 'logps_train/chosen': '-151.88', 'loss/train': '0.67427', 'examples_per_second': '30.002', 'grad_norm': '20.375', 'counters/examples': 90560, 'counters/updates': 2830}
train stats after 90592 examples: {'rewards_train/chosen': '0.1394', 'rewards_train/rejected': '-0.0015167', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14091', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-112.03', 'loss/train': '0.63136', 'examples_per_second': '32.215', 'grad_norm': '18.875', 'counters/examples': 90592, 'counters/updates': 2831}
train stats after 90624 examples: {'rewards_train/chosen': '0.087905', 'rewards_train/rejected': '0.06629', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021615', 'logps_train/rejected': '-125.26', 'logps_train/chosen': '-106.14', 'loss/train': '0.68984', 'examples_per_second': '32.622', 'grad_norm': '19.5', 'counters/examples': 90624, 'counters/updates': 2832}
skipping logging after 90656 examples to avoid logging too frequently
train stats after 90688 examples: {'rewards_train/chosen': '0.11194', 'rewards_train/rejected': '0.059432', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052513', 'logps_train/rejected': '-109.31', 'logps_train/chosen': '-117.27', 'loss/train': '0.67252', 'examples_per_second': '32.175', 'grad_norm': '19.25', 'counters/examples': 90688, 'counters/updates': 2834}
train stats after 90720 examples: {'rewards_train/chosen': '0.17828', 'rewards_train/rejected': '0.10476', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073527', 'logps_train/rejected': '-147.87', 'logps_train/chosen': '-160.63', 'loss/train': '0.66723', 'examples_per_second': '31.26', 'grad_norm': '21.5', 'counters/examples': 90720, 'counters/updates': 2835}
train stats after 90752 examples: {'rewards_train/chosen': '0.11144', 'rewards_train/rejected': '0.013224', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098219', 'logps_train/rejected': '-71.913', 'logps_train/chosen': '-105.89', 'loss/train': '0.64925', 'examples_per_second': '32.336', 'grad_norm': '16.5', 'counters/examples': 90752, 'counters/updates': 2836}
skipping logging after 90784 examples to avoid logging too frequently
train stats after 90816 examples: {'rewards_train/chosen': '0.11344', 'rewards_train/rejected': '0.044984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068456', 'logps_train/rejected': '-162.48', 'logps_train/chosen': '-137.19', 'loss/train': '0.6655', 'examples_per_second': '31.668', 'grad_norm': '21.875', 'counters/examples': 90816, 'counters/updates': 2838}
train stats after 90848 examples: {'rewards_train/chosen': '0.10243', 'rewards_train/rejected': '-0.0034373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10586', 'logps_train/rejected': '-113.12', 'logps_train/chosen': '-152.38', 'loss/train': '0.65233', 'examples_per_second': '31.007', 'grad_norm': '20.625', 'counters/examples': 90848, 'counters/updates': 2839}
train stats after 90880 examples: {'rewards_train/chosen': '0.14743', 'rewards_train/rejected': '0.146', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0014309', 'logps_train/rejected': '-180.89', 'logps_train/chosen': '-169.83', 'loss/train': '0.70355', 'examples_per_second': '31.538', 'grad_norm': '24.125', 'counters/examples': 90880, 'counters/updates': 2840}
train stats after 90912 examples: {'rewards_train/chosen': '0.13939', 'rewards_train/rejected': '0.058293', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081098', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-162.03', 'loss/train': '0.66012', 'examples_per_second': '31.1', 'grad_norm': '21.125', 'counters/examples': 90912, 'counters/updates': 2841}
train stats after 90944 examples: {'rewards_train/chosen': '0.1201', 'rewards_train/rejected': '-0.019032', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13913', 'logps_train/rejected': '-109.8', 'logps_train/chosen': '-146.75', 'loss/train': '0.63314', 'examples_per_second': '33.165', 'grad_norm': '18.625', 'counters/examples': 90944, 'counters/updates': 2842}
train stats after 90976 examples: {'rewards_train/chosen': '0.021943', 'rewards_train/rejected': '0.069845', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.047903', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-123.61', 'loss/train': '0.72337', 'examples_per_second': '31.681', 'grad_norm': '20.625', 'counters/examples': 90976, 'counters/updates': 2843}
train stats after 91008 examples: {'rewards_train/chosen': '0.10827', 'rewards_train/rejected': '0.028182', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080087', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-123.69', 'loss/train': '0.6606', 'examples_per_second': '31.196', 'grad_norm': '20.625', 'counters/examples': 91008, 'counters/updates': 2844}
train stats after 91040 examples: {'rewards_train/chosen': '0.09878', 'rewards_train/rejected': '0.067616', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031164', 'logps_train/rejected': '-109.01', 'logps_train/chosen': '-117', 'loss/train': '0.68355', 'examples_per_second': '32.081', 'grad_norm': '18.5', 'counters/examples': 91040, 'counters/updates': 2845}
train stats after 91072 examples: {'rewards_train/chosen': '0.085169', 'rewards_train/rejected': '0.024472', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060697', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-140.87', 'loss/train': '0.67138', 'examples_per_second': '32.106', 'grad_norm': '20.625', 'counters/examples': 91072, 'counters/updates': 2846}
train stats after 91104 examples: {'rewards_train/chosen': '0.13857', 'rewards_train/rejected': '0.027155', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11141', 'logps_train/rejected': '-112.93', 'logps_train/chosen': '-137.22', 'loss/train': '0.64607', 'examples_per_second': '32.57', 'grad_norm': '18.875', 'counters/examples': 91104, 'counters/updates': 2847}
train stats after 91136 examples: {'rewards_train/chosen': '0.036695', 'rewards_train/rejected': '0.036317', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00037759', 'logps_train/rejected': '-93.317', 'logps_train/chosen': '-135.69', 'loss/train': '0.69713', 'examples_per_second': '31.717', 'grad_norm': '20.875', 'counters/examples': 91136, 'counters/updates': 2848}
train stats after 91168 examples: {'rewards_train/chosen': '0.14236', 'rewards_train/rejected': '0.10567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036694', 'logps_train/rejected': '-111.57', 'logps_train/chosen': '-126.82', 'loss/train': '0.67895', 'examples_per_second': '32.02', 'grad_norm': '19.75', 'counters/examples': 91168, 'counters/updates': 2849}
skipping logging after 91200 examples to avoid logging too frequently
train stats after 91232 examples: {'rewards_train/chosen': '0.13481', 'rewards_train/rejected': '0.032211', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1026', 'logps_train/rejected': '-139.77', 'logps_train/chosen': '-143.47', 'loss/train': '0.65442', 'examples_per_second': '32.593', 'grad_norm': '20.625', 'counters/examples': 91232, 'counters/updates': 2851}
train stats after 91264 examples: {'rewards_train/chosen': '0.13208', 'rewards_train/rejected': '-0.0097783', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14186', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-127.01', 'loss/train': '0.63185', 'examples_per_second': '31.113', 'grad_norm': '18.75', 'counters/examples': 91264, 'counters/updates': 2852}
train stats after 91296 examples: {'rewards_train/chosen': '0.058967', 'rewards_train/rejected': '0.044319', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014648', 'logps_train/rejected': '-132.14', 'logps_train/chosen': '-125.6', 'loss/train': '0.69416', 'examples_per_second': '30.039', 'grad_norm': '20.875', 'counters/examples': 91296, 'counters/updates': 2853}
train stats after 91328 examples: {'rewards_train/chosen': '0.1516', 'rewards_train/rejected': '0.1501', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.0015005', 'logps_train/rejected': '-99.543', 'logps_train/chosen': '-131.87', 'loss/train': '0.70264', 'examples_per_second': '31.759', 'grad_norm': '19.375', 'counters/examples': 91328, 'counters/updates': 2854}
train stats after 91360 examples: {'rewards_train/chosen': '0.10361', 'rewards_train/rejected': '0.069293', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034319', 'logps_train/rejected': '-130', 'logps_train/chosen': '-130.28', 'loss/train': '0.67988', 'examples_per_second': '32.073', 'grad_norm': '20', 'counters/examples': 91360, 'counters/updates': 2855}
train stats after 91392 examples: {'rewards_train/chosen': '0.1152', 'rewards_train/rejected': '0.042516', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072681', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-137.14', 'loss/train': '0.66292', 'examples_per_second': '30.698', 'grad_norm': '18.5', 'counters/examples': 91392, 'counters/updates': 2856}
skipping logging after 91424 examples to avoid logging too frequently
train stats after 91456 examples: {'rewards_train/chosen': '0.06791', 'rewards_train/rejected': '0.022267', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045643', 'logps_train/rejected': '-104.38', 'logps_train/chosen': '-154.5', 'loss/train': '0.67808', 'examples_per_second': '31.668', 'grad_norm': '20', 'counters/examples': 91456, 'counters/updates': 2858}
train stats after 91488 examples: {'rewards_train/chosen': '0.074165', 'rewards_train/rejected': '-0.0072304', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081396', 'logps_train/rejected': '-132.65', 'logps_train/chosen': '-112.07', 'loss/train': '0.66138', 'examples_per_second': '32.48', 'grad_norm': '20', 'counters/examples': 91488, 'counters/updates': 2859}
skipping logging after 91520 examples to avoid logging too frequently
train stats after 91552 examples: {'rewards_train/chosen': '0.078551', 'rewards_train/rejected': '0.054018', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024532', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-133.17', 'loss/train': '0.68911', 'examples_per_second': '31.111', 'grad_norm': '21.25', 'counters/examples': 91552, 'counters/updates': 2861}
train stats after 91584 examples: {'rewards_train/chosen': '0.10121', 'rewards_train/rejected': '0.069851', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031358', 'logps_train/rejected': '-152.73', 'logps_train/chosen': '-143.08', 'loss/train': '0.68438', 'examples_per_second': '31.244', 'grad_norm': '21.5', 'counters/examples': 91584, 'counters/updates': 2862}
train stats after 91616 examples: {'rewards_train/chosen': '0.097308', 'rewards_train/rejected': '0.010562', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086745', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-111.84', 'loss/train': '0.65747', 'examples_per_second': '32.558', 'grad_norm': '19.25', 'counters/examples': 91616, 'counters/updates': 2863}
train stats after 91648 examples: {'rewards_train/chosen': '0.098931', 'rewards_train/rejected': '0.075183', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023747', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-127.07', 'loss/train': '0.68933', 'examples_per_second': '31.014', 'grad_norm': '20.5', 'counters/examples': 91648, 'counters/updates': 2864}
skipping logging after 91680 examples to avoid logging too frequently
train stats after 91712 examples: {'rewards_train/chosen': '0.13919', 'rewards_train/rejected': '0.023793', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1154', 'logps_train/rejected': '-99.5', 'logps_train/chosen': '-135.65', 'loss/train': '0.64231', 'examples_per_second': '31.65', 'grad_norm': '18.5', 'counters/examples': 91712, 'counters/updates': 2866}
train stats after 91744 examples: {'rewards_train/chosen': '0.090733', 'rewards_train/rejected': '0.03406', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056673', 'logps_train/rejected': '-110.29', 'logps_train/chosen': '-125.54', 'loss/train': '0.67214', 'examples_per_second': '31.228', 'grad_norm': '19.5', 'counters/examples': 91744, 'counters/updates': 2867}
train stats after 91776 examples: {'rewards_train/chosen': '0.13134', 'rewards_train/rejected': '0.065172', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066165', 'logps_train/rejected': '-87.492', 'logps_train/chosen': '-133.33', 'loss/train': '0.66863', 'examples_per_second': '30.967', 'grad_norm': '19.875', 'counters/examples': 91776, 'counters/updates': 2868}
train stats after 91808 examples: {'rewards_train/chosen': '0.15717', 'rewards_train/rejected': '0.054961', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10221', 'logps_train/rejected': '-99.922', 'logps_train/chosen': '-144.69', 'loss/train': '0.65043', 'examples_per_second': '32.002', 'grad_norm': '19.25', 'counters/examples': 91808, 'counters/updates': 2869}
train stats after 91840 examples: {'rewards_train/chosen': '0.12647', 'rewards_train/rejected': '0.070676', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055791', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-156.62', 'loss/train': '0.6746', 'examples_per_second': '33.111', 'grad_norm': '21.625', 'counters/examples': 91840, 'counters/updates': 2870}
train stats after 91872 examples: {'rewards_train/chosen': '0.10784', 'rewards_train/rejected': '0.049399', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058436', 'logps_train/rejected': '-143.37', 'logps_train/chosen': '-114.77', 'loss/train': '0.67066', 'examples_per_second': '31.308', 'grad_norm': '21.125', 'counters/examples': 91872, 'counters/updates': 2871}
train stats after 91904 examples: {'rewards_train/chosen': '0.11767', 'rewards_train/rejected': '0.07988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037786', 'logps_train/rejected': '-119', 'logps_train/chosen': '-153.19', 'loss/train': '0.68802', 'examples_per_second': '32.459', 'grad_norm': '21.5', 'counters/examples': 91904, 'counters/updates': 2872}
train stats after 91936 examples: {'rewards_train/chosen': '0.11338', 'rewards_train/rejected': '0.0008587', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11253', 'logps_train/rejected': '-83.113', 'logps_train/chosen': '-113.37', 'loss/train': '0.64794', 'examples_per_second': '31.164', 'grad_norm': '17.5', 'counters/examples': 91936, 'counters/updates': 2873}
train stats after 91968 examples: {'rewards_train/chosen': '0.17602', 'rewards_train/rejected': '0.11039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065626', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-159.22', 'loss/train': '0.66692', 'examples_per_second': '31.606', 'grad_norm': '21.625', 'counters/examples': 91968, 'counters/updates': 2874}
train stats after 92000 examples: {'rewards_train/chosen': '0.11357', 'rewards_train/rejected': '0.10152', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012049', 'logps_train/rejected': '-154.29', 'logps_train/chosen': '-143', 'loss/train': '0.69555', 'examples_per_second': '30.312', 'grad_norm': '22', 'counters/examples': 92000, 'counters/updates': 2875}
train stats after 92032 examples: {'rewards_train/chosen': '0.094936', 'rewards_train/rejected': '-0.011929', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10687', 'logps_train/rejected': '-104.87', 'logps_train/chosen': '-136.06', 'loss/train': '0.65036', 'examples_per_second': '26.898', 'grad_norm': '18.875', 'counters/examples': 92032, 'counters/updates': 2876}
train stats after 92064 examples: {'rewards_train/chosen': '0.061271', 'rewards_train/rejected': '0.03213', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02914', 'logps_train/rejected': '-106.18', 'logps_train/chosen': '-141.35', 'loss/train': '0.6831', 'examples_per_second': '32.11', 'grad_norm': '20', 'counters/examples': 92064, 'counters/updates': 2877}
train stats after 92096 examples: {'rewards_train/chosen': '0.14735', 'rewards_train/rejected': '0.092195', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055154', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-134.06', 'loss/train': '0.67087', 'examples_per_second': '32.134', 'grad_norm': '21.125', 'counters/examples': 92096, 'counters/updates': 2878}
train stats after 92128 examples: {'rewards_train/chosen': '0.13212', 'rewards_train/rejected': '0.072161', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059959', 'logps_train/rejected': '-107.44', 'logps_train/chosen': '-145.21', 'loss/train': '0.6741', 'examples_per_second': '31.68', 'grad_norm': '19.625', 'counters/examples': 92128, 'counters/updates': 2879}
skipping logging after 92160 examples to avoid logging too frequently
train stats after 92192 examples: {'rewards_train/chosen': '0.12677', 'rewards_train/rejected': '0.07066', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.056107', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-156.97', 'loss/train': '0.67592', 'examples_per_second': '31.077', 'grad_norm': '20.875', 'counters/examples': 92192, 'counters/updates': 2881}
train stats after 92224 examples: {'rewards_train/chosen': '0.18261', 'rewards_train/rejected': '0.12116', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061447', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-143.76', 'loss/train': '0.67105', 'examples_per_second': '31.651', 'grad_norm': '20.375', 'counters/examples': 92224, 'counters/updates': 2882}
skipping logging after 92256 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '0.13861', 'rewards_train/rejected': '0.028203', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11041', 'logps_train/rejected': '-98.426', 'logps_train/chosen': '-139.87', 'loss/train': '0.64507', 'examples_per_second': '31.641', 'grad_norm': '18.625', 'counters/examples': 92288, 'counters/updates': 2884}
train stats after 92320 examples: {'rewards_train/chosen': '0.14468', 'rewards_train/rejected': '0.030849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11383', 'logps_train/rejected': '-106.92', 'logps_train/chosen': '-174.15', 'loss/train': '0.64995', 'examples_per_second': '32.326', 'grad_norm': '21.125', 'counters/examples': 92320, 'counters/updates': 2885}
train stats after 92352 examples: {'rewards_train/chosen': '0.078904', 'rewards_train/rejected': '0.05921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019695', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-176.01', 'loss/train': '0.69081', 'examples_per_second': '30.688', 'grad_norm': '22.125', 'counters/examples': 92352, 'counters/updates': 2886}
train stats after 92384 examples: {'rewards_train/chosen': '0.11461', 'rewards_train/rejected': '0.05688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057733', 'logps_train/rejected': '-144.18', 'logps_train/chosen': '-121.77', 'loss/train': '0.67607', 'examples_per_second': '31.603', 'grad_norm': '20.5', 'counters/examples': 92384, 'counters/updates': 2887}
skipping logging after 92416 examples to avoid logging too frequently
train stats after 92448 examples: {'rewards_train/chosen': '0.13441', 'rewards_train/rejected': '0.047426', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086983', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-138.9', 'loss/train': '0.6568', 'examples_per_second': '31.689', 'grad_norm': '20.375', 'counters/examples': 92448, 'counters/updates': 2889}
train stats after 92480 examples: {'rewards_train/chosen': '0.088644', 'rewards_train/rejected': '0.037837', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.050806', 'logps_train/rejected': '-113.61', 'logps_train/chosen': '-150.25', 'loss/train': '0.67437', 'examples_per_second': '31.158', 'grad_norm': '22.125', 'counters/examples': 92480, 'counters/updates': 2890}
skipping logging after 92512 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '0.083886', 'rewards_train/rejected': '0.035962', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047925', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-140.39', 'loss/train': '0.67673', 'examples_per_second': '32.094', 'grad_norm': '20.5', 'counters/examples': 92544, 'counters/updates': 2892}
train stats after 92576 examples: {'rewards_train/chosen': '0.10351', 'rewards_train/rejected': '0.02628', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077235', 'logps_train/rejected': '-81.99', 'logps_train/chosen': '-127.24', 'loss/train': '0.66505', 'examples_per_second': '31.311', 'grad_norm': '17.75', 'counters/examples': 92576, 'counters/updates': 2893}
train stats after 92608 examples: {'rewards_train/chosen': '0.11445', 'rewards_train/rejected': '0.071557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042888', 'logps_train/rejected': '-113.95', 'logps_train/chosen': '-162.88', 'loss/train': '0.67617', 'examples_per_second': '30.194', 'grad_norm': '21', 'counters/examples': 92608, 'counters/updates': 2894}
train stats after 92640 examples: {'rewards_train/chosen': '0.13562', 'rewards_train/rejected': '0.018616', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11701', 'logps_train/rejected': '-105.51', 'logps_train/chosen': '-138.86', 'loss/train': '0.6434', 'examples_per_second': '30.126', 'grad_norm': '20.125', 'counters/examples': 92640, 'counters/updates': 2895}
skipping logging after 92672 examples to avoid logging too frequently
train stats after 92704 examples: {'rewards_train/chosen': '0.1289', 'rewards_train/rejected': '0.0040657', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12483', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-144.89', 'loss/train': '0.64045', 'examples_per_second': '31.931', 'grad_norm': '19.625', 'counters/examples': 92704, 'counters/updates': 2897}
train stats after 92736 examples: {'rewards_train/chosen': '0.10162', 'rewards_train/rejected': '0.066136', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035487', 'logps_train/rejected': '-97.206', 'logps_train/chosen': '-117.72', 'loss/train': '0.68445', 'examples_per_second': '33.078', 'grad_norm': '20.125', 'counters/examples': 92736, 'counters/updates': 2898}
train stats after 92768 examples: {'rewards_train/chosen': '0.086221', 'rewards_train/rejected': '0.048646', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037575', 'logps_train/rejected': '-107.01', 'logps_train/chosen': '-106.83', 'loss/train': '0.68285', 'examples_per_second': '31.296', 'grad_norm': '18.875', 'counters/examples': 92768, 'counters/updates': 2899}
skipping logging after 92800 examples to avoid logging too frequently
train stats after 92832 examples: {'rewards_train/chosen': '0.14222', 'rewards_train/rejected': '0.037944', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10427', 'logps_train/rejected': '-146.29', 'logps_train/chosen': '-163.1', 'loss/train': '0.65365', 'examples_per_second': '33.983', 'grad_norm': '22.75', 'counters/examples': 92832, 'counters/updates': 2901}
train stats after 92864 examples: {'rewards_train/chosen': '0.18544', 'rewards_train/rejected': '0.057723', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12772', 'logps_train/rejected': '-95.754', 'logps_train/chosen': '-148.13', 'loss/train': '0.63846', 'examples_per_second': '30.292', 'grad_norm': '19.5', 'counters/examples': 92864, 'counters/updates': 2902}
train stats after 92896 examples: {'rewards_train/chosen': '0.10926', 'rewards_train/rejected': '0.13202', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02276', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-97.151', 'loss/train': '0.70977', 'examples_per_second': '31.616', 'grad_norm': '20.5', 'counters/examples': 92896, 'counters/updates': 2903}
train stats after 92928 examples: {'rewards_train/chosen': '0.1048', 'rewards_train/rejected': '0.012759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092037', 'logps_train/rejected': '-96.459', 'logps_train/chosen': '-129.4', 'loss/train': '0.65958', 'examples_per_second': '31.076', 'grad_norm': '19.125', 'counters/examples': 92928, 'counters/updates': 2904}
train stats after 92960 examples: {'rewards_train/chosen': '0.11522', 'rewards_train/rejected': '0.039435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075784', 'logps_train/rejected': '-79.338', 'logps_train/chosen': '-144.08', 'loss/train': '0.6619', 'examples_per_second': '31.152', 'grad_norm': '19.375', 'counters/examples': 92960, 'counters/updates': 2905}
skipping logging after 92992 examples to avoid logging too frequently
train stats after 93024 examples: {'rewards_train/chosen': '0.11807', 'rewards_train/rejected': '0.044611', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073456', 'logps_train/rejected': '-136.89', 'logps_train/chosen': '-163.32', 'loss/train': '0.66683', 'examples_per_second': '31.026', 'grad_norm': '20.875', 'counters/examples': 93024, 'counters/updates': 2907}
skipping logging after 93056 examples to avoid logging too frequently
train stats after 93088 examples: {'rewards_train/chosen': '0.098058', 'rewards_train/rejected': '0.038675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059383', 'logps_train/rejected': '-164.35', 'logps_train/chosen': '-145.14', 'loss/train': '0.67702', 'examples_per_second': '33.593', 'grad_norm': '22.375', 'counters/examples': 93088, 'counters/updates': 2909}
train stats after 93120 examples: {'rewards_train/chosen': '0.18107', 'rewards_train/rejected': '0.12083', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.060245', 'logps_train/rejected': '-139.06', 'logps_train/chosen': '-145.95', 'loss/train': '0.67013', 'examples_per_second': '30.359', 'grad_norm': '21.625', 'counters/examples': 93120, 'counters/updates': 2910}
train stats after 93152 examples: {'rewards_train/chosen': '0.12691', 'rewards_train/rejected': '0.033563', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093347', 'logps_train/rejected': '-73.169', 'logps_train/chosen': '-115.53', 'loss/train': '0.6535', 'examples_per_second': '31.657', 'grad_norm': '17.625', 'counters/examples': 93152, 'counters/updates': 2911}
skipping logging after 93184 examples to avoid logging too frequently
train stats after 93216 examples: {'rewards_train/chosen': '0.17424', 'rewards_train/rejected': '0.086152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088084', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-165.91', 'loss/train': '0.65512', 'examples_per_second': '33.347', 'grad_norm': '21.875', 'counters/examples': 93216, 'counters/updates': 2913}
train stats after 93248 examples: {'rewards_train/chosen': '0.1525', 'rewards_train/rejected': '0.060641', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091859', 'logps_train/rejected': '-132.69', 'logps_train/chosen': '-117.35', 'loss/train': '0.65724', 'examples_per_second': '31.415', 'grad_norm': '19.875', 'counters/examples': 93248, 'counters/updates': 2914}
train stats after 93280 examples: {'rewards_train/chosen': '0.13754', 'rewards_train/rejected': '0.13458', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0029571', 'logps_train/rejected': '-114.34', 'logps_train/chosen': '-161.04', 'loss/train': '0.70109', 'examples_per_second': '31.811', 'grad_norm': '20.625', 'counters/examples': 93280, 'counters/updates': 2915}
train stats after 93312 examples: {'rewards_train/chosen': '0.12648', 'rewards_train/rejected': '0.043948', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082536', 'logps_train/rejected': '-109.79', 'logps_train/chosen': '-97.204', 'loss/train': '0.65906', 'examples_per_second': '31.461', 'grad_norm': '17.625', 'counters/examples': 93312, 'counters/updates': 2916}
train stats after 93344 examples: {'rewards_train/chosen': '0.15062', 'rewards_train/rejected': '0.084472', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06615', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-150.68', 'loss/train': '0.67296', 'examples_per_second': '31.627', 'grad_norm': '22.25', 'counters/examples': 93344, 'counters/updates': 2917}
train stats after 93376 examples: {'rewards_train/chosen': '0.12424', 'rewards_train/rejected': '0.13258', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.008335', 'logps_train/rejected': '-158.52', 'logps_train/chosen': '-144.79', 'loss/train': '0.7048', 'examples_per_second': '31.395', 'grad_norm': '23.125', 'counters/examples': 93376, 'counters/updates': 2918}
train stats after 93408 examples: {'rewards_train/chosen': '0.18118', 'rewards_train/rejected': '0.059189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12199', 'logps_train/rejected': '-144.92', 'logps_train/chosen': '-165.22', 'loss/train': '0.64325', 'examples_per_second': '31.355', 'grad_norm': '22.625', 'counters/examples': 93408, 'counters/updates': 2919}
train stats after 93440 examples: {'rewards_train/chosen': '0.14496', 'rewards_train/rejected': '0.034946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11001', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-149.66', 'loss/train': '0.64834', 'examples_per_second': '31.678', 'grad_norm': '19.75', 'counters/examples': 93440, 'counters/updates': 2920}
train stats after 93472 examples: {'rewards_train/chosen': '0.13204', 'rewards_train/rejected': '0.040527', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091513', 'logps_train/rejected': '-105', 'logps_train/chosen': '-129.55', 'loss/train': '0.65577', 'examples_per_second': '31.002', 'grad_norm': '18', 'counters/examples': 93472, 'counters/updates': 2921}
train stats after 93504 examples: {'rewards_train/chosen': '0.13677', 'rewards_train/rejected': '0.03793', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098845', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-146.24', 'loss/train': '0.65176', 'examples_per_second': '31.641', 'grad_norm': '21', 'counters/examples': 93504, 'counters/updates': 2922}
train stats after 93536 examples: {'rewards_train/chosen': '0.07175', 'rewards_train/rejected': '0.053436', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018315', 'logps_train/rejected': '-113.75', 'logps_train/chosen': '-171.78', 'loss/train': '0.69377', 'examples_per_second': '31.66', 'grad_norm': '23.5', 'counters/examples': 93536, 'counters/updates': 2923}
train stats after 93568 examples: {'rewards_train/chosen': '0.025307', 'rewards_train/rejected': '0.022034', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0032731', 'logps_train/rejected': '-171.45', 'logps_train/chosen': '-170.62', 'loss/train': '0.69905', 'examples_per_second': '31.604', 'grad_norm': '25.125', 'counters/examples': 93568, 'counters/updates': 2924}
train stats after 93600 examples: {'rewards_train/chosen': '0.13119', 'rewards_train/rejected': '0.074158', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057028', 'logps_train/rejected': '-128.31', 'logps_train/chosen': '-149.15', 'loss/train': '0.67505', 'examples_per_second': '29.947', 'grad_norm': '21.625', 'counters/examples': 93600, 'counters/updates': 2925}
train stats after 93632 examples: {'rewards_train/chosen': '0.097314', 'rewards_train/rejected': '0.019203', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078111', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-147.56', 'loss/train': '0.66147', 'examples_per_second': '31.191', 'grad_norm': '21.875', 'counters/examples': 93632, 'counters/updates': 2926}
train stats after 93664 examples: {'rewards_train/chosen': '0.17768', 'rewards_train/rejected': '0.097943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079738', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-154.44', 'loss/train': '0.66008', 'examples_per_second': '30.044', 'grad_norm': '20.625', 'counters/examples': 93664, 'counters/updates': 2927}
train stats after 93696 examples: {'rewards_train/chosen': '0.18613', 'rewards_train/rejected': '0.11683', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069308', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-163.1', 'loss/train': '0.6666', 'examples_per_second': '31.621', 'grad_norm': '20.625', 'counters/examples': 93696, 'counters/updates': 2928}
train stats after 93728 examples: {'rewards_train/chosen': '0.10131', 'rewards_train/rejected': '0.091868', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0094459', 'logps_train/rejected': '-149.55', 'logps_train/chosen': '-104.84', 'loss/train': '0.69316', 'examples_per_second': '31.59', 'grad_norm': '22', 'counters/examples': 93728, 'counters/updates': 2929}
skipping logging after 93760 examples to avoid logging too frequently
train stats after 93792 examples: {'rewards_train/chosen': '0.15598', 'rewards_train/rejected': '0.085943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070034', 'logps_train/rejected': '-132.96', 'logps_train/chosen': '-137.34', 'loss/train': '0.66939', 'examples_per_second': '30.159', 'grad_norm': '21.25', 'counters/examples': 93792, 'counters/updates': 2931}
train stats after 93824 examples: {'rewards_train/chosen': '0.082791', 'rewards_train/rejected': '0.039523', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043268', 'logps_train/rejected': '-89.443', 'logps_train/chosen': '-122.46', 'loss/train': '0.68059', 'examples_per_second': '31.639', 'grad_norm': '18.625', 'counters/examples': 93824, 'counters/updates': 2932}
train stats after 93856 examples: {'rewards_train/chosen': '0.048839', 'rewards_train/rejected': '0.041399', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0074408', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-153.72', 'loss/train': '0.69823', 'examples_per_second': '31.926', 'grad_norm': '20.125', 'counters/examples': 93856, 'counters/updates': 2933}
skipping logging after 93888 examples to avoid logging too frequently
train stats after 93920 examples: {'rewards_train/chosen': '0.12005', 'rewards_train/rejected': '0.069126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050923', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-147.1', 'loss/train': '0.67501', 'examples_per_second': '29.951', 'grad_norm': '22.25', 'counters/examples': 93920, 'counters/updates': 2935}
train stats after 93952 examples: {'rewards_train/chosen': '0.12226', 'rewards_train/rejected': '0.10118', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021077', 'logps_train/rejected': '-106.87', 'logps_train/chosen': '-131.04', 'loss/train': '0.69282', 'examples_per_second': '33.134', 'grad_norm': '19', 'counters/examples': 93952, 'counters/updates': 2936}
train stats after 93984 examples: {'rewards_train/chosen': '0.091794', 'rewards_train/rejected': '0.049035', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042759', 'logps_train/rejected': '-142.46', 'logps_train/chosen': '-155.61', 'loss/train': '0.67681', 'examples_per_second': '32.166', 'grad_norm': '21.875', 'counters/examples': 93984, 'counters/updates': 2937}
train stats after 94016 examples: {'rewards_train/chosen': '0.10464', 'rewards_train/rejected': '0.12594', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.021302', 'logps_train/rejected': '-120.07', 'logps_train/chosen': '-136.48', 'loss/train': '0.71353', 'examples_per_second': '30.177', 'grad_norm': '20.75', 'counters/examples': 94016, 'counters/updates': 2938}
skipping logging after 94048 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '0.21374', 'rewards_train/rejected': '0.11253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10121', 'logps_train/rejected': '-146.4', 'logps_train/chosen': '-156.2', 'loss/train': '0.65218', 'examples_per_second': '31.499', 'grad_norm': '21.75', 'counters/examples': 94080, 'counters/updates': 2940}
skipping logging after 94112 examples to avoid logging too frequently
train stats after 94144 examples: {'rewards_train/chosen': '0.13948', 'rewards_train/rejected': '0.038108', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10137', 'logps_train/rejected': '-122.57', 'logps_train/chosen': '-170.66', 'loss/train': '0.65127', 'examples_per_second': '31.47', 'grad_norm': '21.625', 'counters/examples': 94144, 'counters/updates': 2942}
train stats after 94176 examples: {'rewards_train/chosen': '0.10253', 'rewards_train/rejected': '0.05576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046775', 'logps_train/rejected': '-134.16', 'logps_train/chosen': '-152.77', 'loss/train': '0.67746', 'examples_per_second': '31.619', 'grad_norm': '22.875', 'counters/examples': 94176, 'counters/updates': 2943}
train stats after 94208 examples: {'rewards_train/chosen': '0.12402', 'rewards_train/rejected': '0.053728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070295', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-170.61', 'loss/train': '0.66592', 'examples_per_second': '31.582', 'grad_norm': '22.875', 'counters/examples': 94208, 'counters/updates': 2944}
train stats after 94240 examples: {'rewards_train/chosen': '0.066837', 'rewards_train/rejected': '-0.00074672', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067583', 'logps_train/rejected': '-83.217', 'logps_train/chosen': '-105.74', 'loss/train': '0.66589', 'examples_per_second': '30.065', 'grad_norm': '17.75', 'counters/examples': 94240, 'counters/updates': 2945}
train stats after 94272 examples: {'rewards_train/chosen': '0.092932', 'rewards_train/rejected': '0.02268', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070252', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-153.14', 'loss/train': '0.6675', 'examples_per_second': '33.143', 'grad_norm': '22.875', 'counters/examples': 94272, 'counters/updates': 2946}
train stats after 94304 examples: {'rewards_train/chosen': '0.16086', 'rewards_train/rejected': '0.06486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095995', 'logps_train/rejected': '-98.431', 'logps_train/chosen': '-137.94', 'loss/train': '0.66083', 'examples_per_second': '32.195', 'grad_norm': '19.625', 'counters/examples': 94304, 'counters/updates': 2947}
train stats after 94336 examples: {'rewards_train/chosen': '0.029399', 'rewards_train/rejected': '0.065141', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.035742', 'logps_train/rejected': '-107.16', 'logps_train/chosen': '-114.88', 'loss/train': '0.71731', 'examples_per_second': '30.62', 'grad_norm': '19.75', 'counters/examples': 94336, 'counters/updates': 2948}
train stats after 94368 examples: {'rewards_train/chosen': '0.076965', 'rewards_train/rejected': '0.043973', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032993', 'logps_train/rejected': '-101.21', 'logps_train/chosen': '-104.93', 'loss/train': '0.68512', 'examples_per_second': '32.261', 'grad_norm': '18.25', 'counters/examples': 94368, 'counters/updates': 2949}
skipping logging after 94400 examples to avoid logging too frequently
train stats after 94432 examples: {'rewards_train/chosen': '0.13856', 'rewards_train/rejected': '-0.0031784', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14173', 'logps_train/rejected': '-109.96', 'logps_train/chosen': '-151.04', 'loss/train': '0.63348', 'examples_per_second': '31.91', 'grad_norm': '19.5', 'counters/examples': 94432, 'counters/updates': 2951}
train stats after 94464 examples: {'rewards_train/chosen': '0.19778', 'rewards_train/rejected': '0.10685', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090927', 'logps_train/rejected': '-163.42', 'logps_train/chosen': '-149.33', 'loss/train': '0.66093', 'examples_per_second': '31.613', 'grad_norm': '22.75', 'counters/examples': 94464, 'counters/updates': 2952}
train stats after 94496 examples: {'rewards_train/chosen': '0.1511', 'rewards_train/rejected': '0.11655', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034544', 'logps_train/rejected': '-146.39', 'logps_train/chosen': '-155.87', 'loss/train': '0.68215', 'examples_per_second': '31.255', 'grad_norm': '22', 'counters/examples': 94496, 'counters/updates': 2953}
train stats after 94528 examples: {'rewards_train/chosen': '0.14901', 'rewards_train/rejected': '0.10179', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047227', 'logps_train/rejected': '-99.693', 'logps_train/chosen': '-113.13', 'loss/train': '0.67752', 'examples_per_second': '30.261', 'grad_norm': '18.625', 'counters/examples': 94528, 'counters/updates': 2954}
train stats after 94560 examples: {'rewards_train/chosen': '0.12899', 'rewards_train/rejected': '0.085772', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043216', 'logps_train/rejected': '-99.474', 'logps_train/chosen': '-140.98', 'loss/train': '0.68229', 'examples_per_second': '31.341', 'grad_norm': '19.875', 'counters/examples': 94560, 'counters/updates': 2955}
train stats after 94592 examples: {'rewards_train/chosen': '0.14132', 'rewards_train/rejected': '0.10727', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034052', 'logps_train/rejected': '-138.24', 'logps_train/chosen': '-136.24', 'loss/train': '0.68716', 'examples_per_second': '31.324', 'grad_norm': '22', 'counters/examples': 94592, 'counters/updates': 2956}
train stats after 94624 examples: {'rewards_train/chosen': '0.16438', 'rewards_train/rejected': '0.082219', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08216', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-177.74', 'loss/train': '0.66104', 'examples_per_second': '30.28', 'grad_norm': '22.25', 'counters/examples': 94624, 'counters/updates': 2957}
skipping logging after 94656 examples to avoid logging too frequently
train stats after 94688 examples: {'rewards_train/chosen': '0.077725', 'rewards_train/rejected': '0.029218', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048507', 'logps_train/rejected': '-96.989', 'logps_train/chosen': '-96.686', 'loss/train': '0.67384', 'examples_per_second': '31.459', 'grad_norm': '18.25', 'counters/examples': 94688, 'counters/updates': 2959}
skipping logging after 94720 examples to avoid logging too frequently
train stats after 94752 examples: {'rewards_train/chosen': '0.20535', 'rewards_train/rejected': '0.10808', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097275', 'logps_train/rejected': '-142.44', 'logps_train/chosen': '-136.36', 'loss/train': '0.65091', 'examples_per_second': '31.144', 'grad_norm': '19.75', 'counters/examples': 94752, 'counters/updates': 2961}
train stats after 94784 examples: {'rewards_train/chosen': '0.11384', 'rewards_train/rejected': '0.02749', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.086348', 'logps_train/rejected': '-105.38', 'logps_train/chosen': '-121.6', 'loss/train': '0.6548', 'examples_per_second': '31.658', 'grad_norm': '20.375', 'counters/examples': 94784, 'counters/updates': 2962}
skipping logging after 94816 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '0.032658', 'rewards_train/rejected': '0.00097417', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031684', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-130.38', 'loss/train': '0.68119', 'examples_per_second': '32.395', 'grad_norm': '20.875', 'counters/examples': 94848, 'counters/updates': 2964}
train stats after 94880 examples: {'rewards_train/chosen': '0.096808', 'rewards_train/rejected': '-0.0026251', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.099433', 'logps_train/rejected': '-91.894', 'logps_train/chosen': '-131.31', 'loss/train': '0.65552', 'examples_per_second': '31.611', 'grad_norm': '20.375', 'counters/examples': 94880, 'counters/updates': 2965}
train stats after 94912 examples: {'rewards_train/chosen': '0.096223', 'rewards_train/rejected': '0.1105', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014277', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-121.35', 'loss/train': '0.70754', 'examples_per_second': '32.229', 'grad_norm': '22', 'counters/examples': 94912, 'counters/updates': 2966}
skipping logging after 94944 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '0.11271', 'rewards_train/rejected': '0.024419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088289', 'logps_train/rejected': '-155', 'logps_train/chosen': '-130.78', 'loss/train': '0.65961', 'examples_per_second': '32.337', 'grad_norm': '20.875', 'counters/examples': 94976, 'counters/updates': 2968}
train stats after 95008 examples: {'rewards_train/chosen': '0.11339', 'rewards_train/rejected': '0.097917', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015477', 'logps_train/rejected': '-133.53', 'logps_train/chosen': '-133.63', 'loss/train': '0.69564', 'examples_per_second': '32.395', 'grad_norm': '22', 'counters/examples': 95008, 'counters/updates': 2969}
train stats after 95040 examples: {'rewards_train/chosen': '0.093014', 'rewards_train/rejected': '0.055856', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037158', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-118.25', 'loss/train': '0.67972', 'examples_per_second': '30.756', 'grad_norm': '21.125', 'counters/examples': 95040, 'counters/updates': 2970}
train stats after 95072 examples: {'rewards_train/chosen': '0.15787', 'rewards_train/rejected': '0.066026', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09184', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-154.89', 'loss/train': '0.65735', 'examples_per_second': '31.658', 'grad_norm': '20.75', 'counters/examples': 95072, 'counters/updates': 2971}
train stats after 95104 examples: {'rewards_train/chosen': '0.122', 'rewards_train/rejected': '0.098761', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023236', 'logps_train/rejected': '-97.889', 'logps_train/chosen': '-147.02', 'loss/train': '0.68942', 'examples_per_second': '31.589', 'grad_norm': '20.625', 'counters/examples': 95104, 'counters/updates': 2972}
train stats after 95136 examples: {'rewards_train/chosen': '0.12612', 'rewards_train/rejected': '0.031379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094739', 'logps_train/rejected': '-104.49', 'logps_train/chosen': '-111.55', 'loss/train': '0.65204', 'examples_per_second': '31.929', 'grad_norm': '18.25', 'counters/examples': 95136, 'counters/updates': 2973}
skipping logging after 95168 examples to avoid logging too frequently
train stats after 95200 examples: {'rewards_train/chosen': '0.070922', 'rewards_train/rejected': '0.032937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037985', 'logps_train/rejected': '-90.464', 'logps_train/chosen': '-126.15', 'loss/train': '0.67971', 'examples_per_second': '32.458', 'grad_norm': '19', 'counters/examples': 95200, 'counters/updates': 2975}
train stats after 95232 examples: {'rewards_train/chosen': '0.089432', 'rewards_train/rejected': '0.068556', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020876', 'logps_train/rejected': '-129.68', 'logps_train/chosen': '-117.72', 'loss/train': '0.69013', 'examples_per_second': '33.218', 'grad_norm': '20.25', 'counters/examples': 95232, 'counters/updates': 2976}
train stats after 95264 examples: {'rewards_train/chosen': '0.11025', 'rewards_train/rejected': '0.041936', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068311', 'logps_train/rejected': '-107.87', 'logps_train/chosen': '-123.19', 'loss/train': '0.66465', 'examples_per_second': '30.916', 'grad_norm': '18.125', 'counters/examples': 95264, 'counters/updates': 2977}
train stats after 95296 examples: {'rewards_train/chosen': '0.1085', 'rewards_train/rejected': '0.056489', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052015', 'logps_train/rejected': '-103.19', 'logps_train/chosen': '-127.08', 'loss/train': '0.68157', 'examples_per_second': '31.571', 'grad_norm': '21.25', 'counters/examples': 95296, 'counters/updates': 2978}
train stats after 95328 examples: {'rewards_train/chosen': '0.11116', 'rewards_train/rejected': '0.090988', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.020175', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-154.28', 'loss/train': '0.69211', 'examples_per_second': '32.383', 'grad_norm': '21.875', 'counters/examples': 95328, 'counters/updates': 2979}
train stats after 95360 examples: {'rewards_train/chosen': '0.10662', 'rewards_train/rejected': '0.010641', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095983', 'logps_train/rejected': '-136.66', 'logps_train/chosen': '-112.69', 'loss/train': '0.65464', 'examples_per_second': '31.579', 'grad_norm': '19', 'counters/examples': 95360, 'counters/updates': 2980}
train stats after 95392 examples: {'rewards_train/chosen': '0.13945', 'rewards_train/rejected': '0.056737', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082718', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-133.5', 'loss/train': '0.66195', 'examples_per_second': '31.643', 'grad_norm': '20.5', 'counters/examples': 95392, 'counters/updates': 2981}
train stats after 95424 examples: {'rewards_train/chosen': '0.09219', 'rewards_train/rejected': '0.011717', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080473', 'logps_train/rejected': '-144.51', 'logps_train/chosen': '-155.35', 'loss/train': '0.66605', 'examples_per_second': '30.413', 'grad_norm': '21.5', 'counters/examples': 95424, 'counters/updates': 2982}
train stats after 95456 examples: {'rewards_train/chosen': '0.14088', 'rewards_train/rejected': '0.06079', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080094', 'logps_train/rejected': '-91.675', 'logps_train/chosen': '-129.1', 'loss/train': '0.66273', 'examples_per_second': '30.255', 'grad_norm': '19.625', 'counters/examples': 95456, 'counters/updates': 2983}
train stats after 95488 examples: {'rewards_train/chosen': '0.11698', 'rewards_train/rejected': '0.076416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040567', 'logps_train/rejected': '-157.41', 'logps_train/chosen': '-131.97', 'loss/train': '0.67983', 'examples_per_second': '31.595', 'grad_norm': '21.625', 'counters/examples': 95488, 'counters/updates': 2984}
train stats after 95520 examples: {'rewards_train/chosen': '0.15472', 'rewards_train/rejected': '0.04605', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10867', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-131.28', 'loss/train': '0.64619', 'examples_per_second': '33.149', 'grad_norm': '20.25', 'counters/examples': 95520, 'counters/updates': 2985}
train stats after 95552 examples: {'rewards_train/chosen': '0.15679', 'rewards_train/rejected': '0.05016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10664', 'logps_train/rejected': '-113.31', 'logps_train/chosen': '-148.24', 'loss/train': '0.65681', 'examples_per_second': '31.475', 'grad_norm': '20', 'counters/examples': 95552, 'counters/updates': 2986}
train stats after 95584 examples: {'rewards_train/chosen': '0.12936', 'rewards_train/rejected': '0.04664', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082717', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-128.51', 'loss/train': '0.65525', 'examples_per_second': '31.147', 'grad_norm': '19.375', 'counters/examples': 95584, 'counters/updates': 2987}
train stats after 95616 examples: {'rewards_train/chosen': '0.15129', 'rewards_train/rejected': '-0.001929', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15322', 'logps_train/rejected': '-121.72', 'logps_train/chosen': '-178.64', 'loss/train': '0.6274', 'examples_per_second': '31.606', 'grad_norm': '21.25', 'counters/examples': 95616, 'counters/updates': 2988}
train stats after 95648 examples: {'rewards_train/chosen': '0.15815', 'rewards_train/rejected': '0.11088', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047267', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-153.35', 'loss/train': '0.67652', 'examples_per_second': '33.22', 'grad_norm': '20.875', 'counters/examples': 95648, 'counters/updates': 2989}
train stats after 95680 examples: {'rewards_train/chosen': '0.09335', 'rewards_train/rejected': '0.042525', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050825', 'logps_train/rejected': '-113.35', 'logps_train/chosen': '-163.61', 'loss/train': '0.67269', 'examples_per_second': '24.314', 'grad_norm': '20.875', 'counters/examples': 95680, 'counters/updates': 2990}
train stats after 95712 examples: {'rewards_train/chosen': '0.077766', 'rewards_train/rejected': '0.045088', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032678', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-121.34', 'loss/train': '0.68664', 'examples_per_second': '32.108', 'grad_norm': '19.375', 'counters/examples': 95712, 'counters/updates': 2991}
train stats after 95744 examples: {'rewards_train/chosen': '0.093062', 'rewards_train/rejected': '0.083819', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0092424', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-135.64', 'loss/train': '0.69502', 'examples_per_second': '30.182', 'grad_norm': '21', 'counters/examples': 95744, 'counters/updates': 2992}
train stats after 95776 examples: {'rewards_train/chosen': '0.089128', 'rewards_train/rejected': '0.077545', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011584', 'logps_train/rejected': '-116.64', 'logps_train/chosen': '-134.23', 'loss/train': '0.69447', 'examples_per_second': '24.369', 'grad_norm': '20.625', 'counters/examples': 95776, 'counters/updates': 2993}
train stats after 95808 examples: {'rewards_train/chosen': '0.054266', 'rewards_train/rejected': '0.093796', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.03953', 'logps_train/rejected': '-142.38', 'logps_train/chosen': '-141.32', 'loss/train': '0.72188', 'examples_per_second': '30.313', 'grad_norm': '22.375', 'counters/examples': 95808, 'counters/updates': 2994}
skipping logging after 95840 examples to avoid logging too frequently
train stats after 95872 examples: {'rewards_train/chosen': '0.13487', 'rewards_train/rejected': '0.14536', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010488', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-132.96', 'loss/train': '0.70669', 'examples_per_second': '30.951', 'grad_norm': '20.625', 'counters/examples': 95872, 'counters/updates': 2996}
train stats after 95904 examples: {'rewards_train/chosen': '0.073679', 'rewards_train/rejected': '0.084806', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011126', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-157.06', 'loss/train': '0.70801', 'examples_per_second': '31.511', 'grad_norm': '23', 'counters/examples': 95904, 'counters/updates': 2997}
train stats after 95936 examples: {'rewards_train/chosen': '0.09098', 'rewards_train/rejected': '0.020429', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.070551', 'logps_train/rejected': '-102.47', 'logps_train/chosen': '-99.001', 'loss/train': '0.66527', 'examples_per_second': '30.624', 'grad_norm': '16.5', 'counters/examples': 95936, 'counters/updates': 2998}
skipping logging after 95968 examples to avoid logging too frequently
train stats after 96000 examples: {'rewards_train/chosen': '0.12762', 'rewards_train/rejected': '0.069369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058254', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-152.93', 'loss/train': '0.67521', 'examples_per_second': '31.601', 'grad_norm': '20.75', 'counters/examples': 96000, 'counters/updates': 3000}
Running evaluation after 96000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.89it/s]
eval after 96000: {'rewards_eval/chosen': '0.13766', 'rewards_eval/rejected': '0.061384', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.076275', 'logps_eval/rejected': '-114.78', 'logps_eval/chosen': '-134.17', 'loss/eval': '0.66532'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933/step-96000...
writing checkpoint to .cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933/step-96000/policy.pt...
train stats after 96032 examples: {'rewards_train/chosen': '0.085977', 'rewards_train/rejected': '-0.016053', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10203', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-172.62', 'loss/train': '0.65404', 'examples_per_second': '22.114', 'grad_norm': '20', 'counters/examples': 96032, 'counters/updates': 3001}
train stats after 96064 examples: {'rewards_train/chosen': '0.21977', 'rewards_train/rejected': '0.052858', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16691', 'logps_train/rejected': '-92.158', 'logps_train/chosen': '-144', 'loss/train': '0.6204', 'examples_per_second': '30.274', 'grad_norm': '18.625', 'counters/examples': 96064, 'counters/updates': 3002}
train stats after 96096 examples: {'rewards_train/chosen': '0.17419', 'rewards_train/rejected': '0.037603', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13659', 'logps_train/rejected': '-103.41', 'logps_train/chosen': '-158.86', 'loss/train': '0.63992', 'examples_per_second': '31.948', 'grad_norm': '20', 'counters/examples': 96096, 'counters/updates': 3003}
train stats after 96128 examples: {'rewards_train/chosen': '0.18218', 'rewards_train/rejected': '0.073451', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10873', 'logps_train/rejected': '-135.03', 'logps_train/chosen': '-150.83', 'loss/train': '0.64478', 'examples_per_second': '31.539', 'grad_norm': '20.5', 'counters/examples': 96128, 'counters/updates': 3004}
train stats after 96160 examples: {'rewards_train/chosen': '0.041997', 'rewards_train/rejected': '0.020976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.021021', 'logps_train/rejected': '-97.669', 'logps_train/chosen': '-118.26', 'loss/train': '0.69018', 'examples_per_second': '30.23', 'grad_norm': '20.75', 'counters/examples': 96160, 'counters/updates': 3005}
train stats after 96192 examples: {'rewards_train/chosen': '0.11967', 'rewards_train/rejected': '0.074892', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044775', 'logps_train/rejected': '-101', 'logps_train/chosen': '-117.35', 'loss/train': '0.67838', 'examples_per_second': '31.68', 'grad_norm': '20.125', 'counters/examples': 96192, 'counters/updates': 3006}
train stats after 96224 examples: {'rewards_train/chosen': '0.16228', 'rewards_train/rejected': '0.062787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099496', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-164.67', 'loss/train': '0.65198', 'examples_per_second': '30.612', 'grad_norm': '20.875', 'counters/examples': 96224, 'counters/updates': 3007}
train stats after 96256 examples: {'rewards_train/chosen': '0.15497', 'rewards_train/rejected': '0.086179', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068791', 'logps_train/rejected': '-112.88', 'logps_train/chosen': '-160.56', 'loss/train': '0.66647', 'examples_per_second': '32.624', 'grad_norm': '20', 'counters/examples': 96256, 'counters/updates': 3008}
train stats after 96288 examples: {'rewards_train/chosen': '0.085466', 'rewards_train/rejected': '0.083555', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0019107', 'logps_train/rejected': '-98.621', 'logps_train/chosen': '-118.18', 'loss/train': '0.69648', 'examples_per_second': '31.609', 'grad_norm': '19.875', 'counters/examples': 96288, 'counters/updates': 3009}
train stats after 96320 examples: {'rewards_train/chosen': '0.11738', 'rewards_train/rejected': '0.095587', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021796', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-148.15', 'loss/train': '0.69021', 'examples_per_second': '31.415', 'grad_norm': '21.25', 'counters/examples': 96320, 'counters/updates': 3010}
train stats after 96352 examples: {'rewards_train/chosen': '0.14291', 'rewards_train/rejected': '0.069832', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073074', 'logps_train/rejected': '-114.66', 'logps_train/chosen': '-134.99', 'loss/train': '0.66505', 'examples_per_second': '30.423', 'grad_norm': '20.625', 'counters/examples': 96352, 'counters/updates': 3011}
train stats after 96384 examples: {'rewards_train/chosen': '0.1413', 'rewards_train/rejected': '0.067077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07422', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-173.74', 'loss/train': '0.6639', 'examples_per_second': '31.577', 'grad_norm': '22.375', 'counters/examples': 96384, 'counters/updates': 3012}
train stats after 96416 examples: {'rewards_train/chosen': '0.077398', 'rewards_train/rejected': '0.036965', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040433', 'logps_train/rejected': '-114.65', 'logps_train/chosen': '-116.17', 'loss/train': '0.6847', 'examples_per_second': '30.471', 'grad_norm': '19.125', 'counters/examples': 96416, 'counters/updates': 3013}
train stats after 96448 examples: {'rewards_train/chosen': '0.16836', 'rewards_train/rejected': '-0.0060846', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17444', 'logps_train/rejected': '-90.634', 'logps_train/chosen': '-130.68', 'loss/train': '0.62035', 'examples_per_second': '30.87', 'grad_norm': '17.875', 'counters/examples': 96448, 'counters/updates': 3014}
train stats after 96480 examples: {'rewards_train/chosen': '0.095289', 'rewards_train/rejected': '0.047388', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047901', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-180.46', 'loss/train': '0.67533', 'examples_per_second': '31.638', 'grad_norm': '22.625', 'counters/examples': 96480, 'counters/updates': 3015}
train stats after 96512 examples: {'rewards_train/chosen': '0.17926', 'rewards_train/rejected': '0.064959', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1143', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-142.24', 'loss/train': '0.64286', 'examples_per_second': '31.511', 'grad_norm': '20', 'counters/examples': 96512, 'counters/updates': 3016}
train stats after 96544 examples: {'rewards_train/chosen': '0.16037', 'rewards_train/rejected': '0.07559', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084777', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-125', 'loss/train': '0.65925', 'examples_per_second': '31.818', 'grad_norm': '19', 'counters/examples': 96544, 'counters/updates': 3017}
skipping logging after 96576 examples to avoid logging too frequently
train stats after 96608 examples: {'rewards_train/chosen': '0.13023', 'rewards_train/rejected': '0.085028', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045198', 'logps_train/rejected': '-160.08', 'logps_train/chosen': '-146.27', 'loss/train': '0.68044', 'examples_per_second': '31.634', 'grad_norm': '22.25', 'counters/examples': 96608, 'counters/updates': 3019}
train stats after 96640 examples: {'rewards_train/chosen': '0.12372', 'rewards_train/rejected': '0.057575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066149', 'logps_train/rejected': '-154.25', 'logps_train/chosen': '-137.79', 'loss/train': '0.67008', 'examples_per_second': '31.646', 'grad_norm': '22', 'counters/examples': 96640, 'counters/updates': 3020}
train stats after 96672 examples: {'rewards_train/chosen': '0.14107', 'rewards_train/rejected': '0.091192', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049876', 'logps_train/rejected': '-137.29', 'logps_train/chosen': '-147.55', 'loss/train': '0.67574', 'examples_per_second': '31.624', 'grad_norm': '21.625', 'counters/examples': 96672, 'counters/updates': 3021}
train stats after 96704 examples: {'rewards_train/chosen': '0.074981', 'rewards_train/rejected': '0.012194', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062787', 'logps_train/rejected': '-97.089', 'logps_train/chosen': '-121.62', 'loss/train': '0.67297', 'examples_per_second': '31.284', 'grad_norm': '18.875', 'counters/examples': 96704, 'counters/updates': 3022}
train stats after 96736 examples: {'rewards_train/chosen': '0.23136', 'rewards_train/rejected': '0.027986', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20337', 'logps_train/rejected': '-107.9', 'logps_train/chosen': '-148.18', 'loss/train': '0.6101', 'examples_per_second': '32.266', 'grad_norm': '18.375', 'counters/examples': 96736, 'counters/updates': 3023}
train stats after 96768 examples: {'rewards_train/chosen': '0.16647', 'rewards_train/rejected': '0.092862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073611', 'logps_train/rejected': '-150.77', 'logps_train/chosen': '-151.81', 'loss/train': '0.664', 'examples_per_second': '31.272', 'grad_norm': '21.25', 'counters/examples': 96768, 'counters/updates': 3024}
train stats after 96800 examples: {'rewards_train/chosen': '0.13197', 'rewards_train/rejected': '0.057149', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074824', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-124.7', 'loss/train': '0.66196', 'examples_per_second': '31.027', 'grad_norm': '19.875', 'counters/examples': 96800, 'counters/updates': 3025}
skipping logging after 96832 examples to avoid logging too frequently
train stats after 96864 examples: {'rewards_train/chosen': '0.16926', 'rewards_train/rejected': '0.089501', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079763', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-147.13', 'loss/train': '0.6599', 'examples_per_second': '31.695', 'grad_norm': '21.25', 'counters/examples': 96864, 'counters/updates': 3027}
train stats after 96896 examples: {'rewards_train/chosen': '0.102', 'rewards_train/rejected': '-0.014709', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11671', 'logps_train/rejected': '-118.81', 'logps_train/chosen': '-142.23', 'loss/train': '0.64582', 'examples_per_second': '31.262', 'grad_norm': '19.375', 'counters/examples': 96896, 'counters/updates': 3028}
train stats after 96928 examples: {'rewards_train/chosen': '0.091691', 'rewards_train/rejected': '0.0057457', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085945', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-161.45', 'loss/train': '0.65763', 'examples_per_second': '32.525', 'grad_norm': '20.5', 'counters/examples': 96928, 'counters/updates': 3029}
train stats after 96960 examples: {'rewards_train/chosen': '0.14442', 'rewards_train/rejected': '0.06625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078169', 'logps_train/rejected': '-115.64', 'logps_train/chosen': '-126', 'loss/train': '0.66035', 'examples_per_second': '32.292', 'grad_norm': '18.625', 'counters/examples': 96960, 'counters/updates': 3030}
train stats after 96992 examples: {'rewards_train/chosen': '0.17761', 'rewards_train/rejected': '0.1132', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064411', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-145.81', 'loss/train': '0.6725', 'examples_per_second': '31.153', 'grad_norm': '21', 'counters/examples': 96992, 'counters/updates': 3031}
train stats after 97024 examples: {'rewards_train/chosen': '0.074128', 'rewards_train/rejected': '0.0013472', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072781', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-111.25', 'loss/train': '0.66065', 'examples_per_second': '31.648', 'grad_norm': '19.25', 'counters/examples': 97024, 'counters/updates': 3032}
train stats after 97056 examples: {'rewards_train/chosen': '0.071802', 'rewards_train/rejected': '0.027024', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044778', 'logps_train/rejected': '-97.835', 'logps_train/chosen': '-128.33', 'loss/train': '0.67594', 'examples_per_second': '31.469', 'grad_norm': '19.75', 'counters/examples': 97056, 'counters/updates': 3033}
train stats after 97088 examples: {'rewards_train/chosen': '0.066147', 'rewards_train/rejected': '0.040855', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025292', 'logps_train/rejected': '-101.5', 'logps_train/chosen': '-131.1', 'loss/train': '0.68589', 'examples_per_second': '31.206', 'grad_norm': '19.75', 'counters/examples': 97088, 'counters/updates': 3034}
train stats after 97120 examples: {'rewards_train/chosen': '0.10254', 'rewards_train/rejected': '0.070109', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032429', 'logps_train/rejected': '-144.95', 'logps_train/chosen': '-130.14', 'loss/train': '0.68599', 'examples_per_second': '33.089', 'grad_norm': '20.75', 'counters/examples': 97120, 'counters/updates': 3035}
train stats after 97152 examples: {'rewards_train/chosen': '0.20218', 'rewards_train/rejected': '0.0079754', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19421', 'logps_train/rejected': '-171.13', 'logps_train/chosen': '-114.51', 'loss/train': '0.6093', 'examples_per_second': '31.593', 'grad_norm': '20.875', 'counters/examples': 97152, 'counters/updates': 3036}
train stats after 97184 examples: {'rewards_train/chosen': '0.058141', 'rewards_train/rejected': '-0.033098', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091239', 'logps_train/rejected': '-102.32', 'logps_train/chosen': '-124.06', 'loss/train': '0.65361', 'examples_per_second': '30.991', 'grad_norm': '18.25', 'counters/examples': 97184, 'counters/updates': 3037}
train stats after 97216 examples: {'rewards_train/chosen': '0.15245', 'rewards_train/rejected': '0.096738', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055713', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-137.57', 'loss/train': '0.67631', 'examples_per_second': '32.91', 'grad_norm': '20.5', 'counters/examples': 97216, 'counters/updates': 3038}
train stats after 97248 examples: {'rewards_train/chosen': '0.037585', 'rewards_train/rejected': '0.15532', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.11774', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-107.81', 'loss/train': '0.76142', 'examples_per_second': '31.158', 'grad_norm': '23.125', 'counters/examples': 97248, 'counters/updates': 3039}
train stats after 97280 examples: {'rewards_train/chosen': '0.123', 'rewards_train/rejected': '0.029317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093685', 'logps_train/rejected': '-134.75', 'logps_train/chosen': '-122.54', 'loss/train': '0.66011', 'examples_per_second': '30.124', 'grad_norm': '20.25', 'counters/examples': 97280, 'counters/updates': 3040}
train stats after 97312 examples: {'rewards_train/chosen': '0.12736', 'rewards_train/rejected': '0.1163', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011064', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-155.08', 'loss/train': '0.6943', 'examples_per_second': '31.594', 'grad_norm': '23', 'counters/examples': 97312, 'counters/updates': 3041}
train stats after 97344 examples: {'rewards_train/chosen': '0.15024', 'rewards_train/rejected': '0.066809', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083429', 'logps_train/rejected': '-95.817', 'logps_train/chosen': '-138.97', 'loss/train': '0.66083', 'examples_per_second': '31.009', 'grad_norm': '19.125', 'counters/examples': 97344, 'counters/updates': 3042}
train stats after 97376 examples: {'rewards_train/chosen': '0.17538', 'rewards_train/rejected': '0.084001', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091379', 'logps_train/rejected': '-88.206', 'logps_train/chosen': '-131.95', 'loss/train': '0.65579', 'examples_per_second': '31.633', 'grad_norm': '19.5', 'counters/examples': 97376, 'counters/updates': 3043}
train stats after 97408 examples: {'rewards_train/chosen': '0.10951', 'rewards_train/rejected': '0.056481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053029', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-125.16', 'loss/train': '0.67336', 'examples_per_second': '30.708', 'grad_norm': '20', 'counters/examples': 97408, 'counters/updates': 3044}
train stats after 97440 examples: {'rewards_train/chosen': '0.12666', 'rewards_train/rejected': '0.047525', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079136', 'logps_train/rejected': '-181.35', 'logps_train/chosen': '-131.45', 'loss/train': '0.66347', 'examples_per_second': '30.468', 'grad_norm': '21.75', 'counters/examples': 97440, 'counters/updates': 3045}
train stats after 97472 examples: {'rewards_train/chosen': '0.17338', 'rewards_train/rejected': '0.075634', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097749', 'logps_train/rejected': '-121.52', 'logps_train/chosen': '-130.03', 'loss/train': '0.65771', 'examples_per_second': '31.621', 'grad_norm': '19.625', 'counters/examples': 97472, 'counters/updates': 3046}
train stats after 97504 examples: {'rewards_train/chosen': '0.15873', 'rewards_train/rejected': '0.040177', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11855', 'logps_train/rejected': '-101', 'logps_train/chosen': '-153.16', 'loss/train': '0.64463', 'examples_per_second': '30.823', 'grad_norm': '19.25', 'counters/examples': 97504, 'counters/updates': 3047}
train stats after 97536 examples: {'rewards_train/chosen': '0.13213', 'rewards_train/rejected': '0.038311', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093814', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-137.79', 'loss/train': '0.6567', 'examples_per_second': '31.795', 'grad_norm': '19.75', 'counters/examples': 97536, 'counters/updates': 3048}
train stats after 97568 examples: {'rewards_train/chosen': '0.1876', 'rewards_train/rejected': '0.088612', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098992', 'logps_train/rejected': '-101.48', 'logps_train/chosen': '-118.99', 'loss/train': '0.65331', 'examples_per_second': '32.205', 'grad_norm': '18.25', 'counters/examples': 97568, 'counters/updates': 3049}
train stats after 97600 examples: {'rewards_train/chosen': '0.17226', 'rewards_train/rejected': '0.064069', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10819', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-150.8', 'loss/train': '0.65014', 'examples_per_second': '31.668', 'grad_norm': '20.75', 'counters/examples': 97600, 'counters/updates': 3050}
train stats after 97632 examples: {'rewards_train/chosen': '0.077316', 'rewards_train/rejected': '0.042739', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034577', 'logps_train/rejected': '-105.66', 'logps_train/chosen': '-130.04', 'loss/train': '0.68362', 'examples_per_second': '24.693', 'grad_norm': '19.625', 'counters/examples': 97632, 'counters/updates': 3051}
train stats after 97664 examples: {'rewards_train/chosen': '0.041945', 'rewards_train/rejected': '0.027541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014404', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-128.3', 'loss/train': '0.69546', 'examples_per_second': '31.615', 'grad_norm': '20.625', 'counters/examples': 97664, 'counters/updates': 3052}
train stats after 97696 examples: {'rewards_train/chosen': '0.098449', 'rewards_train/rejected': '-0.0014708', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099919', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-148.15', 'loss/train': '0.65117', 'examples_per_second': '31.592', 'grad_norm': '20.625', 'counters/examples': 97696, 'counters/updates': 3053}
train stats after 97728 examples: {'rewards_train/chosen': '0.077409', 'rewards_train/rejected': '0.015767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061642', 'logps_train/rejected': '-102.99', 'logps_train/chosen': '-157.43', 'loss/train': '0.66732', 'examples_per_second': '31.63', 'grad_norm': '20.125', 'counters/examples': 97728, 'counters/updates': 3054}
skipping logging after 97760 examples to avoid logging too frequently
train stats after 97792 examples: {'rewards_train/chosen': '0.10386', 'rewards_train/rejected': '0.039214', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064644', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-164.3', 'loss/train': '0.67244', 'examples_per_second': '31.652', 'grad_norm': '22', 'counters/examples': 97792, 'counters/updates': 3056}
skipping logging after 97824 examples to avoid logging too frequently
train stats after 97856 examples: {'rewards_train/chosen': '0.18486', 'rewards_train/rejected': '0.011483', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17338', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-162.52', 'loss/train': '0.61559', 'examples_per_second': '31.406', 'grad_norm': '20.25', 'counters/examples': 97856, 'counters/updates': 3058}
skipping logging after 97888 examples to avoid logging too frequently
train stats after 97920 examples: {'rewards_train/chosen': '0.14406', 'rewards_train/rejected': '0.098505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045551', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-124.93', 'loss/train': '0.67747', 'examples_per_second': '32.357', 'grad_norm': '19.5', 'counters/examples': 97920, 'counters/updates': 3060}
train stats after 97952 examples: {'rewards_train/chosen': '0.083389', 'rewards_train/rejected': '0.033546', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049843', 'logps_train/rejected': '-96.634', 'logps_train/chosen': '-96.967', 'loss/train': '0.67087', 'examples_per_second': '32.436', 'grad_norm': '17', 'counters/examples': 97952, 'counters/updates': 3061}
train stats after 97984 examples: {'rewards_train/chosen': '0.061641', 'rewards_train/rejected': '0.054359', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0072819', 'logps_train/rejected': '-111.88', 'logps_train/chosen': '-104.07', 'loss/train': '0.69669', 'examples_per_second': '31.475', 'grad_norm': '19.125', 'counters/examples': 97984, 'counters/updates': 3062}
train stats after 98016 examples: {'rewards_train/chosen': '0.19179', 'rewards_train/rejected': '-0.010793', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.20258', 'logps_train/rejected': '-120.25', 'logps_train/chosen': '-149.82', 'loss/train': '0.60365', 'examples_per_second': '30.629', 'grad_norm': '18.875', 'counters/examples': 98016, 'counters/updates': 3063}
train stats after 98048 examples: {'rewards_train/chosen': '0.12293', 'rewards_train/rejected': '0.029532', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093396', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-117.89', 'loss/train': '0.65535', 'examples_per_second': '32.189', 'grad_norm': '18.125', 'counters/examples': 98048, 'counters/updates': 3064}
train stats after 98080 examples: {'rewards_train/chosen': '0.16437', 'rewards_train/rejected': '0.11511', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049267', 'logps_train/rejected': '-146.26', 'logps_train/chosen': '-131.56', 'loss/train': '0.67889', 'examples_per_second': '30.517', 'grad_norm': '20.125', 'counters/examples': 98080, 'counters/updates': 3065}
train stats after 98112 examples: {'rewards_train/chosen': '0.0964', 'rewards_train/rejected': '0.018168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078232', 'logps_train/rejected': '-106.55', 'logps_train/chosen': '-160.8', 'loss/train': '0.66157', 'examples_per_second': '31.618', 'grad_norm': '20.5', 'counters/examples': 98112, 'counters/updates': 3066}
train stats after 98144 examples: {'rewards_train/chosen': '0.098561', 'rewards_train/rejected': '0.065274', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033287', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-163.88', 'loss/train': '0.68534', 'examples_per_second': '30.181', 'grad_norm': '22.375', 'counters/examples': 98144, 'counters/updates': 3067}
train stats after 98176 examples: {'rewards_train/chosen': '0.21871', 'rewards_train/rejected': '0.064321', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15439', 'logps_train/rejected': '-89.487', 'logps_train/chosen': '-129.83', 'loss/train': '0.63284', 'examples_per_second': '31.691', 'grad_norm': '19', 'counters/examples': 98176, 'counters/updates': 3068}
train stats after 98208 examples: {'rewards_train/chosen': '0.07162', 'rewards_train/rejected': '0.051911', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019709', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-115.01', 'loss/train': '0.68843', 'examples_per_second': '31.598', 'grad_norm': '20.375', 'counters/examples': 98208, 'counters/updates': 3069}
train stats after 98240 examples: {'rewards_train/chosen': '0.14184', 'rewards_train/rejected': '0.024958', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11688', 'logps_train/rejected': '-104.76', 'logps_train/chosen': '-126.69', 'loss/train': '0.64936', 'examples_per_second': '30.73', 'grad_norm': '19.375', 'counters/examples': 98240, 'counters/updates': 3070}
train stats after 98272 examples: {'rewards_train/chosen': '0.096839', 'rewards_train/rejected': '0.07348', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023359', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-96.318', 'loss/train': '0.68879', 'examples_per_second': '32.383', 'grad_norm': '18.25', 'counters/examples': 98272, 'counters/updates': 3071}
train stats after 98304 examples: {'rewards_train/chosen': '0.11989', 'rewards_train/rejected': '0.093154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026741', 'logps_train/rejected': '-153.72', 'logps_train/chosen': '-118.75', 'loss/train': '0.6881', 'examples_per_second': '31.592', 'grad_norm': '22.25', 'counters/examples': 98304, 'counters/updates': 3072}
train stats after 98336 examples: {'rewards_train/chosen': '0.12305', 'rewards_train/rejected': '0.02508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097966', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-165.05', 'loss/train': '0.65103', 'examples_per_second': '31.573', 'grad_norm': '19.875', 'counters/examples': 98336, 'counters/updates': 3073}
train stats after 98368 examples: {'rewards_train/chosen': '0.17117', 'rewards_train/rejected': '0.026027', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14514', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-172.63', 'loss/train': '0.62977', 'examples_per_second': '32.177', 'grad_norm': '20.5', 'counters/examples': 98368, 'counters/updates': 3074}
train stats after 98400 examples: {'rewards_train/chosen': '0.15279', 'rewards_train/rejected': '0.12745', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025336', 'logps_train/rejected': '-153.84', 'logps_train/chosen': '-142.3', 'loss/train': '0.69073', 'examples_per_second': '32.889', 'grad_norm': '22', 'counters/examples': 98400, 'counters/updates': 3075}
train stats after 98432 examples: {'rewards_train/chosen': '0.17464', 'rewards_train/rejected': '0.055164', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11948', 'logps_train/rejected': '-133.45', 'logps_train/chosen': '-159.02', 'loss/train': '0.64197', 'examples_per_second': '31.563', 'grad_norm': '20.5', 'counters/examples': 98432, 'counters/updates': 3076}
skipping logging after 98464 examples to avoid logging too frequently
train stats after 98496 examples: {'rewards_train/chosen': '0.14933', 'rewards_train/rejected': '0.053995', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095338', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-143.18', 'loss/train': '0.65568', 'examples_per_second': '30.18', 'grad_norm': '20.125', 'counters/examples': 98496, 'counters/updates': 3078}
train stats after 98528 examples: {'rewards_train/chosen': '0.088936', 'rewards_train/rejected': '0.072118', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016819', 'logps_train/rejected': '-124.91', 'logps_train/chosen': '-145.05', 'loss/train': '0.6935', 'examples_per_second': '30.192', 'grad_norm': '22.5', 'counters/examples': 98528, 'counters/updates': 3079}
train stats after 98560 examples: {'rewards_train/chosen': '0.14438', 'rewards_train/rejected': '0.0055364', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13884', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-142.14', 'loss/train': '0.63512', 'examples_per_second': '31.667', 'grad_norm': '20.5', 'counters/examples': 98560, 'counters/updates': 3080}
skipping logging after 98592 examples to avoid logging too frequently
train stats after 98624 examples: {'rewards_train/chosen': '0.15172', 'rewards_train/rejected': '0.093314', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058406', 'logps_train/rejected': '-132', 'logps_train/chosen': '-137.79', 'loss/train': '0.67167', 'examples_per_second': '31.306', 'grad_norm': '21', 'counters/examples': 98624, 'counters/updates': 3082}
train stats after 98656 examples: {'rewards_train/chosen': '0.1702', 'rewards_train/rejected': '0.014119', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15608', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-142.12', 'loss/train': '0.6273', 'examples_per_second': '30.514', 'grad_norm': '19.5', 'counters/examples': 98656, 'counters/updates': 3083}
train stats after 98688 examples: {'rewards_train/chosen': '0.12346', 'rewards_train/rejected': '0.023904', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.099557', 'logps_train/rejected': '-108.75', 'logps_train/chosen': '-133.2', 'loss/train': '0.65145', 'examples_per_second': '31.262', 'grad_norm': '18.875', 'counters/examples': 98688, 'counters/updates': 3084}
train stats after 98720 examples: {'rewards_train/chosen': '0.054529', 'rewards_train/rejected': '0.034186', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020343', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-144.29', 'loss/train': '0.69413', 'examples_per_second': '30.67', 'grad_norm': '21.125', 'counters/examples': 98720, 'counters/updates': 3085}
train stats after 98752 examples: {'rewards_train/chosen': '0.085014', 'rewards_train/rejected': '0.044186', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040828', 'logps_train/rejected': '-153.13', 'logps_train/chosen': '-129.26', 'loss/train': '0.67929', 'examples_per_second': '31.602', 'grad_norm': '21.625', 'counters/examples': 98752, 'counters/updates': 3086}
skipping logging after 98784 examples to avoid logging too frequently
train stats after 98816 examples: {'rewards_train/chosen': '0.14532', 'rewards_train/rejected': '0.090952', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054366', 'logps_train/rejected': '-134.86', 'logps_train/chosen': '-121.28', 'loss/train': '0.67678', 'examples_per_second': '31.764', 'grad_norm': '20.625', 'counters/examples': 98816, 'counters/updates': 3088}
train stats after 98848 examples: {'rewards_train/chosen': '0.18324', 'rewards_train/rejected': '0.084021', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099224', 'logps_train/rejected': '-163.4', 'logps_train/chosen': '-161.02', 'loss/train': '0.65293', 'examples_per_second': '31.637', 'grad_norm': '22.375', 'counters/examples': 98848, 'counters/updates': 3089}
train stats after 98880 examples: {'rewards_train/chosen': '0.13952', 'rewards_train/rejected': '0.048832', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.090691', 'logps_train/rejected': '-93.882', 'logps_train/chosen': '-121.85', 'loss/train': '0.65297', 'examples_per_second': '31.553', 'grad_norm': '18', 'counters/examples': 98880, 'counters/updates': 3090}
train stats after 98912 examples: {'rewards_train/chosen': '0.1845', 'rewards_train/rejected': '0.11697', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067531', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-152.32', 'loss/train': '0.66846', 'examples_per_second': '31.635', 'grad_norm': '20.625', 'counters/examples': 98912, 'counters/updates': 3091}
skipping logging after 98944 examples to avoid logging too frequently
train stats after 98976 examples: {'rewards_train/chosen': '0.12008', 'rewards_train/rejected': '0.022642', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097443', 'logps_train/rejected': '-103.86', 'logps_train/chosen': '-136.1', 'loss/train': '0.65038', 'examples_per_second': '34.253', 'grad_norm': '19.25', 'counters/examples': 98976, 'counters/updates': 3093}
train stats after 99008 examples: {'rewards_train/chosen': '0.097023', 'rewards_train/rejected': '0.020442', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07658', 'logps_train/rejected': '-129', 'logps_train/chosen': '-130.22', 'loss/train': '0.66303', 'examples_per_second': '30.444', 'grad_norm': '19.5', 'counters/examples': 99008, 'counters/updates': 3094}
train stats after 99040 examples: {'rewards_train/chosen': '0.12553', 'rewards_train/rejected': '0.078893', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.046637', 'logps_train/rejected': '-135', 'logps_train/chosen': '-115.84', 'loss/train': '0.67895', 'examples_per_second': '31.553', 'grad_norm': '20.125', 'counters/examples': 99040, 'counters/updates': 3095}
train stats after 99072 examples: {'rewards_train/chosen': '0.14564', 'rewards_train/rejected': '0.12896', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016684', 'logps_train/rejected': '-125.1', 'logps_train/chosen': '-131.4', 'loss/train': '0.69099', 'examples_per_second': '32.473', 'grad_norm': '20.875', 'counters/examples': 99072, 'counters/updates': 3096}
train stats after 99104 examples: {'rewards_train/chosen': '0.13212', 'rewards_train/rejected': '0.018792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11333', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-136.63', 'loss/train': '0.64792', 'examples_per_second': '30.545', 'grad_norm': '19.5', 'counters/examples': 99104, 'counters/updates': 3097}
skipping logging after 99136 examples to avoid logging too frequently
train stats after 99168 examples: {'rewards_train/chosen': '0.11187', 'rewards_train/rejected': '0.021977', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089893', 'logps_train/rejected': '-99.964', 'logps_train/chosen': '-115.56', 'loss/train': '0.66085', 'examples_per_second': '32.652', 'grad_norm': '19.125', 'counters/examples': 99168, 'counters/updates': 3099}
train stats after 99200 examples: {'rewards_train/chosen': '0.055839', 'rewards_train/rejected': '0.063889', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0080502', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-131.87', 'loss/train': '0.7085', 'examples_per_second': '30.668', 'grad_norm': '20.375', 'counters/examples': 99200, 'counters/updates': 3100}
train stats after 99232 examples: {'rewards_train/chosen': '0.11486', 'rewards_train/rejected': '0.055106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059752', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-133.24', 'loss/train': '0.66978', 'examples_per_second': '32.698', 'grad_norm': '20.25', 'counters/examples': 99232, 'counters/updates': 3101}
train stats after 99264 examples: {'rewards_train/chosen': '0.14008', 'rewards_train/rejected': '0.045132', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094947', 'logps_train/rejected': '-111.25', 'logps_train/chosen': '-145.29', 'loss/train': '0.6595', 'examples_per_second': '30.892', 'grad_norm': '19.125', 'counters/examples': 99264, 'counters/updates': 3102}
train stats after 99296 examples: {'rewards_train/chosen': '0.098026', 'rewards_train/rejected': '0.071846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02618', 'logps_train/rejected': '-134.55', 'logps_train/chosen': '-120.02', 'loss/train': '0.68688', 'examples_per_second': '31.634', 'grad_norm': '20.5', 'counters/examples': 99296, 'counters/updates': 3103}
train stats after 99328 examples: {'rewards_train/chosen': '0.1887', 'rewards_train/rejected': '0.08049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10821', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-149.37', 'loss/train': '0.64849', 'examples_per_second': '30.544', 'grad_norm': '20.375', 'counters/examples': 99328, 'counters/updates': 3104}
train stats after 99360 examples: {'rewards_train/chosen': '0.044171', 'rewards_train/rejected': '-0.026693', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070863', 'logps_train/rejected': '-120.08', 'logps_train/chosen': '-114.98', 'loss/train': '0.66323', 'examples_per_second': '32.654', 'grad_norm': '19', 'counters/examples': 99360, 'counters/updates': 3105}
train stats after 99392 examples: {'rewards_train/chosen': '0.11831', 'rewards_train/rejected': '0.0013775', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11694', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-139.33', 'loss/train': '0.64349', 'examples_per_second': '30.099', 'grad_norm': '19.625', 'counters/examples': 99392, 'counters/updates': 3106}
skipping logging after 99424 examples to avoid logging too frequently
train stats after 99456 examples: {'rewards_train/chosen': '0.13564', 'rewards_train/rejected': '0.071742', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063897', 'logps_train/rejected': '-127.85', 'logps_train/chosen': '-131.92', 'loss/train': '0.67037', 'examples_per_second': '32.811', 'grad_norm': '20.625', 'counters/examples': 99456, 'counters/updates': 3108}
train stats after 99488 examples: {'rewards_train/chosen': '0.14622', 'rewards_train/rejected': '0.019087', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12714', 'logps_train/rejected': '-118.4', 'logps_train/chosen': '-118.04', 'loss/train': '0.63736', 'examples_per_second': '33.145', 'grad_norm': '18.25', 'counters/examples': 99488, 'counters/updates': 3109}
train stats after 99520 examples: {'rewards_train/chosen': '0.032523', 'rewards_train/rejected': '0.035021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0024973', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-125.11', 'loss/train': '0.708', 'examples_per_second': '32.15', 'grad_norm': '21.875', 'counters/examples': 99520, 'counters/updates': 3110}
train stats after 99552 examples: {'rewards_train/chosen': '0.10341', 'rewards_train/rejected': '0.03864', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064774', 'logps_train/rejected': '-113.82', 'logps_train/chosen': '-121.91', 'loss/train': '0.6669', 'examples_per_second': '32.66', 'grad_norm': '19', 'counters/examples': 99552, 'counters/updates': 3111}
train stats after 99584 examples: {'rewards_train/chosen': '0.13155', 'rewards_train/rejected': '0.048859', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082694', 'logps_train/rejected': '-136.51', 'logps_train/chosen': '-159.62', 'loss/train': '0.66148', 'examples_per_second': '31.771', 'grad_norm': '20.75', 'counters/examples': 99584, 'counters/updates': 3112}
skipping logging after 99616 examples to avoid logging too frequently
train stats after 99648 examples: {'rewards_train/chosen': '0.17689', 'rewards_train/rejected': '0.043785', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13311', 'logps_train/rejected': '-111.58', 'logps_train/chosen': '-117.02', 'loss/train': '0.63461', 'examples_per_second': '32.893', 'grad_norm': '19.5', 'counters/examples': 99648, 'counters/updates': 3114}
train stats after 99680 examples: {'rewards_train/chosen': '0.17046', 'rewards_train/rejected': '0.044509', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12595', 'logps_train/rejected': '-98.695', 'logps_train/chosen': '-120.03', 'loss/train': '0.64034', 'examples_per_second': '31.629', 'grad_norm': '18.25', 'counters/examples': 99680, 'counters/updates': 3115}
skipping logging after 99712 examples to avoid logging too frequently
train stats after 99744 examples: {'rewards_train/chosen': '0.12581', 'rewards_train/rejected': '0.070235', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055574', 'logps_train/rejected': '-87.458', 'logps_train/chosen': '-110.24', 'loss/train': '0.67036', 'examples_per_second': '31.55', 'grad_norm': '17.25', 'counters/examples': 99744, 'counters/updates': 3117}
train stats after 99776 examples: {'rewards_train/chosen': '0.13426', 'rewards_train/rejected': '0.046484', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087777', 'logps_train/rejected': '-139.37', 'logps_train/chosen': '-110.86', 'loss/train': '0.65758', 'examples_per_second': '30.765', 'grad_norm': '19.125', 'counters/examples': 99776, 'counters/updates': 3118}
skipping logging after 99808 examples to avoid logging too frequently
train stats after 99840 examples: {'rewards_train/chosen': '0.15778', 'rewards_train/rejected': '0.06606', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091716', 'logps_train/rejected': '-157.71', 'logps_train/chosen': '-173.43', 'loss/train': '0.65701', 'examples_per_second': '30.008', 'grad_norm': '22.75', 'counters/examples': 99840, 'counters/updates': 3120}
train stats after 99872 examples: {'rewards_train/chosen': '0.18714', 'rewards_train/rejected': '0.093338', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093799', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-131.17', 'loss/train': '0.65349', 'examples_per_second': '30.634', 'grad_norm': '19.375', 'counters/examples': 99872, 'counters/updates': 3121}
train stats after 99904 examples: {'rewards_train/chosen': '0.13558', 'rewards_train/rejected': '0.045807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08977', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-171.74', 'loss/train': '0.66142', 'examples_per_second': '31.646', 'grad_norm': '23', 'counters/examples': 99904, 'counters/updates': 3122}
train stats after 99936 examples: {'rewards_train/chosen': '0.13018', 'rewards_train/rejected': '0.11681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.013374', 'logps_train/rejected': '-162.17', 'logps_train/chosen': '-178.4', 'loss/train': '0.69631', 'examples_per_second': '32.311', 'grad_norm': '24.875', 'counters/examples': 99936, 'counters/updates': 3123}
skipping logging after 99968 examples to avoid logging too frequently
train stats after 100000 examples: {'rewards_train/chosen': '0.15859', 'rewards_train/rejected': '0.0066702', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15192', 'logps_train/rejected': '-104.66', 'logps_train/chosen': '-143.08', 'loss/train': '0.62791', 'examples_per_second': '31.612', 'grad_norm': '20.25', 'counters/examples': 100000, 'counters/updates': 3125}
train stats after 100032 examples: {'rewards_train/chosen': '0.13773', 'rewards_train/rejected': '0.036004', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10172', 'logps_train/rejected': '-102.54', 'logps_train/chosen': '-120.37', 'loss/train': '0.65244', 'examples_per_second': '32.849', 'grad_norm': '19.25', 'counters/examples': 100032, 'counters/updates': 3126}
skipping logging after 100064 examples to avoid logging too frequently
train stats after 100096 examples: {'rewards_train/chosen': '0.10222', 'rewards_train/rejected': '0.045883', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056337', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-125.83', 'loss/train': '0.67639', 'examples_per_second': '33.521', 'grad_norm': '19', 'counters/examples': 100096, 'counters/updates': 3128}
skipping logging after 100128 examples to avoid logging too frequently
train stats after 100160 examples: {'rewards_train/chosen': '0.1679', 'rewards_train/rejected': '0.026928', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14097', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-134.38', 'loss/train': '0.63781', 'examples_per_second': '31.633', 'grad_norm': '20.75', 'counters/examples': 100160, 'counters/updates': 3130}
train stats after 100192 examples: {'rewards_train/chosen': '0.054911', 'rewards_train/rejected': '0.002419', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052491', 'logps_train/rejected': '-115.82', 'logps_train/chosen': '-109.4', 'loss/train': '0.67268', 'examples_per_second': '31.955', 'grad_norm': '19.5', 'counters/examples': 100192, 'counters/updates': 3131}
skipping logging after 100224 examples to avoid logging too frequently
train stats after 100256 examples: {'rewards_train/chosen': '0.2281', 'rewards_train/rejected': '0.081608', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14649', 'logps_train/rejected': '-102.94', 'logps_train/chosen': '-128.6', 'loss/train': '0.63067', 'examples_per_second': '31.478', 'grad_norm': '17.625', 'counters/examples': 100256, 'counters/updates': 3133}
skipping logging after 100288 examples to avoid logging too frequently
train stats after 100320 examples: {'rewards_train/chosen': '0.13132', 'rewards_train/rejected': '0.068251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063065', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-116.22', 'loss/train': '0.66861', 'examples_per_second': '31.524', 'grad_norm': '19.375', 'counters/examples': 100320, 'counters/updates': 3135}
train stats after 100352 examples: {'rewards_train/chosen': '0.17358', 'rewards_train/rejected': '0.078534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095049', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-124.31', 'loss/train': '0.65544', 'examples_per_second': '31.182', 'grad_norm': '19.125', 'counters/examples': 100352, 'counters/updates': 3136}
train stats after 100384 examples: {'rewards_train/chosen': '0.17445', 'rewards_train/rejected': '0.04955', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1249', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-142.57', 'loss/train': '0.63883', 'examples_per_second': '30.575', 'grad_norm': '20.75', 'counters/examples': 100384, 'counters/updates': 3137}
train stats after 100416 examples: {'rewards_train/chosen': '0.07666', 'rewards_train/rejected': '-0.023333', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.099993', 'logps_train/rejected': '-100.16', 'logps_train/chosen': '-106.07', 'loss/train': '0.65', 'examples_per_second': '32.814', 'grad_norm': '17.875', 'counters/examples': 100416, 'counters/updates': 3138}
train stats after 100448 examples: {'rewards_train/chosen': '0.13521', 'rewards_train/rejected': '0.051', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084212', 'logps_train/rejected': '-147.09', 'logps_train/chosen': '-140.74', 'loss/train': '0.65845', 'examples_per_second': '31.56', 'grad_norm': '20.75', 'counters/examples': 100448, 'counters/updates': 3139}
train stats after 100480 examples: {'rewards_train/chosen': '0.13616', 'rewards_train/rejected': '0.11334', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022823', 'logps_train/rejected': '-112.48', 'logps_train/chosen': '-108.25', 'loss/train': '0.68767', 'examples_per_second': '30.161', 'grad_norm': '18.875', 'counters/examples': 100480, 'counters/updates': 3140}
train stats after 100512 examples: {'rewards_train/chosen': '0.17235', 'rewards_train/rejected': '0.054588', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11777', 'logps_train/rejected': '-109.55', 'logps_train/chosen': '-122.82', 'loss/train': '0.64057', 'examples_per_second': '32.042', 'grad_norm': '18.625', 'counters/examples': 100512, 'counters/updates': 3141}
train stats after 100544 examples: {'rewards_train/chosen': '0.12492', 'rewards_train/rejected': '0.030926', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09399', 'logps_train/rejected': '-109.98', 'logps_train/chosen': '-138.88', 'loss/train': '0.65287', 'examples_per_second': '30.907', 'grad_norm': '18.75', 'counters/examples': 100544, 'counters/updates': 3142}
train stats after 100576 examples: {'rewards_train/chosen': '0.13088', 'rewards_train/rejected': '0.064925', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065951', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-154.31', 'loss/train': '0.66581', 'examples_per_second': '32.992', 'grad_norm': '21.25', 'counters/examples': 100576, 'counters/updates': 3143}
train stats after 100608 examples: {'rewards_train/chosen': '0.14382', 'rewards_train/rejected': '0.056565', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087252', 'logps_train/rejected': '-139.95', 'logps_train/chosen': '-167.58', 'loss/train': '0.65943', 'examples_per_second': '30.632', 'grad_norm': '21.625', 'counters/examples': 100608, 'counters/updates': 3144}
train stats after 100640 examples: {'rewards_train/chosen': '0.048934', 'rewards_train/rejected': '0.079939', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.031005', 'logps_train/rejected': '-102.02', 'logps_train/chosen': '-165.79', 'loss/train': '0.71866', 'examples_per_second': '31.566', 'grad_norm': '22.625', 'counters/examples': 100640, 'counters/updates': 3145}
skipping logging after 100672 examples to avoid logging too frequently
train stats after 100704 examples: {'rewards_train/chosen': '0.16984', 'rewards_train/rejected': '0.092899', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076939', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-148.32', 'loss/train': '0.66831', 'examples_per_second': '30.237', 'grad_norm': '20.75', 'counters/examples': 100704, 'counters/updates': 3147}
train stats after 100736 examples: {'rewards_train/chosen': '0.15147', 'rewards_train/rejected': '0.051226', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10024', 'logps_train/rejected': '-144.61', 'logps_train/chosen': '-193.17', 'loss/train': '0.65405', 'examples_per_second': '30.627', 'grad_norm': '21.875', 'counters/examples': 100736, 'counters/updates': 3148}
train stats after 100768 examples: {'rewards_train/chosen': '0.073674', 'rewards_train/rejected': '0.041745', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031928', 'logps_train/rejected': '-130.83', 'logps_train/chosen': '-116.21', 'loss/train': '0.684', 'examples_per_second': '32.195', 'grad_norm': '19.875', 'counters/examples': 100768, 'counters/updates': 3149}
train stats after 100800 examples: {'rewards_train/chosen': '0.17021', 'rewards_train/rejected': '0.03239', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13782', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-137.87', 'loss/train': '0.63745', 'examples_per_second': '31.547', 'grad_norm': '19.125', 'counters/examples': 100800, 'counters/updates': 3150}
train stats after 100832 examples: {'rewards_train/chosen': '0.15185', 'rewards_train/rejected': '0.098391', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053456', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-189.12', 'loss/train': '0.67694', 'examples_per_second': '30.545', 'grad_norm': '21.375', 'counters/examples': 100832, 'counters/updates': 3151}
train stats after 100864 examples: {'rewards_train/chosen': '0.083776', 'rewards_train/rejected': '0.012619', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071156', 'logps_train/rejected': '-87.838', 'logps_train/chosen': '-124.73', 'loss/train': '0.66586', 'examples_per_second': '31.271', 'grad_norm': '18', 'counters/examples': 100864, 'counters/updates': 3152}
train stats after 100896 examples: {'rewards_train/chosen': '0.17646', 'rewards_train/rejected': '0.07208', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10438', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-135.37', 'loss/train': '0.64995', 'examples_per_second': '30.546', 'grad_norm': '20.125', 'counters/examples': 100896, 'counters/updates': 3153}
train stats after 100928 examples: {'rewards_train/chosen': '0.12111', 'rewards_train/rejected': '0.047965', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073149', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-109.69', 'loss/train': '0.67003', 'examples_per_second': '31.108', 'grad_norm': '20.375', 'counters/examples': 100928, 'counters/updates': 3154}
train stats after 100960 examples: {'rewards_train/chosen': '0.048291', 'rewards_train/rejected': '0.0006767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047614', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-150.12', 'loss/train': '0.6747', 'examples_per_second': '30.503', 'grad_norm': '20.75', 'counters/examples': 100960, 'counters/updates': 3155}
train stats after 100992 examples: {'rewards_train/chosen': '0.14381', 'rewards_train/rejected': '0.050198', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093609', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-111.03', 'loss/train': '0.65128', 'examples_per_second': '31.175', 'grad_norm': '17.625', 'counters/examples': 100992, 'counters/updates': 3156}
train stats after 101024 examples: {'rewards_train/chosen': '0.21997', 'rewards_train/rejected': '0.07656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14341', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-130.72', 'loss/train': '0.63354', 'examples_per_second': '30.291', 'grad_norm': '20.75', 'counters/examples': 101024, 'counters/updates': 3157}
skipping logging after 101056 examples to avoid logging too frequently
train stats after 101088 examples: {'rewards_train/chosen': '0.11376', 'rewards_train/rejected': '0.081602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032162', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-128.59', 'loss/train': '0.68437', 'examples_per_second': '30.173', 'grad_norm': '20.5', 'counters/examples': 101088, 'counters/updates': 3159}
train stats after 101120 examples: {'rewards_train/chosen': '0.14075', 'rewards_train/rejected': '0.022018', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11873', 'logps_train/rejected': '-145.23', 'logps_train/chosen': '-168.21', 'loss/train': '0.64318', 'examples_per_second': '32.014', 'grad_norm': '22.25', 'counters/examples': 101120, 'counters/updates': 3160}
train stats after 101152 examples: {'rewards_train/chosen': '0.13213', 'rewards_train/rejected': '0.057709', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.074417', 'logps_train/rejected': '-102.82', 'logps_train/chosen': '-132.44', 'loss/train': '0.66229', 'examples_per_second': '31.196', 'grad_norm': '19.125', 'counters/examples': 101152, 'counters/updates': 3161}
train stats after 101184 examples: {'rewards_train/chosen': '0.1639', 'rewards_train/rejected': '0.084831', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079073', 'logps_train/rejected': '-104.52', 'logps_train/chosen': '-140.97', 'loss/train': '0.66352', 'examples_per_second': '31.732', 'grad_norm': '19.5', 'counters/examples': 101184, 'counters/updates': 3162}
train stats after 101216 examples: {'rewards_train/chosen': '0.12806', 'rewards_train/rejected': '0.051923', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07614', 'logps_train/rejected': '-122.62', 'logps_train/chosen': '-150.51', 'loss/train': '0.66039', 'examples_per_second': '31.507', 'grad_norm': '19.875', 'counters/examples': 101216, 'counters/updates': 3163}
skipping logging after 101248 examples to avoid logging too frequently
train stats after 101280 examples: {'rewards_train/chosen': '0.1683', 'rewards_train/rejected': '0.016015', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15228', 'logps_train/rejected': '-111.42', 'logps_train/chosen': '-125.96', 'loss/train': '0.62637', 'examples_per_second': '31.128', 'grad_norm': '19.125', 'counters/examples': 101280, 'counters/updates': 3165}
skipping logging after 101312 examples to avoid logging too frequently
train stats after 101344 examples: {'rewards_train/chosen': '0.12663', 'rewards_train/rejected': '0.034119', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092515', 'logps_train/rejected': '-79.443', 'logps_train/chosen': '-106.3', 'loss/train': '0.655', 'examples_per_second': '30.335', 'grad_norm': '15.875', 'counters/examples': 101344, 'counters/updates': 3167}
skipping logging after 101376 examples to avoid logging too frequently
train stats after 101408 examples: {'rewards_train/chosen': '0.17222', 'rewards_train/rejected': '0.058771', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11345', 'logps_train/rejected': '-172.25', 'logps_train/chosen': '-153.29', 'loss/train': '0.65137', 'examples_per_second': '30.645', 'grad_norm': '22.625', 'counters/examples': 101408, 'counters/updates': 3169}
train stats after 101440 examples: {'rewards_train/chosen': '0.11729', 'rewards_train/rejected': '0.074873', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042414', 'logps_train/rejected': '-133.47', 'logps_train/chosen': '-112.9', 'loss/train': '0.68121', 'examples_per_second': '30.695', 'grad_norm': '20.375', 'counters/examples': 101440, 'counters/updates': 3170}
train stats after 101472 examples: {'rewards_train/chosen': '0.15439', 'rewards_train/rejected': '0.061845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09255', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-144.52', 'loss/train': '0.65719', 'examples_per_second': '31.667', 'grad_norm': '20.25', 'counters/examples': 101472, 'counters/updates': 3171}
train stats after 101504 examples: {'rewards_train/chosen': '0.10741', 'rewards_train/rejected': '0.010047', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097365', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-157.27', 'loss/train': '0.65399', 'examples_per_second': '31.658', 'grad_norm': '20.375', 'counters/examples': 101504, 'counters/updates': 3172}
train stats after 101536 examples: {'rewards_train/chosen': '0.15606', 'rewards_train/rejected': '0.086665', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069397', 'logps_train/rejected': '-104.18', 'logps_train/chosen': '-114.1', 'loss/train': '0.66411', 'examples_per_second': '30.541', 'grad_norm': '18.25', 'counters/examples': 101536, 'counters/updates': 3173}
train stats after 101568 examples: {'rewards_train/chosen': '0.13039', 'rewards_train/rejected': '0.013932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11645', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-145.26', 'loss/train': '0.64203', 'examples_per_second': '30.333', 'grad_norm': '20.125', 'counters/examples': 101568, 'counters/updates': 3174}
skipping logging after 101600 examples to avoid logging too frequently
train stats after 101632 examples: {'rewards_train/chosen': '0.14137', 'rewards_train/rejected': '0.061031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080341', 'logps_train/rejected': '-127.8', 'logps_train/chosen': '-142.75', 'loss/train': '0.65898', 'examples_per_second': '30.313', 'grad_norm': '20.375', 'counters/examples': 101632, 'counters/updates': 3176}
train stats after 101664 examples: {'rewards_train/chosen': '0.14876', 'rewards_train/rejected': '0.021289', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12747', 'logps_train/rejected': '-91.363', 'logps_train/chosen': '-120.66', 'loss/train': '0.64233', 'examples_per_second': '31.678', 'grad_norm': '16.875', 'counters/examples': 101664, 'counters/updates': 3177}
train stats after 101696 examples: {'rewards_train/chosen': '0.14899', 'rewards_train/rejected': '0.023016', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12598', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-102', 'loss/train': '0.63877', 'examples_per_second': '30.631', 'grad_norm': '17.375', 'counters/examples': 101696, 'counters/updates': 3178}
train stats after 101728 examples: {'rewards_train/chosen': '0.10336', 'rewards_train/rejected': '0.092661', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010697', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-129.57', 'loss/train': '0.69773', 'examples_per_second': '31.143', 'grad_norm': '21.5', 'counters/examples': 101728, 'counters/updates': 3179}
train stats after 101760 examples: {'rewards_train/chosen': '0.13843', 'rewards_train/rejected': '0.11842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02001', 'logps_train/rejected': '-108.62', 'logps_train/chosen': '-106.1', 'loss/train': '0.68839', 'examples_per_second': '31.669', 'grad_norm': '18', 'counters/examples': 101760, 'counters/updates': 3180}
train stats after 101792 examples: {'rewards_train/chosen': '0.11364', 'rewards_train/rejected': '0.069253', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044391', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-121.37', 'loss/train': '0.68103', 'examples_per_second': '31.728', 'grad_norm': '20.125', 'counters/examples': 101792, 'counters/updates': 3181}
skipping logging after 101824 examples to avoid logging too frequently
train stats after 101856 examples: {'rewards_train/chosen': '0.14316', 'rewards_train/rejected': '0.082484', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060679', 'logps_train/rejected': '-99.67', 'logps_train/chosen': '-119.85', 'loss/train': '0.67101', 'examples_per_second': '34.367', 'grad_norm': '18.5', 'counters/examples': 101856, 'counters/updates': 3183}
train stats after 101888 examples: {'rewards_train/chosen': '0.10306', 'rewards_train/rejected': '-0.028344', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13141', 'logps_train/rejected': '-129.8', 'logps_train/chosen': '-150.04', 'loss/train': '0.63743', 'examples_per_second': '32.459', 'grad_norm': '20.5', 'counters/examples': 101888, 'counters/updates': 3184}
train stats after 101920 examples: {'rewards_train/chosen': '0.11371', 'rewards_train/rejected': '0.026883', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086825', 'logps_train/rejected': '-112.74', 'logps_train/chosen': '-170.32', 'loss/train': '0.6591', 'examples_per_second': '31.596', 'grad_norm': '20', 'counters/examples': 101920, 'counters/updates': 3185}
train stats after 101952 examples: {'rewards_train/chosen': '0.050528', 'rewards_train/rejected': '-0.0056901', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056218', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-138.58', 'loss/train': '0.67333', 'examples_per_second': '30.24', 'grad_norm': '19.625', 'counters/examples': 101952, 'counters/updates': 3186}
train stats after 101984 examples: {'rewards_train/chosen': '0.15061', 'rewards_train/rejected': '0.077955', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072654', 'logps_train/rejected': '-113.26', 'logps_train/chosen': '-159.72', 'loss/train': '0.66471', 'examples_per_second': '31.603', 'grad_norm': '22', 'counters/examples': 101984, 'counters/updates': 3187}
train stats after 102016 examples: {'rewards_train/chosen': '0.070022', 'rewards_train/rejected': '0.040058', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029963', 'logps_train/rejected': '-141.08', 'logps_train/chosen': '-124.2', 'loss/train': '0.68428', 'examples_per_second': '31.653', 'grad_norm': '21.875', 'counters/examples': 102016, 'counters/updates': 3188}
skipping logging after 102048 examples to avoid logging too frequently
train stats after 102080 examples: {'rewards_train/chosen': '0.11265', 'rewards_train/rejected': '0.10287', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0097849', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-144.48', 'loss/train': '0.69764', 'examples_per_second': '31.528', 'grad_norm': '22.375', 'counters/examples': 102080, 'counters/updates': 3190}
train stats after 102112 examples: {'rewards_train/chosen': '0.13149', 'rewards_train/rejected': '-0.002075', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13357', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-152.1', 'loss/train': '0.64028', 'examples_per_second': '30.571', 'grad_norm': '20.125', 'counters/examples': 102112, 'counters/updates': 3191}
train stats after 102144 examples: {'rewards_train/chosen': '0.10264', 'rewards_train/rejected': '0.068004', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034631', 'logps_train/rejected': '-90.193', 'logps_train/chosen': '-102.72', 'loss/train': '0.68327', 'examples_per_second': '32.697', 'grad_norm': '18.375', 'counters/examples': 102144, 'counters/updates': 3192}
train stats after 102176 examples: {'rewards_train/chosen': '0.10783', 'rewards_train/rejected': '0.0026999', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10513', 'logps_train/rejected': '-104.91', 'logps_train/chosen': '-138.29', 'loss/train': '0.64789', 'examples_per_second': '31.174', 'grad_norm': '18.125', 'counters/examples': 102176, 'counters/updates': 3193}
train stats after 102208 examples: {'rewards_train/chosen': '0.13715', 'rewards_train/rejected': '0.094564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042588', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-150.2', 'loss/train': '0.68091', 'examples_per_second': '31.658', 'grad_norm': '21', 'counters/examples': 102208, 'counters/updates': 3194}
train stats after 102240 examples: {'rewards_train/chosen': '0.095244', 'rewards_train/rejected': '0.062367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032877', 'logps_train/rejected': '-113.9', 'logps_train/chosen': '-122.46', 'loss/train': '0.6854', 'examples_per_second': '32.524', 'grad_norm': '19.5', 'counters/examples': 102240, 'counters/updates': 3195}
train stats after 102272 examples: {'rewards_train/chosen': '0.062856', 'rewards_train/rejected': '0.058321', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0045348', 'logps_train/rejected': '-81.68', 'logps_train/chosen': '-124.09', 'loss/train': '0.69505', 'examples_per_second': '30.978', 'grad_norm': '18.375', 'counters/examples': 102272, 'counters/updates': 3196}
train stats after 102304 examples: {'rewards_train/chosen': '0.1314', 'rewards_train/rejected': '0.031183', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10022', 'logps_train/rejected': '-117.51', 'logps_train/chosen': '-175.99', 'loss/train': '0.65363', 'examples_per_second': '33.144', 'grad_norm': '21.25', 'counters/examples': 102304, 'counters/updates': 3197}
skipping logging after 102336 examples to avoid logging too frequently
train stats after 102368 examples: {'rewards_train/chosen': '0.15889', 'rewards_train/rejected': '-0.042332', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20122', 'logps_train/rejected': '-96.711', 'logps_train/chosen': '-117.75', 'loss/train': '0.60624', 'examples_per_second': '31.982', 'grad_norm': '17.375', 'counters/examples': 102368, 'counters/updates': 3199}
train stats after 102400 examples: {'rewards_train/chosen': '0.12317', 'rewards_train/rejected': '0.10621', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016958', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-103.05', 'loss/train': '0.69439', 'examples_per_second': '31.009', 'grad_norm': '20.375', 'counters/examples': 102400, 'counters/updates': 3200}
train stats after 102432 examples: {'rewards_train/chosen': '0.11091', 'rewards_train/rejected': '0.080247', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030667', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-133.35', 'loss/train': '0.68891', 'examples_per_second': '30.273', 'grad_norm': '21.5', 'counters/examples': 102432, 'counters/updates': 3201}
train stats after 102464 examples: {'rewards_train/chosen': '0.14882', 'rewards_train/rejected': '0.060129', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08869', 'logps_train/rejected': '-134.41', 'logps_train/chosen': '-144.83', 'loss/train': '0.65579', 'examples_per_second': '30.244', 'grad_norm': '20.625', 'counters/examples': 102464, 'counters/updates': 3202}
skipping logging after 102496 examples to avoid logging too frequently
train stats after 102528 examples: {'rewards_train/chosen': '0.16588', 'rewards_train/rejected': '0.048036', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11784', 'logps_train/rejected': '-102.06', 'logps_train/chosen': '-106.92', 'loss/train': '0.65089', 'examples_per_second': '32.752', 'grad_norm': '17.25', 'counters/examples': 102528, 'counters/updates': 3204}
train stats after 102560 examples: {'rewards_train/chosen': '0.056997', 'rewards_train/rejected': '0.04497', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012027', 'logps_train/rejected': '-139.64', 'logps_train/chosen': '-127.71', 'loss/train': '0.69767', 'examples_per_second': '24.074', 'grad_norm': '22.25', 'counters/examples': 102560, 'counters/updates': 3205}
train stats after 102592 examples: {'rewards_train/chosen': '0.195', 'rewards_train/rejected': '0.053177', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14182', 'logps_train/rejected': '-126.47', 'logps_train/chosen': '-150.23', 'loss/train': '0.63713', 'examples_per_second': '30.143', 'grad_norm': '20.625', 'counters/examples': 102592, 'counters/updates': 3206}
train stats after 102624 examples: {'rewards_train/chosen': '0.14152', 'rewards_train/rejected': '0.080581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060938', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-155.49', 'loss/train': '0.67525', 'examples_per_second': '30.288', 'grad_norm': '21.375', 'counters/examples': 102624, 'counters/updates': 3207}
train stats after 102656 examples: {'rewards_train/chosen': '0.13205', 'rewards_train/rejected': '0.05045', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081598', 'logps_train/rejected': '-138.9', 'logps_train/chosen': '-152.63', 'loss/train': '0.65918', 'examples_per_second': '24.1', 'grad_norm': '21', 'counters/examples': 102656, 'counters/updates': 3208}
train stats after 102688 examples: {'rewards_train/chosen': '0.081344', 'rewards_train/rejected': '0.054486', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026858', 'logps_train/rejected': '-132.03', 'logps_train/chosen': '-135.98', 'loss/train': '0.68643', 'examples_per_second': '32.773', 'grad_norm': '21.625', 'counters/examples': 102688, 'counters/updates': 3209}
train stats after 102720 examples: {'rewards_train/chosen': '0.089559', 'rewards_train/rejected': '-0.0054663', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095026', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-141.62', 'loss/train': '0.65302', 'examples_per_second': '31.34', 'grad_norm': '20.75', 'counters/examples': 102720, 'counters/updates': 3210}
train stats after 102752 examples: {'rewards_train/chosen': '0.18044', 'rewards_train/rejected': '0.059959', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12048', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-128.99', 'loss/train': '0.64139', 'examples_per_second': '31.681', 'grad_norm': '19.25', 'counters/examples': 102752, 'counters/updates': 3211}
train stats after 102784 examples: {'rewards_train/chosen': '0.085847', 'rewards_train/rejected': '0.0013379', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08451', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-137.29', 'loss/train': '0.65731', 'examples_per_second': '31.089', 'grad_norm': '20', 'counters/examples': 102784, 'counters/updates': 3212}
train stats after 102816 examples: {'rewards_train/chosen': '0.22159', 'rewards_train/rejected': '0.085598', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13599', 'logps_train/rejected': '-127.74', 'logps_train/chosen': '-152.71', 'loss/train': '0.63991', 'examples_per_second': '31.692', 'grad_norm': '20.25', 'counters/examples': 102816, 'counters/updates': 3213}
train stats after 102848 examples: {'rewards_train/chosen': '0.093269', 'rewards_train/rejected': '0.069375', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023894', 'logps_train/rejected': '-108.15', 'logps_train/chosen': '-128.31', 'loss/train': '0.68733', 'examples_per_second': '31.266', 'grad_norm': '20.125', 'counters/examples': 102848, 'counters/updates': 3214}
skipping logging after 102880 examples to avoid logging too frequently
train stats after 102912 examples: {'rewards_train/chosen': '0.15621', 'rewards_train/rejected': '0.041395', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11481', 'logps_train/rejected': '-122.08', 'logps_train/chosen': '-150.43', 'loss/train': '0.64666', 'examples_per_second': '31.625', 'grad_norm': '21.125', 'counters/examples': 102912, 'counters/updates': 3216}
train stats after 102944 examples: {'rewards_train/chosen': '0.15825', 'rewards_train/rejected': '0.053849', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1044', 'logps_train/rejected': '-88.138', 'logps_train/chosen': '-118.59', 'loss/train': '0.65118', 'examples_per_second': '30.979', 'grad_norm': '16.875', 'counters/examples': 102944, 'counters/updates': 3217}
train stats after 102976 examples: {'rewards_train/chosen': '0.078783', 'rewards_train/rejected': '0.092955', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014173', 'logps_train/rejected': '-161.42', 'logps_train/chosen': '-138.73', 'loss/train': '0.70589', 'examples_per_second': '30.889', 'grad_norm': '23', 'counters/examples': 102976, 'counters/updates': 3218}
train stats after 103008 examples: {'rewards_train/chosen': '0.067283', 'rewards_train/rejected': '0.01959', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047693', 'logps_train/rejected': '-112.01', 'logps_train/chosen': '-113.75', 'loss/train': '0.67667', 'examples_per_second': '30.291', 'grad_norm': '18.625', 'counters/examples': 103008, 'counters/updates': 3219}
train stats after 103040 examples: {'rewards_train/chosen': '0.12621', 'rewards_train/rejected': '0.013344', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11286', 'logps_train/rejected': '-180.82', 'logps_train/chosen': '-177.14', 'loss/train': '0.64547', 'examples_per_second': '31.682', 'grad_norm': '23.125', 'counters/examples': 103040, 'counters/updates': 3220}
skipping logging after 103072 examples to avoid logging too frequently
train stats after 103104 examples: {'rewards_train/chosen': '0.098152', 'rewards_train/rejected': '0.085628', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012525', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-131.59', 'loss/train': '0.69952', 'examples_per_second': '30.042', 'grad_norm': '20.375', 'counters/examples': 103104, 'counters/updates': 3222}
skipping logging after 103136 examples to avoid logging too frequently
train stats after 103168 examples: {'rewards_train/chosen': '0.040806', 'rewards_train/rejected': '0.022936', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017869', 'logps_train/rejected': '-143.3', 'logps_train/chosen': '-164.87', 'loss/train': '0.68988', 'examples_per_second': '30.185', 'grad_norm': '23.875', 'counters/examples': 103168, 'counters/updates': 3224}
train stats after 103200 examples: {'rewards_train/chosen': '0.1646', 'rewards_train/rejected': '0.068368', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096231', 'logps_train/rejected': '-97.577', 'logps_train/chosen': '-132.98', 'loss/train': '0.6583', 'examples_per_second': '33.121', 'grad_norm': '19.375', 'counters/examples': 103200, 'counters/updates': 3225}
train stats after 103232 examples: {'rewards_train/chosen': '0.10005', 'rewards_train/rejected': '0.034306', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065746', 'logps_train/rejected': '-103.26', 'logps_train/chosen': '-116.04', 'loss/train': '0.67015', 'examples_per_second': '32.792', 'grad_norm': '18.75', 'counters/examples': 103232, 'counters/updates': 3226}
train stats after 103264 examples: {'rewards_train/chosen': '0.13765', 'rewards_train/rejected': '0.093237', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044414', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-146.46', 'loss/train': '0.68038', 'examples_per_second': '30.24', 'grad_norm': '22', 'counters/examples': 103264, 'counters/updates': 3227}
train stats after 103296 examples: {'rewards_train/chosen': '0.17734', 'rewards_train/rejected': '0.12124', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056096', 'logps_train/rejected': '-129.67', 'logps_train/chosen': '-171.92', 'loss/train': '0.66961', 'examples_per_second': '30.228', 'grad_norm': '21.25', 'counters/examples': 103296, 'counters/updates': 3228}
train stats after 103328 examples: {'rewards_train/chosen': '0.13495', 'rewards_train/rejected': '0.053345', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081604', 'logps_train/rejected': '-141.86', 'logps_train/chosen': '-129.2', 'loss/train': '0.65939', 'examples_per_second': '31.636', 'grad_norm': '20.5', 'counters/examples': 103328, 'counters/updates': 3229}
train stats after 103360 examples: {'rewards_train/chosen': '0.13039', 'rewards_train/rejected': '0.13213', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0017345', 'logps_train/rejected': '-111.96', 'logps_train/chosen': '-114.37', 'loss/train': '0.70051', 'examples_per_second': '31.064', 'grad_norm': '20.125', 'counters/examples': 103360, 'counters/updates': 3230}
train stats after 103392 examples: {'rewards_train/chosen': '0.10724', 'rewards_train/rejected': '-0.029012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13625', 'logps_train/rejected': '-94.471', 'logps_train/chosen': '-148.8', 'loss/train': '0.64014', 'examples_per_second': '32.837', 'grad_norm': '18.625', 'counters/examples': 103392, 'counters/updates': 3231}
skipping logging after 103424 examples to avoid logging too frequently
train stats after 103456 examples: {'rewards_train/chosen': '0.16087', 'rewards_train/rejected': '0.068767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092099', 'logps_train/rejected': '-130.8', 'logps_train/chosen': '-157.18', 'loss/train': '0.66009', 'examples_per_second': '33.895', 'grad_norm': '20.875', 'counters/examples': 103456, 'counters/updates': 3233}
train stats after 103488 examples: {'rewards_train/chosen': '0.092463', 'rewards_train/rejected': '-0.060301', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15276', 'logps_train/rejected': '-154.74', 'logps_train/chosen': '-108.86', 'loss/train': '0.63399', 'examples_per_second': '31.155', 'grad_norm': '20.875', 'counters/examples': 103488, 'counters/updates': 3234}
skipping logging after 103520 examples to avoid logging too frequently
train stats after 103552 examples: {'rewards_train/chosen': '0.10705', 'rewards_train/rejected': '0.026904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080142', 'logps_train/rejected': '-140.99', 'logps_train/chosen': '-148.79', 'loss/train': '0.66234', 'examples_per_second': '33.169', 'grad_norm': '20.875', 'counters/examples': 103552, 'counters/updates': 3236}
skipping logging after 103584 examples to avoid logging too frequently
train stats after 103616 examples: {'rewards_train/chosen': '0.10677', 'rewards_train/rejected': '0.0018453', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10492', 'logps_train/rejected': '-93.151', 'logps_train/chosen': '-131.52', 'loss/train': '0.64872', 'examples_per_second': '34.52', 'grad_norm': '17.75', 'counters/examples': 103616, 'counters/updates': 3238}
train stats after 103648 examples: {'rewards_train/chosen': '0.13952', 'rewards_train/rejected': '0.0082986', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.13122', 'logps_train/rejected': '-87.099', 'logps_train/chosen': '-137.85', 'loss/train': '0.63252', 'examples_per_second': '31.846', 'grad_norm': '18', 'counters/examples': 103648, 'counters/updates': 3239}
train stats after 103680 examples: {'rewards_train/chosen': '0.13671', 'rewards_train/rejected': '0.063131', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073581', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-178.81', 'loss/train': '0.66898', 'examples_per_second': '29.891', 'grad_norm': '21.75', 'counters/examples': 103680, 'counters/updates': 3240}
train stats after 103712 examples: {'rewards_train/chosen': '0.14525', 'rewards_train/rejected': '0.0073468', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1379', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-99.37', 'loss/train': '0.6355', 'examples_per_second': '31.003', 'grad_norm': '17', 'counters/examples': 103712, 'counters/updates': 3241}
skipping logging after 103744 examples to avoid logging too frequently
train stats after 103776 examples: {'rewards_train/chosen': '0.1203', 'rewards_train/rejected': '0.029654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09065', 'logps_train/rejected': '-96.871', 'logps_train/chosen': '-119.03', 'loss/train': '0.65422', 'examples_per_second': '31.656', 'grad_norm': '19.375', 'counters/examples': 103776, 'counters/updates': 3243}
skipping logging after 103808 examples to avoid logging too frequently
train stats after 103840 examples: {'rewards_train/chosen': '0.14412', 'rewards_train/rejected': '0.073683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070436', 'logps_train/rejected': '-101.16', 'logps_train/chosen': '-115.11', 'loss/train': '0.66796', 'examples_per_second': '30.996', 'grad_norm': '17.875', 'counters/examples': 103840, 'counters/updates': 3245}
train stats after 103872 examples: {'rewards_train/chosen': '0.093825', 'rewards_train/rejected': '0.0090131', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084812', 'logps_train/rejected': '-104.06', 'logps_train/chosen': '-89.955', 'loss/train': '0.65508', 'examples_per_second': '31.62', 'grad_norm': '17', 'counters/examples': 103872, 'counters/updates': 3246}
train stats after 103904 examples: {'rewards_train/chosen': '0.15177', 'rewards_train/rejected': '0.10972', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042048', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-138.9', 'loss/train': '0.68174', 'examples_per_second': '30.855', 'grad_norm': '21.25', 'counters/examples': 103904, 'counters/updates': 3247}
train stats after 103936 examples: {'rewards_train/chosen': '0.18312', 'rewards_train/rejected': '0.1063', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076821', 'logps_train/rejected': '-192.71', 'logps_train/chosen': '-182.71', 'loss/train': '0.66425', 'examples_per_second': '30.542', 'grad_norm': '22.875', 'counters/examples': 103936, 'counters/updates': 3248}
train stats after 103968 examples: {'rewards_train/chosen': '0.16644', 'rewards_train/rejected': '-0.00094953', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16739', 'logps_train/rejected': '-103.64', 'logps_train/chosen': '-123.44', 'loss/train': '0.62666', 'examples_per_second': '31.572', 'grad_norm': '17.625', 'counters/examples': 103968, 'counters/updates': 3249}
train stats after 104000 examples: {'rewards_train/chosen': '0.14916', 'rewards_train/rejected': '0.043693', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10547', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-132.72', 'loss/train': '0.64665', 'examples_per_second': '30.615', 'grad_norm': '19.25', 'counters/examples': 104000, 'counters/updates': 3250}
train stats after 104032 examples: {'rewards_train/chosen': '0.12014', 'rewards_train/rejected': '0.052533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06761', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-181.12', 'loss/train': '0.677', 'examples_per_second': '31.267', 'grad_norm': '23.5', 'counters/examples': 104032, 'counters/updates': 3251}
train stats after 104064 examples: {'rewards_train/chosen': '0.12774', 'rewards_train/rejected': '0.064024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063713', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-145.55', 'loss/train': '0.67538', 'examples_per_second': '31.153', 'grad_norm': '22.5', 'counters/examples': 104064, 'counters/updates': 3252}
skipping logging after 104096 examples to avoid logging too frequently
train stats after 104128 examples: {'rewards_train/chosen': '0.072618', 'rewards_train/rejected': '0.0068341', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.065784', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-163.17', 'loss/train': '0.66837', 'examples_per_second': '30.049', 'grad_norm': '21.25', 'counters/examples': 104128, 'counters/updates': 3254}
skipping logging after 104160 examples to avoid logging too frequently
train stats after 104192 examples: {'rewards_train/chosen': '0.16882', 'rewards_train/rejected': '0.079616', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089207', 'logps_train/rejected': '-138.63', 'logps_train/chosen': '-145.74', 'loss/train': '0.66142', 'examples_per_second': '32.231', 'grad_norm': '20.625', 'counters/examples': 104192, 'counters/updates': 3256}
train stats after 104224 examples: {'rewards_train/chosen': '0.09697', 'rewards_train/rejected': '0.0060117', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090958', 'logps_train/rejected': '-135.76', 'logps_train/chosen': '-115.62', 'loss/train': '0.65405', 'examples_per_second': '31.655', 'grad_norm': '19.5', 'counters/examples': 104224, 'counters/updates': 3257}
train stats after 104256 examples: {'rewards_train/chosen': '0.1121', 'rewards_train/rejected': '0.04862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063483', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-128.54', 'loss/train': '0.67109', 'examples_per_second': '30.189', 'grad_norm': '20', 'counters/examples': 104256, 'counters/updates': 3258}
skipping logging after 104288 examples to avoid logging too frequently
train stats after 104320 examples: {'rewards_train/chosen': '0.12104', 'rewards_train/rejected': '0.018602', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10243', 'logps_train/rejected': '-120.35', 'logps_train/chosen': '-182.68', 'loss/train': '0.64943', 'examples_per_second': '31.606', 'grad_norm': '23.375', 'counters/examples': 104320, 'counters/updates': 3260}
train stats after 104352 examples: {'rewards_train/chosen': '0.082724', 'rewards_train/rejected': '0.026017', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056706', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-99.541', 'loss/train': '0.67085', 'examples_per_second': '31.348', 'grad_norm': '20.375', 'counters/examples': 104352, 'counters/updates': 3261}
train stats after 104384 examples: {'rewards_train/chosen': '0.081193', 'rewards_train/rejected': '0.058586', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022607', 'logps_train/rejected': '-103.02', 'logps_train/chosen': '-110.63', 'loss/train': '0.68831', 'examples_per_second': '31.615', 'grad_norm': '19.875', 'counters/examples': 104384, 'counters/updates': 3262}
train stats after 104416 examples: {'rewards_train/chosen': '0.096472', 'rewards_train/rejected': '0.09068', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0057919', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-123.36', 'loss/train': '0.69778', 'examples_per_second': '30.674', 'grad_norm': '20', 'counters/examples': 104416, 'counters/updates': 3263}
train stats after 104448 examples: {'rewards_train/chosen': '0.12349', 'rewards_train/rejected': '0.01946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10403', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-123.25', 'loss/train': '0.64999', 'examples_per_second': '30.912', 'grad_norm': '19.75', 'counters/examples': 104448, 'counters/updates': 3264}
skipping logging after 104480 examples to avoid logging too frequently
train stats after 104512 examples: {'rewards_train/chosen': '0.16057', 'rewards_train/rejected': '0.031975', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1286', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-128.15', 'loss/train': '0.64277', 'examples_per_second': '31.63', 'grad_norm': '19.75', 'counters/examples': 104512, 'counters/updates': 3266}
train stats after 104544 examples: {'rewards_train/chosen': '0.15564', 'rewards_train/rejected': '-0.021749', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17739', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-133.63', 'loss/train': '0.62093', 'examples_per_second': '31.648', 'grad_norm': '19.625', 'counters/examples': 104544, 'counters/updates': 3267}
train stats after 104576 examples: {'rewards_train/chosen': '0.20273', 'rewards_train/rejected': '0.014188', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18855', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-125.92', 'loss/train': '0.61468', 'examples_per_second': '31.58', 'grad_norm': '18.5', 'counters/examples': 104576, 'counters/updates': 3268}
train stats after 104608 examples: {'rewards_train/chosen': '0.15873', 'rewards_train/rejected': '0.074629', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084098', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-149.38', 'loss/train': '0.66421', 'examples_per_second': '31.608', 'grad_norm': '20.75', 'counters/examples': 104608, 'counters/updates': 3269}
train stats after 104640 examples: {'rewards_train/chosen': '0.11462', 'rewards_train/rejected': '0.056603', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058017', 'logps_train/rejected': '-94.515', 'logps_train/chosen': '-115.82', 'loss/train': '0.67055', 'examples_per_second': '22.401', 'grad_norm': '17.125', 'counters/examples': 104640, 'counters/updates': 3270}
train stats after 104672 examples: {'rewards_train/chosen': '0.15476', 'rewards_train/rejected': '0.017122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13764', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-153.78', 'loss/train': '0.64324', 'examples_per_second': '29.988', 'grad_norm': '20.5', 'counters/examples': 104672, 'counters/updates': 3271}
train stats after 104704 examples: {'rewards_train/chosen': '0.15471', 'rewards_train/rejected': '0.071501', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08321', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-143.65', 'loss/train': '0.65717', 'examples_per_second': '31.578', 'grad_norm': '20.375', 'counters/examples': 104704, 'counters/updates': 3272}
train stats after 104736 examples: {'rewards_train/chosen': '0.074806', 'rewards_train/rejected': '0.069949', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0048576', 'logps_train/rejected': '-109.71', 'logps_train/chosen': '-117.73', 'loss/train': '0.69659', 'examples_per_second': '31.617', 'grad_norm': '20.75', 'counters/examples': 104736, 'counters/updates': 3273}
train stats after 104768 examples: {'rewards_train/chosen': '0.1541', 'rewards_train/rejected': '0.010518', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14358', 'logps_train/rejected': '-118.5', 'logps_train/chosen': '-148.41', 'loss/train': '0.6299', 'examples_per_second': '32.209', 'grad_norm': '20', 'counters/examples': 104768, 'counters/updates': 3274}
train stats after 104800 examples: {'rewards_train/chosen': '0.11776', 'rewards_train/rejected': '0.017915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09985', 'logps_train/rejected': '-106.88', 'logps_train/chosen': '-120.35', 'loss/train': '0.65315', 'examples_per_second': '31.664', 'grad_norm': '17.75', 'counters/examples': 104800, 'counters/updates': 3275}
train stats after 104832 examples: {'rewards_train/chosen': '0.12416', 'rewards_train/rejected': '0.017363', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1068', 'logps_train/rejected': '-101.94', 'logps_train/chosen': '-145.84', 'loss/train': '0.64859', 'examples_per_second': '31.237', 'grad_norm': '19.875', 'counters/examples': 104832, 'counters/updates': 3276}
skipping logging after 104864 examples to avoid logging too frequently
train stats after 104896 examples: {'rewards_train/chosen': '0.1177', 'rewards_train/rejected': '0.099117', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018584', 'logps_train/rejected': '-109.71', 'logps_train/chosen': '-133.66', 'loss/train': '0.69123', 'examples_per_second': '33.194', 'grad_norm': '20.25', 'counters/examples': 104896, 'counters/updates': 3278}
train stats after 104928 examples: {'rewards_train/chosen': '0.21358', 'rewards_train/rejected': '0.083155', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13042', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-137.31', 'loss/train': '0.63988', 'examples_per_second': '31.598', 'grad_norm': '20.125', 'counters/examples': 104928, 'counters/updates': 3279}
train stats after 104960 examples: {'rewards_train/chosen': '0.20108', 'rewards_train/rejected': '0.1057', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095376', 'logps_train/rejected': '-105.07', 'logps_train/chosen': '-152.36', 'loss/train': '0.65742', 'examples_per_second': '31.605', 'grad_norm': '19.25', 'counters/examples': 104960, 'counters/updates': 3280}
train stats after 104992 examples: {'rewards_train/chosen': '0.14368', 'rewards_train/rejected': '0.0094523', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13423', 'logps_train/rejected': '-146.09', 'logps_train/chosen': '-132.62', 'loss/train': '0.63645', 'examples_per_second': '33.066', 'grad_norm': '20', 'counters/examples': 104992, 'counters/updates': 3281}
train stats after 105024 examples: {'rewards_train/chosen': '0.15195', 'rewards_train/rejected': '0.093076', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058873', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-134.58', 'loss/train': '0.66987', 'examples_per_second': '30.332', 'grad_norm': '19', 'counters/examples': 105024, 'counters/updates': 3282}
train stats after 105056 examples: {'rewards_train/chosen': '0.14543', 'rewards_train/rejected': '0.027745', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11768', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-144.72', 'loss/train': '0.64195', 'examples_per_second': '30.195', 'grad_norm': '20.75', 'counters/examples': 105056, 'counters/updates': 3283}
train stats after 105088 examples: {'rewards_train/chosen': '0.11074', 'rewards_train/rejected': '0.080275', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030465', 'logps_train/rejected': '-145.5', 'logps_train/chosen': '-136.76', 'loss/train': '0.68456', 'examples_per_second': '31.628', 'grad_norm': '22.5', 'counters/examples': 105088, 'counters/updates': 3284}
train stats after 105120 examples: {'rewards_train/chosen': '0.18208', 'rewards_train/rejected': '0.072161', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10992', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-129.79', 'loss/train': '0.65495', 'examples_per_second': '31.682', 'grad_norm': '20.875', 'counters/examples': 105120, 'counters/updates': 3285}
train stats after 105152 examples: {'rewards_train/chosen': '0.16295', 'rewards_train/rejected': '0.048486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11446', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-118.43', 'loss/train': '0.64481', 'examples_per_second': '30.602', 'grad_norm': '18.375', 'counters/examples': 105152, 'counters/updates': 3286}
train stats after 105184 examples: {'rewards_train/chosen': '0.083176', 'rewards_train/rejected': '0.028823', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054353', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-143.83', 'loss/train': '0.67577', 'examples_per_second': '31.612', 'grad_norm': '22.875', 'counters/examples': 105184, 'counters/updates': 3287}
train stats after 105216 examples: {'rewards_train/chosen': '0.15003', 'rewards_train/rejected': '0.15287', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0028488', 'logps_train/rejected': '-137.5', 'logps_train/chosen': '-110.04', 'loss/train': '0.7064', 'examples_per_second': '33.141', 'grad_norm': '19.625', 'counters/examples': 105216, 'counters/updates': 3288}
train stats after 105248 examples: {'rewards_train/chosen': '0.021644', 'rewards_train/rejected': '0.009868', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011776', 'logps_train/rejected': '-112.41', 'logps_train/chosen': '-100.65', 'loss/train': '0.69448', 'examples_per_second': '31.161', 'grad_norm': '17.875', 'counters/examples': 105248, 'counters/updates': 3289}
train stats after 105280 examples: {'rewards_train/chosen': '0.10086', 'rewards_train/rejected': '0.010852', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090004', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-121.78', 'loss/train': '0.65814', 'examples_per_second': '31.542', 'grad_norm': '20.125', 'counters/examples': 105280, 'counters/updates': 3290}
train stats after 105312 examples: {'rewards_train/chosen': '0.16823', 'rewards_train/rejected': '0.086837', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.081392', 'logps_train/rejected': '-150.6', 'logps_train/chosen': '-137.47', 'loss/train': '0.66807', 'examples_per_second': '31.666', 'grad_norm': '20.75', 'counters/examples': 105312, 'counters/updates': 3291}
train stats after 105344 examples: {'rewards_train/chosen': '0.12758', 'rewards_train/rejected': '0.063444', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064134', 'logps_train/rejected': '-102.79', 'logps_train/chosen': '-142.63', 'loss/train': '0.67085', 'examples_per_second': '30.726', 'grad_norm': '20.875', 'counters/examples': 105344, 'counters/updates': 3292}
train stats after 105376 examples: {'rewards_train/chosen': '0.11855', 'rewards_train/rejected': '0.11623', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0023191', 'logps_train/rejected': '-153.22', 'logps_train/chosen': '-131.94', 'loss/train': '0.69882', 'examples_per_second': '32.335', 'grad_norm': '21.625', 'counters/examples': 105376, 'counters/updates': 3293}
skipping logging after 105408 examples to avoid logging too frequently
train stats after 105440 examples: {'rewards_train/chosen': '0.12915', 'rewards_train/rejected': '-0.0020782', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13122', 'logps_train/rejected': '-100.48', 'logps_train/chosen': '-121.38', 'loss/train': '0.63785', 'examples_per_second': '33.456', 'grad_norm': '17.875', 'counters/examples': 105440, 'counters/updates': 3295}
skipping logging after 105472 examples to avoid logging too frequently
train stats after 105504 examples: {'rewards_train/chosen': '0.10537', 'rewards_train/rejected': '-0.014543', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11992', 'logps_train/rejected': '-85.071', 'logps_train/chosen': '-119.57', 'loss/train': '0.63976', 'examples_per_second': '31.886', 'grad_norm': '17.625', 'counters/examples': 105504, 'counters/updates': 3297}
skipping logging after 105536 examples to avoid logging too frequently
train stats after 105568 examples: {'rewards_train/chosen': '0.11048', 'rewards_train/rejected': '0.009954', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10053', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-128.1', 'loss/train': '0.65062', 'examples_per_second': '31.603', 'grad_norm': '18', 'counters/examples': 105568, 'counters/updates': 3299}
train stats after 105600 examples: {'rewards_train/chosen': '0.10832', 'rewards_train/rejected': '0.060776', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04754', 'logps_train/rejected': '-93.167', 'logps_train/chosen': '-133.19', 'loss/train': '0.67583', 'examples_per_second': '31.546', 'grad_norm': '20.25', 'counters/examples': 105600, 'counters/updates': 3300}
train stats after 105632 examples: {'rewards_train/chosen': '0.15211', 'rewards_train/rejected': '0.07496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077154', 'logps_train/rejected': '-91.626', 'logps_train/chosen': '-142.12', 'loss/train': '0.66014', 'examples_per_second': '32.368', 'grad_norm': '19.125', 'counters/examples': 105632, 'counters/updates': 3301}
train stats after 105664 examples: {'rewards_train/chosen': '0.10432', 'rewards_train/rejected': '0.001731', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10259', 'logps_train/rejected': '-91.74', 'logps_train/chosen': '-102.74', 'loss/train': '0.64833', 'examples_per_second': '31.603', 'grad_norm': '16.875', 'counters/examples': 105664, 'counters/updates': 3302}
train stats after 105696 examples: {'rewards_train/chosen': '0.13618', 'rewards_train/rejected': '0.062903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073273', 'logps_train/rejected': '-96.812', 'logps_train/chosen': '-128.04', 'loss/train': '0.66571', 'examples_per_second': '31.219', 'grad_norm': '18.625', 'counters/examples': 105696, 'counters/updates': 3303}
train stats after 105728 examples: {'rewards_train/chosen': '0.15453', 'rewards_train/rejected': '0.037449', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11709', 'logps_train/rejected': '-92.546', 'logps_train/chosen': '-144.77', 'loss/train': '0.64363', 'examples_per_second': '31.377', 'grad_norm': '19.125', 'counters/examples': 105728, 'counters/updates': 3304}
train stats after 105760 examples: {'rewards_train/chosen': '0.27508', 'rewards_train/rejected': '0.075839', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19924', 'logps_train/rejected': '-110.36', 'logps_train/chosen': '-163.31', 'loss/train': '0.60726', 'examples_per_second': '30.566', 'grad_norm': '19.375', 'counters/examples': 105760, 'counters/updates': 3305}
train stats after 105792 examples: {'rewards_train/chosen': '0.1706', 'rewards_train/rejected': '0.055529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11507', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-192.67', 'loss/train': '0.64883', 'examples_per_second': '30.624', 'grad_norm': '21.625', 'counters/examples': 105792, 'counters/updates': 3306}
train stats after 105824 examples: {'rewards_train/chosen': '0.14851', 'rewards_train/rejected': '0.048168', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10035', 'logps_train/rejected': '-121.75', 'logps_train/chosen': '-137.29', 'loss/train': '0.65613', 'examples_per_second': '30.276', 'grad_norm': '20', 'counters/examples': 105824, 'counters/updates': 3307}
train stats after 105856 examples: {'rewards_train/chosen': '0.089233', 'rewards_train/rejected': '0.094348', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0051154', 'logps_train/rejected': '-109.76', 'logps_train/chosen': '-152.97', 'loss/train': '0.70604', 'examples_per_second': '30.09', 'grad_norm': '22.125', 'counters/examples': 105856, 'counters/updates': 3308}
train stats after 105888 examples: {'rewards_train/chosen': '0.12021', 'rewards_train/rejected': '0.041154', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079061', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-133.45', 'loss/train': '0.66821', 'examples_per_second': '31.542', 'grad_norm': '20.25', 'counters/examples': 105888, 'counters/updates': 3309}
train stats after 105920 examples: {'rewards_train/chosen': '0.17798', 'rewards_train/rejected': '0.038743', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13923', 'logps_train/rejected': '-126.64', 'logps_train/chosen': '-160.49', 'loss/train': '0.63563', 'examples_per_second': '30.671', 'grad_norm': '20.75', 'counters/examples': 105920, 'counters/updates': 3310}
train stats after 105952 examples: {'rewards_train/chosen': '0.1862', 'rewards_train/rejected': '0.1159', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070303', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-150.44', 'loss/train': '0.66819', 'examples_per_second': '31.245', 'grad_norm': '20.875', 'counters/examples': 105952, 'counters/updates': 3311}
skipping logging after 105984 examples to avoid logging too frequently
train stats after 106016 examples: {'rewards_train/chosen': '0.14796', 'rewards_train/rejected': '0.033477', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11449', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-180.84', 'loss/train': '0.64691', 'examples_per_second': '31.586', 'grad_norm': '22', 'counters/examples': 106016, 'counters/updates': 3313}
train stats after 106048 examples: {'rewards_train/chosen': '0.14978', 'rewards_train/rejected': '-0.0061895', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15597', 'logps_train/rejected': '-97.752', 'logps_train/chosen': '-147.46', 'loss/train': '0.62546', 'examples_per_second': '30.227', 'grad_norm': '19.75', 'counters/examples': 106048, 'counters/updates': 3314}
train stats after 106080 examples: {'rewards_train/chosen': '0.085967', 'rewards_train/rejected': '0.091146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0051784', 'logps_train/rejected': '-103.86', 'logps_train/chosen': '-134.28', 'loss/train': '0.70282', 'examples_per_second': '31.585', 'grad_norm': '19.625', 'counters/examples': 106080, 'counters/updates': 3315}
skipping logging after 106112 examples to avoid logging too frequently
train stats after 106144 examples: {'rewards_train/chosen': '0.066138', 'rewards_train/rejected': '0.0014357', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064702', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-153.74', 'loss/train': '0.66732', 'examples_per_second': '31.915', 'grad_norm': '21.125', 'counters/examples': 106144, 'counters/updates': 3317}
skipping logging after 106176 examples to avoid logging too frequently
train stats after 106208 examples: {'rewards_train/chosen': '0.10264', 'rewards_train/rejected': '0.066868', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035776', 'logps_train/rejected': '-90.628', 'logps_train/chosen': '-117.73', 'loss/train': '0.68339', 'examples_per_second': '30.531', 'grad_norm': '19', 'counters/examples': 106208, 'counters/updates': 3319}
train stats after 106240 examples: {'rewards_train/chosen': '0.14937', 'rewards_train/rejected': '0.09098', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058387', 'logps_train/rejected': '-92.828', 'logps_train/chosen': '-127.72', 'loss/train': '0.67183', 'examples_per_second': '31.124', 'grad_norm': '18.625', 'counters/examples': 106240, 'counters/updates': 3320}
train stats after 106272 examples: {'rewards_train/chosen': '0.14906', 'rewards_train/rejected': '0.10129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047768', 'logps_train/rejected': '-104.64', 'logps_train/chosen': '-127.92', 'loss/train': '0.67376', 'examples_per_second': '31.983', 'grad_norm': '20.625', 'counters/examples': 106272, 'counters/updates': 3321}
skipping logging after 106304 examples to avoid logging too frequently
train stats after 106336 examples: {'rewards_train/chosen': '0.12925', 'rewards_train/rejected': '0.085027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044219', 'logps_train/rejected': '-102.49', 'logps_train/chosen': '-111.3', 'loss/train': '0.6783', 'examples_per_second': '33.089', 'grad_norm': '18.625', 'counters/examples': 106336, 'counters/updates': 3323}
train stats after 106368 examples: {'rewards_train/chosen': '0.13844', 'rewards_train/rejected': '0.062381', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076059', 'logps_train/rejected': '-104.56', 'logps_train/chosen': '-140.85', 'loss/train': '0.66537', 'examples_per_second': '31.645', 'grad_norm': '18.75', 'counters/examples': 106368, 'counters/updates': 3324}
train stats after 106400 examples: {'rewards_train/chosen': '0.12353', 'rewards_train/rejected': '-0.027402', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15093', 'logps_train/rejected': '-120.93', 'logps_train/chosen': '-131.85', 'loss/train': '0.62806', 'examples_per_second': '30.069', 'grad_norm': '20', 'counters/examples': 106400, 'counters/updates': 3325}
skipping logging after 106432 examples to avoid logging too frequently
train stats after 106464 examples: {'rewards_train/chosen': '0.1239', 'rewards_train/rejected': '0.10793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015964', 'logps_train/rejected': '-124.03', 'logps_train/chosen': '-161.45', 'loss/train': '0.6941', 'examples_per_second': '29.891', 'grad_norm': '21.75', 'counters/examples': 106464, 'counters/updates': 3327}
train stats after 106496 examples: {'rewards_train/chosen': '0.14721', 'rewards_train/rejected': '0.0015313', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14568', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-126.83', 'loss/train': '0.6395', 'examples_per_second': '31.346', 'grad_norm': '19.375', 'counters/examples': 106496, 'counters/updates': 3328}
train stats after 106528 examples: {'rewards_train/chosen': '0.13845', 'rewards_train/rejected': '0.054823', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.083629', 'logps_train/rejected': '-154.27', 'logps_train/chosen': '-172.63', 'loss/train': '0.65518', 'examples_per_second': '31.585', 'grad_norm': '22.875', 'counters/examples': 106528, 'counters/updates': 3329}
train stats after 106560 examples: {'rewards_train/chosen': '0.098226', 'rewards_train/rejected': '0.063252', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034974', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-155.94', 'loss/train': '0.68687', 'examples_per_second': '31.084', 'grad_norm': '22.375', 'counters/examples': 106560, 'counters/updates': 3330}
skipping logging after 106592 examples to avoid logging too frequently
train stats after 106624 examples: {'rewards_train/chosen': '0.17149', 'rewards_train/rejected': '0.070725', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10077', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-112.19', 'loss/train': '0.65278', 'examples_per_second': '31.591', 'grad_norm': '18.75', 'counters/examples': 106624, 'counters/updates': 3332}
train stats after 106656 examples: {'rewards_train/chosen': '0.094499', 'rewards_train/rejected': '0.052113', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042387', 'logps_train/rejected': '-120.15', 'logps_train/chosen': '-123.01', 'loss/train': '0.68131', 'examples_per_second': '32.04', 'grad_norm': '20', 'counters/examples': 106656, 'counters/updates': 3333}
train stats after 106688 examples: {'rewards_train/chosen': '0.056139', 'rewards_train/rejected': '0.0088125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047326', 'logps_train/rejected': '-109.02', 'logps_train/chosen': '-135.9', 'loss/train': '0.67558', 'examples_per_second': '31.647', 'grad_norm': '19.5', 'counters/examples': 106688, 'counters/updates': 3334}
train stats after 106720 examples: {'rewards_train/chosen': '0.17966', 'rewards_train/rejected': '0.10779', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071874', 'logps_train/rejected': '-113.18', 'logps_train/chosen': '-130.99', 'loss/train': '0.66799', 'examples_per_second': '30.079', 'grad_norm': '18.625', 'counters/examples': 106720, 'counters/updates': 3335}
train stats after 106752 examples: {'rewards_train/chosen': '0.13695', 'rewards_train/rejected': '0.045747', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091199', 'logps_train/rejected': '-107.82', 'logps_train/chosen': '-133.12', 'loss/train': '0.66382', 'examples_per_second': '31.427', 'grad_norm': '20.125', 'counters/examples': 106752, 'counters/updates': 3336}
skipping logging after 106784 examples to avoid logging too frequently
train stats after 106816 examples: {'rewards_train/chosen': '0.04879', 'rewards_train/rejected': '0.012145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036645', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-100.64', 'loss/train': '0.68145', 'examples_per_second': '31.596', 'grad_norm': '18.375', 'counters/examples': 106816, 'counters/updates': 3338}
train stats after 106848 examples: {'rewards_train/chosen': '0.14261', 'rewards_train/rejected': '0.14497', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0023525', 'logps_train/rejected': '-160.95', 'logps_train/chosen': '-133.79', 'loss/train': '0.70808', 'examples_per_second': '31.573', 'grad_norm': '23', 'counters/examples': 106848, 'counters/updates': 3339}
train stats after 106880 examples: {'rewards_train/chosen': '0.09955', 'rewards_train/rejected': '0.027524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072026', 'logps_train/rejected': '-106.35', 'logps_train/chosen': '-157.68', 'loss/train': '0.66454', 'examples_per_second': '31.596', 'grad_norm': '21.125', 'counters/examples': 106880, 'counters/updates': 3340}
train stats after 106912 examples: {'rewards_train/chosen': '0.13676', 'rewards_train/rejected': '0.011228', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12553', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-139.87', 'loss/train': '0.63889', 'examples_per_second': '30.06', 'grad_norm': '20', 'counters/examples': 106912, 'counters/updates': 3341}
train stats after 106944 examples: {'rewards_train/chosen': '0.13625', 'rewards_train/rejected': '-0.016073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15232', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-132.75', 'loss/train': '0.62571', 'examples_per_second': '31.607', 'grad_norm': '20', 'counters/examples': 106944, 'counters/updates': 3342}
train stats after 106976 examples: {'rewards_train/chosen': '0.12635', 'rewards_train/rejected': '-0.019628', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14598', 'logps_train/rejected': '-85.96', 'logps_train/chosen': '-120.64', 'loss/train': '0.63433', 'examples_per_second': '31.767', 'grad_norm': '17.375', 'counters/examples': 106976, 'counters/updates': 3343}
train stats after 107008 examples: {'rewards_train/chosen': '0.15995', 'rewards_train/rejected': '0.058192', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10176', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-162.79', 'loss/train': '0.65495', 'examples_per_second': '31.819', 'grad_norm': '21.375', 'counters/examples': 107008, 'counters/updates': 3344}
train stats after 107040 examples: {'rewards_train/chosen': '0.13849', 'rewards_train/rejected': '0.026062', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11243', 'logps_train/rejected': '-103.22', 'logps_train/chosen': '-128.4', 'loss/train': '0.65046', 'examples_per_second': '30.279', 'grad_norm': '18.625', 'counters/examples': 107040, 'counters/updates': 3345}
skipping logging after 107072 examples to avoid logging too frequently
train stats after 107104 examples: {'rewards_train/chosen': '0.061891', 'rewards_train/rejected': '0.040676', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021216', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-117.09', 'loss/train': '0.68921', 'examples_per_second': '32.493', 'grad_norm': '20.875', 'counters/examples': 107104, 'counters/updates': 3347}
train stats after 107136 examples: {'rewards_train/chosen': '0.11107', 'rewards_train/rejected': '0.040778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070287', 'logps_train/rejected': '-115.75', 'logps_train/chosen': '-125.83', 'loss/train': '0.67002', 'examples_per_second': '31.488', 'grad_norm': '20.75', 'counters/examples': 107136, 'counters/updates': 3348}
train stats after 107168 examples: {'rewards_train/chosen': '0.13947', 'rewards_train/rejected': '0.043924', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09555', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-157.81', 'loss/train': '0.65697', 'examples_per_second': '32.365', 'grad_norm': '20.375', 'counters/examples': 107168, 'counters/updates': 3349}
train stats after 107200 examples: {'rewards_train/chosen': '0.076266', 'rewards_train/rejected': '0.039764', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036502', 'logps_train/rejected': '-113.4', 'logps_train/chosen': '-105.41', 'loss/train': '0.68211', 'examples_per_second': '31.65', 'grad_norm': '18.625', 'counters/examples': 107200, 'counters/updates': 3350}
skipping logging after 107232 examples to avoid logging too frequently
train stats after 107264 examples: {'rewards_train/chosen': '0.14145', 'rewards_train/rejected': '-0.013369', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15482', 'logps_train/rejected': '-110.49', 'logps_train/chosen': '-120.08', 'loss/train': '0.63236', 'examples_per_second': '31.56', 'grad_norm': '18', 'counters/examples': 107264, 'counters/updates': 3352}
train stats after 107296 examples: {'rewards_train/chosen': '0.12988', 'rewards_train/rejected': '-0.001517', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1314', 'logps_train/rejected': '-139.65', 'logps_train/chosen': '-93.907', 'loss/train': '0.64023', 'examples_per_second': '30.107', 'grad_norm': '19', 'counters/examples': 107296, 'counters/updates': 3353}
train stats after 107328 examples: {'rewards_train/chosen': '0.12934', 'rewards_train/rejected': '0.059473', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06987', 'logps_train/rejected': '-96.296', 'logps_train/chosen': '-121.73', 'loss/train': '0.66718', 'examples_per_second': '31.079', 'grad_norm': '18.25', 'counters/examples': 107328, 'counters/updates': 3354}
skipping logging after 107360 examples to avoid logging too frequently
train stats after 107392 examples: {'rewards_train/chosen': '0.18533', 'rewards_train/rejected': '0.05148', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13385', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-143.63', 'loss/train': '0.63605', 'examples_per_second': '31.565', 'grad_norm': '20.375', 'counters/examples': 107392, 'counters/updates': 3356}
train stats after 107424 examples: {'rewards_train/chosen': '0.12992', 'rewards_train/rejected': '0.058479', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071439', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-168.66', 'loss/train': '0.6655', 'examples_per_second': '31.524', 'grad_norm': '22.125', 'counters/examples': 107424, 'counters/updates': 3357}
train stats after 107456 examples: {'rewards_train/chosen': '0.16764', 'rewards_train/rejected': '0.045729', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.12192', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-135.84', 'loss/train': '0.64024', 'examples_per_second': '31.26', 'grad_norm': '19.5', 'counters/examples': 107456, 'counters/updates': 3358}
train stats after 107488 examples: {'rewards_train/chosen': '0.069843', 'rewards_train/rejected': '0.0010428', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0688', 'logps_train/rejected': '-141.36', 'logps_train/chosen': '-119.64', 'loss/train': '0.66714', 'examples_per_second': '31.714', 'grad_norm': '21.5', 'counters/examples': 107488, 'counters/updates': 3359}
train stats after 107520 examples: {'rewards_train/chosen': '0.20447', 'rewards_train/rejected': '0.15895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045515', 'logps_train/rejected': '-130.48', 'logps_train/chosen': '-155.13', 'loss/train': '0.67868', 'examples_per_second': '31.588', 'grad_norm': '21.125', 'counters/examples': 107520, 'counters/updates': 3360}
train stats after 107552 examples: {'rewards_train/chosen': '0.17021', 'rewards_train/rejected': '0.063989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10622', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-162.25', 'loss/train': '0.65659', 'examples_per_second': '31.558', 'grad_norm': '20.875', 'counters/examples': 107552, 'counters/updates': 3361}
skipping logging after 107584 examples to avoid logging too frequently
train stats after 107616 examples: {'rewards_train/chosen': '0.15119', 'rewards_train/rejected': '0.051116', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10007', 'logps_train/rejected': '-140.03', 'logps_train/chosen': '-142.51', 'loss/train': '0.65428', 'examples_per_second': '31.568', 'grad_norm': '20.5', 'counters/examples': 107616, 'counters/updates': 3363}
skipping logging after 107648 examples to avoid logging too frequently
train stats after 107680 examples: {'rewards_train/chosen': '0.038066', 'rewards_train/rejected': '0.066199', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028133', 'logps_train/rejected': '-129.22', 'logps_train/chosen': '-111.2', 'loss/train': '0.71672', 'examples_per_second': '31.724', 'grad_norm': '22.25', 'counters/examples': 107680, 'counters/updates': 3365}
train stats after 107712 examples: {'rewards_train/chosen': '0.11642', 'rewards_train/rejected': '0.067727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048689', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-126.69', 'loss/train': '0.67497', 'examples_per_second': '31.122', 'grad_norm': '20.625', 'counters/examples': 107712, 'counters/updates': 3366}
train stats after 107744 examples: {'rewards_train/chosen': '0.17722', 'rewards_train/rejected': '0.025698', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15152', 'logps_train/rejected': '-153.61', 'logps_train/chosen': '-107.25', 'loss/train': '0.63909', 'examples_per_second': '31.609', 'grad_norm': '20.625', 'counters/examples': 107744, 'counters/updates': 3367}
train stats after 107776 examples: {'rewards_train/chosen': '0.15665', 'rewards_train/rejected': '0.068817', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087832', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-134.72', 'loss/train': '0.66162', 'examples_per_second': '32.647', 'grad_norm': '19.125', 'counters/examples': 107776, 'counters/updates': 3368}
skipping logging after 107808 examples to avoid logging too frequently
train stats after 107840 examples: {'rewards_train/chosen': '0.1263', 'rewards_train/rejected': '0.063582', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062719', 'logps_train/rejected': '-139.41', 'logps_train/chosen': '-110.67', 'loss/train': '0.67102', 'examples_per_second': '31.513', 'grad_norm': '20.5', 'counters/examples': 107840, 'counters/updates': 3370}
skipping logging after 107872 examples to avoid logging too frequently
train stats after 107904 examples: {'rewards_train/chosen': '0.11833', 'rewards_train/rejected': '0.055222', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063105', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-133.92', 'loss/train': '0.66754', 'examples_per_second': '31.291', 'grad_norm': '20.5', 'counters/examples': 107904, 'counters/updates': 3372}
train stats after 107936 examples: {'rewards_train/chosen': '0.089289', 'rewards_train/rejected': '0.017581', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071708', 'logps_train/rejected': '-95.422', 'logps_train/chosen': '-120.45', 'loss/train': '0.66713', 'examples_per_second': '32.162', 'grad_norm': '17.875', 'counters/examples': 107936, 'counters/updates': 3373}
skipping logging after 107968 examples to avoid logging too frequently
train stats after 108000 examples: {'rewards_train/chosen': '0.16276', 'rewards_train/rejected': '0.12055', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042212', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-140.53', 'loss/train': '0.68582', 'examples_per_second': '31.846', 'grad_norm': '21', 'counters/examples': 108000, 'counters/updates': 3375}
Running evaluation after 108000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.89it/s]
eval after 108000: {'rewards_eval/chosen': '0.12539', 'rewards_eval/rejected': '0.049105', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.076283', 'logps_eval/rejected': '-114.9', 'logps_eval/chosen': '-134.29', 'loss/eval': '0.66615'}
skipping save for non epoch
train stats after 108032 examples: {'rewards_train/chosen': '0.087177', 'rewards_train/rejected': '-0.013738', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10092', 'logps_train/rejected': '-108.6', 'logps_train/chosen': '-114.4', 'loss/train': '0.64886', 'examples_per_second': '34.103', 'grad_norm': '18.375', 'counters/examples': 108032, 'counters/updates': 3376}
skipping logging after 108064 examples to avoid logging too frequently
train stats after 108096 examples: {'rewards_train/chosen': '0.13865', 'rewards_train/rejected': '-0.0021672', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14082', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-125.22', 'loss/train': '0.63189', 'examples_per_second': '30.614', 'grad_norm': '19.75', 'counters/examples': 108096, 'counters/updates': 3378}
skipping logging after 108128 examples to avoid logging too frequently
train stats after 108160 examples: {'rewards_train/chosen': '0.14403', 'rewards_train/rejected': '0.043653', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.10038', 'logps_train/rejected': '-83.951', 'logps_train/chosen': '-140.47', 'loss/train': '0.65236', 'examples_per_second': '26.305', 'grad_norm': '18.875', 'counters/examples': 108160, 'counters/updates': 3380}
train stats after 108192 examples: {'rewards_train/chosen': '0.11208', 'rewards_train/rejected': '0.019251', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092831', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-149.86', 'loss/train': '0.65434', 'examples_per_second': '32.406', 'grad_norm': '21.625', 'counters/examples': 108192, 'counters/updates': 3381}
train stats after 108224 examples: {'rewards_train/chosen': '0.16197', 'rewards_train/rejected': '0.085396', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076571', 'logps_train/rejected': '-173.04', 'logps_train/chosen': '-145.6', 'loss/train': '0.66247', 'examples_per_second': '31.53', 'grad_norm': '22.5', 'counters/examples': 108224, 'counters/updates': 3382}
train stats after 108256 examples: {'rewards_train/chosen': '0.18823', 'rewards_train/rejected': '0.07582', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11241', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-135.51', 'loss/train': '0.64864', 'examples_per_second': '24.408', 'grad_norm': '19.625', 'counters/examples': 108256, 'counters/updates': 3383}
skipping logging after 108288 examples to avoid logging too frequently
train stats after 108320 examples: {'rewards_train/chosen': '0.13404', 'rewards_train/rejected': '0.04142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092621', 'logps_train/rejected': '-105.02', 'logps_train/chosen': '-115.72', 'loss/train': '0.65336', 'examples_per_second': '31.296', 'grad_norm': '18.375', 'counters/examples': 108320, 'counters/updates': 3385}
skipping logging after 108352 examples to avoid logging too frequently
train stats after 108384 examples: {'rewards_train/chosen': '0.17748', 'rewards_train/rejected': '0.062493', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11498', 'logps_train/rejected': '-114.41', 'logps_train/chosen': '-137.79', 'loss/train': '0.64808', 'examples_per_second': '30.206', 'grad_norm': '19.625', 'counters/examples': 108384, 'counters/updates': 3387}
skipping logging after 108416 examples to avoid logging too frequently
train stats after 108448 examples: {'rewards_train/chosen': '0.15133', 'rewards_train/rejected': '0.07966', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071675', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-174.38', 'loss/train': '0.66632', 'examples_per_second': '29.999', 'grad_norm': '22.5', 'counters/examples': 108448, 'counters/updates': 3389}
train stats after 108480 examples: {'rewards_train/chosen': '0.1114', 'rewards_train/rejected': '0.099393', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012003', 'logps_train/rejected': '-127.39', 'logps_train/chosen': '-128.36', 'loss/train': '0.69639', 'examples_per_second': '30.848', 'grad_norm': '20.375', 'counters/examples': 108480, 'counters/updates': 3390}
train stats after 108512 examples: {'rewards_train/chosen': '0.11972', 'rewards_train/rejected': '0.067146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052571', 'logps_train/rejected': '-150.74', 'logps_train/chosen': '-157.17', 'loss/train': '0.67498', 'examples_per_second': '31.322', 'grad_norm': '21.875', 'counters/examples': 108512, 'counters/updates': 3391}
skipping logging after 108544 examples to avoid logging too frequently
train stats after 108576 examples: {'rewards_train/chosen': '0.16962', 'rewards_train/rejected': '0.063214', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10641', 'logps_train/rejected': '-144.02', 'logps_train/chosen': '-162.53', 'loss/train': '0.64785', 'examples_per_second': '31.41', 'grad_norm': '21.75', 'counters/examples': 108576, 'counters/updates': 3393}
train stats after 108608 examples: {'rewards_train/chosen': '0.14348', 'rewards_train/rejected': '0.094882', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048596', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-126.19', 'loss/train': '0.67392', 'examples_per_second': '29.805', 'grad_norm': '20.125', 'counters/examples': 108608, 'counters/updates': 3394}
skipping logging after 108640 examples to avoid logging too frequently
train stats after 108672 examples: {'rewards_train/chosen': '0.16286', 'rewards_train/rejected': '0.029472', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-126.83', 'loss/train': '0.63606', 'examples_per_second': '32.135', 'grad_norm': '18.375', 'counters/examples': 108672, 'counters/updates': 3396}
train stats after 108704 examples: {'rewards_train/chosen': '0.090519', 'rewards_train/rejected': '0.10739', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.016873', 'logps_train/rejected': '-115.91', 'logps_train/chosen': '-130.16', 'loss/train': '0.71025', 'examples_per_second': '31.082', 'grad_norm': '20.375', 'counters/examples': 108704, 'counters/updates': 3397}
train stats after 108736 examples: {'rewards_train/chosen': '0.25035', 'rewards_train/rejected': '0.11526', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13508', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-156.38', 'loss/train': '0.64092', 'examples_per_second': '30.456', 'grad_norm': '20.5', 'counters/examples': 108736, 'counters/updates': 3398}
train stats after 108768 examples: {'rewards_train/chosen': '0.16931', 'rewards_train/rejected': '0.11209', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057221', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-144.14', 'loss/train': '0.67586', 'examples_per_second': '30.149', 'grad_norm': '21.25', 'counters/examples': 108768, 'counters/updates': 3399}
train stats after 108800 examples: {'rewards_train/chosen': '0.067185', 'rewards_train/rejected': '-0.020238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087423', 'logps_train/rejected': '-81.199', 'logps_train/chosen': '-103.24', 'loss/train': '0.66348', 'examples_per_second': '30.622', 'grad_norm': '16.875', 'counters/examples': 108800, 'counters/updates': 3400}
skipping logging after 108832 examples to avoid logging too frequently
train stats after 108864 examples: {'rewards_train/chosen': '0.15721', 'rewards_train/rejected': '0.064266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092943', 'logps_train/rejected': '-82.134', 'logps_train/chosen': '-112.39', 'loss/train': '0.65521', 'examples_per_second': '31.407', 'grad_norm': '16.75', 'counters/examples': 108864, 'counters/updates': 3402}
train stats after 108896 examples: {'rewards_train/chosen': '0.10443', 'rewards_train/rejected': '0.026735', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077692', 'logps_train/rejected': '-143.15', 'logps_train/chosen': '-160.49', 'loss/train': '0.66562', 'examples_per_second': '29.459', 'grad_norm': '22.125', 'counters/examples': 108896, 'counters/updates': 3403}
skipping logging after 108928 examples to avoid logging too frequently
train stats after 108960 examples: {'rewards_train/chosen': '0.098613', 'rewards_train/rejected': '0.07952', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019092', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-123.08', 'loss/train': '0.69357', 'examples_per_second': '35.675', 'grad_norm': '19.875', 'counters/examples': 108960, 'counters/updates': 3405}
train stats after 108992 examples: {'rewards_train/chosen': '0.18033', 'rewards_train/rejected': '0.12', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060326', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-165.25', 'loss/train': '0.67436', 'examples_per_second': '30.572', 'grad_norm': '22.375', 'counters/examples': 108992, 'counters/updates': 3406}
train stats after 109024 examples: {'rewards_train/chosen': '0.11021', 'rewards_train/rejected': '0.034378', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075835', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-147', 'loss/train': '0.66466', 'examples_per_second': '32.965', 'grad_norm': '20.625', 'counters/examples': 109024, 'counters/updates': 3407}
train stats after 109056 examples: {'rewards_train/chosen': '0.21933', 'rewards_train/rejected': '0.063735', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1556', 'logps_train/rejected': '-127.86', 'logps_train/chosen': '-195.52', 'loss/train': '0.62836', 'examples_per_second': '31.482', 'grad_norm': '22.875', 'counters/examples': 109056, 'counters/updates': 3408}
train stats after 109088 examples: {'rewards_train/chosen': '0.064175', 'rewards_train/rejected': '0.049408', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014767', 'logps_train/rejected': '-92.574', 'logps_train/chosen': '-145.67', 'loss/train': '0.69519', 'examples_per_second': '32.293', 'grad_norm': '19.875', 'counters/examples': 109088, 'counters/updates': 3409}
train stats after 109120 examples: {'rewards_train/chosen': '0.12964', 'rewards_train/rejected': '0.01893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11071', 'logps_train/rejected': '-112.34', 'logps_train/chosen': '-133.65', 'loss/train': '0.6454', 'examples_per_second': '32.415', 'grad_norm': '19', 'counters/examples': 109120, 'counters/updates': 3410}
train stats after 109152 examples: {'rewards_train/chosen': '0.11368', 'rewards_train/rejected': '-0.045413', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15909', 'logps_train/rejected': '-87.068', 'logps_train/chosen': '-126.62', 'loss/train': '0.62455', 'examples_per_second': '32.608', 'grad_norm': '16.5', 'counters/examples': 109152, 'counters/updates': 3411}
skipping logging after 109184 examples to avoid logging too frequently
train stats after 109216 examples: {'rewards_train/chosen': '0.14121', 'rewards_train/rejected': '0.078422', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062785', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-106.43', 'loss/train': '0.674', 'examples_per_second': '33.197', 'grad_norm': '18.25', 'counters/examples': 109216, 'counters/updates': 3413}
skipping logging after 109248 examples to avoid logging too frequently
train stats after 109280 examples: {'rewards_train/chosen': '0.13809', 'rewards_train/rejected': '0.070088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067999', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-151.81', 'loss/train': '0.66618', 'examples_per_second': '32.96', 'grad_norm': '21.875', 'counters/examples': 109280, 'counters/updates': 3415}
train stats after 109312 examples: {'rewards_train/chosen': '0.14358', 'rewards_train/rejected': '0.029744', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11384', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-150.85', 'loss/train': '0.64319', 'examples_per_second': '31.47', 'grad_norm': '20', 'counters/examples': 109312, 'counters/updates': 3416}
train stats after 109344 examples: {'rewards_train/chosen': '0.16547', 'rewards_train/rejected': '0.03943', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12604', 'logps_train/rejected': '-118.84', 'logps_train/chosen': '-131.3', 'loss/train': '0.6424', 'examples_per_second': '30.57', 'grad_norm': '19.25', 'counters/examples': 109344, 'counters/updates': 3417}
train stats after 109376 examples: {'rewards_train/chosen': '0.12377', 'rewards_train/rejected': '0.014778', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.109', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-142.68', 'loss/train': '0.64647', 'examples_per_second': '31.312', 'grad_norm': '19.5', 'counters/examples': 109376, 'counters/updates': 3418}
train stats after 109408 examples: {'rewards_train/chosen': '0.1276', 'rewards_train/rejected': '-0.0017702', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12937', 'logps_train/rejected': '-132', 'logps_train/chosen': '-122.46', 'loss/train': '0.64311', 'examples_per_second': '30.93', 'grad_norm': '19.5', 'counters/examples': 109408, 'counters/updates': 3419}
train stats after 109440 examples: {'rewards_train/chosen': '0.12467', 'rewards_train/rejected': '0.026591', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098075', 'logps_train/rejected': '-131.83', 'logps_train/chosen': '-124.08', 'loss/train': '0.65559', 'examples_per_second': '31.508', 'grad_norm': '20.75', 'counters/examples': 109440, 'counters/updates': 3420}
train stats after 109472 examples: {'rewards_train/chosen': '0.18115', 'rewards_train/rejected': '0.050533', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13062', 'logps_train/rejected': '-113.81', 'logps_train/chosen': '-140.3', 'loss/train': '0.63991', 'examples_per_second': '32.002', 'grad_norm': '19.125', 'counters/examples': 109472, 'counters/updates': 3421}
train stats after 109504 examples: {'rewards_train/chosen': '0.07039', 'rewards_train/rejected': '0.0075001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06289', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-153.52', 'loss/train': '0.66619', 'examples_per_second': '32.607', 'grad_norm': '20.625', 'counters/examples': 109504, 'counters/updates': 3422}
train stats after 109536 examples: {'rewards_train/chosen': '0.22647', 'rewards_train/rejected': '0.1119', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11457', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-145.63', 'loss/train': '0.65115', 'examples_per_second': '31.181', 'grad_norm': '20.875', 'counters/examples': 109536, 'counters/updates': 3423}
skipping logging after 109568 examples to avoid logging too frequently
train stats after 109600 examples: {'rewards_train/chosen': '0.15616', 'rewards_train/rejected': '0.035867', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12029', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-142.32', 'loss/train': '0.64168', 'examples_per_second': '30.905', 'grad_norm': '19.5', 'counters/examples': 109600, 'counters/updates': 3425}
train stats after 109632 examples: {'rewards_train/chosen': '0.085758', 'rewards_train/rejected': '-0.011605', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097363', 'logps_train/rejected': '-113.61', 'logps_train/chosen': '-129.98', 'loss/train': '0.65463', 'examples_per_second': '31.218', 'grad_norm': '18.625', 'counters/examples': 109632, 'counters/updates': 3426}
train stats after 109664 examples: {'rewards_train/chosen': '0.1995', 'rewards_train/rejected': '0.070779', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12872', 'logps_train/rejected': '-128.31', 'logps_train/chosen': '-161.08', 'loss/train': '0.64022', 'examples_per_second': '31.507', 'grad_norm': '20.125', 'counters/examples': 109664, 'counters/updates': 3427}
train stats after 109696 examples: {'rewards_train/chosen': '0.14375', 'rewards_train/rejected': '0.1129', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.030843', 'logps_train/rejected': '-171.21', 'logps_train/chosen': '-169.32', 'loss/train': '0.68744', 'examples_per_second': '30.591', 'grad_norm': '23.375', 'counters/examples': 109696, 'counters/updates': 3428}
skipping logging after 109728 examples to avoid logging too frequently
train stats after 109760 examples: {'rewards_train/chosen': '0.169', 'rewards_train/rejected': '0.083429', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085569', 'logps_train/rejected': '-131.59', 'logps_train/chosen': '-113.22', 'loss/train': '0.66296', 'examples_per_second': '30.967', 'grad_norm': '19.125', 'counters/examples': 109760, 'counters/updates': 3430}
train stats after 109792 examples: {'rewards_train/chosen': '0.051366', 'rewards_train/rejected': '0.028207', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023159', 'logps_train/rejected': '-110.07', 'logps_train/chosen': '-114.43', 'loss/train': '0.68733', 'examples_per_second': '31.76', 'grad_norm': '19.75', 'counters/examples': 109792, 'counters/updates': 3431}
train stats after 109824 examples: {'rewards_train/chosen': '0.19517', 'rewards_train/rejected': '-0.002', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19717', 'logps_train/rejected': '-99.667', 'logps_train/chosen': '-174.99', 'loss/train': '0.60883', 'examples_per_second': '31.533', 'grad_norm': '20.25', 'counters/examples': 109824, 'counters/updates': 3432}
train stats after 109856 examples: {'rewards_train/chosen': '0.043413', 'rewards_train/rejected': '-0.042359', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085772', 'logps_train/rejected': '-107.04', 'logps_train/chosen': '-148.49', 'loss/train': '0.65656', 'examples_per_second': '30.581', 'grad_norm': '20.375', 'counters/examples': 109856, 'counters/updates': 3433}
train stats after 109888 examples: {'rewards_train/chosen': '0.15744', 'rewards_train/rejected': '0.023432', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13401', 'logps_train/rejected': '-136.77', 'logps_train/chosen': '-186.49', 'loss/train': '0.63986', 'examples_per_second': '32.42', 'grad_norm': '21.25', 'counters/examples': 109888, 'counters/updates': 3434}
train stats after 109920 examples: {'rewards_train/chosen': '0.12698', 'rewards_train/rejected': '0.062617', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064359', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-159.81', 'loss/train': '0.6715', 'examples_per_second': '30.437', 'grad_norm': '22.875', 'counters/examples': 109920, 'counters/updates': 3435}
train stats after 109952 examples: {'rewards_train/chosen': '0.10831', 'rewards_train/rejected': '0.01907', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089238', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-128.75', 'loss/train': '0.65865', 'examples_per_second': '31.471', 'grad_norm': '21', 'counters/examples': 109952, 'counters/updates': 3436}
train stats after 109984 examples: {'rewards_train/chosen': '0.11759', 'rewards_train/rejected': '0.10783', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.009758', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-113.79', 'loss/train': '0.69486', 'examples_per_second': '31.49', 'grad_norm': '21', 'counters/examples': 109984, 'counters/updates': 3437}
train stats after 110016 examples: {'rewards_train/chosen': '0.11715', 'rewards_train/rejected': '0.062129', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055018', 'logps_train/rejected': '-109.11', 'logps_train/chosen': '-118.97', 'loss/train': '0.67586', 'examples_per_second': '31.716', 'grad_norm': '19', 'counters/examples': 110016, 'counters/updates': 3438}
train stats after 110048 examples: {'rewards_train/chosen': '0.10971', 'rewards_train/rejected': '0.061841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047868', 'logps_train/rejected': '-97.65', 'logps_train/chosen': '-130.7', 'loss/train': '0.67262', 'examples_per_second': '31.218', 'grad_norm': '19.125', 'counters/examples': 110048, 'counters/updates': 3439}
train stats after 110080 examples: {'rewards_train/chosen': '0.073023', 'rewards_train/rejected': '0.022183', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05084', 'logps_train/rejected': '-114.26', 'logps_train/chosen': '-119.43', 'loss/train': '0.67766', 'examples_per_second': '30.622', 'grad_norm': '19.5', 'counters/examples': 110080, 'counters/updates': 3440}
train stats after 110112 examples: {'rewards_train/chosen': '0.13261', 'rewards_train/rejected': '0.078433', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054175', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-131.18', 'loss/train': '0.67709', 'examples_per_second': '31.545', 'grad_norm': '20.875', 'counters/examples': 110112, 'counters/updates': 3441}
train stats after 110144 examples: {'rewards_train/chosen': '0.10022', 'rewards_train/rejected': '0.048474', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051743', 'logps_train/rejected': '-94.857', 'logps_train/chosen': '-113.15', 'loss/train': '0.6776', 'examples_per_second': '31.582', 'grad_norm': '17.875', 'counters/examples': 110144, 'counters/updates': 3442}
train stats after 110176 examples: {'rewards_train/chosen': '0.13492', 'rewards_train/rejected': '0.051719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083202', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-123.53', 'loss/train': '0.65974', 'examples_per_second': '30.106', 'grad_norm': '20.5', 'counters/examples': 110176, 'counters/updates': 3443}
train stats after 110208 examples: {'rewards_train/chosen': '0.13391', 'rewards_train/rejected': '0.067653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066257', 'logps_train/rejected': '-102.22', 'logps_train/chosen': '-130.71', 'loss/train': '0.67385', 'examples_per_second': '31.02', 'grad_norm': '19.875', 'counters/examples': 110208, 'counters/updates': 3444}
train stats after 110240 examples: {'rewards_train/chosen': '0.17728', 'rewards_train/rejected': '0.029272', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14801', 'logps_train/rejected': '-109.57', 'logps_train/chosen': '-138.09', 'loss/train': '0.64181', 'examples_per_second': '24.635', 'grad_norm': '19.5', 'counters/examples': 110240, 'counters/updates': 3445}
train stats after 110272 examples: {'rewards_train/chosen': '0.12494', 'rewards_train/rejected': '0.055358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069583', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-137.34', 'loss/train': '0.67003', 'examples_per_second': '31.48', 'grad_norm': '21.625', 'counters/examples': 110272, 'counters/updates': 3446}
skipping logging after 110304 examples to avoid logging too frequently
train stats after 110336 examples: {'rewards_train/chosen': '0.16219', 'rewards_train/rejected': '-0.0033775', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16557', 'logps_train/rejected': '-113.68', 'logps_train/chosen': '-111.16', 'loss/train': '0.62036', 'examples_per_second': '35.464', 'grad_norm': '16.625', 'counters/examples': 110336, 'counters/updates': 3448}
train stats after 110368 examples: {'rewards_train/chosen': '0.12431', 'rewards_train/rejected': '0.04356', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080753', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-133.42', 'loss/train': '0.66463', 'examples_per_second': '30.178', 'grad_norm': '22.125', 'counters/examples': 110368, 'counters/updates': 3449}
train stats after 110400 examples: {'rewards_train/chosen': '0.11853', 'rewards_train/rejected': '0.040757', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077778', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-142.25', 'loss/train': '0.66763', 'examples_per_second': '30.94', 'grad_norm': '20.125', 'counters/examples': 110400, 'counters/updates': 3450}
train stats after 110432 examples: {'rewards_train/chosen': '0.19392', 'rewards_train/rejected': '0.075355', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11856', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-137.09', 'loss/train': '0.64579', 'examples_per_second': '31.23', 'grad_norm': '20.75', 'counters/examples': 110432, 'counters/updates': 3451}
train stats after 110464 examples: {'rewards_train/chosen': '0.13326', 'rewards_train/rejected': '0.04583', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.087434', 'logps_train/rejected': '-104.36', 'logps_train/chosen': '-147.59', 'loss/train': '0.65666', 'examples_per_second': '32.529', 'grad_norm': '19.625', 'counters/examples': 110464, 'counters/updates': 3452}
skipping logging after 110496 examples to avoid logging too frequently
train stats after 110528 examples: {'rewards_train/chosen': '0.17222', 'rewards_train/rejected': '0.0012081', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17101', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-152.25', 'loss/train': '0.62212', 'examples_per_second': '31.512', 'grad_norm': '18.875', 'counters/examples': 110528, 'counters/updates': 3454}
train stats after 110560 examples: {'rewards_train/chosen': '0.15099', 'rewards_train/rejected': '0.057041', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093952', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-132.14', 'loss/train': '0.65898', 'examples_per_second': '32.401', 'grad_norm': '18.75', 'counters/examples': 110560, 'counters/updates': 3455}
train stats after 110592 examples: {'rewards_train/chosen': '0.17084', 'rewards_train/rejected': '0.10415', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066695', 'logps_train/rejected': '-147.08', 'logps_train/chosen': '-138.98', 'loss/train': '0.6807', 'examples_per_second': '31.721', 'grad_norm': '22.625', 'counters/examples': 110592, 'counters/updates': 3456}
skipping logging after 110624 examples to avoid logging too frequently
train stats after 110656 examples: {'rewards_train/chosen': '0.16261', 'rewards_train/rejected': '0.044385', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11822', 'logps_train/rejected': '-98.128', 'logps_train/chosen': '-136.54', 'loss/train': '0.64629', 'examples_per_second': '31.469', 'grad_norm': '18', 'counters/examples': 110656, 'counters/updates': 3458}
train stats after 110688 examples: {'rewards_train/chosen': '0.030482', 'rewards_train/rejected': '0.0075415', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02294', 'logps_train/rejected': '-143.27', 'logps_train/chosen': '-158.08', 'loss/train': '0.69131', 'examples_per_second': '31.602', 'grad_norm': '22.75', 'counters/examples': 110688, 'counters/updates': 3459}
skipping logging after 110720 examples to avoid logging too frequently
train stats after 110752 examples: {'rewards_train/chosen': '0.076279', 'rewards_train/rejected': '-0.029568', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10585', 'logps_train/rejected': '-93.297', 'logps_train/chosen': '-94.706', 'loss/train': '0.64991', 'examples_per_second': '33.31', 'grad_norm': '16.5', 'counters/examples': 110752, 'counters/updates': 3461}
train stats after 110784 examples: {'rewards_train/chosen': '0.15458', 'rewards_train/rejected': '0.052736', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10185', 'logps_train/rejected': '-108.55', 'logps_train/chosen': '-147.63', 'loss/train': '0.65357', 'examples_per_second': '32.809', 'grad_norm': '19.625', 'counters/examples': 110784, 'counters/updates': 3462}
train stats after 110816 examples: {'rewards_train/chosen': '0.16515', 'rewards_train/rejected': '0.10084', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064316', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-162.61', 'loss/train': '0.67259', 'examples_per_second': '31.531', 'grad_norm': '21.5', 'counters/examples': 110816, 'counters/updates': 3463}
skipping logging after 110848 examples to avoid logging too frequently
train stats after 110880 examples: {'rewards_train/chosen': '0.12742', 'rewards_train/rejected': '0.083697', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043719', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-137.5', 'loss/train': '0.67661', 'examples_per_second': '31.512', 'grad_norm': '20.75', 'counters/examples': 110880, 'counters/updates': 3465}
train stats after 110912 examples: {'rewards_train/chosen': '0.1645', 'rewards_train/rejected': '0.053133', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11137', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-133.5', 'loss/train': '0.64442', 'examples_per_second': '31.506', 'grad_norm': '19.5', 'counters/examples': 110912, 'counters/updates': 3466}
train stats after 110944 examples: {'rewards_train/chosen': '0.16677', 'rewards_train/rejected': '0.0487', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11807', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-118.76', 'loss/train': '0.64409', 'examples_per_second': '31.296', 'grad_norm': '19.125', 'counters/examples': 110944, 'counters/updates': 3467}
skipping logging after 110976 examples to avoid logging too frequently
train stats after 111008 examples: {'rewards_train/chosen': '0.13021', 'rewards_train/rejected': '0.012212', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.118', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-136.71', 'loss/train': '0.64168', 'examples_per_second': '32.454', 'grad_norm': '18.875', 'counters/examples': 111008, 'counters/updates': 3469}
train stats after 111040 examples: {'rewards_train/chosen': '0.14399', 'rewards_train/rejected': '0.055579', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08841', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-146.5', 'loss/train': '0.66152', 'examples_per_second': '31.484', 'grad_norm': '19.625', 'counters/examples': 111040, 'counters/updates': 3470}
train stats after 111072 examples: {'rewards_train/chosen': '0.12975', 'rewards_train/rejected': '0.113', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01675', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-130.17', 'loss/train': '0.69222', 'examples_per_second': '30.184', 'grad_norm': '21.625', 'counters/examples': 111072, 'counters/updates': 3471}
skipping logging after 111104 examples to avoid logging too frequently
train stats after 111136 examples: {'rewards_train/chosen': '0.18171', 'rewards_train/rejected': '0.051139', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13057', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-128.28', 'loss/train': '0.63998', 'examples_per_second': '30.002', 'grad_norm': '18.125', 'counters/examples': 111136, 'counters/updates': 3473}
train stats after 111168 examples: {'rewards_train/chosen': '0.049543', 'rewards_train/rejected': '-0.003881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053424', 'logps_train/rejected': '-110.84', 'logps_train/chosen': '-117.41', 'loss/train': '0.67053', 'examples_per_second': '31.577', 'grad_norm': '18.875', 'counters/examples': 111168, 'counters/updates': 3474}
train stats after 111200 examples: {'rewards_train/chosen': '0.11584', 'rewards_train/rejected': '0.067501', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048335', 'logps_train/rejected': '-143.94', 'logps_train/chosen': '-204.2', 'loss/train': '0.67832', 'examples_per_second': '31.435', 'grad_norm': '24.25', 'counters/examples': 111200, 'counters/updates': 3475}
train stats after 111232 examples: {'rewards_train/chosen': '0.14409', 'rewards_train/rejected': '0.0048371', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13925', 'logps_train/rejected': '-107.21', 'logps_train/chosen': '-135.56', 'loss/train': '0.64341', 'examples_per_second': '30.797', 'grad_norm': '19.75', 'counters/examples': 111232, 'counters/updates': 3476}
skipping logging after 111264 examples to avoid logging too frequently
train stats after 111296 examples: {'rewards_train/chosen': '0.14284', 'rewards_train/rejected': '0.052049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090788', 'logps_train/rejected': '-112.47', 'logps_train/chosen': '-115.99', 'loss/train': '0.65441', 'examples_per_second': '32.096', 'grad_norm': '18.75', 'counters/examples': 111296, 'counters/updates': 3478}
train stats after 111328 examples: {'rewards_train/chosen': '0.15054', 'rewards_train/rejected': '0.051546', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098995', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-158.97', 'loss/train': '0.6533', 'examples_per_second': '31.271', 'grad_norm': '19.75', 'counters/examples': 111328, 'counters/updates': 3479}
skipping logging after 111360 examples to avoid logging too frequently
train stats after 111392 examples: {'rewards_train/chosen': '0.12777', 'rewards_train/rejected': '0.060578', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067195', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-135.05', 'loss/train': '0.66674', 'examples_per_second': '31.55', 'grad_norm': '22', 'counters/examples': 111392, 'counters/updates': 3481}
train stats after 111424 examples: {'rewards_train/chosen': '0.18882', 'rewards_train/rejected': '0.081435', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10738', 'logps_train/rejected': '-118.83', 'logps_train/chosen': '-162.31', 'loss/train': '0.65598', 'examples_per_second': '31.575', 'grad_norm': '20.75', 'counters/examples': 111424, 'counters/updates': 3482}
train stats after 111456 examples: {'rewards_train/chosen': '0.16447', 'rewards_train/rejected': '0.070688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093778', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-159.25', 'loss/train': '0.65779', 'examples_per_second': '30.291', 'grad_norm': '20.75', 'counters/examples': 111456, 'counters/updates': 3483}
skipping logging after 111488 examples to avoid logging too frequently
train stats after 111520 examples: {'rewards_train/chosen': '0.20084', 'rewards_train/rejected': '0.04367', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15717', 'logps_train/rejected': '-85.061', 'logps_train/chosen': '-139.34', 'loss/train': '0.62514', 'examples_per_second': '31.634', 'grad_norm': '17', 'counters/examples': 111520, 'counters/updates': 3485}
train stats after 111552 examples: {'rewards_train/chosen': '0.07937', 'rewards_train/rejected': '0.035989', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043381', 'logps_train/rejected': '-97.447', 'logps_train/chosen': '-115.05', 'loss/train': '0.67843', 'examples_per_second': '32.747', 'grad_norm': '18.125', 'counters/examples': 111552, 'counters/updates': 3486}
train stats after 111584 examples: {'rewards_train/chosen': '0.10009', 'rewards_train/rejected': '0.07207', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028024', 'logps_train/rejected': '-146.74', 'logps_train/chosen': '-120.38', 'loss/train': '0.69256', 'examples_per_second': '30.134', 'grad_norm': '20.75', 'counters/examples': 111584, 'counters/updates': 3487}
train stats after 111616 examples: {'rewards_train/chosen': '0.07166', 'rewards_train/rejected': '0.031699', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039962', 'logps_train/rejected': '-84.051', 'logps_train/chosen': '-149.97', 'loss/train': '0.68136', 'examples_per_second': '30.292', 'grad_norm': '19.625', 'counters/examples': 111616, 'counters/updates': 3488}
train stats after 111648 examples: {'rewards_train/chosen': '0.11634', 'rewards_train/rejected': '-0.031046', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14739', 'logps_train/rejected': '-95.779', 'logps_train/chosen': '-103.71', 'loss/train': '0.63157', 'examples_per_second': '32.623', 'grad_norm': '17', 'counters/examples': 111648, 'counters/updates': 3489}
train stats after 111680 examples: {'rewards_train/chosen': '0.15331', 'rewards_train/rejected': '-0.068444', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.22176', 'logps_train/rejected': '-81.834', 'logps_train/chosen': '-144.75', 'loss/train': '0.59956', 'examples_per_second': '31.842', 'grad_norm': '18.75', 'counters/examples': 111680, 'counters/updates': 3490}
train stats after 111712 examples: {'rewards_train/chosen': '0.14497', 'rewards_train/rejected': '0.074803', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070171', 'logps_train/rejected': '-102.7', 'logps_train/chosen': '-145.41', 'loss/train': '0.66345', 'examples_per_second': '29.943', 'grad_norm': '20', 'counters/examples': 111712, 'counters/updates': 3491}
train stats after 111744 examples: {'rewards_train/chosen': '0.13455', 'rewards_train/rejected': '0.05414', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080405', 'logps_train/rejected': '-97.981', 'logps_train/chosen': '-150.95', 'loss/train': '0.66745', 'examples_per_second': '30.283', 'grad_norm': '20.25', 'counters/examples': 111744, 'counters/updates': 3492}
train stats after 111776 examples: {'rewards_train/chosen': '0.13429', 'rewards_train/rejected': '-0.022531', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15682', 'logps_train/rejected': '-110.07', 'logps_train/chosen': '-115.41', 'loss/train': '0.62689', 'examples_per_second': '31.623', 'grad_norm': '18.125', 'counters/examples': 111776, 'counters/updates': 3493}
skipping logging after 111808 examples to avoid logging too frequently
train stats after 111840 examples: {'rewards_train/chosen': '0.18676', 'rewards_train/rejected': '0.081152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1056', 'logps_train/rejected': '-106.33', 'logps_train/chosen': '-143.93', 'loss/train': '0.65066', 'examples_per_second': '30.532', 'grad_norm': '19.75', 'counters/examples': 111840, 'counters/updates': 3495}
train stats after 111872 examples: {'rewards_train/chosen': '0.11562', 'rewards_train/rejected': '0.057566', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058056', 'logps_train/rejected': '-82.142', 'logps_train/chosen': '-121.18', 'loss/train': '0.66885', 'examples_per_second': '31.429', 'grad_norm': '17.25', 'counters/examples': 111872, 'counters/updates': 3496}
train stats after 111904 examples: {'rewards_train/chosen': '0.08663', 'rewards_train/rejected': '0.087179', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00054896', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-117.42', 'loss/train': '0.70249', 'examples_per_second': '32.39', 'grad_norm': '19.625', 'counters/examples': 111904, 'counters/updates': 3497}
train stats after 111936 examples: {'rewards_train/chosen': '0.072482', 'rewards_train/rejected': '0.074767', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0022857', 'logps_train/rejected': '-127.43', 'logps_train/chosen': '-107.83', 'loss/train': '0.69941', 'examples_per_second': '31.522', 'grad_norm': '20.375', 'counters/examples': 111936, 'counters/updates': 3498}
train stats after 111968 examples: {'rewards_train/chosen': '0.17585', 'rewards_train/rejected': '0.09454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081311', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-142.14', 'loss/train': '0.66155', 'examples_per_second': '32.161', 'grad_norm': '21', 'counters/examples': 111968, 'counters/updates': 3499}
train stats after 112000 examples: {'rewards_train/chosen': '0.080419', 'rewards_train/rejected': '0.086597', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0061784', 'logps_train/rejected': '-108.61', 'logps_train/chosen': '-112.37', 'loss/train': '0.70518', 'examples_per_second': '30.485', 'grad_norm': '21', 'counters/examples': 112000, 'counters/updates': 3500}
train stats after 112032 examples: {'rewards_train/chosen': '0.1454', 'rewards_train/rejected': '0.067445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07795', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-154.35', 'loss/train': '0.66392', 'examples_per_second': '31.685', 'grad_norm': '21.875', 'counters/examples': 112032, 'counters/updates': 3501}
train stats after 112064 examples: {'rewards_train/chosen': '0.1324', 'rewards_train/rejected': '0.049025', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083371', 'logps_train/rejected': '-156.16', 'logps_train/chosen': '-185.46', 'loss/train': '0.66821', 'examples_per_second': '30.276', 'grad_norm': '24.125', 'counters/examples': 112064, 'counters/updates': 3502}
train stats after 112096 examples: {'rewards_train/chosen': '0.21109', 'rewards_train/rejected': '0.045221', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16587', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-143.03', 'loss/train': '0.62271', 'examples_per_second': '31.013', 'grad_norm': '19.75', 'counters/examples': 112096, 'counters/updates': 3503}
train stats after 112128 examples: {'rewards_train/chosen': '0.18512', 'rewards_train/rejected': '0.072434', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11269', 'logps_train/rejected': '-116.77', 'logps_train/chosen': '-150.06', 'loss/train': '0.64922', 'examples_per_second': '31.083', 'grad_norm': '20.25', 'counters/examples': 112128, 'counters/updates': 3504}
train stats after 112160 examples: {'rewards_train/chosen': '0.20956', 'rewards_train/rejected': '0.052877', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15668', 'logps_train/rejected': '-141.54', 'logps_train/chosen': '-190.3', 'loss/train': '0.62855', 'examples_per_second': '31.589', 'grad_norm': '21.125', 'counters/examples': 112160, 'counters/updates': 3505}
train stats after 112192 examples: {'rewards_train/chosen': '0.13183', 'rewards_train/rejected': '0.053852', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077975', 'logps_train/rejected': '-106.48', 'logps_train/chosen': '-158.23', 'loss/train': '0.66641', 'examples_per_second': '31.931', 'grad_norm': '21.25', 'counters/examples': 112192, 'counters/updates': 3506}
train stats after 112224 examples: {'rewards_train/chosen': '0.16814', 'rewards_train/rejected': '0.066514', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10162', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-135.59', 'loss/train': '0.65236', 'examples_per_second': '31.672', 'grad_norm': '19.5', 'counters/examples': 112224, 'counters/updates': 3507}
skipping logging after 112256 examples to avoid logging too frequently
train stats after 112288 examples: {'rewards_train/chosen': '0.11721', 'rewards_train/rejected': '0.072042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04517', 'logps_train/rejected': '-114.61', 'logps_train/chosen': '-108.36', 'loss/train': '0.67967', 'examples_per_second': '32.716', 'grad_norm': '20', 'counters/examples': 112288, 'counters/updates': 3509}
train stats after 112320 examples: {'rewards_train/chosen': '0.1219', 'rewards_train/rejected': '0.02271', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099187', 'logps_train/rejected': '-105.13', 'logps_train/chosen': '-125.34', 'loss/train': '0.65045', 'examples_per_second': '30.895', 'grad_norm': '18.625', 'counters/examples': 112320, 'counters/updates': 3510}
train stats after 112352 examples: {'rewards_train/chosen': '0.11233', 'rewards_train/rejected': '0.01567', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096664', 'logps_train/rejected': '-91.059', 'logps_train/chosen': '-107', 'loss/train': '0.65712', 'examples_per_second': '32.136', 'grad_norm': '17.25', 'counters/examples': 112352, 'counters/updates': 3511}
skipping logging after 112384 examples to avoid logging too frequently
train stats after 112416 examples: {'rewards_train/chosen': '0.17411', 'rewards_train/rejected': '0.10101', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073102', 'logps_train/rejected': '-116.12', 'logps_train/chosen': '-156.28', 'loss/train': '0.66694', 'examples_per_second': '31.634', 'grad_norm': '21.875', 'counters/examples': 112416, 'counters/updates': 3513}
train stats after 112448 examples: {'rewards_train/chosen': '0.13589', 'rewards_train/rejected': '0.059638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076252', 'logps_train/rejected': '-147.76', 'logps_train/chosen': '-158.08', 'loss/train': '0.66374', 'examples_per_second': '31.647', 'grad_norm': '23.625', 'counters/examples': 112448, 'counters/updates': 3514}
train stats after 112480 examples: {'rewards_train/chosen': '0.18', 'rewards_train/rejected': '0.025356', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15465', 'logps_train/rejected': '-95.764', 'logps_train/chosen': '-122.07', 'loss/train': '0.62713', 'examples_per_second': '33.098', 'grad_norm': '18.375', 'counters/examples': 112480, 'counters/updates': 3515}
train stats after 112512 examples: {'rewards_train/chosen': '0.15545', 'rewards_train/rejected': '0.04641', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10904', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-136.12', 'loss/train': '0.64571', 'examples_per_second': '30.156', 'grad_norm': '20', 'counters/examples': 112512, 'counters/updates': 3516}
train stats after 112544 examples: {'rewards_train/chosen': '0.080118', 'rewards_train/rejected': '0.065808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01431', 'logps_train/rejected': '-99.977', 'logps_train/chosen': '-139.54', 'loss/train': '0.69259', 'examples_per_second': '32.151', 'grad_norm': '19.625', 'counters/examples': 112544, 'counters/updates': 3517}
train stats after 112576 examples: {'rewards_train/chosen': '0.19276', 'rewards_train/rejected': '0.093102', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099659', 'logps_train/rejected': '-118.29', 'logps_train/chosen': '-132.71', 'loss/train': '0.6525', 'examples_per_second': '31.583', 'grad_norm': '20.25', 'counters/examples': 112576, 'counters/updates': 3518}
train stats after 112608 examples: {'rewards_train/chosen': '0.13443', 'rewards_train/rejected': '0.07022', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064208', 'logps_train/rejected': '-94.381', 'logps_train/chosen': '-137.77', 'loss/train': '0.67248', 'examples_per_second': '30.131', 'grad_norm': '19.125', 'counters/examples': 112608, 'counters/updates': 3519}
train stats after 112640 examples: {'rewards_train/chosen': '0.15104', 'rewards_train/rejected': '0.059724', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091314', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-135.9', 'loss/train': '0.65478', 'examples_per_second': '31.662', 'grad_norm': '19.5', 'counters/examples': 112640, 'counters/updates': 3520}
train stats after 112672 examples: {'rewards_train/chosen': '0.12439', 'rewards_train/rejected': '0.055135', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069256', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-150.24', 'loss/train': '0.66614', 'examples_per_second': '30.709', 'grad_norm': '21', 'counters/examples': 112672, 'counters/updates': 3521}
skipping logging after 112704 examples to avoid logging too frequently
train stats after 112736 examples: {'rewards_train/chosen': '0.14261', 'rewards_train/rejected': '0.017466', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12515', 'logps_train/rejected': '-95.375', 'logps_train/chosen': '-137.07', 'loss/train': '0.64451', 'examples_per_second': '33.602', 'grad_norm': '19', 'counters/examples': 112736, 'counters/updates': 3523}
train stats after 112768 examples: {'rewards_train/chosen': '0.15979', 'rewards_train/rejected': '0.060588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099205', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-113.53', 'loss/train': '0.65315', 'examples_per_second': '31.153', 'grad_norm': '20', 'counters/examples': 112768, 'counters/updates': 3524}
skipping logging after 112800 examples to avoid logging too frequently
train stats after 112832 examples: {'rewards_train/chosen': '0.1166', 'rewards_train/rejected': '0.092091', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.024511', 'logps_train/rejected': '-152.78', 'logps_train/chosen': '-145.75', 'loss/train': '0.6895', 'examples_per_second': '31.144', 'grad_norm': '22.625', 'counters/examples': 112832, 'counters/updates': 3526}
skipping logging after 112864 examples to avoid logging too frequently
train stats after 112896 examples: {'rewards_train/chosen': '0.14631', 'rewards_train/rejected': '0.060898', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08541', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-169.44', 'loss/train': '0.65917', 'examples_per_second': '31.995', 'grad_norm': '21.75', 'counters/examples': 112896, 'counters/updates': 3528}
skipping logging after 112928 examples to avoid logging too frequently
train stats after 112960 examples: {'rewards_train/chosen': '0.19864', 'rewards_train/rejected': '0.077219', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12142', 'logps_train/rejected': '-162.06', 'logps_train/chosen': '-138.98', 'loss/train': '0.64632', 'examples_per_second': '31.09', 'grad_norm': '21.75', 'counters/examples': 112960, 'counters/updates': 3530}
train stats after 112992 examples: {'rewards_train/chosen': '0.10538', 'rewards_train/rejected': '0.059554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045821', 'logps_train/rejected': '-127.58', 'logps_train/chosen': '-141.52', 'loss/train': '0.68101', 'examples_per_second': '29.952', 'grad_norm': '21.625', 'counters/examples': 112992, 'counters/updates': 3531}
train stats after 113024 examples: {'rewards_train/chosen': '0.17598', 'rewards_train/rejected': '0.036799', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13918', 'logps_train/rejected': '-148.93', 'logps_train/chosen': '-197.87', 'loss/train': '0.63697', 'examples_per_second': '31.641', 'grad_norm': '22.25', 'counters/examples': 113024, 'counters/updates': 3532}
train stats after 113056 examples: {'rewards_train/chosen': '0.1138', 'rewards_train/rejected': '0.050261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063537', 'logps_train/rejected': '-97.996', 'logps_train/chosen': '-114.96', 'loss/train': '0.66676', 'examples_per_second': '30.185', 'grad_norm': '18', 'counters/examples': 113056, 'counters/updates': 3533}
train stats after 113088 examples: {'rewards_train/chosen': '0.096537', 'rewards_train/rejected': '0.056474', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040063', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-152.08', 'loss/train': '0.68114', 'examples_per_second': '31.586', 'grad_norm': '22.125', 'counters/examples': 113088, 'counters/updates': 3534}
train stats after 113120 examples: {'rewards_train/chosen': '0.1222', 'rewards_train/rejected': '0.055642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066557', 'logps_train/rejected': '-105.07', 'logps_train/chosen': '-125.37', 'loss/train': '0.66809', 'examples_per_second': '30.323', 'grad_norm': '20.375', 'counters/examples': 113120, 'counters/updates': 3535}
train stats after 113152 examples: {'rewards_train/chosen': '0.20703', 'rewards_train/rejected': '0.12964', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077388', 'logps_train/rejected': '-144.38', 'logps_train/chosen': '-167.57', 'loss/train': '0.66369', 'examples_per_second': '31.642', 'grad_norm': '22.625', 'counters/examples': 113152, 'counters/updates': 3536}
train stats after 113184 examples: {'rewards_train/chosen': '0.14343', 'rewards_train/rejected': '0.071638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07179', 'logps_train/rejected': '-142.33', 'logps_train/chosen': '-142.63', 'loss/train': '0.66666', 'examples_per_second': '32.116', 'grad_norm': '21.25', 'counters/examples': 113184, 'counters/updates': 3537}
train stats after 113216 examples: {'rewards_train/chosen': '0.15393', 'rewards_train/rejected': '0.069404', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084526', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-168.08', 'loss/train': '0.66441', 'examples_per_second': '30.718', 'grad_norm': '21.75', 'counters/examples': 113216, 'counters/updates': 3538}
train stats after 113248 examples: {'rewards_train/chosen': '0.11084', 'rewards_train/rejected': '0.050386', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060453', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-120.93', 'loss/train': '0.67516', 'examples_per_second': '31.492', 'grad_norm': '19.25', 'counters/examples': 113248, 'counters/updates': 3539}
skipping logging after 113280 examples to avoid logging too frequently
train stats after 113312 examples: {'rewards_train/chosen': '0.063885', 'rewards_train/rejected': '-0.008291', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072176', 'logps_train/rejected': '-109.57', 'logps_train/chosen': '-142.5', 'loss/train': '0.66589', 'examples_per_second': '31.659', 'grad_norm': '20.875', 'counters/examples': 113312, 'counters/updates': 3541}
skipping logging after 113344 examples to avoid logging too frequently
train stats after 113376 examples: {'rewards_train/chosen': '0.1372', 'rewards_train/rejected': '0.037745', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099459', 'logps_train/rejected': '-96.029', 'logps_train/chosen': '-134.61', 'loss/train': '0.65461', 'examples_per_second': '31.593', 'grad_norm': '19.875', 'counters/examples': 113376, 'counters/updates': 3543}
train stats after 113408 examples: {'rewards_train/chosen': '0.14631', 'rewards_train/rejected': '0.077384', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068922', 'logps_train/rejected': '-103.4', 'logps_train/chosen': '-143.54', 'loss/train': '0.67013', 'examples_per_second': '30.299', 'grad_norm': '20.375', 'counters/examples': 113408, 'counters/updates': 3544}
train stats after 113440 examples: {'rewards_train/chosen': '0.17189', 'rewards_train/rejected': '0.088444', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083443', 'logps_train/rejected': '-93.519', 'logps_train/chosen': '-148.84', 'loss/train': '0.66787', 'examples_per_second': '30.605', 'grad_norm': '19.625', 'counters/examples': 113440, 'counters/updates': 3545}
train stats after 113472 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '0.072608', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074219', 'logps_train/rejected': '-116.42', 'logps_train/chosen': '-148.16', 'loss/train': '0.66557', 'examples_per_second': '30.667', 'grad_norm': '21', 'counters/examples': 113472, 'counters/updates': 3546}
train stats after 113504 examples: {'rewards_train/chosen': '0.13243', 'rewards_train/rejected': '0.019184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11324', 'logps_train/rejected': '-118.29', 'logps_train/chosen': '-133.33', 'loss/train': '0.64385', 'examples_per_second': '33.144', 'grad_norm': '18.5', 'counters/examples': 113504, 'counters/updates': 3547}
train stats after 113536 examples: {'rewards_train/chosen': '0.14286', 'rewards_train/rejected': '0.072005', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070855', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-168.48', 'loss/train': '0.66744', 'examples_per_second': '31.169', 'grad_norm': '22.625', 'counters/examples': 113536, 'counters/updates': 3548}
train stats after 113568 examples: {'rewards_train/chosen': '0.162', 'rewards_train/rejected': '0.10718', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054825', 'logps_train/rejected': '-108.75', 'logps_train/chosen': '-110.41', 'loss/train': '0.66998', 'examples_per_second': '31.664', 'grad_norm': '18.875', 'counters/examples': 113568, 'counters/updates': 3549}
train stats after 113600 examples: {'rewards_train/chosen': '0.14529', 'rewards_train/rejected': '0.026483', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11881', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-120.33', 'loss/train': '0.64086', 'examples_per_second': '31.641', 'grad_norm': '18.25', 'counters/examples': 113600, 'counters/updates': 3550}
train stats after 113632 examples: {'rewards_train/chosen': '0.11382', 'rewards_train/rejected': '0.078675', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03514', 'logps_train/rejected': '-139.72', 'logps_train/chosen': '-171.5', 'loss/train': '0.6824', 'examples_per_second': '24.478', 'grad_norm': '24', 'counters/examples': 113632, 'counters/updates': 3551}
train stats after 113664 examples: {'rewards_train/chosen': '0.20924', 'rewards_train/rejected': '-0.017754', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.227', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-159.92', 'loss/train': '0.59832', 'examples_per_second': '31.969', 'grad_norm': '19.25', 'counters/examples': 113664, 'counters/updates': 3552}
train stats after 113696 examples: {'rewards_train/chosen': '0.11653', 'rewards_train/rejected': '-0.0091422', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12568', 'logps_train/rejected': '-74.813', 'logps_train/chosen': '-110.24', 'loss/train': '0.63841', 'examples_per_second': '32.511', 'grad_norm': '16.5', 'counters/examples': 113696, 'counters/updates': 3553}
train stats after 113728 examples: {'rewards_train/chosen': '0.16173', 'rewards_train/rejected': '0.1209', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040825', 'logps_train/rejected': '-117.24', 'logps_train/chosen': '-119.44', 'loss/train': '0.68301', 'examples_per_second': '24.88', 'grad_norm': '21.25', 'counters/examples': 113728, 'counters/updates': 3554}
train stats after 113760 examples: {'rewards_train/chosen': '0.11674', 'rewards_train/rejected': '-0.026031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14277', 'logps_train/rejected': '-114.22', 'logps_train/chosen': '-120.12', 'loss/train': '0.63757', 'examples_per_second': '32.42', 'grad_norm': '19.125', 'counters/examples': 113760, 'counters/updates': 3555}
train stats after 113792 examples: {'rewards_train/chosen': '0.12748', 'rewards_train/rejected': '-0.012877', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.14035', 'logps_train/rejected': '-95.58', 'logps_train/chosen': '-148.94', 'loss/train': '0.63063', 'examples_per_second': '31.664', 'grad_norm': '19.75', 'counters/examples': 113792, 'counters/updates': 3556}
skipping logging after 113824 examples to avoid logging too frequently
train stats after 113856 examples: {'rewards_train/chosen': '0.12882', 'rewards_train/rejected': '0.070208', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058616', 'logps_train/rejected': '-149.81', 'logps_train/chosen': '-151.53', 'loss/train': '0.66975', 'examples_per_second': '31.757', 'grad_norm': '22.5', 'counters/examples': 113856, 'counters/updates': 3558}
train stats after 113888 examples: {'rewards_train/chosen': '0.15635', 'rewards_train/rejected': '0.053363', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10298', 'logps_train/rejected': '-134.16', 'logps_train/chosen': '-150.53', 'loss/train': '0.65163', 'examples_per_second': '31.307', 'grad_norm': '21.625', 'counters/examples': 113888, 'counters/updates': 3559}
train stats after 113920 examples: {'rewards_train/chosen': '0.12055', 'rewards_train/rejected': '0.11608', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0044748', 'logps_train/rejected': '-135.64', 'logps_train/chosen': '-138.46', 'loss/train': '0.70018', 'examples_per_second': '32.482', 'grad_norm': '21.5', 'counters/examples': 113920, 'counters/updates': 3560}
skipping logging after 113952 examples to avoid logging too frequently
train stats after 113984 examples: {'rewards_train/chosen': '0.181', 'rewards_train/rejected': '0.067658', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11335', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-122.22', 'loss/train': '0.647', 'examples_per_second': '30.699', 'grad_norm': '18.75', 'counters/examples': 113984, 'counters/updates': 3562}
skipping logging after 114016 examples to avoid logging too frequently
train stats after 114048 examples: {'rewards_train/chosen': '0.12266', 'rewards_train/rejected': '-0.015288', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13794', 'logps_train/rejected': '-94.281', 'logps_train/chosen': '-106.31', 'loss/train': '0.64214', 'examples_per_second': '34.719', 'grad_norm': '16.875', 'counters/examples': 114048, 'counters/updates': 3564}
skipping logging after 114080 examples to avoid logging too frequently
train stats after 114112 examples: {'rewards_train/chosen': '0.17774', 'rewards_train/rejected': '0.070803', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10693', 'logps_train/rejected': '-118.82', 'logps_train/chosen': '-122.32', 'loss/train': '0.6558', 'examples_per_second': '32.775', 'grad_norm': '20.125', 'counters/examples': 114112, 'counters/updates': 3566}
skipping logging after 114144 examples to avoid logging too frequently
train stats after 114176 examples: {'rewards_train/chosen': '0.13071', 'rewards_train/rejected': '0.0033007', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12741', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-144.07', 'loss/train': '0.6401', 'examples_per_second': '34.531', 'grad_norm': '19.375', 'counters/examples': 114176, 'counters/updates': 3568}
train stats after 114208 examples: {'rewards_train/chosen': '0.06246', 'rewards_train/rejected': '0.065689', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.003229', 'logps_train/rejected': '-104.31', 'logps_train/chosen': '-98.008', 'loss/train': '0.70089', 'examples_per_second': '32.574', 'grad_norm': '18.75', 'counters/examples': 114208, 'counters/updates': 3569}
train stats after 114240 examples: {'rewards_train/chosen': '0.14928', 'rewards_train/rejected': '0.024857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12442', 'logps_train/rejected': '-149.68', 'logps_train/chosen': '-140.01', 'loss/train': '0.64855', 'examples_per_second': '30.009', 'grad_norm': '21.5', 'counters/examples': 114240, 'counters/updates': 3570}
train stats after 114272 examples: {'rewards_train/chosen': '0.11438', 'rewards_train/rejected': '0.052326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062056', 'logps_train/rejected': '-114.12', 'logps_train/chosen': '-120.93', 'loss/train': '0.67036', 'examples_per_second': '30.953', 'grad_norm': '18.25', 'counters/examples': 114272, 'counters/updates': 3571}
train stats after 114304 examples: {'rewards_train/chosen': '0.11177', 'rewards_train/rejected': '-0.0046561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11642', 'logps_train/rejected': '-95.054', 'logps_train/chosen': '-120.48', 'loss/train': '0.65238', 'examples_per_second': '30.178', 'grad_norm': '18.75', 'counters/examples': 114304, 'counters/updates': 3572}
train stats after 114336 examples: {'rewards_train/chosen': '0.16154', 'rewards_train/rejected': '0.08698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074563', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-143.23', 'loss/train': '0.66363', 'examples_per_second': '31.753', 'grad_norm': '20.375', 'counters/examples': 114336, 'counters/updates': 3573}
train stats after 114368 examples: {'rewards_train/chosen': '0.15386', 'rewards_train/rejected': '0.071521', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082338', 'logps_train/rejected': '-133.18', 'logps_train/chosen': '-143.23', 'loss/train': '0.65933', 'examples_per_second': '31.647', 'grad_norm': '20.875', 'counters/examples': 114368, 'counters/updates': 3574}
train stats after 114400 examples: {'rewards_train/chosen': '0.084918', 'rewards_train/rejected': '0.049402', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.035516', 'logps_train/rejected': '-102.87', 'logps_train/chosen': '-131.72', 'loss/train': '0.68784', 'examples_per_second': '32.771', 'grad_norm': '20', 'counters/examples': 114400, 'counters/updates': 3575}
train stats after 114432 examples: {'rewards_train/chosen': '0.14177', 'rewards_train/rejected': '-0.022695', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16447', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-144.23', 'loss/train': '0.62548', 'examples_per_second': '31.628', 'grad_norm': '18.625', 'counters/examples': 114432, 'counters/updates': 3576}
train stats after 114464 examples: {'rewards_train/chosen': '0.10784', 'rewards_train/rejected': '0.1067', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0011441', 'logps_train/rejected': '-147.73', 'logps_train/chosen': '-126.88', 'loss/train': '0.70155', 'examples_per_second': '30.129', 'grad_norm': '21.625', 'counters/examples': 114464, 'counters/updates': 3577}
train stats after 114496 examples: {'rewards_train/chosen': '0.17154', 'rewards_train/rejected': '0.0064675', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16507', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-151.85', 'loss/train': '0.61906', 'examples_per_second': '30.876', 'grad_norm': '19.75', 'counters/examples': 114496, 'counters/updates': 3578}
skipping logging after 114528 examples to avoid logging too frequently
train stats after 114560 examples: {'rewards_train/chosen': '0.19137', 'rewards_train/rejected': '0.068617', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12275', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-195.84', 'loss/train': '0.6438', 'examples_per_second': '31.674', 'grad_norm': '21.875', 'counters/examples': 114560, 'counters/updates': 3580}
train stats after 114592 examples: {'rewards_train/chosen': '0.16053', 'rewards_train/rejected': '0.058545', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10198', 'logps_train/rejected': '-99.909', 'logps_train/chosen': '-144.82', 'loss/train': '0.66199', 'examples_per_second': '31.86', 'grad_norm': '19.375', 'counters/examples': 114592, 'counters/updates': 3581}
skipping logging after 114624 examples to avoid logging too frequently
train stats after 114656 examples: {'rewards_train/chosen': '0.14998', 'rewards_train/rejected': '-0.0079549', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15794', 'logps_train/rejected': '-104.55', 'logps_train/chosen': '-117.63', 'loss/train': '0.62951', 'examples_per_second': '32.19', 'grad_norm': '18.125', 'counters/examples': 114656, 'counters/updates': 3583}
train stats after 114688 examples: {'rewards_train/chosen': '0.1274', 'rewards_train/rejected': '0.067037', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060363', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-141.81', 'loss/train': '0.6694', 'examples_per_second': '31.391', 'grad_norm': '21.75', 'counters/examples': 114688, 'counters/updates': 3584}
train stats after 114720 examples: {'rewards_train/chosen': '0.11496', 'rewards_train/rejected': '-0.023683', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13864', 'logps_train/rejected': '-136.29', 'logps_train/chosen': '-139.08', 'loss/train': '0.63635', 'examples_per_second': '31.625', 'grad_norm': '19.5', 'counters/examples': 114720, 'counters/updates': 3585}
train stats after 114752 examples: {'rewards_train/chosen': '0.11873', 'rewards_train/rejected': '0.076071', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042663', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-150.27', 'loss/train': '0.68578', 'examples_per_second': '31.914', 'grad_norm': '21.25', 'counters/examples': 114752, 'counters/updates': 3586}
train stats after 114784 examples: {'rewards_train/chosen': '0.15073', 'rewards_train/rejected': '0.019294', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13143', 'logps_train/rejected': '-111', 'logps_train/chosen': '-147.52', 'loss/train': '0.64008', 'examples_per_second': '31.646', 'grad_norm': '19.375', 'counters/examples': 114784, 'counters/updates': 3587}
train stats after 114816 examples: {'rewards_train/chosen': '0.16612', 'rewards_train/rejected': '0.049577', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11654', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-178.7', 'loss/train': '0.64456', 'examples_per_second': '31.782', 'grad_norm': '21.625', 'counters/examples': 114816, 'counters/updates': 3588}
skipping logging after 114848 examples to avoid logging too frequently
train stats after 114880 examples: {'rewards_train/chosen': '0.19932', 'rewards_train/rejected': '0.10132', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097998', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-126.69', 'loss/train': '0.65835', 'examples_per_second': '32.183', 'grad_norm': '19.875', 'counters/examples': 114880, 'counters/updates': 3590}
train stats after 114912 examples: {'rewards_train/chosen': '0.17145', 'rewards_train/rejected': '0.057477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11397', 'logps_train/rejected': '-133.03', 'logps_train/chosen': '-165.3', 'loss/train': '0.64807', 'examples_per_second': '30.12', 'grad_norm': '21', 'counters/examples': 114912, 'counters/updates': 3591}
train stats after 114944 examples: {'rewards_train/chosen': '0.18356', 'rewards_train/rejected': '0.042491', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14107', 'logps_train/rejected': '-100.83', 'logps_train/chosen': '-151.22', 'loss/train': '0.6323', 'examples_per_second': '32.571', 'grad_norm': '19.25', 'counters/examples': 114944, 'counters/updates': 3592}
train stats after 114976 examples: {'rewards_train/chosen': '0.054443', 'rewards_train/rejected': '0.03949', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014953', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-115', 'loss/train': '0.68873', 'examples_per_second': '31.091', 'grad_norm': '18.625', 'counters/examples': 114976, 'counters/updates': 3593}
train stats after 115008 examples: {'rewards_train/chosen': '0.086084', 'rewards_train/rejected': '0.017928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068157', 'logps_train/rejected': '-108.84', 'logps_train/chosen': '-116.9', 'loss/train': '0.66592', 'examples_per_second': '31.147', 'grad_norm': '18.75', 'counters/examples': 115008, 'counters/updates': 3594}
skipping logging after 115040 examples to avoid logging too frequently
train stats after 115072 examples: {'rewards_train/chosen': '0.20755', 'rewards_train/rejected': '0.078217', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12934', 'logps_train/rejected': '-123.95', 'logps_train/chosen': '-149.11', 'loss/train': '0.64407', 'examples_per_second': '31.219', 'grad_norm': '21.625', 'counters/examples': 115072, 'counters/updates': 3596}
train stats after 115104 examples: {'rewards_train/chosen': '0.099673', 'rewards_train/rejected': '0.082865', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016808', 'logps_train/rejected': '-104.66', 'logps_train/chosen': '-107.22', 'loss/train': '0.69213', 'examples_per_second': '30.648', 'grad_norm': '18.625', 'counters/examples': 115104, 'counters/updates': 3597}
skipping logging after 115136 examples to avoid logging too frequently
train stats after 115168 examples: {'rewards_train/chosen': '0.074872', 'rewards_train/rejected': '0.034009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040863', 'logps_train/rejected': '-104.51', 'logps_train/chosen': '-134.66', 'loss/train': '0.6777', 'examples_per_second': '31.824', 'grad_norm': '19.625', 'counters/examples': 115168, 'counters/updates': 3599}
skipping logging after 115200 examples to avoid logging too frequently
train stats after 115232 examples: {'rewards_train/chosen': '0.070834', 'rewards_train/rejected': '0.016907', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.053927', 'logps_train/rejected': '-93.043', 'logps_train/chosen': '-111.82', 'loss/train': '0.6736', 'examples_per_second': '34.608', 'grad_norm': '17.125', 'counters/examples': 115232, 'counters/updates': 3601}
skipping logging after 115264 examples to avoid logging too frequently
train stats after 115296 examples: {'rewards_train/chosen': '0.16845', 'rewards_train/rejected': '0.016197', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15226', 'logps_train/rejected': '-113.6', 'logps_train/chosen': '-141.76', 'loss/train': '0.627', 'examples_per_second': '31.652', 'grad_norm': '19.25', 'counters/examples': 115296, 'counters/updates': 3603}
train stats after 115328 examples: {'rewards_train/chosen': '0.13117', 'rewards_train/rejected': '0.032495', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098673', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-100.22', 'loss/train': '0.65158', 'examples_per_second': '31.6', 'grad_norm': '18.625', 'counters/examples': 115328, 'counters/updates': 3604}
train stats after 115360 examples: {'rewards_train/chosen': '0.084094', 'rewards_train/rejected': '-0.016834', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10093', 'logps_train/rejected': '-105.56', 'logps_train/chosen': '-160.77', 'loss/train': '0.65471', 'examples_per_second': '32.681', 'grad_norm': '20.5', 'counters/examples': 115360, 'counters/updates': 3605}
skipping logging after 115392 examples to avoid logging too frequently
train stats after 115424 examples: {'rewards_train/chosen': '0.14022', 'rewards_train/rejected': '0.058198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082026', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-133.45', 'loss/train': '0.66169', 'examples_per_second': '30.106', 'grad_norm': '19.875', 'counters/examples': 115424, 'counters/updates': 3607}
train stats after 115456 examples: {'rewards_train/chosen': '0.11022', 'rewards_train/rejected': '0.0074059', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10281', 'logps_train/rejected': '-112.36', 'logps_train/chosen': '-143.19', 'loss/train': '0.65137', 'examples_per_second': '30.479', 'grad_norm': '20.5', 'counters/examples': 115456, 'counters/updates': 3608}
skipping logging after 115488 examples to avoid logging too frequently
train stats after 115520 examples: {'rewards_train/chosen': '0.1189', 'rewards_train/rejected': '0.079941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038958', 'logps_train/rejected': '-139.74', 'logps_train/chosen': '-165.45', 'loss/train': '0.68758', 'examples_per_second': '30.248', 'grad_norm': '22.625', 'counters/examples': 115520, 'counters/updates': 3610}
train stats after 115552 examples: {'rewards_train/chosen': '0.12356', 'rewards_train/rejected': '-0.021178', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14474', 'logps_train/rejected': '-110.06', 'logps_train/chosen': '-153.58', 'loss/train': '0.63039', 'examples_per_second': '31.935', 'grad_norm': '19.375', 'counters/examples': 115552, 'counters/updates': 3611}
skipping logging after 115584 examples to avoid logging too frequently
train stats after 115616 examples: {'rewards_train/chosen': '0.13584', 'rewards_train/rejected': '0.069989', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.065855', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-132.18', 'loss/train': '0.66708', 'examples_per_second': '31.621', 'grad_norm': '20.75', 'counters/examples': 115616, 'counters/updates': 3613}
train stats after 115648 examples: {'rewards_train/chosen': '0.094434', 'rewards_train/rejected': '0.069534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0249', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-118.05', 'loss/train': '0.69153', 'examples_per_second': '31.398', 'grad_norm': '19', 'counters/examples': 115648, 'counters/updates': 3614}
train stats after 115680 examples: {'rewards_train/chosen': '0.14485', 'rewards_train/rejected': '0.17975', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034899', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-154.25', 'loss/train': '0.71918', 'examples_per_second': '31.333', 'grad_norm': '23.125', 'counters/examples': 115680, 'counters/updates': 3615}
train stats after 115712 examples: {'rewards_train/chosen': '0.067354', 'rewards_train/rejected': '0.10079', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033435', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-134.95', 'loss/train': '0.71845', 'examples_per_second': '30.224', 'grad_norm': '21.5', 'counters/examples': 115712, 'counters/updates': 3616}
train stats after 115744 examples: {'rewards_train/chosen': '0.058486', 'rewards_train/rejected': '0.018423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040063', 'logps_train/rejected': '-104.65', 'logps_train/chosen': '-103.06', 'loss/train': '0.68044', 'examples_per_second': '31.204', 'grad_norm': '17.625', 'counters/examples': 115744, 'counters/updates': 3617}
train stats after 115776 examples: {'rewards_train/chosen': '0.13036', 'rewards_train/rejected': '0.043871', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086487', 'logps_train/rejected': '-135.42', 'logps_train/chosen': '-187.45', 'loss/train': '0.66197', 'examples_per_second': '31.417', 'grad_norm': '22.125', 'counters/examples': 115776, 'counters/updates': 3618}
train stats after 115808 examples: {'rewards_train/chosen': '0.24858', 'rewards_train/rejected': '0.051165', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19741', 'logps_train/rejected': '-159.91', 'logps_train/chosen': '-155.31', 'loss/train': '0.6106', 'examples_per_second': '24.465', 'grad_norm': '21.625', 'counters/examples': 115808, 'counters/updates': 3619}
train stats after 115840 examples: {'rewards_train/chosen': '0.15908', 'rewards_train/rejected': '0.026041', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13304', 'logps_train/rejected': '-101.97', 'logps_train/chosen': '-129.25', 'loss/train': '0.63438', 'examples_per_second': '32.12', 'grad_norm': '18.875', 'counters/examples': 115840, 'counters/updates': 3620}
skipping logging after 115872 examples to avoid logging too frequently
train stats after 115904 examples: {'rewards_train/chosen': '0.18695', 'rewards_train/rejected': '0.0035921', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18336', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-169.82', 'loss/train': '0.61952', 'examples_per_second': '31.618', 'grad_norm': '20.125', 'counters/examples': 115904, 'counters/updates': 3622}
skipping logging after 115936 examples to avoid logging too frequently
train stats after 115968 examples: {'rewards_train/chosen': '0.15036', 'rewards_train/rejected': '0.080326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070037', 'logps_train/rejected': '-108.37', 'logps_train/chosen': '-150.04', 'loss/train': '0.66949', 'examples_per_second': '30.142', 'grad_norm': '19.75', 'counters/examples': 115968, 'counters/updates': 3624}
train stats after 116000 examples: {'rewards_train/chosen': '0.15233', 'rewards_train/rejected': '0.10974', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042589', 'logps_train/rejected': '-117.35', 'logps_train/chosen': '-137.47', 'loss/train': '0.68405', 'examples_per_second': '30.855', 'grad_norm': '21.125', 'counters/examples': 116000, 'counters/updates': 3625}
train stats after 116032 examples: {'rewards_train/chosen': '0.10516', 'rewards_train/rejected': '0.063954', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041204', 'logps_train/rejected': '-124.36', 'logps_train/chosen': '-114.67', 'loss/train': '0.6832', 'examples_per_second': '30.631', 'grad_norm': '19.75', 'counters/examples': 116032, 'counters/updates': 3626}
train stats after 116064 examples: {'rewards_train/chosen': '0.14608', 'rewards_train/rejected': '0.052276', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093801', 'logps_train/rejected': '-104.37', 'logps_train/chosen': '-121.15', 'loss/train': '0.65674', 'examples_per_second': '31.193', 'grad_norm': '17.75', 'counters/examples': 116064, 'counters/updates': 3627}
train stats after 116096 examples: {'rewards_train/chosen': '0.17501', 'rewards_train/rejected': '0.14426', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030749', 'logps_train/rejected': '-146.11', 'logps_train/chosen': '-145.02', 'loss/train': '0.6852', 'examples_per_second': '31.53', 'grad_norm': '23.625', 'counters/examples': 116096, 'counters/updates': 3628}
skipping logging after 116128 examples to avoid logging too frequently
train stats after 116160 examples: {'rewards_train/chosen': '0.15365', 'rewards_train/rejected': '0.11741', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036244', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-119.23', 'loss/train': '0.68323', 'examples_per_second': '30.657', 'grad_norm': '20.5', 'counters/examples': 116160, 'counters/updates': 3630}
train stats after 116192 examples: {'rewards_train/chosen': '0.15661', 'rewards_train/rejected': '-0.015492', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.17211', 'logps_train/rejected': '-137.82', 'logps_train/chosen': '-122.5', 'loss/train': '0.61746', 'examples_per_second': '30.132', 'grad_norm': '20.125', 'counters/examples': 116192, 'counters/updates': 3631}
train stats after 116224 examples: {'rewards_train/chosen': '0.19207', 'rewards_train/rejected': '0.066279', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12579', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-151.89', 'loss/train': '0.64057', 'examples_per_second': '30.709', 'grad_norm': '20.375', 'counters/examples': 116224, 'counters/updates': 3632}
skipping logging after 116256 examples to avoid logging too frequently
train stats after 116288 examples: {'rewards_train/chosen': '0.08569', 'rewards_train/rejected': '0.058589', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027102', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-137.05', 'loss/train': '0.68906', 'examples_per_second': '30.022', 'grad_norm': '20.25', 'counters/examples': 116288, 'counters/updates': 3634}
train stats after 116320 examples: {'rewards_train/chosen': '0.18212', 'rewards_train/rejected': '0.015131', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16699', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-139.68', 'loss/train': '0.62013', 'examples_per_second': '31.43', 'grad_norm': '20.125', 'counters/examples': 116320, 'counters/updates': 3635}
train stats after 116352 examples: {'rewards_train/chosen': '0.13706', 'rewards_train/rejected': '0.035205', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10186', 'logps_train/rejected': '-94.183', 'logps_train/chosen': '-121.37', 'loss/train': '0.65732', 'examples_per_second': '32.18', 'grad_norm': '18.625', 'counters/examples': 116352, 'counters/updates': 3636}
train stats after 116384 examples: {'rewards_train/chosen': '0.19045', 'rewards_train/rejected': '0.078597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11185', 'logps_train/rejected': '-148.49', 'logps_train/chosen': '-125.79', 'loss/train': '0.64752', 'examples_per_second': '30.112', 'grad_norm': '20.25', 'counters/examples': 116384, 'counters/updates': 3637}
train stats after 116416 examples: {'rewards_train/chosen': '0.2005', 'rewards_train/rejected': '0.091625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10887', 'logps_train/rejected': '-129.87', 'logps_train/chosen': '-159.96', 'loss/train': '0.64957', 'examples_per_second': '31.599', 'grad_norm': '22', 'counters/examples': 116416, 'counters/updates': 3638}
train stats after 116448 examples: {'rewards_train/chosen': '0.10553', 'rewards_train/rejected': '0.069305', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036228', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-124.24', 'loss/train': '0.68517', 'examples_per_second': '31.951', 'grad_norm': '20.25', 'counters/examples': 116448, 'counters/updates': 3639}
train stats after 116480 examples: {'rewards_train/chosen': '0.12224', 'rewards_train/rejected': '0.018804', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10344', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-136.3', 'loss/train': '0.65405', 'examples_per_second': '30.588', 'grad_norm': '19.5', 'counters/examples': 116480, 'counters/updates': 3640}
skipping logging after 116512 examples to avoid logging too frequently
train stats after 116544 examples: {'rewards_train/chosen': '0.17485', 'rewards_train/rejected': '0.06928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10557', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-154.25', 'loss/train': '0.64887', 'examples_per_second': '29.979', 'grad_norm': '21', 'counters/examples': 116544, 'counters/updates': 3642}
train stats after 116576 examples: {'rewards_train/chosen': '0.11643', 'rewards_train/rejected': '0.083918', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032513', 'logps_train/rejected': '-97.893', 'logps_train/chosen': '-117.67', 'loss/train': '0.68529', 'examples_per_second': '31.624', 'grad_norm': '18', 'counters/examples': 116576, 'counters/updates': 3643}
train stats after 116608 examples: {'rewards_train/chosen': '0.146', 'rewards_train/rejected': '0.0021305', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14387', 'logps_train/rejected': '-103.01', 'logps_train/chosen': '-107.88', 'loss/train': '0.62906', 'examples_per_second': '31.744', 'grad_norm': '17.5', 'counters/examples': 116608, 'counters/updates': 3644}
train stats after 116640 examples: {'rewards_train/chosen': '0.11518', 'rewards_train/rejected': '-0.015191', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13037', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-136.16', 'loss/train': '0.63513', 'examples_per_second': '31.501', 'grad_norm': '18.875', 'counters/examples': 116640, 'counters/updates': 3645}
train stats after 116672 examples: {'rewards_train/chosen': '0.13198', 'rewards_train/rejected': '-0.017478', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14946', 'logps_train/rejected': '-94.749', 'logps_train/chosen': '-111.73', 'loss/train': '0.62482', 'examples_per_second': '32.475', 'grad_norm': '17.5', 'counters/examples': 116672, 'counters/updates': 3646}
skipping logging after 116704 examples to avoid logging too frequently
train stats after 116736 examples: {'rewards_train/chosen': '0.14367', 'rewards_train/rejected': '0.049253', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094415', 'logps_train/rejected': '-141.36', 'logps_train/chosen': '-134.5', 'loss/train': '0.66121', 'examples_per_second': '31.076', 'grad_norm': '20.75', 'counters/examples': 116736, 'counters/updates': 3648}
train stats after 116768 examples: {'rewards_train/chosen': '0.1892', 'rewards_train/rejected': '0.085882', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10332', 'logps_train/rejected': '-139.13', 'logps_train/chosen': '-109.2', 'loss/train': '0.65606', 'examples_per_second': '31.556', 'grad_norm': '19.875', 'counters/examples': 116768, 'counters/updates': 3649}
skipping logging after 116800 examples to avoid logging too frequently
train stats after 116832 examples: {'rewards_train/chosen': '0.18', 'rewards_train/rejected': '0.060018', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11998', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-152.33', 'loss/train': '0.64613', 'examples_per_second': '31.036', 'grad_norm': '20.75', 'counters/examples': 116832, 'counters/updates': 3651}
train stats after 116864 examples: {'rewards_train/chosen': '0.19175', 'rewards_train/rejected': '0.076334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11542', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-98.06', 'loss/train': '0.64716', 'examples_per_second': '31.59', 'grad_norm': '18.75', 'counters/examples': 116864, 'counters/updates': 3652}
train stats after 116896 examples: {'rewards_train/chosen': '0.14461', 'rewards_train/rejected': '0.032849', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11176', 'logps_train/rejected': '-112.92', 'logps_train/chosen': '-126.97', 'loss/train': '0.65511', 'examples_per_second': '30.272', 'grad_norm': '23', 'counters/examples': 116896, 'counters/updates': 3653}
train stats after 116928 examples: {'rewards_train/chosen': '0.12228', 'rewards_train/rejected': '0.071522', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050761', 'logps_train/rejected': '-128.49', 'logps_train/chosen': '-132.88', 'loss/train': '0.6766', 'examples_per_second': '31.598', 'grad_norm': '20.5', 'counters/examples': 116928, 'counters/updates': 3654}
train stats after 116960 examples: {'rewards_train/chosen': '0.15489', 'rewards_train/rejected': '0.075081', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07981', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-127.94', 'loss/train': '0.65859', 'examples_per_second': '31.256', 'grad_norm': '21.75', 'counters/examples': 116960, 'counters/updates': 3655}
train stats after 116992 examples: {'rewards_train/chosen': '0.082278', 'rewards_train/rejected': '-0.017408', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099686', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-155.26', 'loss/train': '0.65025', 'examples_per_second': '31.509', 'grad_norm': '21.125', 'counters/examples': 116992, 'counters/updates': 3656}
train stats after 117024 examples: {'rewards_train/chosen': '0.14637', 'rewards_train/rejected': '0.038405', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10796', 'logps_train/rejected': '-99.257', 'logps_train/chosen': '-141.68', 'loss/train': '0.64879', 'examples_per_second': '32.067', 'grad_norm': '18.75', 'counters/examples': 117024, 'counters/updates': 3657}
train stats after 117056 examples: {'rewards_train/chosen': '0.11696', 'rewards_train/rejected': '0.0088089', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10815', 'logps_train/rejected': '-97.624', 'logps_train/chosen': '-132.12', 'loss/train': '0.64656', 'examples_per_second': '31.59', 'grad_norm': '18.125', 'counters/examples': 117056, 'counters/updates': 3658}
train stats after 117088 examples: {'rewards_train/chosen': '0.1376', 'rewards_train/rejected': '0.085276', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052326', 'logps_train/rejected': '-100.61', 'logps_train/chosen': '-116.38', 'loss/train': '0.67433', 'examples_per_second': '31.049', 'grad_norm': '19.125', 'counters/examples': 117088, 'counters/updates': 3659}
train stats after 117120 examples: {'rewards_train/chosen': '0.094313', 'rewards_train/rejected': '0.082804', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011509', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-153', 'loss/train': '0.69428', 'examples_per_second': '32.906', 'grad_norm': '23', 'counters/examples': 117120, 'counters/updates': 3660}
skipping logging after 117152 examples to avoid logging too frequently
train stats after 117184 examples: {'rewards_train/chosen': '0.17104', 'rewards_train/rejected': '0.044303', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12674', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-114.08', 'loss/train': '0.6479', 'examples_per_second': '30.653', 'grad_norm': '19.125', 'counters/examples': 117184, 'counters/updates': 3662}
train stats after 117216 examples: {'rewards_train/chosen': '0.11382', 'rewards_train/rejected': '0.021763', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092062', 'logps_train/rejected': '-106.63', 'logps_train/chosen': '-132.65', 'loss/train': '0.65487', 'examples_per_second': '32.123', 'grad_norm': '19.125', 'counters/examples': 117216, 'counters/updates': 3663}
train stats after 117248 examples: {'rewards_train/chosen': '0.1112', 'rewards_train/rejected': '0.022175', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089027', 'logps_train/rejected': '-111.94', 'logps_train/chosen': '-113.72', 'loss/train': '0.65886', 'examples_per_second': '30.283', 'grad_norm': '17.375', 'counters/examples': 117248, 'counters/updates': 3664}
train stats after 117280 examples: {'rewards_train/chosen': '0.08948', 'rewards_train/rejected': '0.053238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036243', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-157.69', 'loss/train': '0.68693', 'examples_per_second': '30.277', 'grad_norm': '22.875', 'counters/examples': 117280, 'counters/updates': 3665}
train stats after 117312 examples: {'rewards_train/chosen': '0.15538', 'rewards_train/rejected': '0.06182', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.093564', 'logps_train/rejected': '-113.56', 'logps_train/chosen': '-130.79', 'loss/train': '0.65157', 'examples_per_second': '31.6', 'grad_norm': '18.875', 'counters/examples': 117312, 'counters/updates': 3666}
train stats after 117344 examples: {'rewards_train/chosen': '0.023507', 'rewards_train/rejected': '-0.028152', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.051659', 'logps_train/rejected': '-96.469', 'logps_train/chosen': '-131.74', 'loss/train': '0.67054', 'examples_per_second': '30.084', 'grad_norm': '20.375', 'counters/examples': 117344, 'counters/updates': 3667}
train stats after 117376 examples: {'rewards_train/chosen': '0.12838', 'rewards_train/rejected': '0.082164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046217', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-167.96', 'loss/train': '0.6772', 'examples_per_second': '32.493', 'grad_norm': '21.875', 'counters/examples': 117376, 'counters/updates': 3668}
skipping logging after 117408 examples to avoid logging too frequently
train stats after 117440 examples: {'rewards_train/chosen': '0.093381', 'rewards_train/rejected': '0.070078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023302', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-113.18', 'loss/train': '0.68785', 'examples_per_second': '31.608', 'grad_norm': '19.75', 'counters/examples': 117440, 'counters/updates': 3670}
train stats after 117472 examples: {'rewards_train/chosen': '0.10865', 'rewards_train/rejected': '-0.022916', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13156', 'logps_train/rejected': '-140.28', 'logps_train/chosen': '-147.89', 'loss/train': '0.63619', 'examples_per_second': '31.605', 'grad_norm': '20.125', 'counters/examples': 117472, 'counters/updates': 3671}
skipping logging after 117504 examples to avoid logging too frequently
train stats after 117536 examples: {'rewards_train/chosen': '0.15516', 'rewards_train/rejected': '0.028459', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1267', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-142.15', 'loss/train': '0.64282', 'examples_per_second': '32.672', 'grad_norm': '19.375', 'counters/examples': 117536, 'counters/updates': 3673}
train stats after 117568 examples: {'rewards_train/chosen': '0.036126', 'rewards_train/rejected': '-0.068804', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10493', 'logps_train/rejected': '-96.593', 'logps_train/chosen': '-108.68', 'loss/train': '0.64975', 'examples_per_second': '32.87', 'grad_norm': '17.125', 'counters/examples': 117568, 'counters/updates': 3674}
train stats after 117600 examples: {'rewards_train/chosen': '0.13074', 'rewards_train/rejected': '0.059025', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071711', 'logps_train/rejected': '-97.464', 'logps_train/chosen': '-158.39', 'loss/train': '0.67323', 'examples_per_second': '31.539', 'grad_norm': '21.375', 'counters/examples': 117600, 'counters/updates': 3675}
skipping logging after 117632 examples to avoid logging too frequently
train stats after 117664 examples: {'rewards_train/chosen': '0.1221', 'rewards_train/rejected': '0.014683', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10742', 'logps_train/rejected': '-97.693', 'logps_train/chosen': '-106.98', 'loss/train': '0.65107', 'examples_per_second': '30.693', 'grad_norm': '17.5', 'counters/examples': 117664, 'counters/updates': 3677}
train stats after 117696 examples: {'rewards_train/chosen': '0.12191', 'rewards_train/rejected': '0.028449', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.09346', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-162.04', 'loss/train': '0.65662', 'examples_per_second': '30.492', 'grad_norm': '21.125', 'counters/examples': 117696, 'counters/updates': 3678}
skipping logging after 117728 examples to avoid logging too frequently
train stats after 117760 examples: {'rewards_train/chosen': '0.074662', 'rewards_train/rejected': '0.053099', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021563', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-119.03', 'loss/train': '0.69042', 'examples_per_second': '31.584', 'grad_norm': '18.875', 'counters/examples': 117760, 'counters/updates': 3680}
skipping logging after 117792 examples to avoid logging too frequently
train stats after 117824 examples: {'rewards_train/chosen': '0.20811', 'rewards_train/rejected': '0.078631', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12948', 'logps_train/rejected': '-104.64', 'logps_train/chosen': '-135.32', 'loss/train': '0.63613', 'examples_per_second': '30.459', 'grad_norm': '18.25', 'counters/examples': 117824, 'counters/updates': 3682}
skipping logging after 117856 examples to avoid logging too frequently
train stats after 117888 examples: {'rewards_train/chosen': '0.16711', 'rewards_train/rejected': '0.092257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074858', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-149.08', 'loss/train': '0.665', 'examples_per_second': '30.492', 'grad_norm': '21.75', 'counters/examples': 117888, 'counters/updates': 3684}
train stats after 117920 examples: {'rewards_train/chosen': '0.14737', 'rewards_train/rejected': '0.07968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067693', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-116.78', 'loss/train': '0.66681', 'examples_per_second': '31.554', 'grad_norm': '20.25', 'counters/examples': 117920, 'counters/updates': 3685}
train stats after 117952 examples: {'rewards_train/chosen': '0.17487', 'rewards_train/rejected': '0.057974', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1169', 'logps_train/rejected': '-105.51', 'logps_train/chosen': '-116.67', 'loss/train': '0.64653', 'examples_per_second': '32.335', 'grad_norm': '17.5', 'counters/examples': 117952, 'counters/updates': 3686}
train stats after 117984 examples: {'rewards_train/chosen': '0.16204', 'rewards_train/rejected': '0.021872', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14017', 'logps_train/rejected': '-102.37', 'logps_train/chosen': '-113.07', 'loss/train': '0.63063', 'examples_per_second': '32.072', 'grad_norm': '17.875', 'counters/examples': 117984, 'counters/updates': 3687}
train stats after 118016 examples: {'rewards_train/chosen': '0.11288', 'rewards_train/rejected': '0.056869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056011', 'logps_train/rejected': '-114.47', 'logps_train/chosen': '-120.78', 'loss/train': '0.674', 'examples_per_second': '31.762', 'grad_norm': '20', 'counters/examples': 118016, 'counters/updates': 3688}
train stats after 118048 examples: {'rewards_train/chosen': '0.099769', 'rewards_train/rejected': '0.018877', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.080892', 'logps_train/rejected': '-98.89', 'logps_train/chosen': '-104.06', 'loss/train': '0.65978', 'examples_per_second': '31.589', 'grad_norm': '17.625', 'counters/examples': 118048, 'counters/updates': 3689}
train stats after 118080 examples: {'rewards_train/chosen': '0.23461', 'rewards_train/rejected': '0.11377', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12084', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-142.72', 'loss/train': '0.65306', 'examples_per_second': '29.994', 'grad_norm': '19.625', 'counters/examples': 118080, 'counters/updates': 3690}
train stats after 118112 examples: {'rewards_train/chosen': '0.12123', 'rewards_train/rejected': '0.079722', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041509', 'logps_train/rejected': '-134.33', 'logps_train/chosen': '-134.53', 'loss/train': '0.67655', 'examples_per_second': '30.583', 'grad_norm': '20.875', 'counters/examples': 118112, 'counters/updates': 3691}
skipping logging after 118144 examples to avoid logging too frequently
train stats after 118176 examples: {'rewards_train/chosen': '0.13585', 'rewards_train/rejected': '0.059974', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07588', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-99.816', 'loss/train': '0.66256', 'examples_per_second': '31.579', 'grad_norm': '19', 'counters/examples': 118176, 'counters/updates': 3693}
train stats after 118208 examples: {'rewards_train/chosen': '0.095678', 'rewards_train/rejected': '0.044319', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051359', 'logps_train/rejected': '-151.75', 'logps_train/chosen': '-160.7', 'loss/train': '0.67809', 'examples_per_second': '31.546', 'grad_norm': '22.125', 'counters/examples': 118208, 'counters/updates': 3694}
skipping logging after 118240 examples to avoid logging too frequently
train stats after 118272 examples: {'rewards_train/chosen': '0.15041', 'rewards_train/rejected': '0.06282', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087593', 'logps_train/rejected': '-89.463', 'logps_train/chosen': '-147.94', 'loss/train': '0.65692', 'examples_per_second': '30.039', 'grad_norm': '19.375', 'counters/examples': 118272, 'counters/updates': 3696}
skipping logging after 118304 examples to avoid logging too frequently
train stats after 118336 examples: {'rewards_train/chosen': '0.20615', 'rewards_train/rejected': '0.052427', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15373', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-126.5', 'loss/train': '0.62474', 'examples_per_second': '31.207', 'grad_norm': '19.625', 'counters/examples': 118336, 'counters/updates': 3698}
train stats after 118368 examples: {'rewards_train/chosen': '0.16238', 'rewards_train/rejected': '0.060233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10214', 'logps_train/rejected': '-106.82', 'logps_train/chosen': '-143.22', 'loss/train': '0.65284', 'examples_per_second': '32.01', 'grad_norm': '19.125', 'counters/examples': 118368, 'counters/updates': 3699}
train stats after 118400 examples: {'rewards_train/chosen': '0.14139', 'rewards_train/rejected': '0.028463', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11293', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-108.13', 'loss/train': '0.64505', 'examples_per_second': '31.685', 'grad_norm': '17.5', 'counters/examples': 118400, 'counters/updates': 3700}
skipping logging after 118432 examples to avoid logging too frequently
train stats after 118464 examples: {'rewards_train/chosen': '0.10376', 'rewards_train/rejected': '0.067', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036763', 'logps_train/rejected': '-152.08', 'logps_train/chosen': '-138.02', 'loss/train': '0.6857', 'examples_per_second': '31.627', 'grad_norm': '21.875', 'counters/examples': 118464, 'counters/updates': 3702}
train stats after 118496 examples: {'rewards_train/chosen': '0.19723', 'rewards_train/rejected': '0.079983', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11724', 'logps_train/rejected': '-108.06', 'logps_train/chosen': '-133.33', 'loss/train': '0.64436', 'examples_per_second': '31.316', 'grad_norm': '19.375', 'counters/examples': 118496, 'counters/updates': 3703}
skipping logging after 118528 examples to avoid logging too frequently
train stats after 118560 examples: {'rewards_train/chosen': '0.099091', 'rewards_train/rejected': '0.058996', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040095', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-123.1', 'loss/train': '0.68083', 'examples_per_second': '33.467', 'grad_norm': '19.375', 'counters/examples': 118560, 'counters/updates': 3705}
train stats after 118592 examples: {'rewards_train/chosen': '0.08548', 'rewards_train/rejected': '-0.037468', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12295', 'logps_train/rejected': '-100.32', 'logps_train/chosen': '-96.334', 'loss/train': '0.64152', 'examples_per_second': '31.504', 'grad_norm': '17.625', 'counters/examples': 118592, 'counters/updates': 3706}
train stats after 118624 examples: {'rewards_train/chosen': '0.19249', 'rewards_train/rejected': '0.076177', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11632', 'logps_train/rejected': '-99.639', 'logps_train/chosen': '-136.73', 'loss/train': '0.64465', 'examples_per_second': '32.019', 'grad_norm': '18.875', 'counters/examples': 118624, 'counters/updates': 3707}
skipping logging after 118656 examples to avoid logging too frequently
train stats after 118688 examples: {'rewards_train/chosen': '0.087141', 'rewards_train/rejected': '0.071932', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01521', 'logps_train/rejected': '-98.344', 'logps_train/chosen': '-118.88', 'loss/train': '0.69085', 'examples_per_second': '35.858', 'grad_norm': '19.125', 'counters/examples': 118688, 'counters/updates': 3709}
train stats after 118720 examples: {'rewards_train/chosen': '0.18287', 'rewards_train/rejected': '0.091536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091333', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-143.71', 'loss/train': '0.65718', 'examples_per_second': '33.029', 'grad_norm': '21.625', 'counters/examples': 118720, 'counters/updates': 3710}
train stats after 118752 examples: {'rewards_train/chosen': '0.14406', 'rewards_train/rejected': '0.14184', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0022161', 'logps_train/rejected': '-110.29', 'logps_train/chosen': '-157.29', 'loss/train': '0.69802', 'examples_per_second': '31.548', 'grad_norm': '21.125', 'counters/examples': 118752, 'counters/updates': 3711}
train stats after 118784 examples: {'rewards_train/chosen': '0.088112', 'rewards_train/rejected': '0.025499', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062613', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-135.68', 'loss/train': '0.66975', 'examples_per_second': '31.573', 'grad_norm': '20', 'counters/examples': 118784, 'counters/updates': 3712}
skipping logging after 118816 examples to avoid logging too frequently
train stats after 118848 examples: {'rewards_train/chosen': '0.13878', 'rewards_train/rejected': '0.033938', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10485', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-154.35', 'loss/train': '0.65229', 'examples_per_second': '30.996', 'grad_norm': '21', 'counters/examples': 118848, 'counters/updates': 3714}
train stats after 118880 examples: {'rewards_train/chosen': '0.10375', 'rewards_train/rejected': '0.027822', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075933', 'logps_train/rejected': '-112.64', 'logps_train/chosen': '-133.29', 'loss/train': '0.66741', 'examples_per_second': '31.646', 'grad_norm': '20.375', 'counters/examples': 118880, 'counters/updates': 3715}
skipping logging after 118912 examples to avoid logging too frequently
train stats after 118944 examples: {'rewards_train/chosen': '0.15708', 'rewards_train/rejected': '0.11671', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040369', 'logps_train/rejected': '-88.305', 'logps_train/chosen': '-111.68', 'loss/train': '0.68318', 'examples_per_second': '33.183', 'grad_norm': '18.125', 'counters/examples': 118944, 'counters/updates': 3717}
train stats after 118976 examples: {'rewards_train/chosen': '0.084252', 'rewards_train/rejected': '0.019152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0651', 'logps_train/rejected': '-106.09', 'logps_train/chosen': '-142.76', 'loss/train': '0.67074', 'examples_per_second': '30.204', 'grad_norm': '19.125', 'counters/examples': 118976, 'counters/updates': 3718}
train stats after 119008 examples: {'rewards_train/chosen': '0.1715', 'rewards_train/rejected': '0.062529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10897', 'logps_train/rejected': '-84.825', 'logps_train/chosen': '-138.54', 'loss/train': '0.64753', 'examples_per_second': '31.281', 'grad_norm': '18.125', 'counters/examples': 119008, 'counters/updates': 3719}
skipping logging after 119040 examples to avoid logging too frequently
train stats after 119072 examples: {'rewards_train/chosen': '0.12243', 'rewards_train/rejected': '0.0064415', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11599', 'logps_train/rejected': '-108.37', 'logps_train/chosen': '-132.22', 'loss/train': '0.64767', 'examples_per_second': '35.821', 'grad_norm': '19.125', 'counters/examples': 119072, 'counters/updates': 3721}
train stats after 119104 examples: {'rewards_train/chosen': '0.14531', 'rewards_train/rejected': '0.0028117', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1425', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-117.64', 'loss/train': '0.63458', 'examples_per_second': '26.55', 'grad_norm': '18', 'counters/examples': 119104, 'counters/updates': 3722}
train stats after 119136 examples: {'rewards_train/chosen': '0.13195', 'rewards_train/rejected': '0.044433', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.087514', 'logps_train/rejected': '-110.89', 'logps_train/chosen': '-136.55', 'loss/train': '0.65772', 'examples_per_second': '30.682', 'grad_norm': '19.375', 'counters/examples': 119136, 'counters/updates': 3723}
train stats after 119168 examples: {'rewards_train/chosen': '0.17642', 'rewards_train/rejected': '0.11106', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065362', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-127.47', 'loss/train': '0.66491', 'examples_per_second': '31.558', 'grad_norm': '18.875', 'counters/examples': 119168, 'counters/updates': 3724}
train stats after 119200 examples: {'rewards_train/chosen': '0.2295', 'rewards_train/rejected': '0.068533', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16097', 'logps_train/rejected': '-100.74', 'logps_train/chosen': '-131.28', 'loss/train': '0.62633', 'examples_per_second': '24.915', 'grad_norm': '18', 'counters/examples': 119200, 'counters/updates': 3725}
train stats after 119232 examples: {'rewards_train/chosen': '0.1628', 'rewards_train/rejected': '0.071501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091294', 'logps_train/rejected': '-136.52', 'logps_train/chosen': '-151.29', 'loss/train': '0.65915', 'examples_per_second': '30.526', 'grad_norm': '21.75', 'counters/examples': 119232, 'counters/updates': 3726}
train stats after 119264 examples: {'rewards_train/chosen': '0.17512', 'rewards_train/rejected': '0.058862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11626', 'logps_train/rejected': '-122.96', 'logps_train/chosen': '-155.65', 'loss/train': '0.64456', 'examples_per_second': '31.265', 'grad_norm': '20.625', 'counters/examples': 119264, 'counters/updates': 3727}
train stats after 119296 examples: {'rewards_train/chosen': '0.11493', 'rewards_train/rejected': '0.07035', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044585', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-123.55', 'loss/train': '0.67827', 'examples_per_second': '30.089', 'grad_norm': '21.125', 'counters/examples': 119296, 'counters/updates': 3728}
train stats after 119328 examples: {'rewards_train/chosen': '0.076915', 'rewards_train/rejected': '0.091723', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014807', 'logps_train/rejected': '-97.046', 'logps_train/chosen': '-123.8', 'loss/train': '0.7064', 'examples_per_second': '32.68', 'grad_norm': '19.375', 'counters/examples': 119328, 'counters/updates': 3729}
skipping logging after 119360 examples to avoid logging too frequently
train stats after 119392 examples: {'rewards_train/chosen': '0.15659', 'rewards_train/rejected': '0.053408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10318', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-126.78', 'loss/train': '0.65777', 'examples_per_second': '33.31', 'grad_norm': '19', 'counters/examples': 119392, 'counters/updates': 3731}
train stats after 119424 examples: {'rewards_train/chosen': '0.14745', 'rewards_train/rejected': '0.068205', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079249', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-165.05', 'loss/train': '0.67026', 'examples_per_second': '31.598', 'grad_norm': '21.375', 'counters/examples': 119424, 'counters/updates': 3732}
skipping logging after 119456 examples to avoid logging too frequently
train stats after 119488 examples: {'rewards_train/chosen': '0.17895', 'rewards_train/rejected': '0.083611', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.09534', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-146.13', 'loss/train': '0.65976', 'examples_per_second': '31.586', 'grad_norm': '20.75', 'counters/examples': 119488, 'counters/updates': 3734}
train stats after 119520 examples: {'rewards_train/chosen': '0.11286', 'rewards_train/rejected': '0.13935', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.026493', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-136.2', 'loss/train': '0.71408', 'examples_per_second': '30.743', 'grad_norm': '22', 'counters/examples': 119520, 'counters/updates': 3735}
train stats after 119552 examples: {'rewards_train/chosen': '0.089667', 'rewards_train/rejected': '0.095621', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0059537', 'logps_train/rejected': '-93.568', 'logps_train/chosen': '-141.98', 'loss/train': '0.71403', 'examples_per_second': '31.798', 'grad_norm': '20.875', 'counters/examples': 119552, 'counters/updates': 3736}
train stats after 119584 examples: {'rewards_train/chosen': '0.11674', 'rewards_train/rejected': '0.06549', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051251', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-163.91', 'loss/train': '0.67311', 'examples_per_second': '31.288', 'grad_norm': '21.5', 'counters/examples': 119584, 'counters/updates': 3737}
train stats after 119616 examples: {'rewards_train/chosen': '0.18316', 'rewards_train/rejected': '0.091737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091426', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-173.92', 'loss/train': '0.65737', 'examples_per_second': '30.931', 'grad_norm': '21.625', 'counters/examples': 119616, 'counters/updates': 3738}
train stats after 119648 examples: {'rewards_train/chosen': '0.15885', 'rewards_train/rejected': '0.0035813', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15527', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-132.53', 'loss/train': '0.62367', 'examples_per_second': '32.472', 'grad_norm': '18.25', 'counters/examples': 119648, 'counters/updates': 3739}
train stats after 119680 examples: {'rewards_train/chosen': '0.087328', 'rewards_train/rejected': '0.0076013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079726', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-140.98', 'loss/train': '0.66294', 'examples_per_second': '31.828', 'grad_norm': '20.375', 'counters/examples': 119680, 'counters/updates': 3740}
train stats after 119712 examples: {'rewards_train/chosen': '0.13488', 'rewards_train/rejected': '0.011189', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12369', 'logps_train/rejected': '-99.528', 'logps_train/chosen': '-162.59', 'loss/train': '0.64013', 'examples_per_second': '30.572', 'grad_norm': '20.625', 'counters/examples': 119712, 'counters/updates': 3741}
train stats after 119744 examples: {'rewards_train/chosen': '0.13955', 'rewards_train/rejected': '-0.031033', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17058', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-149.48', 'loss/train': '0.62035', 'examples_per_second': '30.382', 'grad_norm': '19.625', 'counters/examples': 119744, 'counters/updates': 3742}
train stats after 119776 examples: {'rewards_train/chosen': '0.12489', 'rewards_train/rejected': '0.02171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10318', 'logps_train/rejected': '-107.73', 'logps_train/chosen': '-134.82', 'loss/train': '0.65031', 'examples_per_second': '32.096', 'grad_norm': '19', 'counters/examples': 119776, 'counters/updates': 3743}
train stats after 119808 examples: {'rewards_train/chosen': '0.12176', 'rewards_train/rejected': '0.083246', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038511', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-142.94', 'loss/train': '0.68131', 'examples_per_second': '31.539', 'grad_norm': '20.875', 'counters/examples': 119808, 'counters/updates': 3744}
train stats after 119840 examples: {'rewards_train/chosen': '0.15605', 'rewards_train/rejected': '0.11151', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044535', 'logps_train/rejected': '-142.8', 'logps_train/chosen': '-146.75', 'loss/train': '0.68036', 'examples_per_second': '30.219', 'grad_norm': '21.375', 'counters/examples': 119840, 'counters/updates': 3745}
train stats after 119872 examples: {'rewards_train/chosen': '0.11458', 'rewards_train/rejected': '0.041247', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073332', 'logps_train/rejected': '-113.6', 'logps_train/chosen': '-136.29', 'loss/train': '0.67018', 'examples_per_second': '31.591', 'grad_norm': '18.875', 'counters/examples': 119872, 'counters/updates': 3746}
train stats after 119904 examples: {'rewards_train/chosen': '0.16206', 'rewards_train/rejected': '0.020596', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14146', 'logps_train/rejected': '-140.5', 'logps_train/chosen': '-146.49', 'loss/train': '0.63374', 'examples_per_second': '29.941', 'grad_norm': '20.125', 'counters/examples': 119904, 'counters/updates': 3747}
train stats after 119936 examples: {'rewards_train/chosen': '0.17695', 'rewards_train/rejected': '0.041608', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13535', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-137.48', 'loss/train': '0.6408', 'examples_per_second': '32.098', 'grad_norm': '20.375', 'counters/examples': 119936, 'counters/updates': 3748}
skipping logging after 119968 examples to avoid logging too frequently
train stats after 120000 examples: {'rewards_train/chosen': '0.096779', 'rewards_train/rejected': '0.091608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0051706', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-114.41', 'loss/train': '0.69815', 'examples_per_second': '31.201', 'grad_norm': '20.875', 'counters/examples': 120000, 'counters/updates': 3750}
Running evaluation after 120000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.88it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.88it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.89it/s]
eval after 120000: {'rewards_eval/chosen': '0.13755', 'rewards_eval/rejected': '0.061715', 'rewards_eval/accuracies': '0.54688', 'rewards_eval/margins': '0.075834', 'logps_eval/rejected': '-114.78', 'logps_eval/chosen': '-134.17', 'loss/eval': '0.6674'}
skipping save for non epoch
train stats after 120032 examples: {'rewards_train/chosen': '0.1055', 'rewards_train/rejected': '0.040173', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06533', 'logps_train/rejected': '-87.308', 'logps_train/chosen': '-142.19', 'loss/train': '0.67107', 'examples_per_second': '33.841', 'grad_norm': '20', 'counters/examples': 120032, 'counters/updates': 3751}
skipping logging after 120064 examples to avoid logging too frequently
train stats after 120096 examples: {'rewards_train/chosen': '0.14257', 'rewards_train/rejected': '0.048716', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093851', 'logps_train/rejected': '-90.61', 'logps_train/chosen': '-133.19', 'loss/train': '0.65971', 'examples_per_second': '32.85', 'grad_norm': '18.5', 'counters/examples': 120096, 'counters/updates': 3753}
train stats after 120128 examples: {'rewards_train/chosen': '0.13878', 'rewards_train/rejected': '0.087638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051139', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-146.91', 'loss/train': '0.68008', 'examples_per_second': '31.586', 'grad_norm': '21.25', 'counters/examples': 120128, 'counters/updates': 3754}
skipping logging after 120160 examples to avoid logging too frequently
train stats after 120192 examples: {'rewards_train/chosen': '0.14904', 'rewards_train/rejected': '0.056696', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092339', 'logps_train/rejected': '-114.22', 'logps_train/chosen': '-122.59', 'loss/train': '0.65498', 'examples_per_second': '32.642', 'grad_norm': '18.75', 'counters/examples': 120192, 'counters/updates': 3756}
train stats after 120224 examples: {'rewards_train/chosen': '0.14268', 'rewards_train/rejected': '0.033768', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10891', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-172.28', 'loss/train': '0.65014', 'examples_per_second': '33.062', 'grad_norm': '23.375', 'counters/examples': 120224, 'counters/updates': 3757}
train stats after 120256 examples: {'rewards_train/chosen': '0.18281', 'rewards_train/rejected': '0.050822', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13198', 'logps_train/rejected': '-106.76', 'logps_train/chosen': '-169.9', 'loss/train': '0.64146', 'examples_per_second': '30.039', 'grad_norm': '21.5', 'counters/examples': 120256, 'counters/updates': 3758}
train stats after 120288 examples: {'rewards_train/chosen': '0.20165', 'rewards_train/rejected': '0.0019805', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19967', 'logps_train/rejected': '-104.91', 'logps_train/chosen': '-125.39', 'loss/train': '0.62045', 'examples_per_second': '30.519', 'grad_norm': '18.375', 'counters/examples': 120288, 'counters/updates': 3759}
train stats after 120320 examples: {'rewards_train/chosen': '0.10638', 'rewards_train/rejected': '0.051176', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055209', 'logps_train/rejected': '-116.93', 'logps_train/chosen': '-139.13', 'loss/train': '0.67542', 'examples_per_second': '31.148', 'grad_norm': '21.75', 'counters/examples': 120320, 'counters/updates': 3760}
train stats after 120352 examples: {'rewards_train/chosen': '0.17191', 'rewards_train/rejected': '0.073215', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098695', 'logps_train/rejected': '-128.18', 'logps_train/chosen': '-141.29', 'loss/train': '0.65189', 'examples_per_second': '30.539', 'grad_norm': '20.75', 'counters/examples': 120352, 'counters/updates': 3761}
train stats after 120384 examples: {'rewards_train/chosen': '0.1696', 'rewards_train/rejected': '-0.025315', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19491', 'logps_train/rejected': '-161.62', 'logps_train/chosen': '-187.76', 'loss/train': '0.61549', 'examples_per_second': '31.512', 'grad_norm': '22.875', 'counters/examples': 120384, 'counters/updates': 3762}
train stats after 120416 examples: {'rewards_train/chosen': '0.11178', 'rewards_train/rejected': '0.019433', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092351', 'logps_train/rejected': '-125.67', 'logps_train/chosen': '-129.67', 'loss/train': '0.66199', 'examples_per_second': '32.281', 'grad_norm': '20.375', 'counters/examples': 120416, 'counters/updates': 3763}
train stats after 120448 examples: {'rewards_train/chosen': '0.10561', 'rewards_train/rejected': '-0.0099987', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11561', 'logps_train/rejected': '-94.114', 'logps_train/chosen': '-129.15', 'loss/train': '0.64533', 'examples_per_second': '30.468', 'grad_norm': '18.25', 'counters/examples': 120448, 'counters/updates': 3764}
skipping logging after 120480 examples to avoid logging too frequently
train stats after 120512 examples: {'rewards_train/chosen': '0.11273', 'rewards_train/rejected': '0.014471', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098264', 'logps_train/rejected': '-105.23', 'logps_train/chosen': '-133.41', 'loss/train': '0.65372', 'examples_per_second': '30.165', 'grad_norm': '19.375', 'counters/examples': 120512, 'counters/updates': 3766}
train stats after 120544 examples: {'rewards_train/chosen': '0.16572', 'rewards_train/rejected': '0.0082468', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15747', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-157.59', 'loss/train': '0.62857', 'examples_per_second': '31.179', 'grad_norm': '19.5', 'counters/examples': 120544, 'counters/updates': 3767}
skipping logging after 120576 examples to avoid logging too frequently
train stats after 120608 examples: {'rewards_train/chosen': '0.15751', 'rewards_train/rejected': '0.024893', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13262', 'logps_train/rejected': '-177.61', 'logps_train/chosen': '-132.74', 'loss/train': '0.63562', 'examples_per_second': '31.627', 'grad_norm': '21.625', 'counters/examples': 120608, 'counters/updates': 3769}
train stats after 120640 examples: {'rewards_train/chosen': '0.18095', 'rewards_train/rejected': '0.053079', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12787', 'logps_train/rejected': '-99.367', 'logps_train/chosen': '-134.53', 'loss/train': '0.63821', 'examples_per_second': '31.048', 'grad_norm': '18.5', 'counters/examples': 120640, 'counters/updates': 3770}
train stats after 120672 examples: {'rewards_train/chosen': '0.12522', 'rewards_train/rejected': '0.16597', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.040749', 'logps_train/rejected': '-151.25', 'logps_train/chosen': '-145.34', 'loss/train': '0.7196', 'examples_per_second': '32.508', 'grad_norm': '22.625', 'counters/examples': 120672, 'counters/updates': 3771}
train stats after 120704 examples: {'rewards_train/chosen': '0.20059', 'rewards_train/rejected': '0.069135', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13146', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-147.69', 'loss/train': '0.63842', 'examples_per_second': '32.738', 'grad_norm': '19.75', 'counters/examples': 120704, 'counters/updates': 3772}
train stats after 120736 examples: {'rewards_train/chosen': '0.098702', 'rewards_train/rejected': '0.023412', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07529', 'logps_train/rejected': '-148.76', 'logps_train/chosen': '-171.15', 'loss/train': '0.66852', 'examples_per_second': '31.113', 'grad_norm': '24.625', 'counters/examples': 120736, 'counters/updates': 3773}
train stats after 120768 examples: {'rewards_train/chosen': '0.17287', 'rewards_train/rejected': '0.075308', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09756', 'logps_train/rejected': '-136.13', 'logps_train/chosen': '-142.92', 'loss/train': '0.65806', 'examples_per_second': '30.272', 'grad_norm': '21', 'counters/examples': 120768, 'counters/updates': 3774}
train stats after 120800 examples: {'rewards_train/chosen': '0.16258', 'rewards_train/rejected': '0.039997', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12258', 'logps_train/rejected': '-142.35', 'logps_train/chosen': '-168.64', 'loss/train': '0.64172', 'examples_per_second': '31.513', 'grad_norm': '22.125', 'counters/examples': 120800, 'counters/updates': 3775}
skipping logging after 120832 examples to avoid logging too frequently
train stats after 120864 examples: {'rewards_train/chosen': '0.17708', 'rewards_train/rejected': '0.036114', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14097', 'logps_train/rejected': '-111.21', 'logps_train/chosen': '-140.78', 'loss/train': '0.63471', 'examples_per_second': '31.965', 'grad_norm': '19.375', 'counters/examples': 120864, 'counters/updates': 3777}
train stats after 120896 examples: {'rewards_train/chosen': '0.084677', 'rewards_train/rejected': '-0.018558', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10323', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-152.86', 'loss/train': '0.65047', 'examples_per_second': '32.203', 'grad_norm': '21.375', 'counters/examples': 120896, 'counters/updates': 3778}
skipping logging after 120928 examples to avoid logging too frequently
train stats after 120960 examples: {'rewards_train/chosen': '0.19487', 'rewards_train/rejected': '0.01946', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17541', 'logps_train/rejected': '-118.8', 'logps_train/chosen': '-147.65', 'loss/train': '0.61787', 'examples_per_second': '30.419', 'grad_norm': '19.625', 'counters/examples': 120960, 'counters/updates': 3780}
train stats after 120992 examples: {'rewards_train/chosen': '0.21662', 'rewards_train/rejected': '0.041079', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17554', 'logps_train/rejected': '-137.26', 'logps_train/chosen': '-164.16', 'loss/train': '0.62191', 'examples_per_second': '31.307', 'grad_norm': '21.875', 'counters/examples': 120992, 'counters/updates': 3781}
train stats after 121024 examples: {'rewards_train/chosen': '0.29555', 'rewards_train/rejected': '0.19389', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10166', 'logps_train/rejected': '-143.96', 'logps_train/chosen': '-144.77', 'loss/train': '0.6582', 'examples_per_second': '33.173', 'grad_norm': '20.5', 'counters/examples': 121024, 'counters/updates': 3782}
train stats after 121056 examples: {'rewards_train/chosen': '0.12284', 'rewards_train/rejected': '0.074851', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.047986', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-143.32', 'loss/train': '0.67488', 'examples_per_second': '32.97', 'grad_norm': '20', 'counters/examples': 121056, 'counters/updates': 3783}
train stats after 121088 examples: {'rewards_train/chosen': '0.20735', 'rewards_train/rejected': '0.043792', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16356', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-133.77', 'loss/train': '0.63004', 'examples_per_second': '31.947', 'grad_norm': '19.375', 'counters/examples': 121088, 'counters/updates': 3784}
train stats after 121120 examples: {'rewards_train/chosen': '0.18146', 'rewards_train/rejected': '-0.0082271', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18969', 'logps_train/rejected': '-105.34', 'logps_train/chosen': '-144.05', 'loss/train': '0.61694', 'examples_per_second': '30.472', 'grad_norm': '17', 'counters/examples': 121120, 'counters/updates': 3785}
train stats after 121152 examples: {'rewards_train/chosen': '0.18949', 'rewards_train/rejected': '0.052996', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13649', 'logps_train/rejected': '-94.785', 'logps_train/chosen': '-134.37', 'loss/train': '0.64298', 'examples_per_second': '31.522', 'grad_norm': '19.5', 'counters/examples': 121152, 'counters/updates': 3786}
skipping logging after 121184 examples to avoid logging too frequently
train stats after 121216 examples: {'rewards_train/chosen': '0.14294', 'rewards_train/rejected': '0.059439', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.083497', 'logps_train/rejected': '-148.31', 'logps_train/chosen': '-125.85', 'loss/train': '0.65976', 'examples_per_second': '31.375', 'grad_norm': '20.125', 'counters/examples': 121216, 'counters/updates': 3788}
train stats after 121248 examples: {'rewards_train/chosen': '0.13819', 'rewards_train/rejected': '-0.0056321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14382', 'logps_train/rejected': '-99.774', 'logps_train/chosen': '-151.24', 'loss/train': '0.63036', 'examples_per_second': '31.423', 'grad_norm': '19.375', 'counters/examples': 121248, 'counters/updates': 3789}
skipping logging after 121280 examples to avoid logging too frequently
train stats after 121312 examples: {'rewards_train/chosen': '0.068707', 'rewards_train/rejected': '0.097604', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028897', 'logps_train/rejected': '-138.99', 'logps_train/chosen': '-131.06', 'loss/train': '0.71804', 'examples_per_second': '31.458', 'grad_norm': '22', 'counters/examples': 121312, 'counters/updates': 3791}
train stats after 121344 examples: {'rewards_train/chosen': '0.14684', 'rewards_train/rejected': '0.033052', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11379', 'logps_train/rejected': '-109.99', 'logps_train/chosen': '-151.73', 'loss/train': '0.64561', 'examples_per_second': '31.487', 'grad_norm': '20.375', 'counters/examples': 121344, 'counters/updates': 3792}
train stats after 121376 examples: {'rewards_train/chosen': '0.12127', 'rewards_train/rejected': '0.072154', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049118', 'logps_train/rejected': '-154.21', 'logps_train/chosen': '-146.19', 'loss/train': '0.68008', 'examples_per_second': '24.43', 'grad_norm': '21.875', 'counters/examples': 121376, 'counters/updates': 3793}
train stats after 121408 examples: {'rewards_train/chosen': '0.1546', 'rewards_train/rejected': '0.015664', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13894', 'logps_train/rejected': '-117.01', 'logps_train/chosen': '-152.64', 'loss/train': '0.64587', 'examples_per_second': '33.061', 'grad_norm': '20.625', 'counters/examples': 121408, 'counters/updates': 3794}
train stats after 121440 examples: {'rewards_train/chosen': '0.10047', 'rewards_train/rejected': '0.09743', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0030443', 'logps_train/rejected': '-97.915', 'logps_train/chosen': '-123.46', 'loss/train': '0.69814', 'examples_per_second': '30.45', 'grad_norm': '18.875', 'counters/examples': 121440, 'counters/updates': 3795}
skipping logging after 121472 examples to avoid logging too frequently
train stats after 121504 examples: {'rewards_train/chosen': '0.10439', 'rewards_train/rejected': '0.055573', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.04882', 'logps_train/rejected': '-112.77', 'logps_train/chosen': '-131.31', 'loss/train': '0.68132', 'examples_per_second': '34.316', 'grad_norm': '19.125', 'counters/examples': 121504, 'counters/updates': 3797}
train stats after 121536 examples: {'rewards_train/chosen': '0.11956', 'rewards_train/rejected': '0.1071', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012456', 'logps_train/rejected': '-111.05', 'logps_train/chosen': '-153.37', 'loss/train': '0.69547', 'examples_per_second': '30.001', 'grad_norm': '20.5', 'counters/examples': 121536, 'counters/updates': 3798}
skipping logging after 121568 examples to avoid logging too frequently
train stats after 121600 examples: {'rewards_train/chosen': '0.13027', 'rewards_train/rejected': '0.037053', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093216', 'logps_train/rejected': '-124.29', 'logps_train/chosen': '-120.5', 'loss/train': '0.65597', 'examples_per_second': '34.202', 'grad_norm': '19.25', 'counters/examples': 121600, 'counters/updates': 3800}
train stats after 121632 examples: {'rewards_train/chosen': '0.10888', 'rewards_train/rejected': '-0.0053258', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11421', 'logps_train/rejected': '-113.6', 'logps_train/chosen': '-116.1', 'loss/train': '0.64614', 'examples_per_second': '32.744', 'grad_norm': '18.875', 'counters/examples': 121632, 'counters/updates': 3801}
train stats after 121664 examples: {'rewards_train/chosen': '0.11156', 'rewards_train/rejected': '0.0065512', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10501', 'logps_train/rejected': '-118.49', 'logps_train/chosen': '-125.03', 'loss/train': '0.64879', 'examples_per_second': '31.135', 'grad_norm': '20.875', 'counters/examples': 121664, 'counters/updates': 3802}
train stats after 121696 examples: {'rewards_train/chosen': '0.045404', 'rewards_train/rejected': '-0.00031094', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045715', 'logps_train/rejected': '-140.77', 'logps_train/chosen': '-131.93', 'loss/train': '0.67947', 'examples_per_second': '29.285', 'grad_norm': '20.5', 'counters/examples': 121696, 'counters/updates': 3803}
train stats after 121728 examples: {'rewards_train/chosen': '0.22404', 'rewards_train/rejected': '0.11208', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11196', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-163.57', 'loss/train': '0.64925', 'examples_per_second': '31.459', 'grad_norm': '22.125', 'counters/examples': 121728, 'counters/updates': 3804}
train stats after 121760 examples: {'rewards_train/chosen': '0.11393', 'rewards_train/rejected': '0.012296', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10163', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-141.74', 'loss/train': '0.65169', 'examples_per_second': '31.019', 'grad_norm': '18.75', 'counters/examples': 121760, 'counters/updates': 3805}
train stats after 121792 examples: {'rewards_train/chosen': '0.16336', 'rewards_train/rejected': '0.032756', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1306', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-154.28', 'loss/train': '0.64445', 'examples_per_second': '31.782', 'grad_norm': '22.125', 'counters/examples': 121792, 'counters/updates': 3806}
train stats after 121824 examples: {'rewards_train/chosen': '0.17814', 'rewards_train/rejected': '0.083757', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094384', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-164.12', 'loss/train': '0.65691', 'examples_per_second': '29.879', 'grad_norm': '21.625', 'counters/examples': 121824, 'counters/updates': 3807}
train stats after 121856 examples: {'rewards_train/chosen': '0.12802', 'rewards_train/rejected': '0.036487', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091535', 'logps_train/rejected': '-92.919', 'logps_train/chosen': '-121.88', 'loss/train': '0.6581', 'examples_per_second': '31.999', 'grad_norm': '18.125', 'counters/examples': 121856, 'counters/updates': 3808}
skipping logging after 121888 examples to avoid logging too frequently
train stats after 121920 examples: {'rewards_train/chosen': '0.12584', 'rewards_train/rejected': '0.038525', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087318', 'logps_train/rejected': '-97.648', 'logps_train/chosen': '-116.91', 'loss/train': '0.65703', 'examples_per_second': '36.03', 'grad_norm': '18.75', 'counters/examples': 121920, 'counters/updates': 3810}
train stats after 121952 examples: {'rewards_train/chosen': '0.12357', 'rewards_train/rejected': '0.029536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094034', 'logps_train/rejected': '-87.497', 'logps_train/chosen': '-126.52', 'loss/train': '0.66151', 'examples_per_second': '32.495', 'grad_norm': '19.125', 'counters/examples': 121952, 'counters/updates': 3811}
train stats after 121984 examples: {'rewards_train/chosen': '0.15105', 'rewards_train/rejected': '0.06142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089627', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-117.67', 'loss/train': '0.6551', 'examples_per_second': '30.01', 'grad_norm': '19.625', 'counters/examples': 121984, 'counters/updates': 3812}
train stats after 122016 examples: {'rewards_train/chosen': '0.1593', 'rewards_train/rejected': '0.074129', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085169', 'logps_train/rejected': '-120.47', 'logps_train/chosen': '-123.86', 'loss/train': '0.6649', 'examples_per_second': '31.531', 'grad_norm': '20.125', 'counters/examples': 122016, 'counters/updates': 3813}
train stats after 122048 examples: {'rewards_train/chosen': '0.22025', 'rewards_train/rejected': '0.025107', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19515', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-148.97', 'loss/train': '0.61782', 'examples_per_second': '31.132', 'grad_norm': '19.5', 'counters/examples': 122048, 'counters/updates': 3814}
skipping logging after 122080 examples to avoid logging too frequently
train stats after 122112 examples: {'rewards_train/chosen': '0.10953', 'rewards_train/rejected': '-0.02749', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13702', 'logps_train/rejected': '-114.54', 'logps_train/chosen': '-118.79', 'loss/train': '0.64043', 'examples_per_second': '31.401', 'grad_norm': '17.875', 'counters/examples': 122112, 'counters/updates': 3816}
train stats after 122144 examples: {'rewards_train/chosen': '0.22395', 'rewards_train/rejected': '0.011794', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21216', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-166.86', 'loss/train': '0.60722', 'examples_per_second': '33.111', 'grad_norm': '20.25', 'counters/examples': 122144, 'counters/updates': 3817}
skipping logging after 122176 examples to avoid logging too frequently
train stats after 122208 examples: {'rewards_train/chosen': '0.14753', 'rewards_train/rejected': '0.0093505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13818', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-126.16', 'loss/train': '0.63523', 'examples_per_second': '30.867', 'grad_norm': '19.125', 'counters/examples': 122208, 'counters/updates': 3819}
skipping logging after 122240 examples to avoid logging too frequently
train stats after 122272 examples: {'rewards_train/chosen': '0.19302', 'rewards_train/rejected': '0.11296', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080065', 'logps_train/rejected': '-122.79', 'logps_train/chosen': '-155.77', 'loss/train': '0.66484', 'examples_per_second': '35.696', 'grad_norm': '20.875', 'counters/examples': 122272, 'counters/updates': 3821}
train stats after 122304 examples: {'rewards_train/chosen': '0.11031', 'rewards_train/rejected': '0.001621', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10869', 'logps_train/rejected': '-121.59', 'logps_train/chosen': '-98.778', 'loss/train': '0.65029', 'examples_per_second': '33.123', 'grad_norm': '18.25', 'counters/examples': 122304, 'counters/updates': 3822}
train stats after 122336 examples: {'rewards_train/chosen': '0.087083', 'rewards_train/rejected': '-0.010635', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097718', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-105.74', 'loss/train': '0.65198', 'examples_per_second': '31.238', 'grad_norm': '18.75', 'counters/examples': 122336, 'counters/updates': 3823}
train stats after 122368 examples: {'rewards_train/chosen': '0.051176', 'rewards_train/rejected': '0.049234', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0019421', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-117.28', 'loss/train': '0.70139', 'examples_per_second': '32.039', 'grad_norm': '21.25', 'counters/examples': 122368, 'counters/updates': 3824}
skipping logging after 122400 examples to avoid logging too frequently
train stats after 122432 examples: {'rewards_train/chosen': '0.049569', 'rewards_train/rejected': '0.016824', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032745', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-135.26', 'loss/train': '0.68677', 'examples_per_second': '31.544', 'grad_norm': '19.75', 'counters/examples': 122432, 'counters/updates': 3826}
train stats after 122464 examples: {'rewards_train/chosen': '0.13108', 'rewards_train/rejected': '0.14151', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01043', 'logps_train/rejected': '-111.89', 'logps_train/chosen': '-107.7', 'loss/train': '0.70886', 'examples_per_second': '32.015', 'grad_norm': '19.875', 'counters/examples': 122464, 'counters/updates': 3827}
train stats after 122496 examples: {'rewards_train/chosen': '0.23144', 'rewards_train/rejected': '0.047036', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18441', 'logps_train/rejected': '-99.656', 'logps_train/chosen': '-140.97', 'loss/train': '0.61696', 'examples_per_second': '31.537', 'grad_norm': '18.25', 'counters/examples': 122496, 'counters/updates': 3828}
train stats after 122528 examples: {'rewards_train/chosen': '0.14677', 'rewards_train/rejected': '0.0087679', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13801', 'logps_train/rejected': '-95.106', 'logps_train/chosen': '-119.58', 'loss/train': '0.63494', 'examples_per_second': '30.401', 'grad_norm': '17.375', 'counters/examples': 122528, 'counters/updates': 3829}
train stats after 122560 examples: {'rewards_train/chosen': '0.12624', 'rewards_train/rejected': '0.068856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057382', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-134.05', 'loss/train': '0.67153', 'examples_per_second': '31.182', 'grad_norm': '20', 'counters/examples': 122560, 'counters/updates': 3830}
train stats after 122592 examples: {'rewards_train/chosen': '0.17536', 'rewards_train/rejected': '0.092794', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082565', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-139.45', 'loss/train': '0.66285', 'examples_per_second': '30.36', 'grad_norm': '19.125', 'counters/examples': 122592, 'counters/updates': 3831}
train stats after 122624 examples: {'rewards_train/chosen': '0.11609', 'rewards_train/rejected': '0.033583', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082502', 'logps_train/rejected': '-149.2', 'logps_train/chosen': '-151.4', 'loss/train': '0.66545', 'examples_per_second': '31.396', 'grad_norm': '23', 'counters/examples': 122624, 'counters/updates': 3832}
skipping logging after 122656 examples to avoid logging too frequently
train stats after 122688 examples: {'rewards_train/chosen': '0.12459', 'rewards_train/rejected': '0.11496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0096291', 'logps_train/rejected': '-114.32', 'logps_train/chosen': '-147.64', 'loss/train': '0.6935', 'examples_per_second': '30.123', 'grad_norm': '20.875', 'counters/examples': 122688, 'counters/updates': 3834}
skipping logging after 122720 examples to avoid logging too frequently
train stats after 122752 examples: {'rewards_train/chosen': '0.1084', 'rewards_train/rejected': '-0.037265', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14566', 'logps_train/rejected': '-93.795', 'logps_train/chosen': '-104.53', 'loss/train': '0.63145', 'examples_per_second': '31.763', 'grad_norm': '16.625', 'counters/examples': 122752, 'counters/updates': 3836}
train stats after 122784 examples: {'rewards_train/chosen': '0.099241', 'rewards_train/rejected': '-0.015407', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11465', 'logps_train/rejected': '-84.146', 'logps_train/chosen': '-94.031', 'loss/train': '0.65013', 'examples_per_second': '31.068', 'grad_norm': '16.375', 'counters/examples': 122784, 'counters/updates': 3837}
skipping logging after 122816 examples to avoid logging too frequently
train stats after 122848 examples: {'rewards_train/chosen': '0.18209', 'rewards_train/rejected': '0.095237', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086854', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-122.01', 'loss/train': '0.66162', 'examples_per_second': '32.101', 'grad_norm': '19.25', 'counters/examples': 122848, 'counters/updates': 3839}
train stats after 122880 examples: {'rewards_train/chosen': '0.19095', 'rewards_train/rejected': '0.050629', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14032', 'logps_train/rejected': '-183.9', 'logps_train/chosen': '-193.02', 'loss/train': '0.63875', 'examples_per_second': '31.391', 'grad_norm': '24.75', 'counters/examples': 122880, 'counters/updates': 3840}
train stats after 122912 examples: {'rewards_train/chosen': '0.11886', 'rewards_train/rejected': '0.057635', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061225', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-123.12', 'loss/train': '0.67058', 'examples_per_second': '30.543', 'grad_norm': '19.875', 'counters/examples': 122912, 'counters/updates': 3841}
skipping logging after 122944 examples to avoid logging too frequently
train stats after 122976 examples: {'rewards_train/chosen': '0.25448', 'rewards_train/rejected': '0.063141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19134', 'logps_train/rejected': '-115.67', 'logps_train/chosen': '-147.29', 'loss/train': '0.61435', 'examples_per_second': '34.262', 'grad_norm': '19.25', 'counters/examples': 122976, 'counters/updates': 3843}
skipping logging after 123008 examples to avoid logging too frequently
train stats after 123040 examples: {'rewards_train/chosen': '0.13374', 'rewards_train/rejected': '-0.011356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1451', 'logps_train/rejected': '-104.07', 'logps_train/chosen': '-124.77', 'loss/train': '0.63351', 'examples_per_second': '31.131', 'grad_norm': '17.625', 'counters/examples': 123040, 'counters/updates': 3845}
train stats after 123072 examples: {'rewards_train/chosen': '0.13142', 'rewards_train/rejected': '0.048051', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083367', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-150.27', 'loss/train': '0.6614', 'examples_per_second': '31.953', 'grad_norm': '20', 'counters/examples': 123072, 'counters/updates': 3846}
train stats after 123104 examples: {'rewards_train/chosen': '0.20482', 'rewards_train/rejected': '0.22369', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01887', 'logps_train/rejected': '-133.84', 'logps_train/chosen': '-125.87', 'loss/train': '0.71406', 'examples_per_second': '30.398', 'grad_norm': '20.25', 'counters/examples': 123104, 'counters/updates': 3847}
skipping logging after 123136 examples to avoid logging too frequently
train stats after 123168 examples: {'rewards_train/chosen': '0.13291', 'rewards_train/rejected': '0.003286', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12963', 'logps_train/rejected': '-104.71', 'logps_train/chosen': '-133.02', 'loss/train': '0.64396', 'examples_per_second': '31.44', 'grad_norm': '20.25', 'counters/examples': 123168, 'counters/updates': 3849}
train stats after 123200 examples: {'rewards_train/chosen': '0.14977', 'rewards_train/rejected': '-0.0054351', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15521', 'logps_train/rejected': '-118.28', 'logps_train/chosen': '-147.08', 'loss/train': '0.62476', 'examples_per_second': '31.384', 'grad_norm': '18.75', 'counters/examples': 123200, 'counters/updates': 3850}
train stats after 123232 examples: {'rewards_train/chosen': '0.1238', 'rewards_train/rejected': '0.054686', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069117', 'logps_train/rejected': '-112.21', 'logps_train/chosen': '-136.22', 'loss/train': '0.67422', 'examples_per_second': '31.43', 'grad_norm': '20.75', 'counters/examples': 123232, 'counters/updates': 3851}
train stats after 123264 examples: {'rewards_train/chosen': '0.12814', 'rewards_train/rejected': '-0.041807', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.16995', 'logps_train/rejected': '-132.31', 'logps_train/chosen': '-155.28', 'loss/train': '0.62119', 'examples_per_second': '30.318', 'grad_norm': '20', 'counters/examples': 123264, 'counters/updates': 3852}
train stats after 123296 examples: {'rewards_train/chosen': '0.14922', 'rewards_train/rejected': '0.1016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047618', 'logps_train/rejected': '-124.89', 'logps_train/chosen': '-117.29', 'loss/train': '0.67532', 'examples_per_second': '32.298', 'grad_norm': '19.625', 'counters/examples': 123296, 'counters/updates': 3853}
train stats after 123328 examples: {'rewards_train/chosen': '0.13208', 'rewards_train/rejected': '0.020122', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11196', 'logps_train/rejected': '-133.8', 'logps_train/chosen': '-135.25', 'loss/train': '0.64941', 'examples_per_second': '31.35', 'grad_norm': '22.125', 'counters/examples': 123328, 'counters/updates': 3854}
train stats after 123360 examples: {'rewards_train/chosen': '0.10606', 'rewards_train/rejected': '0.035932', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070132', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-174.07', 'loss/train': '0.66761', 'examples_per_second': '31.154', 'grad_norm': '21.625', 'counters/examples': 123360, 'counters/updates': 3855}
train stats after 123392 examples: {'rewards_train/chosen': '0.1326', 'rewards_train/rejected': '0.055463', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077138', 'logps_train/rejected': '-119.52', 'logps_train/chosen': '-168.91', 'loss/train': '0.66828', 'examples_per_second': '31.261', 'grad_norm': '22.125', 'counters/examples': 123392, 'counters/updates': 3856}
skipping logging after 123424 examples to avoid logging too frequently
train stats after 123456 examples: {'rewards_train/chosen': '0.12525', 'rewards_train/rejected': '0.076867', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048378', 'logps_train/rejected': '-161.06', 'logps_train/chosen': '-169.39', 'loss/train': '0.67582', 'examples_per_second': '31.342', 'grad_norm': '23.25', 'counters/examples': 123456, 'counters/updates': 3858}
train stats after 123488 examples: {'rewards_train/chosen': '0.1227', 'rewards_train/rejected': '0.039541', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083155', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-187.34', 'loss/train': '0.6588', 'examples_per_second': '30.442', 'grad_norm': '21.5', 'counters/examples': 123488, 'counters/updates': 3859}
train stats after 123520 examples: {'rewards_train/chosen': '0.098631', 'rewards_train/rejected': '0.057948', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040683', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-133.38', 'loss/train': '0.68329', 'examples_per_second': '31.493', 'grad_norm': '20.5', 'counters/examples': 123520, 'counters/updates': 3860}
train stats after 123552 examples: {'rewards_train/chosen': '0.17572', 'rewards_train/rejected': '0.064261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11146', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-125.5', 'loss/train': '0.64887', 'examples_per_second': '30.65', 'grad_norm': '20.375', 'counters/examples': 123552, 'counters/updates': 3861}
train stats after 123584 examples: {'rewards_train/chosen': '0.20622', 'rewards_train/rejected': '0.090098', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11612', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-167.03', 'loss/train': '0.64469', 'examples_per_second': '31.409', 'grad_norm': '22.125', 'counters/examples': 123584, 'counters/updates': 3862}
train stats after 123616 examples: {'rewards_train/chosen': '0.10891', 'rewards_train/rejected': '0.054385', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054525', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-155.59', 'loss/train': '0.67277', 'examples_per_second': '31.473', 'grad_norm': '21.375', 'counters/examples': 123616, 'counters/updates': 3863}
train stats after 123648 examples: {'rewards_train/chosen': '0.10819', 'rewards_train/rejected': '0.056988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0512', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-122.17', 'loss/train': '0.67361', 'examples_per_second': '31.447', 'grad_norm': '19.875', 'counters/examples': 123648, 'counters/updates': 3864}
train stats after 123680 examples: {'rewards_train/chosen': '0.098851', 'rewards_train/rejected': '0.050538', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048312', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-121.85', 'loss/train': '0.67743', 'examples_per_second': '30.545', 'grad_norm': '20.875', 'counters/examples': 123680, 'counters/updates': 3865}
train stats after 123712 examples: {'rewards_train/chosen': '0.16542', 'rewards_train/rejected': '0.055756', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10967', 'logps_train/rejected': '-122.68', 'logps_train/chosen': '-137.3', 'loss/train': '0.65085', 'examples_per_second': '31.41', 'grad_norm': '19.875', 'counters/examples': 123712, 'counters/updates': 3866}
train stats after 123744 examples: {'rewards_train/chosen': '0.10706', 'rewards_train/rejected': '0.045544', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.061517', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-131.43', 'loss/train': '0.66601', 'examples_per_second': '30.154', 'grad_norm': '19.25', 'counters/examples': 123744, 'counters/updates': 3867}
train stats after 123776 examples: {'rewards_train/chosen': '0.18025', 'rewards_train/rejected': '0.075083', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-101.14', 'logps_train/chosen': '-129.25', 'loss/train': '0.6486', 'examples_per_second': '31.007', 'grad_norm': '20', 'counters/examples': 123776, 'counters/updates': 3868}
train stats after 123808 examples: {'rewards_train/chosen': '0.13516', 'rewards_train/rejected': '0.032676', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10248', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-156', 'loss/train': '0.65155', 'examples_per_second': '32.488', 'grad_norm': '21.25', 'counters/examples': 123808, 'counters/updates': 3869}
train stats after 123840 examples: {'rewards_train/chosen': '0.14133', 'rewards_train/rejected': '0.03464', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10669', 'logps_train/rejected': '-100.51', 'logps_train/chosen': '-112.97', 'loss/train': '0.64771', 'examples_per_second': '30.633', 'grad_norm': '18.875', 'counters/examples': 123840, 'counters/updates': 3870}
train stats after 123872 examples: {'rewards_train/chosen': '0.16834', 'rewards_train/rejected': '-0.020746', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18909', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-127.92', 'loss/train': '0.61012', 'examples_per_second': '30.513', 'grad_norm': '18.125', 'counters/examples': 123872, 'counters/updates': 3871}
train stats after 123904 examples: {'rewards_train/chosen': '0.16681', 'rewards_train/rejected': '0.027897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13891', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-150.56', 'loss/train': '0.6361', 'examples_per_second': '31.781', 'grad_norm': '19.75', 'counters/examples': 123904, 'counters/updates': 3872}
skipping logging after 123936 examples to avoid logging too frequently
train stats after 123968 examples: {'rewards_train/chosen': '0.1523', 'rewards_train/rejected': '0.060981', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091321', 'logps_train/rejected': '-100.42', 'logps_train/chosen': '-119.7', 'loss/train': '0.66202', 'examples_per_second': '33.282', 'grad_norm': '18.875', 'counters/examples': 123968, 'counters/updates': 3874}
train stats after 124000 examples: {'rewards_train/chosen': '0.14793', 'rewards_train/rejected': '0.041312', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10662', 'logps_train/rejected': '-115.23', 'logps_train/chosen': '-136.81', 'loss/train': '0.65107', 'examples_per_second': '29.979', 'grad_norm': '20.875', 'counters/examples': 124000, 'counters/updates': 3875}
skipping logging after 124032 examples to avoid logging too frequently
train stats after 124064 examples: {'rewards_train/chosen': '0.17369', 'rewards_train/rejected': '0.063364', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11033', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-166.46', 'loss/train': '0.64372', 'examples_per_second': '31.43', 'grad_norm': '21.125', 'counters/examples': 124064, 'counters/updates': 3877}
train stats after 124096 examples: {'rewards_train/chosen': '0.14976', 'rewards_train/rejected': '0.053018', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096746', 'logps_train/rejected': '-125.32', 'logps_train/chosen': '-138.83', 'loss/train': '0.65505', 'examples_per_second': '32.549', 'grad_norm': '20.125', 'counters/examples': 124096, 'counters/updates': 3878}
train stats after 124128 examples: {'rewards_train/chosen': '0.17923', 'rewards_train/rejected': '0.062355', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11688', 'logps_train/rejected': '-117.21', 'logps_train/chosen': '-137.81', 'loss/train': '0.64361', 'examples_per_second': '31.072', 'grad_norm': '19.375', 'counters/examples': 124128, 'counters/updates': 3879}
train stats after 124160 examples: {'rewards_train/chosen': '0.082334', 'rewards_train/rejected': '0.028939', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053395', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-167.73', 'loss/train': '0.67433', 'examples_per_second': '30.544', 'grad_norm': '21', 'counters/examples': 124160, 'counters/updates': 3880}
train stats after 124192 examples: {'rewards_train/chosen': '0.12068', 'rewards_train/rejected': '0.03803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082652', 'logps_train/rejected': '-135.06', 'logps_train/chosen': '-152.37', 'loss/train': '0.66863', 'examples_per_second': '32.3', 'grad_norm': '21.5', 'counters/examples': 124192, 'counters/updates': 3881}
train stats after 124224 examples: {'rewards_train/chosen': '0.18466', 'rewards_train/rejected': '0.1115', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073158', 'logps_train/rejected': '-119.98', 'logps_train/chosen': '-170.54', 'loss/train': '0.66453', 'examples_per_second': '33.026', 'grad_norm': '21.125', 'counters/examples': 124224, 'counters/updates': 3882}
train stats after 124256 examples: {'rewards_train/chosen': '0.1968', 'rewards_train/rejected': '0.044316', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15249', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-181.33', 'loss/train': '0.63523', 'examples_per_second': '31.451', 'grad_norm': '21.5', 'counters/examples': 124256, 'counters/updates': 3883}
train stats after 124288 examples: {'rewards_train/chosen': '0.11297', 'rewards_train/rejected': '0.06077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052196', 'logps_train/rejected': '-131.81', 'logps_train/chosen': '-135.79', 'loss/train': '0.6787', 'examples_per_second': '31.411', 'grad_norm': '20.75', 'counters/examples': 124288, 'counters/updates': 3884}
train stats after 124320 examples: {'rewards_train/chosen': '0.14243', 'rewards_train/rejected': '0.04655', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095878', 'logps_train/rejected': '-111.3', 'logps_train/chosen': '-165.87', 'loss/train': '0.655', 'examples_per_second': '31.431', 'grad_norm': '20.5', 'counters/examples': 124320, 'counters/updates': 3885}
train stats after 124352 examples: {'rewards_train/chosen': '0.16567', 'rewards_train/rejected': '0.055934', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10973', 'logps_train/rejected': '-94.412', 'logps_train/chosen': '-129.36', 'loss/train': '0.65347', 'examples_per_second': '29.906', 'grad_norm': '19.125', 'counters/examples': 124352, 'counters/updates': 3886}
skipping logging after 124384 examples to avoid logging too frequently
train stats after 124416 examples: {'rewards_train/chosen': '0.19825', 'rewards_train/rejected': '-0.0035326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20178', 'logps_train/rejected': '-109.92', 'logps_train/chosen': '-155.64', 'loss/train': '0.61609', 'examples_per_second': '33.78', 'grad_norm': '19.875', 'counters/examples': 124416, 'counters/updates': 3888}
train stats after 124448 examples: {'rewards_train/chosen': '0.14765', 'rewards_train/rejected': '0.088218', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059434', 'logps_train/rejected': '-150.54', 'logps_train/chosen': '-123.48', 'loss/train': '0.67908', 'examples_per_second': '31.368', 'grad_norm': '20.75', 'counters/examples': 124448, 'counters/updates': 3889}
train stats after 124480 examples: {'rewards_train/chosen': '0.19694', 'rewards_train/rejected': '0.10083', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096109', 'logps_train/rejected': '-118.3', 'logps_train/chosen': '-149.29', 'loss/train': '0.65085', 'examples_per_second': '32.399', 'grad_norm': '19.625', 'counters/examples': 124480, 'counters/updates': 3890}
train stats after 124512 examples: {'rewards_train/chosen': '0.090441', 'rewards_train/rejected': '0.10515', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014712', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-99.057', 'loss/train': '0.71316', 'examples_per_second': '32.245', 'grad_norm': '22.25', 'counters/examples': 124512, 'counters/updates': 3891}
skipping logging after 124544 examples to avoid logging too frequently
train stats after 124576 examples: {'rewards_train/chosen': '0.099128', 'rewards_train/rejected': '-0.028829', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12796', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-128.5', 'loss/train': '0.63869', 'examples_per_second': '25.163', 'grad_norm': '17.875', 'counters/examples': 124576, 'counters/updates': 3893}
train stats after 124608 examples: {'rewards_train/chosen': '0.095627', 'rewards_train/rejected': '-0.052612', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14824', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-132.72', 'loss/train': '0.62911', 'examples_per_second': '30.79', 'grad_norm': '19.25', 'counters/examples': 124608, 'counters/updates': 3894}
train stats after 124640 examples: {'rewards_train/chosen': '0.076757', 'rewards_train/rejected': '0.11409', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03733', 'logps_train/rejected': '-178.39', 'logps_train/chosen': '-135.23', 'loss/train': '0.72523', 'examples_per_second': '32.684', 'grad_norm': '24.625', 'counters/examples': 124640, 'counters/updates': 3895}
train stats after 124672 examples: {'rewards_train/chosen': '0.22843', 'rewards_train/rejected': '0.19468', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033749', 'logps_train/rejected': '-124', 'logps_train/chosen': '-149.43', 'loss/train': '0.68564', 'examples_per_second': '23.932', 'grad_norm': '21.375', 'counters/examples': 124672, 'counters/updates': 3896}
train stats after 124704 examples: {'rewards_train/chosen': '0.12878', 'rewards_train/rejected': '0.087255', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041524', 'logps_train/rejected': '-113.3', 'logps_train/chosen': '-140.32', 'loss/train': '0.68147', 'examples_per_second': '30.623', 'grad_norm': '21.25', 'counters/examples': 124704, 'counters/updates': 3897}
train stats after 124736 examples: {'rewards_train/chosen': '0.1558', 'rewards_train/rejected': '0.041856', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11394', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-168.38', 'loss/train': '0.64835', 'examples_per_second': '30.689', 'grad_norm': '23', 'counters/examples': 124736, 'counters/updates': 3898}
train stats after 124768 examples: {'rewards_train/chosen': '0.18459', 'rewards_train/rejected': '0.083474', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10112', 'logps_train/rejected': '-110.9', 'logps_train/chosen': '-128.86', 'loss/train': '0.65498', 'examples_per_second': '30.755', 'grad_norm': '18.375', 'counters/examples': 124768, 'counters/updates': 3899}
skipping logging after 124800 examples to avoid logging too frequently
train stats after 124832 examples: {'rewards_train/chosen': '0.20989', 'rewards_train/rejected': '0.037785', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1721', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-127.92', 'loss/train': '0.62457', 'examples_per_second': '32.321', 'grad_norm': '19.125', 'counters/examples': 124832, 'counters/updates': 3901}
train stats after 124864 examples: {'rewards_train/chosen': '0.14844', 'rewards_train/rejected': '0.027367', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12107', 'logps_train/rejected': '-107.58', 'logps_train/chosen': '-184.91', 'loss/train': '0.64262', 'examples_per_second': '31.544', 'grad_norm': '20.875', 'counters/examples': 124864, 'counters/updates': 3902}
train stats after 124896 examples: {'rewards_train/chosen': '0.12486', 'rewards_train/rejected': '0.058507', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06635', 'logps_train/rejected': '-128.71', 'logps_train/chosen': '-132.54', 'loss/train': '0.66893', 'examples_per_second': '31.316', 'grad_norm': '21.125', 'counters/examples': 124896, 'counters/updates': 3903}
train stats after 124928 examples: {'rewards_train/chosen': '0.17937', 'rewards_train/rejected': '0.0532', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12617', 'logps_train/rejected': '-119.91', 'logps_train/chosen': '-113.16', 'loss/train': '0.63729', 'examples_per_second': '31.508', 'grad_norm': '18.375', 'counters/examples': 124928, 'counters/updates': 3904}
train stats after 124960 examples: {'rewards_train/chosen': '0.14632', 'rewards_train/rejected': '0.096145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050173', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-135.26', 'loss/train': '0.67438', 'examples_per_second': '32.993', 'grad_norm': '19.875', 'counters/examples': 124960, 'counters/updates': 3905}
train stats after 124992 examples: {'rewards_train/chosen': '0.18863', 'rewards_train/rejected': '0.064392', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12424', 'logps_train/rejected': '-134.46', 'logps_train/chosen': '-162.65', 'loss/train': '0.64704', 'examples_per_second': '31.465', 'grad_norm': '21', 'counters/examples': 124992, 'counters/updates': 3906}
train stats after 125024 examples: {'rewards_train/chosen': '0.095543', 'rewards_train/rejected': '0.059948', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035595', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-116.57', 'loss/train': '0.68526', 'examples_per_second': '31.5', 'grad_norm': '20', 'counters/examples': 125024, 'counters/updates': 3907}
train stats after 125056 examples: {'rewards_train/chosen': '0.14761', 'rewards_train/rejected': '0.053942', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093666', 'logps_train/rejected': '-116.54', 'logps_train/chosen': '-163.42', 'loss/train': '0.66274', 'examples_per_second': '31.512', 'grad_norm': '22.25', 'counters/examples': 125056, 'counters/updates': 3908}
train stats after 125088 examples: {'rewards_train/chosen': '0.071896', 'rewards_train/rejected': '-0.021894', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09379', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-131.21', 'loss/train': '0.65706', 'examples_per_second': '31.152', 'grad_norm': '20.25', 'counters/examples': 125088, 'counters/updates': 3909}
train stats after 125120 examples: {'rewards_train/chosen': '0.19773', 'rewards_train/rejected': '0.066896', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13084', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-176.98', 'loss/train': '0.64457', 'examples_per_second': '29.859', 'grad_norm': '21', 'counters/examples': 125120, 'counters/updates': 3910}
train stats after 125152 examples: {'rewards_train/chosen': '0.23283', 'rewards_train/rejected': '0.10411', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12872', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-169.06', 'loss/train': '0.64391', 'examples_per_second': '30.058', 'grad_norm': '22.375', 'counters/examples': 125152, 'counters/updates': 3911}
skipping logging after 125184 examples to avoid logging too frequently
train stats after 125216 examples: {'rewards_train/chosen': '0.10751', 'rewards_train/rejected': '0.065426', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042087', 'logps_train/rejected': '-112.76', 'logps_train/chosen': '-129.74', 'loss/train': '0.67828', 'examples_per_second': '34.798', 'grad_norm': '19.625', 'counters/examples': 125216, 'counters/updates': 3913}
train stats after 125248 examples: {'rewards_train/chosen': '0.1952', 'rewards_train/rejected': '0.081309', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11389', 'logps_train/rejected': '-122.79', 'logps_train/chosen': '-144.78', 'loss/train': '0.65643', 'examples_per_second': '32.412', 'grad_norm': '20.875', 'counters/examples': 125248, 'counters/updates': 3914}
train stats after 125280 examples: {'rewards_train/chosen': '0.1318', 'rewards_train/rejected': '0.044337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087466', 'logps_train/rejected': '-102.25', 'logps_train/chosen': '-120.45', 'loss/train': '0.65947', 'examples_per_second': '31.133', 'grad_norm': '19.375', 'counters/examples': 125280, 'counters/updates': 3915}
skipping logging after 125312 examples to avoid logging too frequently
train stats after 125344 examples: {'rewards_train/chosen': '0.14136', 'rewards_train/rejected': '0.029734', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11162', 'logps_train/rejected': '-124.4', 'logps_train/chosen': '-141.16', 'loss/train': '0.64399', 'examples_per_second': '33.748', 'grad_norm': '20.5', 'counters/examples': 125344, 'counters/updates': 3917}
train stats after 125376 examples: {'rewards_train/chosen': '0.09278', 'rewards_train/rejected': '0.066769', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026011', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-149.81', 'loss/train': '0.69029', 'examples_per_second': '33.221', 'grad_norm': '20', 'counters/examples': 125376, 'counters/updates': 3918}
skipping logging after 125408 examples to avoid logging too frequently
train stats after 125440 examples: {'rewards_train/chosen': '0.11414', 'rewards_train/rejected': '0.030764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083375', 'logps_train/rejected': '-92.114', 'logps_train/chosen': '-146.23', 'loss/train': '0.662', 'examples_per_second': '30.427', 'grad_norm': '20.25', 'counters/examples': 125440, 'counters/updates': 3920}
train stats after 125472 examples: {'rewards_train/chosen': '0.086949', 'rewards_train/rejected': '0.06531', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02164', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-118.4', 'loss/train': '0.69217', 'examples_per_second': '31.505', 'grad_norm': '20.75', 'counters/examples': 125472, 'counters/updates': 3921}
train stats after 125504 examples: {'rewards_train/chosen': '0.097316', 'rewards_train/rejected': '0.031778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065538', 'logps_train/rejected': '-98.832', 'logps_train/chosen': '-98.572', 'loss/train': '0.66619', 'examples_per_second': '30.166', 'grad_norm': '17.75', 'counters/examples': 125504, 'counters/updates': 3922}
train stats after 125536 examples: {'rewards_train/chosen': '0.15451', 'rewards_train/rejected': '0.13127', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023248', 'logps_train/rejected': '-100.32', 'logps_train/chosen': '-119.47', 'loss/train': '0.69175', 'examples_per_second': '31.541', 'grad_norm': '18', 'counters/examples': 125536, 'counters/updates': 3923}
train stats after 125568 examples: {'rewards_train/chosen': '0.1275', 'rewards_train/rejected': '0.017305', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1102', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-149.74', 'loss/train': '0.64893', 'examples_per_second': '30.388', 'grad_norm': '20.25', 'counters/examples': 125568, 'counters/updates': 3924}
train stats after 125600 examples: {'rewards_train/chosen': '0.11752', 'rewards_train/rejected': '0.049933', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067583', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-137.05', 'loss/train': '0.67239', 'examples_per_second': '31.431', 'grad_norm': '19.375', 'counters/examples': 125600, 'counters/updates': 3925}
train stats after 125632 examples: {'rewards_train/chosen': '0.13758', 'rewards_train/rejected': '0.040468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09711', 'logps_train/rejected': '-92.575', 'logps_train/chosen': '-159.78', 'loss/train': '0.65884', 'examples_per_second': '30.167', 'grad_norm': '19.75', 'counters/examples': 125632, 'counters/updates': 3926}
train stats after 125664 examples: {'rewards_train/chosen': '0.10462', 'rewards_train/rejected': '0.036612', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068004', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-134.8', 'loss/train': '0.66514', 'examples_per_second': '29.992', 'grad_norm': '20.375', 'counters/examples': 125664, 'counters/updates': 3927}
train stats after 125696 examples: {'rewards_train/chosen': '0.16827', 'rewards_train/rejected': '0.106', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06227', 'logps_train/rejected': '-105.42', 'logps_train/chosen': '-124.83', 'loss/train': '0.67852', 'examples_per_second': '31.661', 'grad_norm': '19.25', 'counters/examples': 125696, 'counters/updates': 3928}
train stats after 125728 examples: {'rewards_train/chosen': '0.1733', 'rewards_train/rejected': '0.037181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13612', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-161.39', 'loss/train': '0.64668', 'examples_per_second': '31.379', 'grad_norm': '22.125', 'counters/examples': 125728, 'counters/updates': 3929}
train stats after 125760 examples: {'rewards_train/chosen': '0.17146', 'rewards_train/rejected': '0.036353', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13511', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-149.53', 'loss/train': '0.63559', 'examples_per_second': '31.448', 'grad_norm': '20', 'counters/examples': 125760, 'counters/updates': 3930}
train stats after 125792 examples: {'rewards_train/chosen': '0.16427', 'rewards_train/rejected': '0.15616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.008115', 'logps_train/rejected': '-152.55', 'logps_train/chosen': '-169.78', 'loss/train': '0.70724', 'examples_per_second': '31.474', 'grad_norm': '22.25', 'counters/examples': 125792, 'counters/updates': 3931}
train stats after 125824 examples: {'rewards_train/chosen': '0.20579', 'rewards_train/rejected': '0.020717', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18507', 'logps_train/rejected': '-135.24', 'logps_train/chosen': '-138.47', 'loss/train': '0.61981', 'examples_per_second': '32.082', 'grad_norm': '19.625', 'counters/examples': 125824, 'counters/updates': 3932}
train stats after 125856 examples: {'rewards_train/chosen': '0.10418', 'rewards_train/rejected': '0.058541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045638', 'logps_train/rejected': '-134.41', 'logps_train/chosen': '-130.13', 'loss/train': '0.67567', 'examples_per_second': '31.606', 'grad_norm': '20.375', 'counters/examples': 125856, 'counters/updates': 3933}
skipping logging after 125888 examples to avoid logging too frequently
train stats after 125920 examples: {'rewards_train/chosen': '0.14467', 'rewards_train/rejected': '0.039507', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-174.94', 'loss/train': '0.65832', 'examples_per_second': '30.499', 'grad_norm': '22.75', 'counters/examples': 125920, 'counters/updates': 3935}
train stats after 125952 examples: {'rewards_train/chosen': '0.19338', 'rewards_train/rejected': '0.076296', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11708', 'logps_train/rejected': '-116.04', 'logps_train/chosen': '-149.86', 'loss/train': '0.64271', 'examples_per_second': '31.195', 'grad_norm': '19.75', 'counters/examples': 125952, 'counters/updates': 3936}
skipping logging after 125984 examples to avoid logging too frequently
train stats after 126016 examples: {'rewards_train/chosen': '0.12058', 'rewards_train/rejected': '0.036567', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084013', 'logps_train/rejected': '-113.88', 'logps_train/chosen': '-130.44', 'loss/train': '0.65938', 'examples_per_second': '31.264', 'grad_norm': '19', 'counters/examples': 126016, 'counters/updates': 3938}
train stats after 126048 examples: {'rewards_train/chosen': '0.099897', 'rewards_train/rejected': '0.073631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026266', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-123', 'loss/train': '0.68843', 'examples_per_second': '30.919', 'grad_norm': '20.125', 'counters/examples': 126048, 'counters/updates': 3939}
train stats after 126080 examples: {'rewards_train/chosen': '0.1439', 'rewards_train/rejected': '0.12924', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014661', 'logps_train/rejected': '-115.04', 'logps_train/chosen': '-174.01', 'loss/train': '0.69365', 'examples_per_second': '31.481', 'grad_norm': '22.625', 'counters/examples': 126080, 'counters/updates': 3940}
skipping logging after 126112 examples to avoid logging too frequently
train stats after 126144 examples: {'rewards_train/chosen': '0.083903', 'rewards_train/rejected': '0.011026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072877', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-151.65', 'loss/train': '0.66856', 'examples_per_second': '30.006', 'grad_norm': '21.5', 'counters/examples': 126144, 'counters/updates': 3942}
train stats after 126176 examples: {'rewards_train/chosen': '0.11354', 'rewards_train/rejected': '0.069316', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044227', 'logps_train/rejected': '-103.75', 'logps_train/chosen': '-111.43', 'loss/train': '0.68308', 'examples_per_second': '32.035', 'grad_norm': '18.875', 'counters/examples': 126176, 'counters/updates': 3943}
train stats after 126208 examples: {'rewards_train/chosen': '0.080552', 'rewards_train/rejected': '0.041432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03912', 'logps_train/rejected': '-107.99', 'logps_train/chosen': '-123.55', 'loss/train': '0.6896', 'examples_per_second': '33.035', 'grad_norm': '19.625', 'counters/examples': 126208, 'counters/updates': 3944}
train stats after 126240 examples: {'rewards_train/chosen': '0.10429', 'rewards_train/rejected': '0.067832', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036454', 'logps_train/rejected': '-102.71', 'logps_train/chosen': '-134.01', 'loss/train': '0.68277', 'examples_per_second': '31.185', 'grad_norm': '20.125', 'counters/examples': 126240, 'counters/updates': 3945}
train stats after 126272 examples: {'rewards_train/chosen': '0.14657', 'rewards_train/rejected': '0.083015', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.063551', 'logps_train/rejected': '-130.97', 'logps_train/chosen': '-167.82', 'loss/train': '0.67805', 'examples_per_second': '30.328', 'grad_norm': '22.75', 'counters/examples': 126272, 'counters/updates': 3946}
train stats after 126304 examples: {'rewards_train/chosen': '0.17186', 'rewards_train/rejected': '0.054142', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11771', 'logps_train/rejected': '-140.59', 'logps_train/chosen': '-183.36', 'loss/train': '0.64844', 'examples_per_second': '30.636', 'grad_norm': '22.875', 'counters/examples': 126304, 'counters/updates': 3947}
train stats after 126336 examples: {'rewards_train/chosen': '0.12355', 'rewards_train/rejected': '0.10539', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018159', 'logps_train/rejected': '-154.17', 'logps_train/chosen': '-111.41', 'loss/train': '0.69169', 'examples_per_second': '30.523', 'grad_norm': '22.125', 'counters/examples': 126336, 'counters/updates': 3948}
train stats after 126368 examples: {'rewards_train/chosen': '0.23055', 'rewards_train/rejected': '0.02081', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20974', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-183.48', 'loss/train': '0.60801', 'examples_per_second': '30.34', 'grad_norm': '20.75', 'counters/examples': 126368, 'counters/updates': 3949}
train stats after 126400 examples: {'rewards_train/chosen': '0.083847', 'rewards_train/rejected': '0.01013', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073717', 'logps_train/rejected': '-103.5', 'logps_train/chosen': '-172.93', 'loss/train': '0.66365', 'examples_per_second': '30.462', 'grad_norm': '22.75', 'counters/examples': 126400, 'counters/updates': 3950}
train stats after 126432 examples: {'rewards_train/chosen': '0.10498', 'rewards_train/rejected': '0.0039181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10106', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-151.56', 'loss/train': '0.65291', 'examples_per_second': '30.987', 'grad_norm': '21', 'counters/examples': 126432, 'counters/updates': 3951}
train stats after 126464 examples: {'rewards_train/chosen': '0.12035', 'rewards_train/rejected': '0.061957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058395', 'logps_train/rejected': '-155.92', 'logps_train/chosen': '-159.34', 'loss/train': '0.68007', 'examples_per_second': '31.213', 'grad_norm': '24.5', 'counters/examples': 126464, 'counters/updates': 3952}
train stats after 126496 examples: {'rewards_train/chosen': '0.14182', 'rewards_train/rejected': '0.081277', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.060547', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-121.63', 'loss/train': '0.6747', 'examples_per_second': '31.473', 'grad_norm': '20.125', 'counters/examples': 126496, 'counters/updates': 3953}
train stats after 126528 examples: {'rewards_train/chosen': '0.11759', 'rewards_train/rejected': '0.041789', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075796', 'logps_train/rejected': '-88.354', 'logps_train/chosen': '-125.98', 'loss/train': '0.67253', 'examples_per_second': '31.404', 'grad_norm': '18.125', 'counters/examples': 126528, 'counters/updates': 3954}
train stats after 126560 examples: {'rewards_train/chosen': '0.017492', 'rewards_train/rejected': '0.038685', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.021193', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-113.77', 'loss/train': '0.71159', 'examples_per_second': '31.194', 'grad_norm': '19.75', 'counters/examples': 126560, 'counters/updates': 3955}
train stats after 126592 examples: {'rewards_train/chosen': '0.13886', 'rewards_train/rejected': '-0.020186', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15905', 'logps_train/rejected': '-90.413', 'logps_train/chosen': '-114.97', 'loss/train': '0.62321', 'examples_per_second': '30.387', 'grad_norm': '17.125', 'counters/examples': 126592, 'counters/updates': 3956}
train stats after 126624 examples: {'rewards_train/chosen': '0.23', 'rewards_train/rejected': '0.070051', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15995', 'logps_train/rejected': '-103.4', 'logps_train/chosen': '-125.48', 'loss/train': '0.62559', 'examples_per_second': '29.972', 'grad_norm': '17', 'counters/examples': 126624, 'counters/updates': 3957}
train stats after 126656 examples: {'rewards_train/chosen': '0.14133', 'rewards_train/rejected': '0.057347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083984', 'logps_train/rejected': '-156.27', 'logps_train/chosen': '-153.96', 'loss/train': '0.66856', 'examples_per_second': '32.216', 'grad_norm': '22', 'counters/examples': 126656, 'counters/updates': 3958}
skipping logging after 126688 examples to avoid logging too frequently
train stats after 126720 examples: {'rewards_train/chosen': '0.21995', 'rewards_train/rejected': '0.10948', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11047', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-147.64', 'loss/train': '0.6439', 'examples_per_second': '35.653', 'grad_norm': '19.875', 'counters/examples': 126720, 'counters/updates': 3960}
skipping logging after 126752 examples to avoid logging too frequently
train stats after 126784 examples: {'rewards_train/chosen': '0.15907', 'rewards_train/rejected': '0.050108', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10896', 'logps_train/rejected': '-95.22', 'logps_train/chosen': '-110.05', 'loss/train': '0.64715', 'examples_per_second': '31.281', 'grad_norm': '18', 'counters/examples': 126784, 'counters/updates': 3962}
train stats after 126816 examples: {'rewards_train/chosen': '0.14117', 'rewards_train/rejected': '0.063289', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077883', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-131.79', 'loss/train': '0.66303', 'examples_per_second': '30.496', 'grad_norm': '20.75', 'counters/examples': 126816, 'counters/updates': 3963}
train stats after 126848 examples: {'rewards_train/chosen': '0.15024', 'rewards_train/rejected': '0.10046', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049772', 'logps_train/rejected': '-107.52', 'logps_train/chosen': '-142.8', 'loss/train': '0.6785', 'examples_per_second': '30.361', 'grad_norm': '20.625', 'counters/examples': 126848, 'counters/updates': 3964}
train stats after 126880 examples: {'rewards_train/chosen': '0.15447', 'rewards_train/rejected': '0.018569', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1359', 'logps_train/rejected': '-95.433', 'logps_train/chosen': '-142.46', 'loss/train': '0.63666', 'examples_per_second': '31.333', 'grad_norm': '18.75', 'counters/examples': 126880, 'counters/updates': 3965}
train stats after 126912 examples: {'rewards_train/chosen': '0.020025', 'rewards_train/rejected': '0.05197', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.031945', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-98.787', 'loss/train': '0.71518', 'examples_per_second': '31.133', 'grad_norm': '19.875', 'counters/examples': 126912, 'counters/updates': 3966}
train stats after 126944 examples: {'rewards_train/chosen': '0.16013', 'rewards_train/rejected': '0.0016344', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1585', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-127.07', 'loss/train': '0.62233', 'examples_per_second': '24.472', 'grad_norm': '18', 'counters/examples': 126944, 'counters/updates': 3967}
train stats after 126976 examples: {'rewards_train/chosen': '0.19217', 'rewards_train/rejected': '0.076477', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11569', 'logps_train/rejected': '-113.88', 'logps_train/chosen': '-138.14', 'loss/train': '0.64629', 'examples_per_second': '31.432', 'grad_norm': '19.25', 'counters/examples': 126976, 'counters/updates': 3968}
train stats after 127008 examples: {'rewards_train/chosen': '0.093308', 'rewards_train/rejected': '0.059416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033892', 'logps_train/rejected': '-108.21', 'logps_train/chosen': '-139.13', 'loss/train': '0.68295', 'examples_per_second': '31.536', 'grad_norm': '18.625', 'counters/examples': 127008, 'counters/updates': 3969}
train stats after 127040 examples: {'rewards_train/chosen': '0.13933', 'rewards_train/rejected': '0.022189', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11714', 'logps_train/rejected': '-98.907', 'logps_train/chosen': '-123.13', 'loss/train': '0.64298', 'examples_per_second': '31.144', 'grad_norm': '18.625', 'counters/examples': 127040, 'counters/updates': 3970}
train stats after 127072 examples: {'rewards_train/chosen': '0.15891', 'rewards_train/rejected': '0.076865', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082046', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-178.33', 'loss/train': '0.6652', 'examples_per_second': '31.524', 'grad_norm': '23', 'counters/examples': 127072, 'counters/updates': 3971}
train stats after 127104 examples: {'rewards_train/chosen': '0.14558', 'rewards_train/rejected': '-0.044225', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18981', 'logps_train/rejected': '-112.38', 'logps_train/chosen': '-120.51', 'loss/train': '0.61312', 'examples_per_second': '30.301', 'grad_norm': '18.25', 'counters/examples': 127104, 'counters/updates': 3972}
train stats after 127136 examples: {'rewards_train/chosen': '0.1933', 'rewards_train/rejected': '0.018948', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17435', 'logps_train/rejected': '-89.901', 'logps_train/chosen': '-160.51', 'loss/train': '0.61827', 'examples_per_second': '32.677', 'grad_norm': '19.625', 'counters/examples': 127136, 'counters/updates': 3973}
skipping logging after 127168 examples to avoid logging too frequently
train stats after 127200 examples: {'rewards_train/chosen': '0.12621', 'rewards_train/rejected': '0.075726', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.050488', 'logps_train/rejected': '-115.11', 'logps_train/chosen': '-116.91', 'loss/train': '0.68575', 'examples_per_second': '29.909', 'grad_norm': '20.625', 'counters/examples': 127200, 'counters/updates': 3975}
train stats after 127232 examples: {'rewards_train/chosen': '0.14131', 'rewards_train/rejected': '0.0064189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13489', 'logps_train/rejected': '-89.521', 'logps_train/chosen': '-163.17', 'loss/train': '0.636', 'examples_per_second': '30.619', 'grad_norm': '20.75', 'counters/examples': 127232, 'counters/updates': 3976}
train stats after 127264 examples: {'rewards_train/chosen': '0.1991', 'rewards_train/rejected': '0.099926', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099178', 'logps_train/rejected': '-137.37', 'logps_train/chosen': '-183.82', 'loss/train': '0.65292', 'examples_per_second': '31.463', 'grad_norm': '22.375', 'counters/examples': 127264, 'counters/updates': 3977}
train stats after 127296 examples: {'rewards_train/chosen': '0.13587', 'rewards_train/rejected': '0.045619', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090247', 'logps_train/rejected': '-141.33', 'logps_train/chosen': '-160.58', 'loss/train': '0.6583', 'examples_per_second': '31.465', 'grad_norm': '20.875', 'counters/examples': 127296, 'counters/updates': 3978}
train stats after 127328 examples: {'rewards_train/chosen': '0.1845', 'rewards_train/rejected': '0.077482', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10702', 'logps_train/rejected': '-107.5', 'logps_train/chosen': '-128.62', 'loss/train': '0.64926', 'examples_per_second': '32.173', 'grad_norm': '19.625', 'counters/examples': 127328, 'counters/updates': 3979}
train stats after 127360 examples: {'rewards_train/chosen': '0.12541', 'rewards_train/rejected': '0.059777', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.065632', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-116.25', 'loss/train': '0.66529', 'examples_per_second': '31.006', 'grad_norm': '19.5', 'counters/examples': 127360, 'counters/updates': 3980}
train stats after 127392 examples: {'rewards_train/chosen': '0.1941', 'rewards_train/rejected': '0.020246', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17386', 'logps_train/rejected': '-157.16', 'logps_train/chosen': '-174.82', 'loss/train': '0.621', 'examples_per_second': '31.388', 'grad_norm': '21.25', 'counters/examples': 127392, 'counters/updates': 3981}
train stats after 127424 examples: {'rewards_train/chosen': '0.12203', 'rewards_train/rejected': '0.088522', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033513', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-160.74', 'loss/train': '0.68515', 'examples_per_second': '30.287', 'grad_norm': '21.875', 'counters/examples': 127424, 'counters/updates': 3982}
skipping logging after 127456 examples to avoid logging too frequently
train stats after 127488 examples: {'rewards_train/chosen': '0.096395', 'rewards_train/rejected': '-0.001667', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098062', 'logps_train/rejected': '-109.84', 'logps_train/chosen': '-119.97', 'loss/train': '0.65278', 'examples_per_second': '30.484', 'grad_norm': '19.75', 'counters/examples': 127488, 'counters/updates': 3984}
train stats after 127520 examples: {'rewards_train/chosen': '0.14807', 'rewards_train/rejected': '0.12315', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024917', 'logps_train/rejected': '-147.29', 'logps_train/chosen': '-135.63', 'loss/train': '0.6863', 'examples_per_second': '33.04', 'grad_norm': '21.75', 'counters/examples': 127520, 'counters/updates': 3985}
train stats after 127552 examples: {'rewards_train/chosen': '0.098408', 'rewards_train/rejected': '0.049655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048753', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-128.07', 'loss/train': '0.67651', 'examples_per_second': '31.261', 'grad_norm': '19.875', 'counters/examples': 127552, 'counters/updates': 3986}
skipping logging after 127584 examples to avoid logging too frequently
train stats after 127616 examples: {'rewards_train/chosen': '0.15437', 'rewards_train/rejected': '0.05438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099992', 'logps_train/rejected': '-114.84', 'logps_train/chosen': '-125.93', 'loss/train': '0.64991', 'examples_per_second': '30.064', 'grad_norm': '18.75', 'counters/examples': 127616, 'counters/updates': 3988}
skipping logging after 127648 examples to avoid logging too frequently
train stats after 127680 examples: {'rewards_train/chosen': '0.14767', 'rewards_train/rejected': '0.094615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053059', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-123.61', 'loss/train': '0.67718', 'examples_per_second': '32.395', 'grad_norm': '20.375', 'counters/examples': 127680, 'counters/updates': 3990}
train stats after 127712 examples: {'rewards_train/chosen': '0.17562', 'rewards_train/rejected': '0.10767', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.067948', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-167.98', 'loss/train': '0.66761', 'examples_per_second': '31.409', 'grad_norm': '22', 'counters/examples': 127712, 'counters/updates': 3991}
train stats after 127744 examples: {'rewards_train/chosen': '0.10899', 'rewards_train/rejected': '0.09181', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017178', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-130.11', 'loss/train': '0.69134', 'examples_per_second': '30.09', 'grad_norm': '21.125', 'counters/examples': 127744, 'counters/updates': 3992}
train stats after 127776 examples: {'rewards_train/chosen': '0.081482', 'rewards_train/rejected': '0.019072', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06241', 'logps_train/rejected': '-121.73', 'logps_train/chosen': '-111.58', 'loss/train': '0.67122', 'examples_per_second': '31.426', 'grad_norm': '19.25', 'counters/examples': 127776, 'counters/updates': 3993}
train stats after 127808 examples: {'rewards_train/chosen': '0.1811', 'rewards_train/rejected': '0.043248', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13786', 'logps_train/rejected': '-140.04', 'logps_train/chosen': '-137.46', 'loss/train': '0.63562', 'examples_per_second': '29.951', 'grad_norm': '21', 'counters/examples': 127808, 'counters/updates': 3994}
train stats after 127840 examples: {'rewards_train/chosen': '0.14745', 'rewards_train/rejected': '0.060495', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086954', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-166.02', 'loss/train': '0.66183', 'examples_per_second': '32.149', 'grad_norm': '20.5', 'counters/examples': 127840, 'counters/updates': 3995}
train stats after 127872 examples: {'rewards_train/chosen': '0.1303', 'rewards_train/rejected': '0.055783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074515', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-121.89', 'loss/train': '0.66889', 'examples_per_second': '31.345', 'grad_norm': '18.75', 'counters/examples': 127872, 'counters/updates': 3996}
train stats after 127904 examples: {'rewards_train/chosen': '0.10138', 'rewards_train/rejected': '0.0081616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093214', 'logps_train/rejected': '-81.73', 'logps_train/chosen': '-88.151', 'loss/train': '0.65484', 'examples_per_second': '32.468', 'grad_norm': '15.312', 'counters/examples': 127904, 'counters/updates': 3997}
train stats after 127936 examples: {'rewards_train/chosen': '0.05657', 'rewards_train/rejected': '0.012577', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043993', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-150.47', 'loss/train': '0.6785', 'examples_per_second': '33.094', 'grad_norm': '21.75', 'counters/examples': 127936, 'counters/updates': 3998}
train stats after 127968 examples: {'rewards_train/chosen': '0.082815', 'rewards_train/rejected': '0.081141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0016742', 'logps_train/rejected': '-104.11', 'logps_train/chosen': '-153.09', 'loss/train': '0.70127', 'examples_per_second': '31.5', 'grad_norm': '23.5', 'counters/examples': 127968, 'counters/updates': 3999}
train stats after 128000 examples: {'rewards_train/chosen': '0.026873', 'rewards_train/rejected': '-0.003507', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03038', 'logps_train/rejected': '-92.302', 'logps_train/chosen': '-103.42', 'loss/train': '0.68577', 'examples_per_second': '30.447', 'grad_norm': '20.375', 'counters/examples': 128000, 'counters/updates': 4000}
skipping logging after 128032 examples to avoid logging too frequently
train stats after 128064 examples: {'rewards_train/chosen': '0.1579', 'rewards_train/rejected': '0.097759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060144', 'logps_train/rejected': '-145.66', 'logps_train/chosen': '-119.6', 'loss/train': '0.66744', 'examples_per_second': '32.052', 'grad_norm': '19.5', 'counters/examples': 128064, 'counters/updates': 4002}
train stats after 128096 examples: {'rewards_train/chosen': '0.11169', 'rewards_train/rejected': '0.073068', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03862', 'logps_train/rejected': '-161.5', 'logps_train/chosen': '-123.4', 'loss/train': '0.68185', 'examples_per_second': '31.401', 'grad_norm': '22', 'counters/examples': 128096, 'counters/updates': 4003}
train stats after 128128 examples: {'rewards_train/chosen': '0.075786', 'rewards_train/rejected': '0.041977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033808', 'logps_train/rejected': '-107.83', 'logps_train/chosen': '-106.44', 'loss/train': '0.68652', 'examples_per_second': '31.41', 'grad_norm': '18.5', 'counters/examples': 128128, 'counters/updates': 4004}
train stats after 128160 examples: {'rewards_train/chosen': '0.14597', 'rewards_train/rejected': '0.022772', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1232', 'logps_train/rejected': '-94.322', 'logps_train/chosen': '-111.41', 'loss/train': '0.64312', 'examples_per_second': '31.13', 'grad_norm': '16.875', 'counters/examples': 128160, 'counters/updates': 4005}
train stats after 128192 examples: {'rewards_train/chosen': '0.14198', 'rewards_train/rejected': '0.0077005', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13428', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-126.95', 'loss/train': '0.63627', 'examples_per_second': '32.048', 'grad_norm': '18.75', 'counters/examples': 128192, 'counters/updates': 4006}
train stats after 128224 examples: {'rewards_train/chosen': '0.1201', 'rewards_train/rejected': '0.046797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073306', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-130.88', 'loss/train': '0.67168', 'examples_per_second': '31.5', 'grad_norm': '20', 'counters/examples': 128224, 'counters/updates': 4007}
train stats after 128256 examples: {'rewards_train/chosen': '0.17549', 'rewards_train/rejected': '-0.025972', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20146', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-160.94', 'loss/train': '0.61494', 'examples_per_second': '31.594', 'grad_norm': '19.375', 'counters/examples': 128256, 'counters/updates': 4008}
skipping logging after 128288 examples to avoid logging too frequently
train stats after 128320 examples: {'rewards_train/chosen': '0.18175', 'rewards_train/rejected': '0.011005', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17075', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-165.93', 'loss/train': '0.62405', 'examples_per_second': '29.673', 'grad_norm': '20.5', 'counters/examples': 128320, 'counters/updates': 4010}
train stats after 128352 examples: {'rewards_train/chosen': '0.14009', 'rewards_train/rejected': '-0.018384', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15847', 'logps_train/rejected': '-112.48', 'logps_train/chosen': '-114.92', 'loss/train': '0.6263', 'examples_per_second': '30.841', 'grad_norm': '18.375', 'counters/examples': 128352, 'counters/updates': 4011}
train stats after 128384 examples: {'rewards_train/chosen': '0.078125', 'rewards_train/rejected': '0.14', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.061875', 'logps_train/rejected': '-146.33', 'logps_train/chosen': '-158.65', 'loss/train': '0.7368', 'examples_per_second': '31.439', 'grad_norm': '23.875', 'counters/examples': 128384, 'counters/updates': 4012}
train stats after 128416 examples: {'rewards_train/chosen': '0.22933', 'rewards_train/rejected': '0.015515', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21381', 'logps_train/rejected': '-105.37', 'logps_train/chosen': '-126.75', 'loss/train': '0.60373', 'examples_per_second': '31.43', 'grad_norm': '18', 'counters/examples': 128416, 'counters/updates': 4013}
train stats after 128448 examples: {'rewards_train/chosen': '0.080742', 'rewards_train/rejected': '-0.047264', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12801', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-111.8', 'loss/train': '0.63736', 'examples_per_second': '30.821', 'grad_norm': '19.625', 'counters/examples': 128448, 'counters/updates': 4014}
skipping logging after 128480 examples to avoid logging too frequently
train stats after 128512 examples: {'rewards_train/chosen': '0.21885', 'rewards_train/rejected': '0.067795', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15105', 'logps_train/rejected': '-141.52', 'logps_train/chosen': '-147.14', 'loss/train': '0.63049', 'examples_per_second': '31.048', 'grad_norm': '19.75', 'counters/examples': 128512, 'counters/updates': 4016}
train stats after 128544 examples: {'rewards_train/chosen': '0.050087', 'rewards_train/rejected': '0.0329', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017187', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-125.02', 'loss/train': '0.69356', 'examples_per_second': '32.296', 'grad_norm': '20.625', 'counters/examples': 128544, 'counters/updates': 4017}
train stats after 128576 examples: {'rewards_train/chosen': '0.13946', 'rewards_train/rejected': '0.052857', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0866', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-138.19', 'loss/train': '0.66363', 'examples_per_second': '30.804', 'grad_norm': '20.75', 'counters/examples': 128576, 'counters/updates': 4018}
train stats after 128608 examples: {'rewards_train/chosen': '0.096082', 'rewards_train/rejected': '0.045377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050705', 'logps_train/rejected': '-105.82', 'logps_train/chosen': '-123.61', 'loss/train': '0.6794', 'examples_per_second': '31.45', 'grad_norm': '19.625', 'counters/examples': 128608, 'counters/updates': 4019}
train stats after 128640 examples: {'rewards_train/chosen': '0.16737', 'rewards_train/rejected': '0.13887', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028493', 'logps_train/rejected': '-137.96', 'logps_train/chosen': '-162.14', 'loss/train': '0.68596', 'examples_per_second': '30.131', 'grad_norm': '22.75', 'counters/examples': 128640, 'counters/updates': 4020}
train stats after 128672 examples: {'rewards_train/chosen': '0.12166', 'rewards_train/rejected': '0.048028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073629', 'logps_train/rejected': '-157.26', 'logps_train/chosen': '-151.53', 'loss/train': '0.66671', 'examples_per_second': '30.765', 'grad_norm': '21.625', 'counters/examples': 128672, 'counters/updates': 4021}
train stats after 128704 examples: {'rewards_train/chosen': '0.1172', 'rewards_train/rejected': '0.047547', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069649', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-112.68', 'loss/train': '0.67005', 'examples_per_second': '31.558', 'grad_norm': '21.625', 'counters/examples': 128704, 'counters/updates': 4022}
skipping logging after 128736 examples to avoid logging too frequently
train stats after 128768 examples: {'rewards_train/chosen': '0.15261', 'rewards_train/rejected': '0.0012704', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15134', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-170.22', 'loss/train': '0.62983', 'examples_per_second': '31.44', 'grad_norm': '21.375', 'counters/examples': 128768, 'counters/updates': 4024}
train stats after 128800 examples: {'rewards_train/chosen': '0.091526', 'rewards_train/rejected': '0.018134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073393', 'logps_train/rejected': '-95.618', 'logps_train/chosen': '-104.27', 'loss/train': '0.66681', 'examples_per_second': '30.883', 'grad_norm': '17.125', 'counters/examples': 128800, 'counters/updates': 4025}
train stats after 128832 examples: {'rewards_train/chosen': '0.11992', 'rewards_train/rejected': '0.0075489', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11237', 'logps_train/rejected': '-144.68', 'logps_train/chosen': '-150.02', 'loss/train': '0.65166', 'examples_per_second': '31.499', 'grad_norm': '22.25', 'counters/examples': 128832, 'counters/updates': 4026}
skipping logging after 128864 examples to avoid logging too frequently
train stats after 128896 examples: {'rewards_train/chosen': '0.14834', 'rewards_train/rejected': '0.093704', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.054637', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-135.67', 'loss/train': '0.68213', 'examples_per_second': '31.841', 'grad_norm': '22.75', 'counters/examples': 128896, 'counters/updates': 4028}
train stats after 128928 examples: {'rewards_train/chosen': '0.18924', 'rewards_train/rejected': '0.073729', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11551', 'logps_train/rejected': '-152.63', 'logps_train/chosen': '-145.75', 'loss/train': '0.6541', 'examples_per_second': '30.447', 'grad_norm': '22.375', 'counters/examples': 128928, 'counters/updates': 4029}
train stats after 128960 examples: {'rewards_train/chosen': '0.12309', 'rewards_train/rejected': '0.028053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095032', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-134.4', 'loss/train': '0.65751', 'examples_per_second': '32.087', 'grad_norm': '20.625', 'counters/examples': 128960, 'counters/updates': 4030}
train stats after 128992 examples: {'rewards_train/chosen': '0.12331', 'rewards_train/rejected': '0.017732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10557', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-170.56', 'loss/train': '0.64807', 'examples_per_second': '32.159', 'grad_norm': '21.25', 'counters/examples': 128992, 'counters/updates': 4031}
train stats after 129024 examples: {'rewards_train/chosen': '0.10488', 'rewards_train/rejected': '0.039496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065379', 'logps_train/rejected': '-108.06', 'logps_train/chosen': '-98.378', 'loss/train': '0.67092', 'examples_per_second': '31.012', 'grad_norm': '18.5', 'counters/examples': 129024, 'counters/updates': 4032}
train stats after 129056 examples: {'rewards_train/chosen': '0.10889', 'rewards_train/rejected': '0.045769', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063118', 'logps_train/rejected': '-109.74', 'logps_train/chosen': '-121.4', 'loss/train': '0.67375', 'examples_per_second': '32.867', 'grad_norm': '19.25', 'counters/examples': 129056, 'counters/updates': 4033}
train stats after 129088 examples: {'rewards_train/chosen': '0.18187', 'rewards_train/rejected': '0.013656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16822', 'logps_train/rejected': '-99.588', 'logps_train/chosen': '-135.25', 'loss/train': '0.63102', 'examples_per_second': '32.283', 'grad_norm': '19.125', 'counters/examples': 129088, 'counters/updates': 4034}
train stats after 129120 examples: {'rewards_train/chosen': '0.07102', 'rewards_train/rejected': '0.072651', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0016308', 'logps_train/rejected': '-146.06', 'logps_train/chosen': '-126.76', 'loss/train': '0.70633', 'examples_per_second': '32.643', 'grad_norm': '22.375', 'counters/examples': 129120, 'counters/updates': 4035}
train stats after 129152 examples: {'rewards_train/chosen': '0.11527', 'rewards_train/rejected': '0.044684', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070582', 'logps_train/rejected': '-93.567', 'logps_train/chosen': '-123.56', 'loss/train': '0.66419', 'examples_per_second': '31.483', 'grad_norm': '20.125', 'counters/examples': 129152, 'counters/updates': 4036}
train stats after 129184 examples: {'rewards_train/chosen': '0.16314', 'rewards_train/rejected': '0.053893', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10925', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-138.79', 'loss/train': '0.65245', 'examples_per_second': '31.254', 'grad_norm': '19.125', 'counters/examples': 129184, 'counters/updates': 4037}
skipping logging after 129216 examples to avoid logging too frequently
train stats after 129248 examples: {'rewards_train/chosen': '0.19023', 'rewards_train/rejected': '0.073943', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11628', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-151.48', 'loss/train': '0.64593', 'examples_per_second': '34.521', 'grad_norm': '20.75', 'counters/examples': 129248, 'counters/updates': 4039}
train stats after 129280 examples: {'rewards_train/chosen': '0.12465', 'rewards_train/rejected': '0.071857', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052791', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-120.32', 'loss/train': '0.67658', 'examples_per_second': '31.433', 'grad_norm': '18.75', 'counters/examples': 129280, 'counters/updates': 4040}
train stats after 129312 examples: {'rewards_train/chosen': '0.11403', 'rewards_train/rejected': '0.071398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042633', 'logps_train/rejected': '-120.5', 'logps_train/chosen': '-140.29', 'loss/train': '0.68333', 'examples_per_second': '32.608', 'grad_norm': '20.625', 'counters/examples': 129312, 'counters/updates': 4041}
train stats after 129344 examples: {'rewards_train/chosen': '0.17209', 'rewards_train/rejected': '0.049168', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12292', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-121.96', 'loss/train': '0.64608', 'examples_per_second': '30.028', 'grad_norm': '19.125', 'counters/examples': 129344, 'counters/updates': 4042}
train stats after 129376 examples: {'rewards_train/chosen': '0.14791', 'rewards_train/rejected': '0.019588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12833', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-109.61', 'loss/train': '0.63996', 'examples_per_second': '31.035', 'grad_norm': '17.5', 'counters/examples': 129376, 'counters/updates': 4043}
train stats after 129408 examples: {'rewards_train/chosen': '0.15793', 'rewards_train/rejected': '0.11753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040395', 'logps_train/rejected': '-126.98', 'logps_train/chosen': '-145.85', 'loss/train': '0.68202', 'examples_per_second': '32.658', 'grad_norm': '21.875', 'counters/examples': 129408, 'counters/updates': 4044}
train stats after 129440 examples: {'rewards_train/chosen': '0.15262', 'rewards_train/rejected': '0.061143', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091481', 'logps_train/rejected': '-166.62', 'logps_train/chosen': '-121.61', 'loss/train': '0.65734', 'examples_per_second': '31.513', 'grad_norm': '21.375', 'counters/examples': 129440, 'counters/updates': 4045}
train stats after 129472 examples: {'rewards_train/chosen': '0.11144', 'rewards_train/rejected': '0.065121', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046315', 'logps_train/rejected': '-118', 'logps_train/chosen': '-105.93', 'loss/train': '0.67673', 'examples_per_second': '31.349', 'grad_norm': '19.5', 'counters/examples': 129472, 'counters/updates': 4046}
train stats after 129504 examples: {'rewards_train/chosen': '0.15523', 'rewards_train/rejected': '0.082072', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073159', 'logps_train/rejected': '-157.18', 'logps_train/chosen': '-155.14', 'loss/train': '0.6688', 'examples_per_second': '31.336', 'grad_norm': '23.25', 'counters/examples': 129504, 'counters/updates': 4047}
train stats after 129536 examples: {'rewards_train/chosen': '0.10026', 'rewards_train/rejected': '0.04387', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056387', 'logps_train/rejected': '-107.82', 'logps_train/chosen': '-147.23', 'loss/train': '0.67333', 'examples_per_second': '31.437', 'grad_norm': '20.75', 'counters/examples': 129536, 'counters/updates': 4048}
train stats after 129568 examples: {'rewards_train/chosen': '0.098137', 'rewards_train/rejected': '0.13073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032597', 'logps_train/rejected': '-149.9', 'logps_train/chosen': '-116.92', 'loss/train': '0.71696', 'examples_per_second': '31.465', 'grad_norm': '22.625', 'counters/examples': 129568, 'counters/updates': 4049}
train stats after 129600 examples: {'rewards_train/chosen': '0.19808', 'rewards_train/rejected': '0.14219', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055891', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-177.45', 'loss/train': '0.67435', 'examples_per_second': '31.418', 'grad_norm': '23.75', 'counters/examples': 129600, 'counters/updates': 4050}
train stats after 129632 examples: {'rewards_train/chosen': '0.18042', 'rewards_train/rejected': '0.090605', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089814', 'logps_train/rejected': '-152.44', 'logps_train/chosen': '-153.01', 'loss/train': '0.66544', 'examples_per_second': '32.887', 'grad_norm': '21.875', 'counters/examples': 129632, 'counters/updates': 4051}
train stats after 129664 examples: {'rewards_train/chosen': '0.17144', 'rewards_train/rejected': '0.069529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10191', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-131.78', 'loss/train': '0.65218', 'examples_per_second': '30.649', 'grad_norm': '19.375', 'counters/examples': 129664, 'counters/updates': 4052}
train stats after 129696 examples: {'rewards_train/chosen': '0.15061', 'rewards_train/rejected': '0.080262', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070351', 'logps_train/rejected': '-111.85', 'logps_train/chosen': '-128.87', 'loss/train': '0.66638', 'examples_per_second': '31.32', 'grad_norm': '20', 'counters/examples': 129696, 'counters/updates': 4053}
skipping logging after 129728 examples to avoid logging too frequently
train stats after 129760 examples: {'rewards_train/chosen': '0.14449', 'rewards_train/rejected': '0.05963', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084859', 'logps_train/rejected': '-97.126', 'logps_train/chosen': '-138.38', 'loss/train': '0.66189', 'examples_per_second': '31.674', 'grad_norm': '20.125', 'counters/examples': 129760, 'counters/updates': 4055}
train stats after 129792 examples: {'rewards_train/chosen': '0.090359', 'rewards_train/rejected': '0.025284', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065075', 'logps_train/rejected': '-162.72', 'logps_train/chosen': '-148.75', 'loss/train': '0.66894', 'examples_per_second': '31.461', 'grad_norm': '24.25', 'counters/examples': 129792, 'counters/updates': 4056}
train stats after 129824 examples: {'rewards_train/chosen': '0.20295', 'rewards_train/rejected': '0.18687', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016088', 'logps_train/rejected': '-146.95', 'logps_train/chosen': '-138', 'loss/train': '0.69546', 'examples_per_second': '31.44', 'grad_norm': '22.25', 'counters/examples': 129824, 'counters/updates': 4057}
skipping logging after 129856 examples to avoid logging too frequently
train stats after 129888 examples: {'rewards_train/chosen': '0.078929', 'rewards_train/rejected': '0.083189', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0042596', 'logps_train/rejected': '-134.39', 'logps_train/chosen': '-138', 'loss/train': '0.70696', 'examples_per_second': '33.755', 'grad_norm': '23.375', 'counters/examples': 129888, 'counters/updates': 4059}
skipping logging after 129920 examples to avoid logging too frequently
train stats after 129952 examples: {'rewards_train/chosen': '0.15982', 'rewards_train/rejected': '0.069114', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090708', 'logps_train/rejected': '-106.63', 'logps_train/chosen': '-114.06', 'loss/train': '0.65678', 'examples_per_second': '30.789', 'grad_norm': '18.375', 'counters/examples': 129952, 'counters/updates': 4061}
train stats after 129984 examples: {'rewards_train/chosen': '0.034165', 'rewards_train/rejected': '0.028314', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0058517', 'logps_train/rejected': '-83.675', 'logps_train/chosen': '-129.45', 'loss/train': '0.69489', 'examples_per_second': '32.082', 'grad_norm': '18.625', 'counters/examples': 129984, 'counters/updates': 4062}
train stats after 130016 examples: {'rewards_train/chosen': '0.18426', 'rewards_train/rejected': '0.045487', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13877', 'logps_train/rejected': '-151.06', 'logps_train/chosen': '-123.87', 'loss/train': '0.63386', 'examples_per_second': '31.518', 'grad_norm': '20.875', 'counters/examples': 130016, 'counters/updates': 4063}
train stats after 130048 examples: {'rewards_train/chosen': '0.1499', 'rewards_train/rejected': '0.04519', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10471', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-103.83', 'loss/train': '0.65291', 'examples_per_second': '23.533', 'grad_norm': '19.875', 'counters/examples': 130048, 'counters/updates': 4064}
train stats after 130080 examples: {'rewards_train/chosen': '0.10519', 'rewards_train/rejected': '0.080435', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024756', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-116.33', 'loss/train': '0.6889', 'examples_per_second': '30.065', 'grad_norm': '19.5', 'counters/examples': 130080, 'counters/updates': 4065}
skipping logging after 130112 examples to avoid logging too frequently
train stats after 130144 examples: {'rewards_train/chosen': '0.088581', 'rewards_train/rejected': '0.068198', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020382', 'logps_train/rejected': '-96.899', 'logps_train/chosen': '-99.405', 'loss/train': '0.69262', 'examples_per_second': '29.457', 'grad_norm': '18', 'counters/examples': 130144, 'counters/updates': 4067}
skipping logging after 130176 examples to avoid logging too frequently
train stats after 130208 examples: {'rewards_train/chosen': '0.1355', 'rewards_train/rejected': '0.013992', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12151', 'logps_train/rejected': '-100.46', 'logps_train/chosen': '-150.7', 'loss/train': '0.64833', 'examples_per_second': '32.068', 'grad_norm': '18.75', 'counters/examples': 130208, 'counters/updates': 4069}
train stats after 130240 examples: {'rewards_train/chosen': '0.069897', 'rewards_train/rejected': '-0.027325', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097223', 'logps_train/rejected': '-86.948', 'logps_train/chosen': '-134.77', 'loss/train': '0.65996', 'examples_per_second': '32.382', 'grad_norm': '19.375', 'counters/examples': 130240, 'counters/updates': 4070}
skipping logging after 130272 examples to avoid logging too frequently
train stats after 130304 examples: {'rewards_train/chosen': '0.12963', 'rewards_train/rejected': '0.070985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058641', 'logps_train/rejected': '-96.112', 'logps_train/chosen': '-125.93', 'loss/train': '0.67151', 'examples_per_second': '31.444', 'grad_norm': '18.75', 'counters/examples': 130304, 'counters/updates': 4072}
train stats after 130336 examples: {'rewards_train/chosen': '0.26097', 'rewards_train/rejected': '0.0012211', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25975', 'logps_train/rejected': '-144.5', 'logps_train/chosen': '-179.57', 'loss/train': '0.58622', 'examples_per_second': '31.53', 'grad_norm': '21.375', 'counters/examples': 130336, 'counters/updates': 4073}
train stats after 130368 examples: {'rewards_train/chosen': '0.15168', 'rewards_train/rejected': '0.10326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048427', 'logps_train/rejected': '-104.2', 'logps_train/chosen': '-90.316', 'loss/train': '0.67844', 'examples_per_second': '30.587', 'grad_norm': '17.375', 'counters/examples': 130368, 'counters/updates': 4074}
train stats after 130400 examples: {'rewards_train/chosen': '0.046207', 'rewards_train/rejected': '-0.026774', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072981', 'logps_train/rejected': '-92.552', 'logps_train/chosen': '-99.767', 'loss/train': '0.66342', 'examples_per_second': '32.779', 'grad_norm': '17.875', 'counters/examples': 130400, 'counters/updates': 4075}
train stats after 130432 examples: {'rewards_train/chosen': '0.077287', 'rewards_train/rejected': '0.017771', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059516', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-107.49', 'loss/train': '0.68523', 'examples_per_second': '32.584', 'grad_norm': '20.25', 'counters/examples': 130432, 'counters/updates': 4076}
skipping logging after 130464 examples to avoid logging too frequently
train stats after 130496 examples: {'rewards_train/chosen': '0.106', 'rewards_train/rejected': '0.075756', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030242', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-149.92', 'loss/train': '0.6845', 'examples_per_second': '30.981', 'grad_norm': '22', 'counters/examples': 130496, 'counters/updates': 4078}
train stats after 130528 examples: {'rewards_train/chosen': '0.12904', 'rewards_train/rejected': '-0.00099856', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13004', 'logps_train/rejected': '-107.72', 'logps_train/chosen': '-157.87', 'loss/train': '0.63612', 'examples_per_second': '30.795', 'grad_norm': '19', 'counters/examples': 130528, 'counters/updates': 4079}
train stats after 130560 examples: {'rewards_train/chosen': '0.22156', 'rewards_train/rejected': '0.089019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13254', 'logps_train/rejected': '-127.95', 'logps_train/chosen': '-169.23', 'loss/train': '0.6453', 'examples_per_second': '31.548', 'grad_norm': '20.75', 'counters/examples': 130560, 'counters/updates': 4080}
train stats after 130592 examples: {'rewards_train/chosen': '0.094195', 'rewards_train/rejected': '0.025773', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068422', 'logps_train/rejected': '-93.054', 'logps_train/chosen': '-96.054', 'loss/train': '0.66886', 'examples_per_second': '30.311', 'grad_norm': '17.625', 'counters/examples': 130592, 'counters/updates': 4081}
train stats after 130624 examples: {'rewards_train/chosen': '0.20304', 'rewards_train/rejected': '0.076703', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12634', 'logps_train/rejected': '-104.05', 'logps_train/chosen': '-136.34', 'loss/train': '0.64456', 'examples_per_second': '32.759', 'grad_norm': '19.125', 'counters/examples': 130624, 'counters/updates': 4082}
skipping logging after 130656 examples to avoid logging too frequently
train stats after 130688 examples: {'rewards_train/chosen': '0.15119', 'rewards_train/rejected': '0.11555', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.035647', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-135.62', 'loss/train': '0.68153', 'examples_per_second': '32.707', 'grad_norm': '22.375', 'counters/examples': 130688, 'counters/updates': 4084}
train stats after 130720 examples: {'rewards_train/chosen': '0.12538', 'rewards_train/rejected': '0.089698', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.03568', 'logps_train/rejected': '-134.47', 'logps_train/chosen': '-174.33', 'loss/train': '0.686', 'examples_per_second': '31.6', 'grad_norm': '22.875', 'counters/examples': 130720, 'counters/updates': 4085}
train stats after 130752 examples: {'rewards_train/chosen': '0.26499', 'rewards_train/rejected': '0.026007', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23899', 'logps_train/rejected': '-104.51', 'logps_train/chosen': '-130.14', 'loss/train': '0.59644', 'examples_per_second': '31.087', 'grad_norm': '17.5', 'counters/examples': 130752, 'counters/updates': 4086}
train stats after 130784 examples: {'rewards_train/chosen': '0.18837', 'rewards_train/rejected': '0.060561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12781', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-126.99', 'loss/train': '0.63868', 'examples_per_second': '31.721', 'grad_norm': '20.125', 'counters/examples': 130784, 'counters/updates': 4087}
train stats after 130816 examples: {'rewards_train/chosen': '0.13933', 'rewards_train/rejected': '0.048657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090669', 'logps_train/rejected': '-113.38', 'logps_train/chosen': '-122.07', 'loss/train': '0.66653', 'examples_per_second': '30.97', 'grad_norm': '19.625', 'counters/examples': 130816, 'counters/updates': 4088}
skipping logging after 130848 examples to avoid logging too frequently
train stats after 130880 examples: {'rewards_train/chosen': '0.13465', 'rewards_train/rejected': '0.13166', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0029978', 'logps_train/rejected': '-127.5', 'logps_train/chosen': '-136.4', 'loss/train': '0.70587', 'examples_per_second': '30.113', 'grad_norm': '21.25', 'counters/examples': 130880, 'counters/updates': 4090}
train stats after 130912 examples: {'rewards_train/chosen': '0.12879', 'rewards_train/rejected': '0.067197', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061591', 'logps_train/rejected': '-116.13', 'logps_train/chosen': '-156.61', 'loss/train': '0.67338', 'examples_per_second': '31.65', 'grad_norm': '21.375', 'counters/examples': 130912, 'counters/updates': 4091}
skipping logging after 130944 examples to avoid logging too frequently
train stats after 130976 examples: {'rewards_train/chosen': '0.17109', 'rewards_train/rejected': '0.027866', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14322', 'logps_train/rejected': '-96.291', 'logps_train/chosen': '-128.34', 'loss/train': '0.63805', 'examples_per_second': '30.695', 'grad_norm': '17.375', 'counters/examples': 130976, 'counters/updates': 4093}
train stats after 131008 examples: {'rewards_train/chosen': '0.12009', 'rewards_train/rejected': '0.087669', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.032423', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-140.33', 'loss/train': '0.68731', 'examples_per_second': '30.079', 'grad_norm': '21.25', 'counters/examples': 131008, 'counters/updates': 4094}
train stats after 131040 examples: {'rewards_train/chosen': '0.18496', 'rewards_train/rejected': '0.058465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12649', 'logps_train/rejected': '-175.93', 'logps_train/chosen': '-196.14', 'loss/train': '0.64097', 'examples_per_second': '31.643', 'grad_norm': '23.875', 'counters/examples': 131040, 'counters/updates': 4095}
train stats after 131072 examples: {'rewards_train/chosen': '0.12305', 'rewards_train/rejected': '0.0063874', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11667', 'logps_train/rejected': '-142.27', 'logps_train/chosen': '-130.05', 'loss/train': '0.64658', 'examples_per_second': '31.232', 'grad_norm': '19.625', 'counters/examples': 131072, 'counters/updates': 4096}
skipping logging after 131104 examples to avoid logging too frequently
train stats after 131136 examples: {'rewards_train/chosen': '0.18509', 'rewards_train/rejected': '0.080448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10465', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-150.83', 'loss/train': '0.65696', 'examples_per_second': '30.684', 'grad_norm': '20.75', 'counters/examples': 131136, 'counters/updates': 4098}
train stats after 131168 examples: {'rewards_train/chosen': '0.11777', 'rewards_train/rejected': '0.16218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044406', 'logps_train/rejected': '-135.13', 'logps_train/chosen': '-147.75', 'loss/train': '0.72637', 'examples_per_second': '31.795', 'grad_norm': '21.5', 'counters/examples': 131168, 'counters/updates': 4099}
train stats after 131200 examples: {'rewards_train/chosen': '0.13891', 'rewards_train/rejected': '0.028767', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11015', 'logps_train/rejected': '-105.95', 'logps_train/chosen': '-153.06', 'loss/train': '0.64968', 'examples_per_second': '31.32', 'grad_norm': '21.75', 'counters/examples': 131200, 'counters/updates': 4100}
train stats after 131232 examples: {'rewards_train/chosen': '0.063583', 'rewards_train/rejected': '0.050783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012799', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-139.34', 'loss/train': '0.69857', 'examples_per_second': '30.898', 'grad_norm': '22', 'counters/examples': 131232, 'counters/updates': 4101}
train stats after 131264 examples: {'rewards_train/chosen': '0.16789', 'rewards_train/rejected': '0.047342', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12055', 'logps_train/rejected': '-139.99', 'logps_train/chosen': '-161.33', 'loss/train': '0.6445', 'examples_per_second': '30.183', 'grad_norm': '21.375', 'counters/examples': 131264, 'counters/updates': 4102}
skipping logging after 131296 examples to avoid logging too frequently
train stats after 131328 examples: {'rewards_train/chosen': '0.14664', 'rewards_train/rejected': '0.1159', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030735', 'logps_train/rejected': '-166.77', 'logps_train/chosen': '-135.48', 'loss/train': '0.68512', 'examples_per_second': '31.585', 'grad_norm': '23', 'counters/examples': 131328, 'counters/updates': 4104}
train stats after 131360 examples: {'rewards_train/chosen': '0.11453', 'rewards_train/rejected': '0.056638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057897', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-108.71', 'loss/train': '0.67413', 'examples_per_second': '30.14', 'grad_norm': '18.75', 'counters/examples': 131360, 'counters/updates': 4105}
train stats after 131392 examples: {'rewards_train/chosen': '0.23397', 'rewards_train/rejected': '0.08841', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14556', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-170.32', 'loss/train': '0.63131', 'examples_per_second': '30.713', 'grad_norm': '21.875', 'counters/examples': 131392, 'counters/updates': 4106}
train stats after 131424 examples: {'rewards_train/chosen': '0.18431', 'rewards_train/rejected': '0.044318', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13999', 'logps_train/rejected': '-105.02', 'logps_train/chosen': '-131.23', 'loss/train': '0.6357', 'examples_per_second': '31.615', 'grad_norm': '18.25', 'counters/examples': 131424, 'counters/updates': 4107}
skipping logging after 131456 examples to avoid logging too frequently
train stats after 131488 examples: {'rewards_train/chosen': '0.19481', 'rewards_train/rejected': '0.011658', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18316', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-174.8', 'loss/train': '0.61298', 'examples_per_second': '31.494', 'grad_norm': '20.625', 'counters/examples': 131488, 'counters/updates': 4109}
skipping logging after 131520 examples to avoid logging too frequently
train stats after 131552 examples: {'rewards_train/chosen': '0.053798', 'rewards_train/rejected': '0.045591', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0082071', 'logps_train/rejected': '-153.57', 'logps_train/chosen': '-145.26', 'loss/train': '0.70238', 'examples_per_second': '33.092', 'grad_norm': '24.25', 'counters/examples': 131552, 'counters/updates': 4111}
train stats after 131584 examples: {'rewards_train/chosen': '0.1725', 'rewards_train/rejected': '0.071014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10149', 'logps_train/rejected': '-113.95', 'logps_train/chosen': '-115.27', 'loss/train': '0.65073', 'examples_per_second': '30.888', 'grad_norm': '18.375', 'counters/examples': 131584, 'counters/updates': 4112}
skipping logging after 131616 examples to avoid logging too frequently
train stats after 131648 examples: {'rewards_train/chosen': '0.10412', 'rewards_train/rejected': '0.016727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087395', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-104.77', 'loss/train': '0.66394', 'examples_per_second': '31.831', 'grad_norm': '19', 'counters/examples': 131648, 'counters/updates': 4114}
train stats after 131680 examples: {'rewards_train/chosen': '0.052632', 'rewards_train/rejected': '0.029781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022852', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-127.26', 'loss/train': '0.69492', 'examples_per_second': '31.046', 'grad_norm': '21.875', 'counters/examples': 131680, 'counters/updates': 4115}
train stats after 131712 examples: {'rewards_train/chosen': '0.14137', 'rewards_train/rejected': '0.10878', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032594', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-142.11', 'loss/train': '0.6853', 'examples_per_second': '32.678', 'grad_norm': '21.75', 'counters/examples': 131712, 'counters/updates': 4116}
train stats after 131744 examples: {'rewards_train/chosen': '0.095359', 'rewards_train/rejected': '0.030569', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06479', 'logps_train/rejected': '-83.002', 'logps_train/chosen': '-120.5', 'loss/train': '0.66624', 'examples_per_second': '30.474', 'grad_norm': '17.75', 'counters/examples': 131744, 'counters/updates': 4117}
skipping logging after 131776 examples to avoid logging too frequently
train stats after 131808 examples: {'rewards_train/chosen': '0.12959', 'rewards_train/rejected': '0.052929', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076663', 'logps_train/rejected': '-122.39', 'logps_train/chosen': '-129.71', 'loss/train': '0.66322', 'examples_per_second': '30.228', 'grad_norm': '21.25', 'counters/examples': 131808, 'counters/updates': 4119}
skipping logging after 131840 examples to avoid logging too frequently
train stats after 131872 examples: {'rewards_train/chosen': '0.21748', 'rewards_train/rejected': '0.093409', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12407', 'logps_train/rejected': '-125.36', 'logps_train/chosen': '-125.83', 'loss/train': '0.64366', 'examples_per_second': '38.404', 'grad_norm': '20.125', 'counters/examples': 131872, 'counters/updates': 4121}
train stats after 131904 examples: {'rewards_train/chosen': '0.11986', 'rewards_train/rejected': '0.069953', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049904', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-140.75', 'loss/train': '0.6798', 'examples_per_second': '32.378', 'grad_norm': '22.125', 'counters/examples': 131904, 'counters/updates': 4122}
skipping logging after 131936 examples to avoid logging too frequently
train stats after 131968 examples: {'rewards_train/chosen': '0.17102', 'rewards_train/rejected': '0.042885', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12814', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-131.02', 'loss/train': '0.64272', 'examples_per_second': '32.832', 'grad_norm': '19.625', 'counters/examples': 131968, 'counters/updates': 4124}
skipping logging after 132000 examples to avoid logging too frequently
Running evaluation after 132000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.91it/s]
eval after 132000: {'rewards_eval/chosen': '0.15311', 'rewards_eval/rejected': '0.051541', 'rewards_eval/accuracies': '0.60938', 'rewards_eval/margins': '0.10157', 'logps_eval/rejected': '-114.88', 'logps_eval/chosen': '-134.02', 'loss/eval': '0.65549'}
skipping save for non epoch
train stats after 132032 examples: {'rewards_train/chosen': '0.091851', 'rewards_train/rejected': '0.057543', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034307', 'logps_train/rejected': '-120.71', 'logps_train/chosen': '-153.26', 'loss/train': '0.68562', 'examples_per_second': '30.797', 'grad_norm': '21.75', 'counters/examples': 132032, 'counters/updates': 4126}
train stats after 132064 examples: {'rewards_train/chosen': '0.10548', 'rewards_train/rejected': '0.081062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024417', 'logps_train/rejected': '-109.51', 'logps_train/chosen': '-124.68', 'loss/train': '0.69478', 'examples_per_second': '30.748', 'grad_norm': '20', 'counters/examples': 132064, 'counters/updates': 4127}
skipping logging after 132096 examples to avoid logging too frequently
train stats after 132128 examples: {'rewards_train/chosen': '0.094301', 'rewards_train/rejected': '0.057866', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036435', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-88.072', 'loss/train': '0.68171', 'examples_per_second': '30.735', 'grad_norm': '19', 'counters/examples': 132128, 'counters/updates': 4129}
train stats after 132160 examples: {'rewards_train/chosen': '0.090036', 'rewards_train/rejected': '0.064586', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02545', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-129.22', 'loss/train': '0.6858', 'examples_per_second': '31.68', 'grad_norm': '20.625', 'counters/examples': 132160, 'counters/updates': 4130}
train stats after 132192 examples: {'rewards_train/chosen': '0.11059', 'rewards_train/rejected': '0.087144', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02345', 'logps_train/rejected': '-130.56', 'logps_train/chosen': '-161.6', 'loss/train': '0.69076', 'examples_per_second': '31.177', 'grad_norm': '22.125', 'counters/examples': 132192, 'counters/updates': 4131}
train stats after 132224 examples: {'rewards_train/chosen': '0.12459', 'rewards_train/rejected': '0.039794', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084799', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-134.19', 'loss/train': '0.65782', 'examples_per_second': '31.688', 'grad_norm': '19.625', 'counters/examples': 132224, 'counters/updates': 4132}
train stats after 132256 examples: {'rewards_train/chosen': '0.11252', 'rewards_train/rejected': '0.11024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0022835', 'logps_train/rejected': '-132.15', 'logps_train/chosen': '-143.16', 'loss/train': '0.70087', 'examples_per_second': '32.553', 'grad_norm': '21.375', 'counters/examples': 132256, 'counters/updates': 4133}
train stats after 132288 examples: {'rewards_train/chosen': '0.21976', 'rewards_train/rejected': '0.050276', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16948', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-160.59', 'loss/train': '0.62247', 'examples_per_second': '30.118', 'grad_norm': '19.875', 'counters/examples': 132288, 'counters/updates': 4134}
train stats after 132320 examples: {'rewards_train/chosen': '0.17789', 'rewards_train/rejected': '0.0048631', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17303', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-141.58', 'loss/train': '0.62485', 'examples_per_second': '32.804', 'grad_norm': '20.125', 'counters/examples': 132320, 'counters/updates': 4135}
train stats after 132352 examples: {'rewards_train/chosen': '0.20116', 'rewards_train/rejected': '0.027737', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17342', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-166.86', 'loss/train': '0.62277', 'examples_per_second': '32.198', 'grad_norm': '19.875', 'counters/examples': 132352, 'counters/updates': 4136}
train stats after 132384 examples: {'rewards_train/chosen': '0.093191', 'rewards_train/rejected': '0.044453', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048738', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-113.12', 'loss/train': '0.68069', 'examples_per_second': '31.702', 'grad_norm': '19.75', 'counters/examples': 132384, 'counters/updates': 4137}
train stats after 132416 examples: {'rewards_train/chosen': '0.17496', 'rewards_train/rejected': '0.13102', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043936', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-185.01', 'loss/train': '0.69154', 'examples_per_second': '31.708', 'grad_norm': '23.25', 'counters/examples': 132416, 'counters/updates': 4138}
train stats after 132448 examples: {'rewards_train/chosen': '0.065827', 'rewards_train/rejected': '0.035182', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030645', 'logps_train/rejected': '-102.62', 'logps_train/chosen': '-160.46', 'loss/train': '0.68491', 'examples_per_second': '30.68', 'grad_norm': '21', 'counters/examples': 132448, 'counters/updates': 4139}
train stats after 132480 examples: {'rewards_train/chosen': '0.15215', 'rewards_train/rejected': '0.038134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11402', 'logps_train/rejected': '-111.08', 'logps_train/chosen': '-143.7', 'loss/train': '0.65001', 'examples_per_second': '31.637', 'grad_norm': '19.875', 'counters/examples': 132480, 'counters/updates': 4140}
train stats after 132512 examples: {'rewards_train/chosen': '0.13198', 'rewards_train/rejected': '0.013503', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11848', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-135.47', 'loss/train': '0.64496', 'examples_per_second': '23.81', 'grad_norm': '19.375', 'counters/examples': 132512, 'counters/updates': 4141}
train stats after 132544 examples: {'rewards_train/chosen': '0.17412', 'rewards_train/rejected': '0.10003', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074089', 'logps_train/rejected': '-156.99', 'logps_train/chosen': '-157.43', 'loss/train': '0.67101', 'examples_per_second': '31.082', 'grad_norm': '21.125', 'counters/examples': 132544, 'counters/updates': 4142}
train stats after 132576 examples: {'rewards_train/chosen': '0.12848', 'rewards_train/rejected': '0.075288', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053191', 'logps_train/rejected': '-107.81', 'logps_train/chosen': '-106.42', 'loss/train': '0.67356', 'examples_per_second': '30.559', 'grad_norm': '18.625', 'counters/examples': 132576, 'counters/updates': 4143}
train stats after 132608 examples: {'rewards_train/chosen': '0.063741', 'rewards_train/rejected': '-0.018516', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082257', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-123.43', 'loss/train': '0.66532', 'examples_per_second': '30.989', 'grad_norm': '19.25', 'counters/examples': 132608, 'counters/updates': 4144}
skipping logging after 132640 examples to avoid logging too frequently
train stats after 132672 examples: {'rewards_train/chosen': '0.15655', 'rewards_train/rejected': '0.03881', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11774', 'logps_train/rejected': '-107.04', 'logps_train/chosen': '-153.01', 'loss/train': '0.64793', 'examples_per_second': '31.647', 'grad_norm': '20.625', 'counters/examples': 132672, 'counters/updates': 4146}
train stats after 132704 examples: {'rewards_train/chosen': '0.13022', 'rewards_train/rejected': '0.051787', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078428', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-121.22', 'loss/train': '0.6696', 'examples_per_second': '30.981', 'grad_norm': '19.875', 'counters/examples': 132704, 'counters/updates': 4147}
train stats after 132736 examples: {'rewards_train/chosen': '0.15061', 'rewards_train/rejected': '-0.017371', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16798', 'logps_train/rejected': '-80.576', 'logps_train/chosen': '-136.17', 'loss/train': '0.62393', 'examples_per_second': '29.907', 'grad_norm': '18.25', 'counters/examples': 132736, 'counters/updates': 4148}
train stats after 132768 examples: {'rewards_train/chosen': '0.095247', 'rewards_train/rejected': '0.13868', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.043433', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-142.52', 'loss/train': '0.72778', 'examples_per_second': '30.042', 'grad_norm': '23.5', 'counters/examples': 132768, 'counters/updates': 4149}
train stats after 132800 examples: {'rewards_train/chosen': '0.12505', 'rewards_train/rejected': '0.05504', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070013', 'logps_train/rejected': '-106.65', 'logps_train/chosen': '-134.74', 'loss/train': '0.67189', 'examples_per_second': '31.619', 'grad_norm': '19.25', 'counters/examples': 132800, 'counters/updates': 4150}
train stats after 132832 examples: {'rewards_train/chosen': '0.14763', 'rewards_train/rejected': '0.088298', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059329', 'logps_train/rejected': '-153.85', 'logps_train/chosen': '-183.06', 'loss/train': '0.6727', 'examples_per_second': '32.016', 'grad_norm': '23.125', 'counters/examples': 132832, 'counters/updates': 4151}
train stats after 132864 examples: {'rewards_train/chosen': '0.10536', 'rewards_train/rejected': '0.041636', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063728', 'logps_train/rejected': '-129.37', 'logps_train/chosen': '-145.27', 'loss/train': '0.67202', 'examples_per_second': '31.876', 'grad_norm': '19.75', 'counters/examples': 132864, 'counters/updates': 4152}
train stats after 132896 examples: {'rewards_train/chosen': '0.11607', 'rewards_train/rejected': '0.11406', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0020073', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-144.9', 'loss/train': '0.70368', 'examples_per_second': '31.5', 'grad_norm': '21.5', 'counters/examples': 132896, 'counters/updates': 4153}
train stats after 132928 examples: {'rewards_train/chosen': '0.15358', 'rewards_train/rejected': '0.097423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056154', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-132.66', 'loss/train': '0.67687', 'examples_per_second': '30.203', 'grad_norm': '21', 'counters/examples': 132928, 'counters/updates': 4154}
train stats after 132960 examples: {'rewards_train/chosen': '0.11964', 'rewards_train/rejected': '0.056566', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063072', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-142.16', 'loss/train': '0.66855', 'examples_per_second': '30.134', 'grad_norm': '20.5', 'counters/examples': 132960, 'counters/updates': 4155}
train stats after 132992 examples: {'rewards_train/chosen': '0.12056', 'rewards_train/rejected': '0.073249', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047311', 'logps_train/rejected': '-138.3', 'logps_train/chosen': '-159.78', 'loss/train': '0.67853', 'examples_per_second': '31.858', 'grad_norm': '21.125', 'counters/examples': 132992, 'counters/updates': 4156}
train stats after 133024 examples: {'rewards_train/chosen': '0.16054', 'rewards_train/rejected': '0.020932', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13961', 'logps_train/rejected': '-140.5', 'logps_train/chosen': '-149.15', 'loss/train': '0.64004', 'examples_per_second': '31.158', 'grad_norm': '21.25', 'counters/examples': 133024, 'counters/updates': 4157}
train stats after 133056 examples: {'rewards_train/chosen': '0.17703', 'rewards_train/rejected': '0.024212', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15282', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-150.66', 'loss/train': '0.62686', 'examples_per_second': '31.757', 'grad_norm': '19', 'counters/examples': 133056, 'counters/updates': 4158}
skipping logging after 133088 examples to avoid logging too frequently
train stats after 133120 examples: {'rewards_train/chosen': '0.14745', 'rewards_train/rejected': '0.0455', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10195', 'logps_train/rejected': '-153.3', 'logps_train/chosen': '-146.08', 'loss/train': '0.66055', 'examples_per_second': '31.566', 'grad_norm': '24', 'counters/examples': 133120, 'counters/updates': 4160}
train stats after 133152 examples: {'rewards_train/chosen': '0.12132', 'rewards_train/rejected': '0.038778', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.082545', 'logps_train/rejected': '-97.196', 'logps_train/chosen': '-97.499', 'loss/train': '0.65815', 'examples_per_second': '31.458', 'grad_norm': '18.375', 'counters/examples': 133152, 'counters/updates': 4161}
train stats after 133184 examples: {'rewards_train/chosen': '0.17988', 'rewards_train/rejected': '0.085156', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.094719', 'logps_train/rejected': '-121', 'logps_train/chosen': '-114.09', 'loss/train': '0.65874', 'examples_per_second': '31.658', 'grad_norm': '20.5', 'counters/examples': 133184, 'counters/updates': 4162}
skipping logging after 133216 examples to avoid logging too frequently
train stats after 133248 examples: {'rewards_train/chosen': '0.23022', 'rewards_train/rejected': '0.04181', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18841', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-153.16', 'loss/train': '0.61454', 'examples_per_second': '31.639', 'grad_norm': '19.375', 'counters/examples': 133248, 'counters/updates': 4164}
skipping logging after 133280 examples to avoid logging too frequently
train stats after 133312 examples: {'rewards_train/chosen': '0.098965', 'rewards_train/rejected': '-0.040418', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13938', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-143.23', 'loss/train': '0.63289', 'examples_per_second': '31.607', 'grad_norm': '19.75', 'counters/examples': 133312, 'counters/updates': 4166}
train stats after 133344 examples: {'rewards_train/chosen': '0.18669', 'rewards_train/rejected': '0.059044', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12765', 'logps_train/rejected': '-140.9', 'logps_train/chosen': '-186.57', 'loss/train': '0.64079', 'examples_per_second': '30.191', 'grad_norm': '22.625', 'counters/examples': 133344, 'counters/updates': 4167}
train stats after 133376 examples: {'rewards_train/chosen': '0.12969', 'rewards_train/rejected': '0.019554', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11013', 'logps_train/rejected': '-104.47', 'logps_train/chosen': '-120.98', 'loss/train': '0.64574', 'examples_per_second': '31.559', 'grad_norm': '18.75', 'counters/examples': 133376, 'counters/updates': 4168}
train stats after 133408 examples: {'rewards_train/chosen': '0.10094', 'rewards_train/rejected': '0.019296', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081644', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-154.9', 'loss/train': '0.66226', 'examples_per_second': '31.685', 'grad_norm': '20.5', 'counters/examples': 133408, 'counters/updates': 4169}
skipping logging after 133440 examples to avoid logging too frequently
train stats after 133472 examples: {'rewards_train/chosen': '0.11157', 'rewards_train/rejected': '0.020905', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090663', 'logps_train/rejected': '-80.712', 'logps_train/chosen': '-106.68', 'loss/train': '0.65305', 'examples_per_second': '31.583', 'grad_norm': '16.875', 'counters/examples': 133472, 'counters/updates': 4171}
train stats after 133504 examples: {'rewards_train/chosen': '0.16607', 'rewards_train/rejected': '0.080607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085459', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-125.63', 'loss/train': '0.66095', 'examples_per_second': '30.272', 'grad_norm': '20.875', 'counters/examples': 133504, 'counters/updates': 4172}
train stats after 133536 examples: {'rewards_train/chosen': '0.075607', 'rewards_train/rejected': '0.028075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047533', 'logps_train/rejected': '-137.34', 'logps_train/chosen': '-132.81', 'loss/train': '0.67676', 'examples_per_second': '31.657', 'grad_norm': '21.5', 'counters/examples': 133536, 'counters/updates': 4173}
train stats after 133568 examples: {'rewards_train/chosen': '0.15862', 'rewards_train/rejected': '0.069316', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089307', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-135.24', 'loss/train': '0.65581', 'examples_per_second': '32.021', 'grad_norm': '20.5', 'counters/examples': 133568, 'counters/updates': 4174}
train stats after 133600 examples: {'rewards_train/chosen': '0.20271', 'rewards_train/rejected': '0.11313', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089584', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-172.05', 'loss/train': '0.65769', 'examples_per_second': '30.735', 'grad_norm': '22.25', 'counters/examples': 133600, 'counters/updates': 4175}
train stats after 133632 examples: {'rewards_train/chosen': '0.2363', 'rewards_train/rejected': '0.036328', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19997', 'logps_train/rejected': '-111.49', 'logps_train/chosen': '-136.57', 'loss/train': '0.60871', 'examples_per_second': '31.644', 'grad_norm': '18.5', 'counters/examples': 133632, 'counters/updates': 4176}
train stats after 133664 examples: {'rewards_train/chosen': '0.11045', 'rewards_train/rejected': '0.0090535', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1014', 'logps_train/rejected': '-87.971', 'logps_train/chosen': '-119.47', 'loss/train': '0.65184', 'examples_per_second': '30.718', 'grad_norm': '17.625', 'counters/examples': 133664, 'counters/updates': 4177}
train stats after 133696 examples: {'rewards_train/chosen': '0.13649', 'rewards_train/rejected': '0.047838', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088655', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-173.42', 'loss/train': '0.65861', 'examples_per_second': '33.279', 'grad_norm': '21.625', 'counters/examples': 133696, 'counters/updates': 4178}
train stats after 133728 examples: {'rewards_train/chosen': '0.070604', 'rewards_train/rejected': '0.057452', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013152', 'logps_train/rejected': '-109.39', 'logps_train/chosen': '-114.16', 'loss/train': '0.69902', 'examples_per_second': '31.656', 'grad_norm': '20', 'counters/examples': 133728, 'counters/updates': 4179}
train stats after 133760 examples: {'rewards_train/chosen': '0.12417', 'rewards_train/rejected': '0.077785', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046388', 'logps_train/rejected': '-92.465', 'logps_train/chosen': '-158.59', 'loss/train': '0.68309', 'examples_per_second': '32.57', 'grad_norm': '24.625', 'counters/examples': 133760, 'counters/updates': 4180}
train stats after 133792 examples: {'rewards_train/chosen': '0.13627', 'rewards_train/rejected': '0.0064481', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12983', 'logps_train/rejected': '-101.93', 'logps_train/chosen': '-125.59', 'loss/train': '0.63732', 'examples_per_second': '32.801', 'grad_norm': '18.75', 'counters/examples': 133792, 'counters/updates': 4181}
train stats after 133824 examples: {'rewards_train/chosen': '0.10821', 'rewards_train/rejected': '0.02818', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080026', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-127.45', 'loss/train': '0.67352', 'examples_per_second': '32.181', 'grad_norm': '20.375', 'counters/examples': 133824, 'counters/updates': 4182}
train stats after 133856 examples: {'rewards_train/chosen': '0.064804', 'rewards_train/rejected': '0.032058', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032746', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-127.09', 'loss/train': '0.68618', 'examples_per_second': '31.386', 'grad_norm': '21.125', 'counters/examples': 133856, 'counters/updates': 4183}
train stats after 133888 examples: {'rewards_train/chosen': '0.11866', 'rewards_train/rejected': '0.067126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051529', 'logps_train/rejected': '-101.64', 'logps_train/chosen': '-144.46', 'loss/train': '0.67582', 'examples_per_second': '31.811', 'grad_norm': '20.625', 'counters/examples': 133888, 'counters/updates': 4184}
train stats after 133920 examples: {'rewards_train/chosen': '0.18836', 'rewards_train/rejected': '0.020037', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16832', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-136.91', 'loss/train': '0.62931', 'examples_per_second': '32.666', 'grad_norm': '19.375', 'counters/examples': 133920, 'counters/updates': 4185}
train stats after 133952 examples: {'rewards_train/chosen': '0.18857', 'rewards_train/rejected': '0.032044', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15652', 'logps_train/rejected': '-116', 'logps_train/chosen': '-135.54', 'loss/train': '0.63126', 'examples_per_second': '32.602', 'grad_norm': '18.375', 'counters/examples': 133952, 'counters/updates': 4186}
train stats after 133984 examples: {'rewards_train/chosen': '0.10288', 'rewards_train/rejected': '-0.0051673', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10805', 'logps_train/rejected': '-103.63', 'logps_train/chosen': '-97.959', 'loss/train': '0.64837', 'examples_per_second': '32.942', 'grad_norm': '18', 'counters/examples': 133984, 'counters/updates': 4187}
train stats after 134016 examples: {'rewards_train/chosen': '0.16852', 'rewards_train/rejected': '-0.052218', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22073', 'logps_train/rejected': '-155.66', 'logps_train/chosen': '-143.61', 'loss/train': '0.60136', 'examples_per_second': '30.187', 'grad_norm': '20.25', 'counters/examples': 134016, 'counters/updates': 4188}
train stats after 134048 examples: {'rewards_train/chosen': '0.15978', 'rewards_train/rejected': '0.056743', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10303', 'logps_train/rejected': '-103.18', 'logps_train/chosen': '-135.78', 'loss/train': '0.66237', 'examples_per_second': '31.677', 'grad_norm': '18.5', 'counters/examples': 134048, 'counters/updates': 4189}
train stats after 134080 examples: {'rewards_train/chosen': '0.16059', 'rewards_train/rejected': '0.054953', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10564', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-141.27', 'loss/train': '0.648', 'examples_per_second': '31.498', 'grad_norm': '20', 'counters/examples': 134080, 'counters/updates': 4190}
train stats after 134112 examples: {'rewards_train/chosen': '0.13056', 'rewards_train/rejected': '0.087875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04269', 'logps_train/rejected': '-108.26', 'logps_train/chosen': '-143.35', 'loss/train': '0.68008', 'examples_per_second': '31.869', 'grad_norm': '20.625', 'counters/examples': 134112, 'counters/updates': 4191}
train stats after 134144 examples: {'rewards_train/chosen': '0.10389', 'rewards_train/rejected': '0.0017603', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10213', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-145.43', 'loss/train': '0.65169', 'examples_per_second': '31.489', 'grad_norm': '20.25', 'counters/examples': 134144, 'counters/updates': 4192}
train stats after 134176 examples: {'rewards_train/chosen': '0.09786', 'rewards_train/rejected': '0.0034738', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094386', 'logps_train/rejected': '-98.746', 'logps_train/chosen': '-103.67', 'loss/train': '0.65902', 'examples_per_second': '30.614', 'grad_norm': '17.25', 'counters/examples': 134176, 'counters/updates': 4193}
train stats after 134208 examples: {'rewards_train/chosen': '0.21265', 'rewards_train/rejected': '-0.019507', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23216', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-176.05', 'loss/train': '0.59377', 'examples_per_second': '31.024', 'grad_norm': '20.125', 'counters/examples': 134208, 'counters/updates': 4194}
train stats after 134240 examples: {'rewards_train/chosen': '0.16178', 'rewards_train/rejected': '0.036991', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12479', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-136.23', 'loss/train': '0.6507', 'examples_per_second': '31.628', 'grad_norm': '20.25', 'counters/examples': 134240, 'counters/updates': 4195}
train stats after 134272 examples: {'rewards_train/chosen': '0.20676', 'rewards_train/rejected': '0.11407', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092691', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-150.87', 'loss/train': '0.66443', 'examples_per_second': '31.691', 'grad_norm': '20.75', 'counters/examples': 134272, 'counters/updates': 4196}
train stats after 134304 examples: {'rewards_train/chosen': '0.20511', 'rewards_train/rejected': '0.10794', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097172', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-124.31', 'loss/train': '0.65261', 'examples_per_second': '31.645', 'grad_norm': '20.5', 'counters/examples': 134304, 'counters/updates': 4197}
train stats after 134336 examples: {'rewards_train/chosen': '0.18592', 'rewards_train/rejected': '0.031184', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15474', 'logps_train/rejected': '-105.6', 'logps_train/chosen': '-138.63', 'loss/train': '0.62753', 'examples_per_second': '31.553', 'grad_norm': '18.625', 'counters/examples': 134336, 'counters/updates': 4198}
train stats after 134368 examples: {'rewards_train/chosen': '0.15008', 'rewards_train/rejected': '0.02651', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12357', 'logps_train/rejected': '-154.78', 'logps_train/chosen': '-127.9', 'loss/train': '0.64353', 'examples_per_second': '30.705', 'grad_norm': '20.5', 'counters/examples': 134368, 'counters/updates': 4199}
train stats after 134400 examples: {'rewards_train/chosen': '0.13837', 'rewards_train/rejected': '-0.013786', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15216', 'logps_train/rejected': '-87.614', 'logps_train/chosen': '-149.22', 'loss/train': '0.62834', 'examples_per_second': '32.716', 'grad_norm': '17.5', 'counters/examples': 134400, 'counters/updates': 4200}
train stats after 134432 examples: {'rewards_train/chosen': '0.088911', 'rewards_train/rejected': '0.0034791', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085432', 'logps_train/rejected': '-95.347', 'logps_train/chosen': '-116.52', 'loss/train': '0.65816', 'examples_per_second': '31.767', 'grad_norm': '18.625', 'counters/examples': 134432, 'counters/updates': 4201}
train stats after 134464 examples: {'rewards_train/chosen': '0.14103', 'rewards_train/rejected': '0.035566', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10546', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-138.82', 'loss/train': '0.65019', 'examples_per_second': '31.159', 'grad_norm': '20.5', 'counters/examples': 134464, 'counters/updates': 4202}
train stats after 134496 examples: {'rewards_train/chosen': '0.16241', 'rewards_train/rejected': '0.0066733', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15574', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-145.23', 'loss/train': '0.62741', 'examples_per_second': '30.858', 'grad_norm': '19.875', 'counters/examples': 134496, 'counters/updates': 4203}
train stats after 134528 examples: {'rewards_train/chosen': '0.20609', 'rewards_train/rejected': '0.034359', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17173', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-138.34', 'loss/train': '0.63051', 'examples_per_second': '30.955', 'grad_norm': '20.75', 'counters/examples': 134528, 'counters/updates': 4204}
train stats after 134560 examples: {'rewards_train/chosen': '0.086048', 'rewards_train/rejected': '0.026897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05915', 'logps_train/rejected': '-105.35', 'logps_train/chosen': '-110.55', 'loss/train': '0.67165', 'examples_per_second': '31.643', 'grad_norm': '18.375', 'counters/examples': 134560, 'counters/updates': 4205}
train stats after 134592 examples: {'rewards_train/chosen': '0.19294', 'rewards_train/rejected': '0.051271', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14166', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-134.95', 'loss/train': '0.63511', 'examples_per_second': '31.47', 'grad_norm': '19.625', 'counters/examples': 134592, 'counters/updates': 4206}
train stats after 134624 examples: {'rewards_train/chosen': '0.14773', 'rewards_train/rejected': '0.045287', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10244', 'logps_train/rejected': '-137.1', 'logps_train/chosen': '-157.6', 'loss/train': '0.65669', 'examples_per_second': '31.674', 'grad_norm': '21.125', 'counters/examples': 134624, 'counters/updates': 4207}
train stats after 134656 examples: {'rewards_train/chosen': '0.050235', 'rewards_train/rejected': '0.035224', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015011', 'logps_train/rejected': '-128.5', 'logps_train/chosen': '-146.65', 'loss/train': '0.69318', 'examples_per_second': '32.397', 'grad_norm': '21.125', 'counters/examples': 134656, 'counters/updates': 4208}
train stats after 134688 examples: {'rewards_train/chosen': '0.14904', 'rewards_train/rejected': '-0.035282', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18432', 'logps_train/rejected': '-115.45', 'logps_train/chosen': '-148.15', 'loss/train': '0.6178', 'examples_per_second': '31.81', 'grad_norm': '19.875', 'counters/examples': 134688, 'counters/updates': 4209}
train stats after 134720 examples: {'rewards_train/chosen': '0.13886', 'rewards_train/rejected': '0.051751', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087108', 'logps_train/rejected': '-125.96', 'logps_train/chosen': '-142.01', 'loss/train': '0.66096', 'examples_per_second': '30.255', 'grad_norm': '19.625', 'counters/examples': 134720, 'counters/updates': 4210}
train stats after 134752 examples: {'rewards_train/chosen': '0.16975', 'rewards_train/rejected': '0.078445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091303', 'logps_train/rejected': '-119.72', 'logps_train/chosen': '-152.68', 'loss/train': '0.66127', 'examples_per_second': '31.576', 'grad_norm': '20.625', 'counters/examples': 134752, 'counters/updates': 4211}
skipping logging after 134784 examples to avoid logging too frequently
train stats after 134816 examples: {'rewards_train/chosen': '0.15367', 'rewards_train/rejected': '0.032132', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12154', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-134.01', 'loss/train': '0.64138', 'examples_per_second': '31.002', 'grad_norm': '19.875', 'counters/examples': 134816, 'counters/updates': 4213}
train stats after 134848 examples: {'rewards_train/chosen': '0.10671', 'rewards_train/rejected': '0.11221', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0055018', 'logps_train/rejected': '-136.07', 'logps_train/chosen': '-150.27', 'loss/train': '0.71648', 'examples_per_second': '31.018', 'grad_norm': '24.625', 'counters/examples': 134848, 'counters/updates': 4214}
train stats after 134880 examples: {'rewards_train/chosen': '0.17244', 'rewards_train/rejected': '0.034273', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13816', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-131.56', 'loss/train': '0.64191', 'examples_per_second': '30.494', 'grad_norm': '22.25', 'counters/examples': 134880, 'counters/updates': 4215}
skipping logging after 134912 examples to avoid logging too frequently
train stats after 134944 examples: {'rewards_train/chosen': '0.17735', 'rewards_train/rejected': '0.03742', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13993', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-149.58', 'loss/train': '0.64329', 'examples_per_second': '34.535', 'grad_norm': '22.5', 'counters/examples': 134944, 'counters/updates': 4217}
train stats after 134976 examples: {'rewards_train/chosen': '0.2167', 'rewards_train/rejected': '0.080535', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13617', 'logps_train/rejected': '-102.48', 'logps_train/chosen': '-106.26', 'loss/train': '0.6358', 'examples_per_second': '31.649', 'grad_norm': '17.25', 'counters/examples': 134976, 'counters/updates': 4218}
train stats after 135008 examples: {'rewards_train/chosen': '0.11894', 'rewards_train/rejected': '0.029528', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089413', 'logps_train/rejected': '-105.33', 'logps_train/chosen': '-127.77', 'loss/train': '0.66014', 'examples_per_second': '31.048', 'grad_norm': '18.875', 'counters/examples': 135008, 'counters/updates': 4219}
train stats after 135040 examples: {'rewards_train/chosen': '0.16032', 'rewards_train/rejected': '0.083529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076793', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-128.76', 'loss/train': '0.65926', 'examples_per_second': '30.416', 'grad_norm': '19.625', 'counters/examples': 135040, 'counters/updates': 4220}
skipping logging after 135072 examples to avoid logging too frequently
train stats after 135104 examples: {'rewards_train/chosen': '0.17836', 'rewards_train/rejected': '0.023317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15504', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-155.63', 'loss/train': '0.63238', 'examples_per_second': '32.462', 'grad_norm': '21.25', 'counters/examples': 135104, 'counters/updates': 4222}
train stats after 135136 examples: {'rewards_train/chosen': '0.099179', 'rewards_train/rejected': '0.059888', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039291', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-126.81', 'loss/train': '0.6791', 'examples_per_second': '30.252', 'grad_norm': '20.75', 'counters/examples': 135136, 'counters/updates': 4223}
skipping logging after 135168 examples to avoid logging too frequently
train stats after 135200 examples: {'rewards_train/chosen': '0.15708', 'rewards_train/rejected': '0.067744', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089337', 'logps_train/rejected': '-152.55', 'logps_train/chosen': '-131.4', 'loss/train': '0.6559', 'examples_per_second': '31.641', 'grad_norm': '22.5', 'counters/examples': 135200, 'counters/updates': 4225}
skipping logging after 135232 examples to avoid logging too frequently
train stats after 135264 examples: {'rewards_train/chosen': '0.22436', 'rewards_train/rejected': '0.012092', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21227', 'logps_train/rejected': '-103.76', 'logps_train/chosen': '-150.81', 'loss/train': '0.60421', 'examples_per_second': '31.63', 'grad_norm': '20.5', 'counters/examples': 135264, 'counters/updates': 4227}
train stats after 135296 examples: {'rewards_train/chosen': '0.083158', 'rewards_train/rejected': '0.098181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015023', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-131.84', 'loss/train': '0.71286', 'examples_per_second': '32.127', 'grad_norm': '21.25', 'counters/examples': 135296, 'counters/updates': 4228}
train stats after 135328 examples: {'rewards_train/chosen': '0.15776', 'rewards_train/rejected': '0.072062', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085693', 'logps_train/rejected': '-120.93', 'logps_train/chosen': '-139.56', 'loss/train': '0.6595', 'examples_per_second': '31.801', 'grad_norm': '20.75', 'counters/examples': 135328, 'counters/updates': 4229}
train stats after 135360 examples: {'rewards_train/chosen': '0.22411', 'rewards_train/rejected': '0.060295', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16382', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-151.68', 'loss/train': '0.62359', 'examples_per_second': '32.696', 'grad_norm': '19.625', 'counters/examples': 135360, 'counters/updates': 4230}
skipping logging after 135392 examples to avoid logging too frequently
train stats after 135424 examples: {'rewards_train/chosen': '0.15387', 'rewards_train/rejected': '0.065522', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088351', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-141.27', 'loss/train': '0.66231', 'examples_per_second': '30.529', 'grad_norm': '20.5', 'counters/examples': 135424, 'counters/updates': 4232}
train stats after 135456 examples: {'rewards_train/chosen': '0.071693', 'rewards_train/rejected': '0.015589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056103', 'logps_train/rejected': '-103.08', 'logps_train/chosen': '-110.7', 'loss/train': '0.68385', 'examples_per_second': '32.464', 'grad_norm': '18.875', 'counters/examples': 135456, 'counters/updates': 4233}
train stats after 135488 examples: {'rewards_train/chosen': '0.073441', 'rewards_train/rejected': '0.095176', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.021735', 'logps_train/rejected': '-94.061', 'logps_train/chosen': '-114.71', 'loss/train': '0.71211', 'examples_per_second': '31.104', 'grad_norm': '18.25', 'counters/examples': 135488, 'counters/updates': 4234}
train stats after 135520 examples: {'rewards_train/chosen': '0.19264', 'rewards_train/rejected': '0.001886', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19076', 'logps_train/rejected': '-97.618', 'logps_train/chosen': '-144.05', 'loss/train': '0.62726', 'examples_per_second': '23.733', 'grad_norm': '19.25', 'counters/examples': 135520, 'counters/updates': 4235}
train stats after 135552 examples: {'rewards_train/chosen': '0.18612', 'rewards_train/rejected': '0.10341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082707', 'logps_train/rejected': '-104.97', 'logps_train/chosen': '-131.95', 'loss/train': '0.66458', 'examples_per_second': '31.663', 'grad_norm': '19.75', 'counters/examples': 135552, 'counters/updates': 4236}
train stats after 135584 examples: {'rewards_train/chosen': '0.084885', 'rewards_train/rejected': '0.019156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065729', 'logps_train/rejected': '-97.107', 'logps_train/chosen': '-118.56', 'loss/train': '0.66841', 'examples_per_second': '31.672', 'grad_norm': '19.75', 'counters/examples': 135584, 'counters/updates': 4237}
train stats after 135616 examples: {'rewards_train/chosen': '0.15257', 'rewards_train/rejected': '0.049543', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10303', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-125.45', 'loss/train': '0.66358', 'examples_per_second': '24.93', 'grad_norm': '19.75', 'counters/examples': 135616, 'counters/updates': 4238}
train stats after 135648 examples: {'rewards_train/chosen': '0.082046', 'rewards_train/rejected': '0.028174', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053872', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-139.78', 'loss/train': '0.67733', 'examples_per_second': '32.795', 'grad_norm': '21.5', 'counters/examples': 135648, 'counters/updates': 4239}
train stats after 135680 examples: {'rewards_train/chosen': '0.14325', 'rewards_train/rejected': '0.065648', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077602', 'logps_train/rejected': '-107.07', 'logps_train/chosen': '-151.1', 'loss/train': '0.66738', 'examples_per_second': '31.526', 'grad_norm': '20', 'counters/examples': 135680, 'counters/updates': 4240}
train stats after 135712 examples: {'rewards_train/chosen': '0.17139', 'rewards_train/rejected': '0.094749', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076637', 'logps_train/rejected': '-147.25', 'logps_train/chosen': '-166.35', 'loss/train': '0.66835', 'examples_per_second': '31.097', 'grad_norm': '22.875', 'counters/examples': 135712, 'counters/updates': 4241}
train stats after 135744 examples: {'rewards_train/chosen': '0.12637', 'rewards_train/rejected': '-0.0058955', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13227', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-125.43', 'loss/train': '0.64731', 'examples_per_second': '32.062', 'grad_norm': '19.75', 'counters/examples': 135744, 'counters/updates': 4242}
train stats after 135776 examples: {'rewards_train/chosen': '0.22706', 'rewards_train/rejected': '0.10142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12564', 'logps_train/rejected': '-124.93', 'logps_train/chosen': '-146.38', 'loss/train': '0.64313', 'examples_per_second': '31.43', 'grad_norm': '19.875', 'counters/examples': 135776, 'counters/updates': 4243}
train stats after 135808 examples: {'rewards_train/chosen': '0.18994', 'rewards_train/rejected': '0.118', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071947', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-142.69', 'loss/train': '0.66657', 'examples_per_second': '31.879', 'grad_norm': '22.75', 'counters/examples': 135808, 'counters/updates': 4244}
train stats after 135840 examples: {'rewards_train/chosen': '0.12243', 'rewards_train/rejected': '0.031265', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091164', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-138.9', 'loss/train': '0.6568', 'examples_per_second': '30.685', 'grad_norm': '21', 'counters/examples': 135840, 'counters/updates': 4245}
train stats after 135872 examples: {'rewards_train/chosen': '0.19939', 'rewards_train/rejected': '0.043331', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15606', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-115.5', 'loss/train': '0.6307', 'examples_per_second': '32.768', 'grad_norm': '17.875', 'counters/examples': 135872, 'counters/updates': 4246}
train stats after 135904 examples: {'rewards_train/chosen': '0.14384', 'rewards_train/rejected': '0.065936', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077907', 'logps_train/rejected': '-89.615', 'logps_train/chosen': '-118.23', 'loss/train': '0.66254', 'examples_per_second': '32.164', 'grad_norm': '18.875', 'counters/examples': 135904, 'counters/updates': 4247}
train stats after 135936 examples: {'rewards_train/chosen': '0.095603', 'rewards_train/rejected': '0.068301', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027302', 'logps_train/rejected': '-100.89', 'logps_train/chosen': '-173.95', 'loss/train': '0.69438', 'examples_per_second': '30.844', 'grad_norm': '21.75', 'counters/examples': 135936, 'counters/updates': 4248}
skipping logging after 135968 examples to avoid logging too frequently
train stats after 136000 examples: {'rewards_train/chosen': '0.097344', 'rewards_train/rejected': '0.026205', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071139', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-120', 'loss/train': '0.66951', 'examples_per_second': '30.866', 'grad_norm': '19.125', 'counters/examples': 136000, 'counters/updates': 4250}
skipping logging after 136032 examples to avoid logging too frequently
train stats after 136064 examples: {'rewards_train/chosen': '0.09514', 'rewards_train/rejected': '0.049052', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046088', 'logps_train/rejected': '-109.23', 'logps_train/chosen': '-125.2', 'loss/train': '0.67975', 'examples_per_second': '30.547', 'grad_norm': '20.625', 'counters/examples': 136064, 'counters/updates': 4252}
train stats after 136096 examples: {'rewards_train/chosen': '0.19082', 'rewards_train/rejected': '0.12052', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.070304', 'logps_train/rejected': '-141.53', 'logps_train/chosen': '-172.85', 'loss/train': '0.67258', 'examples_per_second': '32.004', 'grad_norm': '21.75', 'counters/examples': 136096, 'counters/updates': 4253}
skipping logging after 136128 examples to avoid logging too frequently
train stats after 136160 examples: {'rewards_train/chosen': '0.18907', 'rewards_train/rejected': '0.00024067', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18883', 'logps_train/rejected': '-91.457', 'logps_train/chosen': '-146.29', 'loss/train': '0.61159', 'examples_per_second': '33.234', 'grad_norm': '17.75', 'counters/examples': 136160, 'counters/updates': 4255}
train stats after 136192 examples: {'rewards_train/chosen': '0.07927', 'rewards_train/rejected': '0.10413', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.024864', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-141.94', 'loss/train': '0.71785', 'examples_per_second': '31.611', 'grad_norm': '22.75', 'counters/examples': 136192, 'counters/updates': 4256}
train stats after 136224 examples: {'rewards_train/chosen': '0.11585', 'rewards_train/rejected': '0.0066186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10923', 'logps_train/rejected': '-134.6', 'logps_train/chosen': '-125.53', 'loss/train': '0.65442', 'examples_per_second': '30.89', 'grad_norm': '19.875', 'counters/examples': 136224, 'counters/updates': 4257}
train stats after 136256 examples: {'rewards_train/chosen': '0.13228', 'rewards_train/rejected': '-0.036629', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16891', 'logps_train/rejected': '-96.784', 'logps_train/chosen': '-122.82', 'loss/train': '0.62043', 'examples_per_second': '31.681', 'grad_norm': '17.25', 'counters/examples': 136256, 'counters/updates': 4258}
train stats after 136288 examples: {'rewards_train/chosen': '0.07968', 'rewards_train/rejected': '0.066218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013463', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-136.96', 'loss/train': '0.69369', 'examples_per_second': '31.199', 'grad_norm': '20.625', 'counters/examples': 136288, 'counters/updates': 4259}
skipping logging after 136320 examples to avoid logging too frequently
train stats after 136352 examples: {'rewards_train/chosen': '0.22563', 'rewards_train/rejected': '0.039723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18591', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-163.75', 'loss/train': '0.61752', 'examples_per_second': '31.69', 'grad_norm': '19.25', 'counters/examples': 136352, 'counters/updates': 4261}
skipping logging after 136384 examples to avoid logging too frequently
train stats after 136416 examples: {'rewards_train/chosen': '0.19072', 'rewards_train/rejected': '0.079671', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11105', 'logps_train/rejected': '-137', 'logps_train/chosen': '-136.4', 'loss/train': '0.64837', 'examples_per_second': '31.586', 'grad_norm': '21', 'counters/examples': 136416, 'counters/updates': 4263}
train stats after 136448 examples: {'rewards_train/chosen': '0.12397', 'rewards_train/rejected': '0.04082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083151', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-127.67', 'loss/train': '0.65794', 'examples_per_second': '30.476', 'grad_norm': '19.875', 'counters/examples': 136448, 'counters/updates': 4264}
train stats after 136480 examples: {'rewards_train/chosen': '0.14549', 'rewards_train/rejected': '0.04856', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096928', 'logps_train/rejected': '-110.04', 'logps_train/chosen': '-157.51', 'loss/train': '0.66002', 'examples_per_second': '31.457', 'grad_norm': '20.5', 'counters/examples': 136480, 'counters/updates': 4265}
skipping logging after 136512 examples to avoid logging too frequently
train stats after 136544 examples: {'rewards_train/chosen': '0.22493', 'rewards_train/rejected': '0.071673', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15326', 'logps_train/rejected': '-120.6', 'logps_train/chosen': '-150.09', 'loss/train': '0.62911', 'examples_per_second': '31.646', 'grad_norm': '20.375', 'counters/examples': 136544, 'counters/updates': 4267}
train stats after 136576 examples: {'rewards_train/chosen': '0.24841', 'rewards_train/rejected': '0.066014', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1824', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-132.42', 'loss/train': '0.61569', 'examples_per_second': '30.326', 'grad_norm': '19.625', 'counters/examples': 136576, 'counters/updates': 4268}
train stats after 136608 examples: {'rewards_train/chosen': '0.15815', 'rewards_train/rejected': '0.11732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040834', 'logps_train/rejected': '-147.92', 'logps_train/chosen': '-122.36', 'loss/train': '0.68262', 'examples_per_second': '31.055', 'grad_norm': '20.5', 'counters/examples': 136608, 'counters/updates': 4269}
train stats after 136640 examples: {'rewards_train/chosen': '0.1619', 'rewards_train/rejected': '0.044147', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11775', 'logps_train/rejected': '-109.5', 'logps_train/chosen': '-120.96', 'loss/train': '0.64699', 'examples_per_second': '31.269', 'grad_norm': '18.875', 'counters/examples': 136640, 'counters/updates': 4270}
train stats after 136672 examples: {'rewards_train/chosen': '0.13036', 'rewards_train/rejected': '0.048227', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082138', 'logps_train/rejected': '-115.18', 'logps_train/chosen': '-159.11', 'loss/train': '0.66979', 'examples_per_second': '30.63', 'grad_norm': '22.5', 'counters/examples': 136672, 'counters/updates': 4271}
train stats after 136704 examples: {'rewards_train/chosen': '0.12912', 'rewards_train/rejected': '0.069331', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059787', 'logps_train/rejected': '-138.54', 'logps_train/chosen': '-159.86', 'loss/train': '0.67919', 'examples_per_second': '31.721', 'grad_norm': '22', 'counters/examples': 136704, 'counters/updates': 4272}
skipping logging after 136736 examples to avoid logging too frequently
train stats after 136768 examples: {'rewards_train/chosen': '0.18041', 'rewards_train/rejected': '0.050833', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12958', 'logps_train/rejected': '-110.08', 'logps_train/chosen': '-117.68', 'loss/train': '0.64332', 'examples_per_second': '33.437', 'grad_norm': '18.75', 'counters/examples': 136768, 'counters/updates': 4274}
train stats after 136800 examples: {'rewards_train/chosen': '0.16793', 'rewards_train/rejected': '-0.019543', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18747', 'logps_train/rejected': '-86.96', 'logps_train/chosen': '-108.74', 'loss/train': '0.61187', 'examples_per_second': '30.651', 'grad_norm': '16.375', 'counters/examples': 136800, 'counters/updates': 4275}
train stats after 136832 examples: {'rewards_train/chosen': '0.1115', 'rewards_train/rejected': '0.03012', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.081378', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-141.16', 'loss/train': '0.6674', 'examples_per_second': '32.736', 'grad_norm': '19.875', 'counters/examples': 136832, 'counters/updates': 4276}
train stats after 136864 examples: {'rewards_train/chosen': '0.17082', 'rewards_train/rejected': '0.066813', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10401', 'logps_train/rejected': '-127.81', 'logps_train/chosen': '-135.65', 'loss/train': '0.65666', 'examples_per_second': '31.027', 'grad_norm': '20.125', 'counters/examples': 136864, 'counters/updates': 4277}
train stats after 136896 examples: {'rewards_train/chosen': '0.089936', 'rewards_train/rejected': '0.067364', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022572', 'logps_train/rejected': '-137.54', 'logps_train/chosen': '-131.04', 'loss/train': '0.69244', 'examples_per_second': '31.7', 'grad_norm': '21.625', 'counters/examples': 136896, 'counters/updates': 4278}
train stats after 136928 examples: {'rewards_train/chosen': '0.075759', 'rewards_train/rejected': '0.007143', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068616', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-134.41', 'loss/train': '0.67428', 'examples_per_second': '31.561', 'grad_norm': '20.75', 'counters/examples': 136928, 'counters/updates': 4279}
train stats after 136960 examples: {'rewards_train/chosen': '0.10412', 'rewards_train/rejected': '0.073149', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030969', 'logps_train/rejected': '-139.08', 'logps_train/chosen': '-150.06', 'loss/train': '0.68713', 'examples_per_second': '33.046', 'grad_norm': '21.5', 'counters/examples': 136960, 'counters/updates': 4280}
train stats after 136992 examples: {'rewards_train/chosen': '0.18136', 'rewards_train/rejected': '0.064059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1173', 'logps_train/rejected': '-83.665', 'logps_train/chosen': '-143.47', 'loss/train': '0.64568', 'examples_per_second': '30.317', 'grad_norm': '18.5', 'counters/examples': 136992, 'counters/updates': 4281}
train stats after 137024 examples: {'rewards_train/chosen': '0.16838', 'rewards_train/rejected': '0.14545', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022931', 'logps_train/rejected': '-158.85', 'logps_train/chosen': '-173.07', 'loss/train': '0.69108', 'examples_per_second': '33.059', 'grad_norm': '23', 'counters/examples': 137024, 'counters/updates': 4282}
train stats after 137056 examples: {'rewards_train/chosen': '0.2043', 'rewards_train/rejected': '0.0048474', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19945', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-150.71', 'loss/train': '0.61758', 'examples_per_second': '31.625', 'grad_norm': '20.75', 'counters/examples': 137056, 'counters/updates': 4283}
train stats after 137088 examples: {'rewards_train/chosen': '0.13086', 'rewards_train/rejected': '0.01034', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12052', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-124.91', 'loss/train': '0.64319', 'examples_per_second': '30.724', 'grad_norm': '18.125', 'counters/examples': 137088, 'counters/updates': 4284}
train stats after 137120 examples: {'rewards_train/chosen': '0.19189', 'rewards_train/rejected': '0.0097711', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18212', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-181.14', 'loss/train': '0.6178', 'examples_per_second': '31.608', 'grad_norm': '20.375', 'counters/examples': 137120, 'counters/updates': 4285}
train stats after 137152 examples: {'rewards_train/chosen': '0.096996', 'rewards_train/rejected': '0.079372', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017624', 'logps_train/rejected': '-130.17', 'logps_train/chosen': '-160.96', 'loss/train': '0.70021', 'examples_per_second': '31.651', 'grad_norm': '22.375', 'counters/examples': 137152, 'counters/updates': 4286}
train stats after 137184 examples: {'rewards_train/chosen': '0.15643', 'rewards_train/rejected': '0.039611', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.11682', 'logps_train/rejected': '-108.71', 'logps_train/chosen': '-165.07', 'loss/train': '0.65161', 'examples_per_second': '31.596', 'grad_norm': '20.625', 'counters/examples': 137184, 'counters/updates': 4287}
train stats after 137216 examples: {'rewards_train/chosen': '0.12184', 'rewards_train/rejected': '0.073943', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.047896', 'logps_train/rejected': '-88.471', 'logps_train/chosen': '-128.03', 'loss/train': '0.68135', 'examples_per_second': '31.995', 'grad_norm': '19.125', 'counters/examples': 137216, 'counters/updates': 4288}
train stats after 137248 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '0.0066421', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14834', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-168.32', 'loss/train': '0.64139', 'examples_per_second': '30.179', 'grad_norm': '21.25', 'counters/examples': 137248, 'counters/updates': 4289}
skipping logging after 137280 examples to avoid logging too frequently
train stats after 137312 examples: {'rewards_train/chosen': '0.16756', 'rewards_train/rejected': '0.041683', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12587', 'logps_train/rejected': '-106.12', 'logps_train/chosen': '-133.42', 'loss/train': '0.63871', 'examples_per_second': '35.873', 'grad_norm': '18.375', 'counters/examples': 137312, 'counters/updates': 4291}
train stats after 137344 examples: {'rewards_train/chosen': '0.12083', 'rewards_train/rejected': '0.032727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088101', 'logps_train/rejected': '-134.39', 'logps_train/chosen': '-129.17', 'loss/train': '0.66169', 'examples_per_second': '29.988', 'grad_norm': '21.125', 'counters/examples': 137344, 'counters/updates': 4292}
train stats after 137376 examples: {'rewards_train/chosen': '0.11175', 'rewards_train/rejected': '0.053834', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057914', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-161.95', 'loss/train': '0.67583', 'examples_per_second': '32.108', 'grad_norm': '21.375', 'counters/examples': 137376, 'counters/updates': 4293}
train stats after 137408 examples: {'rewards_train/chosen': '0.16468', 'rewards_train/rejected': '0.15132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013361', 'logps_train/rejected': '-159.47', 'logps_train/chosen': '-153.08', 'loss/train': '0.69605', 'examples_per_second': '32.456', 'grad_norm': '22.875', 'counters/examples': 137408, 'counters/updates': 4294}
skipping logging after 137440 examples to avoid logging too frequently
train stats after 137472 examples: {'rewards_train/chosen': '0.18048', 'rewards_train/rejected': '0.072792', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10769', 'logps_train/rejected': '-139.79', 'logps_train/chosen': '-157.51', 'loss/train': '0.65727', 'examples_per_second': '31.582', 'grad_norm': '22', 'counters/examples': 137472, 'counters/updates': 4296}
train stats after 137504 examples: {'rewards_train/chosen': '0.14133', 'rewards_train/rejected': '0.026421', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11491', 'logps_train/rejected': '-100.06', 'logps_train/chosen': '-125.66', 'loss/train': '0.64271', 'examples_per_second': '31.631', 'grad_norm': '18.75', 'counters/examples': 137504, 'counters/updates': 4297}
train stats after 137536 examples: {'rewards_train/chosen': '0.059591', 'rewards_train/rejected': '-0.084646', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14424', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-121.61', 'loss/train': '0.63673', 'examples_per_second': '31.596', 'grad_norm': '18.75', 'counters/examples': 137536, 'counters/updates': 4298}
train stats after 137568 examples: {'rewards_train/chosen': '0.17786', 'rewards_train/rejected': '0.025856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.152', 'logps_train/rejected': '-150.52', 'logps_train/chosen': '-174.25', 'loss/train': '0.6308', 'examples_per_second': '30.158', 'grad_norm': '22', 'counters/examples': 137568, 'counters/updates': 4299}
skipping logging after 137600 examples to avoid logging too frequently
train stats after 137632 examples: {'rewards_train/chosen': '0.17848', 'rewards_train/rejected': '-0.010387', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18887', 'logps_train/rejected': '-120.48', 'logps_train/chosen': '-136.96', 'loss/train': '0.61115', 'examples_per_second': '31.795', 'grad_norm': '18.75', 'counters/examples': 137632, 'counters/updates': 4301}
train stats after 137664 examples: {'rewards_train/chosen': '0.1326', 'rewards_train/rejected': '-0.0096', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1422', 'logps_train/rejected': '-81.225', 'logps_train/chosen': '-111.31', 'loss/train': '0.63803', 'examples_per_second': '32.168', 'grad_norm': '15.688', 'counters/examples': 137664, 'counters/updates': 4302}
train stats after 137696 examples: {'rewards_train/chosen': '0.19237', 'rewards_train/rejected': '0.058738', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13364', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-160.25', 'loss/train': '0.63805', 'examples_per_second': '31.707', 'grad_norm': '20.375', 'counters/examples': 137696, 'counters/updates': 4303}
train stats after 137728 examples: {'rewards_train/chosen': '0.057455', 'rewards_train/rejected': '-0.0011991', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058654', 'logps_train/rejected': '-110.09', 'logps_train/chosen': '-137.11', 'loss/train': '0.6719', 'examples_per_second': '32.919', 'grad_norm': '20.25', 'counters/examples': 137728, 'counters/updates': 4304}
train stats after 137760 examples: {'rewards_train/chosen': '0.16434', 'rewards_train/rejected': '0.077467', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086872', 'logps_train/rejected': '-116.04', 'logps_train/chosen': '-122.88', 'loss/train': '0.66256', 'examples_per_second': '30.575', 'grad_norm': '19', 'counters/examples': 137760, 'counters/updates': 4305}
train stats after 137792 examples: {'rewards_train/chosen': '0.097484', 'rewards_train/rejected': '0.029005', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068479', 'logps_train/rejected': '-92.192', 'logps_train/chosen': '-118.41', 'loss/train': '0.67', 'examples_per_second': '32.491', 'grad_norm': '18.5', 'counters/examples': 137792, 'counters/updates': 4306}
train stats after 137824 examples: {'rewards_train/chosen': '0.23728', 'rewards_train/rejected': '0.033558', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20372', 'logps_train/rejected': '-98.849', 'logps_train/chosen': '-153.87', 'loss/train': '0.61442', 'examples_per_second': '31.548', 'grad_norm': '20.5', 'counters/examples': 137824, 'counters/updates': 4307}
train stats after 137856 examples: {'rewards_train/chosen': '0.17821', 'rewards_train/rejected': '0.024186', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15402', 'logps_train/rejected': '-93.974', 'logps_train/chosen': '-114.5', 'loss/train': '0.62761', 'examples_per_second': '30.639', 'grad_norm': '18.75', 'counters/examples': 137856, 'counters/updates': 4308}
train stats after 137888 examples: {'rewards_train/chosen': '0.16459', 'rewards_train/rejected': '0.096452', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068133', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-116.93', 'loss/train': '0.66961', 'examples_per_second': '32.777', 'grad_norm': '19.375', 'counters/examples': 137888, 'counters/updates': 4309}
train stats after 137920 examples: {'rewards_train/chosen': '0.22542', 'rewards_train/rejected': '0.11109', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11433', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-130.29', 'loss/train': '0.64714', 'examples_per_second': '31.426', 'grad_norm': '19.625', 'counters/examples': 137920, 'counters/updates': 4310}
train stats after 137952 examples: {'rewards_train/chosen': '0.11032', 'rewards_train/rejected': '0.086325', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023994', 'logps_train/rejected': '-135.05', 'logps_train/chosen': '-140.18', 'loss/train': '0.69102', 'examples_per_second': '30.749', 'grad_norm': '23.625', 'counters/examples': 137952, 'counters/updates': 4311}
train stats after 137984 examples: {'rewards_train/chosen': '0.1452', 'rewards_train/rejected': '-0.016188', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16139', 'logps_train/rejected': '-134', 'logps_train/chosen': '-166.37', 'loss/train': '0.6309', 'examples_per_second': '30.851', 'grad_norm': '21', 'counters/examples': 137984, 'counters/updates': 4312}
train stats after 138016 examples: {'rewards_train/chosen': '0.1739', 'rewards_train/rejected': '0.053427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12047', 'logps_train/rejected': '-123.31', 'logps_train/chosen': '-135.94', 'loss/train': '0.64635', 'examples_per_second': '30.742', 'grad_norm': '19.75', 'counters/examples': 138016, 'counters/updates': 4313}
train stats after 138048 examples: {'rewards_train/chosen': '0.17122', 'rewards_train/rejected': '0.067066', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10416', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-124.72', 'loss/train': '0.64927', 'examples_per_second': '31.243', 'grad_norm': '19.375', 'counters/examples': 138048, 'counters/updates': 4314}
train stats after 138080 examples: {'rewards_train/chosen': '0.1086', 'rewards_train/rejected': '0.030259', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078344', 'logps_train/rejected': '-99.675', 'logps_train/chosen': '-136.6', 'loss/train': '0.66127', 'examples_per_second': '23.904', 'grad_norm': '19', 'counters/examples': 138080, 'counters/updates': 4315}
train stats after 138112 examples: {'rewards_train/chosen': '0.16633', 'rewards_train/rejected': '0.026702', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13963', 'logps_train/rejected': '-156.81', 'logps_train/chosen': '-179.77', 'loss/train': '0.64142', 'examples_per_second': '31.351', 'grad_norm': '23.375', 'counters/examples': 138112, 'counters/updates': 4316}
train stats after 138144 examples: {'rewards_train/chosen': '0.1791', 'rewards_train/rejected': '0.078334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10076', 'logps_train/rejected': '-129.37', 'logps_train/chosen': '-164.65', 'loss/train': '0.6538', 'examples_per_second': '32.548', 'grad_norm': '20.625', 'counters/examples': 138144, 'counters/updates': 4317}
skipping logging after 138176 examples to avoid logging too frequently
train stats after 138208 examples: {'rewards_train/chosen': '0.14852', 'rewards_train/rejected': '0.049924', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098591', 'logps_train/rejected': '-160.46', 'logps_train/chosen': '-149.86', 'loss/train': '0.65439', 'examples_per_second': '30.095', 'grad_norm': '22', 'counters/examples': 138208, 'counters/updates': 4319}
train stats after 138240 examples: {'rewards_train/chosen': '0.13993', 'rewards_train/rejected': '0.073256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066677', 'logps_train/rejected': '-102.44', 'logps_train/chosen': '-124.45', 'loss/train': '0.67397', 'examples_per_second': '31.797', 'grad_norm': '19.75', 'counters/examples': 138240, 'counters/updates': 4320}
train stats after 138272 examples: {'rewards_train/chosen': '0.18614', 'rewards_train/rejected': '0.08752', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098616', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-92.19', 'loss/train': '0.65623', 'examples_per_second': '31.289', 'grad_norm': '17.875', 'counters/examples': 138272, 'counters/updates': 4321}
train stats after 138304 examples: {'rewards_train/chosen': '0.20258', 'rewards_train/rejected': '0.11637', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086214', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-125.79', 'loss/train': '0.65788', 'examples_per_second': '30.379', 'grad_norm': '19.25', 'counters/examples': 138304, 'counters/updates': 4322}
train stats after 138336 examples: {'rewards_train/chosen': '0.10662', 'rewards_train/rejected': '0.069228', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.037392', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-131.93', 'loss/train': '0.68994', 'examples_per_second': '31.662', 'grad_norm': '21.25', 'counters/examples': 138336, 'counters/updates': 4323}
train stats after 138368 examples: {'rewards_train/chosen': '0.17725', 'rewards_train/rejected': '0.1032', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074044', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-153.85', 'loss/train': '0.66271', 'examples_per_second': '31.278', 'grad_norm': '21', 'counters/examples': 138368, 'counters/updates': 4324}
train stats after 138400 examples: {'rewards_train/chosen': '0.065281', 'rewards_train/rejected': '0.0067473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058534', 'logps_train/rejected': '-97.541', 'logps_train/chosen': '-116.95', 'loss/train': '0.67821', 'examples_per_second': '31.111', 'grad_norm': '19.625', 'counters/examples': 138400, 'counters/updates': 4325}
train stats after 138432 examples: {'rewards_train/chosen': '0.090653', 'rewards_train/rejected': '0.07347', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017182', 'logps_train/rejected': '-116.84', 'logps_train/chosen': '-99.848', 'loss/train': '0.69578', 'examples_per_second': '31.036', 'grad_norm': '19.375', 'counters/examples': 138432, 'counters/updates': 4326}
train stats after 138464 examples: {'rewards_train/chosen': '0.24736', 'rewards_train/rejected': '0.039889', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20747', 'logps_train/rejected': '-156.05', 'logps_train/chosen': '-169.3', 'loss/train': '0.6241', 'examples_per_second': '31.529', 'grad_norm': '23.75', 'counters/examples': 138464, 'counters/updates': 4327}
train stats after 138496 examples: {'rewards_train/chosen': '0.13702', 'rewards_train/rejected': '0.0419', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095117', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-138.27', 'loss/train': '0.65941', 'examples_per_second': '31.652', 'grad_norm': '20.75', 'counters/examples': 138496, 'counters/updates': 4328}
train stats after 138528 examples: {'rewards_train/chosen': '0.18091', 'rewards_train/rejected': '0.10508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075827', 'logps_train/rejected': '-158.42', 'logps_train/chosen': '-156.48', 'loss/train': '0.67578', 'examples_per_second': '31.714', 'grad_norm': '23.125', 'counters/examples': 138528, 'counters/updates': 4329}
train stats after 138560 examples: {'rewards_train/chosen': '0.16848', 'rewards_train/rejected': '0.024345', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14413', 'logps_train/rejected': '-139.09', 'logps_train/chosen': '-140.11', 'loss/train': '0.63219', 'examples_per_second': '31.647', 'grad_norm': '20.25', 'counters/examples': 138560, 'counters/updates': 4330}
train stats after 138592 examples: {'rewards_train/chosen': '0.12557', 'rewards_train/rejected': '0.17291', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.047337', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-139.03', 'loss/train': '0.73581', 'examples_per_second': '30.9', 'grad_norm': '23', 'counters/examples': 138592, 'counters/updates': 4331}
skipping logging after 138624 examples to avoid logging too frequently
train stats after 138656 examples: {'rewards_train/chosen': '0.16346', 'rewards_train/rejected': '0.063498', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099964', 'logps_train/rejected': '-111.34', 'logps_train/chosen': '-132.5', 'loss/train': '0.65255', 'examples_per_second': '32.959', 'grad_norm': '19.125', 'counters/examples': 138656, 'counters/updates': 4333}
train stats after 138688 examples: {'rewards_train/chosen': '0.15581', 'rewards_train/rejected': '0.020841', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13497', 'logps_train/rejected': '-94.916', 'logps_train/chosen': '-115.09', 'loss/train': '0.64171', 'examples_per_second': '30.446', 'grad_norm': '17.375', 'counters/examples': 138688, 'counters/updates': 4334}
train stats after 138720 examples: {'rewards_train/chosen': '0.13333', 'rewards_train/rejected': '0.086209', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047119', 'logps_train/rejected': '-104.53', 'logps_train/chosen': '-146.74', 'loss/train': '0.685', 'examples_per_second': '31.223', 'grad_norm': '21.875', 'counters/examples': 138720, 'counters/updates': 4335}
train stats after 138752 examples: {'rewards_train/chosen': '0.18651', 'rewards_train/rejected': '0.054281', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13223', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-142.34', 'loss/train': '0.63653', 'examples_per_second': '31.42', 'grad_norm': '20.25', 'counters/examples': 138752, 'counters/updates': 4336}
skipping logging after 138784 examples to avoid logging too frequently
train stats after 138816 examples: {'rewards_train/chosen': '0.17619', 'rewards_train/rejected': '0.095133', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081058', 'logps_train/rejected': '-81.959', 'logps_train/chosen': '-103.05', 'loss/train': '0.6609', 'examples_per_second': '36.011', 'grad_norm': '16.5', 'counters/examples': 138816, 'counters/updates': 4338}
train stats after 138848 examples: {'rewards_train/chosen': '0.15337', 'rewards_train/rejected': '0.13468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018682', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-147.79', 'loss/train': '0.69706', 'examples_per_second': '31.621', 'grad_norm': '21.5', 'counters/examples': 138848, 'counters/updates': 4339}
train stats after 138880 examples: {'rewards_train/chosen': '0.13541', 'rewards_train/rejected': '0.077398', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058011', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-129.06', 'loss/train': '0.67364', 'examples_per_second': '32.556', 'grad_norm': '19.75', 'counters/examples': 138880, 'counters/updates': 4340}
train stats after 138912 examples: {'rewards_train/chosen': '0.17383', 'rewards_train/rejected': '0.026015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14781', 'logps_train/rejected': '-130', 'logps_train/chosen': '-153.95', 'loss/train': '0.6334', 'examples_per_second': '30.808', 'grad_norm': '20.125', 'counters/examples': 138912, 'counters/updates': 4341}
skipping logging after 138944 examples to avoid logging too frequently
train stats after 138976 examples: {'rewards_train/chosen': '0.055827', 'rewards_train/rejected': '0.0048488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050978', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-135.63', 'loss/train': '0.686', 'examples_per_second': '31.029', 'grad_norm': '20.125', 'counters/examples': 138976, 'counters/updates': 4343}
train stats after 139008 examples: {'rewards_train/chosen': '0.14714', 'rewards_train/rejected': '0.097744', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049399', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-144.48', 'loss/train': '0.68419', 'examples_per_second': '32.735', 'grad_norm': '20.625', 'counters/examples': 139008, 'counters/updates': 4344}
train stats after 139040 examples: {'rewards_train/chosen': '0.17737', 'rewards_train/rejected': '0.0061393', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17123', 'logps_train/rejected': '-80.165', 'logps_train/chosen': '-144.87', 'loss/train': '0.62307', 'examples_per_second': '30.142', 'grad_norm': '18.125', 'counters/examples': 139040, 'counters/updates': 4345}
train stats after 139072 examples: {'rewards_train/chosen': '0.14932', 'rewards_train/rejected': '0.12996', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.019358', 'logps_train/rejected': '-155.95', 'logps_train/chosen': '-170.05', 'loss/train': '0.69194', 'examples_per_second': '31.611', 'grad_norm': '24.75', 'counters/examples': 139072, 'counters/updates': 4346}
train stats after 139104 examples: {'rewards_train/chosen': '0.11467', 'rewards_train/rejected': '0.063407', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051259', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-113.39', 'loss/train': '0.67858', 'examples_per_second': '30.603', 'grad_norm': '18.5', 'counters/examples': 139104, 'counters/updates': 4347}
train stats after 139136 examples: {'rewards_train/chosen': '0.13186', 'rewards_train/rejected': '-0.0039956', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13586', 'logps_train/rejected': '-113.03', 'logps_train/chosen': '-135.74', 'loss/train': '0.63749', 'examples_per_second': '32.396', 'grad_norm': '19.25', 'counters/examples': 139136, 'counters/updates': 4348}
train stats after 139168 examples: {'rewards_train/chosen': '0.17394', 'rewards_train/rejected': '0.085136', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088801', 'logps_train/rejected': '-152.95', 'logps_train/chosen': '-144.53', 'loss/train': '0.65776', 'examples_per_second': '32.149', 'grad_norm': '21.25', 'counters/examples': 139168, 'counters/updates': 4349}
train stats after 139200 examples: {'rewards_train/chosen': '0.17456', 'rewards_train/rejected': '0.029783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14477', 'logps_train/rejected': '-146.57', 'logps_train/chosen': '-165.86', 'loss/train': '0.64634', 'examples_per_second': '32.585', 'grad_norm': '22.625', 'counters/examples': 139200, 'counters/updates': 4350}
train stats after 139232 examples: {'rewards_train/chosen': '0.1643', 'rewards_train/rejected': '0.092661', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071636', 'logps_train/rejected': '-158.02', 'logps_train/chosen': '-136.37', 'loss/train': '0.66328', 'examples_per_second': '31.611', 'grad_norm': '21.375', 'counters/examples': 139232, 'counters/updates': 4351}
train stats after 139264 examples: {'rewards_train/chosen': '0.16113', 'rewards_train/rejected': '0.10888', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052243', 'logps_train/rejected': '-151.24', 'logps_train/chosen': '-134.02', 'loss/train': '0.67656', 'examples_per_second': '31.541', 'grad_norm': '22.375', 'counters/examples': 139264, 'counters/updates': 4352}
train stats after 139296 examples: {'rewards_train/chosen': '0.11851', 'rewards_train/rejected': '0.06638', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052129', 'logps_train/rejected': '-138.88', 'logps_train/chosen': '-123.2', 'loss/train': '0.67883', 'examples_per_second': '31.653', 'grad_norm': '22.25', 'counters/examples': 139296, 'counters/updates': 4353}
skipping logging after 139328 examples to avoid logging too frequently
train stats after 139360 examples: {'rewards_train/chosen': '0.12835', 'rewards_train/rejected': '0.045119', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083232', 'logps_train/rejected': '-123.99', 'logps_train/chosen': '-133.48', 'loss/train': '0.66358', 'examples_per_second': '32.199', 'grad_norm': '20.375', 'counters/examples': 139360, 'counters/updates': 4355}
train stats after 139392 examples: {'rewards_train/chosen': '0.19408', 'rewards_train/rejected': '0.051708', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14237', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-149.22', 'loss/train': '0.6361', 'examples_per_second': '32.589', 'grad_norm': '20.125', 'counters/examples': 139392, 'counters/updates': 4356}
skipping logging after 139424 examples to avoid logging too frequently
train stats after 139456 examples: {'rewards_train/chosen': '0.17327', 'rewards_train/rejected': '-0.022983', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19625', 'logps_train/rejected': '-151', 'logps_train/chosen': '-147.34', 'loss/train': '0.60939', 'examples_per_second': '31.613', 'grad_norm': '20.375', 'counters/examples': 139456, 'counters/updates': 4358}
train stats after 139488 examples: {'rewards_train/chosen': '0.050355', 'rewards_train/rejected': '0.0088611', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041494', 'logps_train/rejected': '-104.53', 'logps_train/chosen': '-125.09', 'loss/train': '0.68243', 'examples_per_second': '31.023', 'grad_norm': '19.625', 'counters/examples': 139488, 'counters/updates': 4359}
train stats after 139520 examples: {'rewards_train/chosen': '0.1554', 'rewards_train/rejected': '0.01958', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13582', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-138.5', 'loss/train': '0.63883', 'examples_per_second': '31.576', 'grad_norm': '19.875', 'counters/examples': 139520, 'counters/updates': 4360}
train stats after 139552 examples: {'rewards_train/chosen': '0.12289', 'rewards_train/rejected': '0.010736', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11216', 'logps_train/rejected': '-93.052', 'logps_train/chosen': '-125.13', 'loss/train': '0.64855', 'examples_per_second': '31.757', 'grad_norm': '19.125', 'counters/examples': 139552, 'counters/updates': 4361}
train stats after 139584 examples: {'rewards_train/chosen': '0.19139', 'rewards_train/rejected': '0.076574', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11481', 'logps_train/rejected': '-104.99', 'logps_train/chosen': '-122.68', 'loss/train': '0.64871', 'examples_per_second': '31.605', 'grad_norm': '18.75', 'counters/examples': 139584, 'counters/updates': 4362}
train stats after 139616 examples: {'rewards_train/chosen': '0.17345', 'rewards_train/rejected': '0.047449', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.126', 'logps_train/rejected': '-123.97', 'logps_train/chosen': '-171.82', 'loss/train': '0.64249', 'examples_per_second': '32.111', 'grad_norm': '21', 'counters/examples': 139616, 'counters/updates': 4363}
skipping logging after 139648 examples to avoid logging too frequently
train stats after 139680 examples: {'rewards_train/chosen': '0.09196', 'rewards_train/rejected': '0.070779', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021181', 'logps_train/rejected': '-102.6', 'logps_train/chosen': '-134.07', 'loss/train': '0.6967', 'examples_per_second': '35.772', 'grad_norm': '19.75', 'counters/examples': 139680, 'counters/updates': 4365}
train stats after 139712 examples: {'rewards_train/chosen': '0.13864', 'rewards_train/rejected': '0.10469', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033949', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-148.5', 'loss/train': '0.68397', 'examples_per_second': '30.826', 'grad_norm': '21.375', 'counters/examples': 139712, 'counters/updates': 4366}
train stats after 139744 examples: {'rewards_train/chosen': '0.12774', 'rewards_train/rejected': '0.030399', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097344', 'logps_train/rejected': '-110.37', 'logps_train/chosen': '-127.36', 'loss/train': '0.65111', 'examples_per_second': '31.638', 'grad_norm': '20.125', 'counters/examples': 139744, 'counters/updates': 4367}
train stats after 139776 examples: {'rewards_train/chosen': '0.10281', 'rewards_train/rejected': '0.11313', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010316', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-121.07', 'loss/train': '0.70779', 'examples_per_second': '32.464', 'grad_norm': '21.25', 'counters/examples': 139776, 'counters/updates': 4368}
skipping logging after 139808 examples to avoid logging too frequently
train stats after 139840 examples: {'rewards_train/chosen': '0.13688', 'rewards_train/rejected': '0.12857', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0083055', 'logps_train/rejected': '-152.75', 'logps_train/chosen': '-134.79', 'loss/train': '0.70016', 'examples_per_second': '30.435', 'grad_norm': '21.625', 'counters/examples': 139840, 'counters/updates': 4370}
train stats after 139872 examples: {'rewards_train/chosen': '0.16862', 'rewards_train/rejected': '0.036881', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13174', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-139.27', 'loss/train': '0.64249', 'examples_per_second': '31.643', 'grad_norm': '19.5', 'counters/examples': 139872, 'counters/updates': 4371}
train stats after 139904 examples: {'rewards_train/chosen': '0.19875', 'rewards_train/rejected': '0.061295', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13745', 'logps_train/rejected': '-103.89', 'logps_train/chosen': '-136.78', 'loss/train': '0.64794', 'examples_per_second': '30.33', 'grad_norm': '18', 'counters/examples': 139904, 'counters/updates': 4372}
train stats after 139936 examples: {'rewards_train/chosen': '0.13841', 'rewards_train/rejected': '0.037525', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10089', 'logps_train/rejected': '-149.71', 'logps_train/chosen': '-153.62', 'loss/train': '0.65485', 'examples_per_second': '31.637', 'grad_norm': '22.125', 'counters/examples': 139936, 'counters/updates': 4373}
train stats after 139968 examples: {'rewards_train/chosen': '0.20331', 'rewards_train/rejected': '0.047261', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15605', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-130.51', 'loss/train': '0.62467', 'examples_per_second': '31.603', 'grad_norm': '21', 'counters/examples': 139968, 'counters/updates': 4374}
train stats after 140000 examples: {'rewards_train/chosen': '0.086484', 'rewards_train/rejected': '0.1938', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10731', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-123.52', 'loss/train': '0.76348', 'examples_per_second': '31.61', 'grad_norm': '22.875', 'counters/examples': 140000, 'counters/updates': 4375}
train stats after 140032 examples: {'rewards_train/chosen': '0.15061', 'rewards_train/rejected': '0.075878', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074729', 'logps_train/rejected': '-155.17', 'logps_train/chosen': '-120.06', 'loss/train': '0.66735', 'examples_per_second': '30.847', 'grad_norm': '21.125', 'counters/examples': 140032, 'counters/updates': 4376}
train stats after 140064 examples: {'rewards_train/chosen': '0.096563', 'rewards_train/rejected': '0.039344', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05722', 'logps_train/rejected': '-92.347', 'logps_train/chosen': '-120.24', 'loss/train': '0.67736', 'examples_per_second': '31.247', 'grad_norm': '18', 'counters/examples': 140064, 'counters/updates': 4377}
train stats after 140096 examples: {'rewards_train/chosen': '0.19357', 'rewards_train/rejected': '0.0038255', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18975', 'logps_train/rejected': '-84.97', 'logps_train/chosen': '-144.22', 'loss/train': '0.62082', 'examples_per_second': '32.172', 'grad_norm': '16.875', 'counters/examples': 140096, 'counters/updates': 4378}
train stats after 140128 examples: {'rewards_train/chosen': '0.1547', 'rewards_train/rejected': '0.092741', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061961', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-136.08', 'loss/train': '0.66793', 'examples_per_second': '30.146', 'grad_norm': '20.75', 'counters/examples': 140128, 'counters/updates': 4379}
train stats after 140160 examples: {'rewards_train/chosen': '0.17686', 'rewards_train/rejected': '0.065088', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11178', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-141.17', 'loss/train': '0.6511', 'examples_per_second': '30.583', 'grad_norm': '18.5', 'counters/examples': 140160, 'counters/updates': 4380}
train stats after 140192 examples: {'rewards_train/chosen': '0.12719', 'rewards_train/rejected': '0.019522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10767', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-120.26', 'loss/train': '0.65375', 'examples_per_second': '31.609', 'grad_norm': '19.25', 'counters/examples': 140192, 'counters/updates': 4381}
train stats after 140224 examples: {'rewards_train/chosen': '0.15781', 'rewards_train/rejected': '0.013955', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14386', 'logps_train/rejected': '-105.9', 'logps_train/chosen': '-181.88', 'loss/train': '0.63259', 'examples_per_second': '31.594', 'grad_norm': '22.25', 'counters/examples': 140224, 'counters/updates': 4382}
train stats after 140256 examples: {'rewards_train/chosen': '0.16405', 'rewards_train/rejected': '0.05464', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10942', 'logps_train/rejected': '-169.21', 'logps_train/chosen': '-168.68', 'loss/train': '0.65503', 'examples_per_second': '33.034', 'grad_norm': '22.625', 'counters/examples': 140256, 'counters/updates': 4383}
train stats after 140288 examples: {'rewards_train/chosen': '0.098718', 'rewards_train/rejected': '0.036693', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062025', 'logps_train/rejected': '-145.28', 'logps_train/chosen': '-149.57', 'loss/train': '0.6759', 'examples_per_second': '31.474', 'grad_norm': '20.875', 'counters/examples': 140288, 'counters/updates': 4384}
train stats after 140320 examples: {'rewards_train/chosen': '0.17791', 'rewards_train/rejected': '0.078651', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099255', 'logps_train/rejected': '-105.81', 'logps_train/chosen': '-133.89', 'loss/train': '0.65608', 'examples_per_second': '30.62', 'grad_norm': '18.875', 'counters/examples': 140320, 'counters/updates': 4385}
skipping logging after 140352 examples to avoid logging too frequently
train stats after 140384 examples: {'rewards_train/chosen': '0.20142', 'rewards_train/rejected': '0.063932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13748', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-167.41', 'loss/train': '0.64049', 'examples_per_second': '30.986', 'grad_norm': '20.5', 'counters/examples': 140384, 'counters/updates': 4387}
train stats after 140416 examples: {'rewards_train/chosen': '0.16401', 'rewards_train/rejected': '0.046', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11801', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-104.12', 'loss/train': '0.64972', 'examples_per_second': '30.998', 'grad_norm': '20.5', 'counters/examples': 140416, 'counters/updates': 4388}
train stats after 140448 examples: {'rewards_train/chosen': '0.1428', 'rewards_train/rejected': '0.050789', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092008', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-144.73', 'loss/train': '0.65924', 'examples_per_second': '32.687', 'grad_norm': '22.125', 'counters/examples': 140448, 'counters/updates': 4389}
skipping logging after 140480 examples to avoid logging too frequently
train stats after 140512 examples: {'rewards_train/chosen': '0.082439', 'rewards_train/rejected': '0.021302', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061137', 'logps_train/rejected': '-95.948', 'logps_train/chosen': '-128.62', 'loss/train': '0.67106', 'examples_per_second': '32.715', 'grad_norm': '19.25', 'counters/examples': 140512, 'counters/updates': 4391}
train stats after 140544 examples: {'rewards_train/chosen': '0.13369', 'rewards_train/rejected': '0.028592', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1051', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-140.45', 'loss/train': '0.65203', 'examples_per_second': '32.052', 'grad_norm': '19.5', 'counters/examples': 140544, 'counters/updates': 4392}
train stats after 140576 examples: {'rewards_train/chosen': '0.17929', 'rewards_train/rejected': '0.12548', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053809', 'logps_train/rejected': '-143.26', 'logps_train/chosen': '-144.63', 'loss/train': '0.67813', 'examples_per_second': '30.487', 'grad_norm': '21.375', 'counters/examples': 140576, 'counters/updates': 4393}
train stats after 140608 examples: {'rewards_train/chosen': '0.080235', 'rewards_train/rejected': '0.033841', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046395', 'logps_train/rejected': '-110.22', 'logps_train/chosen': '-111.05', 'loss/train': '0.68197', 'examples_per_second': '31.774', 'grad_norm': '19.75', 'counters/examples': 140608, 'counters/updates': 4394}
train stats after 140640 examples: {'rewards_train/chosen': '0.19034', 'rewards_train/rejected': '0.0083897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18195', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-118.89', 'loss/train': '0.61784', 'examples_per_second': '30.153', 'grad_norm': '18.125', 'counters/examples': 140640, 'counters/updates': 4395}
train stats after 140672 examples: {'rewards_train/chosen': '0.13489', 'rewards_train/rejected': '0.079144', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055745', 'logps_train/rejected': '-167.18', 'logps_train/chosen': '-149.79', 'loss/train': '0.67593', 'examples_per_second': '32.457', 'grad_norm': '23.125', 'counters/examples': 140672, 'counters/updates': 4396}
train stats after 140704 examples: {'rewards_train/chosen': '0.2036', 'rewards_train/rejected': '0.04415', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15945', 'logps_train/rejected': '-138.99', 'logps_train/chosen': '-174.47', 'loss/train': '0.62967', 'examples_per_second': '30.982', 'grad_norm': '22.375', 'counters/examples': 140704, 'counters/updates': 4397}
train stats after 140736 examples: {'rewards_train/chosen': '0.15028', 'rewards_train/rejected': '0.039838', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11044', 'logps_train/rejected': '-99.636', 'logps_train/chosen': '-130.61', 'loss/train': '0.64773', 'examples_per_second': '31.595', 'grad_norm': '19.5', 'counters/examples': 140736, 'counters/updates': 4398}
train stats after 140768 examples: {'rewards_train/chosen': '0.11138', 'rewards_train/rejected': '0.076397', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034982', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-130.04', 'loss/train': '0.68651', 'examples_per_second': '30.848', 'grad_norm': '20.375', 'counters/examples': 140768, 'counters/updates': 4399}
train stats after 140800 examples: {'rewards_train/chosen': '0.12072', 'rewards_train/rejected': '-0.018341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13906', 'logps_train/rejected': '-101.4', 'logps_train/chosen': '-145.18', 'loss/train': '0.63433', 'examples_per_second': '32.173', 'grad_norm': '19.5', 'counters/examples': 140800, 'counters/updates': 4400}
train stats after 140832 examples: {'rewards_train/chosen': '0.23328', 'rewards_train/rejected': '0.041236', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19205', 'logps_train/rejected': '-119.94', 'logps_train/chosen': '-165.11', 'loss/train': '0.61179', 'examples_per_second': '30.768', 'grad_norm': '19.5', 'counters/examples': 140832, 'counters/updates': 4401}
train stats after 140864 examples: {'rewards_train/chosen': '0.12848', 'rewards_train/rejected': '0.025545', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10294', 'logps_train/rejected': '-87.749', 'logps_train/chosen': '-131.75', 'loss/train': '0.64974', 'examples_per_second': '30.676', 'grad_norm': '19.75', 'counters/examples': 140864, 'counters/updates': 4402}
train stats after 140896 examples: {'rewards_train/chosen': '0.11076', 'rewards_train/rejected': '-0.00049409', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11126', 'logps_train/rejected': '-76.147', 'logps_train/chosen': '-113.24', 'loss/train': '0.64301', 'examples_per_second': '32.118', 'grad_norm': '16.625', 'counters/examples': 140896, 'counters/updates': 4403}
train stats after 140928 examples: {'rewards_train/chosen': '0.17905', 'rewards_train/rejected': '0.11201', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.067037', 'logps_train/rejected': '-134.33', 'logps_train/chosen': '-125.93', 'loss/train': '0.67771', 'examples_per_second': '32.149', 'grad_norm': '21', 'counters/examples': 140928, 'counters/updates': 4404}
train stats after 140960 examples: {'rewards_train/chosen': '0.11429', 'rewards_train/rejected': '0.069532', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044761', 'logps_train/rejected': '-99.688', 'logps_train/chosen': '-129.24', 'loss/train': '0.68116', 'examples_per_second': '31.005', 'grad_norm': '20.125', 'counters/examples': 140960, 'counters/updates': 4405}
train stats after 140992 examples: {'rewards_train/chosen': '0.18112', 'rewards_train/rejected': '0.088853', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092263', 'logps_train/rejected': '-113.88', 'logps_train/chosen': '-146.15', 'loss/train': '0.65611', 'examples_per_second': '26.168', 'grad_norm': '20.375', 'counters/examples': 140992, 'counters/updates': 4406}
train stats after 141024 examples: {'rewards_train/chosen': '0.14185', 'rewards_train/rejected': '0.074733', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067119', 'logps_train/rejected': '-184.44', 'logps_train/chosen': '-125.88', 'loss/train': '0.66743', 'examples_per_second': '31.563', 'grad_norm': '22', 'counters/examples': 141024, 'counters/updates': 4407}
train stats after 141056 examples: {'rewards_train/chosen': '0.14511', 'rewards_train/rejected': '0.005455', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13965', 'logps_train/rejected': '-94.046', 'logps_train/chosen': '-142.19', 'loss/train': '0.64185', 'examples_per_second': '30.094', 'grad_norm': '19.375', 'counters/examples': 141056, 'counters/updates': 4408}
train stats after 141088 examples: {'rewards_train/chosen': '0.14165', 'rewards_train/rejected': '0.12666', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014985', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-136.9', 'loss/train': '0.69779', 'examples_per_second': '25.429', 'grad_norm': '20.625', 'counters/examples': 141088, 'counters/updates': 4409}
skipping logging after 141120 examples to avoid logging too frequently
train stats after 141152 examples: {'rewards_train/chosen': '0.24822', 'rewards_train/rejected': '0.050883', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19734', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-134.89', 'loss/train': '0.62166', 'examples_per_second': '31.359', 'grad_norm': '19', 'counters/examples': 141152, 'counters/updates': 4411}
train stats after 141184 examples: {'rewards_train/chosen': '0.12885', 'rewards_train/rejected': '-0.016133', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14498', 'logps_train/rejected': '-101.35', 'logps_train/chosen': '-119.78', 'loss/train': '0.63543', 'examples_per_second': '32.126', 'grad_norm': '17.375', 'counters/examples': 141184, 'counters/updates': 4412}
train stats after 141216 examples: {'rewards_train/chosen': '0.16124', 'rewards_train/rejected': '0.084588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076655', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-154.93', 'loss/train': '0.6594', 'examples_per_second': '31.579', 'grad_norm': '20.875', 'counters/examples': 141216, 'counters/updates': 4413}
train stats after 141248 examples: {'rewards_train/chosen': '0.19373', 'rewards_train/rejected': '0.017383', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17635', 'logps_train/rejected': '-103.06', 'logps_train/chosen': '-137', 'loss/train': '0.63034', 'examples_per_second': '31.251', 'grad_norm': '18.875', 'counters/examples': 141248, 'counters/updates': 4414}
train stats after 141280 examples: {'rewards_train/chosen': '0.074114', 'rewards_train/rejected': '0.012728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061387', 'logps_train/rejected': '-112.38', 'logps_train/chosen': '-116.66', 'loss/train': '0.66761', 'examples_per_second': '31.147', 'grad_norm': '18.375', 'counters/examples': 141280, 'counters/updates': 4415}
train stats after 141312 examples: {'rewards_train/chosen': '0.13688', 'rewards_train/rejected': '0.039033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09785', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-158.89', 'loss/train': '0.65377', 'examples_per_second': '31.365', 'grad_norm': '19.875', 'counters/examples': 141312, 'counters/updates': 4416}
train stats after 141344 examples: {'rewards_train/chosen': '0.11654', 'rewards_train/rejected': '-0.039496', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15604', 'logps_train/rejected': '-102.64', 'logps_train/chosen': '-130.71', 'loss/train': '0.62404', 'examples_per_second': '31.598', 'grad_norm': '18.625', 'counters/examples': 141344, 'counters/updates': 4417}
train stats after 141376 examples: {'rewards_train/chosen': '0.17273', 'rewards_train/rejected': '0.039681', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13305', 'logps_train/rejected': '-100.48', 'logps_train/chosen': '-117.23', 'loss/train': '0.63698', 'examples_per_second': '30.238', 'grad_norm': '18.375', 'counters/examples': 141376, 'counters/updates': 4418}
train stats after 141408 examples: {'rewards_train/chosen': '0.14221', 'rewards_train/rejected': '0.067206', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075002', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-141.34', 'loss/train': '0.66392', 'examples_per_second': '31.572', 'grad_norm': '20.75', 'counters/examples': 141408, 'counters/updates': 4419}
train stats after 141440 examples: {'rewards_train/chosen': '0.22878', 'rewards_train/rejected': '0.044452', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18433', 'logps_train/rejected': '-118.72', 'logps_train/chosen': '-156.73', 'loss/train': '0.61719', 'examples_per_second': '32.655', 'grad_norm': '19.875', 'counters/examples': 141440, 'counters/updates': 4420}
train stats after 141472 examples: {'rewards_train/chosen': '0.094356', 'rewards_train/rejected': '0.067323', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027034', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-114.23', 'loss/train': '0.69041', 'examples_per_second': '31.297', 'grad_norm': '20', 'counters/examples': 141472, 'counters/updates': 4421}
train stats after 141504 examples: {'rewards_train/chosen': '0.14974', 'rewards_train/rejected': '0.09575', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053994', 'logps_train/rejected': '-137.39', 'logps_train/chosen': '-130.57', 'loss/train': '0.67452', 'examples_per_second': '31.596', 'grad_norm': '19.75', 'counters/examples': 141504, 'counters/updates': 4422}
train stats after 141536 examples: {'rewards_train/chosen': '0.12376', 'rewards_train/rejected': '0.045099', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078664', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-121.13', 'loss/train': '0.66811', 'examples_per_second': '32.202', 'grad_norm': '19.125', 'counters/examples': 141536, 'counters/updates': 4423}
skipping logging after 141568 examples to avoid logging too frequently
train stats after 141600 examples: {'rewards_train/chosen': '0.14218', 'rewards_train/rejected': '0.0042168', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13796', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-143.1', 'loss/train': '0.63573', 'examples_per_second': '30.712', 'grad_norm': '20', 'counters/examples': 141600, 'counters/updates': 4425}
skipping logging after 141632 examples to avoid logging too frequently
train stats after 141664 examples: {'rewards_train/chosen': '0.12888', 'rewards_train/rejected': '0.022665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10621', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-109.88', 'loss/train': '0.65346', 'examples_per_second': '33.811', 'grad_norm': '19.625', 'counters/examples': 141664, 'counters/updates': 4427}
train stats after 141696 examples: {'rewards_train/chosen': '0.18753', 'rewards_train/rejected': '0.062593', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12494', 'logps_train/rejected': '-133.13', 'logps_train/chosen': '-116.93', 'loss/train': '0.64342', 'examples_per_second': '32.514', 'grad_norm': '19.125', 'counters/examples': 141696, 'counters/updates': 4428}
train stats after 141728 examples: {'rewards_train/chosen': '0.11568', 'rewards_train/rejected': '0.089801', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025882', 'logps_train/rejected': '-99.554', 'logps_train/chosen': '-157.64', 'loss/train': '0.68979', 'examples_per_second': '32.34', 'grad_norm': '20.375', 'counters/examples': 141728, 'counters/updates': 4429}
train stats after 141760 examples: {'rewards_train/chosen': '0.17316', 'rewards_train/rejected': '0.0023066', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17086', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-132.3', 'loss/train': '0.62253', 'examples_per_second': '31.748', 'grad_norm': '18.375', 'counters/examples': 141760, 'counters/updates': 4430}
train stats after 141792 examples: {'rewards_train/chosen': '0.12966', 'rewards_train/rejected': '0.06868', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060985', 'logps_train/rejected': '-131.88', 'logps_train/chosen': '-131.97', 'loss/train': '0.67314', 'examples_per_second': '31.601', 'grad_norm': '20.5', 'counters/examples': 141792, 'counters/updates': 4431}
skipping logging after 141824 examples to avoid logging too frequently
train stats after 141856 examples: {'rewards_train/chosen': '0.087427', 'rewards_train/rejected': '0.10618', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018758', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-131.3', 'loss/train': '0.71189', 'examples_per_second': '31.596', 'grad_norm': '22.375', 'counters/examples': 141856, 'counters/updates': 4433}
train stats after 141888 examples: {'rewards_train/chosen': '0.1583', 'rewards_train/rejected': '0.042145', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11615', 'logps_train/rejected': '-118.02', 'logps_train/chosen': '-177.63', 'loss/train': '0.65024', 'examples_per_second': '31.577', 'grad_norm': '22.25', 'counters/examples': 141888, 'counters/updates': 4434}
train stats after 141920 examples: {'rewards_train/chosen': '0.13533', 'rewards_train/rejected': '0.088172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047161', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-122.54', 'loss/train': '0.68779', 'examples_per_second': '30.595', 'grad_norm': '21.625', 'counters/examples': 141920, 'counters/updates': 4435}
train stats after 141952 examples: {'rewards_train/chosen': '0.082949', 'rewards_train/rejected': '0.069807', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013142', 'logps_train/rejected': '-101.81', 'logps_train/chosen': '-102.52', 'loss/train': '0.69504', 'examples_per_second': '31.466', 'grad_norm': '18.5', 'counters/examples': 141952, 'counters/updates': 4436}
train stats after 141984 examples: {'rewards_train/chosen': '0.22816', 'rewards_train/rejected': '0.025314', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20285', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-156.96', 'loss/train': '0.60956', 'examples_per_second': '31.368', 'grad_norm': '20.25', 'counters/examples': 141984, 'counters/updates': 4437}
train stats after 142016 examples: {'rewards_train/chosen': '0.13548', 'rewards_train/rejected': '0.028699', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10678', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-145.62', 'loss/train': '0.65439', 'examples_per_second': '31.325', 'grad_norm': '20.75', 'counters/examples': 142016, 'counters/updates': 4438}
skipping logging after 142048 examples to avoid logging too frequently
train stats after 142080 examples: {'rewards_train/chosen': '0.11856', 'rewards_train/rejected': '-0.018106', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13667', 'logps_train/rejected': '-119.14', 'logps_train/chosen': '-104.27', 'loss/train': '0.63983', 'examples_per_second': '31.56', 'grad_norm': '18.125', 'counters/examples': 142080, 'counters/updates': 4440}
train stats after 142112 examples: {'rewards_train/chosen': '0.057776', 'rewards_train/rejected': '0.10127', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.043494', 'logps_train/rejected': '-151.05', 'logps_train/chosen': '-164.39', 'loss/train': '0.72891', 'examples_per_second': '32.13', 'grad_norm': '24.25', 'counters/examples': 142112, 'counters/updates': 4441}
skipping logging after 142144 examples to avoid logging too frequently
train stats after 142176 examples: {'rewards_train/chosen': '0.21901', 'rewards_train/rejected': '0.039047', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17996', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-113.15', 'loss/train': '0.6145', 'examples_per_second': '33.998', 'grad_norm': '18.5', 'counters/examples': 142176, 'counters/updates': 4443}
train stats after 142208 examples: {'rewards_train/chosen': '0.18038', 'rewards_train/rejected': '0.040715', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13966', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-155.06', 'loss/train': '0.63536', 'examples_per_second': '30.429', 'grad_norm': '20.5', 'counters/examples': 142208, 'counters/updates': 4444}
train stats after 142240 examples: {'rewards_train/chosen': '0.28082', 'rewards_train/rejected': '-0.033283', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.31411', 'logps_train/rejected': '-83.62', 'logps_train/chosen': '-145.47', 'loss/train': '0.56556', 'examples_per_second': '31.153', 'grad_norm': '17.375', 'counters/examples': 142240, 'counters/updates': 4445}
skipping logging after 142272 examples to avoid logging too frequently
train stats after 142304 examples: {'rewards_train/chosen': '0.20343', 'rewards_train/rejected': '0.024726', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1787', 'logps_train/rejected': '-108.81', 'logps_train/chosen': '-152.61', 'loss/train': '0.62041', 'examples_per_second': '31.37', 'grad_norm': '20.375', 'counters/examples': 142304, 'counters/updates': 4447}
skipping logging after 142336 examples to avoid logging too frequently
train stats after 142368 examples: {'rewards_train/chosen': '0.11041', 'rewards_train/rejected': '0.044754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065656', 'logps_train/rejected': '-92.348', 'logps_train/chosen': '-99.989', 'loss/train': '0.6655', 'examples_per_second': '30.717', 'grad_norm': '17.5', 'counters/examples': 142368, 'counters/updates': 4449}
train stats after 142400 examples: {'rewards_train/chosen': '0.10778', 'rewards_train/rejected': '0.018065', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089715', 'logps_train/rejected': '-151', 'logps_train/chosen': '-152.62', 'loss/train': '0.65895', 'examples_per_second': '32.061', 'grad_norm': '22.375', 'counters/examples': 142400, 'counters/updates': 4450}
train stats after 142432 examples: {'rewards_train/chosen': '0.16128', 'rewards_train/rejected': '0.062911', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098373', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-119.3', 'loss/train': '0.65367', 'examples_per_second': '31.569', 'grad_norm': '19.25', 'counters/examples': 142432, 'counters/updates': 4451}
train stats after 142464 examples: {'rewards_train/chosen': '0.1531', 'rewards_train/rejected': '0.067242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085861', 'logps_train/rejected': '-116.51', 'logps_train/chosen': '-130.13', 'loss/train': '0.65907', 'examples_per_second': '32.054', 'grad_norm': '19.625', 'counters/examples': 142464, 'counters/updates': 4452}
skipping logging after 142496 examples to avoid logging too frequently
train stats after 142528 examples: {'rewards_train/chosen': '0.12052', 'rewards_train/rejected': '0.030935', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089589', 'logps_train/rejected': '-121.41', 'logps_train/chosen': '-137.06', 'loss/train': '0.65643', 'examples_per_second': '33.095', 'grad_norm': '20', 'counters/examples': 142528, 'counters/updates': 4454}
train stats after 142560 examples: {'rewards_train/chosen': '0.13231', 'rewards_train/rejected': '0.064183', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068125', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-136.54', 'loss/train': '0.67648', 'examples_per_second': '31.399', 'grad_norm': '20.625', 'counters/examples': 142560, 'counters/updates': 4455}
train stats after 142592 examples: {'rewards_train/chosen': '0.19397', 'rewards_train/rejected': '0.045906', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14807', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-162.72', 'loss/train': '0.63655', 'examples_per_second': '30.739', 'grad_norm': '23.25', 'counters/examples': 142592, 'counters/updates': 4456}
train stats after 142624 examples: {'rewards_train/chosen': '0.14995', 'rewards_train/rejected': '0.078594', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071355', 'logps_train/rejected': '-131.89', 'logps_train/chosen': '-172.28', 'loss/train': '0.66665', 'examples_per_second': '31.552', 'grad_norm': '24.125', 'counters/examples': 142624, 'counters/updates': 4457}
train stats after 142656 examples: {'rewards_train/chosen': '0.18365', 'rewards_train/rejected': '0.00062022', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18303', 'logps_train/rejected': '-80.317', 'logps_train/chosen': '-128.13', 'loss/train': '0.62039', 'examples_per_second': '31.414', 'grad_norm': '18.25', 'counters/examples': 142656, 'counters/updates': 4458}
train stats after 142688 examples: {'rewards_train/chosen': '0.369', 'rewards_train/rejected': '0.091945', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.27705', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-148.83', 'loss/train': '0.5795', 'examples_per_second': '31.225', 'grad_norm': '18.875', 'counters/examples': 142688, 'counters/updates': 4459}
skipping logging after 142720 examples to avoid logging too frequently
train stats after 142752 examples: {'rewards_train/chosen': '0.073405', 'rewards_train/rejected': '0.019935', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05347', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-109.51', 'loss/train': '0.67291', 'examples_per_second': '33.224', 'grad_norm': '19.875', 'counters/examples': 142752, 'counters/updates': 4461}
train stats after 142784 examples: {'rewards_train/chosen': '0.14002', 'rewards_train/rejected': '0.067818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072197', 'logps_train/rejected': '-148.23', 'logps_train/chosen': '-117.37', 'loss/train': '0.67001', 'examples_per_second': '32.505', 'grad_norm': '20.25', 'counters/examples': 142784, 'counters/updates': 4462}
train stats after 142816 examples: {'rewards_train/chosen': '0.13446', 'rewards_train/rejected': '0.013638', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12082', 'logps_train/rejected': '-115.92', 'logps_train/chosen': '-127.93', 'loss/train': '0.64183', 'examples_per_second': '31.336', 'grad_norm': '19.375', 'counters/examples': 142816, 'counters/updates': 4463}
train stats after 142848 examples: {'rewards_train/chosen': '0.13558', 'rewards_train/rejected': '0.032534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10305', 'logps_train/rejected': '-104.22', 'logps_train/chosen': '-122.96', 'loss/train': '0.64787', 'examples_per_second': '31.469', 'grad_norm': '18.5', 'counters/examples': 142848, 'counters/updates': 4464}
train stats after 142880 examples: {'rewards_train/chosen': '0.090604', 'rewards_train/rejected': '0.027412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063192', 'logps_train/rejected': '-111.99', 'logps_train/chosen': '-120.87', 'loss/train': '0.67552', 'examples_per_second': '30.425', 'grad_norm': '20.625', 'counters/examples': 142880, 'counters/updates': 4465}
train stats after 142912 examples: {'rewards_train/chosen': '0.1745', 'rewards_train/rejected': '0.062405', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11209', 'logps_train/rejected': '-104.43', 'logps_train/chosen': '-124.93', 'loss/train': '0.65195', 'examples_per_second': '30.16', 'grad_norm': '19', 'counters/examples': 142912, 'counters/updates': 4466}
train stats after 142944 examples: {'rewards_train/chosen': '0.1585', 'rewards_train/rejected': '0.096342', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062156', 'logps_train/rejected': '-132.16', 'logps_train/chosen': '-141.47', 'loss/train': '0.66896', 'examples_per_second': '31.332', 'grad_norm': '21.625', 'counters/examples': 142944, 'counters/updates': 4467}
train stats after 142976 examples: {'rewards_train/chosen': '0.16586', 'rewards_train/rejected': '0.051689', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11417', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-143.88', 'loss/train': '0.65439', 'examples_per_second': '31.617', 'grad_norm': '20.875', 'counters/examples': 142976, 'counters/updates': 4468}
train stats after 143008 examples: {'rewards_train/chosen': '0.087957', 'rewards_train/rejected': '0.11233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.024374', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-149.77', 'loss/train': '0.71938', 'examples_per_second': '32.513', 'grad_norm': '22.375', 'counters/examples': 143008, 'counters/updates': 4469}
train stats after 143040 examples: {'rewards_train/chosen': '0.19214', 'rewards_train/rejected': '-0.0070764', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19922', 'logps_train/rejected': '-95.008', 'logps_train/chosen': '-172.42', 'loss/train': '0.6098', 'examples_per_second': '30.023', 'grad_norm': '20.625', 'counters/examples': 143040, 'counters/updates': 4470}
skipping logging after 143072 examples to avoid logging too frequently
train stats after 143104 examples: {'rewards_train/chosen': '0.16597', 'rewards_train/rejected': '0.077819', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088152', 'logps_train/rejected': '-175.59', 'logps_train/chosen': '-144.83', 'loss/train': '0.65998', 'examples_per_second': '32.405', 'grad_norm': '21.75', 'counters/examples': 143104, 'counters/updates': 4472}
train stats after 143136 examples: {'rewards_train/chosen': '0.21659', 'rewards_train/rejected': '0.040174', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17641', 'logps_train/rejected': '-130.21', 'logps_train/chosen': '-141.47', 'loss/train': '0.61961', 'examples_per_second': '31.548', 'grad_norm': '20', 'counters/examples': 143136, 'counters/updates': 4473}
train stats after 143168 examples: {'rewards_train/chosen': '0.19249', 'rewards_train/rejected': '0.055035', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13746', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-127.47', 'loss/train': '0.64677', 'examples_per_second': '30.229', 'grad_norm': '19.125', 'counters/examples': 143168, 'counters/updates': 4474}
train stats after 143200 examples: {'rewards_train/chosen': '0.15076', 'rewards_train/rejected': '0.10182', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048933', 'logps_train/rejected': '-112.65', 'logps_train/chosen': '-127.08', 'loss/train': '0.67982', 'examples_per_second': '32.447', 'grad_norm': '20.125', 'counters/examples': 143200, 'counters/updates': 4475}
train stats after 143232 examples: {'rewards_train/chosen': '0.2194', 'rewards_train/rejected': '0.025076', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19433', 'logps_train/rejected': '-126.43', 'logps_train/chosen': '-134.36', 'loss/train': '0.61095', 'examples_per_second': '31.513', 'grad_norm': '18.75', 'counters/examples': 143232, 'counters/updates': 4476}
train stats after 143264 examples: {'rewards_train/chosen': '0.2181', 'rewards_train/rejected': '0.076817', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14128', 'logps_train/rejected': '-127.99', 'logps_train/chosen': '-151.42', 'loss/train': '0.63325', 'examples_per_second': '31.537', 'grad_norm': '20.75', 'counters/examples': 143264, 'counters/updates': 4477}
train stats after 143296 examples: {'rewards_train/chosen': '0.1601', 'rewards_train/rejected': '0.093921', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.066181', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-139.31', 'loss/train': '0.67428', 'examples_per_second': '29.997', 'grad_norm': '20', 'counters/examples': 143296, 'counters/updates': 4478}
train stats after 143328 examples: {'rewards_train/chosen': '0.14672', 'rewards_train/rejected': '0.063831', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082892', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-148.22', 'loss/train': '0.67224', 'examples_per_second': '31.58', 'grad_norm': '21.75', 'counters/examples': 143328, 'counters/updates': 4479}
train stats after 143360 examples: {'rewards_train/chosen': '0.20855', 'rewards_train/rejected': '0.083547', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12501', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-175.63', 'loss/train': '0.64913', 'examples_per_second': '30.808', 'grad_norm': '22.125', 'counters/examples': 143360, 'counters/updates': 4480}
train stats after 143392 examples: {'rewards_train/chosen': '0.16409', 'rewards_train/rejected': '0.034817', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12927', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-129.93', 'loss/train': '0.6363', 'examples_per_second': '31.329', 'grad_norm': '18.875', 'counters/examples': 143392, 'counters/updates': 4481}
train stats after 143424 examples: {'rewards_train/chosen': '0.12863', 'rewards_train/rejected': '0.08761', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041021', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-145.35', 'loss/train': '0.68', 'examples_per_second': '31.569', 'grad_norm': '20.5', 'counters/examples': 143424, 'counters/updates': 4482}
train stats after 143456 examples: {'rewards_train/chosen': '0.13254', 'rewards_train/rejected': '0.080229', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052309', 'logps_train/rejected': '-139.07', 'logps_train/chosen': '-146.55', 'loss/train': '0.68209', 'examples_per_second': '30.133', 'grad_norm': '21.125', 'counters/examples': 143456, 'counters/updates': 4483}
skipping logging after 143488 examples to avoid logging too frequently
train stats after 143520 examples: {'rewards_train/chosen': '0.20837', 'rewards_train/rejected': '0.07717', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1312', 'logps_train/rejected': '-122.06', 'logps_train/chosen': '-138.35', 'loss/train': '0.64628', 'examples_per_second': '34.696', 'grad_norm': '19.5', 'counters/examples': 143520, 'counters/updates': 4485}
train stats after 143552 examples: {'rewards_train/chosen': '0.15585', 'rewards_train/rejected': '0.10373', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052116', 'logps_train/rejected': '-159.93', 'logps_train/chosen': '-171.37', 'loss/train': '0.67916', 'examples_per_second': '30.896', 'grad_norm': '22.625', 'counters/examples': 143552, 'counters/updates': 4486}
train stats after 143584 examples: {'rewards_train/chosen': '0.090567', 'rewards_train/rejected': '0.050361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040206', 'logps_train/rejected': '-93.151', 'logps_train/chosen': '-127.36', 'loss/train': '0.68017', 'examples_per_second': '30.455', 'grad_norm': '19.25', 'counters/examples': 143584, 'counters/updates': 4487}
train stats after 143616 examples: {'rewards_train/chosen': '0.087895', 'rewards_train/rejected': '0.07716', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010735', 'logps_train/rejected': '-94.745', 'logps_train/chosen': '-107.06', 'loss/train': '0.69767', 'examples_per_second': '31.775', 'grad_norm': '18.625', 'counters/examples': 143616, 'counters/updates': 4488}
train stats after 143648 examples: {'rewards_train/chosen': '0.14252', 'rewards_train/rejected': '0.049402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093114', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-164.58', 'loss/train': '0.65921', 'examples_per_second': '22.311', 'grad_norm': '22.125', 'counters/examples': 143648, 'counters/updates': 4489}
train stats after 143680 examples: {'rewards_train/chosen': '0.20801', 'rewards_train/rejected': '0.041908', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1661', 'logps_train/rejected': '-95.504', 'logps_train/chosen': '-123.36', 'loss/train': '0.62969', 'examples_per_second': '31.667', 'grad_norm': '18.375', 'counters/examples': 143680, 'counters/updates': 4490}
train stats after 143712 examples: {'rewards_train/chosen': '0.18045', 'rewards_train/rejected': '0.077427', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10302', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-132.87', 'loss/train': '0.64878', 'examples_per_second': '32.742', 'grad_norm': '18.625', 'counters/examples': 143712, 'counters/updates': 4491}
train stats after 143744 examples: {'rewards_train/chosen': '0.16591', 'rewards_train/rejected': '0.040097', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12582', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-152.07', 'loss/train': '0.64268', 'examples_per_second': '31.021', 'grad_norm': '19.75', 'counters/examples': 143744, 'counters/updates': 4492}
train stats after 143776 examples: {'rewards_train/chosen': '0.16032', 'rewards_train/rejected': '0.090983', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069341', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-139.17', 'loss/train': '0.67174', 'examples_per_second': '31.255', 'grad_norm': '19.875', 'counters/examples': 143776, 'counters/updates': 4493}
skipping logging after 143808 examples to avoid logging too frequently
train stats after 143840 examples: {'rewards_train/chosen': '0.14218', 'rewards_train/rejected': '-0.026667', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16885', 'logps_train/rejected': '-95.828', 'logps_train/chosen': '-152.81', 'loss/train': '0.62129', 'examples_per_second': '32.19', 'grad_norm': '19.25', 'counters/examples': 143840, 'counters/updates': 4495}
train stats after 143872 examples: {'rewards_train/chosen': '0.11561', 'rewards_train/rejected': '0.044408', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071204', 'logps_train/rejected': '-112.77', 'logps_train/chosen': '-140.94', 'loss/train': '0.66902', 'examples_per_second': '30.986', 'grad_norm': '19.125', 'counters/examples': 143872, 'counters/updates': 4496}
skipping logging after 143904 examples to avoid logging too frequently
train stats after 143936 examples: {'rewards_train/chosen': '0.1334', 'rewards_train/rejected': '0.084719', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048682', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-132.47', 'loss/train': '0.67958', 'examples_per_second': '30.382', 'grad_norm': '20.375', 'counters/examples': 143936, 'counters/updates': 4498}
train stats after 143968 examples: {'rewards_train/chosen': '0.14666', 'rewards_train/rejected': '0.1156', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031069', 'logps_train/rejected': '-136.21', 'logps_train/chosen': '-133.64', 'loss/train': '0.68516', 'examples_per_second': '31.421', 'grad_norm': '21.625', 'counters/examples': 143968, 'counters/updates': 4499}
skipping logging after 144000 examples to avoid logging too frequently
Running evaluation after 144000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.89it/s]
eval after 144000: {'rewards_eval/chosen': '0.1486', 'rewards_eval/rejected': '0.055351', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.093251', 'logps_eval/rejected': '-114.84', 'logps_eval/chosen': '-134.06', 'loss/eval': '0.66095'}
skipping save for non epoch
train stats after 144032 examples: {'rewards_train/chosen': '0.13972', 'rewards_train/rejected': '0.0062085', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13351', 'logps_train/rejected': '-141.94', 'logps_train/chosen': '-146.81', 'loss/train': '0.64418', 'examples_per_second': '31.619', 'grad_norm': '20.125', 'counters/examples': 144032, 'counters/updates': 4501}
skipping logging after 144064 examples to avoid logging too frequently
train stats after 144096 examples: {'rewards_train/chosen': '0.19791', 'rewards_train/rejected': '0.12122', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076688', 'logps_train/rejected': '-116', 'logps_train/chosen': '-140.13', 'loss/train': '0.66689', 'examples_per_second': '31.43', 'grad_norm': '20.375', 'counters/examples': 144096, 'counters/updates': 4503}
skipping logging after 144128 examples to avoid logging too frequently
train stats after 144160 examples: {'rewards_train/chosen': '0.25699', 'rewards_train/rejected': '0.09221', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16478', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-163.84', 'loss/train': '0.63158', 'examples_per_second': '31.203', 'grad_norm': '20.125', 'counters/examples': 144160, 'counters/updates': 4505}
skipping logging after 144192 examples to avoid logging too frequently
train stats after 144224 examples: {'rewards_train/chosen': '0.1751', 'rewards_train/rejected': '-0.022878', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19798', 'logps_train/rejected': '-88.651', 'logps_train/chosen': '-120.64', 'loss/train': '0.60481', 'examples_per_second': '37.522', 'grad_norm': '16.75', 'counters/examples': 144224, 'counters/updates': 4507}
skipping logging after 144256 examples to avoid logging too frequently
train stats after 144288 examples: {'rewards_train/chosen': '0.11782', 'rewards_train/rejected': '0.0315', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086319', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-149.83', 'loss/train': '0.65976', 'examples_per_second': '36.026', 'grad_norm': '20.875', 'counters/examples': 144288, 'counters/updates': 4509}
train stats after 144320 examples: {'rewards_train/chosen': '0.16524', 'rewards_train/rejected': '0.077966', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087276', 'logps_train/rejected': '-138.73', 'logps_train/chosen': '-157.7', 'loss/train': '0.66316', 'examples_per_second': '30.343', 'grad_norm': '21.125', 'counters/examples': 144320, 'counters/updates': 4510}
train stats after 144352 examples: {'rewards_train/chosen': '0.13029', 'rewards_train/rejected': '0.1019', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028384', 'logps_train/rejected': '-94.707', 'logps_train/chosen': '-160.08', 'loss/train': '0.69632', 'examples_per_second': '31.483', 'grad_norm': '21.25', 'counters/examples': 144352, 'counters/updates': 4511}
train stats after 144384 examples: {'rewards_train/chosen': '0.15035', 'rewards_train/rejected': '0.046721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10363', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-151.95', 'loss/train': '0.65165', 'examples_per_second': '31.541', 'grad_norm': '20.875', 'counters/examples': 144384, 'counters/updates': 4512}
train stats after 144416 examples: {'rewards_train/chosen': '0.21375', 'rewards_train/rejected': '0.082075', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13168', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-145.47', 'loss/train': '0.64131', 'examples_per_second': '30.51', 'grad_norm': '20.25', 'counters/examples': 144416, 'counters/updates': 4513}
train stats after 144448 examples: {'rewards_train/chosen': '0.21087', 'rewards_train/rejected': '0.16951', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041358', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-113.48', 'loss/train': '0.68122', 'examples_per_second': '31.25', 'grad_norm': '20.125', 'counters/examples': 144448, 'counters/updates': 4514}
train stats after 144480 examples: {'rewards_train/chosen': '0.22763', 'rewards_train/rejected': '0.12528', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10236', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-125.03', 'loss/train': '0.65179', 'examples_per_second': '30.393', 'grad_norm': '19.625', 'counters/examples': 144480, 'counters/updates': 4515}
train stats after 144512 examples: {'rewards_train/chosen': '0.19741', 'rewards_train/rejected': '0.066793', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13062', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-137.79', 'loss/train': '0.64189', 'examples_per_second': '30.795', 'grad_norm': '19.625', 'counters/examples': 144512, 'counters/updates': 4516}
train stats after 144544 examples: {'rewards_train/chosen': '0.13954', 'rewards_train/rejected': '0.054802', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084741', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-138.99', 'loss/train': '0.67036', 'examples_per_second': '32.033', 'grad_norm': '21', 'counters/examples': 144544, 'counters/updates': 4517}
train stats after 144576 examples: {'rewards_train/chosen': '0.10039', 'rewards_train/rejected': '-0.014597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11499', 'logps_train/rejected': '-98.275', 'logps_train/chosen': '-151.82', 'loss/train': '0.64448', 'examples_per_second': '31.996', 'grad_norm': '19.375', 'counters/examples': 144576, 'counters/updates': 4518}
train stats after 144608 examples: {'rewards_train/chosen': '0.17131', 'rewards_train/rejected': '0.10643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064883', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-175.39', 'loss/train': '0.67324', 'examples_per_second': '31.462', 'grad_norm': '22.5', 'counters/examples': 144608, 'counters/updates': 4519}
train stats after 144640 examples: {'rewards_train/chosen': '0.18602', 'rewards_train/rejected': '0.083291', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10273', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-123.84', 'loss/train': '0.65361', 'examples_per_second': '31.238', 'grad_norm': '18.25', 'counters/examples': 144640, 'counters/updates': 4520}
skipping logging after 144672 examples to avoid logging too frequently
train stats after 144704 examples: {'rewards_train/chosen': '0.184', 'rewards_train/rejected': '0.02181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16219', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-153.23', 'loss/train': '0.62769', 'examples_per_second': '31.551', 'grad_norm': '20.75', 'counters/examples': 144704, 'counters/updates': 4522}
train stats after 144736 examples: {'rewards_train/chosen': '0.12303', 'rewards_train/rejected': '0.067131', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055896', 'logps_train/rejected': '-102.6', 'logps_train/chosen': '-128.72', 'loss/train': '0.6779', 'examples_per_second': '31.863', 'grad_norm': '18.75', 'counters/examples': 144736, 'counters/updates': 4523}
train stats after 144768 examples: {'rewards_train/chosen': '0.11855', 'rewards_train/rejected': '0.075674', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042876', 'logps_train/rejected': '-145.24', 'logps_train/chosen': '-107.47', 'loss/train': '0.68024', 'examples_per_second': '31.475', 'grad_norm': '20.25', 'counters/examples': 144768, 'counters/updates': 4524}
train stats after 144800 examples: {'rewards_train/chosen': '0.072813', 'rewards_train/rejected': '0.04321', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029604', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-124.49', 'loss/train': '0.68941', 'examples_per_second': '31.204', 'grad_norm': '20.875', 'counters/examples': 144800, 'counters/updates': 4525}
train stats after 144832 examples: {'rewards_train/chosen': '0.22048', 'rewards_train/rejected': '0.014815', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20567', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-181.44', 'loss/train': '0.60615', 'examples_per_second': '30.548', 'grad_norm': '20.25', 'counters/examples': 144832, 'counters/updates': 4526}
train stats after 144864 examples: {'rewards_train/chosen': '0.096702', 'rewards_train/rejected': '0.036083', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060619', 'logps_train/rejected': '-129', 'logps_train/chosen': '-153.1', 'loss/train': '0.67309', 'examples_per_second': '30.118', 'grad_norm': '20.5', 'counters/examples': 144864, 'counters/updates': 4527}
train stats after 144896 examples: {'rewards_train/chosen': '0.09948', 'rewards_train/rejected': '-0.034981', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13446', 'logps_train/rejected': '-123.46', 'logps_train/chosen': '-135.9', 'loss/train': '0.63596', 'examples_per_second': '30.094', 'grad_norm': '20.5', 'counters/examples': 144896, 'counters/updates': 4528}
train stats after 144928 examples: {'rewards_train/chosen': '0.1008', 'rewards_train/rejected': '0.029373', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071423', 'logps_train/rejected': '-111.31', 'logps_train/chosen': '-134.34', 'loss/train': '0.66675', 'examples_per_second': '31.331', 'grad_norm': '18.875', 'counters/examples': 144928, 'counters/updates': 4529}
train stats after 144960 examples: {'rewards_train/chosen': '0.14118', 'rewards_train/rejected': '0.031119', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11006', 'logps_train/rejected': '-103.96', 'logps_train/chosen': '-161.16', 'loss/train': '0.6561', 'examples_per_second': '31.401', 'grad_norm': '21', 'counters/examples': 144960, 'counters/updates': 4530}
train stats after 144992 examples: {'rewards_train/chosen': '0.14571', 'rewards_train/rejected': '0.017077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12863', 'logps_train/rejected': '-102.14', 'logps_train/chosen': '-151.43', 'loss/train': '0.63676', 'examples_per_second': '31.236', 'grad_norm': '20', 'counters/examples': 144992, 'counters/updates': 4531}
train stats after 145024 examples: {'rewards_train/chosen': '0.088144', 'rewards_train/rejected': '0.00054029', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087604', 'logps_train/rejected': '-125.62', 'logps_train/chosen': '-139.03', 'loss/train': '0.66103', 'examples_per_second': '31.516', 'grad_norm': '21.25', 'counters/examples': 145024, 'counters/updates': 4532}
train stats after 145056 examples: {'rewards_train/chosen': '0.088367', 'rewards_train/rejected': '0.059164', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029203', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-117.1', 'loss/train': '0.68925', 'examples_per_second': '30.556', 'grad_norm': '21', 'counters/examples': 145056, 'counters/updates': 4533}
skipping logging after 145088 examples to avoid logging too frequently
train stats after 145120 examples: {'rewards_train/chosen': '0.090028', 'rewards_train/rejected': '0.14936', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.059335', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-120.94', 'loss/train': '0.73234', 'examples_per_second': '31.494', 'grad_norm': '21.125', 'counters/examples': 145120, 'counters/updates': 4535}
train stats after 145152 examples: {'rewards_train/chosen': '0.1942', 'rewards_train/rejected': '0.081352', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11284', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-145.34', 'loss/train': '0.6495', 'examples_per_second': '32.906', 'grad_norm': '19.5', 'counters/examples': 145152, 'counters/updates': 4536}
skipping logging after 145184 examples to avoid logging too frequently
train stats after 145216 examples: {'rewards_train/chosen': '0.1721', 'rewards_train/rejected': '0.039239', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13286', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-138.59', 'loss/train': '0.63416', 'examples_per_second': '33.382', 'grad_norm': '19.375', 'counters/examples': 145216, 'counters/updates': 4538}
skipping logging after 145248 examples to avoid logging too frequently
train stats after 145280 examples: {'rewards_train/chosen': '0.054225', 'rewards_train/rejected': '0.0019422', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052283', 'logps_train/rejected': '-117.04', 'logps_train/chosen': '-130.07', 'loss/train': '0.67424', 'examples_per_second': '32.833', 'grad_norm': '18.625', 'counters/examples': 145280, 'counters/updates': 4540}
train stats after 145312 examples: {'rewards_train/chosen': '0.1246', 'rewards_train/rejected': '0.089573', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.035031', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-134.74', 'loss/train': '0.69577', 'examples_per_second': '31.584', 'grad_norm': '21.75', 'counters/examples': 145312, 'counters/updates': 4541}
skipping logging after 145344 examples to avoid logging too frequently
train stats after 145376 examples: {'rewards_train/chosen': '0.091136', 'rewards_train/rejected': '0.025636', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0655', 'logps_train/rejected': '-139.28', 'logps_train/chosen': '-119.32', 'loss/train': '0.67032', 'examples_per_second': '31.687', 'grad_norm': '20.25', 'counters/examples': 145376, 'counters/updates': 4543}
train stats after 145408 examples: {'rewards_train/chosen': '0.21542', 'rewards_train/rejected': '0.014331', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20109', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-164.57', 'loss/train': '0.61499', 'examples_per_second': '31.545', 'grad_norm': '21.25', 'counters/examples': 145408, 'counters/updates': 4544}
train stats after 145440 examples: {'rewards_train/chosen': '0.21947', 'rewards_train/rejected': '0.012299', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20717', 'logps_train/rejected': '-134.47', 'logps_train/chosen': '-126.09', 'loss/train': '0.61995', 'examples_per_second': '32.924', 'grad_norm': '18.75', 'counters/examples': 145440, 'counters/updates': 4545}
train stats after 145472 examples: {'rewards_train/chosen': '0.13949', 'rewards_train/rejected': '0.12132', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018174', 'logps_train/rejected': '-109.54', 'logps_train/chosen': '-131.09', 'loss/train': '0.69215', 'examples_per_second': '30.837', 'grad_norm': '21.5', 'counters/examples': 145472, 'counters/updates': 4546}
train stats after 145504 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '0.048093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087683', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-150.8', 'loss/train': '0.66246', 'examples_per_second': '30.788', 'grad_norm': '21', 'counters/examples': 145504, 'counters/updates': 4547}
train stats after 145536 examples: {'rewards_train/chosen': '0.1712', 'rewards_train/rejected': '0.054311', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11689', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-159.27', 'loss/train': '0.64783', 'examples_per_second': '31.288', 'grad_norm': '21.25', 'counters/examples': 145536, 'counters/updates': 4548}
train stats after 145568 examples: {'rewards_train/chosen': '0.13133', 'rewards_train/rejected': '0.03234', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098993', 'logps_train/rejected': '-108.52', 'logps_train/chosen': '-127.82', 'loss/train': '0.65727', 'examples_per_second': '31.707', 'grad_norm': '18.375', 'counters/examples': 145568, 'counters/updates': 4549}
train stats after 145600 examples: {'rewards_train/chosen': '0.13585', 'rewards_train/rejected': '0.064881', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070973', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-133.49', 'loss/train': '0.66836', 'examples_per_second': '31.224', 'grad_norm': '19.375', 'counters/examples': 145600, 'counters/updates': 4550}
train stats after 145632 examples: {'rewards_train/chosen': '0.20245', 'rewards_train/rejected': '0.08149', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12096', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-127.25', 'loss/train': '0.64795', 'examples_per_second': '31.532', 'grad_norm': '19.875', 'counters/examples': 145632, 'counters/updates': 4551}
skipping logging after 145664 examples to avoid logging too frequently
train stats after 145696 examples: {'rewards_train/chosen': '0.15392', 'rewards_train/rejected': '0.037776', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11614', 'logps_train/rejected': '-128.88', 'logps_train/chosen': '-135.35', 'loss/train': '0.65864', 'examples_per_second': '32.297', 'grad_norm': '21.5', 'counters/examples': 145696, 'counters/updates': 4553}
train stats after 145728 examples: {'rewards_train/chosen': '0.19798', 'rewards_train/rejected': '0.056402', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14157', 'logps_train/rejected': '-98.116', 'logps_train/chosen': '-139.19', 'loss/train': '0.63918', 'examples_per_second': '30.294', 'grad_norm': '19.25', 'counters/examples': 145728, 'counters/updates': 4554}
skipping logging after 145760 examples to avoid logging too frequently
train stats after 145792 examples: {'rewards_train/chosen': '0.22905', 'rewards_train/rejected': '0.13826', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090793', 'logps_train/rejected': '-143.08', 'logps_train/chosen': '-138.83', 'loss/train': '0.66123', 'examples_per_second': '33.22', 'grad_norm': '21.375', 'counters/examples': 145792, 'counters/updates': 4556}
train stats after 145824 examples: {'rewards_train/chosen': '0.17823', 'rewards_train/rejected': '0.015767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16247', 'logps_train/rejected': '-92.379', 'logps_train/chosen': '-115.46', 'loss/train': '0.62198', 'examples_per_second': '31.543', 'grad_norm': '16.625', 'counters/examples': 145824, 'counters/updates': 4557}
train stats after 145856 examples: {'rewards_train/chosen': '0.13111', 'rewards_train/rejected': '0.018535', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11258', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-109.03', 'loss/train': '0.65225', 'examples_per_second': '30.616', 'grad_norm': '19.875', 'counters/examples': 145856, 'counters/updates': 4558}
train stats after 145888 examples: {'rewards_train/chosen': '0.1303', 'rewards_train/rejected': '0.0043677', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12593', 'logps_train/rejected': '-85.26', 'logps_train/chosen': '-136.25', 'loss/train': '0.6397', 'examples_per_second': '31.113', 'grad_norm': '18.875', 'counters/examples': 145888, 'counters/updates': 4559}
train stats after 145920 examples: {'rewards_train/chosen': '0.12335', 'rewards_train/rejected': '0.064179', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059173', 'logps_train/rejected': '-88.645', 'logps_train/chosen': '-137.94', 'loss/train': '0.67462', 'examples_per_second': '30.792', 'grad_norm': '18.75', 'counters/examples': 145920, 'counters/updates': 4560}
train stats after 145952 examples: {'rewards_train/chosen': '0.1544', 'rewards_train/rejected': '0.058086', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09631', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-143.09', 'loss/train': '0.65817', 'examples_per_second': '30.578', 'grad_norm': '20.75', 'counters/examples': 145952, 'counters/updates': 4561}
skipping logging after 145984 examples to avoid logging too frequently
train stats after 146016 examples: {'rewards_train/chosen': '0.09465', 'rewards_train/rejected': '-0.014292', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10894', 'logps_train/rejected': '-76.432', 'logps_train/chosen': '-121.56', 'loss/train': '0.65209', 'examples_per_second': '34.05', 'grad_norm': '17', 'counters/examples': 146016, 'counters/updates': 4563}
train stats after 146048 examples: {'rewards_train/chosen': '0.17203', 'rewards_train/rejected': '0.056242', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11579', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-151.48', 'loss/train': '0.6487', 'examples_per_second': '30.071', 'grad_norm': '21.375', 'counters/examples': 146048, 'counters/updates': 4564}
train stats after 146080 examples: {'rewards_train/chosen': '0.057617', 'rewards_train/rejected': '-0.030041', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087658', 'logps_train/rejected': '-97.386', 'logps_train/chosen': '-85.304', 'loss/train': '0.65908', 'examples_per_second': '31.809', 'grad_norm': '16.125', 'counters/examples': 146080, 'counters/updates': 4565}
train stats after 146112 examples: {'rewards_train/chosen': '0.16432', 'rewards_train/rejected': '0.047602', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11672', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-148.05', 'loss/train': '0.65355', 'examples_per_second': '32.311', 'grad_norm': '20.375', 'counters/examples': 146112, 'counters/updates': 4566}
train stats after 146144 examples: {'rewards_train/chosen': '0.1461', 'rewards_train/rejected': '0.023533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12257', 'logps_train/rejected': '-131.33', 'logps_train/chosen': '-181.15', 'loss/train': '0.64705', 'examples_per_second': '31.455', 'grad_norm': '22', 'counters/examples': 146144, 'counters/updates': 4567}
train stats after 146176 examples: {'rewards_train/chosen': '0.14655', 'rewards_train/rejected': '-0.035296', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18185', 'logps_train/rejected': '-135.2', 'logps_train/chosen': '-159.94', 'loss/train': '0.61505', 'examples_per_second': '31.521', 'grad_norm': '20.25', 'counters/examples': 146176, 'counters/updates': 4568}
train stats after 146208 examples: {'rewards_train/chosen': '0.14739', 'rewards_train/rejected': '0.016081', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13131', 'logps_train/rejected': '-106.65', 'logps_train/chosen': '-133.01', 'loss/train': '0.64767', 'examples_per_second': '31.228', 'grad_norm': '17.75', 'counters/examples': 146208, 'counters/updates': 4569}
skipping logging after 146240 examples to avoid logging too frequently
train stats after 146272 examples: {'rewards_train/chosen': '0.1425', 'rewards_train/rejected': '0.047829', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094668', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-136.28', 'loss/train': '0.65584', 'examples_per_second': '31.417', 'grad_norm': '20', 'counters/examples': 146272, 'counters/updates': 4571}
train stats after 146304 examples: {'rewards_train/chosen': '0.16566', 'rewards_train/rejected': '0.073249', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092407', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-147.3', 'loss/train': '0.65648', 'examples_per_second': '33.039', 'grad_norm': '20.75', 'counters/examples': 146304, 'counters/updates': 4572}
skipping logging after 146336 examples to avoid logging too frequently
train stats after 146368 examples: {'rewards_train/chosen': '0.14832', 'rewards_train/rejected': '0.020118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1282', 'logps_train/rejected': '-123.57', 'logps_train/chosen': '-152.1', 'loss/train': '0.64094', 'examples_per_second': '30.197', 'grad_norm': '20', 'counters/examples': 146368, 'counters/updates': 4574}
train stats after 146400 examples: {'rewards_train/chosen': '0.08747', 'rewards_train/rejected': '0.065949', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021521', 'logps_train/rejected': '-113.58', 'logps_train/chosen': '-104.6', 'loss/train': '0.69012', 'examples_per_second': '30.987', 'grad_norm': '20.625', 'counters/examples': 146400, 'counters/updates': 4575}
train stats after 146432 examples: {'rewards_train/chosen': '0.16962', 'rewards_train/rejected': '0.043274', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12634', 'logps_train/rejected': '-118.76', 'logps_train/chosen': '-145.02', 'loss/train': '0.64457', 'examples_per_second': '32.815', 'grad_norm': '19.875', 'counters/examples': 146432, 'counters/updates': 4576}
train stats after 146464 examples: {'rewards_train/chosen': '0.19287', 'rewards_train/rejected': '-0.0054293', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1983', 'logps_train/rejected': '-98.063', 'logps_train/chosen': '-157.18', 'loss/train': '0.61304', 'examples_per_second': '25.143', 'grad_norm': '18.75', 'counters/examples': 146464, 'counters/updates': 4577}
train stats after 146496 examples: {'rewards_train/chosen': '0.19288', 'rewards_train/rejected': '0.1037', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089184', 'logps_train/rejected': '-140.12', 'logps_train/chosen': '-168.47', 'loss/train': '0.65798', 'examples_per_second': '32.806', 'grad_norm': '22.125', 'counters/examples': 146496, 'counters/updates': 4578}
train stats after 146528 examples: {'rewards_train/chosen': '0.070315', 'rewards_train/rejected': '0.083709', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013393', 'logps_train/rejected': '-123.02', 'logps_train/chosen': '-143.85', 'loss/train': '0.71043', 'examples_per_second': '33.167', 'grad_norm': '21.875', 'counters/examples': 146528, 'counters/updates': 4579}
train stats after 146560 examples: {'rewards_train/chosen': '0.20644', 'rewards_train/rejected': '0.11282', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.093616', 'logps_train/rejected': '-108.95', 'logps_train/chosen': '-141.8', 'loss/train': '0.66707', 'examples_per_second': '23.591', 'grad_norm': '20', 'counters/examples': 146560, 'counters/updates': 4580}
train stats after 146592 examples: {'rewards_train/chosen': '0.052234', 'rewards_train/rejected': '-0.018855', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071089', 'logps_train/rejected': '-105.71', 'logps_train/chosen': '-94.844', 'loss/train': '0.66646', 'examples_per_second': '30.083', 'grad_norm': '18.5', 'counters/examples': 146592, 'counters/updates': 4581}
skipping logging after 146624 examples to avoid logging too frequently
train stats after 146656 examples: {'rewards_train/chosen': '0.23696', 'rewards_train/rejected': '0.073602', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16335', 'logps_train/rejected': '-105.71', 'logps_train/chosen': '-109.42', 'loss/train': '0.62556', 'examples_per_second': '32.489', 'grad_norm': '16.75', 'counters/examples': 146656, 'counters/updates': 4583}
train stats after 146688 examples: {'rewards_train/chosen': '0.19365', 'rewards_train/rejected': '0.033055', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16059', 'logps_train/rejected': '-124.16', 'logps_train/chosen': '-128.5', 'loss/train': '0.63465', 'examples_per_second': '30.952', 'grad_norm': '19.25', 'counters/examples': 146688, 'counters/updates': 4584}
train stats after 146720 examples: {'rewards_train/chosen': '0.12336', 'rewards_train/rejected': '0.10732', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016044', 'logps_train/rejected': '-160.18', 'logps_train/chosen': '-145', 'loss/train': '0.69619', 'examples_per_second': '31.416', 'grad_norm': '23.5', 'counters/examples': 146720, 'counters/updates': 4585}
train stats after 146752 examples: {'rewards_train/chosen': '0.14112', 'rewards_train/rejected': '0.041175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099943', 'logps_train/rejected': '-122.52', 'logps_train/chosen': '-135.35', 'loss/train': '0.65475', 'examples_per_second': '30.755', 'grad_norm': '20.125', 'counters/examples': 146752, 'counters/updates': 4586}
skipping logging after 146784 examples to avoid logging too frequently
train stats after 146816 examples: {'rewards_train/chosen': '0.12515', 'rewards_train/rejected': '0.055109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070041', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-122.07', 'loss/train': '0.66886', 'examples_per_second': '31.622', 'grad_norm': '19.375', 'counters/examples': 146816, 'counters/updates': 4588}
skipping logging after 146848 examples to avoid logging too frequently
train stats after 146880 examples: {'rewards_train/chosen': '0.10405', 'rewards_train/rejected': '0.01943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084618', 'logps_train/rejected': '-156.18', 'logps_train/chosen': '-142.27', 'loss/train': '0.66478', 'examples_per_second': '31.027', 'grad_norm': '20.875', 'counters/examples': 146880, 'counters/updates': 4590}
train stats after 146912 examples: {'rewards_train/chosen': '0.046614', 'rewards_train/rejected': '0.062769', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016155', 'logps_train/rejected': '-155.82', 'logps_train/chosen': '-137.19', 'loss/train': '0.7118', 'examples_per_second': '31.487', 'grad_norm': '23.5', 'counters/examples': 146912, 'counters/updates': 4591}
skipping logging after 146944 examples to avoid logging too frequently
train stats after 146976 examples: {'rewards_train/chosen': '0.085073', 'rewards_train/rejected': '-0.014601', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099674', 'logps_train/rejected': '-71.52', 'logps_train/chosen': '-87.382', 'loss/train': '0.65269', 'examples_per_second': '33.424', 'grad_norm': '15.188', 'counters/examples': 146976, 'counters/updates': 4593}
train stats after 147008 examples: {'rewards_train/chosen': '0.15242', 'rewards_train/rejected': '-0.044608', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19703', 'logps_train/rejected': '-151.68', 'logps_train/chosen': '-166.44', 'loss/train': '0.61728', 'examples_per_second': '31.476', 'grad_norm': '21.375', 'counters/examples': 147008, 'counters/updates': 4594}
train stats after 147040 examples: {'rewards_train/chosen': '0.16447', 'rewards_train/rejected': '0.081537', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082932', 'logps_train/rejected': '-121.16', 'logps_train/chosen': '-124.24', 'loss/train': '0.66101', 'examples_per_second': '30.549', 'grad_norm': '20.125', 'counters/examples': 147040, 'counters/updates': 4595}
train stats after 147072 examples: {'rewards_train/chosen': '0.23291', 'rewards_train/rejected': '-0.00102', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23393', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-146.61', 'loss/train': '0.60099', 'examples_per_second': '31.468', 'grad_norm': '20.625', 'counters/examples': 147072, 'counters/updates': 4596}
train stats after 147104 examples: {'rewards_train/chosen': '0.21035', 'rewards_train/rejected': '0.097805', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11255', 'logps_train/rejected': '-114.94', 'logps_train/chosen': '-130.86', 'loss/train': '0.65246', 'examples_per_second': '31.563', 'grad_norm': '18.375', 'counters/examples': 147104, 'counters/updates': 4597}
train stats after 147136 examples: {'rewards_train/chosen': '0.17502', 'rewards_train/rejected': '0.0023967', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17262', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-147.82', 'loss/train': '0.62224', 'examples_per_second': '32.479', 'grad_norm': '20.125', 'counters/examples': 147136, 'counters/updates': 4598}
train stats after 147168 examples: {'rewards_train/chosen': '0.13552', 'rewards_train/rejected': '0.074224', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0613', 'logps_train/rejected': '-129.06', 'logps_train/chosen': '-118.67', 'loss/train': '0.66961', 'examples_per_second': '30.968', 'grad_norm': '19.25', 'counters/examples': 147168, 'counters/updates': 4599}
train stats after 147200 examples: {'rewards_train/chosen': '0.13277', 'rewards_train/rejected': '0.071099', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061673', 'logps_train/rejected': '-130.23', 'logps_train/chosen': '-134.17', 'loss/train': '0.67209', 'examples_per_second': '30.057', 'grad_norm': '21.125', 'counters/examples': 147200, 'counters/updates': 4600}
train stats after 147232 examples: {'rewards_train/chosen': '0.2276', 'rewards_train/rejected': '0.1102', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11741', 'logps_train/rejected': '-111.7', 'logps_train/chosen': '-166.58', 'loss/train': '0.65223', 'examples_per_second': '31.529', 'grad_norm': '20.625', 'counters/examples': 147232, 'counters/updates': 4601}
train stats after 147264 examples: {'rewards_train/chosen': '0.1936', 'rewards_train/rejected': '0.072004', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12159', 'logps_train/rejected': '-107.96', 'logps_train/chosen': '-131.17', 'loss/train': '0.6476', 'examples_per_second': '32.806', 'grad_norm': '19.5', 'counters/examples': 147264, 'counters/updates': 4602}
train stats after 147296 examples: {'rewards_train/chosen': '0.16734', 'rewards_train/rejected': '0.025175', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14217', 'logps_train/rejected': '-79.95', 'logps_train/chosen': '-141.08', 'loss/train': '0.63613', 'examples_per_second': '32.359', 'grad_norm': '17.75', 'counters/examples': 147296, 'counters/updates': 4603}
train stats after 147328 examples: {'rewards_train/chosen': '0.10947', 'rewards_train/rejected': '0.0084153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10105', 'logps_train/rejected': '-105.17', 'logps_train/chosen': '-120.39', 'loss/train': '0.65663', 'examples_per_second': '31.554', 'grad_norm': '18.75', 'counters/examples': 147328, 'counters/updates': 4604}
train stats after 147360 examples: {'rewards_train/chosen': '0.22241', 'rewards_train/rejected': '0.072125', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15028', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-150.2', 'loss/train': '0.63046', 'examples_per_second': '31.529', 'grad_norm': '21.125', 'counters/examples': 147360, 'counters/updates': 4605}
skipping logging after 147392 examples to avoid logging too frequently
train stats after 147424 examples: {'rewards_train/chosen': '0.18671', 'rewards_train/rejected': '0.078007', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1087', 'logps_train/rejected': '-206.09', 'logps_train/chosen': '-170.53', 'loss/train': '0.65874', 'examples_per_second': '31.526', 'grad_norm': '24.125', 'counters/examples': 147424, 'counters/updates': 4607}
train stats after 147456 examples: {'rewards_train/chosen': '0.18559', 'rewards_train/rejected': '0.076872', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10872', 'logps_train/rejected': '-173.63', 'logps_train/chosen': '-179.86', 'loss/train': '0.65522', 'examples_per_second': '31.493', 'grad_norm': '25.25', 'counters/examples': 147456, 'counters/updates': 4608}
train stats after 147488 examples: {'rewards_train/chosen': '0.10234', 'rewards_train/rejected': '-0.028045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13038', 'logps_train/rejected': '-78.557', 'logps_train/chosen': '-89.784', 'loss/train': '0.63758', 'examples_per_second': '30.616', 'grad_norm': '16.125', 'counters/examples': 147488, 'counters/updates': 4609}
train stats after 147520 examples: {'rewards_train/chosen': '0.15634', 'rewards_train/rejected': '0.03652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11982', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-132.3', 'loss/train': '0.65245', 'examples_per_second': '30.266', 'grad_norm': '20', 'counters/examples': 147520, 'counters/updates': 4610}
skipping logging after 147552 examples to avoid logging too frequently
train stats after 147584 examples: {'rewards_train/chosen': '0.20619', 'rewards_train/rejected': '0.098558', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10763', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-162.23', 'loss/train': '0.65616', 'examples_per_second': '31.353', 'grad_norm': '22.125', 'counters/examples': 147584, 'counters/updates': 4612}
train stats after 147616 examples: {'rewards_train/chosen': '0.26663', 'rewards_train/rejected': '0.139', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12763', 'logps_train/rejected': '-184.91', 'logps_train/chosen': '-158.31', 'loss/train': '0.64445', 'examples_per_second': '31.49', 'grad_norm': '22.875', 'counters/examples': 147616, 'counters/updates': 4613}
skipping logging after 147648 examples to avoid logging too frequently
train stats after 147680 examples: {'rewards_train/chosen': '0.21094', 'rewards_train/rejected': '0.12213', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088815', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-167', 'loss/train': '0.66128', 'examples_per_second': '36.106', 'grad_norm': '20.625', 'counters/examples': 147680, 'counters/updates': 4615}
train stats after 147712 examples: {'rewards_train/chosen': '0.14431', 'rewards_train/rejected': '0.01384', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13047', 'logps_train/rejected': '-101.32', 'logps_train/chosen': '-156.69', 'loss/train': '0.6403', 'examples_per_second': '30.877', 'grad_norm': '21.625', 'counters/examples': 147712, 'counters/updates': 4616}
train stats after 147744 examples: {'rewards_train/chosen': '0.28157', 'rewards_train/rejected': '-0.0017612', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.28333', 'logps_train/rejected': '-157.98', 'logps_train/chosen': '-181.03', 'loss/train': '0.57501', 'examples_per_second': '31.543', 'grad_norm': '22.25', 'counters/examples': 147744, 'counters/updates': 4617}
train stats after 147776 examples: {'rewards_train/chosen': '0.14353', 'rewards_train/rejected': '0.12672', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016807', 'logps_train/rejected': '-112.58', 'logps_train/chosen': '-132.89', 'loss/train': '0.69083', 'examples_per_second': '31.371', 'grad_norm': '20.75', 'counters/examples': 147776, 'counters/updates': 4618}
train stats after 147808 examples: {'rewards_train/chosen': '0.17731', 'rewards_train/rejected': '0.06715', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11016', 'logps_train/rejected': '-116.26', 'logps_train/chosen': '-148.67', 'loss/train': '0.65496', 'examples_per_second': '32.278', 'grad_norm': '20.375', 'counters/examples': 147808, 'counters/updates': 4619}
train stats after 147840 examples: {'rewards_train/chosen': '0.18089', 'rewards_train/rejected': '0.062192', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1187', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-153.2', 'loss/train': '0.64301', 'examples_per_second': '31.257', 'grad_norm': '20', 'counters/examples': 147840, 'counters/updates': 4620}
train stats after 147872 examples: {'rewards_train/chosen': '0.214', 'rewards_train/rejected': '0.088459', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12554', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-137.25', 'loss/train': '0.6473', 'examples_per_second': '30.022', 'grad_norm': '20.375', 'counters/examples': 147872, 'counters/updates': 4621}
train stats after 147904 examples: {'rewards_train/chosen': '0.15508', 'rewards_train/rejected': '0.0070661', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14801', 'logps_train/rejected': '-96.446', 'logps_train/chosen': '-118.08', 'loss/train': '0.63739', 'examples_per_second': '31.532', 'grad_norm': '18', 'counters/examples': 147904, 'counters/updates': 4622}
train stats after 147936 examples: {'rewards_train/chosen': '0.17189', 'rewards_train/rejected': '0.030681', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1412', 'logps_train/rejected': '-114.61', 'logps_train/chosen': '-132.75', 'loss/train': '0.63657', 'examples_per_second': '31.546', 'grad_norm': '18.5', 'counters/examples': 147936, 'counters/updates': 4623}
train stats after 147968 examples: {'rewards_train/chosen': '0.099449', 'rewards_train/rejected': '0.044897', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054552', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-114.56', 'loss/train': '0.67317', 'examples_per_second': '32.221', 'grad_norm': '19.625', 'counters/examples': 147968, 'counters/updates': 4624}
train stats after 148000 examples: {'rewards_train/chosen': '0.21264', 'rewards_train/rejected': '0.060699', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15194', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-172.03', 'loss/train': '0.63654', 'examples_per_second': '31.283', 'grad_norm': '19.75', 'counters/examples': 148000, 'counters/updates': 4625}
train stats after 148032 examples: {'rewards_train/chosen': '0.24558', 'rewards_train/rejected': '0.058153', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18742', 'logps_train/rejected': '-115.05', 'logps_train/chosen': '-138.04', 'loss/train': '0.61747', 'examples_per_second': '32.219', 'grad_norm': '19.375', 'counters/examples': 148032, 'counters/updates': 4626}
train stats after 148064 examples: {'rewards_train/chosen': '0.15115', 'rewards_train/rejected': '0.075137', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076016', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-127.88', 'loss/train': '0.6833', 'examples_per_second': '31.419', 'grad_norm': '20', 'counters/examples': 148064, 'counters/updates': 4627}
skipping logging after 148096 examples to avoid logging too frequently
train stats after 148128 examples: {'rewards_train/chosen': '0.19257', 'rewards_train/rejected': '0.028349', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16423', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-150.33', 'loss/train': '0.62942', 'examples_per_second': '31.67', 'grad_norm': '20', 'counters/examples': 148128, 'counters/updates': 4629}
train stats after 148160 examples: {'rewards_train/chosen': '0.092857', 'rewards_train/rejected': '0.030957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0619', 'logps_train/rejected': '-110.34', 'logps_train/chosen': '-133.12', 'loss/train': '0.67054', 'examples_per_second': '32.267', 'grad_norm': '19', 'counters/examples': 148160, 'counters/updates': 4630}
skipping logging after 148192 examples to avoid logging too frequently
train stats after 148224 examples: {'rewards_train/chosen': '0.1589', 'rewards_train/rejected': '0.085565', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073331', 'logps_train/rejected': '-105.94', 'logps_train/chosen': '-110.4', 'loss/train': '0.66938', 'examples_per_second': '33.103', 'grad_norm': '17.875', 'counters/examples': 148224, 'counters/updates': 4632}
train stats after 148256 examples: {'rewards_train/chosen': '0.15432', 'rewards_train/rejected': '0.064387', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089934', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-121.05', 'loss/train': '0.6565', 'examples_per_second': '30.341', 'grad_norm': '19.375', 'counters/examples': 148256, 'counters/updates': 4633}
skipping logging after 148288 examples to avoid logging too frequently
train stats after 148320 examples: {'rewards_train/chosen': '0.16671', 'rewards_train/rejected': '0.089835', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076873', 'logps_train/rejected': '-101.55', 'logps_train/chosen': '-128.23', 'loss/train': '0.66406', 'examples_per_second': '31.135', 'grad_norm': '18.375', 'counters/examples': 148320, 'counters/updates': 4635}
skipping logging after 148352 examples to avoid logging too frequently
train stats after 148384 examples: {'rewards_train/chosen': '0.13633', 'rewards_train/rejected': '0.0087256', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12761', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-125.3', 'loss/train': '0.63953', 'examples_per_second': '31.533', 'grad_norm': '19.25', 'counters/examples': 148384, 'counters/updates': 4637}
train stats after 148416 examples: {'rewards_train/chosen': '0.11779', 'rewards_train/rejected': '0.13317', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015373', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-143.31', 'loss/train': '0.70987', 'examples_per_second': '30.533', 'grad_norm': '21.5', 'counters/examples': 148416, 'counters/updates': 4638}
skipping logging after 148448 examples to avoid logging too frequently
train stats after 148480 examples: {'rewards_train/chosen': '0.096924', 'rewards_train/rejected': '0.035964', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06096', 'logps_train/rejected': '-106.04', 'logps_train/chosen': '-99.109', 'loss/train': '0.66939', 'examples_per_second': '33.483', 'grad_norm': '17.625', 'counters/examples': 148480, 'counters/updates': 4640}
train stats after 148512 examples: {'rewards_train/chosen': '0.18513', 'rewards_train/rejected': '0.089985', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095147', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-175.42', 'loss/train': '0.65736', 'examples_per_second': '31.227', 'grad_norm': '22', 'counters/examples': 148512, 'counters/updates': 4641}
train stats after 148544 examples: {'rewards_train/chosen': '0.15613', 'rewards_train/rejected': '0.042355', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11377', 'logps_train/rejected': '-114.85', 'logps_train/chosen': '-132.06', 'loss/train': '0.65351', 'examples_per_second': '32.861', 'grad_norm': '19', 'counters/examples': 148544, 'counters/updates': 4642}
train stats after 148576 examples: {'rewards_train/chosen': '0.17373', 'rewards_train/rejected': '0.1137', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06003', 'logps_train/rejected': '-102.23', 'logps_train/chosen': '-134.55', 'loss/train': '0.67023', 'examples_per_second': '30.485', 'grad_norm': '20', 'counters/examples': 148576, 'counters/updates': 4643}
train stats after 148608 examples: {'rewards_train/chosen': '0.099748', 'rewards_train/rejected': '0.10616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0064071', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-150.71', 'loss/train': '0.70781', 'examples_per_second': '31.688', 'grad_norm': '22.125', 'counters/examples': 148608, 'counters/updates': 4644}
train stats after 148640 examples: {'rewards_train/chosen': '0.11642', 'rewards_train/rejected': '0.055832', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060588', 'logps_train/rejected': '-95.948', 'logps_train/chosen': '-133.47', 'loss/train': '0.67152', 'examples_per_second': '31.805', 'grad_norm': '17.875', 'counters/examples': 148640, 'counters/updates': 4645}
train stats after 148672 examples: {'rewards_train/chosen': '0.2513', 'rewards_train/rejected': '0.046334', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20497', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-134.7', 'loss/train': '0.61196', 'examples_per_second': '32.477', 'grad_norm': '19.25', 'counters/examples': 148672, 'counters/updates': 4646}
skipping logging after 148704 examples to avoid logging too frequently
train stats after 148736 examples: {'rewards_train/chosen': '0.0804', 'rewards_train/rejected': '0.066924', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013476', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-136.7', 'loss/train': '0.69525', 'examples_per_second': '34.142', 'grad_norm': '21', 'counters/examples': 148736, 'counters/updates': 4648}
train stats after 148768 examples: {'rewards_train/chosen': '0.068697', 'rewards_train/rejected': '0.02394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044757', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-133.26', 'loss/train': '0.67993', 'examples_per_second': '32.793', 'grad_norm': '21', 'counters/examples': 148768, 'counters/updates': 4649}
train stats after 148800 examples: {'rewards_train/chosen': '0.16131', 'rewards_train/rejected': '0.063102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098208', 'logps_train/rejected': '-95.05', 'logps_train/chosen': '-102.69', 'loss/train': '0.65351', 'examples_per_second': '32.48', 'grad_norm': '16.625', 'counters/examples': 148800, 'counters/updates': 4650}
train stats after 148832 examples: {'rewards_train/chosen': '0.06882', 'rewards_train/rejected': '0.044759', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02406', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-130.81', 'loss/train': '0.6957', 'examples_per_second': '32.084', 'grad_norm': '21.875', 'counters/examples': 148832, 'counters/updates': 4651}
train stats after 148864 examples: {'rewards_train/chosen': '0.15203', 'rewards_train/rejected': '0.0653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086727', 'logps_train/rejected': '-111.56', 'logps_train/chosen': '-118.95', 'loss/train': '0.65466', 'examples_per_second': '30.741', 'grad_norm': '18.625', 'counters/examples': 148864, 'counters/updates': 4652}
train stats after 148896 examples: {'rewards_train/chosen': '0.14018', 'rewards_train/rejected': '0.044191', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095989', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-123.78', 'loss/train': '0.65377', 'examples_per_second': '31.538', 'grad_norm': '20.875', 'counters/examples': 148896, 'counters/updates': 4653}
skipping logging after 148928 examples to avoid logging too frequently
train stats after 148960 examples: {'rewards_train/chosen': '0.11554', 'rewards_train/rejected': '0.10685', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0086914', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-141.97', 'loss/train': '0.69998', 'examples_per_second': '30.517', 'grad_norm': '21.375', 'counters/examples': 148960, 'counters/updates': 4655}
train stats after 148992 examples: {'rewards_train/chosen': '0.097457', 'rewards_train/rejected': '0.0084791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088978', 'logps_train/rejected': '-91.711', 'logps_train/chosen': '-145.21', 'loss/train': '0.66581', 'examples_per_second': '31.107', 'grad_norm': '19.625', 'counters/examples': 148992, 'counters/updates': 4656}
train stats after 149024 examples: {'rewards_train/chosen': '0.20812', 'rewards_train/rejected': '0.078354', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12977', 'logps_train/rejected': '-129.51', 'logps_train/chosen': '-148.3', 'loss/train': '0.64234', 'examples_per_second': '33.199', 'grad_norm': '20.25', 'counters/examples': 149024, 'counters/updates': 4657}
train stats after 149056 examples: {'rewards_train/chosen': '0.11004', 'rewards_train/rejected': '0.020077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089964', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-162.02', 'loss/train': '0.65931', 'examples_per_second': '31.598', 'grad_norm': '21.125', 'counters/examples': 149056, 'counters/updates': 4658}
train stats after 149088 examples: {'rewards_train/chosen': '0.16834', 'rewards_train/rejected': '-0.010003', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17835', 'logps_train/rejected': '-127.63', 'logps_train/chosen': '-138.35', 'loss/train': '0.62442', 'examples_per_second': '31.637', 'grad_norm': '20.75', 'counters/examples': 149088, 'counters/updates': 4659}
train stats after 149120 examples: {'rewards_train/chosen': '0.094114', 'rewards_train/rejected': '0.040453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053661', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-142.46', 'loss/train': '0.67689', 'examples_per_second': '31.239', 'grad_norm': '20.625', 'counters/examples': 149120, 'counters/updates': 4660}
train stats after 149152 examples: {'rewards_train/chosen': '0.16696', 'rewards_train/rejected': '0.027903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13905', 'logps_train/rejected': '-116.27', 'logps_train/chosen': '-152.16', 'loss/train': '0.64142', 'examples_per_second': '32.537', 'grad_norm': '20.625', 'counters/examples': 149152, 'counters/updates': 4661}
skipping logging after 149184 examples to avoid logging too frequently
train stats after 149216 examples: {'rewards_train/chosen': '0.11717', 'rewards_train/rejected': '0.011541', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10563', 'logps_train/rejected': '-97.695', 'logps_train/chosen': '-115.28', 'loss/train': '0.66401', 'examples_per_second': '23.803', 'grad_norm': '18.875', 'counters/examples': 149216, 'counters/updates': 4663}
train stats after 149248 examples: {'rewards_train/chosen': '0.20186', 'rewards_train/rejected': '0.045663', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1562', 'logps_train/rejected': '-110.15', 'logps_train/chosen': '-147.09', 'loss/train': '0.62815', 'examples_per_second': '32.226', 'grad_norm': '19.75', 'counters/examples': 149248, 'counters/updates': 4664}
train stats after 149280 examples: {'rewards_train/chosen': '0.11314', 'rewards_train/rejected': '-0.057816', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17096', 'logps_train/rejected': '-84.275', 'logps_train/chosen': '-113.85', 'loss/train': '0.62223', 'examples_per_second': '31.889', 'grad_norm': '17.625', 'counters/examples': 149280, 'counters/updates': 4665}
train stats after 149312 examples: {'rewards_train/chosen': '0.067086', 'rewards_train/rejected': '0.005462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061624', 'logps_train/rejected': '-100.05', 'logps_train/chosen': '-117.84', 'loss/train': '0.66894', 'examples_per_second': '31.828', 'grad_norm': '19.25', 'counters/examples': 149312, 'counters/updates': 4666}
train stats after 149344 examples: {'rewards_train/chosen': '0.12156', 'rewards_train/rejected': '0.09151', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030049', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-118.31', 'loss/train': '0.6848', 'examples_per_second': '32.702', 'grad_norm': '18.5', 'counters/examples': 149344, 'counters/updates': 4667}
skipping logging after 149376 examples to avoid logging too frequently
train stats after 149408 examples: {'rewards_train/chosen': '0.17594', 'rewards_train/rejected': '0.020524', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15541', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-134.61', 'loss/train': '0.63013', 'examples_per_second': '30.672', 'grad_norm': '19.75', 'counters/examples': 149408, 'counters/updates': 4669}
train stats after 149440 examples: {'rewards_train/chosen': '0.19692', 'rewards_train/rejected': '0.066028', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13089', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-119.71', 'loss/train': '0.64351', 'examples_per_second': '32.343', 'grad_norm': '19.875', 'counters/examples': 149440, 'counters/updates': 4670}
train stats after 149472 examples: {'rewards_train/chosen': '0.11024', 'rewards_train/rejected': '0.10422', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0060204', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-121.15', 'loss/train': '0.70621', 'examples_per_second': '30.317', 'grad_norm': '21.125', 'counters/examples': 149472, 'counters/updates': 4671}
train stats after 149504 examples: {'rewards_train/chosen': '0.11767', 'rewards_train/rejected': '0.13565', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017981', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-123.85', 'loss/train': '0.70928', 'examples_per_second': '31.656', 'grad_norm': '20.25', 'counters/examples': 149504, 'counters/updates': 4672}
train stats after 149536 examples: {'rewards_train/chosen': '0.10544', 'rewards_train/rejected': '0.090422', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015017', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-125.63', 'loss/train': '0.69378', 'examples_per_second': '31.701', 'grad_norm': '20', 'counters/examples': 149536, 'counters/updates': 4673}
train stats after 149568 examples: {'rewards_train/chosen': '0.21031', 'rewards_train/rejected': '0.080034', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13028', 'logps_train/rejected': '-136.83', 'logps_train/chosen': '-153.89', 'loss/train': '0.64104', 'examples_per_second': '31.568', 'grad_norm': '21', 'counters/examples': 149568, 'counters/updates': 4674}
train stats after 149600 examples: {'rewards_train/chosen': '0.16146', 'rewards_train/rejected': '0.10993', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051537', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-131.59', 'loss/train': '0.6759', 'examples_per_second': '31.675', 'grad_norm': '20.5', 'counters/examples': 149600, 'counters/updates': 4675}
skipping logging after 149632 examples to avoid logging too frequently
train stats after 149664 examples: {'rewards_train/chosen': '0.1779', 'rewards_train/rejected': '0.018059', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15984', 'logps_train/rejected': '-158.86', 'logps_train/chosen': '-130.01', 'loss/train': '0.62453', 'examples_per_second': '37.217', 'grad_norm': '19.875', 'counters/examples': 149664, 'counters/updates': 4677}
skipping logging after 149696 examples to avoid logging too frequently
train stats after 149728 examples: {'rewards_train/chosen': '0.13866', 'rewards_train/rejected': '0.075215', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063442', 'logps_train/rejected': '-114.32', 'logps_train/chosen': '-131.21', 'loss/train': '0.67857', 'examples_per_second': '32.21', 'grad_norm': '19.375', 'counters/examples': 149728, 'counters/updates': 4679}
train stats after 149760 examples: {'rewards_train/chosen': '0.073251', 'rewards_train/rejected': '0.015749', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057501', 'logps_train/rejected': '-102.95', 'logps_train/chosen': '-146.09', 'loss/train': '0.67045', 'examples_per_second': '30.149', 'grad_norm': '19.625', 'counters/examples': 149760, 'counters/updates': 4680}
train stats after 149792 examples: {'rewards_train/chosen': '0.18262', 'rewards_train/rejected': '0.081434', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10119', 'logps_train/rejected': '-96.907', 'logps_train/chosen': '-125.52', 'loss/train': '0.64981', 'examples_per_second': '30.895', 'grad_norm': '19.75', 'counters/examples': 149792, 'counters/updates': 4681}
train stats after 149824 examples: {'rewards_train/chosen': '0.15706', 'rewards_train/rejected': '0.11805', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039011', 'logps_train/rejected': '-156.68', 'logps_train/chosen': '-159.2', 'loss/train': '0.68248', 'examples_per_second': '32.03', 'grad_norm': '22.375', 'counters/examples': 149824, 'counters/updates': 4682}
train stats after 149856 examples: {'rewards_train/chosen': '0.16676', 'rewards_train/rejected': '0.11246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054309', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-159.52', 'loss/train': '0.67975', 'examples_per_second': '31.254', 'grad_norm': '22.5', 'counters/examples': 149856, 'counters/updates': 4683}
train stats after 149888 examples: {'rewards_train/chosen': '0.14184', 'rewards_train/rejected': '0.12058', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021253', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-145.64', 'loss/train': '0.69718', 'examples_per_second': '32.235', 'grad_norm': '22.125', 'counters/examples': 149888, 'counters/updates': 4684}
skipping logging after 149920 examples to avoid logging too frequently
train stats after 149952 examples: {'rewards_train/chosen': '0.23491', 'rewards_train/rejected': '0.076859', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15805', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-118.38', 'loss/train': '0.63131', 'examples_per_second': '33.795', 'grad_norm': '18.625', 'counters/examples': 149952, 'counters/updates': 4686}
train stats after 149984 examples: {'rewards_train/chosen': '0.15561', 'rewards_train/rejected': '-0.0024958', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15811', 'logps_train/rejected': '-77.423', 'logps_train/chosen': '-158.58', 'loss/train': '0.6251', 'examples_per_second': '31.3', 'grad_norm': '19.125', 'counters/examples': 149984, 'counters/updates': 4687}
train stats after 150016 examples: {'rewards_train/chosen': '0.098733', 'rewards_train/rejected': '0.061263', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03747', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-136.13', 'loss/train': '0.69115', 'examples_per_second': '30.742', 'grad_norm': '21.5', 'counters/examples': 150016, 'counters/updates': 4688}
train stats after 150048 examples: {'rewards_train/chosen': '0.16961', 'rewards_train/rejected': '0.062946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10666', 'logps_train/rejected': '-116.02', 'logps_train/chosen': '-132.88', 'loss/train': '0.65359', 'examples_per_second': '31.336', 'grad_norm': '19.875', 'counters/examples': 150048, 'counters/updates': 4689}
skipping logging after 150080 examples to avoid logging too frequently
train stats after 150112 examples: {'rewards_train/chosen': '0.13073', 'rewards_train/rejected': '0.03846', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.092267', 'logps_train/rejected': '-85.841', 'logps_train/chosen': '-149.51', 'loss/train': '0.65664', 'examples_per_second': '33.273', 'grad_norm': '19.5', 'counters/examples': 150112, 'counters/updates': 4691}
train stats after 150144 examples: {'rewards_train/chosen': '0.21525', 'rewards_train/rejected': '0.025663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18959', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-139.64', 'loss/train': '0.62144', 'examples_per_second': '32.773', 'grad_norm': '19.25', 'counters/examples': 150144, 'counters/updates': 4692}
train stats after 150176 examples: {'rewards_train/chosen': '0.11121', 'rewards_train/rejected': '0.099776', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011434', 'logps_train/rejected': '-126', 'logps_train/chosen': '-117.92', 'loss/train': '0.69643', 'examples_per_second': '33.061', 'grad_norm': '20', 'counters/examples': 150176, 'counters/updates': 4693}
skipping logging after 150208 examples to avoid logging too frequently
train stats after 150240 examples: {'rewards_train/chosen': '0.16905', 'rewards_train/rejected': '0.059875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10917', 'logps_train/rejected': '-153.19', 'logps_train/chosen': '-147.89', 'loss/train': '0.65836', 'examples_per_second': '31.806', 'grad_norm': '21.625', 'counters/examples': 150240, 'counters/updates': 4695}
train stats after 150272 examples: {'rewards_train/chosen': '0.27825', 'rewards_train/rejected': '0.18018', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098079', 'logps_train/rejected': '-106.22', 'logps_train/chosen': '-122.34', 'loss/train': '0.66306', 'examples_per_second': '30.65', 'grad_norm': '19.75', 'counters/examples': 150272, 'counters/updates': 4696}
train stats after 150304 examples: {'rewards_train/chosen': '0.19955', 'rewards_train/rejected': '0.035239', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16432', 'logps_train/rejected': '-101.86', 'logps_train/chosen': '-143.69', 'loss/train': '0.62688', 'examples_per_second': '31.662', 'grad_norm': '19.25', 'counters/examples': 150304, 'counters/updates': 4697}
train stats after 150336 examples: {'rewards_train/chosen': '0.10969', 'rewards_train/rejected': '-0.025179', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13487', 'logps_train/rejected': '-104.08', 'logps_train/chosen': '-110.78', 'loss/train': '0.63577', 'examples_per_second': '31.546', 'grad_norm': '17.125', 'counters/examples': 150336, 'counters/updates': 4698}
skipping logging after 150368 examples to avoid logging too frequently
train stats after 150400 examples: {'rewards_train/chosen': '0.11811', 'rewards_train/rejected': '0.11405', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0040577', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-118.52', 'loss/train': '0.71051', 'examples_per_second': '39.204', 'grad_norm': '20.25', 'counters/examples': 150400, 'counters/updates': 4700}
train stats after 150432 examples: {'rewards_train/chosen': '0.1069', 'rewards_train/rejected': '0.024866', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.082033', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-117.56', 'loss/train': '0.66885', 'examples_per_second': '31.276', 'grad_norm': '20.125', 'counters/examples': 150432, 'counters/updates': 4701}
train stats after 150464 examples: {'rewards_train/chosen': '0.14095', 'rewards_train/rejected': '0.041394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099556', 'logps_train/rejected': '-95.325', 'logps_train/chosen': '-147.08', 'loss/train': '0.65043', 'examples_per_second': '30.656', 'grad_norm': '19.875', 'counters/examples': 150464, 'counters/updates': 4702}
train stats after 150496 examples: {'rewards_train/chosen': '0.10982', 'rewards_train/rejected': '0.039833', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069987', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-124.64', 'loss/train': '0.67442', 'examples_per_second': '31.657', 'grad_norm': '21.125', 'counters/examples': 150496, 'counters/updates': 4703}
skipping logging after 150528 examples to avoid logging too frequently
train stats after 150560 examples: {'rewards_train/chosen': '0.19227', 'rewards_train/rejected': '0.011201', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18107', 'logps_train/rejected': '-139', 'logps_train/chosen': '-158.81', 'loss/train': '0.6274', 'examples_per_second': '31.647', 'grad_norm': '21.5', 'counters/examples': 150560, 'counters/updates': 4705}
skipping logging after 150592 examples to avoid logging too frequently
train stats after 150624 examples: {'rewards_train/chosen': '0.09823', 'rewards_train/rejected': '0.04144', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05679', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-129.15', 'loss/train': '0.67542', 'examples_per_second': '31.635', 'grad_norm': '20.375', 'counters/examples': 150624, 'counters/updates': 4707}
train stats after 150656 examples: {'rewards_train/chosen': '0.14803', 'rewards_train/rejected': '0.11831', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029715', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-164.8', 'loss/train': '0.6916', 'examples_per_second': '31.42', 'grad_norm': '22', 'counters/examples': 150656, 'counters/updates': 4708}
train stats after 150688 examples: {'rewards_train/chosen': '0.11359', 'rewards_train/rejected': '0.036868', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076723', 'logps_train/rejected': '-93.054', 'logps_train/chosen': '-135.76', 'loss/train': '0.67763', 'examples_per_second': '30.775', 'grad_norm': '20.75', 'counters/examples': 150688, 'counters/updates': 4709}
train stats after 150720 examples: {'rewards_train/chosen': '0.15424', 'rewards_train/rejected': '0.039005', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11523', 'logps_train/rejected': '-138.79', 'logps_train/chosen': '-152.1', 'loss/train': '0.66079', 'examples_per_second': '31.034', 'grad_norm': '23.875', 'counters/examples': 150720, 'counters/updates': 4710}
train stats after 150752 examples: {'rewards_train/chosen': '0.22392', 'rewards_train/rejected': '-0.03081', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.25473', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-146.75', 'loss/train': '0.58617', 'examples_per_second': '31.322', 'grad_norm': '19.25', 'counters/examples': 150752, 'counters/updates': 4711}
train stats after 150784 examples: {'rewards_train/chosen': '0.16927', 'rewards_train/rejected': '0.013589', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15568', 'logps_train/rejected': '-146.25', 'logps_train/chosen': '-140.27', 'loss/train': '0.63538', 'examples_per_second': '31.666', 'grad_norm': '21', 'counters/examples': 150784, 'counters/updates': 4712}
train stats after 150816 examples: {'rewards_train/chosen': '0.14343', 'rewards_train/rejected': '0.12596', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017473', 'logps_train/rejected': '-140.05', 'logps_train/chosen': '-146.71', 'loss/train': '0.69096', 'examples_per_second': '33.125', 'grad_norm': '21.125', 'counters/examples': 150816, 'counters/updates': 4713}
train stats after 150848 examples: {'rewards_train/chosen': '0.14302', 'rewards_train/rejected': '0.028341', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11468', 'logps_train/rejected': '-86.147', 'logps_train/chosen': '-141.92', 'loss/train': '0.65175', 'examples_per_second': '31.69', 'grad_norm': '17.875', 'counters/examples': 150848, 'counters/updates': 4714}
train stats after 150880 examples: {'rewards_train/chosen': '0.17986', 'rewards_train/rejected': '0.02968', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15018', 'logps_train/rejected': '-100.08', 'logps_train/chosen': '-134.74', 'loss/train': '0.63226', 'examples_per_second': '32.514', 'grad_norm': '18.375', 'counters/examples': 150880, 'counters/updates': 4715}
train stats after 150912 examples: {'rewards_train/chosen': '0.10311', 'rewards_train/rejected': '0.062913', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040196', 'logps_train/rejected': '-104.2', 'logps_train/chosen': '-129.29', 'loss/train': '0.68715', 'examples_per_second': '30.813', 'grad_norm': '21', 'counters/examples': 150912, 'counters/updates': 4716}
train stats after 150944 examples: {'rewards_train/chosen': '0.16424', 'rewards_train/rejected': '-0.011535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17578', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-131.37', 'loss/train': '0.62274', 'examples_per_second': '33.127', 'grad_norm': '19.75', 'counters/examples': 150944, 'counters/updates': 4717}
train stats after 150976 examples: {'rewards_train/chosen': '0.17024', 'rewards_train/rejected': '0.040526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12972', 'logps_train/rejected': '-152.1', 'logps_train/chosen': '-123.04', 'loss/train': '0.64851', 'examples_per_second': '30.67', 'grad_norm': '21.375', 'counters/examples': 150976, 'counters/updates': 4718}
skipping logging after 151008 examples to avoid logging too frequently
train stats after 151040 examples: {'rewards_train/chosen': '0.10202', 'rewards_train/rejected': '0.060906', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041119', 'logps_train/rejected': '-97.189', 'logps_train/chosen': '-143.94', 'loss/train': '0.68047', 'examples_per_second': '30.89', 'grad_norm': '19.875', 'counters/examples': 151040, 'counters/updates': 4720}
train stats after 151072 examples: {'rewards_train/chosen': '0.16305', 'rewards_train/rejected': '0.11995', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0431', 'logps_train/rejected': '-137.9', 'logps_train/chosen': '-115.05', 'loss/train': '0.67915', 'examples_per_second': '31.955', 'grad_norm': '20.5', 'counters/examples': 151072, 'counters/updates': 4721}
train stats after 151104 examples: {'rewards_train/chosen': '0.13705', 'rewards_train/rejected': '-0.0018482', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1389', 'logps_train/rejected': '-104.03', 'logps_train/chosen': '-113.44', 'loss/train': '0.63642', 'examples_per_second': '31.483', 'grad_norm': '18.25', 'counters/examples': 151104, 'counters/updates': 4722}
train stats after 151136 examples: {'rewards_train/chosen': '0.21526', 'rewards_train/rejected': '7.928e-05', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21518', 'logps_train/rejected': '-94.43', 'logps_train/chosen': '-128.4', 'loss/train': '0.60643', 'examples_per_second': '30.83', 'grad_norm': '18', 'counters/examples': 151136, 'counters/updates': 4723}
train stats after 151168 examples: {'rewards_train/chosen': '0.18538', 'rewards_train/rejected': '0.085342', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10003', 'logps_train/rejected': '-107.28', 'logps_train/chosen': '-147.52', 'loss/train': '0.65903', 'examples_per_second': '31.706', 'grad_norm': '20.25', 'counters/examples': 151168, 'counters/updates': 4724}
train stats after 151200 examples: {'rewards_train/chosen': '0.16442', 'rewards_train/rejected': '0.062817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10161', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-134.77', 'loss/train': '0.65704', 'examples_per_second': '31.755', 'grad_norm': '19.875', 'counters/examples': 151200, 'counters/updates': 4725}
train stats after 151232 examples: {'rewards_train/chosen': '0.10723', 'rewards_train/rejected': '0.043264', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063971', 'logps_train/rejected': '-105.21', 'logps_train/chosen': '-147.18', 'loss/train': '0.67681', 'examples_per_second': '32.009', 'grad_norm': '20.5', 'counters/examples': 151232, 'counters/updates': 4726}
train stats after 151264 examples: {'rewards_train/chosen': '0.086222', 'rewards_train/rejected': '0.076412', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0098103', 'logps_train/rejected': '-91.822', 'logps_train/chosen': '-116.51', 'loss/train': '0.6975', 'examples_per_second': '32.117', 'grad_norm': '19', 'counters/examples': 151264, 'counters/updates': 4727}
train stats after 151296 examples: {'rewards_train/chosen': '0.20979', 'rewards_train/rejected': '0.074341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13545', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-151.42', 'loss/train': '0.64734', 'examples_per_second': '30.977', 'grad_norm': '19.75', 'counters/examples': 151296, 'counters/updates': 4728}
train stats after 151328 examples: {'rewards_train/chosen': '0.11719', 'rewards_train/rejected': '0.056791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060399', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-134.31', 'loss/train': '0.67635', 'examples_per_second': '33.02', 'grad_norm': '21.5', 'counters/examples': 151328, 'counters/updates': 4729}
train stats after 151360 examples: {'rewards_train/chosen': '0.15535', 'rewards_train/rejected': '0.017604', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13775', 'logps_train/rejected': '-116.67', 'logps_train/chosen': '-122.86', 'loss/train': '0.63845', 'examples_per_second': '31.851', 'grad_norm': '18.75', 'counters/examples': 151360, 'counters/updates': 4730}
train stats after 151392 examples: {'rewards_train/chosen': '0.19221', 'rewards_train/rejected': '0.043246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14897', 'logps_train/rejected': '-106.67', 'logps_train/chosen': '-173.33', 'loss/train': '0.63841', 'examples_per_second': '30.232', 'grad_norm': '21', 'counters/examples': 151392, 'counters/updates': 4731}
train stats after 151424 examples: {'rewards_train/chosen': '0.11492', 'rewards_train/rejected': '-0.017743', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13267', 'logps_train/rejected': '-119.9', 'logps_train/chosen': '-161.8', 'loss/train': '0.63736', 'examples_per_second': '30.794', 'grad_norm': '22.875', 'counters/examples': 151424, 'counters/updates': 4732}
train stats after 151456 examples: {'rewards_train/chosen': '0.16324', 'rewards_train/rejected': '0.16542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0021809', 'logps_train/rejected': '-164.06', 'logps_train/chosen': '-132.07', 'loss/train': '0.70787', 'examples_per_second': '31.479', 'grad_norm': '23.5', 'counters/examples': 151456, 'counters/updates': 4733}
train stats after 151488 examples: {'rewards_train/chosen': '0.16661', 'rewards_train/rejected': '0.057526', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10909', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-131.21', 'loss/train': '0.65048', 'examples_per_second': '31.672', 'grad_norm': '19.25', 'counters/examples': 151488, 'counters/updates': 4734}
skipping logging after 151520 examples to avoid logging too frequently
train stats after 151552 examples: {'rewards_train/chosen': '0.1563', 'rewards_train/rejected': '-0.0034708', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15978', 'logps_train/rejected': '-99.96', 'logps_train/chosen': '-140.13', 'loss/train': '0.62962', 'examples_per_second': '32.029', 'grad_norm': '18', 'counters/examples': 151552, 'counters/updates': 4736}
train stats after 151584 examples: {'rewards_train/chosen': '0.15159', 'rewards_train/rejected': '0.11241', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039176', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-126.11', 'loss/train': '0.69191', 'examples_per_second': '30.514', 'grad_norm': '21.125', 'counters/examples': 151584, 'counters/updates': 4737}
train stats after 151616 examples: {'rewards_train/chosen': '0.18778', 'rewards_train/rejected': '0.05261', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13517', 'logps_train/rejected': '-159.36', 'logps_train/chosen': '-128.3', 'loss/train': '0.64397', 'examples_per_second': '30.297', 'grad_norm': '22.125', 'counters/examples': 151616, 'counters/updates': 4738}
train stats after 151648 examples: {'rewards_train/chosen': '0.15036', 'rewards_train/rejected': '0.032422', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11793', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-138.17', 'loss/train': '0.64946', 'examples_per_second': '32.623', 'grad_norm': '19', 'counters/examples': 151648, 'counters/updates': 4739}
train stats after 151680 examples: {'rewards_train/chosen': '0.13417', 'rewards_train/rejected': '0.047394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086776', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-143.36', 'loss/train': '0.65944', 'examples_per_second': '32.279', 'grad_norm': '19.875', 'counters/examples': 151680, 'counters/updates': 4740}
skipping logging after 151712 examples to avoid logging too frequently
train stats after 151744 examples: {'rewards_train/chosen': '0.17259', 'rewards_train/rejected': '-0.0059493', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17854', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-139.25', 'loss/train': '0.61746', 'examples_per_second': '32.15', 'grad_norm': '19.875', 'counters/examples': 151744, 'counters/updates': 4742}
train stats after 151776 examples: {'rewards_train/chosen': '0.13694', 'rewards_train/rejected': '0.02631', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11063', 'logps_train/rejected': '-118.01', 'logps_train/chosen': '-110.53', 'loss/train': '0.64871', 'examples_per_second': '31.901', 'grad_norm': '18.625', 'counters/examples': 151776, 'counters/updates': 4743}
train stats after 151808 examples: {'rewards_train/chosen': '0.18083', 'rewards_train/rejected': '0.05448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12635', 'logps_train/rejected': '-100.74', 'logps_train/chosen': '-135.8', 'loss/train': '0.64087', 'examples_per_second': '31.677', 'grad_norm': '18.75', 'counters/examples': 151808, 'counters/updates': 4744}
train stats after 151840 examples: {'rewards_train/chosen': '0.15584', 'rewards_train/rejected': '0.10662', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049217', 'logps_train/rejected': '-165.06', 'logps_train/chosen': '-171.26', 'loss/train': '0.67875', 'examples_per_second': '32.152', 'grad_norm': '23.5', 'counters/examples': 151840, 'counters/updates': 4745}
train stats after 151872 examples: {'rewards_train/chosen': '0.11067', 'rewards_train/rejected': '0.075495', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.03518', 'logps_train/rejected': '-128.72', 'logps_train/chosen': '-125.93', 'loss/train': '0.6888', 'examples_per_second': '30.727', 'grad_norm': '20.875', 'counters/examples': 151872, 'counters/updates': 4746}
train stats after 151904 examples: {'rewards_train/chosen': '0.14688', 'rewards_train/rejected': '0.17347', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026589', 'logps_train/rejected': '-103.89', 'logps_train/chosen': '-143.72', 'loss/train': '0.72527', 'examples_per_second': '30.174', 'grad_norm': '22.5', 'counters/examples': 151904, 'counters/updates': 4747}
train stats after 151936 examples: {'rewards_train/chosen': '0.15441', 'rewards_train/rejected': '0.060294', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094115', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-127.72', 'loss/train': '0.66415', 'examples_per_second': '27.693', 'grad_norm': '19.625', 'counters/examples': 151936, 'counters/updates': 4748}
skipping logging after 151968 examples to avoid logging too frequently
train stats after 152000 examples: {'rewards_train/chosen': '0.18786', 'rewards_train/rejected': '0.079564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1083', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-167.19', 'loss/train': '0.65327', 'examples_per_second': '32.125', 'grad_norm': '21.5', 'counters/examples': 152000, 'counters/updates': 4750}
train stats after 152032 examples: {'rewards_train/chosen': '0.11286', 'rewards_train/rejected': '-0.0012692', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11413', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-133.84', 'loss/train': '0.65229', 'examples_per_second': '24.596', 'grad_norm': '21.625', 'counters/examples': 152032, 'counters/updates': 4751}
train stats after 152064 examples: {'rewards_train/chosen': '0.10021', 'rewards_train/rejected': '0.00028822', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099927', 'logps_train/rejected': '-116.45', 'logps_train/chosen': '-95.721', 'loss/train': '0.6565', 'examples_per_second': '31.79', 'grad_norm': '19.75', 'counters/examples': 152064, 'counters/updates': 4752}
train stats after 152096 examples: {'rewards_train/chosen': '0.1978', 'rewards_train/rejected': '0.085722', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11208', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-138.19', 'loss/train': '0.65225', 'examples_per_second': '30.638', 'grad_norm': '19.875', 'counters/examples': 152096, 'counters/updates': 4753}
train stats after 152128 examples: {'rewards_train/chosen': '0.19448', 'rewards_train/rejected': '0.11283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081643', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-166.52', 'loss/train': '0.66726', 'examples_per_second': '31.572', 'grad_norm': '21.375', 'counters/examples': 152128, 'counters/updates': 4754}
skipping logging after 152160 examples to avoid logging too frequently
train stats after 152192 examples: {'rewards_train/chosen': '0.18418', 'rewards_train/rejected': '0.090873', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093311', 'logps_train/rejected': '-145.62', 'logps_train/chosen': '-193.7', 'loss/train': '0.66054', 'examples_per_second': '31.717', 'grad_norm': '22.875', 'counters/examples': 152192, 'counters/updates': 4756}
train stats after 152224 examples: {'rewards_train/chosen': '0.24654', 'rewards_train/rejected': '0.041985', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20455', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-142.23', 'loss/train': '0.60797', 'examples_per_second': '31.738', 'grad_norm': '20.5', 'counters/examples': 152224, 'counters/updates': 4757}
train stats after 152256 examples: {'rewards_train/chosen': '0.22508', 'rewards_train/rejected': '0.13315', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091929', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-139.99', 'loss/train': '0.65992', 'examples_per_second': '31.704', 'grad_norm': '19.25', 'counters/examples': 152256, 'counters/updates': 4758}
train stats after 152288 examples: {'rewards_train/chosen': '0.18699', 'rewards_train/rejected': '0.040263', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14673', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-156.98', 'loss/train': '0.63485', 'examples_per_second': '31.868', 'grad_norm': '20.75', 'counters/examples': 152288, 'counters/updates': 4759}
train stats after 152320 examples: {'rewards_train/chosen': '0.24541', 'rewards_train/rejected': '0.030124', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21529', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-132.04', 'loss/train': '0.60067', 'examples_per_second': '31.716', 'grad_norm': '21.5', 'counters/examples': 152320, 'counters/updates': 4760}
train stats after 152352 examples: {'rewards_train/chosen': '0.17106', 'rewards_train/rejected': '0.039925', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13114', 'logps_train/rejected': '-123.46', 'logps_train/chosen': '-120.97', 'loss/train': '0.64121', 'examples_per_second': '30.841', 'grad_norm': '20', 'counters/examples': 152352, 'counters/updates': 4761}
train stats after 152384 examples: {'rewards_train/chosen': '0.19577', 'rewards_train/rejected': '0.085793', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10998', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-125.42', 'loss/train': '0.64787', 'examples_per_second': '31.534', 'grad_norm': '21.125', 'counters/examples': 152384, 'counters/updates': 4762}
train stats after 152416 examples: {'rewards_train/chosen': '0.202', 'rewards_train/rejected': '0.080944', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12106', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-145.48', 'loss/train': '0.64443', 'examples_per_second': '31.561', 'grad_norm': '21.125', 'counters/examples': 152416, 'counters/updates': 4763}
train stats after 152448 examples: {'rewards_train/chosen': '0.10564', 'rewards_train/rejected': '0.018922', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08672', 'logps_train/rejected': '-118.83', 'logps_train/chosen': '-136.13', 'loss/train': '0.66652', 'examples_per_second': '32.189', 'grad_norm': '20.875', 'counters/examples': 152448, 'counters/updates': 4764}
train stats after 152480 examples: {'rewards_train/chosen': '0.14109', 'rewards_train/rejected': '0.069128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071966', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-142.94', 'loss/train': '0.66567', 'examples_per_second': '32.871', 'grad_norm': '21.125', 'counters/examples': 152480, 'counters/updates': 4765}
train stats after 152512 examples: {'rewards_train/chosen': '0.20024', 'rewards_train/rejected': '0.057038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1432', 'logps_train/rejected': '-96.45', 'logps_train/chosen': '-143.39', 'loss/train': '0.64111', 'examples_per_second': '31.022', 'grad_norm': '19.125', 'counters/examples': 152512, 'counters/updates': 4766}
skipping logging after 152544 examples to avoid logging too frequently
train stats after 152576 examples: {'rewards_train/chosen': '0.1707', 'rewards_train/rejected': '0.10674', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06396', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-141.02', 'loss/train': '0.67009', 'examples_per_second': '33.651', 'grad_norm': '20.125', 'counters/examples': 152576, 'counters/updates': 4768}
train stats after 152608 examples: {'rewards_train/chosen': '0.16823', 'rewards_train/rejected': '0.11008', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058148', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-160.89', 'loss/train': '0.67763', 'examples_per_second': '31.597', 'grad_norm': '20.25', 'counters/examples': 152608, 'counters/updates': 4769}
train stats after 152640 examples: {'rewards_train/chosen': '0.17786', 'rewards_train/rejected': '0.082078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095777', 'logps_train/rejected': '-122.6', 'logps_train/chosen': '-146.19', 'loss/train': '0.65455', 'examples_per_second': '32.828', 'grad_norm': '20.25', 'counters/examples': 152640, 'counters/updates': 4770}
train stats after 152672 examples: {'rewards_train/chosen': '0.16579', 'rewards_train/rejected': '0.0098436', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15595', 'logps_train/rejected': '-99.185', 'logps_train/chosen': '-156.4', 'loss/train': '0.63198', 'examples_per_second': '32.388', 'grad_norm': '18.875', 'counters/examples': 152672, 'counters/updates': 4771}
skipping logging after 152704 examples to avoid logging too frequently
train stats after 152736 examples: {'rewards_train/chosen': '0.21524', 'rewards_train/rejected': '0.13008', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085157', 'logps_train/rejected': '-113.17', 'logps_train/chosen': '-135.09', 'loss/train': '0.65998', 'examples_per_second': '33.45', 'grad_norm': '19.75', 'counters/examples': 152736, 'counters/updates': 4773}
train stats after 152768 examples: {'rewards_train/chosen': '0.15949', 'rewards_train/rejected': '-0.0024458', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16194', 'logps_train/rejected': '-97.44', 'logps_train/chosen': '-117.42', 'loss/train': '0.62567', 'examples_per_second': '32.555', 'grad_norm': '18.125', 'counters/examples': 152768, 'counters/updates': 4774}
train stats after 152800 examples: {'rewards_train/chosen': '0.13658', 'rewards_train/rejected': '0.068396', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068184', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-108.51', 'loss/train': '0.67196', 'examples_per_second': '30.275', 'grad_norm': '20.125', 'counters/examples': 152800, 'counters/updates': 4775}
train stats after 152832 examples: {'rewards_train/chosen': '0.10638', 'rewards_train/rejected': '0.045929', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060455', 'logps_train/rejected': '-155.78', 'logps_train/chosen': '-142.57', 'loss/train': '0.67986', 'examples_per_second': '30.161', 'grad_norm': '22.25', 'counters/examples': 152832, 'counters/updates': 4776}
train stats after 152864 examples: {'rewards_train/chosen': '0.14859', 'rewards_train/rejected': '0.048463', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10013', 'logps_train/rejected': '-146.12', 'logps_train/chosen': '-149.13', 'loss/train': '0.6521', 'examples_per_second': '32.532', 'grad_norm': '20.875', 'counters/examples': 152864, 'counters/updates': 4777}
train stats after 152896 examples: {'rewards_train/chosen': '0.18412', 'rewards_train/rejected': '0.13376', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050362', 'logps_train/rejected': '-139.7', 'logps_train/chosen': '-153.22', 'loss/train': '0.68797', 'examples_per_second': '31.628', 'grad_norm': '22.25', 'counters/examples': 152896, 'counters/updates': 4778}
train stats after 152928 examples: {'rewards_train/chosen': '0.18918', 'rewards_train/rejected': '0.054064', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13511', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-119', 'loss/train': '0.64104', 'examples_per_second': '31.434', 'grad_norm': '19.875', 'counters/examples': 152928, 'counters/updates': 4779}
skipping logging after 152960 examples to avoid logging too frequently
train stats after 152992 examples: {'rewards_train/chosen': '0.09663', 'rewards_train/rejected': '0.059576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037054', 'logps_train/rejected': '-115.61', 'logps_train/chosen': '-130.36', 'loss/train': '0.68454', 'examples_per_second': '32.707', 'grad_norm': '19.875', 'counters/examples': 152992, 'counters/updates': 4781}
skipping logging after 153024 examples to avoid logging too frequently
train stats after 153056 examples: {'rewards_train/chosen': '0.23042', 'rewards_train/rejected': '0.076556', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15386', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-140.2', 'loss/train': '0.63821', 'examples_per_second': '30.607', 'grad_norm': '20.25', 'counters/examples': 153056, 'counters/updates': 4783}
skipping logging after 153088 examples to avoid logging too frequently
train stats after 153120 examples: {'rewards_train/chosen': '0.23613', 'rewards_train/rejected': '0.098023', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13811', 'logps_train/rejected': '-99.002', 'logps_train/chosen': '-147.1', 'loss/train': '0.64003', 'examples_per_second': '31.659', 'grad_norm': '19', 'counters/examples': 153120, 'counters/updates': 4785}
train stats after 153152 examples: {'rewards_train/chosen': '0.11282', 'rewards_train/rejected': '0.055165', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057658', 'logps_train/rejected': '-99.988', 'logps_train/chosen': '-110.51', 'loss/train': '0.67698', 'examples_per_second': '32.814', 'grad_norm': '18.5', 'counters/examples': 153152, 'counters/updates': 4786}
skipping logging after 153184 examples to avoid logging too frequently
train stats after 153216 examples: {'rewards_train/chosen': '0.20771', 'rewards_train/rejected': '0.057853', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14986', 'logps_train/rejected': '-102.59', 'logps_train/chosen': '-137.62', 'loss/train': '0.6304', 'examples_per_second': '35.893', 'grad_norm': '18.875', 'counters/examples': 153216, 'counters/updates': 4788}
train stats after 153248 examples: {'rewards_train/chosen': '0.20045', 'rewards_train/rejected': '0.17245', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027998', 'logps_train/rejected': '-139.15', 'logps_train/chosen': '-163.96', 'loss/train': '0.69128', 'examples_per_second': '31.628', 'grad_norm': '23', 'counters/examples': 153248, 'counters/updates': 4789}
train stats after 153280 examples: {'rewards_train/chosen': '0.056147', 'rewards_train/rejected': '0.063969', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0078219', 'logps_train/rejected': '-104.43', 'logps_train/chosen': '-90.688', 'loss/train': '0.70532', 'examples_per_second': '31.591', 'grad_norm': '18.75', 'counters/examples': 153280, 'counters/updates': 4790}
train stats after 153312 examples: {'rewards_train/chosen': '0.2006', 'rewards_train/rejected': '0.11504', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085557', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-137.48', 'loss/train': '0.6739', 'examples_per_second': '31.708', 'grad_norm': '20.125', 'counters/examples': 153312, 'counters/updates': 4791}
train stats after 153344 examples: {'rewards_train/chosen': '0.095762', 'rewards_train/rejected': '0.14511', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.049343', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-161.02', 'loss/train': '0.72996', 'examples_per_second': '30.761', 'grad_norm': '23.375', 'counters/examples': 153344, 'counters/updates': 4792}
skipping logging after 153376 examples to avoid logging too frequently
train stats after 153408 examples: {'rewards_train/chosen': '0.10618', 'rewards_train/rejected': '0.11861', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012435', 'logps_train/rejected': '-98.246', 'logps_train/chosen': '-99.938', 'loss/train': '0.70581', 'examples_per_second': '35.847', 'grad_norm': '17.5', 'counters/examples': 153408, 'counters/updates': 4794}
train stats after 153440 examples: {'rewards_train/chosen': '0.18259', 'rewards_train/rejected': '0.010853', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17174', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-147.86', 'loss/train': '0.62432', 'examples_per_second': '31.054', 'grad_norm': '19.625', 'counters/examples': 153440, 'counters/updates': 4795}
train stats after 153472 examples: {'rewards_train/chosen': '0.16474', 'rewards_train/rejected': '0.11372', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051024', 'logps_train/rejected': '-131.47', 'logps_train/chosen': '-145.9', 'loss/train': '0.6774', 'examples_per_second': '31.654', 'grad_norm': '21.75', 'counters/examples': 153472, 'counters/updates': 4796}
skipping logging after 153504 examples to avoid logging too frequently
train stats after 153536 examples: {'rewards_train/chosen': '0.13509', 'rewards_train/rejected': '0.026461', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10863', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-147.12', 'loss/train': '0.65417', 'examples_per_second': '34.429', 'grad_norm': '21.125', 'counters/examples': 153536, 'counters/updates': 4798}
skipping logging after 153568 examples to avoid logging too frequently
train stats after 153600 examples: {'rewards_train/chosen': '-0.0068924', 'rewards_train/rejected': '0.021277', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02817', 'logps_train/rejected': '-82.606', 'logps_train/chosen': '-86.501', 'loss/train': '0.7142', 'examples_per_second': '33.257', 'grad_norm': '18.625', 'counters/examples': 153600, 'counters/updates': 4800}
train stats after 153632 examples: {'rewards_train/chosen': '0.22539', 'rewards_train/rejected': '0.085429', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13996', 'logps_train/rejected': '-89.01', 'logps_train/chosen': '-144.05', 'loss/train': '0.63612', 'examples_per_second': '31.165', 'grad_norm': '18.625', 'counters/examples': 153632, 'counters/updates': 4801}
train stats after 153664 examples: {'rewards_train/chosen': '0.15127', 'rewards_train/rejected': '0.027803', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12347', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-171.46', 'loss/train': '0.6454', 'examples_per_second': '30.164', 'grad_norm': '22.75', 'counters/examples': 153664, 'counters/updates': 4802}
train stats after 153696 examples: {'rewards_train/chosen': '0.28816', 'rewards_train/rejected': '0.141', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14716', 'logps_train/rejected': '-153.21', 'logps_train/chosen': '-201.84', 'loss/train': '0.6326', 'examples_per_second': '31.681', 'grad_norm': '22.75', 'counters/examples': 153696, 'counters/updates': 4803}
train stats after 153728 examples: {'rewards_train/chosen': '0.20874', 'rewards_train/rejected': '0.063659', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14508', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-134.57', 'loss/train': '0.63557', 'examples_per_second': '30.618', 'grad_norm': '18.875', 'counters/examples': 153728, 'counters/updates': 4804}
skipping logging after 153760 examples to avoid logging too frequently
train stats after 153792 examples: {'rewards_train/chosen': '0.1699', 'rewards_train/rejected': '0.078596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091301', 'logps_train/rejected': '-118.19', 'logps_train/chosen': '-165.07', 'loss/train': '0.65831', 'examples_per_second': '32.673', 'grad_norm': '20.75', 'counters/examples': 153792, 'counters/updates': 4806}
train stats after 153824 examples: {'rewards_train/chosen': '0.15842', 'rewards_train/rejected': '0.030576', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12784', 'logps_train/rejected': '-146.57', 'logps_train/chosen': '-129.97', 'loss/train': '0.63902', 'examples_per_second': '30.143', 'grad_norm': '22.125', 'counters/examples': 153824, 'counters/updates': 4807}
train stats after 153856 examples: {'rewards_train/chosen': '0.22467', 'rewards_train/rejected': '0.039919', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18475', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-165.53', 'loss/train': '0.62918', 'examples_per_second': '31.509', 'grad_norm': '21.125', 'counters/examples': 153856, 'counters/updates': 4808}
train stats after 153888 examples: {'rewards_train/chosen': '0.11837', 'rewards_train/rejected': '0.06195', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056418', 'logps_train/rejected': '-124.99', 'logps_train/chosen': '-156.87', 'loss/train': '0.67665', 'examples_per_second': '30.652', 'grad_norm': '21.375', 'counters/examples': 153888, 'counters/updates': 4809}
train stats after 153920 examples: {'rewards_train/chosen': '0.17017', 'rewards_train/rejected': '-0.0069857', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17715', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-136.76', 'loss/train': '0.61602', 'examples_per_second': '30.203', 'grad_norm': '18.75', 'counters/examples': 153920, 'counters/updates': 4810}
train stats after 153952 examples: {'rewards_train/chosen': '0.11858', 'rewards_train/rejected': '-0.046462', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16504', 'logps_train/rejected': '-91.142', 'logps_train/chosen': '-107.46', 'loss/train': '0.62569', 'examples_per_second': '31.99', 'grad_norm': '16.625', 'counters/examples': 153952, 'counters/updates': 4811}
train stats after 153984 examples: {'rewards_train/chosen': '0.12728', 'rewards_train/rejected': '0.060783', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.066493', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-152', 'loss/train': '0.6721', 'examples_per_second': '31.019', 'grad_norm': '20.5', 'counters/examples': 153984, 'counters/updates': 4812}
train stats after 154016 examples: {'rewards_train/chosen': '0.12994', 'rewards_train/rejected': '0.073164', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056771', 'logps_train/rejected': '-140.1', 'logps_train/chosen': '-118', 'loss/train': '0.67512', 'examples_per_second': '30.221', 'grad_norm': '21.375', 'counters/examples': 154016, 'counters/updates': 4813}
train stats after 154048 examples: {'rewards_train/chosen': '0.073184', 'rewards_train/rejected': '0.056108', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017076', 'logps_train/rejected': '-145.6', 'logps_train/chosen': '-146.44', 'loss/train': '0.69532', 'examples_per_second': '30.16', 'grad_norm': '21.75', 'counters/examples': 154048, 'counters/updates': 4814}
train stats after 154080 examples: {'rewards_train/chosen': '0.15121', 'rewards_train/rejected': '0.1276', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023619', 'logps_train/rejected': '-138.27', 'logps_train/chosen': '-142.44', 'loss/train': '0.6981', 'examples_per_second': '30.02', 'grad_norm': '22.125', 'counters/examples': 154080, 'counters/updates': 4815}
skipping logging after 154112 examples to avoid logging too frequently
train stats after 154144 examples: {'rewards_train/chosen': '0.23511', 'rewards_train/rejected': '0.015065', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22005', 'logps_train/rejected': '-141.81', 'logps_train/chosen': '-123.05', 'loss/train': '0.59866', 'examples_per_second': '31.64', 'grad_norm': '20.125', 'counters/examples': 154144, 'counters/updates': 4817}
skipping logging after 154176 examples to avoid logging too frequently
train stats after 154208 examples: {'rewards_train/chosen': '0.18579', 'rewards_train/rejected': '0.049385', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1364', 'logps_train/rejected': '-121.55', 'logps_train/chosen': '-120.54', 'loss/train': '0.6388', 'examples_per_second': '30.193', 'grad_norm': '18', 'counters/examples': 154208, 'counters/updates': 4819}
train stats after 154240 examples: {'rewards_train/chosen': '0.1363', 'rewards_train/rejected': '0.049253', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087044', 'logps_train/rejected': '-135.58', 'logps_train/chosen': '-133.08', 'loss/train': '0.66332', 'examples_per_second': '30.022', 'grad_norm': '20.75', 'counters/examples': 154240, 'counters/updates': 4820}
skipping logging after 154272 examples to avoid logging too frequently
train stats after 154304 examples: {'rewards_train/chosen': '0.10888', 'rewards_train/rejected': '0.034063', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074816', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-107.4', 'loss/train': '0.66347', 'examples_per_second': '34.435', 'grad_norm': '18.375', 'counters/examples': 154304, 'counters/updates': 4822}
train stats after 154336 examples: {'rewards_train/chosen': '0.20869', 'rewards_train/rejected': '0.014855', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19383', 'logps_train/rejected': '-106.49', 'logps_train/chosen': '-168.55', 'loss/train': '0.6136', 'examples_per_second': '30.24', 'grad_norm': '19.5', 'counters/examples': 154336, 'counters/updates': 4823}
train stats after 154368 examples: {'rewards_train/chosen': '0.10398', 'rewards_train/rejected': '0.084153', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01983', 'logps_train/rejected': '-163.35', 'logps_train/chosen': '-132.16', 'loss/train': '0.69722', 'examples_per_second': '31.638', 'grad_norm': '22.875', 'counters/examples': 154368, 'counters/updates': 4824}
skipping logging after 154400 examples to avoid logging too frequently
train stats after 154432 examples: {'rewards_train/chosen': '0.15337', 'rewards_train/rejected': '0.066412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086956', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-132.01', 'loss/train': '0.65856', 'examples_per_second': '31.794', 'grad_norm': '20.125', 'counters/examples': 154432, 'counters/updates': 4826}
train stats after 154464 examples: {'rewards_train/chosen': '0.054538', 'rewards_train/rejected': '-0.024497', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079035', 'logps_train/rejected': '-87.227', 'logps_train/chosen': '-127.22', 'loss/train': '0.66673', 'examples_per_second': '32.064', 'grad_norm': '18.75', 'counters/examples': 154464, 'counters/updates': 4827}
train stats after 154496 examples: {'rewards_train/chosen': '0.1278', 'rewards_train/rejected': '0.10117', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026628', 'logps_train/rejected': '-142.05', 'logps_train/chosen': '-157.51', 'loss/train': '0.68759', 'examples_per_second': '31.657', 'grad_norm': '22', 'counters/examples': 154496, 'counters/updates': 4828}
train stats after 154528 examples: {'rewards_train/chosen': '0.12733', 'rewards_train/rejected': '0.050892', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076435', 'logps_train/rejected': '-116.21', 'logps_train/chosen': '-138.47', 'loss/train': '0.66244', 'examples_per_second': '31.593', 'grad_norm': '23.375', 'counters/examples': 154528, 'counters/updates': 4829}
train stats after 154560 examples: {'rewards_train/chosen': '0.20747', 'rewards_train/rejected': '0.04495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16252', 'logps_train/rejected': '-95.683', 'logps_train/chosen': '-132.35', 'loss/train': '0.62601', 'examples_per_second': '31.737', 'grad_norm': '17.875', 'counters/examples': 154560, 'counters/updates': 4830}
train stats after 154592 examples: {'rewards_train/chosen': '0.18262', 'rewards_train/rejected': '-0.02176', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20438', 'logps_train/rejected': '-104.71', 'logps_train/chosen': '-126.69', 'loss/train': '0.60492', 'examples_per_second': '30.722', 'grad_norm': '17', 'counters/examples': 154592, 'counters/updates': 4831}
train stats after 154624 examples: {'rewards_train/chosen': '0.18904', 'rewards_train/rejected': '0.077793', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11125', 'logps_train/rejected': '-134.25', 'logps_train/chosen': '-140.46', 'loss/train': '0.64976', 'examples_per_second': '31.61', 'grad_norm': '20.25', 'counters/examples': 154624, 'counters/updates': 4832}
skipping logging after 154656 examples to avoid logging too frequently
train stats after 154688 examples: {'rewards_train/chosen': '0.1608', 'rewards_train/rejected': '0.094546', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066257', 'logps_train/rejected': '-143.31', 'logps_train/chosen': '-159.33', 'loss/train': '0.68005', 'examples_per_second': '30.827', 'grad_norm': '21.375', 'counters/examples': 154688, 'counters/updates': 4834}
train stats after 154720 examples: {'rewards_train/chosen': '0.13128', 'rewards_train/rejected': '-0.002249', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13353', 'logps_train/rejected': '-126.56', 'logps_train/chosen': '-123.09', 'loss/train': '0.64183', 'examples_per_second': '32.822', 'grad_norm': '20.5', 'counters/examples': 154720, 'counters/updates': 4835}
train stats after 154752 examples: {'rewards_train/chosen': '0.14343', 'rewards_train/rejected': '0.032966', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11047', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-128.45', 'loss/train': '0.65254', 'examples_per_second': '31.614', 'grad_norm': '19.75', 'counters/examples': 154752, 'counters/updates': 4836}
train stats after 154784 examples: {'rewards_train/chosen': '0.065024', 'rewards_train/rejected': '0.031597', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033427', 'logps_train/rejected': '-133.73', 'logps_train/chosen': '-157.5', 'loss/train': '0.68346', 'examples_per_second': '23.751', 'grad_norm': '21.5', 'counters/examples': 154784, 'counters/updates': 4837}
train stats after 154816 examples: {'rewards_train/chosen': '0.1942', 'rewards_train/rejected': '0.049151', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14505', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-151.17', 'loss/train': '0.6343', 'examples_per_second': '32.699', 'grad_norm': '20.625', 'counters/examples': 154816, 'counters/updates': 4838}
train stats after 154848 examples: {'rewards_train/chosen': '0.16177', 'rewards_train/rejected': '0.13035', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031419', 'logps_train/rejected': '-152.42', 'logps_train/chosen': '-139.19', 'loss/train': '0.6871', 'examples_per_second': '31.441', 'grad_norm': '21.625', 'counters/examples': 154848, 'counters/updates': 4839}
train stats after 154880 examples: {'rewards_train/chosen': '0.1525', 'rewards_train/rejected': '-0.0092383', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16174', 'logps_train/rejected': '-121.2', 'logps_train/chosen': '-158.99', 'loss/train': '0.6331', 'examples_per_second': '31.518', 'grad_norm': '19.875', 'counters/examples': 154880, 'counters/updates': 4840}
train stats after 154912 examples: {'rewards_train/chosen': '0.087593', 'rewards_train/rejected': '0.054131', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033461', 'logps_train/rejected': '-123.37', 'logps_train/chosen': '-132.38', 'loss/train': '0.6916', 'examples_per_second': '32.213', 'grad_norm': '20.125', 'counters/examples': 154912, 'counters/updates': 4841}
train stats after 154944 examples: {'rewards_train/chosen': '0.16949', 'rewards_train/rejected': '0.11439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055094', 'logps_train/rejected': '-106.08', 'logps_train/chosen': '-161.11', 'loss/train': '0.67515', 'examples_per_second': '31.542', 'grad_norm': '21.25', 'counters/examples': 154944, 'counters/updates': 4842}
train stats after 154976 examples: {'rewards_train/chosen': '0.20212', 'rewards_train/rejected': '0.083338', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11878', 'logps_train/rejected': '-105.89', 'logps_train/chosen': '-114.2', 'loss/train': '0.65107', 'examples_per_second': '32.959', 'grad_norm': '18.25', 'counters/examples': 154976, 'counters/updates': 4843}
train stats after 155008 examples: {'rewards_train/chosen': '0.17721', 'rewards_train/rejected': '0.059643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11757', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-133.52', 'loss/train': '0.65034', 'examples_per_second': '30.954', 'grad_norm': '20', 'counters/examples': 155008, 'counters/updates': 4844}
skipping logging after 155040 examples to avoid logging too frequently
train stats after 155072 examples: {'rewards_train/chosen': '0.23069', 'rewards_train/rejected': '0.11515', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11553', 'logps_train/rejected': '-152.96', 'logps_train/chosen': '-150.93', 'loss/train': '0.65535', 'examples_per_second': '32.371', 'grad_norm': '22.75', 'counters/examples': 155072, 'counters/updates': 4846}
skipping logging after 155104 examples to avoid logging too frequently
train stats after 155136 examples: {'rewards_train/chosen': '0.14645', 'rewards_train/rejected': '0.035964', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11049', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-124.45', 'loss/train': '0.65082', 'examples_per_second': '32.344', 'grad_norm': '17', 'counters/examples': 155136, 'counters/updates': 4848}
train stats after 155168 examples: {'rewards_train/chosen': '0.17676', 'rewards_train/rejected': '0.023599', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15316', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-170.43', 'loss/train': '0.62829', 'examples_per_second': '31.7', 'grad_norm': '21.375', 'counters/examples': 155168, 'counters/updates': 4849}
train stats after 155200 examples: {'rewards_train/chosen': '0.14592', 'rewards_train/rejected': '0.039097', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10682', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-143.76', 'loss/train': '0.65744', 'examples_per_second': '30.203', 'grad_norm': '20.875', 'counters/examples': 155200, 'counters/updates': 4850}
skipping logging after 155232 examples to avoid logging too frequently
train stats after 155264 examples: {'rewards_train/chosen': '0.13241', 'rewards_train/rejected': '0.040281', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092134', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-122.8', 'loss/train': '0.6574', 'examples_per_second': '31.497', 'grad_norm': '19.25', 'counters/examples': 155264, 'counters/updates': 4852}
skipping logging after 155296 examples to avoid logging too frequently
train stats after 155328 examples: {'rewards_train/chosen': '0.09753', 'rewards_train/rejected': '-0.03571', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13324', 'logps_train/rejected': '-78.465', 'logps_train/chosen': '-121.56', 'loss/train': '0.63818', 'examples_per_second': '31.3', 'grad_norm': '16.5', 'counters/examples': 155328, 'counters/updates': 4854}
skipping logging after 155360 examples to avoid logging too frequently
train stats after 155392 examples: {'rewards_train/chosen': '0.31751', 'rewards_train/rejected': '0.10886', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20864', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-174.08', 'loss/train': '0.61966', 'examples_per_second': '31.545', 'grad_norm': '21.875', 'counters/examples': 155392, 'counters/updates': 4856}
train stats after 155424 examples: {'rewards_train/chosen': '0.17441', 'rewards_train/rejected': '0.024565', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14985', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-130.73', 'loss/train': '0.63263', 'examples_per_second': '30.703', 'grad_norm': '18.75', 'counters/examples': 155424, 'counters/updates': 4857}
train stats after 155456 examples: {'rewards_train/chosen': '0.098393', 'rewards_train/rejected': '0.059106', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039287', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-99.22', 'loss/train': '0.68539', 'examples_per_second': '30.779', 'grad_norm': '18.875', 'counters/examples': 155456, 'counters/updates': 4858}
train stats after 155488 examples: {'rewards_train/chosen': '0.16143', 'rewards_train/rejected': '0.14581', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015621', 'logps_train/rejected': '-139.35', 'logps_train/chosen': '-143.14', 'loss/train': '0.7112', 'examples_per_second': '30.625', 'grad_norm': '23.125', 'counters/examples': 155488, 'counters/updates': 4859}
train stats after 155520 examples: {'rewards_train/chosen': '0.17249', 'rewards_train/rejected': '0.069985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10251', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-138.5', 'loss/train': '0.65086', 'examples_per_second': '30.184', 'grad_norm': '20', 'counters/examples': 155520, 'counters/updates': 4860}
train stats after 155552 examples: {'rewards_train/chosen': '0.1377', 'rewards_train/rejected': '-0.020869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15857', 'logps_train/rejected': '-99.994', 'logps_train/chosen': '-126.19', 'loss/train': '0.62714', 'examples_per_second': '32.03', 'grad_norm': '19.25', 'counters/examples': 155552, 'counters/updates': 4861}
train stats after 155584 examples: {'rewards_train/chosen': '0.22889', 'rewards_train/rejected': '0.041449', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18745', 'logps_train/rejected': '-101.62', 'logps_train/chosen': '-145.69', 'loss/train': '0.62124', 'examples_per_second': '32.422', 'grad_norm': '19', 'counters/examples': 155584, 'counters/updates': 4862}
train stats after 155616 examples: {'rewards_train/chosen': '0.13661', 'rewards_train/rejected': '0.077527', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059085', 'logps_train/rejected': '-142.71', 'logps_train/chosen': '-129.19', 'loss/train': '0.67312', 'examples_per_second': '31.64', 'grad_norm': '21.125', 'counters/examples': 155616, 'counters/updates': 4863}
train stats after 155648 examples: {'rewards_train/chosen': '0.1555', 'rewards_train/rejected': '0.12513', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030371', 'logps_train/rejected': '-159.05', 'logps_train/chosen': '-137.67', 'loss/train': '0.6913', 'examples_per_second': '31.626', 'grad_norm': '23.75', 'counters/examples': 155648, 'counters/updates': 4864}
train stats after 155680 examples: {'rewards_train/chosen': '0.15874', 'rewards_train/rejected': '0.10032', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058423', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-153.29', 'loss/train': '0.67208', 'examples_per_second': '31.944', 'grad_norm': '21.875', 'counters/examples': 155680, 'counters/updates': 4865}
skipping logging after 155712 examples to avoid logging too frequently
train stats after 155744 examples: {'rewards_train/chosen': '0.13806', 'rewards_train/rejected': '0.02624', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11182', 'logps_train/rejected': '-106.49', 'logps_train/chosen': '-138.97', 'loss/train': '0.64402', 'examples_per_second': '33.992', 'grad_norm': '19.25', 'counters/examples': 155744, 'counters/updates': 4867}
train stats after 155776 examples: {'rewards_train/chosen': '0.077597', 'rewards_train/rejected': '0.074029', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0035673', 'logps_train/rejected': '-134.7', 'logps_train/chosen': '-155.33', 'loss/train': '0.70055', 'examples_per_second': '32.377', 'grad_norm': '24.25', 'counters/examples': 155776, 'counters/updates': 4868}
train stats after 155808 examples: {'rewards_train/chosen': '0.19302', 'rewards_train/rejected': '0.035934', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15709', 'logps_train/rejected': '-112.95', 'logps_train/chosen': '-138.8', 'loss/train': '0.63085', 'examples_per_second': '31.107', 'grad_norm': '19.625', 'counters/examples': 155808, 'counters/updates': 4869}
train stats after 155840 examples: {'rewards_train/chosen': '0.15404', 'rewards_train/rejected': '0.051974', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10206', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-124.06', 'loss/train': '0.64774', 'examples_per_second': '32.406', 'grad_norm': '19.625', 'counters/examples': 155840, 'counters/updates': 4870}
train stats after 155872 examples: {'rewards_train/chosen': '0.15659', 'rewards_train/rejected': '0.0015668', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15502', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-144.24', 'loss/train': '0.6298', 'examples_per_second': '31.637', 'grad_norm': '20', 'counters/examples': 155872, 'counters/updates': 4871}
skipping logging after 155904 examples to avoid logging too frequently
train stats after 155936 examples: {'rewards_train/chosen': '0.17874', 'rewards_train/rejected': '0.035027', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14371', 'logps_train/rejected': '-115.08', 'logps_train/chosen': '-136.52', 'loss/train': '0.63811', 'examples_per_second': '30.767', 'grad_norm': '18.875', 'counters/examples': 155936, 'counters/updates': 4873}
train stats after 155968 examples: {'rewards_train/chosen': '0.18915', 'rewards_train/rejected': '-0.020414', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20957', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-178.36', 'loss/train': '0.61497', 'examples_per_second': '31.579', 'grad_norm': '22.25', 'counters/examples': 155968, 'counters/updates': 4874}
train stats after 156000 examples: {'rewards_train/chosen': '0.23206', 'rewards_train/rejected': '0.081191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15086', 'logps_train/rejected': '-175.4', 'logps_train/chosen': '-159.51', 'loss/train': '0.63535', 'examples_per_second': '30.73', 'grad_norm': '22.125', 'counters/examples': 156000, 'counters/updates': 4875}
Running evaluation after 156000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.91it/s]
eval after 156000: {'rewards_eval/chosen': '0.15804', 'rewards_eval/rejected': '0.059383', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.098659', 'logps_eval/rejected': '-114.8', 'logps_eval/chosen': '-133.97', 'loss/eval': '0.65994'}
skipping save for non epoch
train stats after 156032 examples: {'rewards_train/chosen': '0.18468', 'rewards_train/rejected': '0.099305', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085375', 'logps_train/rejected': '-109.03', 'logps_train/chosen': '-126.53', 'loss/train': '0.66225', 'examples_per_second': '33.871', 'grad_norm': '19.125', 'counters/examples': 156032, 'counters/updates': 4876}
train stats after 156064 examples: {'rewards_train/chosen': '0.1707', 'rewards_train/rejected': '0.096643', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074058', 'logps_train/rejected': '-108.84', 'logps_train/chosen': '-123.31', 'loss/train': '0.66848', 'examples_per_second': '31.001', 'grad_norm': '19.75', 'counters/examples': 156064, 'counters/updates': 4877}
skipping logging after 156096 examples to avoid logging too frequently
train stats after 156128 examples: {'rewards_train/chosen': '0.1198', 'rewards_train/rejected': '0.03707', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082735', 'logps_train/rejected': '-125.61', 'logps_train/chosen': '-120.4', 'loss/train': '0.66207', 'examples_per_second': '32.693', 'grad_norm': '19.625', 'counters/examples': 156128, 'counters/updates': 4879}
skipping logging after 156160 examples to avoid logging too frequently
train stats after 156192 examples: {'rewards_train/chosen': '0.1635', 'rewards_train/rejected': '0.081678', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081826', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-112.1', 'loss/train': '0.66462', 'examples_per_second': '31.97', 'grad_norm': '20.125', 'counters/examples': 156192, 'counters/updates': 4881}
train stats after 156224 examples: {'rewards_train/chosen': '0.15725', 'rewards_train/rejected': '0.11978', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037465', 'logps_train/rejected': '-106.75', 'logps_train/chosen': '-115.58', 'loss/train': '0.68556', 'examples_per_second': '31.385', 'grad_norm': '18.625', 'counters/examples': 156224, 'counters/updates': 4882}
skipping logging after 156256 examples to avoid logging too frequently
train stats after 156288 examples: {'rewards_train/chosen': '0.14357', 'rewards_train/rejected': '0.03707', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1065', 'logps_train/rejected': '-98.297', 'logps_train/chosen': '-103.49', 'loss/train': '0.65153', 'examples_per_second': '40.418', 'grad_norm': '16.25', 'counters/examples': 156288, 'counters/updates': 4884}
train stats after 156320 examples: {'rewards_train/chosen': '0.16369', 'rewards_train/rejected': '0.086231', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077456', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-145.82', 'loss/train': '0.67976', 'examples_per_second': '31.602', 'grad_norm': '22.125', 'counters/examples': 156320, 'counters/updates': 4885}
skipping logging after 156352 examples to avoid logging too frequently
train stats after 156384 examples: {'rewards_train/chosen': '0.16199', 'rewards_train/rejected': '0.088515', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073475', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-134.94', 'loss/train': '0.67154', 'examples_per_second': '31.424', 'grad_norm': '19.875', 'counters/examples': 156384, 'counters/updates': 4887}
train stats after 156416 examples: {'rewards_train/chosen': '0.17265', 'rewards_train/rejected': '-0.073249', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.2459', 'logps_train/rejected': '-81.677', 'logps_train/chosen': '-135.05', 'loss/train': '0.58522', 'examples_per_second': '32.109', 'grad_norm': '17.875', 'counters/examples': 156416, 'counters/updates': 4888}
train stats after 156448 examples: {'rewards_train/chosen': '0.14923', 'rewards_train/rejected': '0.065115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08411', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-157.05', 'loss/train': '0.67241', 'examples_per_second': '30.475', 'grad_norm': '21.875', 'counters/examples': 156448, 'counters/updates': 4889}
skipping logging after 156480 examples to avoid logging too frequently
train stats after 156512 examples: {'rewards_train/chosen': '0.22041', 'rewards_train/rejected': '0.043523', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17689', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-161.94', 'loss/train': '0.62844', 'examples_per_second': '31.602', 'grad_norm': '19.625', 'counters/examples': 156512, 'counters/updates': 4891}
train stats after 156544 examples: {'rewards_train/chosen': '0.15228', 'rewards_train/rejected': '0.0036784', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14861', 'logps_train/rejected': '-87.803', 'logps_train/chosen': '-174.52', 'loss/train': '0.64004', 'examples_per_second': '32.194', 'grad_norm': '20.75', 'counters/examples': 156544, 'counters/updates': 4892}
train stats after 156576 examples: {'rewards_train/chosen': '0.13023', 'rewards_train/rejected': '0.019489', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11074', 'logps_train/rejected': '-125.43', 'logps_train/chosen': '-167.71', 'loss/train': '0.6556', 'examples_per_second': '30.316', 'grad_norm': '23', 'counters/examples': 156576, 'counters/updates': 4893}
train stats after 156608 examples: {'rewards_train/chosen': '0.22087', 'rewards_train/rejected': '-0.006342', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22721', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-157.66', 'loss/train': '0.6056', 'examples_per_second': '31.581', 'grad_norm': '20.25', 'counters/examples': 156608, 'counters/updates': 4894}
train stats after 156640 examples: {'rewards_train/chosen': '0.022105', 'rewards_train/rejected': '-0.057585', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.07969', 'logps_train/rejected': '-115.07', 'logps_train/chosen': '-134.66', 'loss/train': '0.66505', 'examples_per_second': '30.507', 'grad_norm': '20.625', 'counters/examples': 156640, 'counters/updates': 4895}
skipping logging after 156672 examples to avoid logging too frequently
train stats after 156704 examples: {'rewards_train/chosen': '0.15315', 'rewards_train/rejected': '-0.0078812', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16103', 'logps_train/rejected': '-141.61', 'logps_train/chosen': '-170.55', 'loss/train': '0.63452', 'examples_per_second': '30.071', 'grad_norm': '22.125', 'counters/examples': 156704, 'counters/updates': 4897}
train stats after 156736 examples: {'rewards_train/chosen': '0.21127', 'rewards_train/rejected': '0.12774', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083528', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-140.35', 'loss/train': '0.66137', 'examples_per_second': '30.193', 'grad_norm': '21.25', 'counters/examples': 156736, 'counters/updates': 4898}
train stats after 156768 examples: {'rewards_train/chosen': '0.14831', 'rewards_train/rejected': '0.045245', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10307', 'logps_train/rejected': '-116.55', 'logps_train/chosen': '-120.37', 'loss/train': '0.65613', 'examples_per_second': '31.594', 'grad_norm': '18.375', 'counters/examples': 156768, 'counters/updates': 4899}
train stats after 156800 examples: {'rewards_train/chosen': '0.22733', 'rewards_train/rejected': '0.079096', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14823', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-183.86', 'loss/train': '0.63027', 'examples_per_second': '30.607', 'grad_norm': '19.75', 'counters/examples': 156800, 'counters/updates': 4900}
skipping logging after 156832 examples to avoid logging too frequently
train stats after 156864 examples: {'rewards_train/chosen': '0.19055', 'rewards_train/rejected': '0.064372', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12618', 'logps_train/rejected': '-104.06', 'logps_train/chosen': '-133.61', 'loss/train': '0.64672', 'examples_per_second': '32.255', 'grad_norm': '18.625', 'counters/examples': 156864, 'counters/updates': 4902}
train stats after 156896 examples: {'rewards_train/chosen': '0.18425', 'rewards_train/rejected': '0.041239', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14301', 'logps_train/rejected': '-122.07', 'logps_train/chosen': '-144.13', 'loss/train': '0.6373', 'examples_per_second': '31.527', 'grad_norm': '19.625', 'counters/examples': 156896, 'counters/updates': 4903}
train stats after 156928 examples: {'rewards_train/chosen': '0.24255', 'rewards_train/rejected': '0.090894', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15165', 'logps_train/rejected': '-113.74', 'logps_train/chosen': '-115.29', 'loss/train': '0.62697', 'examples_per_second': '32.579', 'grad_norm': '18.25', 'counters/examples': 156928, 'counters/updates': 4904}
skipping logging after 156960 examples to avoid logging too frequently
train stats after 156992 examples: {'rewards_train/chosen': '0.062565', 'rewards_train/rejected': '-0.0097427', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072307', 'logps_train/rejected': '-76.328', 'logps_train/chosen': '-117.4', 'loss/train': '0.66912', 'examples_per_second': '34.182', 'grad_norm': '17.5', 'counters/examples': 156992, 'counters/updates': 4906}
skipping logging after 157024 examples to avoid logging too frequently
train stats after 157056 examples: {'rewards_train/chosen': '0.13281', 'rewards_train/rejected': '-0.0098333', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14264', 'logps_train/rejected': '-119.27', 'logps_train/chosen': '-135.99', 'loss/train': '0.63593', 'examples_per_second': '31.541', 'grad_norm': '20.625', 'counters/examples': 157056, 'counters/updates': 4908}
train stats after 157088 examples: {'rewards_train/chosen': '0.20635', 'rewards_train/rejected': '0.1029', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10345', 'logps_train/rejected': '-164.33', 'logps_train/chosen': '-177.59', 'loss/train': '0.65856', 'examples_per_second': '30.607', 'grad_norm': '24.5', 'counters/examples': 157088, 'counters/updates': 4909}
train stats after 157120 examples: {'rewards_train/chosen': '0.10692', 'rewards_train/rejected': '0.076756', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030159', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-113.36', 'loss/train': '0.68508', 'examples_per_second': '31.599', 'grad_norm': '20.125', 'counters/examples': 157120, 'counters/updates': 4910}
train stats after 157152 examples: {'rewards_train/chosen': '0.23114', 'rewards_train/rejected': '0.03226', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19889', 'logps_train/rejected': '-132.73', 'logps_train/chosen': '-143.34', 'loss/train': '0.61109', 'examples_per_second': '31.5', 'grad_norm': '21.375', 'counters/examples': 157152, 'counters/updates': 4911}
train stats after 157184 examples: {'rewards_train/chosen': '0.25654', 'rewards_train/rejected': '0.11087', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14567', 'logps_train/rejected': '-115.08', 'logps_train/chosen': '-139.01', 'loss/train': '0.63474', 'examples_per_second': '31.538', 'grad_norm': '19.375', 'counters/examples': 157184, 'counters/updates': 4912}
train stats after 157216 examples: {'rewards_train/chosen': '0.13735', 'rewards_train/rejected': '0.020981', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11637', 'logps_train/rejected': '-144.58', 'logps_train/chosen': '-156.47', 'loss/train': '0.65028', 'examples_per_second': '30.205', 'grad_norm': '21.125', 'counters/examples': 157216, 'counters/updates': 4913}
train stats after 157248 examples: {'rewards_train/chosen': '0.17057', 'rewards_train/rejected': '0.084651', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085923', 'logps_train/rejected': '-104.62', 'logps_train/chosen': '-112.89', 'loss/train': '0.66263', 'examples_per_second': '31.072', 'grad_norm': '18.375', 'counters/examples': 157248, 'counters/updates': 4914}
train stats after 157280 examples: {'rewards_train/chosen': '0.1884', 'rewards_train/rejected': '0.02433', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16407', 'logps_train/rejected': '-98.353', 'logps_train/chosen': '-112.71', 'loss/train': '0.63089', 'examples_per_second': '30.814', 'grad_norm': '16.875', 'counters/examples': 157280, 'counters/updates': 4915}
train stats after 157312 examples: {'rewards_train/chosen': '0.11562', 'rewards_train/rejected': '0.021761', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093863', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-132.83', 'loss/train': '0.66305', 'examples_per_second': '31.593', 'grad_norm': '19.875', 'counters/examples': 157312, 'counters/updates': 4916}
train stats after 157344 examples: {'rewards_train/chosen': '0.20726', 'rewards_train/rejected': '0.061768', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14549', 'logps_train/rejected': '-135.22', 'logps_train/chosen': '-145.81', 'loss/train': '0.64164', 'examples_per_second': '31.59', 'grad_norm': '21.25', 'counters/examples': 157344, 'counters/updates': 4917}
skipping logging after 157376 examples to avoid logging too frequently
train stats after 157408 examples: {'rewards_train/chosen': '0.18327', 'rewards_train/rejected': '0.052472', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1308', 'logps_train/rejected': '-120', 'logps_train/chosen': '-134.84', 'loss/train': '0.63876', 'examples_per_second': '24.361', 'grad_norm': '19.25', 'counters/examples': 157408, 'counters/updates': 4919}
train stats after 157440 examples: {'rewards_train/chosen': '0.074759', 'rewards_train/rejected': '-0.051154', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12591', 'logps_train/rejected': '-81.683', 'logps_train/chosen': '-121.51', 'loss/train': '0.64484', 'examples_per_second': '32.055', 'grad_norm': '16.875', 'counters/examples': 157440, 'counters/updates': 4920}
train stats after 157472 examples: {'rewards_train/chosen': '0.12295', 'rewards_train/rejected': '-0.0058974', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12884', 'logps_train/rejected': '-87.16', 'logps_train/chosen': '-112.53', 'loss/train': '0.64066', 'examples_per_second': '33.009', 'grad_norm': '17.875', 'counters/examples': 157472, 'counters/updates': 4921}
train stats after 157504 examples: {'rewards_train/chosen': '0.23835', 'rewards_train/rejected': '0.073866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16448', 'logps_train/rejected': '-88.421', 'logps_train/chosen': '-160', 'loss/train': '0.62284', 'examples_per_second': '23.875', 'grad_norm': '19', 'counters/examples': 157504, 'counters/updates': 4922}
train stats after 157536 examples: {'rewards_train/chosen': '0.16866', 'rewards_train/rejected': '-0.0012041', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16987', 'logps_train/rejected': '-87.819', 'logps_train/chosen': '-131.6', 'loss/train': '0.61928', 'examples_per_second': '31.334', 'grad_norm': '17', 'counters/examples': 157536, 'counters/updates': 4923}
train stats after 157568 examples: {'rewards_train/chosen': '0.16436', 'rewards_train/rejected': '0.018633', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14573', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-138.56', 'loss/train': '0.6361', 'examples_per_second': '31.523', 'grad_norm': '20.25', 'counters/examples': 157568, 'counters/updates': 4924}
train stats after 157600 examples: {'rewards_train/chosen': '0.20387', 'rewards_train/rejected': '0.094345', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10952', 'logps_train/rejected': '-137.2', 'logps_train/chosen': '-166.78', 'loss/train': '0.65735', 'examples_per_second': '31.574', 'grad_norm': '21.25', 'counters/examples': 157600, 'counters/updates': 4925}
train stats after 157632 examples: {'rewards_train/chosen': '0.16008', 'rewards_train/rejected': '0.087922', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072155', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-107.43', 'loss/train': '0.67882', 'examples_per_second': '30.122', 'grad_norm': '18.25', 'counters/examples': 157632, 'counters/updates': 4926}
train stats after 157664 examples: {'rewards_train/chosen': '0.16947', 'rewards_train/rejected': '0.043973', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1255', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-129.34', 'loss/train': '0.64332', 'examples_per_second': '30.566', 'grad_norm': '19.625', 'counters/examples': 157664, 'counters/updates': 4927}
train stats after 157696 examples: {'rewards_train/chosen': '0.14848', 'rewards_train/rejected': '0.078756', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069723', 'logps_train/rejected': '-150.94', 'logps_train/chosen': '-136.63', 'loss/train': '0.6736', 'examples_per_second': '32.301', 'grad_norm': '21.375', 'counters/examples': 157696, 'counters/updates': 4928}
train stats after 157728 examples: {'rewards_train/chosen': '0.13752', 'rewards_train/rejected': '0.064079', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073446', 'logps_train/rejected': '-98.862', 'logps_train/chosen': '-147.07', 'loss/train': '0.66895', 'examples_per_second': '30.569', 'grad_norm': '20.625', 'counters/examples': 157728, 'counters/updates': 4929}
train stats after 157760 examples: {'rewards_train/chosen': '0.086416', 'rewards_train/rejected': '-0.077438', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16385', 'logps_train/rejected': '-128.14', 'logps_train/chosen': '-155.3', 'loss/train': '0.62011', 'examples_per_second': '31.459', 'grad_norm': '20', 'counters/examples': 157760, 'counters/updates': 4930}
train stats after 157792 examples: {'rewards_train/chosen': '0.10012', 'rewards_train/rejected': '0.029671', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070451', 'logps_train/rejected': '-101.13', 'logps_train/chosen': '-154.7', 'loss/train': '0.66746', 'examples_per_second': '30.486', 'grad_norm': '20.625', 'counters/examples': 157792, 'counters/updates': 4931}
train stats after 157824 examples: {'rewards_train/chosen': '0.13473', 'rewards_train/rejected': '0.15223', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017505', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-162.11', 'loss/train': '0.71626', 'examples_per_second': '31.326', 'grad_norm': '23', 'counters/examples': 157824, 'counters/updates': 4932}
train stats after 157856 examples: {'rewards_train/chosen': '0.04492', 'rewards_train/rejected': '-0.0033954', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048315', 'logps_train/rejected': '-105.3', 'logps_train/chosen': '-111.06', 'loss/train': '0.67748', 'examples_per_second': '32.016', 'grad_norm': '20.375', 'counters/examples': 157856, 'counters/updates': 4933}
train stats after 157888 examples: {'rewards_train/chosen': '0.14579', 'rewards_train/rejected': '0.073684', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072101', 'logps_train/rejected': '-140.99', 'logps_train/chosen': '-142.31', 'loss/train': '0.66574', 'examples_per_second': '31.783', 'grad_norm': '20.75', 'counters/examples': 157888, 'counters/updates': 4934}
skipping logging after 157920 examples to avoid logging too frequently
train stats after 157952 examples: {'rewards_train/chosen': '0.14261', 'rewards_train/rejected': '-0.0083968', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.15101', 'logps_train/rejected': '-103.75', 'logps_train/chosen': '-132.69', 'loss/train': '0.64003', 'examples_per_second': '32.08', 'grad_norm': '19', 'counters/examples': 157952, 'counters/updates': 4936}
train stats after 157984 examples: {'rewards_train/chosen': '0.16743', 'rewards_train/rejected': '0.071327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096098', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-136.76', 'loss/train': '0.66868', 'examples_per_second': '30.82', 'grad_norm': '21.375', 'counters/examples': 157984, 'counters/updates': 4937}
train stats after 158016 examples: {'rewards_train/chosen': '0.16501', 'rewards_train/rejected': '0.059283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10573', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-178.76', 'loss/train': '0.65651', 'examples_per_second': '30.038', 'grad_norm': '23.125', 'counters/examples': 158016, 'counters/updates': 4938}
skipping logging after 158048 examples to avoid logging too frequently
train stats after 158080 examples: {'rewards_train/chosen': '0.12212', 'rewards_train/rejected': '-0.00080432', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12293', 'logps_train/rejected': '-103.43', 'logps_train/chosen': '-138.77', 'loss/train': '0.64323', 'examples_per_second': '33.598', 'grad_norm': '19.125', 'counters/examples': 158080, 'counters/updates': 4940}
train stats after 158112 examples: {'rewards_train/chosen': '0.11433', 'rewards_train/rejected': '-0.016452', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13078', 'logps_train/rejected': '-118.24', 'logps_train/chosen': '-120.64', 'loss/train': '0.64306', 'examples_per_second': '31.238', 'grad_norm': '19.5', 'counters/examples': 158112, 'counters/updates': 4941}
skipping logging after 158144 examples to avoid logging too frequently
train stats after 158176 examples: {'rewards_train/chosen': '0.14392', 'rewards_train/rejected': '0.13478', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0091342', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-115.08', 'loss/train': '0.69597', 'examples_per_second': '33.191', 'grad_norm': '20.5', 'counters/examples': 158176, 'counters/updates': 4943}
train stats after 158208 examples: {'rewards_train/chosen': '0.27114', 'rewards_train/rejected': '0.13459', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13655', 'logps_train/rejected': '-157.53', 'logps_train/chosen': '-166.21', 'loss/train': '0.64282', 'examples_per_second': '30.696', 'grad_norm': '21.625', 'counters/examples': 158208, 'counters/updates': 4944}
train stats after 158240 examples: {'rewards_train/chosen': '0.23455', 'rewards_train/rejected': '-0.017241', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2518', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-175.3', 'loss/train': '0.58851', 'examples_per_second': '30.919', 'grad_norm': '21.25', 'counters/examples': 158240, 'counters/updates': 4945}
skipping logging after 158272 examples to avoid logging too frequently
train stats after 158304 examples: {'rewards_train/chosen': '0.12195', 'rewards_train/rejected': '0.042538', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.079415', 'logps_train/rejected': '-102.82', 'logps_train/chosen': '-126.74', 'loss/train': '0.66548', 'examples_per_second': '30.426', 'grad_norm': '17.875', 'counters/examples': 158304, 'counters/updates': 4947}
train stats after 158336 examples: {'rewards_train/chosen': '0.20062', 'rewards_train/rejected': '0.062097', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13853', 'logps_train/rejected': '-91.581', 'logps_train/chosen': '-127.91', 'loss/train': '0.63615', 'examples_per_second': '32.238', 'grad_norm': '18.125', 'counters/examples': 158336, 'counters/updates': 4948}
train stats after 158368 examples: {'rewards_train/chosen': '0.20545', 'rewards_train/rejected': '0.043629', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16183', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-155.08', 'loss/train': '0.62469', 'examples_per_second': '30.755', 'grad_norm': '19.875', 'counters/examples': 158368, 'counters/updates': 4949}
train stats after 158400 examples: {'rewards_train/chosen': '0.22051', 'rewards_train/rejected': '0.040272', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18024', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-144.29', 'loss/train': '0.61972', 'examples_per_second': '30.629', 'grad_norm': '19.375', 'counters/examples': 158400, 'counters/updates': 4950}
train stats after 158432 examples: {'rewards_train/chosen': '0.15652', 'rewards_train/rejected': '0.080125', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076392', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-145.53', 'loss/train': '0.66565', 'examples_per_second': '31.347', 'grad_norm': '20.5', 'counters/examples': 158432, 'counters/updates': 4951}
train stats after 158464 examples: {'rewards_train/chosen': '0.17833', 'rewards_train/rejected': '0.11112', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067209', 'logps_train/rejected': '-148.45', 'logps_train/chosen': '-166.35', 'loss/train': '0.67085', 'examples_per_second': '30.891', 'grad_norm': '21.625', 'counters/examples': 158464, 'counters/updates': 4952}
train stats after 158496 examples: {'rewards_train/chosen': '0.036091', 'rewards_train/rejected': '-0.017432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053523', 'logps_train/rejected': '-142.35', 'logps_train/chosen': '-154.44', 'loss/train': '0.68045', 'examples_per_second': '30.581', 'grad_norm': '24.375', 'counters/examples': 158496, 'counters/updates': 4953}
train stats after 158528 examples: {'rewards_train/chosen': '0.18267', 'rewards_train/rejected': '0.041244', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14143', 'logps_train/rejected': '-105.2', 'logps_train/chosen': '-164.29', 'loss/train': '0.64255', 'examples_per_second': '31.703', 'grad_norm': '21.5', 'counters/examples': 158528, 'counters/updates': 4954}
train stats after 158560 examples: {'rewards_train/chosen': '0.12642', 'rewards_train/rejected': '0.053088', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073327', 'logps_train/rejected': '-91.088', 'logps_train/chosen': '-126.54', 'loss/train': '0.66936', 'examples_per_second': '31.309', 'grad_norm': '18.375', 'counters/examples': 158560, 'counters/updates': 4955}
skipping logging after 158592 examples to avoid logging too frequently
train stats after 158624 examples: {'rewards_train/chosen': '0.13734', 'rewards_train/rejected': '0.054945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08239', 'logps_train/rejected': '-126.37', 'logps_train/chosen': '-130.81', 'loss/train': '0.6709', 'examples_per_second': '31.327', 'grad_norm': '19.875', 'counters/examples': 158624, 'counters/updates': 4957}
train stats after 158656 examples: {'rewards_train/chosen': '0.08533', 'rewards_train/rejected': '0.039747', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045583', 'logps_train/rejected': '-104.87', 'logps_train/chosen': '-102.75', 'loss/train': '0.68331', 'examples_per_second': '30.887', 'grad_norm': '18.75', 'counters/examples': 158656, 'counters/updates': 4958}
train stats after 158688 examples: {'rewards_train/chosen': '0.16216', 'rewards_train/rejected': '-0.0060372', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1682', 'logps_train/rejected': '-106.3', 'logps_train/chosen': '-123.57', 'loss/train': '0.62614', 'examples_per_second': '30.74', 'grad_norm': '17.5', 'counters/examples': 158688, 'counters/updates': 4959}
train stats after 158720 examples: {'rewards_train/chosen': '0.19545', 'rewards_train/rejected': '-0.032591', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22804', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-177.14', 'loss/train': '0.59588', 'examples_per_second': '30.445', 'grad_norm': '20.5', 'counters/examples': 158720, 'counters/updates': 4960}
train stats after 158752 examples: {'rewards_train/chosen': '0.21392', 'rewards_train/rejected': '0.08436', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12956', 'logps_train/rejected': '-128.65', 'logps_train/chosen': '-163.79', 'loss/train': '0.64373', 'examples_per_second': '31.538', 'grad_norm': '22', 'counters/examples': 158752, 'counters/updates': 4961}
train stats after 158784 examples: {'rewards_train/chosen': '0.17439', 'rewards_train/rejected': '0.080399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093988', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-136.24', 'loss/train': '0.66059', 'examples_per_second': '30.278', 'grad_norm': '19.75', 'counters/examples': 158784, 'counters/updates': 4962}
train stats after 158816 examples: {'rewards_train/chosen': '0.13169', 'rewards_train/rejected': '0.05198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079708', 'logps_train/rejected': '-116.76', 'logps_train/chosen': '-148.65', 'loss/train': '0.6601', 'examples_per_second': '31.691', 'grad_norm': '20.875', 'counters/examples': 158816, 'counters/updates': 4963}
skipping logging after 158848 examples to avoid logging too frequently
train stats after 158880 examples: {'rewards_train/chosen': '0.062862', 'rewards_train/rejected': '0.0086793', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054182', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-139.83', 'loss/train': '0.67902', 'examples_per_second': '31.579', 'grad_norm': '20.25', 'counters/examples': 158880, 'counters/updates': 4965}
train stats after 158912 examples: {'rewards_train/chosen': '0.15923', 'rewards_train/rejected': '0.057507', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10173', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-136.34', 'loss/train': '0.66388', 'examples_per_second': '30.695', 'grad_norm': '19.5', 'counters/examples': 158912, 'counters/updates': 4966}
train stats after 158944 examples: {'rewards_train/chosen': '0.19016', 'rewards_train/rejected': '0.051067', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13909', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-139.82', 'loss/train': '0.64002', 'examples_per_second': '31.387', 'grad_norm': '20.25', 'counters/examples': 158944, 'counters/updates': 4967}
train stats after 158976 examples: {'rewards_train/chosen': '0.14811', 'rewards_train/rejected': '0.052919', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095191', 'logps_train/rejected': '-142.05', 'logps_train/chosen': '-130.39', 'loss/train': '0.65909', 'examples_per_second': '31.749', 'grad_norm': '20.375', 'counters/examples': 158976, 'counters/updates': 4968}
train stats after 159008 examples: {'rewards_train/chosen': '0.15215', 'rewards_train/rejected': '0.12467', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.027478', 'logps_train/rejected': '-135.09', 'logps_train/chosen': '-168.38', 'loss/train': '0.68773', 'examples_per_second': '30.637', 'grad_norm': '21.75', 'counters/examples': 159008, 'counters/updates': 4969}
skipping logging after 159040 examples to avoid logging too frequently
train stats after 159072 examples: {'rewards_train/chosen': '0.091839', 'rewards_train/rejected': '0.062115', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029724', 'logps_train/rejected': '-98.876', 'logps_train/chosen': '-118.39', 'loss/train': '0.68814', 'examples_per_second': '31.456', 'grad_norm': '19.625', 'counters/examples': 159072, 'counters/updates': 4971}
train stats after 159104 examples: {'rewards_train/chosen': '0.15297', 'rewards_train/rejected': '0.069607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083368', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-139.83', 'loss/train': '0.66385', 'examples_per_second': '31.416', 'grad_norm': '20.75', 'counters/examples': 159104, 'counters/updates': 4972}
train stats after 159136 examples: {'rewards_train/chosen': '0.15596', 'rewards_train/rejected': '0.084747', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071212', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-145.52', 'loss/train': '0.66271', 'examples_per_second': '30.043', 'grad_norm': '20.625', 'counters/examples': 159136, 'counters/updates': 4973}
skipping logging after 159168 examples to avoid logging too frequently
train stats after 159200 examples: {'rewards_train/chosen': '0.19232', 'rewards_train/rejected': '0.20875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016427', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-152.65', 'loss/train': '0.70946', 'examples_per_second': '31.44', 'grad_norm': '23.875', 'counters/examples': 159200, 'counters/updates': 4975}
train stats after 159232 examples: {'rewards_train/chosen': '0.098943', 'rewards_train/rejected': '-0.013868', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11281', 'logps_train/rejected': '-142.04', 'logps_train/chosen': '-179.51', 'loss/train': '0.64705', 'examples_per_second': '32.88', 'grad_norm': '22.625', 'counters/examples': 159232, 'counters/updates': 4976}
skipping logging after 159264 examples to avoid logging too frequently
train stats after 159296 examples: {'rewards_train/chosen': '0.20495', 'rewards_train/rejected': '0.10294', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10201', 'logps_train/rejected': '-98.477', 'logps_train/chosen': '-142.25', 'loss/train': '0.65644', 'examples_per_second': '31.661', 'grad_norm': '19.125', 'counters/examples': 159296, 'counters/updates': 4978}
skipping logging after 159328 examples to avoid logging too frequently
train stats after 159360 examples: {'rewards_train/chosen': '0.11029', 'rewards_train/rejected': '0.097452', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012843', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-131.2', 'loss/train': '0.69401', 'examples_per_second': '33.366', 'grad_norm': '21.75', 'counters/examples': 159360, 'counters/updates': 4980}
train stats after 159392 examples: {'rewards_train/chosen': '0.11159', 'rewards_train/rejected': '0.020941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090645', 'logps_train/rejected': '-104.65', 'logps_train/chosen': '-119.65', 'loss/train': '0.65666', 'examples_per_second': '31.517', 'grad_norm': '18.25', 'counters/examples': 159392, 'counters/updates': 4981}
train stats after 159424 examples: {'rewards_train/chosen': '0.12774', 'rewards_train/rejected': '0.029812', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097929', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-125.62', 'loss/train': '0.65948', 'examples_per_second': '30.749', 'grad_norm': '19.375', 'counters/examples': 159424, 'counters/updates': 4982}
train stats after 159456 examples: {'rewards_train/chosen': '0.16291', 'rewards_train/rejected': '0.032212', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1307', 'logps_train/rejected': '-108.04', 'logps_train/chosen': '-139.71', 'loss/train': '0.64021', 'examples_per_second': '30.462', 'grad_norm': '19.125', 'counters/examples': 159456, 'counters/updates': 4983}
train stats after 159488 examples: {'rewards_train/chosen': '0.11078', 'rewards_train/rejected': '0.12151', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010738', 'logps_train/rejected': '-114.98', 'logps_train/chosen': '-98.656', 'loss/train': '0.71138', 'examples_per_second': '31.916', 'grad_norm': '19.625', 'counters/examples': 159488, 'counters/updates': 4984}
skipping logging after 159520 examples to avoid logging too frequently
train stats after 159552 examples: {'rewards_train/chosen': '0.19254', 'rewards_train/rejected': '0.062254', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13028', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-168.46', 'loss/train': '0.64945', 'examples_per_second': '30.451', 'grad_norm': '22.125', 'counters/examples': 159552, 'counters/updates': 4986}
train stats after 159584 examples: {'rewards_train/chosen': '0.13757', 'rewards_train/rejected': '0.050278', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087293', 'logps_train/rejected': '-112.16', 'logps_train/chosen': '-131.08', 'loss/train': '0.65935', 'examples_per_second': '31.319', 'grad_norm': '19.625', 'counters/examples': 159584, 'counters/updates': 4987}
train stats after 159616 examples: {'rewards_train/chosen': '0.11452', 'rewards_train/rejected': '0.026783', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087737', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-141.09', 'loss/train': '0.65589', 'examples_per_second': '30.418', 'grad_norm': '21.125', 'counters/examples': 159616, 'counters/updates': 4988}
train stats after 159648 examples: {'rewards_train/chosen': '0.19768', 'rewards_train/rejected': '0.10044', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097246', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-124.4', 'loss/train': '0.65904', 'examples_per_second': '30.123', 'grad_norm': '22.5', 'counters/examples': 159648, 'counters/updates': 4989}
train stats after 159680 examples: {'rewards_train/chosen': '0.083164', 'rewards_train/rejected': '0.014723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068441', 'logps_train/rejected': '-118.85', 'logps_train/chosen': '-124.85', 'loss/train': '0.6688', 'examples_per_second': '30.758', 'grad_norm': '20.25', 'counters/examples': 159680, 'counters/updates': 4990}
skipping logging after 159712 examples to avoid logging too frequently
train stats after 159744 examples: {'rewards_train/chosen': '0.12656', 'rewards_train/rejected': '0.13088', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0043225', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-132.15', 'loss/train': '0.70726', 'examples_per_second': '31.834', 'grad_norm': '20', 'counters/examples': 159744, 'counters/updates': 4992}
train stats after 159776 examples: {'rewards_train/chosen': '0.15504', 'rewards_train/rejected': '0.099154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055884', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-146.3', 'loss/train': '0.68058', 'examples_per_second': '32.426', 'grad_norm': '20.5', 'counters/examples': 159776, 'counters/updates': 4993}
train stats after 159808 examples: {'rewards_train/chosen': '0.092931', 'rewards_train/rejected': '-0.078013', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17094', 'logps_train/rejected': '-124.22', 'logps_train/chosen': '-129.64', 'loss/train': '0.62775', 'examples_per_second': '31.48', 'grad_norm': '19.25', 'counters/examples': 159808, 'counters/updates': 4994}
train stats after 159840 examples: {'rewards_train/chosen': '0.13476', 'rewards_train/rejected': '-0.011257', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14602', 'logps_train/rejected': '-111.94', 'logps_train/chosen': '-158.24', 'loss/train': '0.63853', 'examples_per_second': '31.496', 'grad_norm': '19.625', 'counters/examples': 159840, 'counters/updates': 4995}
train stats after 159872 examples: {'rewards_train/chosen': '0.17949', 'rewards_train/rejected': '0.088877', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.090617', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-142.39', 'loss/train': '0.67133', 'examples_per_second': '31.612', 'grad_norm': '20.75', 'counters/examples': 159872, 'counters/updates': 4996}
train stats after 159904 examples: {'rewards_train/chosen': '0.091767', 'rewards_train/rejected': '0.027003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064764', 'logps_train/rejected': '-130.76', 'logps_train/chosen': '-100.63', 'loss/train': '0.66831', 'examples_per_second': '32.608', 'grad_norm': '19.75', 'counters/examples': 159904, 'counters/updates': 4997}
train stats after 159936 examples: {'rewards_train/chosen': '0.18138', 'rewards_train/rejected': '0.075497', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10588', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-128.45', 'loss/train': '0.6565', 'examples_per_second': '29.973', 'grad_norm': '20.25', 'counters/examples': 159936, 'counters/updates': 4998}
train stats after 159968 examples: {'rewards_train/chosen': '0.11609', 'rewards_train/rejected': '0.10179', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014303', 'logps_train/rejected': '-121', 'logps_train/chosen': '-112.49', 'loss/train': '0.69581', 'examples_per_second': '30.917', 'grad_norm': '21.125', 'counters/examples': 159968, 'counters/updates': 4999}
train stats after 160000 examples: {'rewards_train/chosen': '0.14942', 'rewards_train/rejected': '0.071419', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078005', 'logps_train/rejected': '-156.91', 'logps_train/chosen': '-156.95', 'loss/train': '0.65964', 'examples_per_second': '33.072', 'grad_norm': '22.5', 'counters/examples': 160000, 'counters/updates': 5000}
train stats after 160032 examples: {'rewards_train/chosen': '0.085162', 'rewards_train/rejected': '0.036096', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049067', 'logps_train/rejected': '-135.8', 'logps_train/chosen': '-138.53', 'loss/train': '0.67967', 'examples_per_second': '31.489', 'grad_norm': '21.125', 'counters/examples': 160032, 'counters/updates': 5001}
train stats after 160064 examples: {'rewards_train/chosen': '0.21166', 'rewards_train/rejected': '0.017017', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19464', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-161.74', 'loss/train': '0.61115', 'examples_per_second': '31.446', 'grad_norm': '21.25', 'counters/examples': 160064, 'counters/updates': 5002}
skipping logging after 160096 examples to avoid logging too frequently
train stats after 160128 examples: {'rewards_train/chosen': '0.11537', 'rewards_train/rejected': '-0.012589', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12796', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-111.58', 'loss/train': '0.64812', 'examples_per_second': '29.677', 'grad_norm': '18.875', 'counters/examples': 160128, 'counters/updates': 5004}
train stats after 160160 examples: {'rewards_train/chosen': '0.11642', 'rewards_train/rejected': '-0.0037965', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12021', 'logps_train/rejected': '-113.9', 'logps_train/chosen': '-181.62', 'loss/train': '0.64421', 'examples_per_second': '31.492', 'grad_norm': '21.375', 'counters/examples': 160160, 'counters/updates': 5005}
train stats after 160192 examples: {'rewards_train/chosen': '0.28019', 'rewards_train/rejected': '0.11162', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16857', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-151.42', 'loss/train': '0.62482', 'examples_per_second': '31.503', 'grad_norm': '21.25', 'counters/examples': 160192, 'counters/updates': 5006}
train stats after 160224 examples: {'rewards_train/chosen': '0.12329', 'rewards_train/rejected': '0.018135', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10516', 'logps_train/rejected': '-132.22', 'logps_train/chosen': '-154.37', 'loss/train': '0.65402', 'examples_per_second': '32.206', 'grad_norm': '20.75', 'counters/examples': 160224, 'counters/updates': 5007}
train stats after 160256 examples: {'rewards_train/chosen': '0.18579', 'rewards_train/rejected': '0.056946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12885', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-188.48', 'loss/train': '0.64828', 'examples_per_second': '32.044', 'grad_norm': '22.25', 'counters/examples': 160256, 'counters/updates': 5008}
train stats after 160288 examples: {'rewards_train/chosen': '0.17019', 'rewards_train/rejected': '0.036593', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13359', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-138.38', 'loss/train': '0.64191', 'examples_per_second': '32.404', 'grad_norm': '19.625', 'counters/examples': 160288, 'counters/updates': 5009}
train stats after 160320 examples: {'rewards_train/chosen': '0.13597', 'rewards_train/rejected': '0.018584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11738', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-150.76', 'loss/train': '0.64904', 'examples_per_second': '31.309', 'grad_norm': '20.5', 'counters/examples': 160320, 'counters/updates': 5010}
train stats after 160352 examples: {'rewards_train/chosen': '0.20097', 'rewards_train/rejected': '0.11688', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084082', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-154.08', 'loss/train': '0.66777', 'examples_per_second': '24.416', 'grad_norm': '24.375', 'counters/examples': 160352, 'counters/updates': 5011}
train stats after 160384 examples: {'rewards_train/chosen': '0.19762', 'rewards_train/rejected': '-0.0012976', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19892', 'logps_train/rejected': '-96.959', 'logps_train/chosen': '-147.91', 'loss/train': '0.62162', 'examples_per_second': '30.189', 'grad_norm': '18.75', 'counters/examples': 160384, 'counters/updates': 5012}
train stats after 160416 examples: {'rewards_train/chosen': '0.16757', 'rewards_train/rejected': '0.043327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12424', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-138.32', 'loss/train': '0.64448', 'examples_per_second': '32.513', 'grad_norm': '20', 'counters/examples': 160416, 'counters/updates': 5013}
skipping logging after 160448 examples to avoid logging too frequently
train stats after 160480 examples: {'rewards_train/chosen': '0.20115', 'rewards_train/rejected': '0.030066', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17109', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-147.49', 'loss/train': '0.62623', 'examples_per_second': '30.979', 'grad_norm': '20', 'counters/examples': 160480, 'counters/updates': 5015}
train stats after 160512 examples: {'rewards_train/chosen': '0.088819', 'rewards_train/rejected': '0.052277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036542', 'logps_train/rejected': '-83.557', 'logps_train/chosen': '-122.29', 'loss/train': '0.6858', 'examples_per_second': '31.311', 'grad_norm': '18.375', 'counters/examples': 160512, 'counters/updates': 5016}
train stats after 160544 examples: {'rewards_train/chosen': '0.21474', 'rewards_train/rejected': '0.092732', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12201', 'logps_train/rejected': '-120.53', 'logps_train/chosen': '-135.85', 'loss/train': '0.64288', 'examples_per_second': '31.511', 'grad_norm': '18.875', 'counters/examples': 160544, 'counters/updates': 5017}
skipping logging after 160576 examples to avoid logging too frequently
train stats after 160608 examples: {'rewards_train/chosen': '0.15156', 'rewards_train/rejected': '0.077525', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074035', 'logps_train/rejected': '-119', 'logps_train/chosen': '-116.94', 'loss/train': '0.6748', 'examples_per_second': '31.526', 'grad_norm': '20.25', 'counters/examples': 160608, 'counters/updates': 5019}
train stats after 160640 examples: {'rewards_train/chosen': '0.11974', 'rewards_train/rejected': '0.044197', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075542', 'logps_train/rejected': '-122.57', 'logps_train/chosen': '-141.42', 'loss/train': '0.66353', 'examples_per_second': '31.286', 'grad_norm': '19.625', 'counters/examples': 160640, 'counters/updates': 5020}
train stats after 160672 examples: {'rewards_train/chosen': '0.17931', 'rewards_train/rejected': '0.017447', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16186', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-114.2', 'loss/train': '0.6289', 'examples_per_second': '31.536', 'grad_norm': '19.625', 'counters/examples': 160672, 'counters/updates': 5021}
train stats after 160704 examples: {'rewards_train/chosen': '0.11756', 'rewards_train/rejected': '0.060161', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057395', 'logps_train/rejected': '-147.59', 'logps_train/chosen': '-141.03', 'loss/train': '0.67843', 'examples_per_second': '31.225', 'grad_norm': '22.125', 'counters/examples': 160704, 'counters/updates': 5022}
train stats after 160736 examples: {'rewards_train/chosen': '0.12224', 'rewards_train/rejected': '0.08053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041715', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-111.53', 'loss/train': '0.69536', 'examples_per_second': '30.748', 'grad_norm': '20.25', 'counters/examples': 160736, 'counters/updates': 5023}
train stats after 160768 examples: {'rewards_train/chosen': '0.071107', 'rewards_train/rejected': '-0.07142', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14253', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-113.86', 'loss/train': '0.63598', 'examples_per_second': '30.059', 'grad_norm': '16.875', 'counters/examples': 160768, 'counters/updates': 5024}
skipping logging after 160800 examples to avoid logging too frequently
train stats after 160832 examples: {'rewards_train/chosen': '0.13929', 'rewards_train/rejected': '0.040101', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099192', 'logps_train/rejected': '-114.93', 'logps_train/chosen': '-156.26', 'loss/train': '0.653', 'examples_per_second': '35.988', 'grad_norm': '19.75', 'counters/examples': 160832, 'counters/updates': 5026}
skipping logging after 160864 examples to avoid logging too frequently
train stats after 160896 examples: {'rewards_train/chosen': '0.12474', 'rewards_train/rejected': '0.09811', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026634', 'logps_train/rejected': '-138.22', 'logps_train/chosen': '-143.8', 'loss/train': '0.70257', 'examples_per_second': '31.335', 'grad_norm': '21.5', 'counters/examples': 160896, 'counters/updates': 5028}
train stats after 160928 examples: {'rewards_train/chosen': '0.081323', 'rewards_train/rejected': '-0.0028441', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084167', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-112.13', 'loss/train': '0.65919', 'examples_per_second': '31.738', 'grad_norm': '19.75', 'counters/examples': 160928, 'counters/updates': 5029}
skipping logging after 160960 examples to avoid logging too frequently
train stats after 160992 examples: {'rewards_train/chosen': '0.17649', 'rewards_train/rejected': '0.025575', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15091', 'logps_train/rejected': '-143', 'logps_train/chosen': '-159.56', 'loss/train': '0.64457', 'examples_per_second': '32.189', 'grad_norm': '23.625', 'counters/examples': 160992, 'counters/updates': 5031}
train stats after 161024 examples: {'rewards_train/chosen': '0.10925', 'rewards_train/rejected': '-0.00056802', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10981', 'logps_train/rejected': '-134.01', 'logps_train/chosen': '-163.07', 'loss/train': '0.65021', 'examples_per_second': '31.379', 'grad_norm': '22.625', 'counters/examples': 161024, 'counters/updates': 5032}
train stats after 161056 examples: {'rewards_train/chosen': '0.13407', 'rewards_train/rejected': '0.028225', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10585', 'logps_train/rejected': '-100.46', 'logps_train/chosen': '-133.13', 'loss/train': '0.65655', 'examples_per_second': '30.887', 'grad_norm': '19', 'counters/examples': 161056, 'counters/updates': 5033}
train stats after 161088 examples: {'rewards_train/chosen': '0.16841', 'rewards_train/rejected': '0.0074589', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16095', 'logps_train/rejected': '-108.63', 'logps_train/chosen': '-135.39', 'loss/train': '0.62874', 'examples_per_second': '32.426', 'grad_norm': '19.375', 'counters/examples': 161088, 'counters/updates': 5034}
train stats after 161120 examples: {'rewards_train/chosen': '0.14374', 'rewards_train/rejected': '0.04651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097234', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-145.51', 'loss/train': '0.65858', 'examples_per_second': '30.619', 'grad_norm': '20.625', 'counters/examples': 161120, 'counters/updates': 5035}
train stats after 161152 examples: {'rewards_train/chosen': '0.14166', 'rewards_train/rejected': '-0.010638', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1523', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-157.85', 'loss/train': '0.62743', 'examples_per_second': '31.761', 'grad_norm': '19.875', 'counters/examples': 161152, 'counters/updates': 5036}
skipping logging after 161184 examples to avoid logging too frequently
train stats after 161216 examples: {'rewards_train/chosen': '0.13939', 'rewards_train/rejected': '0.035016', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10438', 'logps_train/rejected': '-104.31', 'logps_train/chosen': '-117.86', 'loss/train': '0.65219', 'examples_per_second': '32.476', 'grad_norm': '18.375', 'counters/examples': 161216, 'counters/updates': 5038}
train stats after 161248 examples: {'rewards_train/chosen': '0.1641', 'rewards_train/rejected': '0.13765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026454', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-120.7', 'loss/train': '0.69317', 'examples_per_second': '31.608', 'grad_norm': '20.875', 'counters/examples': 161248, 'counters/updates': 5039}
train stats after 161280 examples: {'rewards_train/chosen': '0.17942', 'rewards_train/rejected': '0.020055', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15937', 'logps_train/rejected': '-108.42', 'logps_train/chosen': '-100.27', 'loss/train': '0.62867', 'examples_per_second': '31.827', 'grad_norm': '17.375', 'counters/examples': 161280, 'counters/updates': 5040}
train stats after 161312 examples: {'rewards_train/chosen': '0.096321', 'rewards_train/rejected': '0.013451', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08287', 'logps_train/rejected': '-99.058', 'logps_train/chosen': '-115.29', 'loss/train': '0.65983', 'examples_per_second': '30.374', 'grad_norm': '19.5', 'counters/examples': 161312, 'counters/updates': 5041}
train stats after 161344 examples: {'rewards_train/chosen': '0.072396', 'rewards_train/rejected': '-0.031759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10415', 'logps_train/rejected': '-99.055', 'logps_train/chosen': '-137.16', 'loss/train': '0.64941', 'examples_per_second': '30.941', 'grad_norm': '19', 'counters/examples': 161344, 'counters/updates': 5042}
train stats after 161376 examples: {'rewards_train/chosen': '0.15872', 'rewards_train/rejected': '0.036217', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1225', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-147.85', 'loss/train': '0.64016', 'examples_per_second': '32.933', 'grad_norm': '20.125', 'counters/examples': 161376, 'counters/updates': 5043}
train stats after 161408 examples: {'rewards_train/chosen': '0.11213', 'rewards_train/rejected': '-0.01307', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1252', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-121.16', 'loss/train': '0.65492', 'examples_per_second': '32.402', 'grad_norm': '19.25', 'counters/examples': 161408, 'counters/updates': 5044}
train stats after 161440 examples: {'rewards_train/chosen': '0.17632', 'rewards_train/rejected': '0.21918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.042862', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-163.24', 'loss/train': '0.72378', 'examples_per_second': '31.636', 'grad_norm': '24', 'counters/examples': 161440, 'counters/updates': 5045}
train stats after 161472 examples: {'rewards_train/chosen': '0.2293', 'rewards_train/rejected': '-0.028892', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25819', 'logps_train/rejected': '-142.2', 'logps_train/chosen': '-152.24', 'loss/train': '0.60385', 'examples_per_second': '31.557', 'grad_norm': '19.25', 'counters/examples': 161472, 'counters/updates': 5046}
skipping logging after 161504 examples to avoid logging too frequently
train stats after 161536 examples: {'rewards_train/chosen': '0.18673', 'rewards_train/rejected': '0.12272', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064009', 'logps_train/rejected': '-111.09', 'logps_train/chosen': '-124.55', 'loss/train': '0.67174', 'examples_per_second': '31.804', 'grad_norm': '19.5', 'counters/examples': 161536, 'counters/updates': 5048}
train stats after 161568 examples: {'rewards_train/chosen': '0.14563', 'rewards_train/rejected': '0.026557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11907', 'logps_train/rejected': '-132.29', 'logps_train/chosen': '-143.87', 'loss/train': '0.64766', 'examples_per_second': '31.378', 'grad_norm': '23', 'counters/examples': 161568, 'counters/updates': 5049}
train stats after 161600 examples: {'rewards_train/chosen': '0.21555', 'rewards_train/rejected': '0.073824', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14173', 'logps_train/rejected': '-128.19', 'logps_train/chosen': '-109.02', 'loss/train': '0.64382', 'examples_per_second': '30.612', 'grad_norm': '19.5', 'counters/examples': 161600, 'counters/updates': 5050}
train stats after 161632 examples: {'rewards_train/chosen': '0.12408', 'rewards_train/rejected': '0.077634', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046442', 'logps_train/rejected': '-121.29', 'logps_train/chosen': '-147.61', 'loss/train': '0.6766', 'examples_per_second': '31.169', 'grad_norm': '20.5', 'counters/examples': 161632, 'counters/updates': 5051}
train stats after 161664 examples: {'rewards_train/chosen': '0.13404', 'rewards_train/rejected': '0.040862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093178', 'logps_train/rejected': '-104.96', 'logps_train/chosen': '-135.79', 'loss/train': '0.6569', 'examples_per_second': '31.103', 'grad_norm': '19.75', 'counters/examples': 161664, 'counters/updates': 5052}
train stats after 161696 examples: {'rewards_train/chosen': '0.26452', 'rewards_train/rejected': '0.02819', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23633', 'logps_train/rejected': '-111.91', 'logps_train/chosen': '-151.93', 'loss/train': '0.60582', 'examples_per_second': '30.068', 'grad_norm': '20.125', 'counters/examples': 161696, 'counters/updates': 5053}
train stats after 161728 examples: {'rewards_train/chosen': '0.17799', 'rewards_train/rejected': '0.14767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030327', 'logps_train/rejected': '-100.71', 'logps_train/chosen': '-151.49', 'loss/train': '0.68587', 'examples_per_second': '31.538', 'grad_norm': '21', 'counters/examples': 161728, 'counters/updates': 5054}
train stats after 161760 examples: {'rewards_train/chosen': '0.1472', 'rewards_train/rejected': '-0.012032', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15923', 'logps_train/rejected': '-147.1', 'logps_train/chosen': '-164.55', 'loss/train': '0.6301', 'examples_per_second': '31.614', 'grad_norm': '22.375', 'counters/examples': 161760, 'counters/updates': 5055}
skipping logging after 161792 examples to avoid logging too frequently
train stats after 161824 examples: {'rewards_train/chosen': '0.23364', 'rewards_train/rejected': '0.0043949', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22924', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-131.38', 'loss/train': '0.59877', 'examples_per_second': '35.884', 'grad_norm': '19', 'counters/examples': 161824, 'counters/updates': 5057}
train stats after 161856 examples: {'rewards_train/chosen': '0.17774', 'rewards_train/rejected': '0.12783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049906', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-122.34', 'loss/train': '0.68156', 'examples_per_second': '30.14', 'grad_norm': '22.125', 'counters/examples': 161856, 'counters/updates': 5058}
train stats after 161888 examples: {'rewards_train/chosen': '0.10885', 'rewards_train/rejected': '-0.00422', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11307', 'logps_train/rejected': '-110.63', 'logps_train/chosen': '-143.9', 'loss/train': '0.6517', 'examples_per_second': '31.763', 'grad_norm': '18.875', 'counters/examples': 161888, 'counters/updates': 5059}
skipping logging after 161920 examples to avoid logging too frequently
train stats after 161952 examples: {'rewards_train/chosen': '0.17038', 'rewards_train/rejected': '0.071993', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09839', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-120.75', 'loss/train': '0.65824', 'examples_per_second': '31.662', 'grad_norm': '19.25', 'counters/examples': 161952, 'counters/updates': 5061}
train stats after 161984 examples: {'rewards_train/chosen': '0.17232', 'rewards_train/rejected': '0.088201', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084119', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-160.01', 'loss/train': '0.66088', 'examples_per_second': '32.392', 'grad_norm': '21.25', 'counters/examples': 161984, 'counters/updates': 5062}
train stats after 162016 examples: {'rewards_train/chosen': '0.10256', 'rewards_train/rejected': '0.043146', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059416', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-126.79', 'loss/train': '0.6763', 'examples_per_second': '31.504', 'grad_norm': '19.75', 'counters/examples': 162016, 'counters/updates': 5063}
train stats after 162048 examples: {'rewards_train/chosen': '0.11904', 'rewards_train/rejected': '-0.0095051', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12855', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-165.21', 'loss/train': '0.63942', 'examples_per_second': '31.663', 'grad_norm': '19.375', 'counters/examples': 162048, 'counters/updates': 5064}
train stats after 162080 examples: {'rewards_train/chosen': '0.15899', 'rewards_train/rejected': '0.034176', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12481', 'logps_train/rejected': '-123.23', 'logps_train/chosen': '-164.29', 'loss/train': '0.6433', 'examples_per_second': '31.635', 'grad_norm': '20.5', 'counters/examples': 162080, 'counters/updates': 5065}
train stats after 162112 examples: {'rewards_train/chosen': '0.22007', 'rewards_train/rejected': '0.062764', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15731', 'logps_train/rejected': '-99.39', 'logps_train/chosen': '-145.21', 'loss/train': '0.63226', 'examples_per_second': '30.434', 'grad_norm': '18.125', 'counters/examples': 162112, 'counters/updates': 5066}
train stats after 162144 examples: {'rewards_train/chosen': '0.16442', 'rewards_train/rejected': '0.016984', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14743', 'logps_train/rejected': '-102.52', 'logps_train/chosen': '-120.86', 'loss/train': '0.62746', 'examples_per_second': '32.686', 'grad_norm': '18.375', 'counters/examples': 162144, 'counters/updates': 5067}
train stats after 162176 examples: {'rewards_train/chosen': '0.13481', 'rewards_train/rejected': '0.010357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12446', 'logps_train/rejected': '-104.28', 'logps_train/chosen': '-128.28', 'loss/train': '0.64725', 'examples_per_second': '31.675', 'grad_norm': '20.75', 'counters/examples': 162176, 'counters/updates': 5068}
train stats after 162208 examples: {'rewards_train/chosen': '0.044309', 'rewards_train/rejected': '0.012766', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031543', 'logps_train/rejected': '-130.37', 'logps_train/chosen': '-101.77', 'loss/train': '0.68934', 'examples_per_second': '30.187', 'grad_norm': '19.5', 'counters/examples': 162208, 'counters/updates': 5069}
skipping logging after 162240 examples to avoid logging too frequently
train stats after 162272 examples: {'rewards_train/chosen': '0.09762', 'rewards_train/rejected': '-0.0071886', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10481', 'logps_train/rejected': '-101.04', 'logps_train/chosen': '-149.36', 'loss/train': '0.65507', 'examples_per_second': '32.847', 'grad_norm': '19.5', 'counters/examples': 162272, 'counters/updates': 5071}
train stats after 162304 examples: {'rewards_train/chosen': '0.10345', 'rewards_train/rejected': '0.0052987', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098153', 'logps_train/rejected': '-88.313', 'logps_train/chosen': '-121.65', 'loss/train': '0.66045', 'examples_per_second': '30.487', 'grad_norm': '18', 'counters/examples': 162304, 'counters/updates': 5072}
train stats after 162336 examples: {'rewards_train/chosen': '0.18014', 'rewards_train/rejected': '-0.021198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20134', 'logps_train/rejected': '-95.086', 'logps_train/chosen': '-142.36', 'loss/train': '0.61353', 'examples_per_second': '32.704', 'grad_norm': '18.75', 'counters/examples': 162336, 'counters/updates': 5073}
skipping logging after 162368 examples to avoid logging too frequently
train stats after 162400 examples: {'rewards_train/chosen': '0.17265', 'rewards_train/rejected': '0.018412', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15423', 'logps_train/rejected': '-91.773', 'logps_train/chosen': '-124.55', 'loss/train': '0.63777', 'examples_per_second': '34.578', 'grad_norm': '18', 'counters/examples': 162400, 'counters/updates': 5075}
train stats after 162432 examples: {'rewards_train/chosen': '0.20785', 'rewards_train/rejected': '0.11704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090814', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-118.41', 'loss/train': '0.66143', 'examples_per_second': '31.859', 'grad_norm': '18.625', 'counters/examples': 162432, 'counters/updates': 5076}
skipping logging after 162464 examples to avoid logging too frequently
train stats after 162496 examples: {'rewards_train/chosen': '0.14228', 'rewards_train/rejected': '0.0028012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13948', 'logps_train/rejected': '-113.62', 'logps_train/chosen': '-135.99', 'loss/train': '0.64047', 'examples_per_second': '31.736', 'grad_norm': '20.5', 'counters/examples': 162496, 'counters/updates': 5078}
train stats after 162528 examples: {'rewards_train/chosen': '0.15319', 'rewards_train/rejected': '0.0040217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14917', 'logps_train/rejected': '-125.42', 'logps_train/chosen': '-131.93', 'loss/train': '0.63268', 'examples_per_second': '31.684', 'grad_norm': '20', 'counters/examples': 162528, 'counters/updates': 5079}
train stats after 162560 examples: {'rewards_train/chosen': '0.1417', 'rewards_train/rejected': '0.0096751', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13202', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-139', 'loss/train': '0.63942', 'examples_per_second': '31.646', 'grad_norm': '20', 'counters/examples': 162560, 'counters/updates': 5080}
train stats after 162592 examples: {'rewards_train/chosen': '0.24962', 'rewards_train/rejected': '0.12416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12546', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-150.18', 'loss/train': '0.63894', 'examples_per_second': '31.791', 'grad_norm': '20.625', 'counters/examples': 162592, 'counters/updates': 5081}
skipping logging after 162624 examples to avoid logging too frequently
train stats after 162656 examples: {'rewards_train/chosen': '0.1488', 'rewards_train/rejected': '-0.0291', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1779', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-172.34', 'loss/train': '0.62582', 'examples_per_second': '31.293', 'grad_norm': '22.25', 'counters/examples': 162656, 'counters/updates': 5083}
train stats after 162688 examples: {'rewards_train/chosen': '0.13519', 'rewards_train/rejected': '0.055987', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0792', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-115.31', 'loss/train': '0.66358', 'examples_per_second': '30.065', 'grad_norm': '19.875', 'counters/examples': 162688, 'counters/updates': 5084}
train stats after 162720 examples: {'rewards_train/chosen': '0.19929', 'rewards_train/rejected': '0.12403', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075261', 'logps_train/rejected': '-138.24', 'logps_train/chosen': '-144.66', 'loss/train': '0.66908', 'examples_per_second': '31.7', 'grad_norm': '21.625', 'counters/examples': 162720, 'counters/updates': 5085}
train stats after 162752 examples: {'rewards_train/chosen': '0.096066', 'rewards_train/rejected': '0.062789', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033277', 'logps_train/rejected': '-106.39', 'logps_train/chosen': '-100.8', 'loss/train': '0.68435', 'examples_per_second': '32.414', 'grad_norm': '18.25', 'counters/examples': 162752, 'counters/updates': 5086}
train stats after 162784 examples: {'rewards_train/chosen': '0.16831', 'rewards_train/rejected': '0.052522', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11579', 'logps_train/rejected': '-122.96', 'logps_train/chosen': '-191.13', 'loss/train': '0.65249', 'examples_per_second': '31.184', 'grad_norm': '23', 'counters/examples': 162784, 'counters/updates': 5087}
train stats after 162816 examples: {'rewards_train/chosen': '0.16652', 'rewards_train/rejected': '0.090347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076173', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-125.42', 'loss/train': '0.67429', 'examples_per_second': '31.527', 'grad_norm': '19.625', 'counters/examples': 162816, 'counters/updates': 5088}
train stats after 162848 examples: {'rewards_train/chosen': '0.070171', 'rewards_train/rejected': '0.094612', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.024441', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-115.52', 'loss/train': '0.71623', 'examples_per_second': '30.537', 'grad_norm': '21.75', 'counters/examples': 162848, 'counters/updates': 5089}
train stats after 162880 examples: {'rewards_train/chosen': '0.19741', 'rewards_train/rejected': '0.064298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13312', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-144.07', 'loss/train': '0.65554', 'examples_per_second': '22.149', 'grad_norm': '22.25', 'counters/examples': 162880, 'counters/updates': 5090}
train stats after 162912 examples: {'rewards_train/chosen': '0.089423', 'rewards_train/rejected': '0.088045', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0013774', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-135.29', 'loss/train': '0.70436', 'examples_per_second': '31.645', 'grad_norm': '21.625', 'counters/examples': 162912, 'counters/updates': 5091}
skipping logging after 162944 examples to avoid logging too frequently
train stats after 162976 examples: {'rewards_train/chosen': '0.23221', 'rewards_train/rejected': '0.056456', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17575', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-144.87', 'loss/train': '0.6221', 'examples_per_second': '23.575', 'grad_norm': '18.5', 'counters/examples': 162976, 'counters/updates': 5093}
train stats after 163008 examples: {'rewards_train/chosen': '0.14564', 'rewards_train/rejected': '0.033103', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11254', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-147.84', 'loss/train': '0.64892', 'examples_per_second': '32.618', 'grad_norm': '20.625', 'counters/examples': 163008, 'counters/updates': 5094}
train stats after 163040 examples: {'rewards_train/chosen': '0.10522', 'rewards_train/rejected': '0.0052999', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099921', 'logps_train/rejected': '-98.77', 'logps_train/chosen': '-137.38', 'loss/train': '0.6583', 'examples_per_second': '30.88', 'grad_norm': '19.875', 'counters/examples': 163040, 'counters/updates': 5095}
train stats after 163072 examples: {'rewards_train/chosen': '0.12431', 'rewards_train/rejected': '0.0051892', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11912', 'logps_train/rejected': '-117.46', 'logps_train/chosen': '-125.43', 'loss/train': '0.65044', 'examples_per_second': '31.143', 'grad_norm': '18', 'counters/examples': 163072, 'counters/updates': 5096}
skipping logging after 163104 examples to avoid logging too frequently
train stats after 163136 examples: {'rewards_train/chosen': '0.17532', 'rewards_train/rejected': '0.082331', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092988', 'logps_train/rejected': '-152.41', 'logps_train/chosen': '-162.05', 'loss/train': '0.66457', 'examples_per_second': '31.523', 'grad_norm': '23.5', 'counters/examples': 163136, 'counters/updates': 5098}
skipping logging after 163168 examples to avoid logging too frequently
train stats after 163200 examples: {'rewards_train/chosen': '0.13275', 'rewards_train/rejected': '0.030371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10237', 'logps_train/rejected': '-125.34', 'logps_train/chosen': '-139.57', 'loss/train': '0.65478', 'examples_per_second': '30.213', 'grad_norm': '20.25', 'counters/examples': 163200, 'counters/updates': 5100}
train stats after 163232 examples: {'rewards_train/chosen': '0.138', 'rewards_train/rejected': '0.058132', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079867', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-159.31', 'loss/train': '0.66469', 'examples_per_second': '30.499', 'grad_norm': '22.75', 'counters/examples': 163232, 'counters/updates': 5101}
skipping logging after 163264 examples to avoid logging too frequently
train stats after 163296 examples: {'rewards_train/chosen': '0.1077', 'rewards_train/rejected': '0.062745', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044959', 'logps_train/rejected': '-92.724', 'logps_train/chosen': '-134.33', 'loss/train': '0.68082', 'examples_per_second': '31.318', 'grad_norm': '18.875', 'counters/examples': 163296, 'counters/updates': 5103}
train stats after 163328 examples: {'rewards_train/chosen': '0.16959', 'rewards_train/rejected': '0.1023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067298', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-127.02', 'loss/train': '0.66957', 'examples_per_second': '31.253', 'grad_norm': '19.5', 'counters/examples': 163328, 'counters/updates': 5104}
train stats after 163360 examples: {'rewards_train/chosen': '0.1787', 'rewards_train/rejected': '0.1262', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052499', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-128.25', 'loss/train': '0.67905', 'examples_per_second': '31.681', 'grad_norm': '20.375', 'counters/examples': 163360, 'counters/updates': 5105}
train stats after 163392 examples: {'rewards_train/chosen': '0.071886', 'rewards_train/rejected': '0.088789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016903', 'logps_train/rejected': '-117.5', 'logps_train/chosen': '-129.44', 'loss/train': '0.71212', 'examples_per_second': '31.767', 'grad_norm': '20.375', 'counters/examples': 163392, 'counters/updates': 5106}
train stats after 163424 examples: {'rewards_train/chosen': '0.23238', 'rewards_train/rejected': '0.065867', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16651', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-155.33', 'loss/train': '0.62678', 'examples_per_second': '31.722', 'grad_norm': '20.125', 'counters/examples': 163424, 'counters/updates': 5107}
train stats after 163456 examples: {'rewards_train/chosen': '0.1029', 'rewards_train/rejected': '0.035213', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06769', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-137.68', 'loss/train': '0.6677', 'examples_per_second': '31.711', 'grad_norm': '20.75', 'counters/examples': 163456, 'counters/updates': 5108}
train stats after 163488 examples: {'rewards_train/chosen': '0.17735', 'rewards_train/rejected': '0.09765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079697', 'logps_train/rejected': '-131.47', 'logps_train/chosen': '-126.27', 'loss/train': '0.66889', 'examples_per_second': '31.212', 'grad_norm': '21', 'counters/examples': 163488, 'counters/updates': 5109}
skipping logging after 163520 examples to avoid logging too frequently
train stats after 163552 examples: {'rewards_train/chosen': '0.10551', 'rewards_train/rejected': '0.065282', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040229', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-155.11', 'loss/train': '0.6842', 'examples_per_second': '32.221', 'grad_norm': '21.75', 'counters/examples': 163552, 'counters/updates': 5111}
train stats after 163584 examples: {'rewards_train/chosen': '0.21528', 'rewards_train/rejected': '0.061759', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15352', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-121.83', 'loss/train': '0.62882', 'examples_per_second': '31.504', 'grad_norm': '18.625', 'counters/examples': 163584, 'counters/updates': 5112}
train stats after 163616 examples: {'rewards_train/chosen': '0.18162', 'rewards_train/rejected': '0.051642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12998', 'logps_train/rejected': '-100.21', 'logps_train/chosen': '-130.07', 'loss/train': '0.65015', 'examples_per_second': '32.819', 'grad_norm': '18.875', 'counters/examples': 163616, 'counters/updates': 5113}
train stats after 163648 examples: {'rewards_train/chosen': '0.15912', 'rewards_train/rejected': '0.027642', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13148', 'logps_train/rejected': '-98.384', 'logps_train/chosen': '-145.63', 'loss/train': '0.64269', 'examples_per_second': '31.83', 'grad_norm': '19.125', 'counters/examples': 163648, 'counters/updates': 5114}
train stats after 163680 examples: {'rewards_train/chosen': '0.19393', 'rewards_train/rejected': '0.11338', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080548', 'logps_train/rejected': '-115.93', 'logps_train/chosen': '-122.61', 'loss/train': '0.67841', 'examples_per_second': '31.684', 'grad_norm': '20', 'counters/examples': 163680, 'counters/updates': 5115}
train stats after 163712 examples: {'rewards_train/chosen': '0.12968', 'rewards_train/rejected': '0.081491', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048189', 'logps_train/rejected': '-127.95', 'logps_train/chosen': '-105.53', 'loss/train': '0.68247', 'examples_per_second': '30.67', 'grad_norm': '21', 'counters/examples': 163712, 'counters/updates': 5116}
train stats after 163744 examples: {'rewards_train/chosen': '0.16891', 'rewards_train/rejected': '0.058828', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11008', 'logps_train/rejected': '-113.57', 'logps_train/chosen': '-124.9', 'loss/train': '0.64729', 'examples_per_second': '30.136', 'grad_norm': '18.5', 'counters/examples': 163744, 'counters/updates': 5117}
skipping logging after 163776 examples to avoid logging too frequently
train stats after 163808 examples: {'rewards_train/chosen': '0.25757', 'rewards_train/rejected': '0.047542', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21003', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-143.54', 'loss/train': '0.60531', 'examples_per_second': '34.671', 'grad_norm': '19.5', 'counters/examples': 163808, 'counters/updates': 5119}
train stats after 163840 examples: {'rewards_train/chosen': '0.16989', 'rewards_train/rejected': '0.010163', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15972', 'logps_train/rejected': '-118.47', 'logps_train/chosen': '-128.53', 'loss/train': '0.64332', 'examples_per_second': '31.497', 'grad_norm': '22', 'counters/examples': 163840, 'counters/updates': 5120}
train stats after 163872 examples: {'rewards_train/chosen': '0.16575', 'rewards_train/rejected': '0.042167', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12358', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-140.2', 'loss/train': '0.64056', 'examples_per_second': '31.642', 'grad_norm': '22.5', 'counters/examples': 163872, 'counters/updates': 5121}
train stats after 163904 examples: {'rewards_train/chosen': '0.24932', 'rewards_train/rejected': '0.058608', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19072', 'logps_train/rejected': '-101.85', 'logps_train/chosen': '-196.98', 'loss/train': '0.62325', 'examples_per_second': '30.179', 'grad_norm': '21.875', 'counters/examples': 163904, 'counters/updates': 5122}
train stats after 163936 examples: {'rewards_train/chosen': '0.14523', 'rewards_train/rejected': '0.085176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060059', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-110.34', 'loss/train': '0.67512', 'examples_per_second': '31.696', 'grad_norm': '19.75', 'counters/examples': 163936, 'counters/updates': 5123}
train stats after 163968 examples: {'rewards_train/chosen': '0.23716', 'rewards_train/rejected': '0.08303', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15413', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-156.44', 'loss/train': '0.63864', 'examples_per_second': '30.299', 'grad_norm': '21.875', 'counters/examples': 163968, 'counters/updates': 5124}
skipping logging after 164000 examples to avoid logging too frequently
train stats after 164032 examples: {'rewards_train/chosen': '0.15059', 'rewards_train/rejected': '0.10262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047974', 'logps_train/rejected': '-101.23', 'logps_train/chosen': '-108.34', 'loss/train': '0.68471', 'examples_per_second': '31.543', 'grad_norm': '18.25', 'counters/examples': 164032, 'counters/updates': 5126}
skipping logging after 164064 examples to avoid logging too frequently
train stats after 164096 examples: {'rewards_train/chosen': '0.17579', 'rewards_train/rejected': '0.028645', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14714', 'logps_train/rejected': '-127.23', 'logps_train/chosen': '-181.35', 'loss/train': '0.63881', 'examples_per_second': '30.175', 'grad_norm': '21.25', 'counters/examples': 164096, 'counters/updates': 5128}
train stats after 164128 examples: {'rewards_train/chosen': '0.1774', 'rewards_train/rejected': '0.032903', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1445', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-145.64', 'loss/train': '0.63451', 'examples_per_second': '32.54', 'grad_norm': '20.5', 'counters/examples': 164128, 'counters/updates': 5129}
skipping logging after 164160 examples to avoid logging too frequently
train stats after 164192 examples: {'rewards_train/chosen': '0.18405', 'rewards_train/rejected': '-0.0045649', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18861', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-174.69', 'loss/train': '0.61372', 'examples_per_second': '30.86', 'grad_norm': '20.5', 'counters/examples': 164192, 'counters/updates': 5131}
train stats after 164224 examples: {'rewards_train/chosen': '0.19557', 'rewards_train/rejected': '-0.059726', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25529', 'logps_train/rejected': '-114.47', 'logps_train/chosen': '-150.25', 'loss/train': '0.58995', 'examples_per_second': '32.071', 'grad_norm': '19.125', 'counters/examples': 164224, 'counters/updates': 5132}
train stats after 164256 examples: {'rewards_train/chosen': '0.18793', 'rewards_train/rejected': '0.044436', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1435', 'logps_train/rejected': '-121.13', 'logps_train/chosen': '-168.88', 'loss/train': '0.64696', 'examples_per_second': '32.58', 'grad_norm': '22.625', 'counters/examples': 164256, 'counters/updates': 5133}
skipping logging after 164288 examples to avoid logging too frequently
train stats after 164320 examples: {'rewards_train/chosen': '0.10548', 'rewards_train/rejected': '0.02558', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079905', 'logps_train/rejected': '-99.565', 'logps_train/chosen': '-110.58', 'loss/train': '0.662', 'examples_per_second': '32.22', 'grad_norm': '17.625', 'counters/examples': 164320, 'counters/updates': 5135}
skipping logging after 164352 examples to avoid logging too frequently
train stats after 164384 examples: {'rewards_train/chosen': '0.05749', 'rewards_train/rejected': '0.022431', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.035059', 'logps_train/rejected': '-102.88', 'logps_train/chosen': '-119.6', 'loss/train': '0.68548', 'examples_per_second': '30.162', 'grad_norm': '19.625', 'counters/examples': 164384, 'counters/updates': 5137}
train stats after 164416 examples: {'rewards_train/chosen': '0.1111', 'rewards_train/rejected': '-0.030816', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14191', 'logps_train/rejected': '-109.47', 'logps_train/chosen': '-135.18', 'loss/train': '0.63194', 'examples_per_second': '31.675', 'grad_norm': '19.625', 'counters/examples': 164416, 'counters/updates': 5138}
train stats after 164448 examples: {'rewards_train/chosen': '0.18161', 'rewards_train/rejected': '0.085807', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095807', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-150.66', 'loss/train': '0.65983', 'examples_per_second': '30.177', 'grad_norm': '21.125', 'counters/examples': 164448, 'counters/updates': 5139}
train stats after 164480 examples: {'rewards_train/chosen': '0.18698', 'rewards_train/rejected': '-0.0057396', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19272', 'logps_train/rejected': '-157.21', 'logps_train/chosen': '-155.79', 'loss/train': '0.61451', 'examples_per_second': '32.797', 'grad_norm': '21', 'counters/examples': 164480, 'counters/updates': 5140}
train stats after 164512 examples: {'rewards_train/chosen': '0.17388', 'rewards_train/rejected': '0.046901', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12698', 'logps_train/rejected': '-176.92', 'logps_train/chosen': '-186.14', 'loss/train': '0.64572', 'examples_per_second': '30.166', 'grad_norm': '25.125', 'counters/examples': 164512, 'counters/updates': 5141}
train stats after 164544 examples: {'rewards_train/chosen': '0.12208', 'rewards_train/rejected': '0.012215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10987', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-138.76', 'loss/train': '0.6496', 'examples_per_second': '30.835', 'grad_norm': '19.5', 'counters/examples': 164544, 'counters/updates': 5142}
skipping logging after 164576 examples to avoid logging too frequently
train stats after 164608 examples: {'rewards_train/chosen': '0.15251', 'rewards_train/rejected': '0.068054', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084459', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-128.95', 'loss/train': '0.66711', 'examples_per_second': '30.326', 'grad_norm': '20.125', 'counters/examples': 164608, 'counters/updates': 5144}
train stats after 164640 examples: {'rewards_train/chosen': '0.17091', 'rewards_train/rejected': '0.0084128', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16249', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-117.97', 'loss/train': '0.62377', 'examples_per_second': '31.029', 'grad_norm': '19', 'counters/examples': 164640, 'counters/updates': 5145}
skipping logging after 164672 examples to avoid logging too frequently
train stats after 164704 examples: {'rewards_train/chosen': '0.19342', 'rewards_train/rejected': '0.047746', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14567', 'logps_train/rejected': '-113.86', 'logps_train/chosen': '-123', 'loss/train': '0.63879', 'examples_per_second': '31.416', 'grad_norm': '20.375', 'counters/examples': 164704, 'counters/updates': 5147}
train stats after 164736 examples: {'rewards_train/chosen': '0.18136', 'rewards_train/rejected': '0.051141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13022', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-129.7', 'loss/train': '0.64089', 'examples_per_second': '31.334', 'grad_norm': '19', 'counters/examples': 164736, 'counters/updates': 5148}
train stats after 164768 examples: {'rewards_train/chosen': '0.26342', 'rewards_train/rejected': '-0.0078363', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27126', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-142.2', 'loss/train': '0.58091', 'examples_per_second': '31.628', 'grad_norm': '16.875', 'counters/examples': 164768, 'counters/updates': 5149}
train stats after 164800 examples: {'rewards_train/chosen': '0.26259', 'rewards_train/rejected': '0.10885', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15374', 'logps_train/rejected': '-124.66', 'logps_train/chosen': '-137.38', 'loss/train': '0.63041', 'examples_per_second': '32.016', 'grad_norm': '19.25', 'counters/examples': 164800, 'counters/updates': 5150}
skipping logging after 164832 examples to avoid logging too frequently
train stats after 164864 examples: {'rewards_train/chosen': '0.24798', 'rewards_train/rejected': '0.15523', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092752', 'logps_train/rejected': '-147.61', 'logps_train/chosen': '-156.07', 'loss/train': '0.66812', 'examples_per_second': '31.627', 'grad_norm': '22.875', 'counters/examples': 164864, 'counters/updates': 5152}
train stats after 164896 examples: {'rewards_train/chosen': '0.14858', 'rewards_train/rejected': '0.039717', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10887', 'logps_train/rejected': '-97.871', 'logps_train/chosen': '-126.86', 'loss/train': '0.65005', 'examples_per_second': '30.896', 'grad_norm': '18.5', 'counters/examples': 164896, 'counters/updates': 5153}
skipping logging after 164928 examples to avoid logging too frequently
train stats after 164960 examples: {'rewards_train/chosen': '0.1177', 'rewards_train/rejected': '0.057804', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059897', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-117.85', 'loss/train': '0.67433', 'examples_per_second': '30.529', 'grad_norm': '19.375', 'counters/examples': 164960, 'counters/updates': 5155}
train stats after 164992 examples: {'rewards_train/chosen': '0.17699', 'rewards_train/rejected': '0.056601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12039', 'logps_train/rejected': '-104.13', 'logps_train/chosen': '-141.6', 'loss/train': '0.65141', 'examples_per_second': '31.505', 'grad_norm': '20.25', 'counters/examples': 164992, 'counters/updates': 5156}
train stats after 165024 examples: {'rewards_train/chosen': '0.16534', 'rewards_train/rejected': '0.012077', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15327', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-112.62', 'loss/train': '0.6275', 'examples_per_second': '30.104', 'grad_norm': '18.75', 'counters/examples': 165024, 'counters/updates': 5157}
train stats after 165056 examples: {'rewards_train/chosen': '0.10858', 'rewards_train/rejected': '0.033101', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075479', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-121.35', 'loss/train': '0.67059', 'examples_per_second': '32.564', 'grad_norm': '19.75', 'counters/examples': 165056, 'counters/updates': 5158}
train stats after 165088 examples: {'rewards_train/chosen': '0.15083', 'rewards_train/rejected': '0.080923', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069904', 'logps_train/rejected': '-128.55', 'logps_train/chosen': '-152.9', 'loss/train': '0.67146', 'examples_per_second': '31.654', 'grad_norm': '21.5', 'counters/examples': 165088, 'counters/updates': 5159}
train stats after 165120 examples: {'rewards_train/chosen': '0.16887', 'rewards_train/rejected': '0.01815', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15072', 'logps_train/rejected': '-120.61', 'logps_train/chosen': '-123.36', 'loss/train': '0.63339', 'examples_per_second': '32.826', 'grad_norm': '20.5', 'counters/examples': 165120, 'counters/updates': 5160}
train stats after 165152 examples: {'rewards_train/chosen': '0.26989', 'rewards_train/rejected': '0.080616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18928', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-132.44', 'loss/train': '0.62244', 'examples_per_second': '30.453', 'grad_norm': '20.875', 'counters/examples': 165152, 'counters/updates': 5161}
train stats after 165184 examples: {'rewards_train/chosen': '0.11413', 'rewards_train/rejected': '-0.025714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13984', 'logps_train/rejected': '-134.77', 'logps_train/chosen': '-163.38', 'loss/train': '0.63624', 'examples_per_second': '31.576', 'grad_norm': '21', 'counters/examples': 165184, 'counters/updates': 5162}
train stats after 165216 examples: {'rewards_train/chosen': '0.11314', 'rewards_train/rejected': '0.11029', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0028539', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-132.73', 'loss/train': '0.70354', 'examples_per_second': '31.566', 'grad_norm': '22.125', 'counters/examples': 165216, 'counters/updates': 5163}
skipping logging after 165248 examples to avoid logging too frequently
train stats after 165280 examples: {'rewards_train/chosen': '0.142', 'rewards_train/rejected': '0.081117', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060879', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-119.22', 'loss/train': '0.67063', 'examples_per_second': '31.669', 'grad_norm': '21.75', 'counters/examples': 165280, 'counters/updates': 5165}
train stats after 165312 examples: {'rewards_train/chosen': '0.20626', 'rewards_train/rejected': '0.035178', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17108', 'logps_train/rejected': '-151.33', 'logps_train/chosen': '-142.58', 'loss/train': '0.63106', 'examples_per_second': '31.354', 'grad_norm': '21.75', 'counters/examples': 165312, 'counters/updates': 5166}
train stats after 165344 examples: {'rewards_train/chosen': '0.13654', 'rewards_train/rejected': '0.024688', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11185', 'logps_train/rejected': '-108.59', 'logps_train/chosen': '-135.57', 'loss/train': '0.65', 'examples_per_second': '30.539', 'grad_norm': '19.75', 'counters/examples': 165344, 'counters/updates': 5167}
skipping logging after 165376 examples to avoid logging too frequently
train stats after 165408 examples: {'rewards_train/chosen': '0.21756', 'rewards_train/rejected': '0.027342', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19021', 'logps_train/rejected': '-117', 'logps_train/chosen': '-128.15', 'loss/train': '0.61309', 'examples_per_second': '31.137', 'grad_norm': '18.25', 'counters/examples': 165408, 'counters/updates': 5169}
train stats after 165440 examples: {'rewards_train/chosen': '0.14501', 'rewards_train/rejected': '0.025412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11959', 'logps_train/rejected': '-110.57', 'logps_train/chosen': '-139.26', 'loss/train': '0.64903', 'examples_per_second': '31.565', 'grad_norm': '19.375', 'counters/examples': 165440, 'counters/updates': 5170}
train stats after 165472 examples: {'rewards_train/chosen': '0.043749', 'rewards_train/rejected': '-0.015198', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058947', 'logps_train/rejected': '-93.111', 'logps_train/chosen': '-150.68', 'loss/train': '0.67324', 'examples_per_second': '31.322', 'grad_norm': '20.625', 'counters/examples': 165472, 'counters/updates': 5171}
train stats after 165504 examples: {'rewards_train/chosen': '0.17125', 'rewards_train/rejected': '-0.050635', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22188', 'logps_train/rejected': '-129.51', 'logps_train/chosen': '-116.74', 'loss/train': '0.60256', 'examples_per_second': '31.084', 'grad_norm': '18.5', 'counters/examples': 165504, 'counters/updates': 5172}
train stats after 165536 examples: {'rewards_train/chosen': '0.24268', 'rewards_train/rejected': '0.14031', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10237', 'logps_train/rejected': '-143.08', 'logps_train/chosen': '-134.66', 'loss/train': '0.65607', 'examples_per_second': '30.779', 'grad_norm': '21.375', 'counters/examples': 165536, 'counters/updates': 5173}
train stats after 165568 examples: {'rewards_train/chosen': '0.03021', 'rewards_train/rejected': '-0.069647', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.099857', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-111.12', 'loss/train': '0.65546', 'examples_per_second': '31.936', 'grad_norm': '19.875', 'counters/examples': 165568, 'counters/updates': 5174}
train stats after 165600 examples: {'rewards_train/chosen': '0.13377', 'rewards_train/rejected': '0.0021625', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13161', 'logps_train/rejected': '-122.69', 'logps_train/chosen': '-139.64', 'loss/train': '0.64376', 'examples_per_second': '30.173', 'grad_norm': '19.625', 'counters/examples': 165600, 'counters/updates': 5175}
skipping logging after 165632 examples to avoid logging too frequently
train stats after 165664 examples: {'rewards_train/chosen': '0.16431', 'rewards_train/rejected': '0.028203', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13611', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-130.05', 'loss/train': '0.64664', 'examples_per_second': '37.164', 'grad_norm': '20.75', 'counters/examples': 165664, 'counters/updates': 5177}
train stats after 165696 examples: {'rewards_train/chosen': '0.21073', 'rewards_train/rejected': '0.029734', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.181', 'logps_train/rejected': '-102.9', 'logps_train/chosen': '-127.39', 'loss/train': '0.62184', 'examples_per_second': '30.556', 'grad_norm': '18.625', 'counters/examples': 165696, 'counters/updates': 5178}
skipping logging after 165728 examples to avoid logging too frequently
train stats after 165760 examples: {'rewards_train/chosen': '0.14136', 'rewards_train/rejected': '0.097788', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04357', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-105.75', 'loss/train': '0.68318', 'examples_per_second': '31.833', 'grad_norm': '19.25', 'counters/examples': 165760, 'counters/updates': 5180}
train stats after 165792 examples: {'rewards_train/chosen': '0.20064', 'rewards_train/rejected': '0.0061562', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19448', 'logps_train/rejected': '-127.71', 'logps_train/chosen': '-134.85', 'loss/train': '0.61719', 'examples_per_second': '31.821', 'grad_norm': '19.75', 'counters/examples': 165792, 'counters/updates': 5181}
train stats after 165824 examples: {'rewards_train/chosen': '0.16784', 'rewards_train/rejected': '0.10538', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062457', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-128.94', 'loss/train': '0.67226', 'examples_per_second': '30.731', 'grad_norm': '21.25', 'counters/examples': 165824, 'counters/updates': 5182}
train stats after 165856 examples: {'rewards_train/chosen': '0.064852', 'rewards_train/rejected': '0.067404', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0025517', 'logps_train/rejected': '-119.95', 'logps_train/chosen': '-161.04', 'loss/train': '0.70625', 'examples_per_second': '30.093', 'grad_norm': '22.25', 'counters/examples': 165856, 'counters/updates': 5183}
train stats after 165888 examples: {'rewards_train/chosen': '0.12764', 'rewards_train/rejected': '0.033478', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09416', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-161.66', 'loss/train': '0.66321', 'examples_per_second': '32.555', 'grad_norm': '21.25', 'counters/examples': 165888, 'counters/updates': 5184}
train stats after 165920 examples: {'rewards_train/chosen': '0.081577', 'rewards_train/rejected': '0.09046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.008883', 'logps_train/rejected': '-101.76', 'logps_train/chosen': '-130.55', 'loss/train': '0.70599', 'examples_per_second': '27.599', 'grad_norm': '20.5', 'counters/examples': 165920, 'counters/updates': 5185}
train stats after 165952 examples: {'rewards_train/chosen': '0.15895', 'rewards_train/rejected': '0.039474', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11948', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-110.55', 'loss/train': '0.64784', 'examples_per_second': '31.581', 'grad_norm': '19.125', 'counters/examples': 165952, 'counters/updates': 5186}
skipping logging after 165984 examples to avoid logging too frequently
train stats after 166016 examples: {'rewards_train/chosen': '0.23614', 'rewards_train/rejected': '0.13887', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097276', 'logps_train/rejected': '-159', 'logps_train/chosen': '-138.55', 'loss/train': '0.65765', 'examples_per_second': '30.436', 'grad_norm': '21.375', 'counters/examples': 166016, 'counters/updates': 5188}
train stats after 166048 examples: {'rewards_train/chosen': '0.19589', 'rewards_train/rejected': '0.027639', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16825', 'logps_train/rejected': '-152.08', 'logps_train/chosen': '-137.64', 'loss/train': '0.6353', 'examples_per_second': '31.647', 'grad_norm': '21.875', 'counters/examples': 166048, 'counters/updates': 5189}
skipping logging after 166080 examples to avoid logging too frequently
train stats after 166112 examples: {'rewards_train/chosen': '0.22977', 'rewards_train/rejected': '0.022733', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20704', 'logps_train/rejected': '-110.62', 'logps_train/chosen': '-141.82', 'loss/train': '0.60776', 'examples_per_second': '33.51', 'grad_norm': '17.375', 'counters/examples': 166112, 'counters/updates': 5191}
train stats after 166144 examples: {'rewards_train/chosen': '0.2122', 'rewards_train/rejected': '0.075755', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13644', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-141.23', 'loss/train': '0.64818', 'examples_per_second': '31.522', 'grad_norm': '22.375', 'counters/examples': 166144, 'counters/updates': 5192}
train stats after 166176 examples: {'rewards_train/chosen': '0.11925', 'rewards_train/rejected': '0.050183', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06907', 'logps_train/rejected': '-123.92', 'logps_train/chosen': '-103.21', 'loss/train': '0.67394', 'examples_per_second': '31.566', 'grad_norm': '18.625', 'counters/examples': 166176, 'counters/updates': 5193}
train stats after 166208 examples: {'rewards_train/chosen': '0.17776', 'rewards_train/rejected': '0.041677', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13608', 'logps_train/rejected': '-117.95', 'logps_train/chosen': '-180.65', 'loss/train': '0.63509', 'examples_per_second': '32.677', 'grad_norm': '21', 'counters/examples': 166208, 'counters/updates': 5194}
train stats after 166240 examples: {'rewards_train/chosen': '0.033676', 'rewards_train/rejected': '0.046689', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013013', 'logps_train/rejected': '-114.62', 'logps_train/chosen': '-128.14', 'loss/train': '0.7093', 'examples_per_second': '30.459', 'grad_norm': '21.75', 'counters/examples': 166240, 'counters/updates': 5195}
train stats after 166272 examples: {'rewards_train/chosen': '0.15175', 'rewards_train/rejected': '0.078527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073219', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-151.91', 'loss/train': '0.67471', 'examples_per_second': '30.543', 'grad_norm': '21.125', 'counters/examples': 166272, 'counters/updates': 5196}
train stats after 166304 examples: {'rewards_train/chosen': '0.075855', 'rewards_train/rejected': '0.069862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0059922', 'logps_train/rejected': '-116.9', 'logps_train/chosen': '-104.83', 'loss/train': '0.7041', 'examples_per_second': '31.626', 'grad_norm': '20.25', 'counters/examples': 166304, 'counters/updates': 5197}
train stats after 166336 examples: {'rewards_train/chosen': '0.15351', 'rewards_train/rejected': '0.090403', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063104', 'logps_train/rejected': '-129.59', 'logps_train/chosen': '-125.79', 'loss/train': '0.68478', 'examples_per_second': '31.441', 'grad_norm': '21.625', 'counters/examples': 166336, 'counters/updates': 5198}
train stats after 166368 examples: {'rewards_train/chosen': '0.12121', 'rewards_train/rejected': '0.049299', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.071906', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-119.52', 'loss/train': '0.66897', 'examples_per_second': '30.506', 'grad_norm': '19.5', 'counters/examples': 166368, 'counters/updates': 5199}
train stats after 166400 examples: {'rewards_train/chosen': '0.13354', 'rewards_train/rejected': '-0.020498', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15404', 'logps_train/rejected': '-119.99', 'logps_train/chosen': '-130.94', 'loss/train': '0.63348', 'examples_per_second': '31.48', 'grad_norm': '18.625', 'counters/examples': 166400, 'counters/updates': 5200}
train stats after 166432 examples: {'rewards_train/chosen': '0.21724', 'rewards_train/rejected': '0.035353', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18189', 'logps_train/rejected': '-146.91', 'logps_train/chosen': '-148.22', 'loss/train': '0.61819', 'examples_per_second': '31.6', 'grad_norm': '20.125', 'counters/examples': 166432, 'counters/updates': 5201}
train stats after 166464 examples: {'rewards_train/chosen': '0.0028197', 'rewards_train/rejected': '0.0036088', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00078915', 'logps_train/rejected': '-89.992', 'logps_train/chosen': '-113.59', 'loss/train': '0.70075', 'examples_per_second': '32.797', 'grad_norm': '18.875', 'counters/examples': 166464, 'counters/updates': 5202}
train stats after 166496 examples: {'rewards_train/chosen': '0.15409', 'rewards_train/rejected': '0.057537', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.096554', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-155.25', 'loss/train': '0.66313', 'examples_per_second': '31.624', 'grad_norm': '20.5', 'counters/examples': 166496, 'counters/updates': 5203}
train stats after 166528 examples: {'rewards_train/chosen': '0.16153', 'rewards_train/rejected': '0.10858', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052958', 'logps_train/rejected': '-96.712', 'logps_train/chosen': '-125.5', 'loss/train': '0.68624', 'examples_per_second': '31.605', 'grad_norm': '19.125', 'counters/examples': 166528, 'counters/updates': 5204}
train stats after 166560 examples: {'rewards_train/chosen': '0.11728', 'rewards_train/rejected': '0.0028196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11446', 'logps_train/rejected': '-115.88', 'logps_train/chosen': '-134.3', 'loss/train': '0.65087', 'examples_per_second': '31.6', 'grad_norm': '20', 'counters/examples': 166560, 'counters/updates': 5205}
train stats after 166592 examples: {'rewards_train/chosen': '0.16184', 'rewards_train/rejected': '0.052875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10896', 'logps_train/rejected': '-117.27', 'logps_train/chosen': '-134.74', 'loss/train': '0.66484', 'examples_per_second': '30.623', 'grad_norm': '21.375', 'counters/examples': 166592, 'counters/updates': 5206}
train stats after 166624 examples: {'rewards_train/chosen': '0.080876', 'rewards_train/rejected': '0.016145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064732', 'logps_train/rejected': '-88.405', 'logps_train/chosen': '-105.97', 'loss/train': '0.66898', 'examples_per_second': '31.2', 'grad_norm': '17.75', 'counters/examples': 166624, 'counters/updates': 5207}
train stats after 166656 examples: {'rewards_train/chosen': '0.15183', 'rewards_train/rejected': '0.080598', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071236', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-130.18', 'loss/train': '0.67155', 'examples_per_second': '31.652', 'grad_norm': '21.125', 'counters/examples': 166656, 'counters/updates': 5208}
train stats after 166688 examples: {'rewards_train/chosen': '0.1773', 'rewards_train/rejected': '0.051928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12537', 'logps_train/rejected': '-121.19', 'logps_train/chosen': '-156.17', 'loss/train': '0.65153', 'examples_per_second': '30.203', 'grad_norm': '21.25', 'counters/examples': 166688, 'counters/updates': 5209}
skipping logging after 166720 examples to avoid logging too frequently
train stats after 166752 examples: {'rewards_train/chosen': '0.19332', 'rewards_train/rejected': '0.050002', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14332', 'logps_train/rejected': '-137.49', 'logps_train/chosen': '-161.34', 'loss/train': '0.64115', 'examples_per_second': '31.547', 'grad_norm': '22.125', 'counters/examples': 166752, 'counters/updates': 5211}
skipping logging after 166784 examples to avoid logging too frequently
train stats after 166816 examples: {'rewards_train/chosen': '0.11005', 'rewards_train/rejected': '-0.019764', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12981', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-148.99', 'loss/train': '0.64836', 'examples_per_second': '30.445', 'grad_norm': '21.5', 'counters/examples': 166816, 'counters/updates': 5213}
train stats after 166848 examples: {'rewards_train/chosen': '0.044528', 'rewards_train/rejected': '0.025464', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019063', 'logps_train/rejected': '-101.31', 'logps_train/chosen': '-89.807', 'loss/train': '0.69213', 'examples_per_second': '31.541', 'grad_norm': '18.25', 'counters/examples': 166848, 'counters/updates': 5214}
train stats after 166880 examples: {'rewards_train/chosen': '0.19758', 'rewards_train/rejected': '0.058619', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13896', 'logps_train/rejected': '-140.02', 'logps_train/chosen': '-165.37', 'loss/train': '0.64068', 'examples_per_second': '31.469', 'grad_norm': '21.875', 'counters/examples': 166880, 'counters/updates': 5215}
skipping logging after 166912 examples to avoid logging too frequently
train stats after 166944 examples: {'rewards_train/chosen': '0.1356', 'rewards_train/rejected': '0.063241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072355', 'logps_train/rejected': '-105.72', 'logps_train/chosen': '-128.96', 'loss/train': '0.66751', 'examples_per_second': '31.083', 'grad_norm': '19.5', 'counters/examples': 166944, 'counters/updates': 5217}
train stats after 166976 examples: {'rewards_train/chosen': '0.2133', 'rewards_train/rejected': '0.0088893', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20442', 'logps_train/rejected': '-116.71', 'logps_train/chosen': '-172.43', 'loss/train': '0.61929', 'examples_per_second': '31.594', 'grad_norm': '20.875', 'counters/examples': 166976, 'counters/updates': 5218}
train stats after 167008 examples: {'rewards_train/chosen': '0.20619', 'rewards_train/rejected': '0.01479', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1914', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-133.8', 'loss/train': '0.61718', 'examples_per_second': '31.255', 'grad_norm': '19.375', 'counters/examples': 167008, 'counters/updates': 5219}
skipping logging after 167040 examples to avoid logging too frequently
train stats after 167072 examples: {'rewards_train/chosen': '0.14968', 'rewards_train/rejected': '0.089444', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060238', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-120.9', 'loss/train': '0.67201', 'examples_per_second': '30.504', 'grad_norm': '19.875', 'counters/examples': 167072, 'counters/updates': 5221}
skipping logging after 167104 examples to avoid logging too frequently
train stats after 167136 examples: {'rewards_train/chosen': '0.11669', 'rewards_train/rejected': '0.10397', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012718', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-134.57', 'loss/train': '0.69726', 'examples_per_second': '30.255', 'grad_norm': '22.125', 'counters/examples': 167136, 'counters/updates': 5223}
skipping logging after 167168 examples to avoid logging too frequently
train stats after 167200 examples: {'rewards_train/chosen': '0.14372', 'rewards_train/rejected': '0.1003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043423', 'logps_train/rejected': '-152.71', 'logps_train/chosen': '-119.2', 'loss/train': '0.6876', 'examples_per_second': '37.24', 'grad_norm': '22.5', 'counters/examples': 167200, 'counters/updates': 5225}
skipping logging after 167232 examples to avoid logging too frequently
train stats after 167264 examples: {'rewards_train/chosen': '0.17105', 'rewards_train/rejected': '0.12603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045011', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-144.91', 'loss/train': '0.67865', 'examples_per_second': '31.234', 'grad_norm': '21.625', 'counters/examples': 167264, 'counters/updates': 5227}
train stats after 167296 examples: {'rewards_train/chosen': '0.17424', 'rewards_train/rejected': '0.10721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067023', 'logps_train/rejected': '-140.37', 'logps_train/chosen': '-162.2', 'loss/train': '0.6693', 'examples_per_second': '31.399', 'grad_norm': '22.875', 'counters/examples': 167296, 'counters/updates': 5228}
train stats after 167328 examples: {'rewards_train/chosen': '0.1641', 'rewards_train/rejected': '0.033603', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1305', 'logps_train/rejected': '-105.24', 'logps_train/chosen': '-136.54', 'loss/train': '0.64071', 'examples_per_second': '31.611', 'grad_norm': '19', 'counters/examples': 167328, 'counters/updates': 5229}
train stats after 167360 examples: {'rewards_train/chosen': '0.23418', 'rewards_train/rejected': '0.13301', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10117', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-165.87', 'loss/train': '0.65852', 'examples_per_second': '31.36', 'grad_norm': '21.25', 'counters/examples': 167360, 'counters/updates': 5230}
skipping logging after 167392 examples to avoid logging too frequently
train stats after 167424 examples: {'rewards_train/chosen': '0.20723', 'rewards_train/rejected': '0.14266', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064568', 'logps_train/rejected': '-138.04', 'logps_train/chosen': '-143.52', 'loss/train': '0.69205', 'examples_per_second': '35.284', 'grad_norm': '21.75', 'counters/examples': 167424, 'counters/updates': 5232}
train stats after 167456 examples: {'rewards_train/chosen': '0.1478', 'rewards_train/rejected': '-0.10209', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.24989', 'logps_train/rejected': '-142.6', 'logps_train/chosen': '-156.81', 'loss/train': '0.59206', 'examples_per_second': '30.824', 'grad_norm': '19.625', 'counters/examples': 167456, 'counters/updates': 5233}
skipping logging after 167488 examples to avoid logging too frequently
train stats after 167520 examples: {'rewards_train/chosen': '0.089332', 'rewards_train/rejected': '0.03704', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052292', 'logps_train/rejected': '-106.94', 'logps_train/chosen': '-137.98', 'loss/train': '0.68104', 'examples_per_second': '31.577', 'grad_norm': '21.125', 'counters/examples': 167520, 'counters/updates': 5235}
skipping logging after 167552 examples to avoid logging too frequently
train stats after 167584 examples: {'rewards_train/chosen': '0.1784', 'rewards_train/rejected': '0.055891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12251', 'logps_train/rejected': '-135.7', 'logps_train/chosen': '-154.47', 'loss/train': '0.6457', 'examples_per_second': '32.605', 'grad_norm': '20.25', 'counters/examples': 167584, 'counters/updates': 5237}
train stats after 167616 examples: {'rewards_train/chosen': '0.2247', 'rewards_train/rejected': '0.040207', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18449', 'logps_train/rejected': '-98.473', 'logps_train/chosen': '-143.17', 'loss/train': '0.61583', 'examples_per_second': '31.938', 'grad_norm': '19.375', 'counters/examples': 167616, 'counters/updates': 5238}
skipping logging after 167648 examples to avoid logging too frequently
train stats after 167680 examples: {'rewards_train/chosen': '0.17976', 'rewards_train/rejected': '0.082568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097189', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-135.44', 'loss/train': '0.6615', 'examples_per_second': '37.772', 'grad_norm': '21.625', 'counters/examples': 167680, 'counters/updates': 5240}
train stats after 167712 examples: {'rewards_train/chosen': '0.15311', 'rewards_train/rejected': '0.074106', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079004', 'logps_train/rejected': '-93.765', 'logps_train/chosen': '-103.48', 'loss/train': '0.67414', 'examples_per_second': '31.071', 'grad_norm': '18.25', 'counters/examples': 167712, 'counters/updates': 5241}
train stats after 167744 examples: {'rewards_train/chosen': '0.23721', 'rewards_train/rejected': '0.074213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.163', 'logps_train/rejected': '-110.38', 'logps_train/chosen': '-139.56', 'loss/train': '0.62788', 'examples_per_second': '30.485', 'grad_norm': '18.375', 'counters/examples': 167744, 'counters/updates': 5242}
train stats after 167776 examples: {'rewards_train/chosen': '0.16653', 'rewards_train/rejected': '0.093053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073474', 'logps_train/rejected': '-87.336', 'logps_train/chosen': '-81.105', 'loss/train': '0.66376', 'examples_per_second': '29.999', 'grad_norm': '15.938', 'counters/examples': 167776, 'counters/updates': 5243}
train stats after 167808 examples: {'rewards_train/chosen': '0.14306', 'rewards_train/rejected': '0.12347', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.019586', 'logps_train/rejected': '-142.59', 'logps_train/chosen': '-146.13', 'loss/train': '0.7004', 'examples_per_second': '30.658', 'grad_norm': '22.875', 'counters/examples': 167808, 'counters/updates': 5244}
skipping logging after 167840 examples to avoid logging too frequently
train stats after 167872 examples: {'rewards_train/chosen': '0.10798', 'rewards_train/rejected': '-0.0056876', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11366', 'logps_train/rejected': '-109.87', 'logps_train/chosen': '-133.62', 'loss/train': '0.65164', 'examples_per_second': '31.56', 'grad_norm': '19.5', 'counters/examples': 167872, 'counters/updates': 5246}
train stats after 167904 examples: {'rewards_train/chosen': '0.16373', 'rewards_train/rejected': '-0.0099978', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17373', 'logps_train/rejected': '-98.954', 'logps_train/chosen': '-127.69', 'loss/train': '0.61978', 'examples_per_second': '31.519', 'grad_norm': '18.25', 'counters/examples': 167904, 'counters/updates': 5247}
train stats after 167936 examples: {'rewards_train/chosen': '0.16027', 'rewards_train/rejected': '0.092025', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068243', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-127.73', 'loss/train': '0.67056', 'examples_per_second': '32.696', 'grad_norm': '19.375', 'counters/examples': 167936, 'counters/updates': 5248}
train stats after 167968 examples: {'rewards_train/chosen': '0.16749', 'rewards_train/rejected': '0.093279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074207', 'logps_train/rejected': '-113.21', 'logps_train/chosen': '-129.03', 'loss/train': '0.66643', 'examples_per_second': '32.15', 'grad_norm': '20', 'counters/examples': 167968, 'counters/updates': 5249}
train stats after 168000 examples: {'rewards_train/chosen': '0.12911', 'rewards_train/rejected': '0.029508', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099604', 'logps_train/rejected': '-107.88', 'logps_train/chosen': '-90.528', 'loss/train': '0.65377', 'examples_per_second': '31.615', 'grad_norm': '17.625', 'counters/examples': 168000, 'counters/updates': 5250}
Running evaluation after 168000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.81it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 168000: {'rewards_eval/chosen': '0.16631', 'rewards_eval/rejected': '0.056193', 'rewards_eval/accuracies': '0.56641', 'rewards_eval/margins': '0.11012', 'logps_eval/rejected': '-114.83', 'logps_eval/chosen': '-133.88', 'loss/eval': '0.65534'}
skipping save for non epoch
train stats after 168032 examples: {'rewards_train/chosen': '0.080271', 'rewards_train/rejected': '0.047514', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.032757', 'logps_train/rejected': '-95.512', 'logps_train/chosen': '-115.21', 'loss/train': '0.68862', 'examples_per_second': '32.268', 'grad_norm': '18.875', 'counters/examples': 168032, 'counters/updates': 5251}
train stats after 168064 examples: {'rewards_train/chosen': '0.12778', 'rewards_train/rejected': '0.053718', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07406', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-146.51', 'loss/train': '0.67597', 'examples_per_second': '32.104', 'grad_norm': '20.625', 'counters/examples': 168064, 'counters/updates': 5252}
train stats after 168096 examples: {'rewards_train/chosen': '0.1208', 'rewards_train/rejected': '0.056665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06414', 'logps_train/rejected': '-106.4', 'logps_train/chosen': '-113.12', 'loss/train': '0.67791', 'examples_per_second': '31.487', 'grad_norm': '18.625', 'counters/examples': 168096, 'counters/updates': 5253}
train stats after 168128 examples: {'rewards_train/chosen': '0.24466', 'rewards_train/rejected': '0.09611', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14855', 'logps_train/rejected': '-97.236', 'logps_train/chosen': '-130.09', 'loss/train': '0.62869', 'examples_per_second': '31.023', 'grad_norm': '18.125', 'counters/examples': 168128, 'counters/updates': 5254}
train stats after 168160 examples: {'rewards_train/chosen': '0.2265', 'rewards_train/rejected': '0.071238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15526', 'logps_train/rejected': '-115.97', 'logps_train/chosen': '-146', 'loss/train': '0.63306', 'examples_per_second': '31.544', 'grad_norm': '19.25', 'counters/examples': 168160, 'counters/updates': 5255}
train stats after 168192 examples: {'rewards_train/chosen': '0.18041', 'rewards_train/rejected': '0.070389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11002', 'logps_train/rejected': '-97.484', 'logps_train/chosen': '-139.87', 'loss/train': '0.65107', 'examples_per_second': '31.74', 'grad_norm': '19.875', 'counters/examples': 168192, 'counters/updates': 5256}
train stats after 168224 examples: {'rewards_train/chosen': '0.16423', 'rewards_train/rejected': '-0.032167', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1964', 'logps_train/rejected': '-144.81', 'logps_train/chosen': '-159.28', 'loss/train': '0.61318', 'examples_per_second': '31.54', 'grad_norm': '20.375', 'counters/examples': 168224, 'counters/updates': 5257}
train stats after 168256 examples: {'rewards_train/chosen': '0.24824', 'rewards_train/rejected': '0.030574', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21767', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-146.53', 'loss/train': '0.61452', 'examples_per_second': '31.497', 'grad_norm': '20.5', 'counters/examples': 168256, 'counters/updates': 5258}
train stats after 168288 examples: {'rewards_train/chosen': '0.09456', 'rewards_train/rejected': '0.0088492', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085711', 'logps_train/rejected': '-98.652', 'logps_train/chosen': '-134.69', 'loss/train': '0.65919', 'examples_per_second': '31.562', 'grad_norm': '20.625', 'counters/examples': 168288, 'counters/updates': 5259}
train stats after 168320 examples: {'rewards_train/chosen': '0.069263', 'rewards_train/rejected': '0.085499', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016235', 'logps_train/rejected': '-118.36', 'logps_train/chosen': '-105.5', 'loss/train': '0.70974', 'examples_per_second': '32.552', 'grad_norm': '18.875', 'counters/examples': 168320, 'counters/updates': 5260}
train stats after 168352 examples: {'rewards_train/chosen': '0.097607', 'rewards_train/rejected': '0.05371', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043897', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-106.59', 'loss/train': '0.67864', 'examples_per_second': '24.575', 'grad_norm': '19.125', 'counters/examples': 168352, 'counters/updates': 5261}
train stats after 168384 examples: {'rewards_train/chosen': '0.2142', 'rewards_train/rejected': '0.0069116', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20729', 'logps_train/rejected': '-115.81', 'logps_train/chosen': '-129.43', 'loss/train': '0.60887', 'examples_per_second': '32.826', 'grad_norm': '18.25', 'counters/examples': 168384, 'counters/updates': 5262}
train stats after 168416 examples: {'rewards_train/chosen': '0.13376', 'rewards_train/rejected': '0.0032617', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1305', 'logps_train/rejected': '-106.69', 'logps_train/chosen': '-152.2', 'loss/train': '0.64207', 'examples_per_second': '30.19', 'grad_norm': '19.25', 'counters/examples': 168416, 'counters/updates': 5263}
train stats after 168448 examples: {'rewards_train/chosen': '0.27036', 'rewards_train/rejected': '0.16048', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10988', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-146.72', 'loss/train': '0.65584', 'examples_per_second': '24.476', 'grad_norm': '21.5', 'counters/examples': 168448, 'counters/updates': 5264}
train stats after 168480 examples: {'rewards_train/chosen': '0.21443', 'rewards_train/rejected': '0.034478', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17995', 'logps_train/rejected': '-78.77', 'logps_train/chosen': '-103.42', 'loss/train': '0.61618', 'examples_per_second': '33.078', 'grad_norm': '15.75', 'counters/examples': 168480, 'counters/updates': 5265}
train stats after 168512 examples: {'rewards_train/chosen': '0.12874', 'rewards_train/rejected': '0.010099', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11864', 'logps_train/rejected': '-113.12', 'logps_train/chosen': '-153.48', 'loss/train': '0.6535', 'examples_per_second': '30.098', 'grad_norm': '20.75', 'counters/examples': 168512, 'counters/updates': 5266}
train stats after 168544 examples: {'rewards_train/chosen': '0.20235', 'rewards_train/rejected': '0.026594', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17576', 'logps_train/rejected': '-158.14', 'logps_train/chosen': '-122.33', 'loss/train': '0.62408', 'examples_per_second': '31.462', 'grad_norm': '20.375', 'counters/examples': 168544, 'counters/updates': 5267}
train stats after 168576 examples: {'rewards_train/chosen': '0.28781', 'rewards_train/rejected': '0.039242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24857', 'logps_train/rejected': '-104.12', 'logps_train/chosen': '-139.38', 'loss/train': '0.59795', 'examples_per_second': '31.343', 'grad_norm': '17.5', 'counters/examples': 168576, 'counters/updates': 5268}
train stats after 168608 examples: {'rewards_train/chosen': '0.099869', 'rewards_train/rejected': '0.059764', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040105', 'logps_train/rejected': '-97.503', 'logps_train/chosen': '-123.51', 'loss/train': '0.68402', 'examples_per_second': '30.839', 'grad_norm': '20.125', 'counters/examples': 168608, 'counters/updates': 5269}
train stats after 168640 examples: {'rewards_train/chosen': '0.090097', 'rewards_train/rejected': '0.016595', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073503', 'logps_train/rejected': '-95.521', 'logps_train/chosen': '-107.04', 'loss/train': '0.66792', 'examples_per_second': '30.7', 'grad_norm': '18.625', 'counters/examples': 168640, 'counters/updates': 5270}
train stats after 168672 examples: {'rewards_train/chosen': '0.087318', 'rewards_train/rejected': '0.097055', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0097371', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-148.82', 'loss/train': '0.7108', 'examples_per_second': '31.59', 'grad_norm': '22.25', 'counters/examples': 168672, 'counters/updates': 5271}
train stats after 168704 examples: {'rewards_train/chosen': '0.13206', 'rewards_train/rejected': '0.050905', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081155', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-130.64', 'loss/train': '0.66907', 'examples_per_second': '31.592', 'grad_norm': '20.125', 'counters/examples': 168704, 'counters/updates': 5272}
train stats after 168736 examples: {'rewards_train/chosen': '0.25876', 'rewards_train/rejected': '0.10764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15112', 'logps_train/rejected': '-158.29', 'logps_train/chosen': '-191.12', 'loss/train': '0.64103', 'examples_per_second': '31.169', 'grad_norm': '24.125', 'counters/examples': 168736, 'counters/updates': 5273}
train stats after 168768 examples: {'rewards_train/chosen': '0.12485', 'rewards_train/rejected': '0.02884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096006', 'logps_train/rejected': '-104.31', 'logps_train/chosen': '-126.92', 'loss/train': '0.65904', 'examples_per_second': '31.687', 'grad_norm': '19.375', 'counters/examples': 168768, 'counters/updates': 5274}
skipping logging after 168800 examples to avoid logging too frequently
train stats after 168832 examples: {'rewards_train/chosen': '0.23731', 'rewards_train/rejected': '0.14197', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095337', 'logps_train/rejected': '-175.42', 'logps_train/chosen': '-147.31', 'loss/train': '0.66401', 'examples_per_second': '30.561', 'grad_norm': '22.875', 'counters/examples': 168832, 'counters/updates': 5276}
train stats after 168864 examples: {'rewards_train/chosen': '0.11397', 'rewards_train/rejected': '0.099688', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014286', 'logps_train/rejected': '-142.64', 'logps_train/chosen': '-119.64', 'loss/train': '0.71008', 'examples_per_second': '32.197', 'grad_norm': '22.625', 'counters/examples': 168864, 'counters/updates': 5277}
train stats after 168896 examples: {'rewards_train/chosen': '0.16227', 'rewards_train/rejected': '0.12417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038098', 'logps_train/rejected': '-102.54', 'logps_train/chosen': '-124.34', 'loss/train': '0.68307', 'examples_per_second': '31.247', 'grad_norm': '19.625', 'counters/examples': 168896, 'counters/updates': 5278}
train stats after 168928 examples: {'rewards_train/chosen': '0.10065', 'rewards_train/rejected': '0.0015764', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.099078', 'logps_train/rejected': '-74.06', 'logps_train/chosen': '-93.429', 'loss/train': '0.65054', 'examples_per_second': '31.675', 'grad_norm': '15.938', 'counters/examples': 168928, 'counters/updates': 5279}
train stats after 168960 examples: {'rewards_train/chosen': '0.19033', 'rewards_train/rejected': '0.036864', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15347', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-147.52', 'loss/train': '0.63027', 'examples_per_second': '31.548', 'grad_norm': '20.875', 'counters/examples': 168960, 'counters/updates': 5280}
train stats after 168992 examples: {'rewards_train/chosen': '0.14524', 'rewards_train/rejected': '0.15085', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0056103', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-118.36', 'loss/train': '0.7094', 'examples_per_second': '31.91', 'grad_norm': '21.625', 'counters/examples': 168992, 'counters/updates': 5281}
train stats after 169024 examples: {'rewards_train/chosen': '0.22166', 'rewards_train/rejected': '0.081234', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14043', 'logps_train/rejected': '-104.54', 'logps_train/chosen': '-141.57', 'loss/train': '0.6363', 'examples_per_second': '31.872', 'grad_norm': '19.125', 'counters/examples': 169024, 'counters/updates': 5282}
train stats after 169056 examples: {'rewards_train/chosen': '0.14985', 'rewards_train/rejected': '0.020212', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12963', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-178.04', 'loss/train': '0.64158', 'examples_per_second': '32.29', 'grad_norm': '20.75', 'counters/examples': 169056, 'counters/updates': 5283}
skipping logging after 169088 examples to avoid logging too frequently
train stats after 169120 examples: {'rewards_train/chosen': '0.2123', 'rewards_train/rejected': '0.011739', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20056', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-134.97', 'loss/train': '0.61435', 'examples_per_second': '30.685', 'grad_norm': '19.625', 'counters/examples': 169120, 'counters/updates': 5285}
train stats after 169152 examples: {'rewards_train/chosen': '0.16362', 'rewards_train/rejected': '-0.021315', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18494', 'logps_train/rejected': '-125.99', 'logps_train/chosen': '-135.2', 'loss/train': '0.61499', 'examples_per_second': '32.189', 'grad_norm': '18.375', 'counters/examples': 169152, 'counters/updates': 5286}
train stats after 169184 examples: {'rewards_train/chosen': '0.1566', 'rewards_train/rejected': '0.039906', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11669', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-157.7', 'loss/train': '0.64721', 'examples_per_second': '31.655', 'grad_norm': '19.625', 'counters/examples': 169184, 'counters/updates': 5287}
train stats after 169216 examples: {'rewards_train/chosen': '0.11198', 'rewards_train/rejected': '0.04269', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069293', 'logps_train/rejected': '-114.01', 'logps_train/chosen': '-116.12', 'loss/train': '0.67184', 'examples_per_second': '31.527', 'grad_norm': '20', 'counters/examples': 169216, 'counters/updates': 5288}
train stats after 169248 examples: {'rewards_train/chosen': '0.24324', 'rewards_train/rejected': '0.063638', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1796', 'logps_train/rejected': '-170.71', 'logps_train/chosen': '-150.27', 'loss/train': '0.6254', 'examples_per_second': '31.644', 'grad_norm': '21.625', 'counters/examples': 169248, 'counters/updates': 5289}
skipping logging after 169280 examples to avoid logging too frequently
train stats after 169312 examples: {'rewards_train/chosen': '0.12268', 'rewards_train/rejected': '0.084242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038438', 'logps_train/rejected': '-100.19', 'logps_train/chosen': '-134.88', 'loss/train': '0.68884', 'examples_per_second': '31.166', 'grad_norm': '19.625', 'counters/examples': 169312, 'counters/updates': 5291}
train stats after 169344 examples: {'rewards_train/chosen': '0.22502', 'rewards_train/rejected': '-0.0021472', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22717', 'logps_train/rejected': '-146.68', 'logps_train/chosen': '-168.82', 'loss/train': '0.60777', 'examples_per_second': '32.337', 'grad_norm': '21.875', 'counters/examples': 169344, 'counters/updates': 5292}
train stats after 169376 examples: {'rewards_train/chosen': '0.11634', 'rewards_train/rejected': '0.036164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080181', 'logps_train/rejected': '-113.9', 'logps_train/chosen': '-121.54', 'loss/train': '0.66537', 'examples_per_second': '31.046', 'grad_norm': '19.125', 'counters/examples': 169376, 'counters/updates': 5293}
train stats after 169408 examples: {'rewards_train/chosen': '0.11291', 'rewards_train/rejected': '0.148', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.03509', 'logps_train/rejected': '-112.06', 'logps_train/chosen': '-112.6', 'loss/train': '0.73483', 'examples_per_second': '30.598', 'grad_norm': '19.5', 'counters/examples': 169408, 'counters/updates': 5294}
train stats after 169440 examples: {'rewards_train/chosen': '0.19334', 'rewards_train/rejected': '-0.018737', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21207', 'logps_train/rejected': '-141.18', 'logps_train/chosen': '-142.83', 'loss/train': '0.60186', 'examples_per_second': '31.472', 'grad_norm': '20', 'counters/examples': 169440, 'counters/updates': 5295}
skipping logging after 169472 examples to avoid logging too frequently
train stats after 169504 examples: {'rewards_train/chosen': '0.11946', 'rewards_train/rejected': '0.1649', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.045438', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-138.15', 'loss/train': '0.73661', 'examples_per_second': '30.758', 'grad_norm': '22.25', 'counters/examples': 169504, 'counters/updates': 5297}
train stats after 169536 examples: {'rewards_train/chosen': '0.20034', 'rewards_train/rejected': '0.03453', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16581', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-107.2', 'loss/train': '0.62803', 'examples_per_second': '31.476', 'grad_norm': '19', 'counters/examples': 169536, 'counters/updates': 5298}
train stats after 169568 examples: {'rewards_train/chosen': '0.17035', 'rewards_train/rejected': '0.084444', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085905', 'logps_train/rejected': '-131.02', 'logps_train/chosen': '-147.89', 'loss/train': '0.66249', 'examples_per_second': '31.303', 'grad_norm': '19.75', 'counters/examples': 169568, 'counters/updates': 5299}
train stats after 169600 examples: {'rewards_train/chosen': '0.059718', 'rewards_train/rejected': '0.063914', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0041965', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-111.14', 'loss/train': '0.70257', 'examples_per_second': '32.878', 'grad_norm': '21.625', 'counters/examples': 169600, 'counters/updates': 5300}
skipping logging after 169632 examples to avoid logging too frequently
train stats after 169664 examples: {'rewards_train/chosen': '0.15', 'rewards_train/rejected': '0.091596', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058402', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-139.85', 'loss/train': '0.67619', 'examples_per_second': '36.866', 'grad_norm': '19.75', 'counters/examples': 169664, 'counters/updates': 5302}
train stats after 169696 examples: {'rewards_train/chosen': '0.17891', 'rewards_train/rejected': '0.082902', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09601', 'logps_train/rejected': '-132.43', 'logps_train/chosen': '-135.95', 'loss/train': '0.65843', 'examples_per_second': '32.555', 'grad_norm': '20.25', 'counters/examples': 169696, 'counters/updates': 5303}
train stats after 169728 examples: {'rewards_train/chosen': '0.22947', 'rewards_train/rejected': '0.10903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12045', 'logps_train/rejected': '-104.55', 'logps_train/chosen': '-143.77', 'loss/train': '0.66081', 'examples_per_second': '31.542', 'grad_norm': '19.875', 'counters/examples': 169728, 'counters/updates': 5304}
train stats after 169760 examples: {'rewards_train/chosen': '0.13487', 'rewards_train/rejected': '0.076745', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058121', 'logps_train/rejected': '-101.38', 'logps_train/chosen': '-118.86', 'loss/train': '0.67584', 'examples_per_second': '31.073', 'grad_norm': '19.75', 'counters/examples': 169760, 'counters/updates': 5305}
train stats after 169792 examples: {'rewards_train/chosen': '0.087316', 'rewards_train/rejected': '0.063121', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024195', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-133.31', 'loss/train': '0.69012', 'examples_per_second': '32.858', 'grad_norm': '19.25', 'counters/examples': 169792, 'counters/updates': 5306}
skipping logging after 169824 examples to avoid logging too frequently
train stats after 169856 examples: {'rewards_train/chosen': '0.19429', 'rewards_train/rejected': '0.069861', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12443', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-134.75', 'loss/train': '0.64173', 'examples_per_second': '31.52', 'grad_norm': '19.875', 'counters/examples': 169856, 'counters/updates': 5308}
skipping logging after 169888 examples to avoid logging too frequently
train stats after 169920 examples: {'rewards_train/chosen': '0.15769', 'rewards_train/rejected': '-0.027853', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18554', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-138.37', 'loss/train': '0.61368', 'examples_per_second': '30.758', 'grad_norm': '19.625', 'counters/examples': 169920, 'counters/updates': 5310}
skipping logging after 169952 examples to avoid logging too frequently
train stats after 169984 examples: {'rewards_train/chosen': '0.26117', 'rewards_train/rejected': '-0.0017782', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26295', 'logps_train/rejected': '-114.74', 'logps_train/chosen': '-129.28', 'loss/train': '0.58552', 'examples_per_second': '32.894', 'grad_norm': '18.125', 'counters/examples': 169984, 'counters/updates': 5312}
train stats after 170016 examples: {'rewards_train/chosen': '0.12789', 'rewards_train/rejected': '0.024601', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10328', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-149.89', 'loss/train': '0.65427', 'examples_per_second': '30.555', 'grad_norm': '21.375', 'counters/examples': 170016, 'counters/updates': 5313}
skipping logging after 170048 examples to avoid logging too frequently
train stats after 170080 examples: {'rewards_train/chosen': '0.086339', 'rewards_train/rejected': '0.059953', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026386', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-78.147', 'loss/train': '0.68903', 'examples_per_second': '34.971', 'grad_norm': '20.375', 'counters/examples': 170080, 'counters/updates': 5315}
train stats after 170112 examples: {'rewards_train/chosen': '0.10134', 'rewards_train/rejected': '0.004695', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.096643', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-134.19', 'loss/train': '0.6592', 'examples_per_second': '29.952', 'grad_norm': '21.75', 'counters/examples': 170112, 'counters/updates': 5316}
train stats after 170144 examples: {'rewards_train/chosen': '0.13911', 'rewards_train/rejected': '0.017997', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12111', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-141.44', 'loss/train': '0.6518', 'examples_per_second': '30.705', 'grad_norm': '19.625', 'counters/examples': 170144, 'counters/updates': 5317}
train stats after 170176 examples: {'rewards_train/chosen': '0.22466', 'rewards_train/rejected': '0.090121', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13454', 'logps_train/rejected': '-99.721', 'logps_train/chosen': '-153.14', 'loss/train': '0.64483', 'examples_per_second': '31.561', 'grad_norm': '19.5', 'counters/examples': 170176, 'counters/updates': 5318}
train stats after 170208 examples: {'rewards_train/chosen': '0.20801', 'rewards_train/rejected': '0.058926', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14909', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-127.37', 'loss/train': '0.63554', 'examples_per_second': '31.764', 'grad_norm': '18.5', 'counters/examples': 170208, 'counters/updates': 5319}
skipping logging after 170240 examples to avoid logging too frequently
train stats after 170272 examples: {'rewards_train/chosen': '0.11791', 'rewards_train/rejected': '-0.061625', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17953', 'logps_train/rejected': '-92.447', 'logps_train/chosen': '-134.77', 'loss/train': '0.61647', 'examples_per_second': '34.007', 'grad_norm': '18.125', 'counters/examples': 170272, 'counters/updates': 5321}
train stats after 170304 examples: {'rewards_train/chosen': '0.12105', 'rewards_train/rejected': '0.085649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035403', 'logps_train/rejected': '-108.85', 'logps_train/chosen': '-145.01', 'loss/train': '0.69307', 'examples_per_second': '31.626', 'grad_norm': '20.75', 'counters/examples': 170304, 'counters/updates': 5322}
train stats after 170336 examples: {'rewards_train/chosen': '0.18037', 'rewards_train/rejected': '0.12189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05848', 'logps_train/rejected': '-152.9', 'logps_train/chosen': '-140.55', 'loss/train': '0.67354', 'examples_per_second': '33.396', 'grad_norm': '21.125', 'counters/examples': 170336, 'counters/updates': 5323}
train stats after 170368 examples: {'rewards_train/chosen': '0.21141', 'rewards_train/rejected': '0.062739', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14867', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-128.75', 'loss/train': '0.64321', 'examples_per_second': '31.562', 'grad_norm': '19.375', 'counters/examples': 170368, 'counters/updates': 5324}
train stats after 170400 examples: {'rewards_train/chosen': '0.08655', 'rewards_train/rejected': '-0.062017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14857', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-134.95', 'loss/train': '0.6422', 'examples_per_second': '31.594', 'grad_norm': '19.875', 'counters/examples': 170400, 'counters/updates': 5325}
train stats after 170432 examples: {'rewards_train/chosen': '0.17405', 'rewards_train/rejected': '0.11744', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056616', 'logps_train/rejected': '-125.92', 'logps_train/chosen': '-128.46', 'loss/train': '0.67179', 'examples_per_second': '31.577', 'grad_norm': '22.125', 'counters/examples': 170432, 'counters/updates': 5326}
train stats after 170464 examples: {'rewards_train/chosen': '0.2081', 'rewards_train/rejected': '0.00064308', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20746', 'logps_train/rejected': '-113.54', 'logps_train/chosen': '-144.69', 'loss/train': '0.61035', 'examples_per_second': '30.529', 'grad_norm': '19.25', 'counters/examples': 170464, 'counters/updates': 5327}
train stats after 170496 examples: {'rewards_train/chosen': '0.10918', 'rewards_train/rejected': '0.089117', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020058', 'logps_train/rejected': '-121.56', 'logps_train/chosen': '-116.53', 'loss/train': '0.69526', 'examples_per_second': '31.555', 'grad_norm': '20.875', 'counters/examples': 170496, 'counters/updates': 5328}
train stats after 170528 examples: {'rewards_train/chosen': '0.11799', 'rewards_train/rejected': '-0.012544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13054', 'logps_train/rejected': '-98.407', 'logps_train/chosen': '-137.23', 'loss/train': '0.64285', 'examples_per_second': '31.487', 'grad_norm': '19.375', 'counters/examples': 170528, 'counters/updates': 5329}
train stats after 170560 examples: {'rewards_train/chosen': '0.15746', 'rewards_train/rejected': '0.0054676', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15199', 'logps_train/rejected': '-90.082', 'logps_train/chosen': '-141.5', 'loss/train': '0.63246', 'examples_per_second': '31.695', 'grad_norm': '18.25', 'counters/examples': 170560, 'counters/updates': 5330}
train stats after 170592 examples: {'rewards_train/chosen': '0.10698', 'rewards_train/rejected': '0.037488', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069488', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-133.41', 'loss/train': '0.66835', 'examples_per_second': '31.516', 'grad_norm': '20.125', 'counters/examples': 170592, 'counters/updates': 5331}
train stats after 170624 examples: {'rewards_train/chosen': '0.074324', 'rewards_train/rejected': '-0.022064', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096388', 'logps_train/rejected': '-106.61', 'logps_train/chosen': '-115.18', 'loss/train': '0.66335', 'examples_per_second': '31.245', 'grad_norm': '19', 'counters/examples': 170624, 'counters/updates': 5332}
train stats after 170656 examples: {'rewards_train/chosen': '0.20809', 'rewards_train/rejected': '0.1039', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10419', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-124.9', 'loss/train': '0.65858', 'examples_per_second': '32.028', 'grad_norm': '20.5', 'counters/examples': 170656, 'counters/updates': 5333}
train stats after 170688 examples: {'rewards_train/chosen': '0.068855', 'rewards_train/rejected': '0.018524', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050331', 'logps_train/rejected': '-95.015', 'logps_train/chosen': '-125.09', 'loss/train': '0.678', 'examples_per_second': '32.39', 'grad_norm': '18.75', 'counters/examples': 170688, 'counters/updates': 5334}
train stats after 170720 examples: {'rewards_train/chosen': '0.081493', 'rewards_train/rejected': '-0.00093273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082426', 'logps_train/rejected': '-142.23', 'logps_train/chosen': '-139.24', 'loss/train': '0.68033', 'examples_per_second': '31.536', 'grad_norm': '22.25', 'counters/examples': 170720, 'counters/updates': 5335}
train stats after 170752 examples: {'rewards_train/chosen': '0.13216', 'rewards_train/rejected': '0.12822', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0039422', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-134.57', 'loss/train': '0.69949', 'examples_per_second': '31.482', 'grad_norm': '20.875', 'counters/examples': 170752, 'counters/updates': 5336}
train stats after 170784 examples: {'rewards_train/chosen': '0.16491', 'rewards_train/rejected': '0.080106', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0848', 'logps_train/rejected': '-153.81', 'logps_train/chosen': '-135.18', 'loss/train': '0.66019', 'examples_per_second': '31.24', 'grad_norm': '20.875', 'counters/examples': 170784, 'counters/updates': 5337}
train stats after 170816 examples: {'rewards_train/chosen': '0.096067', 'rewards_train/rejected': '-0.020854', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11692', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-112.09', 'loss/train': '0.64652', 'examples_per_second': '31.324', 'grad_norm': '19.25', 'counters/examples': 170816, 'counters/updates': 5338}
skipping logging after 170848 examples to avoid logging too frequently
train stats after 170880 examples: {'rewards_train/chosen': '0.21845', 'rewards_train/rejected': '-0.013263', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23172', 'logps_train/rejected': '-94.134', 'logps_train/chosen': '-115.18', 'loss/train': '0.59463', 'examples_per_second': '43.419', 'grad_norm': '16.5', 'counters/examples': 170880, 'counters/updates': 5340}
train stats after 170912 examples: {'rewards_train/chosen': '0.16487', 'rewards_train/rejected': '0.010761', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1541', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-127.48', 'loss/train': '0.63991', 'examples_per_second': '32.401', 'grad_norm': '20.5', 'counters/examples': 170912, 'counters/updates': 5341}
train stats after 170944 examples: {'rewards_train/chosen': '0.16245', 'rewards_train/rejected': '0.12323', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039218', 'logps_train/rejected': '-101.7', 'logps_train/chosen': '-109.97', 'loss/train': '0.68651', 'examples_per_second': '32.296', 'grad_norm': '17.875', 'counters/examples': 170944, 'counters/updates': 5342}
train stats after 170976 examples: {'rewards_train/chosen': '0.17829', 'rewards_train/rejected': '0.00037117', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17791', 'logps_train/rejected': '-98.249', 'logps_train/chosen': '-118.71', 'loss/train': '0.6247', 'examples_per_second': '31.239', 'grad_norm': '17.875', 'counters/examples': 170976, 'counters/updates': 5343}
train stats after 171008 examples: {'rewards_train/chosen': '0.15437', 'rewards_train/rejected': '0.036865', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1175', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-120.71', 'loss/train': '0.64918', 'examples_per_second': '31.713', 'grad_norm': '20.25', 'counters/examples': 171008, 'counters/updates': 5344}
train stats after 171040 examples: {'rewards_train/chosen': '0.24745', 'rewards_train/rejected': '0.075457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.172', 'logps_train/rejected': '-184.41', 'logps_train/chosen': '-170.42', 'loss/train': '0.62865', 'examples_per_second': '31.535', 'grad_norm': '24.25', 'counters/examples': 171040, 'counters/updates': 5345}
train stats after 171072 examples: {'rewards_train/chosen': '0.16435', 'rewards_train/rejected': '0.054462', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10989', 'logps_train/rejected': '-108.03', 'logps_train/chosen': '-154.65', 'loss/train': '0.64673', 'examples_per_second': '31.647', 'grad_norm': '20.75', 'counters/examples': 171072, 'counters/updates': 5346}
skipping logging after 171104 examples to avoid logging too frequently
train stats after 171136 examples: {'rewards_train/chosen': '0.12442', 'rewards_train/rejected': '0.039653', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084763', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-141.16', 'loss/train': '0.66733', 'examples_per_second': '31.55', 'grad_norm': '19.875', 'counters/examples': 171136, 'counters/updates': 5348}
train stats after 171168 examples: {'rewards_train/chosen': '0.098228', 'rewards_train/rejected': '0.075177', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02305', 'logps_train/rejected': '-104.71', 'logps_train/chosen': '-100.08', 'loss/train': '0.69017', 'examples_per_second': '33.132', 'grad_norm': '19.5', 'counters/examples': 171168, 'counters/updates': 5349}
train stats after 171200 examples: {'rewards_train/chosen': '0.14853', 'rewards_train/rejected': '0.059884', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.088649', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-149.71', 'loss/train': '0.66899', 'examples_per_second': '31.616', 'grad_norm': '21.25', 'counters/examples': 171200, 'counters/updates': 5350}
train stats after 171232 examples: {'rewards_train/chosen': '0.21319', 'rewards_train/rejected': '0.079331', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13386', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-150.37', 'loss/train': '0.63828', 'examples_per_second': '31.45', 'grad_norm': '19.75', 'counters/examples': 171232, 'counters/updates': 5351}
train stats after 171264 examples: {'rewards_train/chosen': '0.17302', 'rewards_train/rejected': '0.05703', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11599', 'logps_train/rejected': '-160.19', 'logps_train/chosen': '-157.45', 'loss/train': '0.64923', 'examples_per_second': '32.044', 'grad_norm': '23', 'counters/examples': 171264, 'counters/updates': 5352}
train stats after 171296 examples: {'rewards_train/chosen': '0.26748', 'rewards_train/rejected': '0.054591', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21289', 'logps_train/rejected': '-96.761', 'logps_train/chosen': '-112.65', 'loss/train': '0.60762', 'examples_per_second': '32.084', 'grad_norm': '16.875', 'counters/examples': 171296, 'counters/updates': 5353}
train stats after 171328 examples: {'rewards_train/chosen': '0.1913', 'rewards_train/rejected': '0.12208', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069218', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-127.85', 'loss/train': '0.66998', 'examples_per_second': '30.446', 'grad_norm': '21.25', 'counters/examples': 171328, 'counters/updates': 5354}
train stats after 171360 examples: {'rewards_train/chosen': '0.22547', 'rewards_train/rejected': '0.042503', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18296', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-153.06', 'loss/train': '0.62022', 'examples_per_second': '31.646', 'grad_norm': '21.25', 'counters/examples': 171360, 'counters/updates': 5355}
skipping logging after 171392 examples to avoid logging too frequently
train stats after 171424 examples: {'rewards_train/chosen': '0.1484', 'rewards_train/rejected': '0.045117', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10328', 'logps_train/rejected': '-130.2', 'logps_train/chosen': '-159.32', 'loss/train': '0.6551', 'examples_per_second': '31.494', 'grad_norm': '21.375', 'counters/examples': 171424, 'counters/updates': 5357}
skipping logging after 171456 examples to avoid logging too frequently
train stats after 171488 examples: {'rewards_train/chosen': '0.25824', 'rewards_train/rejected': '0.058969', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19927', 'logps_train/rejected': '-183.51', 'logps_train/chosen': '-189.1', 'loss/train': '0.6212', 'examples_per_second': '24.425', 'grad_norm': '26', 'counters/examples': 171488, 'counters/updates': 5359}
train stats after 171520 examples: {'rewards_train/chosen': '0.13507', 'rewards_train/rejected': '0.1167', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018375', 'logps_train/rejected': '-102.97', 'logps_train/chosen': '-129.72', 'loss/train': '0.7008', 'examples_per_second': '29.669', 'grad_norm': '20.625', 'counters/examples': 171520, 'counters/updates': 5360}
train stats after 171552 examples: {'rewards_train/chosen': '0.12195', 'rewards_train/rejected': '0.17231', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.050355', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-130.97', 'loss/train': '0.73409', 'examples_per_second': '30.644', 'grad_norm': '23.625', 'counters/examples': 171552, 'counters/updates': 5361}
train stats after 171584 examples: {'rewards_train/chosen': '0.10949', 'rewards_train/rejected': '0.0080928', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10139', 'logps_train/rejected': '-101.82', 'logps_train/chosen': '-163.56', 'loss/train': '0.65548', 'examples_per_second': '30.208', 'grad_norm': '20.625', 'counters/examples': 171584, 'counters/updates': 5362}
train stats after 171616 examples: {'rewards_train/chosen': '0.14115', 'rewards_train/rejected': '0.12058', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020572', 'logps_train/rejected': '-156.1', 'logps_train/chosen': '-119.95', 'loss/train': '0.69862', 'examples_per_second': '30.604', 'grad_norm': '21.25', 'counters/examples': 171616, 'counters/updates': 5363}
train stats after 171648 examples: {'rewards_train/chosen': '0.23279', 'rewards_train/rejected': '0.051619', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18117', 'logps_train/rejected': '-129.28', 'logps_train/chosen': '-160.32', 'loss/train': '0.61891', 'examples_per_second': '31.818', 'grad_norm': '20.5', 'counters/examples': 171648, 'counters/updates': 5364}
train stats after 171680 examples: {'rewards_train/chosen': '0.17958', 'rewards_train/rejected': '0.076225', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10336', 'logps_train/rejected': '-131.61', 'logps_train/chosen': '-141.66', 'loss/train': '0.65119', 'examples_per_second': '31.516', 'grad_norm': '21.375', 'counters/examples': 171680, 'counters/updates': 5365}
train stats after 171712 examples: {'rewards_train/chosen': '0.18153', 'rewards_train/rejected': '0.06682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11471', 'logps_train/rejected': '-100.63', 'logps_train/chosen': '-128.98', 'loss/train': '0.65243', 'examples_per_second': '32.582', 'grad_norm': '18.5', 'counters/examples': 171712, 'counters/updates': 5366}
train stats after 171744 examples: {'rewards_train/chosen': '0.18644', 'rewards_train/rejected': '0.029051', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15739', 'logps_train/rejected': '-104.73', 'logps_train/chosen': '-120.88', 'loss/train': '0.62565', 'examples_per_second': '32.945', 'grad_norm': '18.25', 'counters/examples': 171744, 'counters/updates': 5367}
train stats after 171776 examples: {'rewards_train/chosen': '0.20323', 'rewards_train/rejected': '0.075193', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12803', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-126.97', 'loss/train': '0.64432', 'examples_per_second': '31.434', 'grad_norm': '20.25', 'counters/examples': 171776, 'counters/updates': 5368}
train stats after 171808 examples: {'rewards_train/chosen': '0.16491', 'rewards_train/rejected': '0.081414', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083492', 'logps_train/rejected': '-149.01', 'logps_train/chosen': '-151.99', 'loss/train': '0.66087', 'examples_per_second': '32.187', 'grad_norm': '21.875', 'counters/examples': 171808, 'counters/updates': 5369}
train stats after 171840 examples: {'rewards_train/chosen': '0.20712', 'rewards_train/rejected': '0.12862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078501', 'logps_train/rejected': '-160.48', 'logps_train/chosen': '-184.42', 'loss/train': '0.67061', 'examples_per_second': '32.221', 'grad_norm': '24.125', 'counters/examples': 171840, 'counters/updates': 5370}
train stats after 171872 examples: {'rewards_train/chosen': '0.17193', 'rewards_train/rejected': '-0.081343', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25327', 'logps_train/rejected': '-132.31', 'logps_train/chosen': '-161.33', 'loss/train': '0.58673', 'examples_per_second': '30.525', 'grad_norm': '20.125', 'counters/examples': 171872, 'counters/updates': 5371}
train stats after 171904 examples: {'rewards_train/chosen': '0.16914', 'rewards_train/rejected': '0.073818', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095327', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-145.54', 'loss/train': '0.66466', 'examples_per_second': '31.598', 'grad_norm': '20.5', 'counters/examples': 171904, 'counters/updates': 5372}
train stats after 171936 examples: {'rewards_train/chosen': '0.11939', 'rewards_train/rejected': '-0.011818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13121', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-182.39', 'loss/train': '0.64379', 'examples_per_second': '31.501', 'grad_norm': '22.25', 'counters/examples': 171936, 'counters/updates': 5373}
train stats after 171968 examples: {'rewards_train/chosen': '0.14389', 'rewards_train/rejected': '-0.037592', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18149', 'logps_train/rejected': '-89.885', 'logps_train/chosen': '-120.87', 'loss/train': '0.63063', 'examples_per_second': '31.509', 'grad_norm': '18', 'counters/examples': 171968, 'counters/updates': 5374}
train stats after 172000 examples: {'rewards_train/chosen': '0.1093', 'rewards_train/rejected': '0.031004', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078292', 'logps_train/rejected': '-105.84', 'logps_train/chosen': '-134.87', 'loss/train': '0.66264', 'examples_per_second': '31.474', 'grad_norm': '19.375', 'counters/examples': 172000, 'counters/updates': 5375}
train stats after 172032 examples: {'rewards_train/chosen': '0.20664', 'rewards_train/rejected': '0.068912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13773', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-163.95', 'loss/train': '0.63546', 'examples_per_second': '32.374', 'grad_norm': '21.75', 'counters/examples': 172032, 'counters/updates': 5376}
train stats after 172064 examples: {'rewards_train/chosen': '0.20509', 'rewards_train/rejected': '-0.0041615', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20925', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-137.67', 'loss/train': '0.60724', 'examples_per_second': '31.558', 'grad_norm': '18.75', 'counters/examples': 172064, 'counters/updates': 5377}
train stats after 172096 examples: {'rewards_train/chosen': '0.085343', 'rewards_train/rejected': '0.014977', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070366', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-132.04', 'loss/train': '0.67717', 'examples_per_second': '31.537', 'grad_norm': '20.625', 'counters/examples': 172096, 'counters/updates': 5378}
train stats after 172128 examples: {'rewards_train/chosen': '0.12908', 'rewards_train/rejected': '0.062705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066371', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-162.84', 'loss/train': '0.67529', 'examples_per_second': '32.062', 'grad_norm': '23.625', 'counters/examples': 172128, 'counters/updates': 5379}
train stats after 172160 examples: {'rewards_train/chosen': '0.22514', 'rewards_train/rejected': '0.021588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20355', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-130.09', 'loss/train': '0.61147', 'examples_per_second': '30.056', 'grad_norm': '18.5', 'counters/examples': 172160, 'counters/updates': 5380}
train stats after 172192 examples: {'rewards_train/chosen': '0.11758', 'rewards_train/rejected': '0.050083', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067498', 'logps_train/rejected': '-112.18', 'logps_train/chosen': '-144', 'loss/train': '0.66976', 'examples_per_second': '31.539', 'grad_norm': '20.125', 'counters/examples': 172192, 'counters/updates': 5381}
train stats after 172224 examples: {'rewards_train/chosen': '0.036384', 'rewards_train/rejected': '0.024008', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012376', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-138.75', 'loss/train': '0.69911', 'examples_per_second': '32.768', 'grad_norm': '21.125', 'counters/examples': 172224, 'counters/updates': 5382}
train stats after 172256 examples: {'rewards_train/chosen': '0.12872', 'rewards_train/rejected': '0.023211', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10551', 'logps_train/rejected': '-139.66', 'logps_train/chosen': '-110.94', 'loss/train': '0.65839', 'examples_per_second': '31.482', 'grad_norm': '20.125', 'counters/examples': 172256, 'counters/updates': 5383}
train stats after 172288 examples: {'rewards_train/chosen': '0.14858', 'rewards_train/rejected': '0.030217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11836', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-137.55', 'loss/train': '0.6461', 'examples_per_second': '31.445', 'grad_norm': '21', 'counters/examples': 172288, 'counters/updates': 5384}
train stats after 172320 examples: {'rewards_train/chosen': '0.21116', 'rewards_train/rejected': '0.074466', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1367', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-158.14', 'loss/train': '0.63913', 'examples_per_second': '32.192', 'grad_norm': '20.875', 'counters/examples': 172320, 'counters/updates': 5385}
skipping logging after 172352 examples to avoid logging too frequently
train stats after 172384 examples: {'rewards_train/chosen': '0.11304', 'rewards_train/rejected': '0.12141', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0083619', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-114.48', 'loss/train': '0.70732', 'examples_per_second': '36.374', 'grad_norm': '19.75', 'counters/examples': 172384, 'counters/updates': 5387}
train stats after 172416 examples: {'rewards_train/chosen': '0.12356', 'rewards_train/rejected': '-0.033873', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15743', 'logps_train/rejected': '-140.33', 'logps_train/chosen': '-141.5', 'loss/train': '0.63308', 'examples_per_second': '30.23', 'grad_norm': '19.625', 'counters/examples': 172416, 'counters/updates': 5388}
skipping logging after 172448 examples to avoid logging too frequently
train stats after 172480 examples: {'rewards_train/chosen': '0.11679', 'rewards_train/rejected': '-0.041687', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15847', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-141.14', 'loss/train': '0.63171', 'examples_per_second': '30.935', 'grad_norm': '19.75', 'counters/examples': 172480, 'counters/updates': 5390}
train stats after 172512 examples: {'rewards_train/chosen': '0.25292', 'rewards_train/rejected': '0.065399', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18752', 'logps_train/rejected': '-99.062', 'logps_train/chosen': '-174.72', 'loss/train': '0.61597', 'examples_per_second': '30.54', 'grad_norm': '21.125', 'counters/examples': 172512, 'counters/updates': 5391}
train stats after 172544 examples: {'rewards_train/chosen': '0.17234', 'rewards_train/rejected': '0.058238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1141', 'logps_train/rejected': '-96.865', 'logps_train/chosen': '-119.55', 'loss/train': '0.6484', 'examples_per_second': '31.08', 'grad_norm': '18.5', 'counters/examples': 172544, 'counters/updates': 5392}
train stats after 172576 examples: {'rewards_train/chosen': '0.19568', 'rewards_train/rejected': '0.057775', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13791', 'logps_train/rejected': '-162.05', 'logps_train/chosen': '-148.15', 'loss/train': '0.6468', 'examples_per_second': '29.967', 'grad_norm': '21.875', 'counters/examples': 172576, 'counters/updates': 5393}
train stats after 172608 examples: {'rewards_train/chosen': '0.1177', 'rewards_train/rejected': '0.04222', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075481', 'logps_train/rejected': '-89.544', 'logps_train/chosen': '-130.91', 'loss/train': '0.66387', 'examples_per_second': '29.925', 'grad_norm': '18.25', 'counters/examples': 172608, 'counters/updates': 5394}
skipping logging after 172640 examples to avoid logging too frequently
train stats after 172672 examples: {'rewards_train/chosen': '0.21837', 'rewards_train/rejected': '-0.03484', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25321', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-140.89', 'loss/train': '0.59397', 'examples_per_second': '32.566', 'grad_norm': '18.125', 'counters/examples': 172672, 'counters/updates': 5396}
train stats after 172704 examples: {'rewards_train/chosen': '0.25956', 'rewards_train/rejected': '0.058054', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20151', 'logps_train/rejected': '-122.52', 'logps_train/chosen': '-161.8', 'loss/train': '0.61813', 'examples_per_second': '31.439', 'grad_norm': '19.625', 'counters/examples': 172704, 'counters/updates': 5397}
train stats after 172736 examples: {'rewards_train/chosen': '0.1279', 'rewards_train/rejected': '0.059259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068643', 'logps_train/rejected': '-122.6', 'logps_train/chosen': '-148.36', 'loss/train': '0.66633', 'examples_per_second': '31.539', 'grad_norm': '21.125', 'counters/examples': 172736, 'counters/updates': 5398}
train stats after 172768 examples: {'rewards_train/chosen': '0.18743', 'rewards_train/rejected': '0.029745', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15768', 'logps_train/rejected': '-98.668', 'logps_train/chosen': '-127.69', 'loss/train': '0.63119', 'examples_per_second': '31.547', 'grad_norm': '18', 'counters/examples': 172768, 'counters/updates': 5399}
train stats after 172800 examples: {'rewards_train/chosen': '0.17571', 'rewards_train/rejected': '0.048787', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12692', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-120.26', 'loss/train': '0.642', 'examples_per_second': '30.566', 'grad_norm': '19.625', 'counters/examples': 172800, 'counters/updates': 5400}
train stats after 172832 examples: {'rewards_train/chosen': '0.24171', 'rewards_train/rejected': '0.093948', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14776', 'logps_train/rejected': '-149.29', 'logps_train/chosen': '-144.68', 'loss/train': '0.63157', 'examples_per_second': '30.609', 'grad_norm': '19.625', 'counters/examples': 172832, 'counters/updates': 5401}
train stats after 172864 examples: {'rewards_train/chosen': '0.2201', 'rewards_train/rejected': '0.17905', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041051', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-117.05', 'loss/train': '0.68281', 'examples_per_second': '32.516', 'grad_norm': '20', 'counters/examples': 172864, 'counters/updates': 5402}
train stats after 172896 examples: {'rewards_train/chosen': '0.20765', 'rewards_train/rejected': '0.17948', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028164', 'logps_train/rejected': '-116.44', 'logps_train/chosen': '-142.17', 'loss/train': '0.69727', 'examples_per_second': '31.445', 'grad_norm': '21.125', 'counters/examples': 172896, 'counters/updates': 5403}
train stats after 172928 examples: {'rewards_train/chosen': '0.22094', 'rewards_train/rejected': '0.076164', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14478', 'logps_train/rejected': '-135.72', 'logps_train/chosen': '-125.27', 'loss/train': '0.64093', 'examples_per_second': '30.878', 'grad_norm': '19.875', 'counters/examples': 172928, 'counters/updates': 5404}
train stats after 172960 examples: {'rewards_train/chosen': '0.13606', 'rewards_train/rejected': '-0.02798', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16404', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-131.76', 'loss/train': '0.62577', 'examples_per_second': '30.044', 'grad_norm': '18.625', 'counters/examples': 172960, 'counters/updates': 5405}
skipping logging after 172992 examples to avoid logging too frequently
train stats after 173024 examples: {'rewards_train/chosen': '0.18371', 'rewards_train/rejected': '-0.051072', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23478', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-179.14', 'loss/train': '0.60231', 'examples_per_second': '31.463', 'grad_norm': '20.25', 'counters/examples': 173024, 'counters/updates': 5407}
train stats after 173056 examples: {'rewards_train/chosen': '0.21572', 'rewards_train/rejected': '0.13341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082311', 'logps_train/rejected': '-144.07', 'logps_train/chosen': '-135.46', 'loss/train': '0.66568', 'examples_per_second': '30.043', 'grad_norm': '21.25', 'counters/examples': 173056, 'counters/updates': 5408}
skipping logging after 173088 examples to avoid logging too frequently
train stats after 173120 examples: {'rewards_train/chosen': '0.25198', 'rewards_train/rejected': '0.084381', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1676', 'logps_train/rejected': '-136.82', 'logps_train/chosen': '-134.63', 'loss/train': '0.6336', 'examples_per_second': '33.313', 'grad_norm': '21.25', 'counters/examples': 173120, 'counters/updates': 5410}
train stats after 173152 examples: {'rewards_train/chosen': '0.14108', 'rewards_train/rejected': '0.053066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088015', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-124.06', 'loss/train': '0.66243', 'examples_per_second': '32.283', 'grad_norm': '19.5', 'counters/examples': 173152, 'counters/updates': 5411}
train stats after 173184 examples: {'rewards_train/chosen': '0.16078', 'rewards_train/rejected': '0.017767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14301', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-130.45', 'loss/train': '0.63344', 'examples_per_second': '30.772', 'grad_norm': '19.25', 'counters/examples': 173184, 'counters/updates': 5412}
train stats after 173216 examples: {'rewards_train/chosen': '0.18108', 'rewards_train/rejected': '0.027817', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15326', 'logps_train/rejected': '-93.071', 'logps_train/chosen': '-112.49', 'loss/train': '0.62965', 'examples_per_second': '30.063', 'grad_norm': '17.875', 'counters/examples': 173216, 'counters/updates': 5413}
train stats after 173248 examples: {'rewards_train/chosen': '0.13221', 'rewards_train/rejected': '0.073843', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058372', 'logps_train/rejected': '-126', 'logps_train/chosen': '-134.26', 'loss/train': '0.67461', 'examples_per_second': '32.1', 'grad_norm': '21', 'counters/examples': 173248, 'counters/updates': 5414}
train stats after 173280 examples: {'rewards_train/chosen': '0.18837', 'rewards_train/rejected': '0.062764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12561', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-158.32', 'loss/train': '0.65321', 'examples_per_second': '30.171', 'grad_norm': '21.125', 'counters/examples': 173280, 'counters/updates': 5415}
train stats after 173312 examples: {'rewards_train/chosen': '0.19103', 'rewards_train/rejected': '0.028273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16275', 'logps_train/rejected': '-151', 'logps_train/chosen': '-168.12', 'loss/train': '0.62962', 'examples_per_second': '30.868', 'grad_norm': '22.125', 'counters/examples': 173312, 'counters/updates': 5416}
train stats after 173344 examples: {'rewards_train/chosen': '0.13293', 'rewards_train/rejected': '-0.020344', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15327', 'logps_train/rejected': '-113.36', 'logps_train/chosen': '-150.38', 'loss/train': '0.62742', 'examples_per_second': '33.177', 'grad_norm': '19.75', 'counters/examples': 173344, 'counters/updates': 5417}
train stats after 173376 examples: {'rewards_train/chosen': '0.18564', 'rewards_train/rejected': '0.082272', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10337', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-176.88', 'loss/train': '0.65249', 'examples_per_second': '31.546', 'grad_norm': '21.875', 'counters/examples': 173376, 'counters/updates': 5418}
train stats after 173408 examples: {'rewards_train/chosen': '0.19985', 'rewards_train/rejected': '0.071381', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12847', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-153.91', 'loss/train': '0.64419', 'examples_per_second': '32.204', 'grad_norm': '22.5', 'counters/examples': 173408, 'counters/updates': 5419}
skipping logging after 173440 examples to avoid logging too frequently
train stats after 173472 examples: {'rewards_train/chosen': '0.17416', 'rewards_train/rejected': '0.03446', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1397', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-149.22', 'loss/train': '0.63836', 'examples_per_second': '31.583', 'grad_norm': '20', 'counters/examples': 173472, 'counters/updates': 5421}
train stats after 173504 examples: {'rewards_train/chosen': '0.1226', 'rewards_train/rejected': '0.040595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082009', 'logps_train/rejected': '-132.41', 'logps_train/chosen': '-155.72', 'loss/train': '0.66689', 'examples_per_second': '31.955', 'grad_norm': '21.25', 'counters/examples': 173504, 'counters/updates': 5422}
skipping logging after 173536 examples to avoid logging too frequently
train stats after 173568 examples: {'rewards_train/chosen': '0.23123', 'rewards_train/rejected': '0.028623', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20261', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-136.76', 'loss/train': '0.60636', 'examples_per_second': '31.538', 'grad_norm': '19.5', 'counters/examples': 173568, 'counters/updates': 5424}
train stats after 173600 examples: {'rewards_train/chosen': '0.2584', 'rewards_train/rejected': '0.073447', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18496', 'logps_train/rejected': '-135.01', 'logps_train/chosen': '-146.5', 'loss/train': '0.6192', 'examples_per_second': '32.585', 'grad_norm': '18.875', 'counters/examples': 173600, 'counters/updates': 5425}
skipping logging after 173632 examples to avoid logging too frequently
train stats after 173664 examples: {'rewards_train/chosen': '0.13432', 'rewards_train/rejected': '0.066094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068223', 'logps_train/rejected': '-153.16', 'logps_train/chosen': '-166.56', 'loss/train': '0.67542', 'examples_per_second': '29.963', 'grad_norm': '23.5', 'counters/examples': 173664, 'counters/updates': 5427}
train stats after 173696 examples: {'rewards_train/chosen': '0.13895', 'rewards_train/rejected': '0.10222', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036729', 'logps_train/rejected': '-108.21', 'logps_train/chosen': '-132.69', 'loss/train': '0.68685', 'examples_per_second': '30.944', 'grad_norm': '19.5', 'counters/examples': 173696, 'counters/updates': 5428}
train stats after 173728 examples: {'rewards_train/chosen': '0.22792', 'rewards_train/rejected': '0.023239', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20468', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-124.91', 'loss/train': '0.60404', 'examples_per_second': '31.59', 'grad_norm': '22.25', 'counters/examples': 173728, 'counters/updates': 5429}
train stats after 173760 examples: {'rewards_train/chosen': '0.1727', 'rewards_train/rejected': '0.082589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090112', 'logps_train/rejected': '-138.11', 'logps_train/chosen': '-121.82', 'loss/train': '0.66621', 'examples_per_second': '31.163', 'grad_norm': '20.375', 'counters/examples': 173760, 'counters/updates': 5430}
train stats after 173792 examples: {'rewards_train/chosen': '0.16709', 'rewards_train/rejected': '0.098668', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.06842', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-107.36', 'loss/train': '0.6668', 'examples_per_second': '31.211', 'grad_norm': '19.125', 'counters/examples': 173792, 'counters/updates': 5431}
train stats after 173824 examples: {'rewards_train/chosen': '0.11704', 'rewards_train/rejected': '-0.01588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13292', 'logps_train/rejected': '-116.55', 'logps_train/chosen': '-145.15', 'loss/train': '0.63905', 'examples_per_second': '24.879', 'grad_norm': '20.75', 'counters/examples': 173824, 'counters/updates': 5432}
train stats after 173856 examples: {'rewards_train/chosen': '0.21662', 'rewards_train/rejected': '0.090408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12622', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-131.23', 'loss/train': '0.63928', 'examples_per_second': '31.481', 'grad_norm': '19.375', 'counters/examples': 173856, 'counters/updates': 5433}
train stats after 173888 examples: {'rewards_train/chosen': '0.13537', 'rewards_train/rejected': '0.073532', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061835', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-149.09', 'loss/train': '0.67591', 'examples_per_second': '31.238', 'grad_norm': '21.75', 'counters/examples': 173888, 'counters/updates': 5434}
train stats after 173920 examples: {'rewards_train/chosen': '0.20162', 'rewards_train/rejected': '0.053375', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14824', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-188.25', 'loss/train': '0.63518', 'examples_per_second': '25.11', 'grad_norm': '23', 'counters/examples': 173920, 'counters/updates': 5435}
train stats after 173952 examples: {'rewards_train/chosen': '0.10784', 'rewards_train/rejected': '0.014955', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092888', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-146.02', 'loss/train': '0.65999', 'examples_per_second': '32.022', 'grad_norm': '19.5', 'counters/examples': 173952, 'counters/updates': 5436}
train stats after 173984 examples: {'rewards_train/chosen': '0.1715', 'rewards_train/rejected': '0.082697', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.088806', 'logps_train/rejected': '-115.91', 'logps_train/chosen': '-139.76', 'loss/train': '0.66112', 'examples_per_second': '30.669', 'grad_norm': '19.75', 'counters/examples': 173984, 'counters/updates': 5437}
train stats after 174016 examples: {'rewards_train/chosen': '0.19911', 'rewards_train/rejected': '0.14934', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049769', 'logps_train/rejected': '-154.42', 'logps_train/chosen': '-175.32', 'loss/train': '0.68788', 'examples_per_second': '32.357', 'grad_norm': '24.25', 'counters/examples': 174016, 'counters/updates': 5438}
train stats after 174048 examples: {'rewards_train/chosen': '0.11209', 'rewards_train/rejected': '0.017216', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094875', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-121.4', 'loss/train': '0.66083', 'examples_per_second': '31.526', 'grad_norm': '19.75', 'counters/examples': 174048, 'counters/updates': 5439}
train stats after 174080 examples: {'rewards_train/chosen': '0.10134', 'rewards_train/rejected': '0.16003', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.058689', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-149.48', 'loss/train': '0.73872', 'examples_per_second': '31.485', 'grad_norm': '24.875', 'counters/examples': 174080, 'counters/updates': 5440}
train stats after 174112 examples: {'rewards_train/chosen': '0.10201', 'rewards_train/rejected': '-0.033268', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13527', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-111.89', 'loss/train': '0.63626', 'examples_per_second': '31.581', 'grad_norm': '20.375', 'counters/examples': 174112, 'counters/updates': 5441}
train stats after 174144 examples: {'rewards_train/chosen': '0.17803', 'rewards_train/rejected': '0.047346', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13068', 'logps_train/rejected': '-110.03', 'logps_train/chosen': '-125.13', 'loss/train': '0.63671', 'examples_per_second': '30.134', 'grad_norm': '18.125', 'counters/examples': 174144, 'counters/updates': 5442}
train stats after 174176 examples: {'rewards_train/chosen': '0.07401', 'rewards_train/rejected': '0.059307', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014704', 'logps_train/rejected': '-109.25', 'logps_train/chosen': '-123.47', 'loss/train': '0.69514', 'examples_per_second': '31.692', 'grad_norm': '20.5', 'counters/examples': 174176, 'counters/updates': 5443}
train stats after 174208 examples: {'rewards_train/chosen': '0.16149', 'rewards_train/rejected': '0.047081', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11441', 'logps_train/rejected': '-154.77', 'logps_train/chosen': '-128.66', 'loss/train': '0.66124', 'examples_per_second': '33.155', 'grad_norm': '21.875', 'counters/examples': 174208, 'counters/updates': 5444}
train stats after 174240 examples: {'rewards_train/chosen': '0.14875', 'rewards_train/rejected': '0.040496', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10826', 'logps_train/rejected': '-99.908', 'logps_train/chosen': '-146.47', 'loss/train': '0.65031', 'examples_per_second': '31.663', 'grad_norm': '20.75', 'counters/examples': 174240, 'counters/updates': 5445}
train stats after 174272 examples: {'rewards_train/chosen': '0.1948', 'rewards_train/rejected': '0.025318', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16948', 'logps_train/rejected': '-99.721', 'logps_train/chosen': '-126.79', 'loss/train': '0.62366', 'examples_per_second': '31.334', 'grad_norm': '18.875', 'counters/examples': 174272, 'counters/updates': 5446}
train stats after 174304 examples: {'rewards_train/chosen': '0.25085', 'rewards_train/rejected': '0.092041', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15881', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-185.19', 'loss/train': '0.631', 'examples_per_second': '30.917', 'grad_norm': '20.375', 'counters/examples': 174304, 'counters/updates': 5447}
train stats after 174336 examples: {'rewards_train/chosen': '0.12131', 'rewards_train/rejected': '0.097915', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023392', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-132.93', 'loss/train': '0.69568', 'examples_per_second': '32.417', 'grad_norm': '21.125', 'counters/examples': 174336, 'counters/updates': 5448}
train stats after 174368 examples: {'rewards_train/chosen': '0.15686', 'rewards_train/rejected': '0.037957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1189', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-166.71', 'loss/train': '0.64664', 'examples_per_second': '31.568', 'grad_norm': '19.75', 'counters/examples': 174368, 'counters/updates': 5449}
train stats after 174400 examples: {'rewards_train/chosen': '0.17727', 'rewards_train/rejected': '0.0099405', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16733', 'logps_train/rejected': '-121.7', 'logps_train/chosen': '-125.47', 'loss/train': '0.62742', 'examples_per_second': '31.779', 'grad_norm': '19.75', 'counters/examples': 174400, 'counters/updates': 5450}
train stats after 174432 examples: {'rewards_train/chosen': '0.15799', 'rewards_train/rejected': '0.016924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14107', 'logps_train/rejected': '-99.928', 'logps_train/chosen': '-140.27', 'loss/train': '0.64177', 'examples_per_second': '30.79', 'grad_norm': '20.375', 'counters/examples': 174432, 'counters/updates': 5451}
train stats after 174464 examples: {'rewards_train/chosen': '0.13023', 'rewards_train/rejected': '0.057583', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07265', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-126.37', 'loss/train': '0.67285', 'examples_per_second': '31.521', 'grad_norm': '19.625', 'counters/examples': 174464, 'counters/updates': 5452}
train stats after 174496 examples: {'rewards_train/chosen': '0.093185', 'rewards_train/rejected': '0.037844', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055341', 'logps_train/rejected': '-84.379', 'logps_train/chosen': '-103.65', 'loss/train': '0.67507', 'examples_per_second': '32.282', 'grad_norm': '17.625', 'counters/examples': 174496, 'counters/updates': 5453}
skipping logging after 174528 examples to avoid logging too frequently
train stats after 174560 examples: {'rewards_train/chosen': '0.12638', 'rewards_train/rejected': '-0.0091646', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13555', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-136.86', 'loss/train': '0.63935', 'examples_per_second': '33.882', 'grad_norm': '19.875', 'counters/examples': 174560, 'counters/updates': 5455}
train stats after 174592 examples: {'rewards_train/chosen': '0.2287', 'rewards_train/rejected': '-0.028937', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.25764', 'logps_train/rejected': '-117.4', 'logps_train/chosen': '-132.04', 'loss/train': '0.58576', 'examples_per_second': '31.522', 'grad_norm': '18.75', 'counters/examples': 174592, 'counters/updates': 5456}
train stats after 174624 examples: {'rewards_train/chosen': '0.15443', 'rewards_train/rejected': '0.077259', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077176', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-107.61', 'loss/train': '0.67006', 'examples_per_second': '31.024', 'grad_norm': '18.125', 'counters/examples': 174624, 'counters/updates': 5457}
train stats after 174656 examples: {'rewards_train/chosen': '0.24725', 'rewards_train/rejected': '0.01574', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.23151', 'logps_train/rejected': '-132.08', 'logps_train/chosen': '-172.35', 'loss/train': '0.59334', 'examples_per_second': '30.113', 'grad_norm': '19.625', 'counters/examples': 174656, 'counters/updates': 5458}
train stats after 174688 examples: {'rewards_train/chosen': '0.18637', 'rewards_train/rejected': '0.021615', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16475', 'logps_train/rejected': '-82.701', 'logps_train/chosen': '-128.44', 'loss/train': '0.6279', 'examples_per_second': '31.136', 'grad_norm': '17.875', 'counters/examples': 174688, 'counters/updates': 5459}
train stats after 174720 examples: {'rewards_train/chosen': '0.13573', 'rewards_train/rejected': '0.030802', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10492', 'logps_train/rejected': '-135.56', 'logps_train/chosen': '-113.42', 'loss/train': '0.65184', 'examples_per_second': '31.589', 'grad_norm': '19.25', 'counters/examples': 174720, 'counters/updates': 5460}
train stats after 174752 examples: {'rewards_train/chosen': '0.078178', 'rewards_train/rejected': '0.090966', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.012788', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-109.51', 'loss/train': '0.71775', 'examples_per_second': '32.415', 'grad_norm': '20.5', 'counters/examples': 174752, 'counters/updates': 5461}
train stats after 174784 examples: {'rewards_train/chosen': '0.17059', 'rewards_train/rejected': '0.11384', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056752', 'logps_train/rejected': '-105.12', 'logps_train/chosen': '-142.75', 'loss/train': '0.67449', 'examples_per_second': '31.066', 'grad_norm': '21.125', 'counters/examples': 174784, 'counters/updates': 5462}
train stats after 174816 examples: {'rewards_train/chosen': '0.12764', 'rewards_train/rejected': '0.1191', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.008543', 'logps_train/rejected': '-119.55', 'logps_train/chosen': '-134.54', 'loss/train': '0.70174', 'examples_per_second': '31.277', 'grad_norm': '21.75', 'counters/examples': 174816, 'counters/updates': 5463}
train stats after 174848 examples: {'rewards_train/chosen': '0.14644', 'rewards_train/rejected': '0.10445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041986', 'logps_train/rejected': '-111.68', 'logps_train/chosen': '-116.59', 'loss/train': '0.69536', 'examples_per_second': '31.299', 'grad_norm': '20', 'counters/examples': 174848, 'counters/updates': 5464}
train stats after 174880 examples: {'rewards_train/chosen': '0.15231', 'rewards_train/rejected': '0.10406', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048248', 'logps_train/rejected': '-105.22', 'logps_train/chosen': '-148.01', 'loss/train': '0.68301', 'examples_per_second': '31.101', 'grad_norm': '19.875', 'counters/examples': 174880, 'counters/updates': 5465}
train stats after 174912 examples: {'rewards_train/chosen': '0.058687', 'rewards_train/rejected': '-0.011934', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070621', 'logps_train/rejected': '-99.403', 'logps_train/chosen': '-105.58', 'loss/train': '0.67233', 'examples_per_second': '30.117', 'grad_norm': '17.75', 'counters/examples': 174912, 'counters/updates': 5466}
train stats after 174944 examples: {'rewards_train/chosen': '0.14362', 'rewards_train/rejected': '0.023849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11977', 'logps_train/rejected': '-112.03', 'logps_train/chosen': '-152.13', 'loss/train': '0.64095', 'examples_per_second': '30.466', 'grad_norm': '20', 'counters/examples': 174944, 'counters/updates': 5467}
train stats after 174976 examples: {'rewards_train/chosen': '0.12787', 'rewards_train/rejected': '0.077254', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050614', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-144.94', 'loss/train': '0.682', 'examples_per_second': '31.56', 'grad_norm': '21.125', 'counters/examples': 174976, 'counters/updates': 5468}
train stats after 175008 examples: {'rewards_train/chosen': '0.10231', 'rewards_train/rejected': '0.088213', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014094', 'logps_train/rejected': '-151.15', 'logps_train/chosen': '-162.83', 'loss/train': '0.70506', 'examples_per_second': '30.633', 'grad_norm': '24.25', 'counters/examples': 175008, 'counters/updates': 5469}
train stats after 175040 examples: {'rewards_train/chosen': '0.08293', 'rewards_train/rejected': '0.049721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033209', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-144.26', 'loss/train': '0.68856', 'examples_per_second': '32.421', 'grad_norm': '20', 'counters/examples': 175040, 'counters/updates': 5470}
train stats after 175072 examples: {'rewards_train/chosen': '0.11916', 'rewards_train/rejected': '0.050815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068344', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-127.05', 'loss/train': '0.67464', 'examples_per_second': '30.482', 'grad_norm': '20', 'counters/examples': 175072, 'counters/updates': 5471}
train stats after 175104 examples: {'rewards_train/chosen': '0.17158', 'rewards_train/rejected': '0.12294', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048643', 'logps_train/rejected': '-133.37', 'logps_train/chosen': '-157.76', 'loss/train': '0.67972', 'examples_per_second': '30.932', 'grad_norm': '20.5', 'counters/examples': 175104, 'counters/updates': 5472}
train stats after 175136 examples: {'rewards_train/chosen': '0.10755', 'rewards_train/rejected': '0.0628', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044754', 'logps_train/rejected': '-143.81', 'logps_train/chosen': '-147.09', 'loss/train': '0.68199', 'examples_per_second': '31.481', 'grad_norm': '23', 'counters/examples': 175136, 'counters/updates': 5473}
train stats after 175168 examples: {'rewards_train/chosen': '0.12995', 'rewards_train/rejected': '0.036808', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093137', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-139.19', 'loss/train': '0.66089', 'examples_per_second': '31.166', 'grad_norm': '20.75', 'counters/examples': 175168, 'counters/updates': 5474}
train stats after 175200 examples: {'rewards_train/chosen': '0.10611', 'rewards_train/rejected': '-0.018241', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12435', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-132.47', 'loss/train': '0.64411', 'examples_per_second': '31.059', 'grad_norm': '20.75', 'counters/examples': 175200, 'counters/updates': 5475}
train stats after 175232 examples: {'rewards_train/chosen': '0.12027', 'rewards_train/rejected': '0.063769', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056504', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-135.78', 'loss/train': '0.67937', 'examples_per_second': '31.657', 'grad_norm': '20.625', 'counters/examples': 175232, 'counters/updates': 5476}
skipping logging after 175264 examples to avoid logging too frequently
train stats after 175296 examples: {'rewards_train/chosen': '-0.0040235', 'rewards_train/rejected': '-0.045999', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041976', 'logps_train/rejected': '-97.866', 'logps_train/chosen': '-111.03', 'loss/train': '0.67773', 'examples_per_second': '31.562', 'grad_norm': '20.5', 'counters/examples': 175296, 'counters/updates': 5478}
train stats after 175328 examples: {'rewards_train/chosen': '0.17614', 'rewards_train/rejected': '0.059986', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11616', 'logps_train/rejected': '-153.87', 'logps_train/chosen': '-132.99', 'loss/train': '0.65239', 'examples_per_second': '31.447', 'grad_norm': '20.875', 'counters/examples': 175328, 'counters/updates': 5479}
train stats after 175360 examples: {'rewards_train/chosen': '0.096333', 'rewards_train/rejected': '0.046357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049976', 'logps_train/rejected': '-125.51', 'logps_train/chosen': '-172.63', 'loss/train': '0.68176', 'examples_per_second': '30.717', 'grad_norm': '22', 'counters/examples': 175360, 'counters/updates': 5480}
train stats after 175392 examples: {'rewards_train/chosen': '0.074681', 'rewards_train/rejected': '0.068851', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0058298', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-126.56', 'loss/train': '0.70349', 'examples_per_second': '30.727', 'grad_norm': '20.5', 'counters/examples': 175392, 'counters/updates': 5481}
train stats after 175424 examples: {'rewards_train/chosen': '0.14233', 'rewards_train/rejected': '0.10737', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034967', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-117.74', 'loss/train': '0.68499', 'examples_per_second': '31.462', 'grad_norm': '20.5', 'counters/examples': 175424, 'counters/updates': 5482}
train stats after 175456 examples: {'rewards_train/chosen': '0.16268', 'rewards_train/rejected': '-0.038026', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2007', 'logps_train/rejected': '-97.906', 'logps_train/chosen': '-133.65', 'loss/train': '0.60547', 'examples_per_second': '31.343', 'grad_norm': '18.625', 'counters/examples': 175456, 'counters/updates': 5483}
train stats after 175488 examples: {'rewards_train/chosen': '0.17751', 'rewards_train/rejected': '0.063633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11388', 'logps_train/rejected': '-98.589', 'logps_train/chosen': '-122.05', 'loss/train': '0.65307', 'examples_per_second': '30.065', 'grad_norm': '18.75', 'counters/examples': 175488, 'counters/updates': 5484}
train stats after 175520 examples: {'rewards_train/chosen': '0.11895', 'rewards_train/rejected': '0.057863', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061087', 'logps_train/rejected': '-107.61', 'logps_train/chosen': '-134.44', 'loss/train': '0.67407', 'examples_per_second': '30.805', 'grad_norm': '19.125', 'counters/examples': 175520, 'counters/updates': 5485}
train stats after 175552 examples: {'rewards_train/chosen': '0.21988', 'rewards_train/rejected': '0.03372', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18616', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-109.69', 'loss/train': '0.62367', 'examples_per_second': '30.987', 'grad_norm': '18.625', 'counters/examples': 175552, 'counters/updates': 5486}
train stats after 175584 examples: {'rewards_train/chosen': '0.069835', 'rewards_train/rejected': '0.052598', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017237', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-133.69', 'loss/train': '0.69161', 'examples_per_second': '30.078', 'grad_norm': '20.5', 'counters/examples': 175584, 'counters/updates': 5487}
skipping logging after 175616 examples to avoid logging too frequently
train stats after 175648 examples: {'rewards_train/chosen': '0.11073', 'rewards_train/rejected': '0.1175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0067686', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-120.45', 'loss/train': '0.70395', 'examples_per_second': '30.546', 'grad_norm': '19.5', 'counters/examples': 175648, 'counters/updates': 5489}
train stats after 175680 examples: {'rewards_train/chosen': '0.27315', 'rewards_train/rejected': '0.15949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11366', 'logps_train/rejected': '-152.75', 'logps_train/chosen': '-154.34', 'loss/train': '0.6542', 'examples_per_second': '31.384', 'grad_norm': '21.375', 'counters/examples': 175680, 'counters/updates': 5490}
train stats after 175712 examples: {'rewards_train/chosen': '0.26606', 'rewards_train/rejected': '0.16297', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10309', 'logps_train/rejected': '-133.19', 'logps_train/chosen': '-154.08', 'loss/train': '0.66289', 'examples_per_second': '32.637', 'grad_norm': '21.125', 'counters/examples': 175712, 'counters/updates': 5491}
train stats after 175744 examples: {'rewards_train/chosen': '0.18731', 'rewards_train/rejected': '-0.028343', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21565', 'logps_train/rejected': '-89.362', 'logps_train/chosen': '-155.96', 'loss/train': '0.60779', 'examples_per_second': '30.331', 'grad_norm': '18.125', 'counters/examples': 175744, 'counters/updates': 5492}
train stats after 175776 examples: {'rewards_train/chosen': '0.21488', 'rewards_train/rejected': '0.077609', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13727', 'logps_train/rejected': '-104.14', 'logps_train/chosen': '-149.06', 'loss/train': '0.63961', 'examples_per_second': '31.558', 'grad_norm': '20.125', 'counters/examples': 175776, 'counters/updates': 5493}
train stats after 175808 examples: {'rewards_train/chosen': '0.10794', 'rewards_train/rejected': '-0.0083171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11626', 'logps_train/rejected': '-151.29', 'logps_train/chosen': '-144.21', 'loss/train': '0.64785', 'examples_per_second': '31.607', 'grad_norm': '20.875', 'counters/examples': 175808, 'counters/updates': 5494}
train stats after 175840 examples: {'rewards_train/chosen': '0.15535', 'rewards_train/rejected': '0.095973', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059376', 'logps_train/rejected': '-104.64', 'logps_train/chosen': '-119.29', 'loss/train': '0.67763', 'examples_per_second': '31.079', 'grad_norm': '18.5', 'counters/examples': 175840, 'counters/updates': 5495}
train stats after 175872 examples: {'rewards_train/chosen': '0.10262', 'rewards_train/rejected': '0.05887', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043746', 'logps_train/rejected': '-145.35', 'logps_train/chosen': '-121.79', 'loss/train': '0.69935', 'examples_per_second': '32.141', 'grad_norm': '23.25', 'counters/examples': 175872, 'counters/updates': 5496}
train stats after 175904 examples: {'rewards_train/chosen': '0.13588', 'rewards_train/rejected': '0.057405', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.078475', 'logps_train/rejected': '-116.46', 'logps_train/chosen': '-122.46', 'loss/train': '0.6759', 'examples_per_second': '30.707', 'grad_norm': '20', 'counters/examples': 175904, 'counters/updates': 5497}
train stats after 175936 examples: {'rewards_train/chosen': '0.145', 'rewards_train/rejected': '-0.008831', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15383', 'logps_train/rejected': '-113.72', 'logps_train/chosen': '-143.27', 'loss/train': '0.62758', 'examples_per_second': '31.659', 'grad_norm': '19.375', 'counters/examples': 175936, 'counters/updates': 5498}
train stats after 175968 examples: {'rewards_train/chosen': '0.099938', 'rewards_train/rejected': '0.028709', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071229', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-134.6', 'loss/train': '0.67247', 'examples_per_second': '31.803', 'grad_norm': '23', 'counters/examples': 175968, 'counters/updates': 5499}
train stats after 176000 examples: {'rewards_train/chosen': '0.19959', 'rewards_train/rejected': '0.022842', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17675', 'logps_train/rejected': '-146.1', 'logps_train/chosen': '-118.48', 'loss/train': '0.61954', 'examples_per_second': '31.967', 'grad_norm': '20.375', 'counters/examples': 176000, 'counters/updates': 5500}
skipping logging after 176032 examples to avoid logging too frequently
train stats after 176064 examples: {'rewards_train/chosen': '0.052712', 'rewards_train/rejected': '0.035762', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01695', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-145.26', 'loss/train': '0.69868', 'examples_per_second': '34.222', 'grad_norm': '20.625', 'counters/examples': 176064, 'counters/updates': 5502}
train stats after 176096 examples: {'rewards_train/chosen': '0.14352', 'rewards_train/rejected': '0.016246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12727', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-161.19', 'loss/train': '0.64618', 'examples_per_second': '31.465', 'grad_norm': '23', 'counters/examples': 176096, 'counters/updates': 5503}
train stats after 176128 examples: {'rewards_train/chosen': '0.16906', 'rewards_train/rejected': '0.030719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13834', 'logps_train/rejected': '-93.496', 'logps_train/chosen': '-125.54', 'loss/train': '0.63488', 'examples_per_second': '31.744', 'grad_norm': '16.375', 'counters/examples': 176128, 'counters/updates': 5504}
train stats after 176160 examples: {'rewards_train/chosen': '0.24354', 'rewards_train/rejected': '0.13632', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10722', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-169.97', 'loss/train': '0.66248', 'examples_per_second': '31.395', 'grad_norm': '21.625', 'counters/examples': 176160, 'counters/updates': 5505}
train stats after 176192 examples: {'rewards_train/chosen': '0.17138', 'rewards_train/rejected': '0.063504', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10787', 'logps_train/rejected': '-150.11', 'logps_train/chosen': '-146.54', 'loss/train': '0.65025', 'examples_per_second': '31.179', 'grad_norm': '21.25', 'counters/examples': 176192, 'counters/updates': 5506}
train stats after 176224 examples: {'rewards_train/chosen': '0.074315', 'rewards_train/rejected': '-0.022156', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096471', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-125.65', 'loss/train': '0.67101', 'examples_per_second': '31.646', 'grad_norm': '19', 'counters/examples': 176224, 'counters/updates': 5507}
train stats after 176256 examples: {'rewards_train/chosen': '0.20379', 'rewards_train/rejected': '0.028732', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17506', 'logps_train/rejected': '-119.84', 'logps_train/chosen': '-121.86', 'loss/train': '0.62564', 'examples_per_second': '32.023', 'grad_norm': '18.875', 'counters/examples': 176256, 'counters/updates': 5508}
train stats after 176288 examples: {'rewards_train/chosen': '0.21306', 'rewards_train/rejected': '0.15677', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056289', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-136.99', 'loss/train': '0.67525', 'examples_per_second': '32.158', 'grad_norm': '21.125', 'counters/examples': 176288, 'counters/updates': 5509}
train stats after 176320 examples: {'rewards_train/chosen': '0.16343', 'rewards_train/rejected': '0.036893', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12654', 'logps_train/rejected': '-103.09', 'logps_train/chosen': '-144.44', 'loss/train': '0.64494', 'examples_per_second': '30.097', 'grad_norm': '21.25', 'counters/examples': 176320, 'counters/updates': 5510}
train stats after 176352 examples: {'rewards_train/chosen': '0.24771', 'rewards_train/rejected': '0.070395', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17732', 'logps_train/rejected': '-119.89', 'logps_train/chosen': '-168.84', 'loss/train': '0.62813', 'examples_per_second': '32.421', 'grad_norm': '21.75', 'counters/examples': 176352, 'counters/updates': 5511}
train stats after 176384 examples: {'rewards_train/chosen': '0.21312', 'rewards_train/rejected': '0.034902', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17822', 'logps_train/rejected': '-93.499', 'logps_train/chosen': '-145.87', 'loss/train': '0.62656', 'examples_per_second': '31.656', 'grad_norm': '19', 'counters/examples': 176384, 'counters/updates': 5512}
skipping logging after 176416 examples to avoid logging too frequently
train stats after 176448 examples: {'rewards_train/chosen': '0.23558', 'rewards_train/rejected': '-0.0035064', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23908', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-162.53', 'loss/train': '0.59757', 'examples_per_second': '31.671', 'grad_norm': '21.75', 'counters/examples': 176448, 'counters/updates': 5514}
skipping logging after 176480 examples to avoid logging too frequently
train stats after 176512 examples: {'rewards_train/chosen': '0.20102', 'rewards_train/rejected': '0.07927', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12175', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-128.76', 'loss/train': '0.6459', 'examples_per_second': '32.593', 'grad_norm': '20.75', 'counters/examples': 176512, 'counters/updates': 5516}
train stats after 176544 examples: {'rewards_train/chosen': '0.19724', 'rewards_train/rejected': '0.088181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10905', 'logps_train/rejected': '-130.48', 'logps_train/chosen': '-170.16', 'loss/train': '0.66222', 'examples_per_second': '31.795', 'grad_norm': '22.25', 'counters/examples': 176544, 'counters/updates': 5517}
skipping logging after 176576 examples to avoid logging too frequently
train stats after 176608 examples: {'rewards_train/chosen': '0.2736', 'rewards_train/rejected': '0.1903', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083299', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-116.18', 'loss/train': '0.68596', 'examples_per_second': '32.619', 'grad_norm': '19.75', 'counters/examples': 176608, 'counters/updates': 5519}
train stats after 176640 examples: {'rewards_train/chosen': '0.19413', 'rewards_train/rejected': '0.023938', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17019', 'logps_train/rejected': '-89.187', 'logps_train/chosen': '-130.3', 'loss/train': '0.6256', 'examples_per_second': '31.781', 'grad_norm': '17.625', 'counters/examples': 176640, 'counters/updates': 5520}
train stats after 176672 examples: {'rewards_train/chosen': '0.13176', 'rewards_train/rejected': '0.057031', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074734', 'logps_train/rejected': '-129.26', 'logps_train/chosen': '-118.93', 'loss/train': '0.67063', 'examples_per_second': '30.186', 'grad_norm': '19.625', 'counters/examples': 176672, 'counters/updates': 5521}
train stats after 176704 examples: {'rewards_train/chosen': '0.18352', 'rewards_train/rejected': '0.087242', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096279', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-145.21', 'loss/train': '0.66048', 'examples_per_second': '30.465', 'grad_norm': '21.5', 'counters/examples': 176704, 'counters/updates': 5522}
skipping logging after 176736 examples to avoid logging too frequently
train stats after 176768 examples: {'rewards_train/chosen': '0.11076', 'rewards_train/rejected': '0.022575', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088184', 'logps_train/rejected': '-82.131', 'logps_train/chosen': '-115.78', 'loss/train': '0.65783', 'examples_per_second': '32.986', 'grad_norm': '17.5', 'counters/examples': 176768, 'counters/updates': 5524}
train stats after 176800 examples: {'rewards_train/chosen': '0.10425', 'rewards_train/rejected': '0.020894', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083354', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-154.13', 'loss/train': '0.66423', 'examples_per_second': '30.934', 'grad_norm': '21.125', 'counters/examples': 176800, 'counters/updates': 5525}
train stats after 176832 examples: {'rewards_train/chosen': '0.14949', 'rewards_train/rejected': '0.072125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077368', 'logps_train/rejected': '-150.88', 'logps_train/chosen': '-158.7', 'loss/train': '0.67712', 'examples_per_second': '30.14', 'grad_norm': '22.75', 'counters/examples': 176832, 'counters/updates': 5526}
skipping logging after 176864 examples to avoid logging too frequently
train stats after 176896 examples: {'rewards_train/chosen': '0.14955', 'rewards_train/rejected': '0.0055533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.144', 'logps_train/rejected': '-113.73', 'logps_train/chosen': '-125.34', 'loss/train': '0.64033', 'examples_per_second': '31.63', 'grad_norm': '18', 'counters/examples': 176896, 'counters/updates': 5528}
train stats after 176928 examples: {'rewards_train/chosen': '0.22285', 'rewards_train/rejected': '0.1028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12005', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-139.1', 'loss/train': '0.64582', 'examples_per_second': '31.597', 'grad_norm': '20.625', 'counters/examples': 176928, 'counters/updates': 5529}
train stats after 176960 examples: {'rewards_train/chosen': '0.056895', 'rewards_train/rejected': '0.034078', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022817', 'logps_train/rejected': '-94.559', 'logps_train/chosen': '-109.21', 'loss/train': '0.69306', 'examples_per_second': '33.063', 'grad_norm': '19.75', 'counters/examples': 176960, 'counters/updates': 5530}
train stats after 176992 examples: {'rewards_train/chosen': '0.21786', 'rewards_train/rejected': '0.11098', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10688', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-150.42', 'loss/train': '0.65045', 'examples_per_second': '31.169', 'grad_norm': '21.25', 'counters/examples': 176992, 'counters/updates': 5531}
skipping logging after 177024 examples to avoid logging too frequently
train stats after 177056 examples: {'rewards_train/chosen': '0.1556', 'rewards_train/rejected': '0.022879', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13272', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-152.34', 'loss/train': '0.64307', 'examples_per_second': '24.699', 'grad_norm': '19.375', 'counters/examples': 177056, 'counters/updates': 5533}
train stats after 177088 examples: {'rewards_train/chosen': '0.19998', 'rewards_train/rejected': '0.098132', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10185', 'logps_train/rejected': '-148.24', 'logps_train/chosen': '-116.24', 'loss/train': '0.65737', 'examples_per_second': '30.277', 'grad_norm': '20.75', 'counters/examples': 177088, 'counters/updates': 5534}
train stats after 177120 examples: {'rewards_train/chosen': '0.14514', 'rewards_train/rejected': '0.073181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071958', 'logps_train/rejected': '-150.49', 'logps_train/chosen': '-145.48', 'loss/train': '0.67569', 'examples_per_second': '31.683', 'grad_norm': '21.875', 'counters/examples': 177120, 'counters/updates': 5535}
train stats after 177152 examples: {'rewards_train/chosen': '0.11673', 'rewards_train/rejected': '0.013458', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10327', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-134.28', 'loss/train': '0.65657', 'examples_per_second': '30.38', 'grad_norm': '19.5', 'counters/examples': 177152, 'counters/updates': 5536}
train stats after 177184 examples: {'rewards_train/chosen': '0.23427', 'rewards_train/rejected': '0.092659', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14161', 'logps_train/rejected': '-149.9', 'logps_train/chosen': '-184.09', 'loss/train': '0.63439', 'examples_per_second': '30.661', 'grad_norm': '23.125', 'counters/examples': 177184, 'counters/updates': 5537}
train stats after 177216 examples: {'rewards_train/chosen': '0.20893', 'rewards_train/rejected': '0.042252', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16668', 'logps_train/rejected': '-110.09', 'logps_train/chosen': '-145.25', 'loss/train': '0.62703', 'examples_per_second': '31.474', 'grad_norm': '19.25', 'counters/examples': 177216, 'counters/updates': 5538}
skipping logging after 177248 examples to avoid logging too frequently
train stats after 177280 examples: {'rewards_train/chosen': '0.31697', 'rewards_train/rejected': '0.12383', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19314', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-131.59', 'loss/train': '0.61487', 'examples_per_second': '31.857', 'grad_norm': '19', 'counters/examples': 177280, 'counters/updates': 5540}
train stats after 177312 examples: {'rewards_train/chosen': '0.11172', 'rewards_train/rejected': '0.053396', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058323', 'logps_train/rejected': '-89.752', 'logps_train/chosen': '-131.1', 'loss/train': '0.67816', 'examples_per_second': '32.941', 'grad_norm': '18.75', 'counters/examples': 177312, 'counters/updates': 5541}
train stats after 177344 examples: {'rewards_train/chosen': '0.19096', 'rewards_train/rejected': '0.14792', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04304', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-143.95', 'loss/train': '0.69135', 'examples_per_second': '30.69', 'grad_norm': '19.875', 'counters/examples': 177344, 'counters/updates': 5542}
train stats after 177376 examples: {'rewards_train/chosen': '0.34403', 'rewards_train/rejected': '0.10008', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24395', 'logps_train/rejected': '-142.15', 'logps_train/chosen': '-198', 'loss/train': '0.60848', 'examples_per_second': '30.604', 'grad_norm': '22.25', 'counters/examples': 177376, 'counters/updates': 5543}
skipping logging after 177408 examples to avoid logging too frequently
train stats after 177440 examples: {'rewards_train/chosen': '0.17869', 'rewards_train/rejected': '-0.026509', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20519', 'logps_train/rejected': '-86.652', 'logps_train/chosen': '-125.03', 'loss/train': '0.6063', 'examples_per_second': '30.776', 'grad_norm': '17.375', 'counters/examples': 177440, 'counters/updates': 5545}
train stats after 177472 examples: {'rewards_train/chosen': '0.039163', 'rewards_train/rejected': '0.01155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027613', 'logps_train/rejected': '-109', 'logps_train/chosen': '-155.38', 'loss/train': '0.68747', 'examples_per_second': '31.679', 'grad_norm': '21.5', 'counters/examples': 177472, 'counters/updates': 5546}
train stats after 177504 examples: {'rewards_train/chosen': '0.24837', 'rewards_train/rejected': '-0.0021781', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25054', 'logps_train/rejected': '-104.82', 'logps_train/chosen': '-142.08', 'loss/train': '0.58655', 'examples_per_second': '31.335', 'grad_norm': '18.5', 'counters/examples': 177504, 'counters/updates': 5547}
skipping logging after 177536 examples to avoid logging too frequently
train stats after 177568 examples: {'rewards_train/chosen': '0.084846', 'rewards_train/rejected': '0.023359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061487', 'logps_train/rejected': '-123.2', 'logps_train/chosen': '-106.45', 'loss/train': '0.67178', 'examples_per_second': '33.599', 'grad_norm': '20.875', 'counters/examples': 177568, 'counters/updates': 5549}
train stats after 177600 examples: {'rewards_train/chosen': '0.054134', 'rewards_train/rejected': '0.031624', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02251', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-134.38', 'loss/train': '0.69485', 'examples_per_second': '30.597', 'grad_norm': '22.25', 'counters/examples': 177600, 'counters/updates': 5550}
train stats after 177632 examples: {'rewards_train/chosen': '0.16011', 'rewards_train/rejected': '0.060649', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09946', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-127.49', 'loss/train': '0.65099', 'examples_per_second': '32.24', 'grad_norm': '19.875', 'counters/examples': 177632, 'counters/updates': 5551}
train stats after 177664 examples: {'rewards_train/chosen': '0.14595', 'rewards_train/rejected': '0.03177', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11418', 'logps_train/rejected': '-104.59', 'logps_train/chosen': '-128.87', 'loss/train': '0.64557', 'examples_per_second': '31.97', 'grad_norm': '18.625', 'counters/examples': 177664, 'counters/updates': 5552}
train stats after 177696 examples: {'rewards_train/chosen': '0.078703', 'rewards_train/rejected': '0.058636', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020067', 'logps_train/rejected': '-101.78', 'logps_train/chosen': '-161.65', 'loss/train': '0.69524', 'examples_per_second': '30.606', 'grad_norm': '22', 'counters/examples': 177696, 'counters/updates': 5553}
train stats after 177728 examples: {'rewards_train/chosen': '0.24231', 'rewards_train/rejected': '0.053811', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1885', 'logps_train/rejected': '-135.18', 'logps_train/chosen': '-150.22', 'loss/train': '0.61689', 'examples_per_second': '30.79', 'grad_norm': '19.875', 'counters/examples': 177728, 'counters/updates': 5554}
train stats after 177760 examples: {'rewards_train/chosen': '0.15849', 'rewards_train/rejected': '0.11459', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0439', 'logps_train/rejected': '-117.51', 'logps_train/chosen': '-140.29', 'loss/train': '0.6862', 'examples_per_second': '32.24', 'grad_norm': '20.625', 'counters/examples': 177760, 'counters/updates': 5555}
train stats after 177792 examples: {'rewards_train/chosen': '0.18192', 'rewards_train/rejected': '0.062735', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11919', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-155.01', 'loss/train': '0.65143', 'examples_per_second': '31.651', 'grad_norm': '21.625', 'counters/examples': 177792, 'counters/updates': 5556}
train stats after 177824 examples: {'rewards_train/chosen': '0.056736', 'rewards_train/rejected': '0.075212', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.018476', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-128.09', 'loss/train': '0.71532', 'examples_per_second': '31.593', 'grad_norm': '21.5', 'counters/examples': 177824, 'counters/updates': 5557}
train stats after 177856 examples: {'rewards_train/chosen': '0.14963', 'rewards_train/rejected': '-0.027628', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17726', 'logps_train/rejected': '-161.41', 'logps_train/chosen': '-144.12', 'loss/train': '0.62532', 'examples_per_second': '31.607', 'grad_norm': '21.75', 'counters/examples': 177856, 'counters/updates': 5558}
train stats after 177888 examples: {'rewards_train/chosen': '0.2184', 'rewards_train/rejected': '0.14385', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.074545', 'logps_train/rejected': '-158.77', 'logps_train/chosen': '-149.69', 'loss/train': '0.67138', 'examples_per_second': '31.061', 'grad_norm': '22.5', 'counters/examples': 177888, 'counters/updates': 5559}
train stats after 177920 examples: {'rewards_train/chosen': '0.12806', 'rewards_train/rejected': '0.046911', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081152', 'logps_train/rejected': '-108.48', 'logps_train/chosen': '-127.98', 'loss/train': '0.66827', 'examples_per_second': '31.684', 'grad_norm': '19.25', 'counters/examples': 177920, 'counters/updates': 5560}
train stats after 177952 examples: {'rewards_train/chosen': '0.13474', 'rewards_train/rejected': '-0.0065444', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14129', 'logps_train/rejected': '-86.753', 'logps_train/chosen': '-125.26', 'loss/train': '0.63485', 'examples_per_second': '31.649', 'grad_norm': '17.375', 'counters/examples': 177952, 'counters/updates': 5561}
skipping logging after 177984 examples to avoid logging too frequently
train stats after 178016 examples: {'rewards_train/chosen': '0.07409', 'rewards_train/rejected': '0.07419', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-9.9957e-05', 'logps_train/rejected': '-128.39', 'logps_train/chosen': '-146.73', 'loss/train': '0.70258', 'examples_per_second': '35.987', 'grad_norm': '22', 'counters/examples': 178016, 'counters/updates': 5563}
train stats after 178048 examples: {'rewards_train/chosen': '0.23884', 'rewards_train/rejected': '0.031656', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20719', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-165.55', 'loss/train': '0.61284', 'examples_per_second': '30.592', 'grad_norm': '19.375', 'counters/examples': 178048, 'counters/updates': 5564}
train stats after 178080 examples: {'rewards_train/chosen': '0.13269', 'rewards_train/rejected': '0.058699', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07399', 'logps_train/rejected': '-102.18', 'logps_train/chosen': '-126.81', 'loss/train': '0.674', 'examples_per_second': '31.457', 'grad_norm': '18.75', 'counters/examples': 178080, 'counters/updates': 5565}
train stats after 178112 examples: {'rewards_train/chosen': '0.11061', 'rewards_train/rejected': '0.093607', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017005', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-130.95', 'loss/train': '0.6965', 'examples_per_second': '30.582', 'grad_norm': '21.75', 'counters/examples': 178112, 'counters/updates': 5566}
skipping logging after 178144 examples to avoid logging too frequently
train stats after 178176 examples: {'rewards_train/chosen': '0.10158', 'rewards_train/rejected': '-0.094427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.196', 'logps_train/rejected': '-98.493', 'logps_train/chosen': '-102.22', 'loss/train': '0.62121', 'examples_per_second': '32.264', 'grad_norm': '17.5', 'counters/examples': 178176, 'counters/updates': 5568}
train stats after 178208 examples: {'rewards_train/chosen': '0.041711', 'rewards_train/rejected': '0.053621', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01191', 'logps_train/rejected': '-112.03', 'logps_train/chosen': '-145.99', 'loss/train': '0.70732', 'examples_per_second': '31.702', 'grad_norm': '21.5', 'counters/examples': 178208, 'counters/updates': 5569}
train stats after 178240 examples: {'rewards_train/chosen': '0.18979', 'rewards_train/rejected': '0.058036', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13176', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-144.48', 'loss/train': '0.63662', 'examples_per_second': '30.159', 'grad_norm': '20.5', 'counters/examples': 178240, 'counters/updates': 5570}
skipping logging after 178272 examples to avoid logging too frequently
train stats after 178304 examples: {'rewards_train/chosen': '0.14231', 'rewards_train/rejected': '-0.031579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17389', 'logps_train/rejected': '-96.286', 'logps_train/chosen': '-122.57', 'loss/train': '0.62534', 'examples_per_second': '31.74', 'grad_norm': '18.125', 'counters/examples': 178304, 'counters/updates': 5572}
skipping logging after 178336 examples to avoid logging too frequently
train stats after 178368 examples: {'rewards_train/chosen': '0.08328', 'rewards_train/rejected': '-0.050764', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13404', 'logps_train/rejected': '-83.998', 'logps_train/chosen': '-132.55', 'loss/train': '0.64164', 'examples_per_second': '31.043', 'grad_norm': '18.625', 'counters/examples': 178368, 'counters/updates': 5574}
train stats after 178400 examples: {'rewards_train/chosen': '0.17893', 'rewards_train/rejected': '0.077178', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10175', 'logps_train/rejected': '-107.81', 'logps_train/chosen': '-113.63', 'loss/train': '0.64911', 'examples_per_second': '32.694', 'grad_norm': '17.75', 'counters/examples': 178400, 'counters/updates': 5575}
skipping logging after 178432 examples to avoid logging too frequently
train stats after 178464 examples: {'rewards_train/chosen': '0.19634', 'rewards_train/rejected': '0.017248', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17909', 'logps_train/rejected': '-83.383', 'logps_train/chosen': '-141.16', 'loss/train': '0.62142', 'examples_per_second': '30.655', 'grad_norm': '19.125', 'counters/examples': 178464, 'counters/updates': 5577}
train stats after 178496 examples: {'rewards_train/chosen': '0.17943', 'rewards_train/rejected': '0.14753', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031897', 'logps_train/rejected': '-104.7', 'logps_train/chosen': '-147.87', 'loss/train': '0.68601', 'examples_per_second': '31.301', 'grad_norm': '20.75', 'counters/examples': 178496, 'counters/updates': 5578}
train stats after 178528 examples: {'rewards_train/chosen': '0.16924', 'rewards_train/rejected': '0.069868', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099373', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-130.81', 'loss/train': '0.66124', 'examples_per_second': '31.421', 'grad_norm': '20.125', 'counters/examples': 178528, 'counters/updates': 5579}
skipping logging after 178560 examples to avoid logging too frequently
train stats after 178592 examples: {'rewards_train/chosen': '0.11562', 'rewards_train/rejected': '-0.036399', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15202', 'logps_train/rejected': '-80.129', 'logps_train/chosen': '-121.25', 'loss/train': '0.63483', 'examples_per_second': '30.347', 'grad_norm': '17.625', 'counters/examples': 178592, 'counters/updates': 5581}
skipping logging after 178624 examples to avoid logging too frequently
train stats after 178656 examples: {'rewards_train/chosen': '0.2416', 'rewards_train/rejected': '0.16887', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072731', 'logps_train/rejected': '-134.86', 'logps_train/chosen': '-129.31', 'loss/train': '0.68491', 'examples_per_second': '31.054', 'grad_norm': '21.375', 'counters/examples': 178656, 'counters/updates': 5583}
skipping logging after 178688 examples to avoid logging too frequently
train stats after 178720 examples: {'rewards_train/chosen': '0.19658', 'rewards_train/rejected': '0.08675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10983', 'logps_train/rejected': '-85.949', 'logps_train/chosen': '-117.9', 'loss/train': '0.64563', 'examples_per_second': '33.153', 'grad_norm': '16.75', 'counters/examples': 178720, 'counters/updates': 5585}
skipping logging after 178752 examples to avoid logging too frequently
train stats after 178784 examples: {'rewards_train/chosen': '0.20027', 'rewards_train/rejected': '0.063897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13637', 'logps_train/rejected': '-139.58', 'logps_train/chosen': '-186.71', 'loss/train': '0.64782', 'examples_per_second': '31.455', 'grad_norm': '22.5', 'counters/examples': 178784, 'counters/updates': 5587}
skipping logging after 178816 examples to avoid logging too frequently
train stats after 178848 examples: {'rewards_train/chosen': '0.16461', 'rewards_train/rejected': '0.010078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15453', 'logps_train/rejected': '-131.02', 'logps_train/chosen': '-127.7', 'loss/train': '0.64117', 'examples_per_second': '31.687', 'grad_norm': '19', 'counters/examples': 178848, 'counters/updates': 5589}
train stats after 178880 examples: {'rewards_train/chosen': '0.14822', 'rewards_train/rejected': '0.0617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086517', 'logps_train/rejected': '-162.87', 'logps_train/chosen': '-126.03', 'loss/train': '0.66881', 'examples_per_second': '31.343', 'grad_norm': '22.75', 'counters/examples': 178880, 'counters/updates': 5590}
train stats after 178912 examples: {'rewards_train/chosen': '0.20926', 'rewards_train/rejected': '0.10362', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10563', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-143.54', 'loss/train': '0.65839', 'examples_per_second': '31.682', 'grad_norm': '20.875', 'counters/examples': 178912, 'counters/updates': 5591}
train stats after 178944 examples: {'rewards_train/chosen': '0.066908', 'rewards_train/rejected': '0.13829', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.071381', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-137.04', 'loss/train': '0.74975', 'examples_per_second': '31.068', 'grad_norm': '23', 'counters/examples': 178944, 'counters/updates': 5592}
train stats after 178976 examples: {'rewards_train/chosen': '0.15654', 'rewards_train/rejected': '0.030607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12593', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-125.95', 'loss/train': '0.642', 'examples_per_second': '30.997', 'grad_norm': '18.125', 'counters/examples': 178976, 'counters/updates': 5593}
train stats after 179008 examples: {'rewards_train/chosen': '0.058509', 'rewards_train/rejected': '-0.086306', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14481', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-121.49', 'loss/train': '0.63625', 'examples_per_second': '30.37', 'grad_norm': '19.625', 'counters/examples': 179008, 'counters/updates': 5594}
train stats after 179040 examples: {'rewards_train/chosen': '0.18221', 'rewards_train/rejected': '0.07781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1044', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-116.8', 'loss/train': '0.65423', 'examples_per_second': '31.108', 'grad_norm': '18.75', 'counters/examples': 179040, 'counters/updates': 5595}
train stats after 179072 examples: {'rewards_train/chosen': '0.15451', 'rewards_train/rejected': '0.061555', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092958', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-131.65', 'loss/train': '0.65897', 'examples_per_second': '32.623', 'grad_norm': '20.625', 'counters/examples': 179072, 'counters/updates': 5596}
train stats after 179104 examples: {'rewards_train/chosen': '0.16239', 'rewards_train/rejected': '0.081274', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.081113', 'logps_train/rejected': '-94.575', 'logps_train/chosen': '-148.62', 'loss/train': '0.66783', 'examples_per_second': '33.133', 'grad_norm': '19.5', 'counters/examples': 179104, 'counters/updates': 5597}
skipping logging after 179136 examples to avoid logging too frequently
train stats after 179168 examples: {'rewards_train/chosen': '0.087379', 'rewards_train/rejected': '0.023033', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064346', 'logps_train/rejected': '-103.83', 'logps_train/chosen': '-128.61', 'loss/train': '0.67107', 'examples_per_second': '33.096', 'grad_norm': '18.875', 'counters/examples': 179168, 'counters/updates': 5599}
train stats after 179200 examples: {'rewards_train/chosen': '0.12397', 'rewards_train/rejected': '0.049479', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074494', 'logps_train/rejected': '-138.87', 'logps_train/chosen': '-151.56', 'loss/train': '0.66976', 'examples_per_second': '31.474', 'grad_norm': '21.75', 'counters/examples': 179200, 'counters/updates': 5600}
skipping logging after 179232 examples to avoid logging too frequently
train stats after 179264 examples: {'rewards_train/chosen': '0.22383', 'rewards_train/rejected': '0.090852', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13297', 'logps_train/rejected': '-155.97', 'logps_train/chosen': '-156.76', 'loss/train': '0.64255', 'examples_per_second': '30.57', 'grad_norm': '22.5', 'counters/examples': 179264, 'counters/updates': 5602}
train stats after 179296 examples: {'rewards_train/chosen': '0.19718', 'rewards_train/rejected': '0.013583', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18359', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-147.24', 'loss/train': '0.61978', 'examples_per_second': '26', 'grad_norm': '19.5', 'counters/examples': 179296, 'counters/updates': 5603}
train stats after 179328 examples: {'rewards_train/chosen': '0.17889', 'rewards_train/rejected': '0.046902', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13199', 'logps_train/rejected': '-115.7', 'logps_train/chosen': '-133.05', 'loss/train': '0.64369', 'examples_per_second': '30.618', 'grad_norm': '19.625', 'counters/examples': 179328, 'counters/updates': 5604}
train stats after 179360 examples: {'rewards_train/chosen': '0.19118', 'rewards_train/rejected': '0.069406', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12178', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-114.5', 'loss/train': '0.64999', 'examples_per_second': '32.573', 'grad_norm': '20.25', 'counters/examples': 179360, 'counters/updates': 5605}
train stats after 179392 examples: {'rewards_train/chosen': '0.1323', 'rewards_train/rejected': '0.036772', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095528', 'logps_train/rejected': '-98.811', 'logps_train/chosen': '-129.67', 'loss/train': '0.66586', 'examples_per_second': '26.019', 'grad_norm': '18.5', 'counters/examples': 179392, 'counters/updates': 5606}
skipping logging after 179424 examples to avoid logging too frequently
train stats after 179456 examples: {'rewards_train/chosen': '0.0733', 'rewards_train/rejected': '0.02436', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048939', 'logps_train/rejected': '-100.82', 'logps_train/chosen': '-112.2', 'loss/train': '0.6771', 'examples_per_second': '41.746', 'grad_norm': '18.375', 'counters/examples': 179456, 'counters/updates': 5608}
train stats after 179488 examples: {'rewards_train/chosen': '0.21945', 'rewards_train/rejected': '0.097222', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12223', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-124.36', 'loss/train': '0.65011', 'examples_per_second': '31.632', 'grad_norm': '20.625', 'counters/examples': 179488, 'counters/updates': 5609}
train stats after 179520 examples: {'rewards_train/chosen': '0.21341', 'rewards_train/rejected': '-0.047694', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2611', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-153.23', 'loss/train': '0.58613', 'examples_per_second': '31.254', 'grad_norm': '18.5', 'counters/examples': 179520, 'counters/updates': 5610}
train stats after 179552 examples: {'rewards_train/chosen': '0.11932', 'rewards_train/rejected': '0.048289', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071027', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-162.51', 'loss/train': '0.67323', 'examples_per_second': '30.019', 'grad_norm': '22.875', 'counters/examples': 179552, 'counters/updates': 5611}
train stats after 179584 examples: {'rewards_train/chosen': '0.19785', 'rewards_train/rejected': '0.0049186', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19293', 'logps_train/rejected': '-101.09', 'logps_train/chosen': '-146.58', 'loss/train': '0.62178', 'examples_per_second': '31.98', 'grad_norm': '19.375', 'counters/examples': 179584, 'counters/updates': 5612}
skipping logging after 179616 examples to avoid logging too frequently
train stats after 179648 examples: {'rewards_train/chosen': '0.19673', 'rewards_train/rejected': '0.096729', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1', 'logps_train/rejected': '-159.55', 'logps_train/chosen': '-172.37', 'loss/train': '0.65939', 'examples_per_second': '33.609', 'grad_norm': '23.125', 'counters/examples': 179648, 'counters/updates': 5614}
train stats after 179680 examples: {'rewards_train/chosen': '0.14741', 'rewards_train/rejected': '0.064905', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082509', 'logps_train/rejected': '-104.26', 'logps_train/chosen': '-107.61', 'loss/train': '0.66081', 'examples_per_second': '31.02', 'grad_norm': '18.25', 'counters/examples': 179680, 'counters/updates': 5615}
train stats after 179712 examples: {'rewards_train/chosen': '0.24785', 'rewards_train/rejected': '0.035036', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21281', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-143.66', 'loss/train': '0.61558', 'examples_per_second': '30.333', 'grad_norm': '19.75', 'counters/examples': 179712, 'counters/updates': 5616}
skipping logging after 179744 examples to avoid logging too frequently
train stats after 179776 examples: {'rewards_train/chosen': '0.26377', 'rewards_train/rejected': '0.1275', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13626', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-168.83', 'loss/train': '0.64044', 'examples_per_second': '31.613', 'grad_norm': '21.125', 'counters/examples': 179776, 'counters/updates': 5618}
train stats after 179808 examples: {'rewards_train/chosen': '0.15521', 'rewards_train/rejected': '0.056255', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098957', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-130.33', 'loss/train': '0.67018', 'examples_per_second': '31.675', 'grad_norm': '19.625', 'counters/examples': 179808, 'counters/updates': 5619}
train stats after 179840 examples: {'rewards_train/chosen': '0.11955', 'rewards_train/rejected': '0.010015', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10953', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-125.63', 'loss/train': '0.65533', 'examples_per_second': '31.451', 'grad_norm': '19.125', 'counters/examples': 179840, 'counters/updates': 5620}
skipping logging after 179872 examples to avoid logging too frequently
train stats after 179904 examples: {'rewards_train/chosen': '0.10561', 'rewards_train/rejected': '0.02405', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.081558', 'logps_train/rejected': '-96.163', 'logps_train/chosen': '-104.49', 'loss/train': '0.6603', 'examples_per_second': '33.393', 'grad_norm': '18.5', 'counters/examples': 179904, 'counters/updates': 5622}
train stats after 179936 examples: {'rewards_train/chosen': '0.17651', 'rewards_train/rejected': '0.054348', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12216', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-134.38', 'loss/train': '0.64248', 'examples_per_second': '31.673', 'grad_norm': '18.5', 'counters/examples': 179936, 'counters/updates': 5623}
skipping logging after 179968 examples to avoid logging too frequently
train stats after 180000 examples: {'rewards_train/chosen': '0.098697', 'rewards_train/rejected': '0.099045', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00034804', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-131.78', 'loss/train': '0.71179', 'examples_per_second': '31.64', 'grad_norm': '22.5', 'counters/examples': 180000, 'counters/updates': 5625}
Running evaluation after 180000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.91it/s]
eval after 180000: {'rewards_eval/chosen': '0.17022', 'rewards_eval/rejected': '0.039855', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.13037', 'logps_eval/rejected': '-114.99', 'logps_eval/chosen': '-133.85', 'loss/eval': '0.6476'}
skipping save for non epoch
train stats after 180032 examples: {'rewards_train/chosen': '0.13496', 'rewards_train/rejected': '0.051354', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083607', 'logps_train/rejected': '-143.96', 'logps_train/chosen': '-143.5', 'loss/train': '0.66125', 'examples_per_second': '32.1', 'grad_norm': '22.25', 'counters/examples': 180032, 'counters/updates': 5626}
skipping logging after 180064 examples to avoid logging too frequently
train stats after 180096 examples: {'rewards_train/chosen': '0.15832', 'rewards_train/rejected': '0.0016422', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15668', 'logps_train/rejected': '-114.12', 'logps_train/chosen': '-149.83', 'loss/train': '0.62799', 'examples_per_second': '33.232', 'grad_norm': '19.5', 'counters/examples': 180096, 'counters/updates': 5628}
train stats after 180128 examples: {'rewards_train/chosen': '0.16444', 'rewards_train/rejected': '0.082906', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081535', 'logps_train/rejected': '-96.181', 'logps_train/chosen': '-117.67', 'loss/train': '0.66273', 'examples_per_second': '30.105', 'grad_norm': '18.5', 'counters/examples': 180128, 'counters/updates': 5629}
skipping logging after 180160 examples to avoid logging too frequently
train stats after 180192 examples: {'rewards_train/chosen': '0.17287', 'rewards_train/rejected': '0.059946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11292', 'logps_train/rejected': '-120.35', 'logps_train/chosen': '-165.76', 'loss/train': '0.64779', 'examples_per_second': '31.685', 'grad_norm': '21', 'counters/examples': 180192, 'counters/updates': 5631}
train stats after 180224 examples: {'rewards_train/chosen': '0.18062', 'rewards_train/rejected': '0.096346', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084277', 'logps_train/rejected': '-166.57', 'logps_train/chosen': '-133.39', 'loss/train': '0.66662', 'examples_per_second': '30.993', 'grad_norm': '21.75', 'counters/examples': 180224, 'counters/updates': 5632}
train stats after 180256 examples: {'rewards_train/chosen': '0.14923', 'rewards_train/rejected': '0.036114', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11312', 'logps_train/rejected': '-95.317', 'logps_train/chosen': '-175.91', 'loss/train': '0.64658', 'examples_per_second': '30.27', 'grad_norm': '21.625', 'counters/examples': 180256, 'counters/updates': 5633}
train stats after 180288 examples: {'rewards_train/chosen': '0.13112', 'rewards_train/rejected': '0.071794', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.059328', 'logps_train/rejected': '-99.383', 'logps_train/chosen': '-132.26', 'loss/train': '0.67777', 'examples_per_second': '31.167', 'grad_norm': '19.25', 'counters/examples': 180288, 'counters/updates': 5634}
train stats after 180320 examples: {'rewards_train/chosen': '0.10555', 'rewards_train/rejected': '0.02365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081898', 'logps_train/rejected': '-110.62', 'logps_train/chosen': '-156.1', 'loss/train': '0.66156', 'examples_per_second': '32.18', 'grad_norm': '20.875', 'counters/examples': 180320, 'counters/updates': 5635}
train stats after 180352 examples: {'rewards_train/chosen': '0.020326', 'rewards_train/rejected': '0.19923', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.1789', 'logps_train/rejected': '-145.43', 'logps_train/chosen': '-138.45', 'loss/train': '0.80711', 'examples_per_second': '32.165', 'grad_norm': '25.75', 'counters/examples': 180352, 'counters/updates': 5636}
skipping logging after 180384 examples to avoid logging too frequently
train stats after 180416 examples: {'rewards_train/chosen': '0.095874', 'rewards_train/rejected': '-0.089772', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18565', 'logps_train/rejected': '-94.048', 'logps_train/chosen': '-107.23', 'loss/train': '0.61806', 'examples_per_second': '31.704', 'grad_norm': '16.625', 'counters/examples': 180416, 'counters/updates': 5638}
train stats after 180448 examples: {'rewards_train/chosen': '0.095694', 'rewards_train/rejected': '0.060293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035401', 'logps_train/rejected': '-119.33', 'logps_train/chosen': '-103.29', 'loss/train': '0.68831', 'examples_per_second': '31.973', 'grad_norm': '20.625', 'counters/examples': 180448, 'counters/updates': 5639}
train stats after 180480 examples: {'rewards_train/chosen': '0.14272', 'rewards_train/rejected': '0.038453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10427', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-115.3', 'loss/train': '0.65476', 'examples_per_second': '31.391', 'grad_norm': '20.375', 'counters/examples': 180480, 'counters/updates': 5640}
skipping logging after 180512 examples to avoid logging too frequently
train stats after 180544 examples: {'rewards_train/chosen': '0.17946', 'rewards_train/rejected': '0.047082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13238', 'logps_train/rejected': '-90.984', 'logps_train/chosen': '-101.04', 'loss/train': '0.64054', 'examples_per_second': '34.92', 'grad_norm': '17', 'counters/examples': 180544, 'counters/updates': 5642}
train stats after 180576 examples: {'rewards_train/chosen': '0.13069', 'rewards_train/rejected': '0.014213', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11648', 'logps_train/rejected': '-97.83', 'logps_train/chosen': '-155.59', 'loss/train': '0.65084', 'examples_per_second': '30.471', 'grad_norm': '19.875', 'counters/examples': 180576, 'counters/updates': 5643}
train stats after 180608 examples: {'rewards_train/chosen': '0.19115', 'rewards_train/rejected': '0.0039681', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18719', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-168.49', 'loss/train': '0.6241', 'examples_per_second': '30.186', 'grad_norm': '21.75', 'counters/examples': 180608, 'counters/updates': 5644}
train stats after 180640 examples: {'rewards_train/chosen': '0.16097', 'rewards_train/rejected': '0.012034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14893', 'logps_train/rejected': '-101.84', 'logps_train/chosen': '-118.8', 'loss/train': '0.64327', 'examples_per_second': '31.413', 'grad_norm': '18.375', 'counters/examples': 180640, 'counters/updates': 5645}
train stats after 180672 examples: {'rewards_train/chosen': '0.22926', 'rewards_train/rejected': '0.03822', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19104', 'logps_train/rejected': '-91.237', 'logps_train/chosen': '-133.49', 'loss/train': '0.61636', 'examples_per_second': '32.749', 'grad_norm': '17.875', 'counters/examples': 180672, 'counters/updates': 5646}
train stats after 180704 examples: {'rewards_train/chosen': '0.15491', 'rewards_train/rejected': '0.080462', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074449', 'logps_train/rejected': '-110.01', 'logps_train/chosen': '-121.47', 'loss/train': '0.66796', 'examples_per_second': '31.679', 'grad_norm': '18.25', 'counters/examples': 180704, 'counters/updates': 5647}
train stats after 180736 examples: {'rewards_train/chosen': '0.10335', 'rewards_train/rejected': '0.047904', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055448', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-128.47', 'loss/train': '0.67651', 'examples_per_second': '31.287', 'grad_norm': '21.875', 'counters/examples': 180736, 'counters/updates': 5648}
train stats after 180768 examples: {'rewards_train/chosen': '0.16399', 'rewards_train/rejected': '0.12558', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038407', 'logps_train/rejected': '-103.37', 'logps_train/chosen': '-131.85', 'loss/train': '0.68069', 'examples_per_second': '30.845', 'grad_norm': '19.375', 'counters/examples': 180768, 'counters/updates': 5649}
skipping logging after 180800 examples to avoid logging too frequently
train stats after 180832 examples: {'rewards_train/chosen': '0.12691', 'rewards_train/rejected': '0.035007', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091903', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-155.42', 'loss/train': '0.66108', 'examples_per_second': '32.152', 'grad_norm': '22.5', 'counters/examples': 180832, 'counters/updates': 5651}
train stats after 180864 examples: {'rewards_train/chosen': '0.13003', 'rewards_train/rejected': '0.050289', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079744', 'logps_train/rejected': '-110.62', 'logps_train/chosen': '-167.71', 'loss/train': '0.67732', 'examples_per_second': '31.319', 'grad_norm': '22', 'counters/examples': 180864, 'counters/updates': 5652}
train stats after 180896 examples: {'rewards_train/chosen': '0.16347', 'rewards_train/rejected': '0.076014', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08746', 'logps_train/rejected': '-144.7', 'logps_train/chosen': '-154.95', 'loss/train': '0.66197', 'examples_per_second': '30.817', 'grad_norm': '21.5', 'counters/examples': 180896, 'counters/updates': 5653}
train stats after 180928 examples: {'rewards_train/chosen': '0.17144', 'rewards_train/rejected': '0.0050381', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16641', 'logps_train/rejected': '-142.03', 'logps_train/chosen': '-146.89', 'loss/train': '0.6234', 'examples_per_second': '30.601', 'grad_norm': '21.125', 'counters/examples': 180928, 'counters/updates': 5654}
train stats after 180960 examples: {'rewards_train/chosen': '0.25625', 'rewards_train/rejected': '0.044584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21166', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-117.41', 'loss/train': '0.61188', 'examples_per_second': '32.468', 'grad_norm': '19.25', 'counters/examples': 180960, 'counters/updates': 5655}
train stats after 180992 examples: {'rewards_train/chosen': '0.168', 'rewards_train/rejected': '0.094646', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073355', 'logps_train/rejected': '-141.72', 'logps_train/chosen': '-158.93', 'loss/train': '0.67027', 'examples_per_second': '31.672', 'grad_norm': '22.75', 'counters/examples': 180992, 'counters/updates': 5656}
train stats after 181024 examples: {'rewards_train/chosen': '0.10396', 'rewards_train/rejected': '0.080076', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.023884', 'logps_train/rejected': '-102.45', 'logps_train/chosen': '-137.83', 'loss/train': '0.69592', 'examples_per_second': '30.306', 'grad_norm': '20.25', 'counters/examples': 181024, 'counters/updates': 5657}
train stats after 181056 examples: {'rewards_train/chosen': '0.17595', 'rewards_train/rejected': '0.065526', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11042', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-149.72', 'loss/train': '0.64711', 'examples_per_second': '32.919', 'grad_norm': '20.5', 'counters/examples': 181056, 'counters/updates': 5658}
train stats after 181088 examples: {'rewards_train/chosen': '0.28294', 'rewards_train/rejected': '0.11566', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16727', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-172.42', 'loss/train': '0.63215', 'examples_per_second': '30.814', 'grad_norm': '21.125', 'counters/examples': 181088, 'counters/updates': 5659}
train stats after 181120 examples: {'rewards_train/chosen': '0.22613', 'rewards_train/rejected': '0.025419', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20071', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-148.88', 'loss/train': '0.61999', 'examples_per_second': '31.089', 'grad_norm': '19.875', 'counters/examples': 181120, 'counters/updates': 5660}
train stats after 181152 examples: {'rewards_train/chosen': '0.058219', 'rewards_train/rejected': '0.052726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0054925', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-162.27', 'loss/train': '0.70391', 'examples_per_second': '31.614', 'grad_norm': '24', 'counters/examples': 181152, 'counters/updates': 5661}
train stats after 181184 examples: {'rewards_train/chosen': '0.084546', 'rewards_train/rejected': '0.01987', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064676', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-133.36', 'loss/train': '0.67709', 'examples_per_second': '30.025', 'grad_norm': '21.125', 'counters/examples': 181184, 'counters/updates': 5662}
train stats after 181216 examples: {'rewards_train/chosen': '0.23441', 'rewards_train/rejected': '0.086673', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14774', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-168.46', 'loss/train': '0.63682', 'examples_per_second': '30.992', 'grad_norm': '19.625', 'counters/examples': 181216, 'counters/updates': 5663}
train stats after 181248 examples: {'rewards_train/chosen': '0.18069', 'rewards_train/rejected': '0.06566', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11503', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-161.31', 'loss/train': '0.65626', 'examples_per_second': '30.101', 'grad_norm': '21.875', 'counters/examples': 181248, 'counters/updates': 5664}
train stats after 181280 examples: {'rewards_train/chosen': '0.22757', 'rewards_train/rejected': '0.10771', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11986', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-194.01', 'loss/train': '0.6701', 'examples_per_second': '32.651', 'grad_norm': '23.125', 'counters/examples': 181280, 'counters/updates': 5665}
train stats after 181312 examples: {'rewards_train/chosen': '0.11843', 'rewards_train/rejected': '0.022088', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096342', 'logps_train/rejected': '-111.49', 'logps_train/chosen': '-120.64', 'loss/train': '0.66026', 'examples_per_second': '33.252', 'grad_norm': '19', 'counters/examples': 181312, 'counters/updates': 5666}
skipping logging after 181344 examples to avoid logging too frequently
train stats after 181376 examples: {'rewards_train/chosen': '0.059713', 'rewards_train/rejected': '-0.04156', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10127', 'logps_train/rejected': '-148.95', 'logps_train/chosen': '-114.22', 'loss/train': '0.65725', 'examples_per_second': '30.444', 'grad_norm': '22.5', 'counters/examples': 181376, 'counters/updates': 5668}
skipping logging after 181408 examples to avoid logging too frequently
train stats after 181440 examples: {'rewards_train/chosen': '0.22349', 'rewards_train/rejected': '0.068958', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15453', 'logps_train/rejected': '-135.36', 'logps_train/chosen': '-161.01', 'loss/train': '0.63651', 'examples_per_second': '34.848', 'grad_norm': '21.75', 'counters/examples': 181440, 'counters/updates': 5670}
train stats after 181472 examples: {'rewards_train/chosen': '0.23341', 'rewards_train/rejected': '0.10707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12633', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-162.31', 'loss/train': '0.64439', 'examples_per_second': '30.444', 'grad_norm': '20.75', 'counters/examples': 181472, 'counters/updates': 5671}
train stats after 181504 examples: {'rewards_train/chosen': '0.16127', 'rewards_train/rejected': '0.048228', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11304', 'logps_train/rejected': '-118.18', 'logps_train/chosen': '-123.74', 'loss/train': '0.65416', 'examples_per_second': '32.402', 'grad_norm': '19.625', 'counters/examples': 181504, 'counters/updates': 5672}
train stats after 181536 examples: {'rewards_train/chosen': '0.29576', 'rewards_train/rejected': '0.13747', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15829', 'logps_train/rejected': '-99.827', 'logps_train/chosen': '-163.47', 'loss/train': '0.63149', 'examples_per_second': '31.763', 'grad_norm': '19.375', 'counters/examples': 181536, 'counters/updates': 5673}
train stats after 181568 examples: {'rewards_train/chosen': '0.15753', 'rewards_train/rejected': '-0.007908', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16543', 'logps_train/rejected': '-118.83', 'logps_train/chosen': '-148.44', 'loss/train': '0.62921', 'examples_per_second': '31.172', 'grad_norm': '19.625', 'counters/examples': 181568, 'counters/updates': 5674}
train stats after 181600 examples: {'rewards_train/chosen': '0.20313', 'rewards_train/rejected': '0.030459', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17268', 'logps_train/rejected': '-148.53', 'logps_train/chosen': '-129.18', 'loss/train': '0.62664', 'examples_per_second': '30.689', 'grad_norm': '19.5', 'counters/examples': 181600, 'counters/updates': 5675}
train stats after 181632 examples: {'rewards_train/chosen': '0.23918', 'rewards_train/rejected': '0.073529', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16566', 'logps_train/rejected': '-90.918', 'logps_train/chosen': '-109.41', 'loss/train': '0.63537', 'examples_per_second': '32.807', 'grad_norm': '17.625', 'counters/examples': 181632, 'counters/updates': 5676}
train stats after 181664 examples: {'rewards_train/chosen': '0.078069', 'rewards_train/rejected': '-0.12563', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2037', 'logps_train/rejected': '-101.03', 'logps_train/chosen': '-144.96', 'loss/train': '0.60645', 'examples_per_second': '31.489', 'grad_norm': '20.125', 'counters/examples': 181664, 'counters/updates': 5677}
train stats after 181696 examples: {'rewards_train/chosen': '0.13717', 'rewards_train/rejected': '0.0087794', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12839', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-142.44', 'loss/train': '0.64435', 'examples_per_second': '30.974', 'grad_norm': '20.25', 'counters/examples': 181696, 'counters/updates': 5678}
train stats after 181728 examples: {'rewards_train/chosen': '0.15249', 'rewards_train/rejected': '0.079359', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073134', 'logps_train/rejected': '-162.29', 'logps_train/chosen': '-172.65', 'loss/train': '0.67464', 'examples_per_second': '31.591', 'grad_norm': '23.625', 'counters/examples': 181728, 'counters/updates': 5679}
train stats after 181760 examples: {'rewards_train/chosen': '0.1509', 'rewards_train/rejected': '0.011901', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.139', 'logps_train/rejected': '-114.61', 'logps_train/chosen': '-142.83', 'loss/train': '0.64006', 'examples_per_second': '31.499', 'grad_norm': '19.5', 'counters/examples': 181760, 'counters/updates': 5680}
skipping logging after 181792 examples to avoid logging too frequently
train stats after 181824 examples: {'rewards_train/chosen': '0.17292', 'rewards_train/rejected': '0.011726', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16119', 'logps_train/rejected': '-84.88', 'logps_train/chosen': '-104.58', 'loss/train': '0.62422', 'examples_per_second': '31.806', 'grad_norm': '16', 'counters/examples': 181824, 'counters/updates': 5682}
train stats after 181856 examples: {'rewards_train/chosen': '0.21939', 'rewards_train/rejected': '0.078131', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14126', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-146.31', 'loss/train': '0.638', 'examples_per_second': '32.727', 'grad_norm': '20.75', 'counters/examples': 181856, 'counters/updates': 5683}
train stats after 181888 examples: {'rewards_train/chosen': '0.14875', 'rewards_train/rejected': '0.010965', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13778', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-157.38', 'loss/train': '0.64713', 'examples_per_second': '32.132', 'grad_norm': '19.875', 'counters/examples': 181888, 'counters/updates': 5684}
skipping logging after 181920 examples to avoid logging too frequently
train stats after 181952 examples: {'rewards_train/chosen': '0.16655', 'rewards_train/rejected': '0.037148', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12941', 'logps_train/rejected': '-107.58', 'logps_train/chosen': '-100.54', 'loss/train': '0.64808', 'examples_per_second': '35.278', 'grad_norm': '17.75', 'counters/examples': 181952, 'counters/updates': 5686}
train stats after 181984 examples: {'rewards_train/chosen': '0.14666', 'rewards_train/rejected': '-0.00030621', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14696', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-128.83', 'loss/train': '0.63533', 'examples_per_second': '31.641', 'grad_norm': '19.125', 'counters/examples': 181984, 'counters/updates': 5687}
skipping logging after 182016 examples to avoid logging too frequently
train stats after 182048 examples: {'rewards_train/chosen': '0.12266', 'rewards_train/rejected': '0.06152', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061142', 'logps_train/rejected': '-121.43', 'logps_train/chosen': '-128', 'loss/train': '0.66891', 'examples_per_second': '33.94', 'grad_norm': '19.375', 'counters/examples': 182048, 'counters/updates': 5689}
train stats after 182080 examples: {'rewards_train/chosen': '0.19907', 'rewards_train/rejected': '0.030209', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16886', 'logps_train/rejected': '-158.41', 'logps_train/chosen': '-154.33', 'loss/train': '0.62345', 'examples_per_second': '31.66', 'grad_norm': '20.5', 'counters/examples': 182080, 'counters/updates': 5690}
skipping logging after 182112 examples to avoid logging too frequently
train stats after 182144 examples: {'rewards_train/chosen': '0.13044', 'rewards_train/rejected': '-0.005383', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13582', 'logps_train/rejected': '-99.452', 'logps_train/chosen': '-134.76', 'loss/train': '0.64822', 'examples_per_second': '33.375', 'grad_norm': '18.25', 'counters/examples': 182144, 'counters/updates': 5692}
skipping logging after 182176 examples to avoid logging too frequently
train stats after 182208 examples: {'rewards_train/chosen': '0.20683', 'rewards_train/rejected': '0.14864', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05819', 'logps_train/rejected': '-145.94', 'logps_train/chosen': '-148.3', 'loss/train': '0.67747', 'examples_per_second': '31.291', 'grad_norm': '21.75', 'counters/examples': 182208, 'counters/updates': 5694}
skipping logging after 182240 examples to avoid logging too frequently
train stats after 182272 examples: {'rewards_train/chosen': '0.14368', 'rewards_train/rejected': '0.012551', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13112', 'logps_train/rejected': '-117.78', 'logps_train/chosen': '-149.26', 'loss/train': '0.64439', 'examples_per_second': '31.153', 'grad_norm': '20.625', 'counters/examples': 182272, 'counters/updates': 5696}
train stats after 182304 examples: {'rewards_train/chosen': '0.26831', 'rewards_train/rejected': '0.092793', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17552', 'logps_train/rejected': '-91.934', 'logps_train/chosen': '-128.51', 'loss/train': '0.62306', 'examples_per_second': '31.712', 'grad_norm': '17.375', 'counters/examples': 182304, 'counters/updates': 5697}
train stats after 182336 examples: {'rewards_train/chosen': '0.14209', 'rewards_train/rejected': '0.079466', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062628', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-151.75', 'loss/train': '0.67365', 'examples_per_second': '31.793', 'grad_norm': '21.5', 'counters/examples': 182336, 'counters/updates': 5698}
train stats after 182368 examples: {'rewards_train/chosen': '0.10266', 'rewards_train/rejected': '0.044994', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057668', 'logps_train/rejected': '-128.81', 'logps_train/chosen': '-130.46', 'loss/train': '0.67084', 'examples_per_second': '30.628', 'grad_norm': '20.5', 'counters/examples': 182368, 'counters/updates': 5699}
train stats after 182400 examples: {'rewards_train/chosen': '0.21863', 'rewards_train/rejected': '0.012481', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20615', 'logps_train/rejected': '-138.99', 'logps_train/chosen': '-158.34', 'loss/train': '0.61963', 'examples_per_second': '31.607', 'grad_norm': '20', 'counters/examples': 182400, 'counters/updates': 5700}
train stats after 182432 examples: {'rewards_train/chosen': '0.13303', 'rewards_train/rejected': '0.039813', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093221', 'logps_train/rejected': '-99.634', 'logps_train/chosen': '-103.05', 'loss/train': '0.66363', 'examples_per_second': '32.837', 'grad_norm': '17.125', 'counters/examples': 182432, 'counters/updates': 5701}
train stats after 182464 examples: {'rewards_train/chosen': '0.17897', 'rewards_train/rejected': '0.093021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085947', 'logps_train/rejected': '-104.56', 'logps_train/chosen': '-142.38', 'loss/train': '0.66447', 'examples_per_second': '31.347', 'grad_norm': '19.75', 'counters/examples': 182464, 'counters/updates': 5702}
train stats after 182496 examples: {'rewards_train/chosen': '0.076049', 'rewards_train/rejected': '-0.0035555', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079605', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-117.21', 'loss/train': '0.66051', 'examples_per_second': '31.582', 'grad_norm': '19.375', 'counters/examples': 182496, 'counters/updates': 5703}
train stats after 182528 examples: {'rewards_train/chosen': '0.20428', 'rewards_train/rejected': '0.075244', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12904', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-111.53', 'loss/train': '0.64441', 'examples_per_second': '31.47', 'grad_norm': '20.125', 'counters/examples': 182528, 'counters/updates': 5704}
skipping logging after 182560 examples to avoid logging too frequently
train stats after 182592 examples: {'rewards_train/chosen': '0.203', 'rewards_train/rejected': '0.06334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13966', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-122.85', 'loss/train': '0.63783', 'examples_per_second': '33.075', 'grad_norm': '18.5', 'counters/examples': 182592, 'counters/updates': 5706}
train stats after 182624 examples: {'rewards_train/chosen': '0.15788', 'rewards_train/rejected': '0.11131', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046571', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-120.06', 'loss/train': '0.67945', 'examples_per_second': '22.083', 'grad_norm': '20', 'counters/examples': 182624, 'counters/updates': 5707}
skipping logging after 182656 examples to avoid logging too frequently
train stats after 182688 examples: {'rewards_train/chosen': '0.11157', 'rewards_train/rejected': '0.018989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09258', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-124.95', 'loss/train': '0.65981', 'examples_per_second': '31.317', 'grad_norm': '19', 'counters/examples': 182688, 'counters/updates': 5709}
train stats after 182720 examples: {'rewards_train/chosen': '0.094507', 'rewards_train/rejected': '0.074359', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020147', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-162.92', 'loss/train': '0.69351', 'examples_per_second': '29.781', 'grad_norm': '23', 'counters/examples': 182720, 'counters/updates': 5710}
train stats after 182752 examples: {'rewards_train/chosen': '0.15177', 'rewards_train/rejected': '0.041401', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11037', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-119.53', 'loss/train': '0.6489', 'examples_per_second': '31.658', 'grad_norm': '19', 'counters/examples': 182752, 'counters/updates': 5711}
train stats after 182784 examples: {'rewards_train/chosen': '0.20686', 'rewards_train/rejected': '0.0035538', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20331', 'logps_train/rejected': '-84.958', 'logps_train/chosen': '-130.16', 'loss/train': '0.6108', 'examples_per_second': '31.732', 'grad_norm': '17.5', 'counters/examples': 182784, 'counters/updates': 5712}
train stats after 182816 examples: {'rewards_train/chosen': '0.15552', 'rewards_train/rejected': '0.085731', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069791', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-139.2', 'loss/train': '0.66743', 'examples_per_second': '30.528', 'grad_norm': '20.875', 'counters/examples': 182816, 'counters/updates': 5713}
train stats after 182848 examples: {'rewards_train/chosen': '0.157', 'rewards_train/rejected': '0.0015587', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15544', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-112.44', 'loss/train': '0.6283', 'examples_per_second': '31.546', 'grad_norm': '18.875', 'counters/examples': 182848, 'counters/updates': 5714}
train stats after 182880 examples: {'rewards_train/chosen': '0.15216', 'rewards_train/rejected': '0.047776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10439', 'logps_train/rejected': '-113.25', 'logps_train/chosen': '-126.51', 'loss/train': '0.67213', 'examples_per_second': '29.638', 'grad_norm': '19.125', 'counters/examples': 182880, 'counters/updates': 5715}
train stats after 182912 examples: {'rewards_train/chosen': '0.13334', 'rewards_train/rejected': '-0.047508', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18085', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-142.61', 'loss/train': '0.62271', 'examples_per_second': '33.094', 'grad_norm': '19.375', 'counters/examples': 182912, 'counters/updates': 5716}
train stats after 182944 examples: {'rewards_train/chosen': '0.11458', 'rewards_train/rejected': '0.080092', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034483', 'logps_train/rejected': '-127.89', 'logps_train/chosen': '-157.84', 'loss/train': '0.69176', 'examples_per_second': '30.176', 'grad_norm': '22.875', 'counters/examples': 182944, 'counters/updates': 5717}
train stats after 182976 examples: {'rewards_train/chosen': '0.13005', 'rewards_train/rejected': '0.14606', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016013', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-136.27', 'loss/train': '0.71552', 'examples_per_second': '31.589', 'grad_norm': '21.5', 'counters/examples': 182976, 'counters/updates': 5718}
skipping logging after 183008 examples to avoid logging too frequently
train stats after 183040 examples: {'rewards_train/chosen': '0.21116', 'rewards_train/rejected': '0.06447', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14669', 'logps_train/rejected': '-109.96', 'logps_train/chosen': '-163.43', 'loss/train': '0.63562', 'examples_per_second': '34.539', 'grad_norm': '20.375', 'counters/examples': 183040, 'counters/updates': 5720}
skipping logging after 183072 examples to avoid logging too frequently
train stats after 183104 examples: {'rewards_train/chosen': '0.11418', 'rewards_train/rejected': '0.10204', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012139', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-123.48', 'loss/train': '0.69674', 'examples_per_second': '32.398', 'grad_norm': '19.875', 'counters/examples': 183104, 'counters/updates': 5722}
train stats after 183136 examples: {'rewards_train/chosen': '0.15164', 'rewards_train/rejected': '0.059959', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091677', 'logps_train/rejected': '-87.535', 'logps_train/chosen': '-114.64', 'loss/train': '0.65972', 'examples_per_second': '29.979', 'grad_norm': '18', 'counters/examples': 183136, 'counters/updates': 5723}
train stats after 183168 examples: {'rewards_train/chosen': '0.10564', 'rewards_train/rejected': '0.073256', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032383', 'logps_train/rejected': '-116.64', 'logps_train/chosen': '-164.89', 'loss/train': '0.69534', 'examples_per_second': '30.219', 'grad_norm': '22.75', 'counters/examples': 183168, 'counters/updates': 5724}
train stats after 183200 examples: {'rewards_train/chosen': '0.23242', 'rewards_train/rejected': '0.080246', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15217', 'logps_train/rejected': '-130.65', 'logps_train/chosen': '-145.7', 'loss/train': '0.6355', 'examples_per_second': '31.098', 'grad_norm': '21', 'counters/examples': 183200, 'counters/updates': 5725}
train stats after 183232 examples: {'rewards_train/chosen': '0.1575', 'rewards_train/rejected': '0.1194', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.038103', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-149.49', 'loss/train': '0.6879', 'examples_per_second': '31.54', 'grad_norm': '21.75', 'counters/examples': 183232, 'counters/updates': 5726}
train stats after 183264 examples: {'rewards_train/chosen': '0.25321', 'rewards_train/rejected': '0.057435', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19578', 'logps_train/rejected': '-120.3', 'logps_train/chosen': '-133.18', 'loss/train': '0.61421', 'examples_per_second': '31.593', 'grad_norm': '19.5', 'counters/examples': 183264, 'counters/updates': 5727}
train stats after 183296 examples: {'rewards_train/chosen': '0.21235', 'rewards_train/rejected': '0.071642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14071', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-141.08', 'loss/train': '0.63486', 'examples_per_second': '31.765', 'grad_norm': '20.375', 'counters/examples': 183296, 'counters/updates': 5728}
skipping logging after 183328 examples to avoid logging too frequently
train stats after 183360 examples: {'rewards_train/chosen': '0.15821', 'rewards_train/rejected': '0.041136', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11707', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-146.98', 'loss/train': '0.64978', 'examples_per_second': '32.006', 'grad_norm': '21.75', 'counters/examples': 183360, 'counters/updates': 5730}
train stats after 183392 examples: {'rewards_train/chosen': '0.17597', 'rewards_train/rejected': '-0.017634', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1936', 'logps_train/rejected': '-114.99', 'logps_train/chosen': '-127.09', 'loss/train': '0.62942', 'examples_per_second': '31.829', 'grad_norm': '19.875', 'counters/examples': 183392, 'counters/updates': 5731}
train stats after 183424 examples: {'rewards_train/chosen': '0.14256', 'rewards_train/rejected': '-0.029075', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17164', 'logps_train/rejected': '-115.07', 'logps_train/chosen': '-167.82', 'loss/train': '0.62438', 'examples_per_second': '31.636', 'grad_norm': '20.375', 'counters/examples': 183424, 'counters/updates': 5732}
train stats after 183456 examples: {'rewards_train/chosen': '0.19296', 'rewards_train/rejected': '0.01695', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17601', 'logps_train/rejected': '-108.44', 'logps_train/chosen': '-159.29', 'loss/train': '0.62642', 'examples_per_second': '32.754', 'grad_norm': '21.5', 'counters/examples': 183456, 'counters/updates': 5733}
train stats after 183488 examples: {'rewards_train/chosen': '0.13911', 'rewards_train/rejected': '0.080732', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.058376', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-157.01', 'loss/train': '0.68009', 'examples_per_second': '31.899', 'grad_norm': '22.25', 'counters/examples': 183488, 'counters/updates': 5734}
train stats after 183520 examples: {'rewards_train/chosen': '0.32319', 'rewards_train/rejected': '0.11206', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21113', 'logps_train/rejected': '-135.64', 'logps_train/chosen': '-170.1', 'loss/train': '0.60341', 'examples_per_second': '31.261', 'grad_norm': '22.125', 'counters/examples': 183520, 'counters/updates': 5735}
skipping logging after 183552 examples to avoid logging too frequently
train stats after 183584 examples: {'rewards_train/chosen': '0.15763', 'rewards_train/rejected': '0.021739', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13589', 'logps_train/rejected': '-121.87', 'logps_train/chosen': '-152.47', 'loss/train': '0.64705', 'examples_per_second': '31.581', 'grad_norm': '21.125', 'counters/examples': 183584, 'counters/updates': 5737}
train stats after 183616 examples: {'rewards_train/chosen': '0.16533', 'rewards_train/rejected': '0.0062311', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1591', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-148.79', 'loss/train': '0.62977', 'examples_per_second': '31.565', 'grad_norm': '19.375', 'counters/examples': 183616, 'counters/updates': 5738}
train stats after 183648 examples: {'rewards_train/chosen': '0.20582', 'rewards_train/rejected': '0.023045', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18278', 'logps_train/rejected': '-100.44', 'logps_train/chosen': '-133.93', 'loss/train': '0.6192', 'examples_per_second': '31.64', 'grad_norm': '18.375', 'counters/examples': 183648, 'counters/updates': 5739}
train stats after 183680 examples: {'rewards_train/chosen': '0.16409', 'rewards_train/rejected': '0.027578', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13651', 'logps_train/rejected': '-118.71', 'logps_train/chosen': '-136.16', 'loss/train': '0.6423', 'examples_per_second': '31.623', 'grad_norm': '19.625', 'counters/examples': 183680, 'counters/updates': 5740}
train stats after 183712 examples: {'rewards_train/chosen': '-0.01292', 'rewards_train/rejected': '0.022094', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.035014', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-111.85', 'loss/train': '0.72126', 'examples_per_second': '30.476', 'grad_norm': '21.625', 'counters/examples': 183712, 'counters/updates': 5741}
train stats after 183744 examples: {'rewards_train/chosen': '0.16019', 'rewards_train/rejected': '0.15611', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0040807', 'logps_train/rejected': '-136.02', 'logps_train/chosen': '-111.97', 'loss/train': '0.71178', 'examples_per_second': '31.784', 'grad_norm': '22.25', 'counters/examples': 183744, 'counters/updates': 5742}
skipping logging after 183776 examples to avoid logging too frequently
train stats after 183808 examples: {'rewards_train/chosen': '0.12182', 'rewards_train/rejected': '0.049634', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07219', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-106.45', 'loss/train': '0.67584', 'examples_per_second': '32.69', 'grad_norm': '17.75', 'counters/examples': 183808, 'counters/updates': 5744}
train stats after 183840 examples: {'rewards_train/chosen': '0.14615', 'rewards_train/rejected': '0.04112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10503', 'logps_train/rejected': '-148.49', 'logps_train/chosen': '-157.94', 'loss/train': '0.6601', 'examples_per_second': '31.357', 'grad_norm': '22.625', 'counters/examples': 183840, 'counters/updates': 5745}
train stats after 183872 examples: {'rewards_train/chosen': '0.14245', 'rewards_train/rejected': '-0.034836', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17728', 'logps_train/rejected': '-117.04', 'logps_train/chosen': '-110.81', 'loss/train': '0.62328', 'examples_per_second': '31.948', 'grad_norm': '18.625', 'counters/examples': 183872, 'counters/updates': 5746}
train stats after 183904 examples: {'rewards_train/chosen': '0.1866', 'rewards_train/rejected': '0.038955', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14764', 'logps_train/rejected': '-109.08', 'logps_train/chosen': '-152.84', 'loss/train': '0.64368', 'examples_per_second': '30.642', 'grad_norm': '21.625', 'counters/examples': 183904, 'counters/updates': 5747}
train stats after 183936 examples: {'rewards_train/chosen': '0.14543', 'rewards_train/rejected': '0.075684', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069749', 'logps_train/rejected': '-134.25', 'logps_train/chosen': '-155.76', 'loss/train': '0.66732', 'examples_per_second': '32.149', 'grad_norm': '21.125', 'counters/examples': 183936, 'counters/updates': 5748}
skipping logging after 183968 examples to avoid logging too frequently
train stats after 184000 examples: {'rewards_train/chosen': '0.21684', 'rewards_train/rejected': '0.077421', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13942', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-111.76', 'loss/train': '0.64533', 'examples_per_second': '30.119', 'grad_norm': '20.625', 'counters/examples': 184000, 'counters/updates': 5750}
train stats after 184032 examples: {'rewards_train/chosen': '0.16383', 'rewards_train/rejected': '0.067277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096556', 'logps_train/rejected': '-140.45', 'logps_train/chosen': '-170.87', 'loss/train': '0.66329', 'examples_per_second': '30.784', 'grad_norm': '22.75', 'counters/examples': 184032, 'counters/updates': 5751}
train stats after 184064 examples: {'rewards_train/chosen': '0.12766', 'rewards_train/rejected': '0.0080973', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11956', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-137.86', 'loss/train': '0.65244', 'examples_per_second': '29.979', 'grad_norm': '21.375', 'counters/examples': 184064, 'counters/updates': 5752}
train stats after 184096 examples: {'rewards_train/chosen': '0.28134', 'rewards_train/rejected': '0.0058558', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27549', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-185.63', 'loss/train': '0.57998', 'examples_per_second': '31.615', 'grad_norm': '21.5', 'counters/examples': 184096, 'counters/updates': 5753}
train stats after 184128 examples: {'rewards_train/chosen': '0.17425', 'rewards_train/rejected': '0.054069', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12018', 'logps_train/rejected': '-108.55', 'logps_train/chosen': '-125.18', 'loss/train': '0.6463', 'examples_per_second': '31.296', 'grad_norm': '19.125', 'counters/examples': 184128, 'counters/updates': 5754}
train stats after 184160 examples: {'rewards_train/chosen': '0.15219', 'rewards_train/rejected': '0.059828', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092363', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-116.44', 'loss/train': '0.66703', 'examples_per_second': '31.641', 'grad_norm': '20.625', 'counters/examples': 184160, 'counters/updates': 5755}
train stats after 184192 examples: {'rewards_train/chosen': '0.27546', 'rewards_train/rejected': '0.15243', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12303', 'logps_train/rejected': '-137.01', 'logps_train/chosen': '-159.97', 'loss/train': '0.64425', 'examples_per_second': '31.777', 'grad_norm': '22', 'counters/examples': 184192, 'counters/updates': 5756}
skipping logging after 184224 examples to avoid logging too frequently
train stats after 184256 examples: {'rewards_train/chosen': '0.19506', 'rewards_train/rejected': '0.10779', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087269', 'logps_train/rejected': '-159.7', 'logps_train/chosen': '-139.36', 'loss/train': '0.66793', 'examples_per_second': '31.059', 'grad_norm': '23.25', 'counters/examples': 184256, 'counters/updates': 5758}
train stats after 184288 examples: {'rewards_train/chosen': '0.15726', 'rewards_train/rejected': '0.10419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053071', 'logps_train/rejected': '-164.02', 'logps_train/chosen': '-127.83', 'loss/train': '0.67882', 'examples_per_second': '32.049', 'grad_norm': '22.75', 'counters/examples': 184288, 'counters/updates': 5759}
skipping logging after 184320 examples to avoid logging too frequently
train stats after 184352 examples: {'rewards_train/chosen': '0.21446', 'rewards_train/rejected': '0.034766', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17969', 'logps_train/rejected': '-109.96', 'logps_train/chosen': '-116.74', 'loss/train': '0.62435', 'examples_per_second': '30.125', 'grad_norm': '19.25', 'counters/examples': 184352, 'counters/updates': 5761}
skipping logging after 184384 examples to avoid logging too frequently
train stats after 184416 examples: {'rewards_train/chosen': '0.12768', 'rewards_train/rejected': '0.008695', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11899', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-118.63', 'loss/train': '0.64511', 'examples_per_second': '36.303', 'grad_norm': '19.125', 'counters/examples': 184416, 'counters/updates': 5763}
train stats after 184448 examples: {'rewards_train/chosen': '0.10185', 'rewards_train/rejected': '0.024977', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076871', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-128.69', 'loss/train': '0.66639', 'examples_per_second': '31.961', 'grad_norm': '19.75', 'counters/examples': 184448, 'counters/updates': 5764}
skipping logging after 184480 examples to avoid logging too frequently
train stats after 184512 examples: {'rewards_train/chosen': '0.21215', 'rewards_train/rejected': '0.057029', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15512', 'logps_train/rejected': '-97.31', 'logps_train/chosen': '-138.78', 'loss/train': '0.63646', 'examples_per_second': '30.541', 'grad_norm': '18.875', 'counters/examples': 184512, 'counters/updates': 5766}
train stats after 184544 examples: {'rewards_train/chosen': '0.18025', 'rewards_train/rejected': '0.020963', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15929', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-120.93', 'loss/train': '0.63304', 'examples_per_second': '31.54', 'grad_norm': '19.375', 'counters/examples': 184544, 'counters/updates': 5767}
train stats after 184576 examples: {'rewards_train/chosen': '0.2178', 'rewards_train/rejected': '0.038786', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17901', 'logps_train/rejected': '-104.36', 'logps_train/chosen': '-140.95', 'loss/train': '0.62771', 'examples_per_second': '30.691', 'grad_norm': '18.875', 'counters/examples': 184576, 'counters/updates': 5768}
train stats after 184608 examples: {'rewards_train/chosen': '0.15706', 'rewards_train/rejected': '0.061836', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095224', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-145.44', 'loss/train': '0.65774', 'examples_per_second': '31.607', 'grad_norm': '21.25', 'counters/examples': 184608, 'counters/updates': 5769}
skipping logging after 184640 examples to avoid logging too frequently
train stats after 184672 examples: {'rewards_train/chosen': '0.24062', 'rewards_train/rejected': '0.058687', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18194', 'logps_train/rejected': '-155.05', 'logps_train/chosen': '-143.81', 'loss/train': '0.61654', 'examples_per_second': '29.504', 'grad_norm': '21.625', 'counters/examples': 184672, 'counters/updates': 5771}
train stats after 184704 examples: {'rewards_train/chosen': '0.20353', 'rewards_train/rejected': '0.025211', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17832', 'logps_train/rejected': '-116.25', 'logps_train/chosen': '-110.94', 'loss/train': '0.62048', 'examples_per_second': '30.367', 'grad_norm': '18', 'counters/examples': 184704, 'counters/updates': 5772}
skipping logging after 184736 examples to avoid logging too frequently
train stats after 184768 examples: {'rewards_train/chosen': '0.25769', 'rewards_train/rejected': '-0.0197', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.27739', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-127.37', 'loss/train': '0.57756', 'examples_per_second': '25.567', 'grad_norm': '18.875', 'counters/examples': 184768, 'counters/updates': 5774}
train stats after 184800 examples: {'rewards_train/chosen': '0.1536', 'rewards_train/rejected': '0.0088665', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14473', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-125.01', 'loss/train': '0.64039', 'examples_per_second': '32.624', 'grad_norm': '19.25', 'counters/examples': 184800, 'counters/updates': 5775}
train stats after 184832 examples: {'rewards_train/chosen': '0.11355', 'rewards_train/rejected': '0.020112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093433', 'logps_train/rejected': '-140.81', 'logps_train/chosen': '-137.85', 'loss/train': '0.66199', 'examples_per_second': '30.106', 'grad_norm': '21.25', 'counters/examples': 184832, 'counters/updates': 5776}
train stats after 184864 examples: {'rewards_train/chosen': '0.20619', 'rewards_train/rejected': '-0.0058708', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21206', 'logps_train/rejected': '-138.04', 'logps_train/chosen': '-139.79', 'loss/train': '0.60899', 'examples_per_second': '26.26', 'grad_norm': '19.875', 'counters/examples': 184864, 'counters/updates': 5777}
train stats after 184896 examples: {'rewards_train/chosen': '0.19402', 'rewards_train/rejected': '0.048253', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14577', 'logps_train/rejected': '-112.26', 'logps_train/chosen': '-160.75', 'loss/train': '0.63328', 'examples_per_second': '30.971', 'grad_norm': '20.375', 'counters/examples': 184896, 'counters/updates': 5778}
train stats after 184928 examples: {'rewards_train/chosen': '0.12373', 'rewards_train/rejected': '0.036153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087575', 'logps_train/rejected': '-104.05', 'logps_train/chosen': '-118.68', 'loss/train': '0.66565', 'examples_per_second': '31.212', 'grad_norm': '19', 'counters/examples': 184928, 'counters/updates': 5779}
skipping logging after 184960 examples to avoid logging too frequently
train stats after 184992 examples: {'rewards_train/chosen': '0.23707', 'rewards_train/rejected': '0.071108', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16596', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-161.05', 'loss/train': '0.63401', 'examples_per_second': '31.935', 'grad_norm': '22.875', 'counters/examples': 184992, 'counters/updates': 5781}
skipping logging after 185024 examples to avoid logging too frequently
train stats after 185056 examples: {'rewards_train/chosen': '0.14995', 'rewards_train/rejected': '0.061931', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088022', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-124.2', 'loss/train': '0.66113', 'examples_per_second': '31.626', 'grad_norm': '21.25', 'counters/examples': 185056, 'counters/updates': 5783}
skipping logging after 185088 examples to avoid logging too frequently
train stats after 185120 examples: {'rewards_train/chosen': '0.19466', 'rewards_train/rejected': '-0.0047247', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19938', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-151.86', 'loss/train': '0.61337', 'examples_per_second': '30.428', 'grad_norm': '19.75', 'counters/examples': 185120, 'counters/updates': 5785}
skipping logging after 185152 examples to avoid logging too frequently
train stats after 185184 examples: {'rewards_train/chosen': '0.14755', 'rewards_train/rejected': '0.048925', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098628', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-118.19', 'loss/train': '0.66024', 'examples_per_second': '30.328', 'grad_norm': '18.875', 'counters/examples': 185184, 'counters/updates': 5787}
train stats after 185216 examples: {'rewards_train/chosen': '0.16574', 'rewards_train/rejected': '0.067635', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098103', 'logps_train/rejected': '-123.43', 'logps_train/chosen': '-137.03', 'loss/train': '0.65646', 'examples_per_second': '31.133', 'grad_norm': '21', 'counters/examples': 185216, 'counters/updates': 5788}
train stats after 185248 examples: {'rewards_train/chosen': '0.18542', 'rewards_train/rejected': '0.074616', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1108', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-129.22', 'loss/train': '0.65499', 'examples_per_second': '31.08', 'grad_norm': '19.25', 'counters/examples': 185248, 'counters/updates': 5789}
skipping logging after 185280 examples to avoid logging too frequently
train stats after 185312 examples: {'rewards_train/chosen': '0.26776', 'rewards_train/rejected': '0.10412', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16363', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-139.06', 'loss/train': '0.62518', 'examples_per_second': '32.168', 'grad_norm': '20', 'counters/examples': 185312, 'counters/updates': 5791}
train stats after 185344 examples: {'rewards_train/chosen': '0.26115', 'rewards_train/rejected': '0.10728', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15388', 'logps_train/rejected': '-142.93', 'logps_train/chosen': '-126.61', 'loss/train': '0.63004', 'examples_per_second': '30.293', 'grad_norm': '20.125', 'counters/examples': 185344, 'counters/updates': 5792}
skipping logging after 185376 examples to avoid logging too frequently
train stats after 185408 examples: {'rewards_train/chosen': '0.14646', 'rewards_train/rejected': '0.10279', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043662', 'logps_train/rejected': '-98.107', 'logps_train/chosen': '-125.35', 'loss/train': '0.68579', 'examples_per_second': '31.151', 'grad_norm': '19.125', 'counters/examples': 185408, 'counters/updates': 5794}
train stats after 185440 examples: {'rewards_train/chosen': '0.19964', 'rewards_train/rejected': '0.12513', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074512', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-155.56', 'loss/train': '0.67424', 'examples_per_second': '30.062', 'grad_norm': '22', 'counters/examples': 185440, 'counters/updates': 5795}
skipping logging after 185472 examples to avoid logging too frequently
train stats after 185504 examples: {'rewards_train/chosen': '0.17788', 'rewards_train/rejected': '0.080897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096986', 'logps_train/rejected': '-123.99', 'logps_train/chosen': '-157.66', 'loss/train': '0.65745', 'examples_per_second': '31.801', 'grad_norm': '20.875', 'counters/examples': 185504, 'counters/updates': 5797}
train stats after 185536 examples: {'rewards_train/chosen': '0.15165', 'rewards_train/rejected': '0.029592', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12205', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-146.15', 'loss/train': '0.64639', 'examples_per_second': '32.068', 'grad_norm': '20.875', 'counters/examples': 185536, 'counters/updates': 5798}
train stats after 185568 examples: {'rewards_train/chosen': '0.11667', 'rewards_train/rejected': '-0.0089718', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12564', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-137.34', 'loss/train': '0.64567', 'examples_per_second': '31.104', 'grad_norm': '20', 'counters/examples': 185568, 'counters/updates': 5799}
skipping logging after 185600 examples to avoid logging too frequently
train stats after 185632 examples: {'rewards_train/chosen': '0.16519', 'rewards_train/rejected': '0.023241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14195', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-138.73', 'loss/train': '0.63738', 'examples_per_second': '33.454', 'grad_norm': '18.25', 'counters/examples': 185632, 'counters/updates': 5801}
skipping logging after 185664 examples to avoid logging too frequently
train stats after 185696 examples: {'rewards_train/chosen': '0.15331', 'rewards_train/rejected': '0.048921', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10439', 'logps_train/rejected': '-114.34', 'logps_train/chosen': '-159.28', 'loss/train': '0.65762', 'examples_per_second': '34.563', 'grad_norm': '20.75', 'counters/examples': 185696, 'counters/updates': 5803}
train stats after 185728 examples: {'rewards_train/chosen': '0.14794', 'rewards_train/rejected': '0.036607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11134', 'logps_train/rejected': '-99.499', 'logps_train/chosen': '-136.09', 'loss/train': '0.65133', 'examples_per_second': '31.62', 'grad_norm': '19.5', 'counters/examples': 185728, 'counters/updates': 5804}
train stats after 185760 examples: {'rewards_train/chosen': '0.10899', 'rewards_train/rejected': '0.11732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0083263', 'logps_train/rejected': '-134.11', 'logps_train/chosen': '-159.22', 'loss/train': '0.7177', 'examples_per_second': '31.798', 'grad_norm': '25.25', 'counters/examples': 185760, 'counters/updates': 5805}
train stats after 185792 examples: {'rewards_train/chosen': '0.14462', 'rewards_train/rejected': '0.11972', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024898', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-133.54', 'loss/train': '0.69425', 'examples_per_second': '31.631', 'grad_norm': '21', 'counters/examples': 185792, 'counters/updates': 5806}
train stats after 185824 examples: {'rewards_train/chosen': '0.13117', 'rewards_train/rejected': '0.10085', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030315', 'logps_train/rejected': '-106.27', 'logps_train/chosen': '-109.26', 'loss/train': '0.68805', 'examples_per_second': '30.555', 'grad_norm': '19', 'counters/examples': 185824, 'counters/updates': 5807}
train stats after 185856 examples: {'rewards_train/chosen': '0.087333', 'rewards_train/rejected': '-0.024214', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11155', 'logps_train/rejected': '-124.15', 'logps_train/chosen': '-166.69', 'loss/train': '0.6515', 'examples_per_second': '30.172', 'grad_norm': '21.875', 'counters/examples': 185856, 'counters/updates': 5808}
train stats after 185888 examples: {'rewards_train/chosen': '0.16206', 'rewards_train/rejected': '0.042394', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11966', 'logps_train/rejected': '-100.22', 'logps_train/chosen': '-141.94', 'loss/train': '0.64397', 'examples_per_second': '31.322', 'grad_norm': '19.625', 'counters/examples': 185888, 'counters/updates': 5809}
train stats after 185920 examples: {'rewards_train/chosen': '0.18535', 'rewards_train/rejected': '0.060957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12439', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-108.4', 'loss/train': '0.65346', 'examples_per_second': '31.518', 'grad_norm': '19.375', 'counters/examples': 185920, 'counters/updates': 5810}
train stats after 185952 examples: {'rewards_train/chosen': '0.20962', 'rewards_train/rejected': '-0.010533', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22016', 'logps_train/rejected': '-107.22', 'logps_train/chosen': '-153.38', 'loss/train': '0.60852', 'examples_per_second': '30.149', 'grad_norm': '18.5', 'counters/examples': 185952, 'counters/updates': 5811}
train stats after 185984 examples: {'rewards_train/chosen': '0.21018', 'rewards_train/rejected': '-0.030377', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.24056', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-149.52', 'loss/train': '0.59137', 'examples_per_second': '33.077', 'grad_norm': '19.125', 'counters/examples': 185984, 'counters/updates': 5812}
train stats after 186016 examples: {'rewards_train/chosen': '0.23289', 'rewards_train/rejected': '-0.021314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25421', 'logps_train/rejected': '-103.7', 'logps_train/chosen': '-193.88', 'loss/train': '0.59595', 'examples_per_second': '31.597', 'grad_norm': '20.375', 'counters/examples': 186016, 'counters/updates': 5813}
train stats after 186048 examples: {'rewards_train/chosen': '0.16531', 'rewards_train/rejected': '0.07894', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086365', 'logps_train/rejected': '-135.79', 'logps_train/chosen': '-151.93', 'loss/train': '0.66147', 'examples_per_second': '31.558', 'grad_norm': '22', 'counters/examples': 186048, 'counters/updates': 5814}
train stats after 186080 examples: {'rewards_train/chosen': '0.1021', 'rewards_train/rejected': '-0.069307', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17141', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-159.17', 'loss/train': '0.62204', 'examples_per_second': '32.47', 'grad_norm': '19.875', 'counters/examples': 186080, 'counters/updates': 5815}
skipping logging after 186112 examples to avoid logging too frequently
train stats after 186144 examples: {'rewards_train/chosen': '0.16795', 'rewards_train/rejected': '0.026911', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14104', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-97.296', 'loss/train': '0.63613', 'examples_per_second': '38.49', 'grad_norm': '17.5', 'counters/examples': 186144, 'counters/updates': 5817}
train stats after 186176 examples: {'rewards_train/chosen': '0.12007', 'rewards_train/rejected': '0.013343', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10672', 'logps_train/rejected': '-116.81', 'logps_train/chosen': '-113.79', 'loss/train': '0.6522', 'examples_per_second': '31.573', 'grad_norm': '18.625', 'counters/examples': 186176, 'counters/updates': 5818}
train stats after 186208 examples: {'rewards_train/chosen': '0.15696', 'rewards_train/rejected': '0.12107', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035888', 'logps_train/rejected': '-112.33', 'logps_train/chosen': '-134.97', 'loss/train': '0.68864', 'examples_per_second': '32.12', 'grad_norm': '20.25', 'counters/examples': 186208, 'counters/updates': 5819}
train stats after 186240 examples: {'rewards_train/chosen': '0.2035', 'rewards_train/rejected': '0.060216', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14328', 'logps_train/rejected': '-116.41', 'logps_train/chosen': '-116.86', 'loss/train': '0.63828', 'examples_per_second': '31.563', 'grad_norm': '18.875', 'counters/examples': 186240, 'counters/updates': 5820}
train stats after 186272 examples: {'rewards_train/chosen': '0.16449', 'rewards_train/rejected': '0.053978', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11052', 'logps_train/rejected': '-133.2', 'logps_train/chosen': '-148.8', 'loss/train': '0.65178', 'examples_per_second': '32.147', 'grad_norm': '21.625', 'counters/examples': 186272, 'counters/updates': 5821}
skipping logging after 186304 examples to avoid logging too frequently
train stats after 186336 examples: {'rewards_train/chosen': '0.19343', 'rewards_train/rejected': '0.061954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13148', 'logps_train/rejected': '-103.36', 'logps_train/chosen': '-158.08', 'loss/train': '0.64661', 'examples_per_second': '32.713', 'grad_norm': '20.625', 'counters/examples': 186336, 'counters/updates': 5823}
train stats after 186368 examples: {'rewards_train/chosen': '0.18601', 'rewards_train/rejected': '0.042455', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14355', 'logps_train/rejected': '-95.566', 'logps_train/chosen': '-158.42', 'loss/train': '0.63836', 'examples_per_second': '31.566', 'grad_norm': '20.375', 'counters/examples': 186368, 'counters/updates': 5824}
train stats after 186400 examples: {'rewards_train/chosen': '0.13271', 'rewards_train/rejected': '-0.013224', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14593', 'logps_train/rejected': '-82.175', 'logps_train/chosen': '-95.82', 'loss/train': '0.63309', 'examples_per_second': '31.994', 'grad_norm': '15.5', 'counters/examples': 186400, 'counters/updates': 5825}
train stats after 186432 examples: {'rewards_train/chosen': '0.13451', 'rewards_train/rejected': '0.087594', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046917', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-151.23', 'loss/train': '0.69217', 'examples_per_second': '32.016', 'grad_norm': '21.875', 'counters/examples': 186432, 'counters/updates': 5826}
train stats after 186464 examples: {'rewards_train/chosen': '0.099085', 'rewards_train/rejected': '0.048822', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050263', 'logps_train/rejected': '-116.21', 'logps_train/chosen': '-132.54', 'loss/train': '0.67478', 'examples_per_second': '31.559', 'grad_norm': '21', 'counters/examples': 186464, 'counters/updates': 5827}
train stats after 186496 examples: {'rewards_train/chosen': '0.17281', 'rewards_train/rejected': '0.068318', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10449', 'logps_train/rejected': '-101.58', 'logps_train/chosen': '-191.5', 'loss/train': '0.6568', 'examples_per_second': '30.73', 'grad_norm': '20.5', 'counters/examples': 186496, 'counters/updates': 5828}
train stats after 186528 examples: {'rewards_train/chosen': '0.11966', 'rewards_train/rejected': '-0.048876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16854', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-142.05', 'loss/train': '0.63462', 'examples_per_second': '31.542', 'grad_norm': '18.625', 'counters/examples': 186528, 'counters/updates': 5829}
train stats after 186560 examples: {'rewards_train/chosen': '0.16466', 'rewards_train/rejected': '0.077724', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08694', 'logps_train/rejected': '-97.711', 'logps_train/chosen': '-120.47', 'loss/train': '0.66454', 'examples_per_second': '32.454', 'grad_norm': '18.875', 'counters/examples': 186560, 'counters/updates': 5830}
train stats after 186592 examples: {'rewards_train/chosen': '0.11594', 'rewards_train/rejected': '0.023049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092887', 'logps_train/rejected': '-115.97', 'logps_train/chosen': '-147.17', 'loss/train': '0.66226', 'examples_per_second': '30.117', 'grad_norm': '20.875', 'counters/examples': 186592, 'counters/updates': 5831}
train stats after 186624 examples: {'rewards_train/chosen': '0.24271', 'rewards_train/rejected': '0.12527', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11745', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-148.34', 'loss/train': '0.6544', 'examples_per_second': '31.404', 'grad_norm': '22.125', 'counters/examples': 186624, 'counters/updates': 5832}
train stats after 186656 examples: {'rewards_train/chosen': '0.21626', 'rewards_train/rejected': '0.1255', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090759', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-134.58', 'loss/train': '0.67029', 'examples_per_second': '31.374', 'grad_norm': '21', 'counters/examples': 186656, 'counters/updates': 5833}
train stats after 186688 examples: {'rewards_train/chosen': '0.16778', 'rewards_train/rejected': '0.089255', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.07853', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-143.8', 'loss/train': '0.66476', 'examples_per_second': '30.408', 'grad_norm': '21.125', 'counters/examples': 186688, 'counters/updates': 5834}
train stats after 186720 examples: {'rewards_train/chosen': '0.10038', 'rewards_train/rejected': '-0.020084', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12047', 'logps_train/rejected': '-105.66', 'logps_train/chosen': '-112.64', 'loss/train': '0.64594', 'examples_per_second': '31.567', 'grad_norm': '18.375', 'counters/examples': 186720, 'counters/updates': 5835}
train stats after 186752 examples: {'rewards_train/chosen': '0.15973', 'rewards_train/rejected': '0.022967', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13676', 'logps_train/rejected': '-117.43', 'logps_train/chosen': '-152.5', 'loss/train': '0.64517', 'examples_per_second': '32.039', 'grad_norm': '21.375', 'counters/examples': 186752, 'counters/updates': 5836}
train stats after 186784 examples: {'rewards_train/chosen': '0.17548', 'rewards_train/rejected': '0.02744', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14804', 'logps_train/rejected': '-128.3', 'logps_train/chosen': '-173.17', 'loss/train': '0.63479', 'examples_per_second': '31.521', 'grad_norm': '22.25', 'counters/examples': 186784, 'counters/updates': 5837}
train stats after 186816 examples: {'rewards_train/chosen': '0.19089', 'rewards_train/rejected': '0.091335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099556', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-147.35', 'loss/train': '0.65843', 'examples_per_second': '31.934', 'grad_norm': '21', 'counters/examples': 186816, 'counters/updates': 5838}
train stats after 186848 examples: {'rewards_train/chosen': '0.2218', 'rewards_train/rejected': '0.078175', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14363', 'logps_train/rejected': '-102.32', 'logps_train/chosen': '-117.28', 'loss/train': '0.63523', 'examples_per_second': '30.39', 'grad_norm': '18.5', 'counters/examples': 186848, 'counters/updates': 5839}
train stats after 186880 examples: {'rewards_train/chosen': '0.077304', 'rewards_train/rejected': '-0.066789', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14409', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-112.5', 'loss/train': '0.63004', 'examples_per_second': '32.777', 'grad_norm': '19.375', 'counters/examples': 186880, 'counters/updates': 5840}
train stats after 186912 examples: {'rewards_train/chosen': '0.13793', 'rewards_train/rejected': '0.0809', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057031', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-157.88', 'loss/train': '0.6742', 'examples_per_second': '30.844', 'grad_norm': '23.375', 'counters/examples': 186912, 'counters/updates': 5841}
skipping logging after 186944 examples to avoid logging too frequently
train stats after 186976 examples: {'rewards_train/chosen': '0.18458', 'rewards_train/rejected': '0.052648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13193', 'logps_train/rejected': '-104.08', 'logps_train/chosen': '-121.04', 'loss/train': '0.64373', 'examples_per_second': '34.24', 'grad_norm': '19.875', 'counters/examples': 186976, 'counters/updates': 5843}
train stats after 187008 examples: {'rewards_train/chosen': '0.096278', 'rewards_train/rejected': '0.015858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08042', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-119.62', 'loss/train': '0.6724', 'examples_per_second': '30.457', 'grad_norm': '20', 'counters/examples': 187008, 'counters/updates': 5844}
train stats after 187040 examples: {'rewards_train/chosen': '0.15799', 'rewards_train/rejected': '-0.0078491', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16584', 'logps_train/rejected': '-107.38', 'logps_train/chosen': '-137.78', 'loss/train': '0.62333', 'examples_per_second': '31.623', 'grad_norm': '17.75', 'counters/examples': 187040, 'counters/updates': 5845}
train stats after 187072 examples: {'rewards_train/chosen': '0.18719', 'rewards_train/rejected': '0.076929', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11026', 'logps_train/rejected': '-83.821', 'logps_train/chosen': '-120.54', 'loss/train': '0.65116', 'examples_per_second': '32.717', 'grad_norm': '17.625', 'counters/examples': 187072, 'counters/updates': 5846}
train stats after 187104 examples: {'rewards_train/chosen': '0.17716', 'rewards_train/rejected': '0.05752', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11964', 'logps_train/rejected': '-90.799', 'logps_train/chosen': '-137.39', 'loss/train': '0.65126', 'examples_per_second': '29.877', 'grad_norm': '18.625', 'counters/examples': 187104, 'counters/updates': 5847}
train stats after 187136 examples: {'rewards_train/chosen': '0.17642', 'rewards_train/rejected': '0.070541', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10587', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-132.23', 'loss/train': '0.66071', 'examples_per_second': '30.75', 'grad_norm': '19', 'counters/examples': 187136, 'counters/updates': 5848}
train stats after 187168 examples: {'rewards_train/chosen': '0.14967', 'rewards_train/rejected': '-0.031615', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18128', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-126.59', 'loss/train': '0.61813', 'examples_per_second': '31.523', 'grad_norm': '19.125', 'counters/examples': 187168, 'counters/updates': 5849}
train stats after 187200 examples: {'rewards_train/chosen': '0.18988', 'rewards_train/rejected': '0.063476', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12641', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-141.23', 'loss/train': '0.64692', 'examples_per_second': '31.867', 'grad_norm': '19.625', 'counters/examples': 187200, 'counters/updates': 5850}
train stats after 187232 examples: {'rewards_train/chosen': '0.081986', 'rewards_train/rejected': '0.01092', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071066', 'logps_train/rejected': '-96.611', 'logps_train/chosen': '-123.09', 'loss/train': '0.67312', 'examples_per_second': '31.533', 'grad_norm': '19.75', 'counters/examples': 187232, 'counters/updates': 5851}
skipping logging after 187264 examples to avoid logging too frequently
train stats after 187296 examples: {'rewards_train/chosen': '0.18967', 'rewards_train/rejected': '0.086329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10334', 'logps_train/rejected': '-103.25', 'logps_train/chosen': '-139.33', 'loss/train': '0.66531', 'examples_per_second': '35.612', 'grad_norm': '20.5', 'counters/examples': 187296, 'counters/updates': 5853}
train stats after 187328 examples: {'rewards_train/chosen': '0.25135', 'rewards_train/rejected': '0.011937', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23941', 'logps_train/rejected': '-106.22', 'logps_train/chosen': '-130.74', 'loss/train': '0.59187', 'examples_per_second': '31.464', 'grad_norm': '17.5', 'counters/examples': 187328, 'counters/updates': 5854}
train stats after 187360 examples: {'rewards_train/chosen': '0.092232', 'rewards_train/rejected': '0.058983', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033249', 'logps_train/rejected': '-108.12', 'logps_train/chosen': '-134', 'loss/train': '0.69959', 'examples_per_second': '31.47', 'grad_norm': '20.875', 'counters/examples': 187360, 'counters/updates': 5855}
skipping logging after 187392 examples to avoid logging too frequently
train stats after 187424 examples: {'rewards_train/chosen': '0.2175', 'rewards_train/rejected': '0.0096299', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20787', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-146.96', 'loss/train': '0.60665', 'examples_per_second': '32.265', 'grad_norm': '21.5', 'counters/examples': 187424, 'counters/updates': 5857}
train stats after 187456 examples: {'rewards_train/chosen': '0.23384', 'rewards_train/rejected': '0.084015', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14983', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-132.21', 'loss/train': '0.63585', 'examples_per_second': '30.095', 'grad_norm': '17.875', 'counters/examples': 187456, 'counters/updates': 5858}
train stats after 187488 examples: {'rewards_train/chosen': '0.15818', 'rewards_train/rejected': '0.084029', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.074148', 'logps_train/rejected': '-99.185', 'logps_train/chosen': '-126.99', 'loss/train': '0.67447', 'examples_per_second': '30.417', 'grad_norm': '19.25', 'counters/examples': 187488, 'counters/updates': 5859}
train stats after 187520 examples: {'rewards_train/chosen': '0.2579', 'rewards_train/rejected': '0.11764', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14027', 'logps_train/rejected': '-185.87', 'logps_train/chosen': '-157.63', 'loss/train': '0.6451', 'examples_per_second': '31.129', 'grad_norm': '23.5', 'counters/examples': 187520, 'counters/updates': 5860}
train stats after 187552 examples: {'rewards_train/chosen': '0.27495', 'rewards_train/rejected': '0.098986', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17597', 'logps_train/rejected': '-111.53', 'logps_train/chosen': '-133.86', 'loss/train': '0.62554', 'examples_per_second': '30.114', 'grad_norm': '18.75', 'counters/examples': 187552, 'counters/updates': 5861}
train stats after 187584 examples: {'rewards_train/chosen': '0.24438', 'rewards_train/rejected': '0.047106', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19727', 'logps_train/rejected': '-124.93', 'logps_train/chosen': '-123.09', 'loss/train': '0.61865', 'examples_per_second': '32.462', 'grad_norm': '19.125', 'counters/examples': 187584, 'counters/updates': 5862}
train stats after 187616 examples: {'rewards_train/chosen': '0.1202', 'rewards_train/rejected': '0.019675', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10052', 'logps_train/rejected': '-146.55', 'logps_train/chosen': '-131.78', 'loss/train': '0.64976', 'examples_per_second': '31.496', 'grad_norm': '21', 'counters/examples': 187616, 'counters/updates': 5863}
skipping logging after 187648 examples to avoid logging too frequently
train stats after 187680 examples: {'rewards_train/chosen': '0.10456', 'rewards_train/rejected': '0.01109', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093466', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-148.08', 'loss/train': '0.66387', 'examples_per_second': '32.677', 'grad_norm': '21.125', 'counters/examples': 187680, 'counters/updates': 5865}
train stats after 187712 examples: {'rewards_train/chosen': '0.16499', 'rewards_train/rejected': '0.067766', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.097223', 'logps_train/rejected': '-95.378', 'logps_train/chosen': '-120.7', 'loss/train': '0.65742', 'examples_per_second': '31.589', 'grad_norm': '20', 'counters/examples': 187712, 'counters/updates': 5866}
train stats after 187744 examples: {'rewards_train/chosen': '0.15671', 'rewards_train/rejected': '0.039097', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11761', 'logps_train/rejected': '-114.12', 'logps_train/chosen': '-132.32', 'loss/train': '0.64888', 'examples_per_second': '30.71', 'grad_norm': '19.125', 'counters/examples': 187744, 'counters/updates': 5867}
train stats after 187776 examples: {'rewards_train/chosen': '0.29978', 'rewards_train/rejected': '0.078539', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22124', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-135.06', 'loss/train': '0.60968', 'examples_per_second': '30.392', 'grad_norm': '20.375', 'counters/examples': 187776, 'counters/updates': 5868}
train stats after 187808 examples: {'rewards_train/chosen': '0.16026', 'rewards_train/rejected': '-0.014363', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17462', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-130.21', 'loss/train': '0.62471', 'examples_per_second': '31.213', 'grad_norm': '19.25', 'counters/examples': 187808, 'counters/updates': 5869}
train stats after 187840 examples: {'rewards_train/chosen': '0.17774', 'rewards_train/rejected': '0.030209', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14753', 'logps_train/rejected': '-96.376', 'logps_train/chosen': '-129.08', 'loss/train': '0.63289', 'examples_per_second': '30.211', 'grad_norm': '18.125', 'counters/examples': 187840, 'counters/updates': 5870}
train stats after 187872 examples: {'rewards_train/chosen': '0.1283', 'rewards_train/rejected': '0.10659', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021705', 'logps_train/rejected': '-108.11', 'logps_train/chosen': '-167.24', 'loss/train': '0.69665', 'examples_per_second': '31.521', 'grad_norm': '22.125', 'counters/examples': 187872, 'counters/updates': 5871}
train stats after 187904 examples: {'rewards_train/chosen': '0.14305', 'rewards_train/rejected': '0.10455', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038498', 'logps_train/rejected': '-146.38', 'logps_train/chosen': '-139.56', 'loss/train': '0.69198', 'examples_per_second': '31.019', 'grad_norm': '21.625', 'counters/examples': 187904, 'counters/updates': 5872}
train stats after 187936 examples: {'rewards_train/chosen': '0.18372', 'rewards_train/rejected': '-0.026016', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20973', 'logps_train/rejected': '-118.84', 'logps_train/chosen': '-134.4', 'loss/train': '0.60692', 'examples_per_second': '29.989', 'grad_norm': '18.375', 'counters/examples': 187936, 'counters/updates': 5873}
train stats after 187968 examples: {'rewards_train/chosen': '0.17582', 'rewards_train/rejected': '0.049099', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12672', 'logps_train/rejected': '-139.15', 'logps_train/chosen': '-132.98', 'loss/train': '0.65184', 'examples_per_second': '32.655', 'grad_norm': '20.5', 'counters/examples': 187968, 'counters/updates': 5874}
train stats after 188000 examples: {'rewards_train/chosen': '0.3451', 'rewards_train/rejected': '0.06882', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27628', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-129.33', 'loss/train': '0.57962', 'examples_per_second': '31.534', 'grad_norm': '17.625', 'counters/examples': 188000, 'counters/updates': 5875}
train stats after 188032 examples: {'rewards_train/chosen': '0.21768', 'rewards_train/rejected': '0.098857', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11883', 'logps_train/rejected': '-146.61', 'logps_train/chosen': '-169.25', 'loss/train': '0.64632', 'examples_per_second': '30.611', 'grad_norm': '23', 'counters/examples': 188032, 'counters/updates': 5876}
skipping logging after 188064 examples to avoid logging too frequently
train stats after 188096 examples: {'rewards_train/chosen': '0.1631', 'rewards_train/rejected': '0.11281', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050282', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-109.19', 'loss/train': '0.69312', 'examples_per_second': '38.153', 'grad_norm': '21.625', 'counters/examples': 188096, 'counters/updates': 5878}
train stats after 188128 examples: {'rewards_train/chosen': '0.14161', 'rewards_train/rejected': '0.038317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10329', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-165.17', 'loss/train': '0.65827', 'examples_per_second': '31.309', 'grad_norm': '23', 'counters/examples': 188128, 'counters/updates': 5879}
train stats after 188160 examples: {'rewards_train/chosen': '0.11214', 'rewards_train/rejected': '-0.030865', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.143', 'logps_train/rejected': '-99.376', 'logps_train/chosen': '-128.18', 'loss/train': '0.63435', 'examples_per_second': '30.17', 'grad_norm': '18.5', 'counters/examples': 188160, 'counters/updates': 5880}
train stats after 188192 examples: {'rewards_train/chosen': '0.21937', 'rewards_train/rejected': '0.038565', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1808', 'logps_train/rejected': '-100.55', 'logps_train/chosen': '-133.08', 'loss/train': '0.61738', 'examples_per_second': '24.536', 'grad_norm': '19.5', 'counters/examples': 188192, 'counters/updates': 5881}
skipping logging after 188224 examples to avoid logging too frequently
train stats after 188256 examples: {'rewards_train/chosen': '0.15327', 'rewards_train/rejected': '0.063996', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08927', 'logps_train/rejected': '-108.98', 'logps_train/chosen': '-112.48', 'loss/train': '0.6655', 'examples_per_second': '31.168', 'grad_norm': '18.625', 'counters/examples': 188256, 'counters/updates': 5883}
skipping logging after 188288 examples to avoid logging too frequently
train stats after 188320 examples: {'rewards_train/chosen': '0.17343', 'rewards_train/rejected': '0.023259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15018', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-148.44', 'loss/train': '0.64054', 'examples_per_second': '31.507', 'grad_norm': '21.375', 'counters/examples': 188320, 'counters/updates': 5885}
skipping logging after 188352 examples to avoid logging too frequently
train stats after 188384 examples: {'rewards_train/chosen': '0.18375', 'rewards_train/rejected': '0.15934', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024414', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-119.82', 'loss/train': '0.6991', 'examples_per_second': '31.309', 'grad_norm': '20.5', 'counters/examples': 188384, 'counters/updates': 5887}
train stats after 188416 examples: {'rewards_train/chosen': '0.10736', 'rewards_train/rejected': '0.10871', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.001347', 'logps_train/rejected': '-112.03', 'logps_train/chosen': '-100.29', 'loss/train': '0.70082', 'examples_per_second': '31.292', 'grad_norm': '18.875', 'counters/examples': 188416, 'counters/updates': 5888}
train stats after 188448 examples: {'rewards_train/chosen': '0.13312', 'rewards_train/rejected': '0.066442', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066674', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-99.179', 'loss/train': '0.66937', 'examples_per_second': '31.574', 'grad_norm': '18', 'counters/examples': 188448, 'counters/updates': 5889}
train stats after 188480 examples: {'rewards_train/chosen': '0.1562', 'rewards_train/rejected': '0.01315', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14305', 'logps_train/rejected': '-120.72', 'logps_train/chosen': '-142.17', 'loss/train': '0.64785', 'examples_per_second': '30.629', 'grad_norm': '20.25', 'counters/examples': 188480, 'counters/updates': 5890}
train stats after 188512 examples: {'rewards_train/chosen': '0.22002', 'rewards_train/rejected': '0.059445', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16058', 'logps_train/rejected': '-147.86', 'logps_train/chosen': '-165.78', 'loss/train': '0.63198', 'examples_per_second': '31.56', 'grad_norm': '20.875', 'counters/examples': 188512, 'counters/updates': 5891}
train stats after 188544 examples: {'rewards_train/chosen': '0.16444', 'rewards_train/rejected': '-0.0019994', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16644', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-149.04', 'loss/train': '0.62427', 'examples_per_second': '31.497', 'grad_norm': '19.875', 'counters/examples': 188544, 'counters/updates': 5892}
skipping logging after 188576 examples to avoid logging too frequently
train stats after 188608 examples: {'rewards_train/chosen': '0.19468', 'rewards_train/rejected': '0.097863', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096817', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-134.15', 'loss/train': '0.65559', 'examples_per_second': '31.124', 'grad_norm': '20.375', 'counters/examples': 188608, 'counters/updates': 5894}
train stats after 188640 examples: {'rewards_train/chosen': '0.18174', 'rewards_train/rejected': '0.1215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060238', 'logps_train/rejected': '-113.68', 'logps_train/chosen': '-124.67', 'loss/train': '0.67258', 'examples_per_second': '31.408', 'grad_norm': '20.375', 'counters/examples': 188640, 'counters/updates': 5895}
train stats after 188672 examples: {'rewards_train/chosen': '0.097457', 'rewards_train/rejected': '-0.0033176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10077', 'logps_train/rejected': '-101.15', 'logps_train/chosen': '-142.83', 'loss/train': '0.65526', 'examples_per_second': '32.258', 'grad_norm': '19.25', 'counters/examples': 188672, 'counters/updates': 5896}
train stats after 188704 examples: {'rewards_train/chosen': '0.15933', 'rewards_train/rejected': '0.04722', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11211', 'logps_train/rejected': '-103.42', 'logps_train/chosen': '-134.93', 'loss/train': '0.64824', 'examples_per_second': '30.081', 'grad_norm': '19.25', 'counters/examples': 188704, 'counters/updates': 5897}
train stats after 188736 examples: {'rewards_train/chosen': '0.22434', 'rewards_train/rejected': '0.062408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16193', 'logps_train/rejected': '-108.33', 'logps_train/chosen': '-109.26', 'loss/train': '0.63513', 'examples_per_second': '30.85', 'grad_norm': '18.125', 'counters/examples': 188736, 'counters/updates': 5898}
skipping logging after 188768 examples to avoid logging too frequently
train stats after 188800 examples: {'rewards_train/chosen': '0.21024', 'rewards_train/rejected': '0.056503', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15374', 'logps_train/rejected': '-94.565', 'logps_train/chosen': '-136.68', 'loss/train': '0.63154', 'examples_per_second': '34.594', 'grad_norm': '20.25', 'counters/examples': 188800, 'counters/updates': 5900}
skipping logging after 188832 examples to avoid logging too frequently
train stats after 188864 examples: {'rewards_train/chosen': '0.2499', 'rewards_train/rejected': '-0.030828', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28073', 'logps_train/rejected': '-86.611', 'logps_train/chosen': '-141.97', 'loss/train': '0.57673', 'examples_per_second': '32.462', 'grad_norm': '18', 'counters/examples': 188864, 'counters/updates': 5902}
train stats after 188896 examples: {'rewards_train/chosen': '0.12182', 'rewards_train/rejected': '-0.014872', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13669', 'logps_train/rejected': '-86.447', 'logps_train/chosen': '-113.36', 'loss/train': '0.63569', 'examples_per_second': '31.869', 'grad_norm': '17', 'counters/examples': 188896, 'counters/updates': 5903}
train stats after 188928 examples: {'rewards_train/chosen': '0.23609', 'rewards_train/rejected': '0.081401', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15468', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-167.63', 'loss/train': '0.64886', 'examples_per_second': '32.328', 'grad_norm': '22.375', 'counters/examples': 188928, 'counters/updates': 5904}
train stats after 188960 examples: {'rewards_train/chosen': '0.18061', 'rewards_train/rejected': '0.061795', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11882', 'logps_train/rejected': '-115.43', 'logps_train/chosen': '-121.06', 'loss/train': '0.64777', 'examples_per_second': '30.204', 'grad_norm': '19.25', 'counters/examples': 188960, 'counters/updates': 5905}
train stats after 188992 examples: {'rewards_train/chosen': '0.12921', 'rewards_train/rejected': '0.044092', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085122', 'logps_train/rejected': '-127.61', 'logps_train/chosen': '-176.85', 'loss/train': '0.66721', 'examples_per_second': '30.071', 'grad_norm': '22.5', 'counters/examples': 188992, 'counters/updates': 5906}
train stats after 189024 examples: {'rewards_train/chosen': '0.13974', 'rewards_train/rejected': '0.040234', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099508', 'logps_train/rejected': '-146.05', 'logps_train/chosen': '-154.97', 'loss/train': '0.66061', 'examples_per_second': '30.226', 'grad_norm': '23.125', 'counters/examples': 189024, 'counters/updates': 5907}
train stats after 189056 examples: {'rewards_train/chosen': '0.14819', 'rewards_train/rejected': '0.099096', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049092', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-124.94', 'loss/train': '0.682', 'examples_per_second': '30.084', 'grad_norm': '20.75', 'counters/examples': 189056, 'counters/updates': 5908}
train stats after 189088 examples: {'rewards_train/chosen': '0.19974', 'rewards_train/rejected': '0.0071416', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19259', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-144.6', 'loss/train': '0.61679', 'examples_per_second': '32.612', 'grad_norm': '19.25', 'counters/examples': 189088, 'counters/updates': 5909}
train stats after 189120 examples: {'rewards_train/chosen': '0.15151', 'rewards_train/rejected': '0.050967', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10054', 'logps_train/rejected': '-89.357', 'logps_train/chosen': '-85.29', 'loss/train': '0.65889', 'examples_per_second': '31.928', 'grad_norm': '15.875', 'counters/examples': 189120, 'counters/updates': 5910}
train stats after 189152 examples: {'rewards_train/chosen': '0.13429', 'rewards_train/rejected': '0.026645', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10764', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-130.44', 'loss/train': '0.65609', 'examples_per_second': '31.379', 'grad_norm': '19.5', 'counters/examples': 189152, 'counters/updates': 5911}
train stats after 189184 examples: {'rewards_train/chosen': '0.1368', 'rewards_train/rejected': '0.03676', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10004', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-113.77', 'loss/train': '0.65594', 'examples_per_second': '31.288', 'grad_norm': '18.5', 'counters/examples': 189184, 'counters/updates': 5912}
train stats after 189216 examples: {'rewards_train/chosen': '0.18522', 'rewards_train/rejected': '-0.031397', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21662', 'logps_train/rejected': '-106.86', 'logps_train/chosen': '-155.51', 'loss/train': '0.60253', 'examples_per_second': '31.524', 'grad_norm': '18.875', 'counters/examples': 189216, 'counters/updates': 5913}
train stats after 189248 examples: {'rewards_train/chosen': '0.20935', 'rewards_train/rejected': '0.079619', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12973', 'logps_train/rejected': '-131.01', 'logps_train/chosen': '-129.2', 'loss/train': '0.6496', 'examples_per_second': '32.286', 'grad_norm': '20.625', 'counters/examples': 189248, 'counters/updates': 5914}
train stats after 189280 examples: {'rewards_train/chosen': '0.16793', 'rewards_train/rejected': '0.06696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10097', 'logps_train/rejected': '-134.86', 'logps_train/chosen': '-137.59', 'loss/train': '0.65514', 'examples_per_second': '31.524', 'grad_norm': '22.125', 'counters/examples': 189280, 'counters/updates': 5915}
train stats after 189312 examples: {'rewards_train/chosen': '0.25636', 'rewards_train/rejected': '0.06783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18853', 'logps_train/rejected': '-173.76', 'logps_train/chosen': '-120.67', 'loss/train': '0.62422', 'examples_per_second': '31.448', 'grad_norm': '22.125', 'counters/examples': 189312, 'counters/updates': 5916}
skipping logging after 189344 examples to avoid logging too frequently
train stats after 189376 examples: {'rewards_train/chosen': '0.22595', 'rewards_train/rejected': '0.040567', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18539', 'logps_train/rejected': '-101.35', 'logps_train/chosen': '-127.66', 'loss/train': '0.62492', 'examples_per_second': '35.638', 'grad_norm': '18.125', 'counters/examples': 189376, 'counters/updates': 5918}
skipping logging after 189408 examples to avoid logging too frequently
train stats after 189440 examples: {'rewards_train/chosen': '0.12035', 'rewards_train/rejected': '-0.0091958', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12955', 'logps_train/rejected': '-95.67', 'logps_train/chosen': '-126.93', 'loss/train': '0.64665', 'examples_per_second': '36.138', 'grad_norm': '18.875', 'counters/examples': 189440, 'counters/updates': 5920}
train stats after 189472 examples: {'rewards_train/chosen': '0.15058', 'rewards_train/rejected': '0.089161', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061423', 'logps_train/rejected': '-122.42', 'logps_train/chosen': '-162.15', 'loss/train': '0.67322', 'examples_per_second': '31.455', 'grad_norm': '21.5', 'counters/examples': 189472, 'counters/updates': 5921}
train stats after 189504 examples: {'rewards_train/chosen': '0.19837', 'rewards_train/rejected': '0.038007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16036', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-126.51', 'loss/train': '0.62857', 'examples_per_second': '31.638', 'grad_norm': '19.875', 'counters/examples': 189504, 'counters/updates': 5922}
skipping logging after 189536 examples to avoid logging too frequently
train stats after 189568 examples: {'rewards_train/chosen': '0.22178', 'rewards_train/rejected': '0.093112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12867', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-154.3', 'loss/train': '0.64258', 'examples_per_second': '33.365', 'grad_norm': '20.875', 'counters/examples': 189568, 'counters/updates': 5924}
train stats after 189600 examples: {'rewards_train/chosen': '0.27699', 'rewards_train/rejected': '0.092364', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18463', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-191.22', 'loss/train': '0.62146', 'examples_per_second': '30.238', 'grad_norm': '22.25', 'counters/examples': 189600, 'counters/updates': 5925}
train stats after 189632 examples: {'rewards_train/chosen': '0.15557', 'rewards_train/rejected': '-0.085749', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.24132', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-145.33', 'loss/train': '0.58903', 'examples_per_second': '32.008', 'grad_norm': '19.125', 'counters/examples': 189632, 'counters/updates': 5926}
train stats after 189664 examples: {'rewards_train/chosen': '0.23659', 'rewards_train/rejected': '0.028163', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20842', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-132.44', 'loss/train': '0.60597', 'examples_per_second': '31.111', 'grad_norm': '19.25', 'counters/examples': 189664, 'counters/updates': 5927}
train stats after 189696 examples: {'rewards_train/chosen': '0.22015', 'rewards_train/rejected': '0.064891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15526', 'logps_train/rejected': '-138.06', 'logps_train/chosen': '-152.38', 'loss/train': '0.64191', 'examples_per_second': '30.97', 'grad_norm': '22.125', 'counters/examples': 189696, 'counters/updates': 5928}
train stats after 189728 examples: {'rewards_train/chosen': '0.14738', 'rewards_train/rejected': '0.0025867', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1448', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-121.82', 'loss/train': '0.63746', 'examples_per_second': '30.084', 'grad_norm': '18.875', 'counters/examples': 189728, 'counters/updates': 5929}
train stats after 189760 examples: {'rewards_train/chosen': '0.11837', 'rewards_train/rejected': '-0.052851', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17122', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-112.56', 'loss/train': '0.62302', 'examples_per_second': '30.539', 'grad_norm': '18.375', 'counters/examples': 189760, 'counters/updates': 5930}
train stats after 189792 examples: {'rewards_train/chosen': '0.10979', 'rewards_train/rejected': '0.07525', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034542', 'logps_train/rejected': '-115.12', 'logps_train/chosen': '-105.9', 'loss/train': '0.68997', 'examples_per_second': '31.025', 'grad_norm': '19.25', 'counters/examples': 189792, 'counters/updates': 5931}
train stats after 189824 examples: {'rewards_train/chosen': '0.15811', 'rewards_train/rejected': '-0.012888', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.171', 'logps_train/rejected': '-108.67', 'logps_train/chosen': '-153.56', 'loss/train': '0.62426', 'examples_per_second': '32.008', 'grad_norm': '19.75', 'counters/examples': 189824, 'counters/updates': 5932}
train stats after 189856 examples: {'rewards_train/chosen': '0.099444', 'rewards_train/rejected': '0.078623', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020821', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-109', 'loss/train': '0.69703', 'examples_per_second': '30.2', 'grad_norm': '22.125', 'counters/examples': 189856, 'counters/updates': 5933}
train stats after 189888 examples: {'rewards_train/chosen': '0.15208', 'rewards_train/rejected': '0.038784', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1133', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-131.52', 'loss/train': '0.64903', 'examples_per_second': '31.477', 'grad_norm': '18.25', 'counters/examples': 189888, 'counters/updates': 5934}
train stats after 189920 examples: {'rewards_train/chosen': '0.15159', 'rewards_train/rejected': '0.040876', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11072', 'logps_train/rejected': '-155.73', 'logps_train/chosen': '-143.48', 'loss/train': '0.65898', 'examples_per_second': '32.02', 'grad_norm': '22.875', 'counters/examples': 189920, 'counters/updates': 5935}
train stats after 189952 examples: {'rewards_train/chosen': '0.17267', 'rewards_train/rejected': '0.076398', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096276', 'logps_train/rejected': '-126.17', 'logps_train/chosen': '-161.31', 'loss/train': '0.65781', 'examples_per_second': '31.306', 'grad_norm': '21.125', 'counters/examples': 189952, 'counters/updates': 5936}
train stats after 189984 examples: {'rewards_train/chosen': '0.10906', 'rewards_train/rejected': '0.071821', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03724', 'logps_train/rejected': '-125.44', 'logps_train/chosen': '-144.54', 'loss/train': '0.6951', 'examples_per_second': '31.529', 'grad_norm': '21', 'counters/examples': 189984, 'counters/updates': 5937}
train stats after 190016 examples: {'rewards_train/chosen': '0.18176', 'rewards_train/rejected': '-0.0044705', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18623', 'logps_train/rejected': '-102.75', 'logps_train/chosen': '-156.88', 'loss/train': '0.61409', 'examples_per_second': '31.486', 'grad_norm': '20.375', 'counters/examples': 190016, 'counters/updates': 5938}
train stats after 190048 examples: {'rewards_train/chosen': '0.18006', 'rewards_train/rejected': '0.067862', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1122', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-112.99', 'loss/train': '0.64695', 'examples_per_second': '31.209', 'grad_norm': '19.125', 'counters/examples': 190048, 'counters/updates': 5939}
train stats after 190080 examples: {'rewards_train/chosen': '0.18781', 'rewards_train/rejected': '0.072321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11549', 'logps_train/rejected': '-162.14', 'logps_train/chosen': '-170.35', 'loss/train': '0.64638', 'examples_per_second': '31.686', 'grad_norm': '23.875', 'counters/examples': 190080, 'counters/updates': 5940}
train stats after 190112 examples: {'rewards_train/chosen': '0.14986', 'rewards_train/rejected': '0.11636', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0335', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-119.78', 'loss/train': '0.68976', 'examples_per_second': '30.506', 'grad_norm': '22.125', 'counters/examples': 190112, 'counters/updates': 5941}
train stats after 190144 examples: {'rewards_train/chosen': '0.11483', 'rewards_train/rejected': '0.059164', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055666', 'logps_train/rejected': '-100.12', 'logps_train/chosen': '-126.78', 'loss/train': '0.67875', 'examples_per_second': '32.949', 'grad_norm': '19.375', 'counters/examples': 190144, 'counters/updates': 5942}
skipping logging after 190176 examples to avoid logging too frequently
train stats after 190208 examples: {'rewards_train/chosen': '0.20597', 'rewards_train/rejected': '0.087232', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11874', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-128.43', 'loss/train': '0.64741', 'examples_per_second': '29.894', 'grad_norm': '19.875', 'counters/examples': 190208, 'counters/updates': 5944}
train stats after 190240 examples: {'rewards_train/chosen': '0.17305', 'rewards_train/rejected': '0.056485', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11657', 'logps_train/rejected': '-115.3', 'logps_train/chosen': '-134.24', 'loss/train': '0.65538', 'examples_per_second': '24.263', 'grad_norm': '20.375', 'counters/examples': 190240, 'counters/updates': 5945}
train stats after 190272 examples: {'rewards_train/chosen': '0.12194', 'rewards_train/rejected': '-0.070043', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19198', 'logps_train/rejected': '-101.36', 'logps_train/chosen': '-114.98', 'loss/train': '0.61592', 'examples_per_second': '31.104', 'grad_norm': '18.25', 'counters/examples': 190272, 'counters/updates': 5946}
train stats after 190304 examples: {'rewards_train/chosen': '0.11052', 'rewards_train/rejected': '0.046233', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064284', 'logps_train/rejected': '-148.17', 'logps_train/chosen': '-148.44', 'loss/train': '0.67384', 'examples_per_second': '31.186', 'grad_norm': '22.625', 'counters/examples': 190304, 'counters/updates': 5947}
train stats after 190336 examples: {'rewards_train/chosen': '0.15842', 'rewards_train/rejected': '0.046909', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11151', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-133.85', 'loss/train': '0.66601', 'examples_per_second': '23.996', 'grad_norm': '21', 'counters/examples': 190336, 'counters/updates': 5948}
train stats after 190368 examples: {'rewards_train/chosen': '0.18526', 'rewards_train/rejected': '0.028628', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15663', 'logps_train/rejected': '-103.42', 'logps_train/chosen': '-123.38', 'loss/train': '0.63459', 'examples_per_second': '32.483', 'grad_norm': '17.5', 'counters/examples': 190368, 'counters/updates': 5949}
train stats after 190400 examples: {'rewards_train/chosen': '0.12711', 'rewards_train/rejected': '0.042436', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084678', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-166.96', 'loss/train': '0.66015', 'examples_per_second': '31.193', 'grad_norm': '21.875', 'counters/examples': 190400, 'counters/updates': 5950}
train stats after 190432 examples: {'rewards_train/chosen': '0.10132', 'rewards_train/rejected': '-0.017853', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11917', 'logps_train/rejected': '-103.4', 'logps_train/chosen': '-116.42', 'loss/train': '0.64104', 'examples_per_second': '32.893', 'grad_norm': '17.875', 'counters/examples': 190432, 'counters/updates': 5951}
train stats after 190464 examples: {'rewards_train/chosen': '0.20909', 'rewards_train/rejected': '0.0069065', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20219', 'logps_train/rejected': '-97.155', 'logps_train/chosen': '-166.13', 'loss/train': '0.61734', 'examples_per_second': '30.465', 'grad_norm': '19.625', 'counters/examples': 190464, 'counters/updates': 5952}
train stats after 190496 examples: {'rewards_train/chosen': '0.13777', 'rewards_train/rejected': '0.059235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078539', 'logps_train/rejected': '-112.49', 'logps_train/chosen': '-121.04', 'loss/train': '0.66405', 'examples_per_second': '29.841', 'grad_norm': '19.75', 'counters/examples': 190496, 'counters/updates': 5953}
train stats after 190528 examples: {'rewards_train/chosen': '0.21206', 'rewards_train/rejected': '0.12465', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08741', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-109.04', 'loss/train': '0.6633', 'examples_per_second': '32.468', 'grad_norm': '18.875', 'counters/examples': 190528, 'counters/updates': 5954}
train stats after 190560 examples: {'rewards_train/chosen': '0.083951', 'rewards_train/rejected': '-0.0066655', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090617', 'logps_train/rejected': '-170.67', 'logps_train/chosen': '-144', 'loss/train': '0.66249', 'examples_per_second': '30.546', 'grad_norm': '22.625', 'counters/examples': 190560, 'counters/updates': 5955}
train stats after 190592 examples: {'rewards_train/chosen': '0.19378', 'rewards_train/rejected': '-0.016899', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21068', 'logps_train/rejected': '-92.972', 'logps_train/chosen': '-128.32', 'loss/train': '0.61082', 'examples_per_second': '31.956', 'grad_norm': '18.25', 'counters/examples': 190592, 'counters/updates': 5956}
train stats after 190624 examples: {'rewards_train/chosen': '0.14526', 'rewards_train/rejected': '0.047104', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098159', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-166.03', 'loss/train': '0.66003', 'examples_per_second': '31.785', 'grad_norm': '22.875', 'counters/examples': 190624, 'counters/updates': 5957}
train stats after 190656 examples: {'rewards_train/chosen': '0.1924', 'rewards_train/rejected': '0.012172', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18023', 'logps_train/rejected': '-112.49', 'logps_train/chosen': '-159.98', 'loss/train': '0.61961', 'examples_per_second': '32.645', 'grad_norm': '20.125', 'counters/examples': 190656, 'counters/updates': 5958}
train stats after 190688 examples: {'rewards_train/chosen': '0.17385', 'rewards_train/rejected': '-0.0028261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17668', 'logps_train/rejected': '-128.07', 'logps_train/chosen': '-130.56', 'loss/train': '0.62511', 'examples_per_second': '31.185', 'grad_norm': '19.125', 'counters/examples': 190688, 'counters/updates': 5959}
train stats after 190720 examples: {'rewards_train/chosen': '0.14914', 'rewards_train/rejected': '0.012933', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13621', 'logps_train/rejected': '-93.784', 'logps_train/chosen': '-149.91', 'loss/train': '0.63928', 'examples_per_second': '30.512', 'grad_norm': '19.375', 'counters/examples': 190720, 'counters/updates': 5960}
train stats after 190752 examples: {'rewards_train/chosen': '0.1614', 'rewards_train/rejected': '0.054491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10691', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-179.01', 'loss/train': '0.65455', 'examples_per_second': '31.499', 'grad_norm': '22.5', 'counters/examples': 190752, 'counters/updates': 5961}
train stats after 190784 examples: {'rewards_train/chosen': '0.19554', 'rewards_train/rejected': '0.063016', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13252', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-175.07', 'loss/train': '0.64156', 'examples_per_second': '31.008', 'grad_norm': '23.125', 'counters/examples': 190784, 'counters/updates': 5962}
train stats after 190816 examples: {'rewards_train/chosen': '0.13127', 'rewards_train/rejected': '0.027836', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10344', 'logps_train/rejected': '-135.22', 'logps_train/chosen': '-149.22', 'loss/train': '0.65884', 'examples_per_second': '31.647', 'grad_norm': '21', 'counters/examples': 190816, 'counters/updates': 5963}
train stats after 190848 examples: {'rewards_train/chosen': '0.12689', 'rewards_train/rejected': '0.099351', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027539', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-140.28', 'loss/train': '0.69419', 'examples_per_second': '32.002', 'grad_norm': '20.75', 'counters/examples': 190848, 'counters/updates': 5964}
train stats after 190880 examples: {'rewards_train/chosen': '0.25679', 'rewards_train/rejected': '0.010853', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24593', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-163.81', 'loss/train': '0.59697', 'examples_per_second': '32.952', 'grad_norm': '20.375', 'counters/examples': 190880, 'counters/updates': 5965}
train stats after 190912 examples: {'rewards_train/chosen': '0.17279', 'rewards_train/rejected': '0.091645', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081144', 'logps_train/rejected': '-112.28', 'logps_train/chosen': '-114.1', 'loss/train': '0.66402', 'examples_per_second': '30.543', 'grad_norm': '18.375', 'counters/examples': 190912, 'counters/updates': 5966}
skipping logging after 190944 examples to avoid logging too frequently
train stats after 190976 examples: {'rewards_train/chosen': '0.16936', 'rewards_train/rejected': '0.040036', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12932', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-126.01', 'loss/train': '0.63968', 'examples_per_second': '31.406', 'grad_norm': '18.25', 'counters/examples': 190976, 'counters/updates': 5968}
train stats after 191008 examples: {'rewards_train/chosen': '0.10898', 'rewards_train/rejected': '0.008984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-185.77', 'loss/train': '0.66491', 'examples_per_second': '30.668', 'grad_norm': '21.875', 'counters/examples': 191008, 'counters/updates': 5969}
train stats after 191040 examples: {'rewards_train/chosen': '0.15456', 'rewards_train/rejected': '0.047553', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.107', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-152.75', 'loss/train': '0.65624', 'examples_per_second': '31.528', 'grad_norm': '21.25', 'counters/examples': 191040, 'counters/updates': 5970}
train stats after 191072 examples: {'rewards_train/chosen': '0.12806', 'rewards_train/rejected': '0.066814', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06125', 'logps_train/rejected': '-113.73', 'logps_train/chosen': '-90.325', 'loss/train': '0.67482', 'examples_per_second': '31.68', 'grad_norm': '17.5', 'counters/examples': 191072, 'counters/updates': 5971}
train stats after 191104 examples: {'rewards_train/chosen': '0.16541', 'rewards_train/rejected': '-0.019815', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18522', 'logps_train/rejected': '-127.69', 'logps_train/chosen': '-140.32', 'loss/train': '0.63088', 'examples_per_second': '30.174', 'grad_norm': '19.75', 'counters/examples': 191104, 'counters/updates': 5972}
train stats after 191136 examples: {'rewards_train/chosen': '0.13292', 'rewards_train/rejected': '-0.066107', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19902', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-137.93', 'loss/train': '0.61881', 'examples_per_second': '31.493', 'grad_norm': '19.75', 'counters/examples': 191136, 'counters/updates': 5973}
skipping logging after 191168 examples to avoid logging too frequently
train stats after 191200 examples: {'rewards_train/chosen': '0.10726', 'rewards_train/rejected': '0.060233', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047024', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-136.52', 'loss/train': '0.68132', 'examples_per_second': '31.458', 'grad_norm': '21.625', 'counters/examples': 191200, 'counters/updates': 5975}
train stats after 191232 examples: {'rewards_train/chosen': '0.21296', 'rewards_train/rejected': '0.083218', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12974', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-169.04', 'loss/train': '0.63868', 'examples_per_second': '29.987', 'grad_norm': '20.375', 'counters/examples': 191232, 'counters/updates': 5976}
train stats after 191264 examples: {'rewards_train/chosen': '0.20671', 'rewards_train/rejected': '0.084749', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12197', 'logps_train/rejected': '-134.93', 'logps_train/chosen': '-148.74', 'loss/train': '0.64967', 'examples_per_second': '30.317', 'grad_norm': '21', 'counters/examples': 191264, 'counters/updates': 5977}
train stats after 191296 examples: {'rewards_train/chosen': '0.26125', 'rewards_train/rejected': '0.067027', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19422', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-136.91', 'loss/train': '0.62372', 'examples_per_second': '31.192', 'grad_norm': '19', 'counters/examples': 191296, 'counters/updates': 5978}
train stats after 191328 examples: {'rewards_train/chosen': '0.17539', 'rewards_train/rejected': '0.028171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14721', 'logps_train/rejected': '-144.35', 'logps_train/chosen': '-130.91', 'loss/train': '0.64953', 'examples_per_second': '30.622', 'grad_norm': '22.25', 'counters/examples': 191328, 'counters/updates': 5979}
train stats after 191360 examples: {'rewards_train/chosen': '0.11606', 'rewards_train/rejected': '0.025226', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090835', 'logps_train/rejected': '-122.24', 'logps_train/chosen': '-153.27', 'loss/train': '0.65504', 'examples_per_second': '31.643', 'grad_norm': '20.25', 'counters/examples': 191360, 'counters/updates': 5980}
train stats after 191392 examples: {'rewards_train/chosen': '0.26435', 'rewards_train/rejected': '0.0503', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21406', 'logps_train/rejected': '-102.25', 'logps_train/chosen': '-141.87', 'loss/train': '0.60706', 'examples_per_second': '31.497', 'grad_norm': '19', 'counters/examples': 191392, 'counters/updates': 5981}
train stats after 191424 examples: {'rewards_train/chosen': '0.24857', 'rewards_train/rejected': '0.06747', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1811', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-136.33', 'loss/train': '0.62885', 'examples_per_second': '30.481', 'grad_norm': '18.875', 'counters/examples': 191424, 'counters/updates': 5982}
train stats after 191456 examples: {'rewards_train/chosen': '0.16678', 'rewards_train/rejected': '-0.027084', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19387', 'logps_train/rejected': '-120.25', 'logps_train/chosen': '-137.56', 'loss/train': '0.60938', 'examples_per_second': '30.012', 'grad_norm': '18.625', 'counters/examples': 191456, 'counters/updates': 5983}
train stats after 191488 examples: {'rewards_train/chosen': '0.085471', 'rewards_train/rejected': '0.057039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028432', 'logps_train/rejected': '-116.76', 'logps_train/chosen': '-123.83', 'loss/train': '0.68983', 'examples_per_second': '30.076', 'grad_norm': '20.25', 'counters/examples': 191488, 'counters/updates': 5984}
train stats after 191520 examples: {'rewards_train/chosen': '0.23077', 'rewards_train/rejected': '0.030519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20025', 'logps_train/rejected': '-142.34', 'logps_train/chosen': '-157.54', 'loss/train': '0.61608', 'examples_per_second': '31.514', 'grad_norm': '19.375', 'counters/examples': 191520, 'counters/updates': 5985}
train stats after 191552 examples: {'rewards_train/chosen': '0.1371', 'rewards_train/rejected': '0.098838', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038264', 'logps_train/rejected': '-113.31', 'logps_train/chosen': '-129.12', 'loss/train': '0.68268', 'examples_per_second': '31.562', 'grad_norm': '20.375', 'counters/examples': 191552, 'counters/updates': 5986}
train stats after 191584 examples: {'rewards_train/chosen': '0.18077', 'rewards_train/rejected': '0.078723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10205', 'logps_train/rejected': '-145.09', 'logps_train/chosen': '-151.19', 'loss/train': '0.65875', 'examples_per_second': '31.303', 'grad_norm': '21.25', 'counters/examples': 191584, 'counters/updates': 5987}
train stats after 191616 examples: {'rewards_train/chosen': '0.23979', 'rewards_train/rejected': '0.11937', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12042', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-148.83', 'loss/train': '0.65444', 'examples_per_second': '30.725', 'grad_norm': '21.75', 'counters/examples': 191616, 'counters/updates': 5988}
train stats after 191648 examples: {'rewards_train/chosen': '0.11193', 'rewards_train/rejected': '0.010579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10135', 'logps_train/rejected': '-105.95', 'logps_train/chosen': '-122.27', 'loss/train': '0.65063', 'examples_per_second': '31.081', 'grad_norm': '19.375', 'counters/examples': 191648, 'counters/updates': 5989}
skipping logging after 191680 examples to avoid logging too frequently
train stats after 191712 examples: {'rewards_train/chosen': '0.17849', 'rewards_train/rejected': '0.095117', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083369', 'logps_train/rejected': '-153.26', 'logps_train/chosen': '-170.82', 'loss/train': '0.66438', 'examples_per_second': '31.513', 'grad_norm': '22', 'counters/examples': 191712, 'counters/updates': 5991}
train stats after 191744 examples: {'rewards_train/chosen': '0.20635', 'rewards_train/rejected': '0.019926', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18643', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-150.41', 'loss/train': '0.63041', 'examples_per_second': '30.741', 'grad_norm': '20.75', 'counters/examples': 191744, 'counters/updates': 5992}
skipping logging after 191776 examples to avoid logging too frequently
train stats after 191808 examples: {'rewards_train/chosen': '0.13525', 'rewards_train/rejected': '-0.019775', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15503', 'logps_train/rejected': '-76.314', 'logps_train/chosen': '-107.57', 'loss/train': '0.63499', 'examples_per_second': '31.578', 'grad_norm': '16.25', 'counters/examples': 191808, 'counters/updates': 5994}
skipping logging after 191840 examples to avoid logging too frequently
train stats after 191872 examples: {'rewards_train/chosen': '0.12839', 'rewards_train/rejected': '0.14055', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01216', 'logps_train/rejected': '-152.9', 'logps_train/chosen': '-156.31', 'loss/train': '0.71553', 'examples_per_second': '30.295', 'grad_norm': '23.875', 'counters/examples': 191872, 'counters/updates': 5996}
skipping logging after 191904 examples to avoid logging too frequently
train stats after 191936 examples: {'rewards_train/chosen': '0.24861', 'rewards_train/rejected': '0.031006', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2176', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-162.85', 'loss/train': '0.61908', 'examples_per_second': '31.712', 'grad_norm': '20.125', 'counters/examples': 191936, 'counters/updates': 5998}
train stats after 191968 examples: {'rewards_train/chosen': '0.18007', 'rewards_train/rejected': '0.10431', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075767', 'logps_train/rejected': '-131.44', 'logps_train/chosen': '-137.44', 'loss/train': '0.67095', 'examples_per_second': '30.47', 'grad_norm': '21.25', 'counters/examples': 191968, 'counters/updates': 5999}
train stats after 192000 examples: {'rewards_train/chosen': '0.22513', 'rewards_train/rejected': '0.058611', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16652', 'logps_train/rejected': '-117.5', 'logps_train/chosen': '-145.19', 'loss/train': '0.6378', 'examples_per_second': '31.643', 'grad_norm': '19.25', 'counters/examples': 192000, 'counters/updates': 6000}
Running evaluation after 192000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.90it/s]
eval after 192000: {'rewards_eval/chosen': '0.17146', 'rewards_eval/rejected': '0.049398', 'rewards_eval/accuracies': '0.60156', 'rewards_eval/margins': '0.12206', 'logps_eval/rejected': '-114.9', 'logps_eval/chosen': '-133.83', 'loss/eval': '0.65235'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933/step-192000...
writing checkpoint to .cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933/step-192000/policy.pt...
train stats after 192032 examples: {'rewards_train/chosen': '0.12081', 'rewards_train/rejected': '0.028645', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092166', 'logps_train/rejected': '-106.7', 'logps_train/chosen': '-122.74', 'loss/train': '0.66973', 'examples_per_second': '24.708', 'grad_norm': '19.625', 'counters/examples': 192032, 'counters/updates': 6001}
train stats after 192064 examples: {'rewards_train/chosen': '0.25102', 'rewards_train/rejected': '0.018739', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23228', 'logps_train/rejected': '-127.51', 'logps_train/chosen': '-157.18', 'loss/train': '0.60839', 'examples_per_second': '30.187', 'grad_norm': '18.625', 'counters/examples': 192064, 'counters/updates': 6002}
train stats after 192096 examples: {'rewards_train/chosen': '0.24059', 'rewards_train/rejected': '0.16147', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079124', 'logps_train/rejected': '-137.12', 'logps_train/chosen': '-179.57', 'loss/train': '0.6751', 'examples_per_second': '31.987', 'grad_norm': '23.25', 'counters/examples': 192096, 'counters/updates': 6003}
skipping logging after 192128 examples to avoid logging too frequently
train stats after 192160 examples: {'rewards_train/chosen': '0.22828', 'rewards_train/rejected': '0.066337', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16195', 'logps_train/rejected': '-148.49', 'logps_train/chosen': '-146.09', 'loss/train': '0.63577', 'examples_per_second': '31.695', 'grad_norm': '20.25', 'counters/examples': 192160, 'counters/updates': 6005}
train stats after 192192 examples: {'rewards_train/chosen': '0.20482', 'rewards_train/rejected': '0.10106', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10376', 'logps_train/rejected': '-95.947', 'logps_train/chosen': '-165.87', 'loss/train': '0.653', 'examples_per_second': '31.574', 'grad_norm': '20.625', 'counters/examples': 192192, 'counters/updates': 6006}
skipping logging after 192224 examples to avoid logging too frequently
train stats after 192256 examples: {'rewards_train/chosen': '0.11779', 'rewards_train/rejected': '0.063379', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054412', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-126.95', 'loss/train': '0.68233', 'examples_per_second': '31.454', 'grad_norm': '20', 'counters/examples': 192256, 'counters/updates': 6008}
train stats after 192288 examples: {'rewards_train/chosen': '0.12121', 'rewards_train/rejected': '0.032587', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088626', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-146.13', 'loss/train': '0.66247', 'examples_per_second': '31.27', 'grad_norm': '20.75', 'counters/examples': 192288, 'counters/updates': 6009}
train stats after 192320 examples: {'rewards_train/chosen': '0.144', 'rewards_train/rejected': '-0.021438', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16543', 'logps_train/rejected': '-147.39', 'logps_train/chosen': '-113.35', 'loss/train': '0.62573', 'examples_per_second': '31.701', 'grad_norm': '19.875', 'counters/examples': 192320, 'counters/updates': 6010}
skipping logging after 192352 examples to avoid logging too frequently
train stats after 192384 examples: {'rewards_train/chosen': '0.16394', 'rewards_train/rejected': '0.065315', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098628', 'logps_train/rejected': '-92.054', 'logps_train/chosen': '-120.27', 'loss/train': '0.65719', 'examples_per_second': '30.572', 'grad_norm': '18.25', 'counters/examples': 192384, 'counters/updates': 6012}
train stats after 192416 examples: {'rewards_train/chosen': '0.1284', 'rewards_train/rejected': '-0.039048', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16744', 'logps_train/rejected': '-140.45', 'logps_train/chosen': '-134.11', 'loss/train': '0.62731', 'examples_per_second': '31.112', 'grad_norm': '20.125', 'counters/examples': 192416, 'counters/updates': 6013}
train stats after 192448 examples: {'rewards_train/chosen': '0.1631', 'rewards_train/rejected': '0.086435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076667', 'logps_train/rejected': '-115.14', 'logps_train/chosen': '-120.67', 'loss/train': '0.67169', 'examples_per_second': '32.4', 'grad_norm': '19.875', 'counters/examples': 192448, 'counters/updates': 6014}
train stats after 192480 examples: {'rewards_train/chosen': '0.21446', 'rewards_train/rejected': '0.10837', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10609', 'logps_train/rejected': '-153.64', 'logps_train/chosen': '-156.16', 'loss/train': '0.65075', 'examples_per_second': '30.128', 'grad_norm': '21.125', 'counters/examples': 192480, 'counters/updates': 6015}
train stats after 192512 examples: {'rewards_train/chosen': '0.16977', 'rewards_train/rejected': '0.016571', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1532', 'logps_train/rejected': '-154.44', 'logps_train/chosen': '-123.84', 'loss/train': '0.63664', 'examples_per_second': '30.722', 'grad_norm': '21.375', 'counters/examples': 192512, 'counters/updates': 6016}
train stats after 192544 examples: {'rewards_train/chosen': '0.080716', 'rewards_train/rejected': '-0.028604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10932', 'logps_train/rejected': '-141.6', 'logps_train/chosen': '-161.79', 'loss/train': '0.65127', 'examples_per_second': '32.235', 'grad_norm': '21.75', 'counters/examples': 192544, 'counters/updates': 6017}
train stats after 192576 examples: {'rewards_train/chosen': '0.13814', 'rewards_train/rejected': '0.074381', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063755', 'logps_train/rejected': '-95.862', 'logps_train/chosen': '-125.74', 'loss/train': '0.67298', 'examples_per_second': '30.844', 'grad_norm': '19', 'counters/examples': 192576, 'counters/updates': 6018}
train stats after 192608 examples: {'rewards_train/chosen': '0.26791', 'rewards_train/rejected': '-0.0032374', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27115', 'logps_train/rejected': '-132.36', 'logps_train/chosen': '-152.23', 'loss/train': '0.58767', 'examples_per_second': '31.7', 'grad_norm': '19.875', 'counters/examples': 192608, 'counters/updates': 6019}
train stats after 192640 examples: {'rewards_train/chosen': '0.23093', 'rewards_train/rejected': '0.035038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1959', 'logps_train/rejected': '-162.91', 'logps_train/chosen': '-175.67', 'loss/train': '0.63173', 'examples_per_second': '30.38', 'grad_norm': '23.625', 'counters/examples': 192640, 'counters/updates': 6020}
train stats after 192672 examples: {'rewards_train/chosen': '0.16741', 'rewards_train/rejected': '0.097044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070365', 'logps_train/rejected': '-144.94', 'logps_train/chosen': '-153.57', 'loss/train': '0.67592', 'examples_per_second': '29.974', 'grad_norm': '21.125', 'counters/examples': 192672, 'counters/updates': 6021}
train stats after 192704 examples: {'rewards_train/chosen': '0.089885', 'rewards_train/rejected': '-0.033724', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12361', 'logps_train/rejected': '-87.432', 'logps_train/chosen': '-143.43', 'loss/train': '0.6485', 'examples_per_second': '32.678', 'grad_norm': '18.875', 'counters/examples': 192704, 'counters/updates': 6022}
train stats after 192736 examples: {'rewards_train/chosen': '0.14189', 'rewards_train/rejected': '-0.029329', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17122', 'logps_train/rejected': '-109.43', 'logps_train/chosen': '-130.63', 'loss/train': '0.63286', 'examples_per_second': '31.417', 'grad_norm': '18.75', 'counters/examples': 192736, 'counters/updates': 6023}
train stats after 192768 examples: {'rewards_train/chosen': '0.21589', 'rewards_train/rejected': '0.061901', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15399', 'logps_train/rejected': '-101.28', 'logps_train/chosen': '-149.12', 'loss/train': '0.63749', 'examples_per_second': '31.235', 'grad_norm': '19.75', 'counters/examples': 192768, 'counters/updates': 6024}
train stats after 192800 examples: {'rewards_train/chosen': '0.23225', 'rewards_train/rejected': '0.091942', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14031', 'logps_train/rejected': '-98.403', 'logps_train/chosen': '-131.07', 'loss/train': '0.63683', 'examples_per_second': '31.666', 'grad_norm': '18.125', 'counters/examples': 192800, 'counters/updates': 6025}
train stats after 192832 examples: {'rewards_train/chosen': '0.18389', 'rewards_train/rejected': '-0.0090817', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19298', 'logps_train/rejected': '-156.69', 'logps_train/chosen': '-151.84', 'loss/train': '0.61324', 'examples_per_second': '31.319', 'grad_norm': '20.375', 'counters/examples': 192832, 'counters/updates': 6026}
train stats after 192864 examples: {'rewards_train/chosen': '0.1627', 'rewards_train/rejected': '-0.064113', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22682', 'logps_train/rejected': '-84.94', 'logps_train/chosen': '-142.03', 'loss/train': '0.60166', 'examples_per_second': '32.071', 'grad_norm': '17.75', 'counters/examples': 192864, 'counters/updates': 6027}
train stats after 192896 examples: {'rewards_train/chosen': '0.24942', 'rewards_train/rejected': '0.051566', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19786', 'logps_train/rejected': '-151.32', 'logps_train/chosen': '-188.57', 'loss/train': '0.62237', 'examples_per_second': '30.653', 'grad_norm': '23.375', 'counters/examples': 192896, 'counters/updates': 6028}
train stats after 192928 examples: {'rewards_train/chosen': '0.064091', 'rewards_train/rejected': '-0.0068676', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070958', 'logps_train/rejected': '-101.28', 'logps_train/chosen': '-108.67', 'loss/train': '0.67414', 'examples_per_second': '32.162', 'grad_norm': '18.625', 'counters/examples': 192928, 'counters/updates': 6029}
train stats after 192960 examples: {'rewards_train/chosen': '0.12293', 'rewards_train/rejected': '-0.053548', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17647', 'logps_train/rejected': '-79.963', 'logps_train/chosen': '-103.66', 'loss/train': '0.62357', 'examples_per_second': '31.588', 'grad_norm': '15.875', 'counters/examples': 192960, 'counters/updates': 6030}
train stats after 192992 examples: {'rewards_train/chosen': '0.13013', 'rewards_train/rejected': '0.040074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090052', 'logps_train/rejected': '-156.98', 'logps_train/chosen': '-150.42', 'loss/train': '0.6822', 'examples_per_second': '31.448', 'grad_norm': '23.125', 'counters/examples': 192992, 'counters/updates': 6031}
train stats after 193024 examples: {'rewards_train/chosen': '0.13479', 'rewards_train/rejected': '0.043777', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091016', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-125.81', 'loss/train': '0.6631', 'examples_per_second': '31.663', 'grad_norm': '21.125', 'counters/examples': 193024, 'counters/updates': 6032}
skipping logging after 193056 examples to avoid logging too frequently
train stats after 193088 examples: {'rewards_train/chosen': '0.1548', 'rewards_train/rejected': '0.010926', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14387', 'logps_train/rejected': '-97.426', 'logps_train/chosen': '-128.92', 'loss/train': '0.64217', 'examples_per_second': '31.697', 'grad_norm': '19.125', 'counters/examples': 193088, 'counters/updates': 6034}
train stats after 193120 examples: {'rewards_train/chosen': '0.13749', 'rewards_train/rejected': '0.070789', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066698', 'logps_train/rejected': '-118.27', 'logps_train/chosen': '-106.5', 'loss/train': '0.66829', 'examples_per_second': '31.71', 'grad_norm': '19.75', 'counters/examples': 193120, 'counters/updates': 6035}
skipping logging after 193152 examples to avoid logging too frequently
train stats after 193184 examples: {'rewards_train/chosen': '0.23635', 'rewards_train/rejected': '0.054573', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18178', 'logps_train/rejected': '-92.195', 'logps_train/chosen': '-137.29', 'loss/train': '0.62206', 'examples_per_second': '32.427', 'grad_norm': '17.5', 'counters/examples': 193184, 'counters/updates': 6037}
train stats after 193216 examples: {'rewards_train/chosen': '0.17377', 'rewards_train/rejected': '-0.012003', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18577', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-119.46', 'loss/train': '0.61878', 'examples_per_second': '32.859', 'grad_norm': '19.25', 'counters/examples': 193216, 'counters/updates': 6038}
train stats after 193248 examples: {'rewards_train/chosen': '0.11971', 'rewards_train/rejected': '-0.0062654', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12597', 'logps_train/rejected': '-115.95', 'logps_train/chosen': '-121.11', 'loss/train': '0.64624', 'examples_per_second': '31.278', 'grad_norm': '19.5', 'counters/examples': 193248, 'counters/updates': 6039}
train stats after 193280 examples: {'rewards_train/chosen': '0.18686', 'rewards_train/rejected': '0.0082262', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17863', 'logps_train/rejected': '-163.98', 'logps_train/chosen': '-142.49', 'loss/train': '0.6211', 'examples_per_second': '30.677', 'grad_norm': '21', 'counters/examples': 193280, 'counters/updates': 6040}
train stats after 193312 examples: {'rewards_train/chosen': '0.15909', 'rewards_train/rejected': '0.051673', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10742', 'logps_train/rejected': '-151.05', 'logps_train/chosen': '-131.05', 'loss/train': '0.6533', 'examples_per_second': '31.696', 'grad_norm': '20.5', 'counters/examples': 193312, 'counters/updates': 6041}
train stats after 193344 examples: {'rewards_train/chosen': '0.22099', 'rewards_train/rejected': '0.056199', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16479', 'logps_train/rejected': '-150.34', 'logps_train/chosen': '-142.09', 'loss/train': '0.63922', 'examples_per_second': '30.166', 'grad_norm': '21.625', 'counters/examples': 193344, 'counters/updates': 6042}
train stats after 193376 examples: {'rewards_train/chosen': '0.19847', 'rewards_train/rejected': '0.10635', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092124', 'logps_train/rejected': '-138.54', 'logps_train/chosen': '-134.19', 'loss/train': '0.65826', 'examples_per_second': '31.58', 'grad_norm': '21.25', 'counters/examples': 193376, 'counters/updates': 6043}
skipping logging after 193408 examples to avoid logging too frequently
train stats after 193440 examples: {'rewards_train/chosen': '0.15261', 'rewards_train/rejected': '0.025795', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12682', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-144.75', 'loss/train': '0.64996', 'examples_per_second': '31.368', 'grad_norm': '20', 'counters/examples': 193440, 'counters/updates': 6045}
train stats after 193472 examples: {'rewards_train/chosen': '0.24342', 'rewards_train/rejected': '0.051276', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19215', 'logps_train/rejected': '-145.15', 'logps_train/chosen': '-138.48', 'loss/train': '0.61896', 'examples_per_second': '31.544', 'grad_norm': '20.625', 'counters/examples': 193472, 'counters/updates': 6046}
train stats after 193504 examples: {'rewards_train/chosen': '0.12422', 'rewards_train/rejected': '-5.5142e-07', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12422', 'logps_train/rejected': '-92.399', 'logps_train/chosen': '-101.22', 'loss/train': '0.64583', 'examples_per_second': '31.603', 'grad_norm': '18.625', 'counters/examples': 193504, 'counters/updates': 6047}
skipping logging after 193536 examples to avoid logging too frequently
train stats after 193568 examples: {'rewards_train/chosen': '0.19894', 'rewards_train/rejected': '0.038305', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16063', 'logps_train/rejected': '-117.46', 'logps_train/chosen': '-113.09', 'loss/train': '0.63102', 'examples_per_second': '31.64', 'grad_norm': '17.875', 'counters/examples': 193568, 'counters/updates': 6049}
train stats after 193600 examples: {'rewards_train/chosen': '0.18613', 'rewards_train/rejected': '0.10118', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084947', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-142.61', 'loss/train': '0.67771', 'examples_per_second': '30.598', 'grad_norm': '22', 'counters/examples': 193600, 'counters/updates': 6050}
train stats after 193632 examples: {'rewards_train/chosen': '0.2217', 'rewards_train/rejected': '-0.02717', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24887', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-149.68', 'loss/train': '0.59142', 'examples_per_second': '32.736', 'grad_norm': '19', 'counters/examples': 193632, 'counters/updates': 6051}
train stats after 193664 examples: {'rewards_train/chosen': '0.15724', 'rewards_train/rejected': '0.10118', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056059', 'logps_train/rejected': '-93.697', 'logps_train/chosen': '-96.189', 'loss/train': '0.67837', 'examples_per_second': '30.865', 'grad_norm': '17.75', 'counters/examples': 193664, 'counters/updates': 6052}
train stats after 193696 examples: {'rewards_train/chosen': '0.18342', 'rewards_train/rejected': '0.051596', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13182', 'logps_train/rejected': '-97.007', 'logps_train/chosen': '-99.137', 'loss/train': '0.64417', 'examples_per_second': '32.47', 'grad_norm': '17', 'counters/examples': 193696, 'counters/updates': 6053}
train stats after 193728 examples: {'rewards_train/chosen': '0.13626', 'rewards_train/rejected': '0.02539', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11086', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-174.68', 'loss/train': '0.65359', 'examples_per_second': '31.69', 'grad_norm': '22', 'counters/examples': 193728, 'counters/updates': 6054}
train stats after 193760 examples: {'rewards_train/chosen': '0.21558', 'rewards_train/rejected': '-0.026656', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24224', 'logps_train/rejected': '-116.9', 'logps_train/chosen': '-130.31', 'loss/train': '0.59931', 'examples_per_second': '31.666', 'grad_norm': '18.125', 'counters/examples': 193760, 'counters/updates': 6055}
train stats after 193792 examples: {'rewards_train/chosen': '0.14876', 'rewards_train/rejected': '0.027317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12144', 'logps_train/rejected': '-120.7', 'logps_train/chosen': '-145.22', 'loss/train': '0.64108', 'examples_per_second': '32.467', 'grad_norm': '19.75', 'counters/examples': 193792, 'counters/updates': 6056}
train stats after 193824 examples: {'rewards_train/chosen': '0.20422', 'rewards_train/rejected': '0.035728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16849', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-136.89', 'loss/train': '0.6345', 'examples_per_second': '30.339', 'grad_norm': '18.5', 'counters/examples': 193824, 'counters/updates': 6057}
train stats after 193856 examples: {'rewards_train/chosen': '0.19858', 'rewards_train/rejected': '0.11099', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087594', 'logps_train/rejected': '-144.78', 'logps_train/chosen': '-160.8', 'loss/train': '0.65992', 'examples_per_second': '31.649', 'grad_norm': '22.5', 'counters/examples': 193856, 'counters/updates': 6058}
train stats after 193888 examples: {'rewards_train/chosen': '0.11028', 'rewards_train/rejected': '0.062475', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047807', 'logps_train/rejected': '-97.373', 'logps_train/chosen': '-138.38', 'loss/train': '0.68375', 'examples_per_second': '27.051', 'grad_norm': '20.125', 'counters/examples': 193888, 'counters/updates': 6059}
skipping logging after 193920 examples to avoid logging too frequently
train stats after 193952 examples: {'rewards_train/chosen': '0.16274', 'rewards_train/rejected': '0.093543', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069195', 'logps_train/rejected': '-142.16', 'logps_train/chosen': '-118.25', 'loss/train': '0.6783', 'examples_per_second': '31.632', 'grad_norm': '21.25', 'counters/examples': 193952, 'counters/updates': 6061}
train stats after 193984 examples: {'rewards_train/chosen': '0.16593', 'rewards_train/rejected': '0.0024584', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16347', 'logps_train/rejected': '-101.2', 'logps_train/chosen': '-140.91', 'loss/train': '0.63092', 'examples_per_second': '32.538', 'grad_norm': '19.5', 'counters/examples': 193984, 'counters/updates': 6062}
skipping logging after 194016 examples to avoid logging too frequently
train stats after 194048 examples: {'rewards_train/chosen': '0.24498', 'rewards_train/rejected': '0.13981', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-100.13', 'logps_train/chosen': '-149.75', 'loss/train': '0.66555', 'examples_per_second': '34.133', 'grad_norm': '21.375', 'counters/examples': 194048, 'counters/updates': 6064}
train stats after 194080 examples: {'rewards_train/chosen': '0.24538', 'rewards_train/rejected': '0.10336', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14202', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-122.26', 'loss/train': '0.63966', 'examples_per_second': '30.333', 'grad_norm': '19', 'counters/examples': 194080, 'counters/updates': 6065}
train stats after 194112 examples: {'rewards_train/chosen': '0.22935', 'rewards_train/rejected': '0.12737', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10198', 'logps_train/rejected': '-115.97', 'logps_train/chosen': '-136.28', 'loss/train': '0.66088', 'examples_per_second': '30.895', 'grad_norm': '20.5', 'counters/examples': 194112, 'counters/updates': 6066}
train stats after 194144 examples: {'rewards_train/chosen': '0.17289', 'rewards_train/rejected': '-0.024441', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19733', 'logps_train/rejected': '-90.139', 'logps_train/chosen': '-119.06', 'loss/train': '0.60929', 'examples_per_second': '30.911', 'grad_norm': '16.375', 'counters/examples': 194144, 'counters/updates': 6067}
train stats after 194176 examples: {'rewards_train/chosen': '0.15903', 'rewards_train/rejected': '0.07653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082505', 'logps_train/rejected': '-129.3', 'logps_train/chosen': '-161.58', 'loss/train': '0.66362', 'examples_per_second': '31.619', 'grad_norm': '21.875', 'counters/examples': 194176, 'counters/updates': 6068}
skipping logging after 194208 examples to avoid logging too frequently
train stats after 194240 examples: {'rewards_train/chosen': '0.10887', 'rewards_train/rejected': '-0.016674', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12555', 'logps_train/rejected': '-94.279', 'logps_train/chosen': '-127.67', 'loss/train': '0.64206', 'examples_per_second': '30.46', 'grad_norm': '17', 'counters/examples': 194240, 'counters/updates': 6070}
train stats after 194272 examples: {'rewards_train/chosen': '0.19012', 'rewards_train/rejected': '-0.0060902', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19621', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-156.41', 'loss/train': '0.62297', 'examples_per_second': '32.56', 'grad_norm': '20.5', 'counters/examples': 194272, 'counters/updates': 6071}
skipping logging after 194304 examples to avoid logging too frequently
train stats after 194336 examples: {'rewards_train/chosen': '0.13903', 'rewards_train/rejected': '0.04476', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094272', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-130.29', 'loss/train': '0.6598', 'examples_per_second': '30.179', 'grad_norm': '20', 'counters/examples': 194336, 'counters/updates': 6073}
skipping logging after 194368 examples to avoid logging too frequently
train stats after 194400 examples: {'rewards_train/chosen': '0.15813', 'rewards_train/rejected': '0.081505', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076623', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-166.76', 'loss/train': '0.66363', 'examples_per_second': '30.13', 'grad_norm': '22', 'counters/examples': 194400, 'counters/updates': 6075}
train stats after 194432 examples: {'rewards_train/chosen': '0.27745', 'rewards_train/rejected': '0.066332', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21111', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-151.16', 'loss/train': '0.61957', 'examples_per_second': '31.669', 'grad_norm': '20.25', 'counters/examples': 194432, 'counters/updates': 6076}
skipping logging after 194464 examples to avoid logging too frequently
train stats after 194496 examples: {'rewards_train/chosen': '0.19517', 'rewards_train/rejected': '0.060521', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13465', 'logps_train/rejected': '-88.008', 'logps_train/chosen': '-104.84', 'loss/train': '0.64813', 'examples_per_second': '33.105', 'grad_norm': '18.375', 'counters/examples': 194496, 'counters/updates': 6078}
train stats after 194528 examples: {'rewards_train/chosen': '0.13235', 'rewards_train/rejected': '0.062583', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069769', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-128.66', 'loss/train': '0.66984', 'examples_per_second': '32.151', 'grad_norm': '20.75', 'counters/examples': 194528, 'counters/updates': 6079}
train stats after 194560 examples: {'rewards_train/chosen': '0.17793', 'rewards_train/rejected': '0.029044', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14889', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-153.05', 'loss/train': '0.63446', 'examples_per_second': '31.075', 'grad_norm': '20.625', 'counters/examples': 194560, 'counters/updates': 6080}
train stats after 194592 examples: {'rewards_train/chosen': '0.20437', 'rewards_train/rejected': '0.0036497', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20072', 'logps_train/rejected': '-100.7', 'logps_train/chosen': '-152.25', 'loss/train': '0.61186', 'examples_per_second': '31.682', 'grad_norm': '19.25', 'counters/examples': 194592, 'counters/updates': 6081}
train stats after 194624 examples: {'rewards_train/chosen': '0.15681', 'rewards_train/rejected': '0.05515', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10166', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-123.83', 'loss/train': '0.66243', 'examples_per_second': '31.103', 'grad_norm': '19.125', 'counters/examples': 194624, 'counters/updates': 6082}
train stats after 194656 examples: {'rewards_train/chosen': '0.18847', 'rewards_train/rejected': '0.070863', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11761', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-104.77', 'loss/train': '0.64623', 'examples_per_second': '30.905', 'grad_norm': '17.375', 'counters/examples': 194656, 'counters/updates': 6083}
train stats after 194688 examples: {'rewards_train/chosen': '0.24315', 'rewards_train/rejected': '0.10662', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13653', 'logps_train/rejected': '-132.78', 'logps_train/chosen': '-168.14', 'loss/train': '0.64665', 'examples_per_second': '30.485', 'grad_norm': '21.25', 'counters/examples': 194688, 'counters/updates': 6084}
train stats after 194720 examples: {'rewards_train/chosen': '0.068669', 'rewards_train/rejected': '0.00033079', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068338', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-136.34', 'loss/train': '0.66822', 'examples_per_second': '31.613', 'grad_norm': '23.625', 'counters/examples': 194720, 'counters/updates': 6085}
train stats after 194752 examples: {'rewards_train/chosen': '0.23957', 'rewards_train/rejected': '0.12261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11696', 'logps_train/rejected': '-146.59', 'logps_train/chosen': '-148.15', 'loss/train': '0.65337', 'examples_per_second': '30.386', 'grad_norm': '22.75', 'counters/examples': 194752, 'counters/updates': 6086}
train stats after 194784 examples: {'rewards_train/chosen': '0.14889', 'rewards_train/rejected': '0.0031853', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1457', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-115.07', 'loss/train': '0.64272', 'examples_per_second': '31.258', 'grad_norm': '19.875', 'counters/examples': 194784, 'counters/updates': 6087}
train stats after 194816 examples: {'rewards_train/chosen': '0.14497', 'rewards_train/rejected': '-0.01554', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16051', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-153.53', 'loss/train': '0.63413', 'examples_per_second': '31.581', 'grad_norm': '20.625', 'counters/examples': 194816, 'counters/updates': 6088}
train stats after 194848 examples: {'rewards_train/chosen': '0.17115', 'rewards_train/rejected': '-0.023773', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19493', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-176.33', 'loss/train': '0.61561', 'examples_per_second': '31.449', 'grad_norm': '20.875', 'counters/examples': 194848, 'counters/updates': 6089}
train stats after 194880 examples: {'rewards_train/chosen': '0.24757', 'rewards_train/rejected': '0.10552', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14206', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-156.88', 'loss/train': '0.6399', 'examples_per_second': '30.237', 'grad_norm': '21.125', 'counters/examples': 194880, 'counters/updates': 6090}
train stats after 194912 examples: {'rewards_train/chosen': '0.084695', 'rewards_train/rejected': '0.047923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036772', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-145.12', 'loss/train': '0.68489', 'examples_per_second': '31.616', 'grad_norm': '22', 'counters/examples': 194912, 'counters/updates': 6091}
skipping logging after 194944 examples to avoid logging too frequently
train stats after 194976 examples: {'rewards_train/chosen': '0.14744', 'rewards_train/rejected': '-0.058825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20626', 'logps_train/rejected': '-92.232', 'logps_train/chosen': '-131.65', 'loss/train': '0.60662', 'examples_per_second': '35.803', 'grad_norm': '17.625', 'counters/examples': 194976, 'counters/updates': 6093}
train stats after 195008 examples: {'rewards_train/chosen': '0.12873', 'rewards_train/rejected': '-0.023689', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15242', 'logps_train/rejected': '-99.758', 'logps_train/chosen': '-123.79', 'loss/train': '0.63059', 'examples_per_second': '30.755', 'grad_norm': '18', 'counters/examples': 195008, 'counters/updates': 6094}
skipping logging after 195040 examples to avoid logging too frequently
train stats after 195072 examples: {'rewards_train/chosen': '0.26257', 'rewards_train/rejected': '0.090785', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17178', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-145.82', 'loss/train': '0.63134', 'examples_per_second': '32.145', 'grad_norm': '19.125', 'counters/examples': 195072, 'counters/updates': 6096}
train stats after 195104 examples: {'rewards_train/chosen': '0.27693', 'rewards_train/rejected': '0.081753', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19518', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-166.88', 'loss/train': '0.61058', 'examples_per_second': '31.399', 'grad_norm': '20', 'counters/examples': 195104, 'counters/updates': 6097}
train stats after 195136 examples: {'rewards_train/chosen': '0.20998', 'rewards_train/rejected': '0.13613', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.073848', 'logps_train/rejected': '-106.83', 'logps_train/chosen': '-111.89', 'loss/train': '0.66142', 'examples_per_second': '31.094', 'grad_norm': '18.875', 'counters/examples': 195136, 'counters/updates': 6098}
skipping logging after 195168 examples to avoid logging too frequently
train stats after 195200 examples: {'rewards_train/chosen': '0.20863', 'rewards_train/rejected': '0.044213', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16442', 'logps_train/rejected': '-104.45', 'logps_train/chosen': '-143.14', 'loss/train': '0.62896', 'examples_per_second': '31.604', 'grad_norm': '18.875', 'counters/examples': 195200, 'counters/updates': 6100}
train stats after 195232 examples: {'rewards_train/chosen': '0.066582', 'rewards_train/rejected': '0.047314', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019267', 'logps_train/rejected': '-129.63', 'logps_train/chosen': '-116.5', 'loss/train': '0.69318', 'examples_per_second': '31.548', 'grad_norm': '19.875', 'counters/examples': 195232, 'counters/updates': 6101}
train stats after 195264 examples: {'rewards_train/chosen': '0.27536', 'rewards_train/rejected': '0.081852', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19351', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-146.41', 'loss/train': '0.62281', 'examples_per_second': '30.721', 'grad_norm': '22.5', 'counters/examples': 195264, 'counters/updates': 6102}
train stats after 195296 examples: {'rewards_train/chosen': '0.077107', 'rewards_train/rejected': '0.0066874', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07042', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-127.94', 'loss/train': '0.67384', 'examples_per_second': '30.569', 'grad_norm': '20.25', 'counters/examples': 195296, 'counters/updates': 6103}
train stats after 195328 examples: {'rewards_train/chosen': '0.1666', 'rewards_train/rejected': '0.0082275', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15837', 'logps_train/rejected': '-100.78', 'logps_train/chosen': '-109.6', 'loss/train': '0.63155', 'examples_per_second': '30.275', 'grad_norm': '18.125', 'counters/examples': 195328, 'counters/updates': 6104}
train stats after 195360 examples: {'rewards_train/chosen': '0.18194', 'rewards_train/rejected': '0.072003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10994', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-135.03', 'loss/train': '0.65234', 'examples_per_second': '32.55', 'grad_norm': '20.25', 'counters/examples': 195360, 'counters/updates': 6105}
train stats after 195392 examples: {'rewards_train/chosen': '0.24164', 'rewards_train/rejected': '0.08424', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.1574', 'logps_train/rejected': '-105.14', 'logps_train/chosen': '-110.29', 'loss/train': '0.62509', 'examples_per_second': '31.669', 'grad_norm': '18.25', 'counters/examples': 195392, 'counters/updates': 6106}
train stats after 195424 examples: {'rewards_train/chosen': '0.12008', 'rewards_train/rejected': '-0.0017595', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.12184', 'logps_train/rejected': '-96.156', 'logps_train/chosen': '-125.37', 'loss/train': '0.65914', 'examples_per_second': '31.8', 'grad_norm': '17.75', 'counters/examples': 195424, 'counters/updates': 6107}
train stats after 195456 examples: {'rewards_train/chosen': '0.12881', 'rewards_train/rejected': '0.030523', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098287', 'logps_train/rejected': '-109.56', 'logps_train/chosen': '-152.21', 'loss/train': '0.65553', 'examples_per_second': '30.538', 'grad_norm': '20.5', 'counters/examples': 195456, 'counters/updates': 6108}
train stats after 195488 examples: {'rewards_train/chosen': '0.17641', 'rewards_train/rejected': '0.040135', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13628', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-175.1', 'loss/train': '0.64311', 'examples_per_second': '30.169', 'grad_norm': '21.75', 'counters/examples': 195488, 'counters/updates': 6109}
train stats after 195520 examples: {'rewards_train/chosen': '0.15014', 'rewards_train/rejected': '0.063551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086588', 'logps_train/rejected': '-150.8', 'logps_train/chosen': '-137.43', 'loss/train': '0.66673', 'examples_per_second': '31.615', 'grad_norm': '20.625', 'counters/examples': 195520, 'counters/updates': 6110}
train stats after 195552 examples: {'rewards_train/chosen': '0.1157', 'rewards_train/rejected': '0.028309', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087391', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-147.96', 'loss/train': '0.66173', 'examples_per_second': '31.233', 'grad_norm': '21.125', 'counters/examples': 195552, 'counters/updates': 6111}
train stats after 195584 examples: {'rewards_train/chosen': '0.27241', 'rewards_train/rejected': '0.088815', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.1836', 'logps_train/rejected': '-153.42', 'logps_train/chosen': '-157.18', 'loss/train': '0.61355', 'examples_per_second': '32.743', 'grad_norm': '21.75', 'counters/examples': 195584, 'counters/updates': 6112}
train stats after 195616 examples: {'rewards_train/chosen': '0.14315', 'rewards_train/rejected': '0.023762', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11939', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-143.06', 'loss/train': '0.64958', 'examples_per_second': '32.941', 'grad_norm': '20.125', 'counters/examples': 195616, 'counters/updates': 6113}
train stats after 195648 examples: {'rewards_train/chosen': '0.14972', 'rewards_train/rejected': '0.029353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12036', 'logps_train/rejected': '-102.04', 'logps_train/chosen': '-135.35', 'loss/train': '0.64957', 'examples_per_second': '30.992', 'grad_norm': '19.25', 'counters/examples': 195648, 'counters/updates': 6114}
train stats after 195680 examples: {'rewards_train/chosen': '0.29208', 'rewards_train/rejected': '0.18648', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10559', 'logps_train/rejected': '-147.95', 'logps_train/chosen': '-142.63', 'loss/train': '0.65076', 'examples_per_second': '31.654', 'grad_norm': '21.125', 'counters/examples': 195680, 'counters/updates': 6115}
train stats after 195712 examples: {'rewards_train/chosen': '0.20091', 'rewards_train/rejected': '0.074321', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12659', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-170.63', 'loss/train': '0.65546', 'examples_per_second': '29.952', 'grad_norm': '21.875', 'counters/examples': 195712, 'counters/updates': 6116}
train stats after 195744 examples: {'rewards_train/chosen': '0.14512', 'rewards_train/rejected': '-0.0087581', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15388', 'logps_train/rejected': '-98.013', 'logps_train/chosen': '-129.09', 'loss/train': '0.63129', 'examples_per_second': '30.593', 'grad_norm': '17.875', 'counters/examples': 195744, 'counters/updates': 6117}
train stats after 195776 examples: {'rewards_train/chosen': '0.15376', 'rewards_train/rejected': '0.054719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099038', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-136.39', 'loss/train': '0.65386', 'examples_per_second': '30.523', 'grad_norm': '21.25', 'counters/examples': 195776, 'counters/updates': 6118}
train stats after 195808 examples: {'rewards_train/chosen': '0.096016', 'rewards_train/rejected': '0.10217', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0061535', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-127.56', 'loss/train': '0.71515', 'examples_per_second': '30.209', 'grad_norm': '23', 'counters/examples': 195808, 'counters/updates': 6119}
train stats after 195840 examples: {'rewards_train/chosen': '0.14113', 'rewards_train/rejected': '0.034961', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10617', 'logps_train/rejected': '-90.983', 'logps_train/chosen': '-105.51', 'loss/train': '0.65117', 'examples_per_second': '31.427', 'grad_norm': '17.375', 'counters/examples': 195840, 'counters/updates': 6120}
train stats after 195872 examples: {'rewards_train/chosen': '0.23737', 'rewards_train/rejected': '0.1377', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099672', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-163.38', 'loss/train': '0.66072', 'examples_per_second': '31.633', 'grad_norm': '22.125', 'counters/examples': 195872, 'counters/updates': 6121}
train stats after 195904 examples: {'rewards_train/chosen': '0.093969', 'rewards_train/rejected': '0.020236', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073733', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-148.81', 'loss/train': '0.67371', 'examples_per_second': '31.666', 'grad_norm': '22.125', 'counters/examples': 195904, 'counters/updates': 6122}
train stats after 195936 examples: {'rewards_train/chosen': '0.12324', 'rewards_train/rejected': '0.0081019', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11514', 'logps_train/rejected': '-126.08', 'logps_train/chosen': '-144.23', 'loss/train': '0.66195', 'examples_per_second': '31.967', 'grad_norm': '19.75', 'counters/examples': 195936, 'counters/updates': 6123}
skipping logging after 195968 examples to avoid logging too frequently
train stats after 196000 examples: {'rewards_train/chosen': '0.16134', 'rewards_train/rejected': '-0.014741', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17608', 'logps_train/rejected': '-89.48', 'logps_train/chosen': '-87.591', 'loss/train': '0.61841', 'examples_per_second': '34.343', 'grad_norm': '15.75', 'counters/examples': 196000, 'counters/updates': 6125}
skipping logging after 196032 examples to avoid logging too frequently
train stats after 196064 examples: {'rewards_train/chosen': '0.25517', 'rewards_train/rejected': '0.090246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16493', 'logps_train/rejected': '-106.28', 'logps_train/chosen': '-121.73', 'loss/train': '0.63014', 'examples_per_second': '31.382', 'grad_norm': '18.75', 'counters/examples': 196064, 'counters/updates': 6127}
train stats after 196096 examples: {'rewards_train/chosen': '0.13563', 'rewards_train/rejected': '-0.0081147', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14375', 'logps_train/rejected': '-110.03', 'logps_train/chosen': '-138.32', 'loss/train': '0.64044', 'examples_per_second': '31.134', 'grad_norm': '20.25', 'counters/examples': 196096, 'counters/updates': 6128}
skipping logging after 196128 examples to avoid logging too frequently
train stats after 196160 examples: {'rewards_train/chosen': '0.2924', 'rewards_train/rejected': '0.028548', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.26385', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-163.41', 'loss/train': '0.57795', 'examples_per_second': '33.611', 'grad_norm': '19.125', 'counters/examples': 196160, 'counters/updates': 6130}
train stats after 196192 examples: {'rewards_train/chosen': '0.18113', 'rewards_train/rejected': '0.004174', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17696', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-155', 'loss/train': '0.62578', 'examples_per_second': '31.417', 'grad_norm': '21.625', 'counters/examples': 196192, 'counters/updates': 6131}
train stats after 196224 examples: {'rewards_train/chosen': '0.11024', 'rewards_train/rejected': '0.059244', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050992', 'logps_train/rejected': '-143.8', 'logps_train/chosen': '-124.13', 'loss/train': '0.68387', 'examples_per_second': '32.391', 'grad_norm': '20.875', 'counters/examples': 196224, 'counters/updates': 6132}
train stats after 196256 examples: {'rewards_train/chosen': '0.22777', 'rewards_train/rejected': '0.10185', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12592', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-135.56', 'loss/train': '0.65211', 'examples_per_second': '32.939', 'grad_norm': '19.5', 'counters/examples': 196256, 'counters/updates': 6133}
train stats after 196288 examples: {'rewards_train/chosen': '0.18166', 'rewards_train/rejected': '-0.014481', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19614', 'logps_train/rejected': '-125.95', 'logps_train/chosen': '-139.16', 'loss/train': '0.61218', 'examples_per_second': '30.445', 'grad_norm': '19.875', 'counters/examples': 196288, 'counters/updates': 6134}
train stats after 196320 examples: {'rewards_train/chosen': '0.16436', 'rewards_train/rejected': '0.10455', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059806', 'logps_train/rejected': '-98.169', 'logps_train/chosen': '-95.058', 'loss/train': '0.67839', 'examples_per_second': '32.367', 'grad_norm': '17.5', 'counters/examples': 196320, 'counters/updates': 6135}
train stats after 196352 examples: {'rewards_train/chosen': '0.16083', 'rewards_train/rejected': '0.0078923', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15294', 'logps_train/rejected': '-71.095', 'logps_train/chosen': '-125.53', 'loss/train': '0.64033', 'examples_per_second': '32.454', 'grad_norm': '17.25', 'counters/examples': 196352, 'counters/updates': 6136}
skipping logging after 196384 examples to avoid logging too frequently
train stats after 196416 examples: {'rewards_train/chosen': '0.11262', 'rewards_train/rejected': '0.1222', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0095776', 'logps_train/rejected': '-134.92', 'logps_train/chosen': '-115.79', 'loss/train': '0.71442', 'examples_per_second': '31.631', 'grad_norm': '22.875', 'counters/examples': 196416, 'counters/updates': 6138}
train stats after 196448 examples: {'rewards_train/chosen': '0.15427', 'rewards_train/rejected': '0.15356', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.00071026', 'logps_train/rejected': '-150.52', 'logps_train/chosen': '-136.57', 'loss/train': '0.70257', 'examples_per_second': '30.957', 'grad_norm': '24.25', 'counters/examples': 196448, 'counters/updates': 6139}
train stats after 196480 examples: {'rewards_train/chosen': '0.15906', 'rewards_train/rejected': '0.04816', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1109', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-120.65', 'loss/train': '0.65395', 'examples_per_second': '30.838', 'grad_norm': '19', 'counters/examples': 196480, 'counters/updates': 6140}
train stats after 196512 examples: {'rewards_train/chosen': '0.14442', 'rewards_train/rejected': '0.048065', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096351', 'logps_train/rejected': '-152.13', 'logps_train/chosen': '-135.41', 'loss/train': '0.66594', 'examples_per_second': '32.833', 'grad_norm': '24', 'counters/examples': 196512, 'counters/updates': 6141}
train stats after 196544 examples: {'rewards_train/chosen': '0.14806', 'rewards_train/rejected': '0.10959', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038473', 'logps_train/rejected': '-153.85', 'logps_train/chosen': '-126.54', 'loss/train': '0.6854', 'examples_per_second': '31.608', 'grad_norm': '21.5', 'counters/examples': 196544, 'counters/updates': 6142}
train stats after 196576 examples: {'rewards_train/chosen': '0.12681', 'rewards_train/rejected': '0.036831', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089978', 'logps_train/rejected': '-138.52', 'logps_train/chosen': '-146.76', 'loss/train': '0.66277', 'examples_per_second': '31.474', 'grad_norm': '21.875', 'counters/examples': 196576, 'counters/updates': 6143}
train stats after 196608 examples: {'rewards_train/chosen': '0.21627', 'rewards_train/rejected': '0.076986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13928', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-151.02', 'loss/train': '0.64525', 'examples_per_second': '31.588', 'grad_norm': '21.375', 'counters/examples': 196608, 'counters/updates': 6144}
train stats after 196640 examples: {'rewards_train/chosen': '0.16922', 'rewards_train/rejected': '0.093526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075694', 'logps_train/rejected': '-142.11', 'logps_train/chosen': '-156.29', 'loss/train': '0.67684', 'examples_per_second': '31.539', 'grad_norm': '22.75', 'counters/examples': 196640, 'counters/updates': 6145}
train stats after 196672 examples: {'rewards_train/chosen': '0.18517', 'rewards_train/rejected': '0.07036', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11481', 'logps_train/rejected': '-112.93', 'logps_train/chosen': '-151.3', 'loss/train': '0.65024', 'examples_per_second': '31.706', 'grad_norm': '20.25', 'counters/examples': 196672, 'counters/updates': 6146}
train stats after 196704 examples: {'rewards_train/chosen': '0.14518', 'rewards_train/rejected': '0.023294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12188', 'logps_train/rejected': '-114', 'logps_train/chosen': '-124.24', 'loss/train': '0.64148', 'examples_per_second': '32.642', 'grad_norm': '18.375', 'counters/examples': 196704, 'counters/updates': 6147}
train stats after 196736 examples: {'rewards_train/chosen': '0.19098', 'rewards_train/rejected': '0.16095', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.030023', 'logps_train/rejected': '-146.59', 'logps_train/chosen': '-157.28', 'loss/train': '0.6936', 'examples_per_second': '24.478', 'grad_norm': '23.25', 'counters/examples': 196736, 'counters/updates': 6148}
train stats after 196768 examples: {'rewards_train/chosen': '0.097312', 'rewards_train/rejected': '0.013398', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083914', 'logps_train/rejected': '-119.03', 'logps_train/chosen': '-130.66', 'loss/train': '0.67417', 'examples_per_second': '30.096', 'grad_norm': '19.875', 'counters/examples': 196768, 'counters/updates': 6149}
skipping logging after 196800 examples to avoid logging too frequently
train stats after 196832 examples: {'rewards_train/chosen': '0.24851', 'rewards_train/rejected': '0.13295', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11556', 'logps_train/rejected': '-121.37', 'logps_train/chosen': '-132.33', 'loss/train': '0.64701', 'examples_per_second': '24.259', 'grad_norm': '19.5', 'counters/examples': 196832, 'counters/updates': 6151}
train stats after 196864 examples: {'rewards_train/chosen': '0.12204', 'rewards_train/rejected': '0.0010073', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12103', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-150.95', 'loss/train': '0.65302', 'examples_per_second': '30.631', 'grad_norm': '21.5', 'counters/examples': 196864, 'counters/updates': 6152}
skipping logging after 196896 examples to avoid logging too frequently
train stats after 196928 examples: {'rewards_train/chosen': '0.13471', 'rewards_train/rejected': '-0.037932', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17264', 'logps_train/rejected': '-111.44', 'logps_train/chosen': '-132.91', 'loss/train': '0.61829', 'examples_per_second': '31.715', 'grad_norm': '18.25', 'counters/examples': 196928, 'counters/updates': 6154}
train stats after 196960 examples: {'rewards_train/chosen': '0.11397', 'rewards_train/rejected': '-0.00056299', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11453', 'logps_train/rejected': '-123.98', 'logps_train/chosen': '-157.14', 'loss/train': '0.65233', 'examples_per_second': '31.626', 'grad_norm': '20.875', 'counters/examples': 196960, 'counters/updates': 6155}
train stats after 196992 examples: {'rewards_train/chosen': '0.14276', 'rewards_train/rejected': '0.029982', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11277', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-131.56', 'loss/train': '0.65426', 'examples_per_second': '32.013', 'grad_norm': '19.25', 'counters/examples': 196992, 'counters/updates': 6156}
train stats after 197024 examples: {'rewards_train/chosen': '0.14454', 'rewards_train/rejected': '0.065633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078909', 'logps_train/rejected': '-133.82', 'logps_train/chosen': '-154.51', 'loss/train': '0.67107', 'examples_per_second': '30.612', 'grad_norm': '21.75', 'counters/examples': 197024, 'counters/updates': 6157}
train stats after 197056 examples: {'rewards_train/chosen': '0.38633', 'rewards_train/rejected': '0.068067', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.31827', 'logps_train/rejected': '-112.88', 'logps_train/chosen': '-151.51', 'loss/train': '0.57179', 'examples_per_second': '30.309', 'grad_norm': '19.5', 'counters/examples': 197056, 'counters/updates': 6158}
train stats after 197088 examples: {'rewards_train/chosen': '0.12908', 'rewards_train/rejected': '-0.005235', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13431', 'logps_train/rejected': '-73.106', 'logps_train/chosen': '-135.76', 'loss/train': '0.64093', 'examples_per_second': '31.68', 'grad_norm': '18', 'counters/examples': 197088, 'counters/updates': 6159}
train stats after 197120 examples: {'rewards_train/chosen': '0.1011', 'rewards_train/rejected': '0.063989', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037115', 'logps_train/rejected': '-154.18', 'logps_train/chosen': '-118.2', 'loss/train': '0.68697', 'examples_per_second': '31.145', 'grad_norm': '21.625', 'counters/examples': 197120, 'counters/updates': 6160}
train stats after 197152 examples: {'rewards_train/chosen': '0.15524', 'rewards_train/rejected': '0.064899', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.090341', 'logps_train/rejected': '-93.639', 'logps_train/chosen': '-100.53', 'loss/train': '0.66034', 'examples_per_second': '31.222', 'grad_norm': '17', 'counters/examples': 197152, 'counters/updates': 6161}
train stats after 197184 examples: {'rewards_train/chosen': '0.11287', 'rewards_train/rejected': '0.074638', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.038236', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-101.45', 'loss/train': '0.68648', 'examples_per_second': '30.47', 'grad_norm': '18.75', 'counters/examples': 197184, 'counters/updates': 6162}
train stats after 197216 examples: {'rewards_train/chosen': '0.19665', 'rewards_train/rejected': '0.11797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078683', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-135.03', 'loss/train': '0.66769', 'examples_per_second': '31.652', 'grad_norm': '21', 'counters/examples': 197216, 'counters/updates': 6163}
train stats after 197248 examples: {'rewards_train/chosen': '0.14776', 'rewards_train/rejected': '-0.012553', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16032', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-161.55', 'loss/train': '0.62743', 'examples_per_second': '30.102', 'grad_norm': '19.5', 'counters/examples': 197248, 'counters/updates': 6164}
train stats after 197280 examples: {'rewards_train/chosen': '0.15934', 'rewards_train/rejected': '0.020206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13913', 'logps_train/rejected': '-103.18', 'logps_train/chosen': '-137.43', 'loss/train': '0.63633', 'examples_per_second': '31.122', 'grad_norm': '18.625', 'counters/examples': 197280, 'counters/updates': 6165}
train stats after 197312 examples: {'rewards_train/chosen': '0.13306', 'rewards_train/rejected': '0.068468', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064594', 'logps_train/rejected': '-113.06', 'logps_train/chosen': '-143.98', 'loss/train': '0.67195', 'examples_per_second': '30.197', 'grad_norm': '20.125', 'counters/examples': 197312, 'counters/updates': 6166}
train stats after 197344 examples: {'rewards_train/chosen': '0.14201', 'rewards_train/rejected': '0.015663', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12635', 'logps_train/rejected': '-105.97', 'logps_train/chosen': '-125.34', 'loss/train': '0.64763', 'examples_per_second': '31.683', 'grad_norm': '19.125', 'counters/examples': 197344, 'counters/updates': 6167}
train stats after 197376 examples: {'rewards_train/chosen': '0.12515', 'rewards_train/rejected': '0.073406', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051749', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-133.67', 'loss/train': '0.68312', 'examples_per_second': '31.516', 'grad_norm': '21.75', 'counters/examples': 197376, 'counters/updates': 6168}
train stats after 197408 examples: {'rewards_train/chosen': '0.14562', 'rewards_train/rejected': '0.07522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0704', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-133.6', 'loss/train': '0.67886', 'examples_per_second': '30.298', 'grad_norm': '20.625', 'counters/examples': 197408, 'counters/updates': 6169}
train stats after 197440 examples: {'rewards_train/chosen': '0.18773', 'rewards_train/rejected': '0.10547', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082263', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-125.69', 'loss/train': '0.66559', 'examples_per_second': '31.34', 'grad_norm': '20', 'counters/examples': 197440, 'counters/updates': 6170}
skipping logging after 197472 examples to avoid logging too frequently
train stats after 197504 examples: {'rewards_train/chosen': '0.20212', 'rewards_train/rejected': '0.018306', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18382', 'logps_train/rejected': '-128', 'logps_train/chosen': '-125.83', 'loss/train': '0.61513', 'examples_per_second': '34.472', 'grad_norm': '18.875', 'counters/examples': 197504, 'counters/updates': 6172}
skipping logging after 197536 examples to avoid logging too frequently
train stats after 197568 examples: {'rewards_train/chosen': '0.27368', 'rewards_train/rejected': '0.030384', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24329', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-157.37', 'loss/train': '0.60175', 'examples_per_second': '31.397', 'grad_norm': '20.75', 'counters/examples': 197568, 'counters/updates': 6174}
train stats after 197600 examples: {'rewards_train/chosen': '0.15188', 'rewards_train/rejected': '0.085847', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066037', 'logps_train/rejected': '-121.13', 'logps_train/chosen': '-144.39', 'loss/train': '0.67784', 'examples_per_second': '32.246', 'grad_norm': '21.375', 'counters/examples': 197600, 'counters/updates': 6175}
train stats after 197632 examples: {'rewards_train/chosen': '0.19353', 'rewards_train/rejected': '0.02746', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16607', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-135.11', 'loss/train': '0.63884', 'examples_per_second': '30.229', 'grad_norm': '21.5', 'counters/examples': 197632, 'counters/updates': 6176}
train stats after 197664 examples: {'rewards_train/chosen': '0.11329', 'rewards_train/rejected': '0.05264', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060653', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-127.62', 'loss/train': '0.67372', 'examples_per_second': '30.841', 'grad_norm': '19.875', 'counters/examples': 197664, 'counters/updates': 6177}
train stats after 197696 examples: {'rewards_train/chosen': '0.20192', 'rewards_train/rejected': '0.026944', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17498', 'logps_train/rejected': '-119.79', 'logps_train/chosen': '-131.5', 'loss/train': '0.62834', 'examples_per_second': '31.663', 'grad_norm': '19.875', 'counters/examples': 197696, 'counters/updates': 6178}
train stats after 197728 examples: {'rewards_train/chosen': '0.20985', 'rewards_train/rejected': '0.10368', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10617', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-117.45', 'loss/train': '0.66438', 'examples_per_second': '31.636', 'grad_norm': '20.625', 'counters/examples': 197728, 'counters/updates': 6179}
train stats after 197760 examples: {'rewards_train/chosen': '0.1376', 'rewards_train/rejected': '0.052548', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.085048', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-140.39', 'loss/train': '0.66112', 'examples_per_second': '31.387', 'grad_norm': '21.25', 'counters/examples': 197760, 'counters/updates': 6180}
train stats after 197792 examples: {'rewards_train/chosen': '0.17166', 'rewards_train/rejected': '0.027094', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14457', 'logps_train/rejected': '-140.9', 'logps_train/chosen': '-124.41', 'loss/train': '0.63004', 'examples_per_second': '31.651', 'grad_norm': '19.875', 'counters/examples': 197792, 'counters/updates': 6181}
train stats after 197824 examples: {'rewards_train/chosen': '0.11449', 'rewards_train/rejected': '0.070502', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043992', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-102.95', 'loss/train': '0.68075', 'examples_per_second': '31.202', 'grad_norm': '21.125', 'counters/examples': 197824, 'counters/updates': 6182}
train stats after 197856 examples: {'rewards_train/chosen': '0.21571', 'rewards_train/rejected': '0.029323', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18639', 'logps_train/rejected': '-141.81', 'logps_train/chosen': '-145.82', 'loss/train': '0.62087', 'examples_per_second': '32.409', 'grad_norm': '20.875', 'counters/examples': 197856, 'counters/updates': 6183}
train stats after 197888 examples: {'rewards_train/chosen': '0.1005', 'rewards_train/rejected': '0.10007', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00043485', 'logps_train/rejected': '-145.96', 'logps_train/chosen': '-139.47', 'loss/train': '0.70787', 'examples_per_second': '30.927', 'grad_norm': '23.5', 'counters/examples': 197888, 'counters/updates': 6184}
skipping logging after 197920 examples to avoid logging too frequently
train stats after 197952 examples: {'rewards_train/chosen': '0.32174', 'rewards_train/rejected': '0.13411', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18764', 'logps_train/rejected': '-131.22', 'logps_train/chosen': '-160.32', 'loss/train': '0.62363', 'examples_per_second': '33.395', 'grad_norm': '21.625', 'counters/examples': 197952, 'counters/updates': 6186}
train stats after 197984 examples: {'rewards_train/chosen': '0.14176', 'rewards_train/rejected': '0.015141', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12662', 'logps_train/rejected': '-96.323', 'logps_train/chosen': '-121.21', 'loss/train': '0.64554', 'examples_per_second': '30.176', 'grad_norm': '18.125', 'counters/examples': 197984, 'counters/updates': 6187}
train stats after 198016 examples: {'rewards_train/chosen': '0.12672', 'rewards_train/rejected': '-0.034824', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16155', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-174.85', 'loss/train': '0.64242', 'examples_per_second': '31.315', 'grad_norm': '20.875', 'counters/examples': 198016, 'counters/updates': 6188}
skipping logging after 198048 examples to avoid logging too frequently
train stats after 198080 examples: {'rewards_train/chosen': '0.075478', 'rewards_train/rejected': '0.054771', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020707', 'logps_train/rejected': '-126.38', 'logps_train/chosen': '-121.84', 'loss/train': '0.70012', 'examples_per_second': '31.045', 'grad_norm': '21.25', 'counters/examples': 198080, 'counters/updates': 6190}
train stats after 198112 examples: {'rewards_train/chosen': '0.065986', 'rewards_train/rejected': '0.13869', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.072699', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-154.62', 'loss/train': '0.74874', 'examples_per_second': '31.595', 'grad_norm': '24.375', 'counters/examples': 198112, 'counters/updates': 6191}
train stats after 198144 examples: {'rewards_train/chosen': '0.20897', 'rewards_train/rejected': '0.12995', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.07901', 'logps_train/rejected': '-88.497', 'logps_train/chosen': '-128.38', 'loss/train': '0.67468', 'examples_per_second': '31.678', 'grad_norm': '19.625', 'counters/examples': 198144, 'counters/updates': 6192}
train stats after 198176 examples: {'rewards_train/chosen': '0.24159', 'rewards_train/rejected': '0.083764', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15783', 'logps_train/rejected': '-110.19', 'logps_train/chosen': '-174.07', 'loss/train': '0.63339', 'examples_per_second': '31.636', 'grad_norm': '21.25', 'counters/examples': 198176, 'counters/updates': 6193}
train stats after 198208 examples: {'rewards_train/chosen': '0.051537', 'rewards_train/rejected': '-0.018012', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069549', 'logps_train/rejected': '-86.585', 'logps_train/chosen': '-156.85', 'loss/train': '0.67035', 'examples_per_second': '32.413', 'grad_norm': '20.75', 'counters/examples': 198208, 'counters/updates': 6194}
skipping logging after 198240 examples to avoid logging too frequently
train stats after 198272 examples: {'rewards_train/chosen': '0.061845', 'rewards_train/rejected': '-0.0067229', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068568', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-143.76', 'loss/train': '0.67105', 'examples_per_second': '33.075', 'grad_norm': '21', 'counters/examples': 198272, 'counters/updates': 6196}
train stats after 198304 examples: {'rewards_train/chosen': '0.1304', 'rewards_train/rejected': '-0.0078681', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.13827', 'logps_train/rejected': '-96.424', 'logps_train/chosen': '-109.1', 'loss/train': '0.65209', 'examples_per_second': '31.647', 'grad_norm': '18.375', 'counters/examples': 198304, 'counters/updates': 6197}
train stats after 198336 examples: {'rewards_train/chosen': '0.14166', 'rewards_train/rejected': '0.057012', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08465', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-118.32', 'loss/train': '0.66514', 'examples_per_second': '30.705', 'grad_norm': '19.875', 'counters/examples': 198336, 'counters/updates': 6198}
train stats after 198368 examples: {'rewards_train/chosen': '0.17364', 'rewards_train/rejected': '0.10429', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069349', 'logps_train/rejected': '-107.2', 'logps_train/chosen': '-136.11', 'loss/train': '0.66939', 'examples_per_second': '31.584', 'grad_norm': '20.125', 'counters/examples': 198368, 'counters/updates': 6199}
train stats after 198400 examples: {'rewards_train/chosen': '0.21509', 'rewards_train/rejected': '0.057639', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15745', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-152.15', 'loss/train': '0.6423', 'examples_per_second': '30.772', 'grad_norm': '20.125', 'counters/examples': 198400, 'counters/updates': 6200}
train stats after 198432 examples: {'rewards_train/chosen': '0.26614', 'rewards_train/rejected': '0.064485', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20166', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-141.45', 'loss/train': '0.61373', 'examples_per_second': '31.621', 'grad_norm': '19.75', 'counters/examples': 198432, 'counters/updates': 6201}
train stats after 198464 examples: {'rewards_train/chosen': '0.1929', 'rewards_train/rejected': '-0.080005', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2729', 'logps_train/rejected': '-94.571', 'logps_train/chosen': '-121.03', 'loss/train': '0.58483', 'examples_per_second': '32.543', 'grad_norm': '16.875', 'counters/examples': 198464, 'counters/updates': 6202}
train stats after 198496 examples: {'rewards_train/chosen': '0.2753', 'rewards_train/rejected': '0.032863', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24243', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-136.99', 'loss/train': '0.59995', 'examples_per_second': '30.991', 'grad_norm': '20.125', 'counters/examples': 198496, 'counters/updates': 6203}
train stats after 198528 examples: {'rewards_train/chosen': '0.13245', 'rewards_train/rejected': '0.068012', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064434', 'logps_train/rejected': '-109.61', 'logps_train/chosen': '-109.6', 'loss/train': '0.68163', 'examples_per_second': '31.538', 'grad_norm': '19.875', 'counters/examples': 198528, 'counters/updates': 6204}
train stats after 198560 examples: {'rewards_train/chosen': '0.13847', 'rewards_train/rejected': '0.12478', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013691', 'logps_train/rejected': '-101.04', 'logps_train/chosen': '-118.5', 'loss/train': '0.70123', 'examples_per_second': '31.75', 'grad_norm': '20', 'counters/examples': 198560, 'counters/updates': 6205}
train stats after 198592 examples: {'rewards_train/chosen': '0.16496', 'rewards_train/rejected': '0.18034', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015382', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-157.74', 'loss/train': '0.71162', 'examples_per_second': '30.64', 'grad_norm': '22.75', 'counters/examples': 198592, 'counters/updates': 6206}
train stats after 198624 examples: {'rewards_train/chosen': '0.15124', 'rewards_train/rejected': '-0.11054', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.26178', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-123', 'loss/train': '0.59282', 'examples_per_second': '32.213', 'grad_norm': '18.75', 'counters/examples': 198624, 'counters/updates': 6207}
train stats after 198656 examples: {'rewards_train/chosen': '0.10789', 'rewards_train/rejected': '-0.027564', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13545', 'logps_train/rejected': '-100.31', 'logps_train/chosen': '-147.46', 'loss/train': '0.64286', 'examples_per_second': '31.661', 'grad_norm': '19.125', 'counters/examples': 198656, 'counters/updates': 6208}
train stats after 198688 examples: {'rewards_train/chosen': '0.20416', 'rewards_train/rejected': '0.029819', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17434', 'logps_train/rejected': '-135.08', 'logps_train/chosen': '-132.9', 'loss/train': '0.62871', 'examples_per_second': '30.398', 'grad_norm': '19', 'counters/examples': 198688, 'counters/updates': 6209}
train stats after 198720 examples: {'rewards_train/chosen': '0.074204', 'rewards_train/rejected': '0.080334', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0061306', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-166.31', 'loss/train': '0.71442', 'examples_per_second': '31.459', 'grad_norm': '23.375', 'counters/examples': 198720, 'counters/updates': 6210}
train stats after 198752 examples: {'rewards_train/chosen': '0.10737', 'rewards_train/rejected': '0.12416', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016788', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-134.24', 'loss/train': '0.71051', 'examples_per_second': '31.602', 'grad_norm': '22.25', 'counters/examples': 198752, 'counters/updates': 6211}
skipping logging after 198784 examples to avoid logging too frequently
train stats after 198816 examples: {'rewards_train/chosen': '0.093998', 'rewards_train/rejected': '0.040845', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053153', 'logps_train/rejected': '-110.35', 'logps_train/chosen': '-165.75', 'loss/train': '0.67971', 'examples_per_second': '32.968', 'grad_norm': '21.375', 'counters/examples': 198816, 'counters/updates': 6213}
train stats after 198848 examples: {'rewards_train/chosen': '0.22662', 'rewards_train/rejected': '0.038265', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18835', 'logps_train/rejected': '-135.24', 'logps_train/chosen': '-127.35', 'loss/train': '0.62102', 'examples_per_second': '31.683', 'grad_norm': '19.875', 'counters/examples': 198848, 'counters/updates': 6214}
train stats after 198880 examples: {'rewards_train/chosen': '0.17906', 'rewards_train/rejected': '0.029912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14915', 'logps_train/rejected': '-129.37', 'logps_train/chosen': '-133.53', 'loss/train': '0.63497', 'examples_per_second': '30.837', 'grad_norm': '21.125', 'counters/examples': 198880, 'counters/updates': 6215}
train stats after 198912 examples: {'rewards_train/chosen': '0.19994', 'rewards_train/rejected': '0.047308', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15263', 'logps_train/rejected': '-98.261', 'logps_train/chosen': '-131.08', 'loss/train': '0.63705', 'examples_per_second': '31.951', 'grad_norm': '17.875', 'counters/examples': 198912, 'counters/updates': 6216}
skipping logging after 198944 examples to avoid logging too frequently
train stats after 198976 examples: {'rewards_train/chosen': '0.19685', 'rewards_train/rejected': '0.017928', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17892', 'logps_train/rejected': '-102.21', 'logps_train/chosen': '-111.52', 'loss/train': '0.6266', 'examples_per_second': '31.661', 'grad_norm': '16.75', 'counters/examples': 198976, 'counters/updates': 6218}
train stats after 199008 examples: {'rewards_train/chosen': '0.14257', 'rewards_train/rejected': '0.077231', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.065335', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-132.45', 'loss/train': '0.67447', 'examples_per_second': '32.767', 'grad_norm': '20.75', 'counters/examples': 199008, 'counters/updates': 6219}
train stats after 199040 examples: {'rewards_train/chosen': '0.21113', 'rewards_train/rejected': '-0.012164', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22329', 'logps_train/rejected': '-130.11', 'logps_train/chosen': '-179.58', 'loss/train': '0.60116', 'examples_per_second': '31.681', 'grad_norm': '22.125', 'counters/examples': 199040, 'counters/updates': 6220}
train stats after 199072 examples: {'rewards_train/chosen': '0.10132', 'rewards_train/rejected': '-0.020043', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12137', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-184.42', 'loss/train': '0.65022', 'examples_per_second': '31.701', 'grad_norm': '21.875', 'counters/examples': 199072, 'counters/updates': 6221}
train stats after 199104 examples: {'rewards_train/chosen': '0.17156', 'rewards_train/rejected': '0.046527', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12503', 'logps_train/rejected': '-123.2', 'logps_train/chosen': '-170.61', 'loss/train': '0.64582', 'examples_per_second': '31.667', 'grad_norm': '22.875', 'counters/examples': 199104, 'counters/updates': 6222}
skipping logging after 199136 examples to avoid logging too frequently
train stats after 199168 examples: {'rewards_train/chosen': '0.19769', 'rewards_train/rejected': '-0.02506', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22275', 'logps_train/rejected': '-111.95', 'logps_train/chosen': '-133.59', 'loss/train': '0.6008', 'examples_per_second': '35.255', 'grad_norm': '17.5', 'counters/examples': 199168, 'counters/updates': 6224}
train stats after 199200 examples: {'rewards_train/chosen': '0.12166', 'rewards_train/rejected': '-0.0062952', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12795', 'logps_train/rejected': '-98.152', 'logps_train/chosen': '-111.65', 'loss/train': '0.64742', 'examples_per_second': '31.7', 'grad_norm': '16.25', 'counters/examples': 199200, 'counters/updates': 6225}
skipping logging after 199232 examples to avoid logging too frequently
train stats after 199264 examples: {'rewards_train/chosen': '0.10024', 'rewards_train/rejected': '-0.0080581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1083', 'logps_train/rejected': '-104.56', 'logps_train/chosen': '-143.58', 'loss/train': '0.65389', 'examples_per_second': '31.036', 'grad_norm': '19', 'counters/examples': 199264, 'counters/updates': 6227}
skipping logging after 199296 examples to avoid logging too frequently
train stats after 199328 examples: {'rewards_train/chosen': '0.20548', 'rewards_train/rejected': '0.099736', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10574', 'logps_train/rejected': '-161.31', 'logps_train/chosen': '-167.8', 'loss/train': '0.66245', 'examples_per_second': '31.821', 'grad_norm': '22.625', 'counters/examples': 199328, 'counters/updates': 6229}
train stats after 199360 examples: {'rewards_train/chosen': '0.14549', 'rewards_train/rejected': '0.10179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043697', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-129.92', 'loss/train': '0.68275', 'examples_per_second': '30.542', 'grad_norm': '19.25', 'counters/examples': 199360, 'counters/updates': 6230}
train stats after 199392 examples: {'rewards_train/chosen': '0.17033', 'rewards_train/rejected': '0.029036', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1413', 'logps_train/rejected': '-106.41', 'logps_train/chosen': '-116.47', 'loss/train': '0.64052', 'examples_per_second': '30.093', 'grad_norm': '18.625', 'counters/examples': 199392, 'counters/updates': 6231}
skipping logging after 199424 examples to avoid logging too frequently
train stats after 199456 examples: {'rewards_train/chosen': '0.10237', 'rewards_train/rejected': '-0.017073', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11944', 'logps_train/rejected': '-86.869', 'logps_train/chosen': '-107.15', 'loss/train': '0.64127', 'examples_per_second': '34.17', 'grad_norm': '18', 'counters/examples': 199456, 'counters/updates': 6233}
train stats after 199488 examples: {'rewards_train/chosen': '0.15095', 'rewards_train/rejected': '0.068296', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082657', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-118.46', 'loss/train': '0.67059', 'examples_per_second': '33.079', 'grad_norm': '21.625', 'counters/examples': 199488, 'counters/updates': 6234}
train stats after 199520 examples: {'rewards_train/chosen': '0.14015', 'rewards_train/rejected': '0.022816', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11733', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-154.98', 'loss/train': '0.64667', 'examples_per_second': '31.679', 'grad_norm': '21.25', 'counters/examples': 199520, 'counters/updates': 6235}
train stats after 199552 examples: {'rewards_train/chosen': '0.27248', 'rewards_train/rejected': '0.03656', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23592', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-155.21', 'loss/train': '0.59652', 'examples_per_second': '30.805', 'grad_norm': '19.875', 'counters/examples': 199552, 'counters/updates': 6236}
train stats after 199584 examples: {'rewards_train/chosen': '0.10798', 'rewards_train/rejected': '0.03094', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077043', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-122.05', 'loss/train': '0.67338', 'examples_per_second': '31.645', 'grad_norm': '20.625', 'counters/examples': 199584, 'counters/updates': 6237}
train stats after 199616 examples: {'rewards_train/chosen': '0.20587', 'rewards_train/rejected': '0.019291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18658', 'logps_train/rejected': '-121.81', 'logps_train/chosen': '-135.2', 'loss/train': '0.61979', 'examples_per_second': '32.952', 'grad_norm': '18', 'counters/examples': 199616, 'counters/updates': 6238}
skipping logging after 199648 examples to avoid logging too frequently
train stats after 199680 examples: {'rewards_train/chosen': '0.17663', 'rewards_train/rejected': '0.059622', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11701', 'logps_train/rejected': '-105.82', 'logps_train/chosen': '-128.21', 'loss/train': '0.64988', 'examples_per_second': '31.83', 'grad_norm': '18.125', 'counters/examples': 199680, 'counters/updates': 6240}
train stats after 199712 examples: {'rewards_train/chosen': '0.13136', 'rewards_train/rejected': '0.00016705', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13119', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-148.53', 'loss/train': '0.64713', 'examples_per_second': '31.457', 'grad_norm': '20', 'counters/examples': 199712, 'counters/updates': 6241}
train stats after 199744 examples: {'rewards_train/chosen': '0.24682', 'rewards_train/rejected': '0.13812', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1087', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-141.32', 'loss/train': '0.65921', 'examples_per_second': '31.635', 'grad_norm': '21.875', 'counters/examples': 199744, 'counters/updates': 6242}
train stats after 199776 examples: {'rewards_train/chosen': '0.24796', 'rewards_train/rejected': '0.067008', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18095', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-156.57', 'loss/train': '0.6198', 'examples_per_second': '31.206', 'grad_norm': '21.125', 'counters/examples': 199776, 'counters/updates': 6243}
train stats after 199808 examples: {'rewards_train/chosen': '0.14529', 'rewards_train/rejected': '0.047343', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097946', 'logps_train/rejected': '-105.35', 'logps_train/chosen': '-133.29', 'loss/train': '0.65775', 'examples_per_second': '31.619', 'grad_norm': '20', 'counters/examples': 199808, 'counters/updates': 6244}
train stats after 199840 examples: {'rewards_train/chosen': '0.14498', 'rewards_train/rejected': '0.049397', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095581', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-116.85', 'loss/train': '0.65719', 'examples_per_second': '31.632', 'grad_norm': '19.625', 'counters/examples': 199840, 'counters/updates': 6245}
skipping logging after 199872 examples to avoid logging too frequently
train stats after 199904 examples: {'rewards_train/chosen': '0.14823', 'rewards_train/rejected': '-0.045833', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19407', 'logps_train/rejected': '-107.6', 'logps_train/chosen': '-135.58', 'loss/train': '0.6143', 'examples_per_second': '29.866', 'grad_norm': '18.25', 'counters/examples': 199904, 'counters/updates': 6247}
train stats after 199936 examples: {'rewards_train/chosen': '0.23168', 'rewards_train/rejected': '0.062907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16877', 'logps_train/rejected': '-125.1', 'logps_train/chosen': '-140.13', 'loss/train': '0.63329', 'examples_per_second': '31.51', 'grad_norm': '19.625', 'counters/examples': 199936, 'counters/updates': 6248}
train stats after 199968 examples: {'rewards_train/chosen': '0.1566', 'rewards_train/rejected': '0.059155', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097443', 'logps_train/rejected': '-116.27', 'logps_train/chosen': '-123.48', 'loss/train': '0.65284', 'examples_per_second': '30.671', 'grad_norm': '18.75', 'counters/examples': 199968, 'counters/updates': 6249}
train stats after 200000 examples: {'rewards_train/chosen': '0.084512', 'rewards_train/rejected': '-0.066745', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15126', 'logps_train/rejected': '-100.08', 'logps_train/chosen': '-140.88', 'loss/train': '0.63802', 'examples_per_second': '30.234', 'grad_norm': '17.875', 'counters/examples': 200000, 'counters/updates': 6250}
train stats after 200032 examples: {'rewards_train/chosen': '0.1213', 'rewards_train/rejected': '-0.0082659', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12957', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-126.87', 'loss/train': '0.63851', 'examples_per_second': '31.798', 'grad_norm': '19.625', 'counters/examples': 200032, 'counters/updates': 6251}
skipping logging after 200064 examples to avoid logging too frequently
train stats after 200096 examples: {'rewards_train/chosen': '0.13331', 'rewards_train/rejected': '0.02395', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10936', 'logps_train/rejected': '-109.2', 'logps_train/chosen': '-143.93', 'loss/train': '0.65385', 'examples_per_second': '30.037', 'grad_norm': '18.75', 'counters/examples': 200096, 'counters/updates': 6253}
train stats after 200128 examples: {'rewards_train/chosen': '0.22993', 'rewards_train/rejected': '0.071782', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15814', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-135.56', 'loss/train': '0.63026', 'examples_per_second': '31.599', 'grad_norm': '18.25', 'counters/examples': 200128, 'counters/updates': 6254}
train stats after 200160 examples: {'rewards_train/chosen': '0.20018', 'rewards_train/rejected': '-0.014566', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21475', 'logps_train/rejected': '-144.37', 'logps_train/chosen': '-180.89', 'loss/train': '0.60994', 'examples_per_second': '31.52', 'grad_norm': '22', 'counters/examples': 200160, 'counters/updates': 6255}
train stats after 200192 examples: {'rewards_train/chosen': '0.1058', 'rewards_train/rejected': '0.12088', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015078', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-162.29', 'loss/train': '0.7241', 'examples_per_second': '30.179', 'grad_norm': '23.5', 'counters/examples': 200192, 'counters/updates': 6256}
train stats after 200224 examples: {'rewards_train/chosen': '0.21319', 'rewards_train/rejected': '0.069366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14382', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-133.23', 'loss/train': '0.64397', 'examples_per_second': '31.463', 'grad_norm': '18.875', 'counters/examples': 200224, 'counters/updates': 6257}
train stats after 200256 examples: {'rewards_train/chosen': '0.2183', 'rewards_train/rejected': '0.15713', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.061173', 'logps_train/rejected': '-162.43', 'logps_train/chosen': '-225.94', 'loss/train': '0.67844', 'examples_per_second': '30.159', 'grad_norm': '25.75', 'counters/examples': 200256, 'counters/updates': 6258}
train stats after 200288 examples: {'rewards_train/chosen': '0.20889', 'rewards_train/rejected': '0.011137', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19775', 'logps_train/rejected': '-97.571', 'logps_train/chosen': '-138.5', 'loss/train': '0.62224', 'examples_per_second': '30.284', 'grad_norm': '17.75', 'counters/examples': 200288, 'counters/updates': 6259}
train stats after 200320 examples: {'rewards_train/chosen': '0.20935', 'rewards_train/rejected': '0.047549', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1618', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-132.62', 'loss/train': '0.63588', 'examples_per_second': '30.934', 'grad_norm': '19.625', 'counters/examples': 200320, 'counters/updates': 6260}
skipping logging after 200352 examples to avoid logging too frequently
train stats after 200384 examples: {'rewards_train/chosen': '0.22658', 'rewards_train/rejected': '-0.023747', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25033', 'logps_train/rejected': '-99.17', 'logps_train/chosen': '-110.47', 'loss/train': '0.58825', 'examples_per_second': '31.611', 'grad_norm': '17.375', 'counters/examples': 200384, 'counters/updates': 6262}
train stats after 200416 examples: {'rewards_train/chosen': '0.1409', 'rewards_train/rejected': '0.0061723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13473', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-131.57', 'loss/train': '0.63918', 'examples_per_second': '31.74', 'grad_norm': '18.875', 'counters/examples': 200416, 'counters/updates': 6263}
train stats after 200448 examples: {'rewards_train/chosen': '0.10548', 'rewards_train/rejected': '0.040526', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064949', 'logps_train/rejected': '-137.87', 'logps_train/chosen': '-122.09', 'loss/train': '0.67303', 'examples_per_second': '30.294', 'grad_norm': '19.625', 'counters/examples': 200448, 'counters/updates': 6264}
skipping logging after 200480 examples to avoid logging too frequently
train stats after 200512 examples: {'rewards_train/chosen': '0.13416', 'rewards_train/rejected': '-0.012724', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14688', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-150.98', 'loss/train': '0.63968', 'examples_per_second': '30.22', 'grad_norm': '22.5', 'counters/examples': 200512, 'counters/updates': 6266}
train stats after 200544 examples: {'rewards_train/chosen': '0.30849', 'rewards_train/rejected': '0.091434', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21706', 'logps_train/rejected': '-112.76', 'logps_train/chosen': '-140.12', 'loss/train': '0.60869', 'examples_per_second': '30.658', 'grad_norm': '19', 'counters/examples': 200544, 'counters/updates': 6267}
train stats after 200576 examples: {'rewards_train/chosen': '0.11106', 'rewards_train/rejected': '0.0080249', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10304', 'logps_train/rejected': '-125.1', 'logps_train/chosen': '-136.87', 'loss/train': '0.65486', 'examples_per_second': '32.397', 'grad_norm': '20.125', 'counters/examples': 200576, 'counters/updates': 6268}
skipping logging after 200608 examples to avoid logging too frequently
train stats after 200640 examples: {'rewards_train/chosen': '0.13481', 'rewards_train/rejected': '0.029305', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1055', 'logps_train/rejected': '-93.376', 'logps_train/chosen': '-111.36', 'loss/train': '0.65334', 'examples_per_second': '32.426', 'grad_norm': '17.125', 'counters/examples': 200640, 'counters/updates': 6270}
train stats after 200672 examples: {'rewards_train/chosen': '0.19051', 'rewards_train/rejected': '-0.033811', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22432', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-137.1', 'loss/train': '0.60244', 'examples_per_second': '31.643', 'grad_norm': '20', 'counters/examples': 200672, 'counters/updates': 6271}
train stats after 200704 examples: {'rewards_train/chosen': '0.18483', 'rewards_train/rejected': '0.092397', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092431', 'logps_train/rejected': '-111.25', 'logps_train/chosen': '-124.89', 'loss/train': '0.66528', 'examples_per_second': '30.788', 'grad_norm': '20.875', 'counters/examples': 200704, 'counters/updates': 6272}
train stats after 200736 examples: {'rewards_train/chosen': '0.17495', 'rewards_train/rejected': '0.044453', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13049', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-136.4', 'loss/train': '0.64102', 'examples_per_second': '32.894', 'grad_norm': '19.375', 'counters/examples': 200736, 'counters/updates': 6273}
train stats after 200768 examples: {'rewards_train/chosen': '0.22531', 'rewards_train/rejected': '0.18301', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042302', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-160.09', 'loss/train': '0.69239', 'examples_per_second': '32.11', 'grad_norm': '22.375', 'counters/examples': 200768, 'counters/updates': 6274}
train stats after 200800 examples: {'rewards_train/chosen': '0.19333', 'rewards_train/rejected': '0.065605', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.12772', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-154.88', 'loss/train': '0.64337', 'examples_per_second': '30.659', 'grad_norm': '20.875', 'counters/examples': 200800, 'counters/updates': 6275}
train stats after 200832 examples: {'rewards_train/chosen': '0.19654', 'rewards_train/rejected': '0.070069', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12647', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-118.28', 'loss/train': '0.64784', 'examples_per_second': '32.466', 'grad_norm': '19.25', 'counters/examples': 200832, 'counters/updates': 6276}
train stats after 200864 examples: {'rewards_train/chosen': '0.18832', 'rewards_train/rejected': '0.0091695', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17915', 'logps_train/rejected': '-108.06', 'logps_train/chosen': '-127.21', 'loss/train': '0.6227', 'examples_per_second': '30.486', 'grad_norm': '18.75', 'counters/examples': 200864, 'counters/updates': 6277}
train stats after 200896 examples: {'rewards_train/chosen': '0.29329', 'rewards_train/rejected': '0.021353', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27194', 'logps_train/rejected': '-89.251', 'logps_train/chosen': '-150.98', 'loss/train': '0.58354', 'examples_per_second': '26.364', 'grad_norm': '18.375', 'counters/examples': 200896, 'counters/updates': 6278}
train stats after 200928 examples: {'rewards_train/chosen': '0.22623', 'rewards_train/rejected': '0.10862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11761', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-150.19', 'loss/train': '0.65903', 'examples_per_second': '31.642', 'grad_norm': '21.125', 'counters/examples': 200928, 'counters/updates': 6279}
train stats after 200960 examples: {'rewards_train/chosen': '0.12069', 'rewards_train/rejected': '0.059391', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061297', 'logps_train/rejected': '-122.12', 'logps_train/chosen': '-138.3', 'loss/train': '0.67457', 'examples_per_second': '31.462', 'grad_norm': '21.375', 'counters/examples': 200960, 'counters/updates': 6280}
skipping logging after 200992 examples to avoid logging too frequently
train stats after 201024 examples: {'rewards_train/chosen': '0.22062', 'rewards_train/rejected': '0.035023', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1856', 'logps_train/rejected': '-92.346', 'logps_train/chosen': '-127.22', 'loss/train': '0.61979', 'examples_per_second': '31.717', 'grad_norm': '18', 'counters/examples': 201024, 'counters/updates': 6282}
skipping logging after 201056 examples to avoid logging too frequently
train stats after 201088 examples: {'rewards_train/chosen': '0.12856', 'rewards_train/rejected': '0.060089', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068472', 'logps_train/rejected': '-130', 'logps_train/chosen': '-160.41', 'loss/train': '0.67567', 'examples_per_second': '31.759', 'grad_norm': '21.5', 'counters/examples': 201088, 'counters/updates': 6284}
train stats after 201120 examples: {'rewards_train/chosen': '0.17257', 'rewards_train/rejected': '0.016346', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15623', 'logps_train/rejected': '-128.65', 'logps_train/chosen': '-121.27', 'loss/train': '0.62441', 'examples_per_second': '30.167', 'grad_norm': '19.125', 'counters/examples': 201120, 'counters/updates': 6285}
train stats after 201152 examples: {'rewards_train/chosen': '0.22679', 'rewards_train/rejected': '-0.067413', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.29421', 'logps_train/rejected': '-86.539', 'logps_train/chosen': '-114.02', 'loss/train': '0.57809', 'examples_per_second': '32.197', 'grad_norm': '15.75', 'counters/examples': 201152, 'counters/updates': 6286}
train stats after 201184 examples: {'rewards_train/chosen': '0.22809', 'rewards_train/rejected': '0.0097528', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21834', 'logps_train/rejected': '-121.72', 'logps_train/chosen': '-128.6', 'loss/train': '0.60142', 'examples_per_second': '31.521', 'grad_norm': '19.125', 'counters/examples': 201184, 'counters/updates': 6287}
train stats after 201216 examples: {'rewards_train/chosen': '0.15645', 'rewards_train/rejected': '-0.017584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17403', 'logps_train/rejected': '-138.06', 'logps_train/chosen': '-184.98', 'loss/train': '0.62447', 'examples_per_second': '30.703', 'grad_norm': '21.75', 'counters/examples': 201216, 'counters/updates': 6288}
train stats after 201248 examples: {'rewards_train/chosen': '0.15032', 'rewards_train/rejected': '0.11276', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037557', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-132.28', 'loss/train': '0.68191', 'examples_per_second': '32.016', 'grad_norm': '21.25', 'counters/examples': 201248, 'counters/updates': 6289}
train stats after 201280 examples: {'rewards_train/chosen': '0.29598', 'rewards_train/rejected': '0.018642', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.27733', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-116.69', 'loss/train': '0.5759', 'examples_per_second': '31.744', 'grad_norm': '15.938', 'counters/examples': 201280, 'counters/updates': 6290}
train stats after 201312 examples: {'rewards_train/chosen': '0.1267', 'rewards_train/rejected': '0.1244', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0023013', 'logps_train/rejected': '-135.06', 'logps_train/chosen': '-124.16', 'loss/train': '0.70717', 'examples_per_second': '30.401', 'grad_norm': '21.625', 'counters/examples': 201312, 'counters/updates': 6291}
train stats after 201344 examples: {'rewards_train/chosen': '0.1913', 'rewards_train/rejected': '-0.0028901', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19419', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-147.87', 'loss/train': '0.6123', 'examples_per_second': '33.041', 'grad_norm': '21.75', 'counters/examples': 201344, 'counters/updates': 6292}
train stats after 201376 examples: {'rewards_train/chosen': '0.15047', 'rewards_train/rejected': '0.051306', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09916', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-149.99', 'loss/train': '0.66253', 'examples_per_second': '30.172', 'grad_norm': '22.125', 'counters/examples': 201376, 'counters/updates': 6293}
train stats after 201408 examples: {'rewards_train/chosen': '0.13034', 'rewards_train/rejected': '0.04712', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083223', 'logps_train/rejected': '-121.81', 'logps_train/chosen': '-154.98', 'loss/train': '0.67112', 'examples_per_second': '31.662', 'grad_norm': '21.5', 'counters/examples': 201408, 'counters/updates': 6294}
train stats after 201440 examples: {'rewards_train/chosen': '0.16673', 'rewards_train/rejected': '0.088582', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078143', 'logps_train/rejected': '-121.83', 'logps_train/chosen': '-146.36', 'loss/train': '0.66604', 'examples_per_second': '31.174', 'grad_norm': '19.75', 'counters/examples': 201440, 'counters/updates': 6295}
train stats after 201472 examples: {'rewards_train/chosen': '0.16096', 'rewards_train/rejected': '-0.050019', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21098', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-155.07', 'loss/train': '0.60203', 'examples_per_second': '31.131', 'grad_norm': '18.625', 'counters/examples': 201472, 'counters/updates': 6296}
train stats after 201504 examples: {'rewards_train/chosen': '0.15798', 'rewards_train/rejected': '0.040953', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11703', 'logps_train/rejected': '-91.742', 'logps_train/chosen': '-117.56', 'loss/train': '0.64756', 'examples_per_second': '30.329', 'grad_norm': '16.875', 'counters/examples': 201504, 'counters/updates': 6297}
train stats after 201536 examples: {'rewards_train/chosen': '0.23433', 'rewards_train/rejected': '0.081714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15262', 'logps_train/rejected': '-98.623', 'logps_train/chosen': '-145.5', 'loss/train': '0.63085', 'examples_per_second': '30.227', 'grad_norm': '18.875', 'counters/examples': 201536, 'counters/updates': 6298}
train stats after 201568 examples: {'rewards_train/chosen': '0.20596', 'rewards_train/rejected': '0.20189', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0040713', 'logps_train/rejected': '-147.38', 'logps_train/chosen': '-148.3', 'loss/train': '0.70767', 'examples_per_second': '31.66', 'grad_norm': '23.25', 'counters/examples': 201568, 'counters/updates': 6299}
train stats after 201600 examples: {'rewards_train/chosen': '0.1428', 'rewards_train/rejected': '0.091042', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051763', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-121.18', 'loss/train': '0.67816', 'examples_per_second': '31.699', 'grad_norm': '20.625', 'counters/examples': 201600, 'counters/updates': 6300}
train stats after 201632 examples: {'rewards_train/chosen': '0.18643', 'rewards_train/rejected': '0.044912', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14151', 'logps_train/rejected': '-114.17', 'logps_train/chosen': '-157.49', 'loss/train': '0.64236', 'examples_per_second': '30.857', 'grad_norm': '20.375', 'counters/examples': 201632, 'counters/updates': 6301}
train stats after 201664 examples: {'rewards_train/chosen': '0.095287', 'rewards_train/rejected': '0.0053411', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089946', 'logps_train/rejected': '-120.72', 'logps_train/chosen': '-134.37', 'loss/train': '0.66052', 'examples_per_second': '31.552', 'grad_norm': '20', 'counters/examples': 201664, 'counters/updates': 6302}
train stats after 201696 examples: {'rewards_train/chosen': '0.14672', 'rewards_train/rejected': '0.034038', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11268', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-102.89', 'loss/train': '0.65259', 'examples_per_second': '31.482', 'grad_norm': '17.625', 'counters/examples': 201696, 'counters/updates': 6303}
train stats after 201728 examples: {'rewards_train/chosen': '0.15328', 'rewards_train/rejected': '0.039716', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11356', 'logps_train/rejected': '-105.48', 'logps_train/chosen': '-133.91', 'loss/train': '0.65422', 'examples_per_second': '31.975', 'grad_norm': '19.5', 'counters/examples': 201728, 'counters/updates': 6304}
train stats after 201760 examples: {'rewards_train/chosen': '0.080336', 'rewards_train/rejected': '0.054999', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025336', 'logps_train/rejected': '-93.823', 'logps_train/chosen': '-112.36', 'loss/train': '0.70169', 'examples_per_second': '31.532', 'grad_norm': '19.125', 'counters/examples': 201760, 'counters/updates': 6305}
skipping logging after 201792 examples to avoid logging too frequently
train stats after 201824 examples: {'rewards_train/chosen': '0.11862', 'rewards_train/rejected': '0.14103', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022417', 'logps_train/rejected': '-112.12', 'logps_train/chosen': '-147.28', 'loss/train': '0.71255', 'examples_per_second': '32.533', 'grad_norm': '21.875', 'counters/examples': 201824, 'counters/updates': 6307}
train stats after 201856 examples: {'rewards_train/chosen': '0.19659', 'rewards_train/rejected': '0.02501', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17158', 'logps_train/rejected': '-102.71', 'logps_train/chosen': '-129.9', 'loss/train': '0.63171', 'examples_per_second': '30.157', 'grad_norm': '17.375', 'counters/examples': 201856, 'counters/updates': 6308}
train stats after 201888 examples: {'rewards_train/chosen': '0.11094', 'rewards_train/rejected': '0.08115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029793', 'logps_train/rejected': '-92.766', 'logps_train/chosen': '-131.37', 'loss/train': '0.6852', 'examples_per_second': '30.729', 'grad_norm': '19.25', 'counters/examples': 201888, 'counters/updates': 6309}
train stats after 201920 examples: {'rewards_train/chosen': '0.2921', 'rewards_train/rejected': '0.085171', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20693', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-147.11', 'loss/train': '0.60989', 'examples_per_second': '32.492', 'grad_norm': '20.625', 'counters/examples': 201920, 'counters/updates': 6310}
train stats after 201952 examples: {'rewards_train/chosen': '0.26776', 'rewards_train/rejected': '0.0092227', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.25854', 'logps_train/rejected': '-95.139', 'logps_train/chosen': '-144.55', 'loss/train': '0.59083', 'examples_per_second': '32.508', 'grad_norm': '18.625', 'counters/examples': 201952, 'counters/updates': 6311}
train stats after 201984 examples: {'rewards_train/chosen': '0.14731', 'rewards_train/rejected': '0.053212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094096', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-126.41', 'loss/train': '0.65922', 'examples_per_second': '31.641', 'grad_norm': '19.625', 'counters/examples': 201984, 'counters/updates': 6312}
skipping logging after 202016 examples to avoid logging too frequently
train stats after 202048 examples: {'rewards_train/chosen': '0.17452', 'rewards_train/rejected': '0.024374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15014', 'logps_train/rejected': '-98.641', 'logps_train/chosen': '-100.19', 'loss/train': '0.63597', 'examples_per_second': '32.384', 'grad_norm': '16.75', 'counters/examples': 202048, 'counters/updates': 6314}
train stats after 202080 examples: {'rewards_train/chosen': '0.12416', 'rewards_train/rejected': '0.03826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085902', 'logps_train/rejected': '-169.03', 'logps_train/chosen': '-140.33', 'loss/train': '0.66948', 'examples_per_second': '31.647', 'grad_norm': '23.875', 'counters/examples': 202080, 'counters/updates': 6315}
train stats after 202112 examples: {'rewards_train/chosen': '0.07214', 'rewards_train/rejected': '0.088878', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016738', 'logps_train/rejected': '-120.57', 'logps_train/chosen': '-120.94', 'loss/train': '0.71421', 'examples_per_second': '31.625', 'grad_norm': '22.625', 'counters/examples': 202112, 'counters/updates': 6316}
skipping logging after 202144 examples to avoid logging too frequently
train stats after 202176 examples: {'rewards_train/chosen': '0.11807', 'rewards_train/rejected': '0.011595', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10647', 'logps_train/rejected': '-93.643', 'logps_train/chosen': '-102.78', 'loss/train': '0.65097', 'examples_per_second': '31.55', 'grad_norm': '17.5', 'counters/examples': 202176, 'counters/updates': 6318}
train stats after 202208 examples: {'rewards_train/chosen': '0.16377', 'rewards_train/rejected': '0.064502', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099266', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-128.3', 'loss/train': '0.65542', 'examples_per_second': '30.281', 'grad_norm': '19.75', 'counters/examples': 202208, 'counters/updates': 6319}
train stats after 202240 examples: {'rewards_train/chosen': '0.17751', 'rewards_train/rejected': '0.034193', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14331', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-123.11', 'loss/train': '0.64031', 'examples_per_second': '31.531', 'grad_norm': '18.875', 'counters/examples': 202240, 'counters/updates': 6320}
train stats after 202272 examples: {'rewards_train/chosen': '0.18516', 'rewards_train/rejected': '0.091592', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093564', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-148.88', 'loss/train': '0.65976', 'examples_per_second': '32.235', 'grad_norm': '20.5', 'counters/examples': 202272, 'counters/updates': 6321}
train stats after 202304 examples: {'rewards_train/chosen': '0.19688', 'rewards_train/rejected': '0.063875', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13301', 'logps_train/rejected': '-170.99', 'logps_train/chosen': '-177.65', 'loss/train': '0.6462', 'examples_per_second': '31.554', 'grad_norm': '22.75', 'counters/examples': 202304, 'counters/updates': 6322}
train stats after 202336 examples: {'rewards_train/chosen': '0.23389', 'rewards_train/rejected': '0.10742', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12647', 'logps_train/rejected': '-138.28', 'logps_train/chosen': '-164', 'loss/train': '0.65211', 'examples_per_second': '24.516', 'grad_norm': '20.875', 'counters/examples': 202336, 'counters/updates': 6323}
train stats after 202368 examples: {'rewards_train/chosen': '0.15039', 'rewards_train/rejected': '0.070387', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080006', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-142.83', 'loss/train': '0.67101', 'examples_per_second': '32.641', 'grad_norm': '21.375', 'counters/examples': 202368, 'counters/updates': 6324}
skipping logging after 202400 examples to avoid logging too frequently
train stats after 202432 examples: {'rewards_train/chosen': '0.13667', 'rewards_train/rejected': '-0.015683', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15236', 'logps_train/rejected': '-96.708', 'logps_train/chosen': '-136.04', 'loss/train': '0.62891', 'examples_per_second': '23.847', 'grad_norm': '18.125', 'counters/examples': 202432, 'counters/updates': 6326}
skipping logging after 202464 examples to avoid logging too frequently
train stats after 202496 examples: {'rewards_train/chosen': '0.05501', 'rewards_train/rejected': '0.05726', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0022504', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-123.02', 'loss/train': '0.71789', 'examples_per_second': '33.019', 'grad_norm': '21.875', 'counters/examples': 202496, 'counters/updates': 6328}
train stats after 202528 examples: {'rewards_train/chosen': '0.1707', 'rewards_train/rejected': '0.045713', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12499', 'logps_train/rejected': '-99.346', 'logps_train/chosen': '-127.39', 'loss/train': '0.6523', 'examples_per_second': '32.386', 'grad_norm': '19.375', 'counters/examples': 202528, 'counters/updates': 6329}
train stats after 202560 examples: {'rewards_train/chosen': '0.15298', 'rewards_train/rejected': '-0.047712', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20069', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-130.73', 'loss/train': '0.61114', 'examples_per_second': '31.601', 'grad_norm': '19.375', 'counters/examples': 202560, 'counters/updates': 6330}
skipping logging after 202592 examples to avoid logging too frequently
train stats after 202624 examples: {'rewards_train/chosen': '0.13763', 'rewards_train/rejected': '0.10749', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030135', 'logps_train/rejected': '-159.32', 'logps_train/chosen': '-124.27', 'loss/train': '0.69398', 'examples_per_second': '32.076', 'grad_norm': '20.625', 'counters/examples': 202624, 'counters/updates': 6332}
train stats after 202656 examples: {'rewards_train/chosen': '0.13332', 'rewards_train/rejected': '0.02727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10605', 'logps_train/rejected': '-103.15', 'logps_train/chosen': '-105.18', 'loss/train': '0.65299', 'examples_per_second': '32.58', 'grad_norm': '17.875', 'counters/examples': 202656, 'counters/updates': 6333}
train stats after 202688 examples: {'rewards_train/chosen': '0.13147', 'rewards_train/rejected': '0.0068089', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12466', 'logps_train/rejected': '-115.7', 'logps_train/chosen': '-134.29', 'loss/train': '0.64588', 'examples_per_second': '31.65', 'grad_norm': '19.625', 'counters/examples': 202688, 'counters/updates': 6334}
train stats after 202720 examples: {'rewards_train/chosen': '0.21095', 'rewards_train/rejected': '0.086048', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12491', 'logps_train/rejected': '-96.773', 'logps_train/chosen': '-120.69', 'loss/train': '0.64463', 'examples_per_second': '32.042', 'grad_norm': '18', 'counters/examples': 202720, 'counters/updates': 6335}
train stats after 202752 examples: {'rewards_train/chosen': '0.098999', 'rewards_train/rejected': '0.0070804', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091919', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-136.48', 'loss/train': '0.66399', 'examples_per_second': '31.574', 'grad_norm': '20.375', 'counters/examples': 202752, 'counters/updates': 6336}
train stats after 202784 examples: {'rewards_train/chosen': '0.11445', 'rewards_train/rejected': '0.16738', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052938', 'logps_train/rejected': '-149.61', 'logps_train/chosen': '-136.33', 'loss/train': '0.7279', 'examples_per_second': '31.581', 'grad_norm': '24.5', 'counters/examples': 202784, 'counters/updates': 6337}
train stats after 202816 examples: {'rewards_train/chosen': '0.20705', 'rewards_train/rejected': '0.018843', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18821', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-151.24', 'loss/train': '0.62617', 'examples_per_second': '32.658', 'grad_norm': '19.25', 'counters/examples': 202816, 'counters/updates': 6338}
skipping logging after 202848 examples to avoid logging too frequently
train stats after 202880 examples: {'rewards_train/chosen': '0.15853', 'rewards_train/rejected': '0.056011', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10252', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-127.65', 'loss/train': '0.64877', 'examples_per_second': '33.859', 'grad_norm': '19.75', 'counters/examples': 202880, 'counters/updates': 6340}
skipping logging after 202912 examples to avoid logging too frequently
train stats after 202944 examples: {'rewards_train/chosen': '0.11531', 'rewards_train/rejected': '0.01925', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.09606', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-142.18', 'loss/train': '0.67104', 'examples_per_second': '30.951', 'grad_norm': '22.25', 'counters/examples': 202944, 'counters/updates': 6342}
train stats after 202976 examples: {'rewards_train/chosen': '0.14049', 'rewards_train/rejected': '-0.0068654', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14735', 'logps_train/rejected': '-106.01', 'logps_train/chosen': '-134', 'loss/train': '0.64197', 'examples_per_second': '31.602', 'grad_norm': '18.75', 'counters/examples': 202976, 'counters/updates': 6343}
train stats after 203008 examples: {'rewards_train/chosen': '0.047857', 'rewards_train/rejected': '-0.020503', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068361', 'logps_train/rejected': '-110.78', 'logps_train/chosen': '-133.27', 'loss/train': '0.68691', 'examples_per_second': '32.728', 'grad_norm': '20.625', 'counters/examples': 203008, 'counters/updates': 6344}
train stats after 203040 examples: {'rewards_train/chosen': '0.2146', 'rewards_train/rejected': '0.064384', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15022', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-157.89', 'loss/train': '0.63849', 'examples_per_second': '30.097', 'grad_norm': '20.25', 'counters/examples': 203040, 'counters/updates': 6345}
train stats after 203072 examples: {'rewards_train/chosen': '0.072827', 'rewards_train/rejected': '0.003586', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069241', 'logps_train/rejected': '-98.615', 'logps_train/chosen': '-127.36', 'loss/train': '0.67235', 'examples_per_second': '30.644', 'grad_norm': '19.25', 'counters/examples': 203072, 'counters/updates': 6346}
train stats after 203104 examples: {'rewards_train/chosen': '0.12931', 'rewards_train/rejected': '0.037341', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091971', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-114.39', 'loss/train': '0.66112', 'examples_per_second': '30.847', 'grad_norm': '19.75', 'counters/examples': 203104, 'counters/updates': 6347}
train stats after 203136 examples: {'rewards_train/chosen': '0.18571', 'rewards_train/rejected': '0.077772', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10794', 'logps_train/rejected': '-137.4', 'logps_train/chosen': '-161.8', 'loss/train': '0.6611', 'examples_per_second': '32.713', 'grad_norm': '21.375', 'counters/examples': 203136, 'counters/updates': 6348}
train stats after 203168 examples: {'rewards_train/chosen': '0.12759', 'rewards_train/rejected': '-0.10186', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22944', 'logps_train/rejected': '-74.926', 'logps_train/chosen': '-101.68', 'loss/train': '0.59373', 'examples_per_second': '31.189', 'grad_norm': '15.062', 'counters/examples': 203168, 'counters/updates': 6349}
train stats after 203200 examples: {'rewards_train/chosen': '0.18917', 'rewards_train/rejected': '0.045804', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14337', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-127.18', 'loss/train': '0.63254', 'examples_per_second': '31.782', 'grad_norm': '18.75', 'counters/examples': 203200, 'counters/updates': 6350}
train stats after 203232 examples: {'rewards_train/chosen': '0.21517', 'rewards_train/rejected': '0.061595', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15357', 'logps_train/rejected': '-174.15', 'logps_train/chosen': '-161.38', 'loss/train': '0.63522', 'examples_per_second': '31.464', 'grad_norm': '22.5', 'counters/examples': 203232, 'counters/updates': 6351}
train stats after 203264 examples: {'rewards_train/chosen': '0.18652', 'rewards_train/rejected': '0.055267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13125', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-146.08', 'loss/train': '0.64254', 'examples_per_second': '30.637', 'grad_norm': '21.125', 'counters/examples': 203264, 'counters/updates': 6352}
skipping logging after 203296 examples to avoid logging too frequently
train stats after 203328 examples: {'rewards_train/chosen': '0.076286', 'rewards_train/rejected': '-0.019965', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096251', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-124.27', 'loss/train': '0.65646', 'examples_per_second': '30.828', 'grad_norm': '17.75', 'counters/examples': 203328, 'counters/updates': 6354}
train stats after 203360 examples: {'rewards_train/chosen': '0.074353', 'rewards_train/rejected': '-0.017088', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091441', 'logps_train/rejected': '-112.91', 'logps_train/chosen': '-122.44', 'loss/train': '0.65554', 'examples_per_second': '31.59', 'grad_norm': '19.75', 'counters/examples': 203360, 'counters/updates': 6355}
train stats after 203392 examples: {'rewards_train/chosen': '0.15924', 'rewards_train/rejected': '0.042358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11688', 'logps_train/rejected': '-104.33', 'logps_train/chosen': '-113.28', 'loss/train': '0.65578', 'examples_per_second': '31.225', 'grad_norm': '18', 'counters/examples': 203392, 'counters/updates': 6356}
train stats after 203424 examples: {'rewards_train/chosen': '0.17697', 'rewards_train/rejected': '0.04013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13684', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-154.56', 'loss/train': '0.64054', 'examples_per_second': '31.948', 'grad_norm': '20.5', 'counters/examples': 203424, 'counters/updates': 6357}
skipping logging after 203456 examples to avoid logging too frequently
train stats after 203488 examples: {'rewards_train/chosen': '0.19463', 'rewards_train/rejected': '0.068057', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12657', 'logps_train/rejected': '-111.72', 'logps_train/chosen': '-163.21', 'loss/train': '0.65353', 'examples_per_second': '31.536', 'grad_norm': '20.125', 'counters/examples': 203488, 'counters/updates': 6359}
train stats after 203520 examples: {'rewards_train/chosen': '0.031625', 'rewards_train/rejected': '-0.051298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082924', 'logps_train/rejected': '-101.1', 'logps_train/chosen': '-116.69', 'loss/train': '0.66759', 'examples_per_second': '30.157', 'grad_norm': '19.375', 'counters/examples': 203520, 'counters/updates': 6360}
train stats after 203552 examples: {'rewards_train/chosen': '0.24534', 'rewards_train/rejected': '0.020295', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22505', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-131.7', 'loss/train': '0.60597', 'examples_per_second': '29.695', 'grad_norm': '19.25', 'counters/examples': 203552, 'counters/updates': 6361}
train stats after 203584 examples: {'rewards_train/chosen': '0.16101', 'rewards_train/rejected': '0.17221', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011198', 'logps_train/rejected': '-144.57', 'logps_train/chosen': '-176.23', 'loss/train': '0.71565', 'examples_per_second': '30.457', 'grad_norm': '23.25', 'counters/examples': 203584, 'counters/updates': 6362}
train stats after 203616 examples: {'rewards_train/chosen': '0.032674', 'rewards_train/rejected': '0.056351', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.023676', 'logps_train/rejected': '-153.19', 'logps_train/chosen': '-148.78', 'loss/train': '0.71726', 'examples_per_second': '31.624', 'grad_norm': '23.5', 'counters/examples': 203616, 'counters/updates': 6363}
train stats after 203648 examples: {'rewards_train/chosen': '0.12217', 'rewards_train/rejected': '0.047723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074448', 'logps_train/rejected': '-109.75', 'logps_train/chosen': '-137.57', 'loss/train': '0.67132', 'examples_per_second': '31.57', 'grad_norm': '20.125', 'counters/examples': 203648, 'counters/updates': 6364}
train stats after 203680 examples: {'rewards_train/chosen': '0.14944', 'rewards_train/rejected': '0.015086', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13435', 'logps_train/rejected': '-158.27', 'logps_train/chosen': '-123.07', 'loss/train': '0.64869', 'examples_per_second': '30.083', 'grad_norm': '22.125', 'counters/examples': 203680, 'counters/updates': 6365}
skipping logging after 203712 examples to avoid logging too frequently
train stats after 203744 examples: {'rewards_train/chosen': '0.029849', 'rewards_train/rejected': '0.044261', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014412', 'logps_train/rejected': '-98.754', 'logps_train/chosen': '-119.09', 'loss/train': '0.71876', 'examples_per_second': '34.037', 'grad_norm': '20.875', 'counters/examples': 203744, 'counters/updates': 6367}
skipping logging after 203776 examples to avoid logging too frequently
train stats after 203808 examples: {'rewards_train/chosen': '0.14694', 'rewards_train/rejected': '0.0035155', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14342', 'logps_train/rejected': '-109.49', 'logps_train/chosen': '-132.04', 'loss/train': '0.63981', 'examples_per_second': '39.21', 'grad_norm': '18.25', 'counters/examples': 203808, 'counters/updates': 6369}
train stats after 203840 examples: {'rewards_train/chosen': '0.24254', 'rewards_train/rejected': '0.12472', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11781', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-121.09', 'loss/train': '0.64443', 'examples_per_second': '31.043', 'grad_norm': '19', 'counters/examples': 203840, 'counters/updates': 6370}
train stats after 203872 examples: {'rewards_train/chosen': '0.25868', 'rewards_train/rejected': '-0.012091', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27077', 'logps_train/rejected': '-124', 'logps_train/chosen': '-147.63', 'loss/train': '0.5811', 'examples_per_second': '32.492', 'grad_norm': '19.375', 'counters/examples': 203872, 'counters/updates': 6371}
train stats after 203904 examples: {'rewards_train/chosen': '0.15628', 'rewards_train/rejected': '0.11826', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038025', 'logps_train/rejected': '-125.39', 'logps_train/chosen': '-150.95', 'loss/train': '0.69081', 'examples_per_second': '31.604', 'grad_norm': '21.625', 'counters/examples': 203904, 'counters/updates': 6372}
train stats after 203936 examples: {'rewards_train/chosen': '0.056649', 'rewards_train/rejected': '0.022678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033971', 'logps_train/rejected': '-102.48', 'logps_train/chosen': '-93.896', 'loss/train': '0.69698', 'examples_per_second': '32.593', 'grad_norm': '19', 'counters/examples': 203936, 'counters/updates': 6373}
skipping logging after 203968 examples to avoid logging too frequently
train stats after 204000 examples: {'rewards_train/chosen': '0.12625', 'rewards_train/rejected': '0.037702', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088544', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-120.73', 'loss/train': '0.66562', 'examples_per_second': '31.404', 'grad_norm': '20.25', 'counters/examples': 204000, 'counters/updates': 6375}
Running evaluation after 204000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.73it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.88it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.81it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 204000: {'rewards_eval/chosen': '0.17872', 'rewards_eval/rejected': '0.042795', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.13592', 'logps_eval/rejected': '-114.96', 'logps_eval/chosen': '-133.76', 'loss/eval': '0.64583'}
skipping save for non epoch
train stats after 204032 examples: {'rewards_train/chosen': '0.18098', 'rewards_train/rejected': '0.023183', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1578', 'logps_train/rejected': '-112.9', 'logps_train/chosen': '-160.3', 'loss/train': '0.62946', 'examples_per_second': '34.073', 'grad_norm': '20.375', 'counters/examples': 204032, 'counters/updates': 6376}
train stats after 204064 examples: {'rewards_train/chosen': '0.099204', 'rewards_train/rejected': '0.07786', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021344', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-119.44', 'loss/train': '0.69804', 'examples_per_second': '31.729', 'grad_norm': '22.625', 'counters/examples': 204064, 'counters/updates': 6377}
train stats after 204096 examples: {'rewards_train/chosen': '0.25289', 'rewards_train/rejected': '0.14209', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11079', 'logps_train/rejected': '-118.06', 'logps_train/chosen': '-128.37', 'loss/train': '0.65696', 'examples_per_second': '31.831', 'grad_norm': '19.625', 'counters/examples': 204096, 'counters/updates': 6378}
train stats after 204128 examples: {'rewards_train/chosen': '0.12014', 'rewards_train/rejected': '0.072263', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04788', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-123.18', 'loss/train': '0.68155', 'examples_per_second': '32.833', 'grad_norm': '22', 'counters/examples': 204128, 'counters/updates': 6379}
skipping logging after 204160 examples to avoid logging too frequently
train stats after 204192 examples: {'rewards_train/chosen': '0.07127', 'rewards_train/rejected': '0.0096353', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061635', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-114.54', 'loss/train': '0.68229', 'examples_per_second': '31.551', 'grad_norm': '18.75', 'counters/examples': 204192, 'counters/updates': 6381}
train stats after 204224 examples: {'rewards_train/chosen': '0.18535', 'rewards_train/rejected': '-0.036884', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22223', 'logps_train/rejected': '-100.12', 'logps_train/chosen': '-121.85', 'loss/train': '0.60408', 'examples_per_second': '30.676', 'grad_norm': '17.5', 'counters/examples': 204224, 'counters/updates': 6382}
train stats after 204256 examples: {'rewards_train/chosen': '0.19833', 'rewards_train/rejected': '0.06051', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13782', 'logps_train/rejected': '-144.84', 'logps_train/chosen': '-160.46', 'loss/train': '0.64409', 'examples_per_second': '33.153', 'grad_norm': '21.5', 'counters/examples': 204256, 'counters/updates': 6383}
train stats after 204288 examples: {'rewards_train/chosen': '0.19433', 'rewards_train/rejected': '0.007649', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18668', 'logps_train/rejected': '-149.95', 'logps_train/chosen': '-139.27', 'loss/train': '0.62835', 'examples_per_second': '32.169', 'grad_norm': '20.875', 'counters/examples': 204288, 'counters/updates': 6384}
train stats after 204320 examples: {'rewards_train/chosen': '0.18298', 'rewards_train/rejected': '0.041297', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14168', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-148.83', 'loss/train': '0.63857', 'examples_per_second': '31.515', 'grad_norm': '21.25', 'counters/examples': 204320, 'counters/updates': 6385}
train stats after 204352 examples: {'rewards_train/chosen': '0.22012', 'rewards_train/rejected': '0.098316', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1218', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-158.7', 'loss/train': '0.65045', 'examples_per_second': '31.284', 'grad_norm': '20.75', 'counters/examples': 204352, 'counters/updates': 6386}
skipping logging after 204384 examples to avoid logging too frequently
train stats after 204416 examples: {'rewards_train/chosen': '0.13298', 'rewards_train/rejected': '0.090894', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042084', 'logps_train/rejected': '-142.23', 'logps_train/chosen': '-154.59', 'loss/train': '0.69323', 'examples_per_second': '30.207', 'grad_norm': '22.125', 'counters/examples': 204416, 'counters/updates': 6388}
train stats after 204448 examples: {'rewards_train/chosen': '0.071615', 'rewards_train/rejected': '0.093359', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021743', 'logps_train/rejected': '-171.32', 'logps_train/chosen': '-151.68', 'loss/train': '0.71743', 'examples_per_second': '30.627', 'grad_norm': '24.5', 'counters/examples': 204448, 'counters/updates': 6389}
train stats after 204480 examples: {'rewards_train/chosen': '0.23705', 'rewards_train/rejected': '0.074626', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16242', 'logps_train/rejected': '-130.16', 'logps_train/chosen': '-152.42', 'loss/train': '0.63172', 'examples_per_second': '31.952', 'grad_norm': '20.625', 'counters/examples': 204480, 'counters/updates': 6390}
train stats after 204512 examples: {'rewards_train/chosen': '0.16767', 'rewards_train/rejected': '0.06985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097823', 'logps_train/rejected': '-150.01', 'logps_train/chosen': '-120.11', 'loss/train': '0.65886', 'examples_per_second': '31.136', 'grad_norm': '20.125', 'counters/examples': 204512, 'counters/updates': 6391}
train stats after 204544 examples: {'rewards_train/chosen': '0.19095', 'rewards_train/rejected': '0.070371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12058', 'logps_train/rejected': '-108.77', 'logps_train/chosen': '-110.74', 'loss/train': '0.65038', 'examples_per_second': '30.689', 'grad_norm': '18.125', 'counters/examples': 204544, 'counters/updates': 6392}
skipping logging after 204576 examples to avoid logging too frequently
train stats after 204608 examples: {'rewards_train/chosen': '0.15045', 'rewards_train/rejected': '0.067634', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082815', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-150.62', 'loss/train': '0.66451', 'examples_per_second': '31.54', 'grad_norm': '20.125', 'counters/examples': 204608, 'counters/updates': 6394}
train stats after 204640 examples: {'rewards_train/chosen': '0.16621', 'rewards_train/rejected': '0.060713', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10549', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-127.49', 'loss/train': '0.65549', 'examples_per_second': '31.376', 'grad_norm': '20.25', 'counters/examples': 204640, 'counters/updates': 6395}
train stats after 204672 examples: {'rewards_train/chosen': '0.088203', 'rewards_train/rejected': '0.050594', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037608', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-103.81', 'loss/train': '0.68199', 'examples_per_second': '31.651', 'grad_norm': '18.875', 'counters/examples': 204672, 'counters/updates': 6396}
train stats after 204704 examples: {'rewards_train/chosen': '0.14118', 'rewards_train/rejected': '0.041465', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099717', 'logps_train/rejected': '-142.97', 'logps_train/chosen': '-159.67', 'loss/train': '0.65488', 'examples_per_second': '31.657', 'grad_norm': '22.5', 'counters/examples': 204704, 'counters/updates': 6397}
train stats after 204736 examples: {'rewards_train/chosen': '0.24378', 'rewards_train/rejected': '0.028524', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21526', 'logps_train/rejected': '-128.43', 'logps_train/chosen': '-140.39', 'loss/train': '0.60835', 'examples_per_second': '30.469', 'grad_norm': '19.5', 'counters/examples': 204736, 'counters/updates': 6398}
skipping logging after 204768 examples to avoid logging too frequently
train stats after 204800 examples: {'rewards_train/chosen': '0.12508', 'rewards_train/rejected': '0.16435', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039262', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-130.48', 'loss/train': '0.73698', 'examples_per_second': '31.784', 'grad_norm': '21.625', 'counters/examples': 204800, 'counters/updates': 6400}
train stats after 204832 examples: {'rewards_train/chosen': '0.16207', 'rewards_train/rejected': '0.18074', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018674', 'logps_train/rejected': '-154.2', 'logps_train/chosen': '-161.82', 'loss/train': '0.71446', 'examples_per_second': '31.618', 'grad_norm': '24.625', 'counters/examples': 204832, 'counters/updates': 6401}
train stats after 204864 examples: {'rewards_train/chosen': '0.14183', 'rewards_train/rejected': '0.057685', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.084149', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-146.77', 'loss/train': '0.66324', 'examples_per_second': '31.599', 'grad_norm': '20.25', 'counters/examples': 204864, 'counters/updates': 6402}
train stats after 204896 examples: {'rewards_train/chosen': '0.096695', 'rewards_train/rejected': '0.081164', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015532', 'logps_train/rejected': '-95.313', 'logps_train/chosen': '-149.32', 'loss/train': '0.69657', 'examples_per_second': '30.582', 'grad_norm': '20.375', 'counters/examples': 204896, 'counters/updates': 6403}
train stats after 204928 examples: {'rewards_train/chosen': '0.2059', 'rewards_train/rejected': '-0.0095598', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21546', 'logps_train/rejected': '-97.369', 'logps_train/chosen': '-114.27', 'loss/train': '0.60892', 'examples_per_second': '31.155', 'grad_norm': '17.375', 'counters/examples': 204928, 'counters/updates': 6404}
train stats after 204960 examples: {'rewards_train/chosen': '0.17604', 'rewards_train/rejected': '0.0093373', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1667', 'logps_train/rejected': '-117.05', 'logps_train/chosen': '-172.48', 'loss/train': '0.63197', 'examples_per_second': '31.805', 'grad_norm': '21.75', 'counters/examples': 204960, 'counters/updates': 6405}
train stats after 204992 examples: {'rewards_train/chosen': '0.21383', 'rewards_train/rejected': '0.013602', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20022', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-146.38', 'loss/train': '0.61428', 'examples_per_second': '31.679', 'grad_norm': '20.5', 'counters/examples': 204992, 'counters/updates': 6406}
train stats after 205024 examples: {'rewards_train/chosen': '0.16088', 'rewards_train/rejected': '0.11161', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049268', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-136.53', 'loss/train': '0.6807', 'examples_per_second': '30.019', 'grad_norm': '22.625', 'counters/examples': 205024, 'counters/updates': 6407}
train stats after 205056 examples: {'rewards_train/chosen': '0.10225', 'rewards_train/rejected': '0.0069981', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095255', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-134.11', 'loss/train': '0.65746', 'examples_per_second': '32.174', 'grad_norm': '20.125', 'counters/examples': 205056, 'counters/updates': 6408}
train stats after 205088 examples: {'rewards_train/chosen': '0.15698', 'rewards_train/rejected': '0.0099243', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14706', 'logps_train/rejected': '-120.42', 'logps_train/chosen': '-137.39', 'loss/train': '0.63666', 'examples_per_second': '32.78', 'grad_norm': '19.25', 'counters/examples': 205088, 'counters/updates': 6409}
train stats after 205120 examples: {'rewards_train/chosen': '0.12163', 'rewards_train/rejected': '0.023432', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.098202', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-128.23', 'loss/train': '0.65873', 'examples_per_second': '31.238', 'grad_norm': '18.5', 'counters/examples': 205120, 'counters/updates': 6410}
skipping logging after 205152 examples to avoid logging too frequently
train stats after 205184 examples: {'rewards_train/chosen': '0.10751', 'rewards_train/rejected': '0.019746', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087767', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-139.56', 'loss/train': '0.65908', 'examples_per_second': '31.562', 'grad_norm': '21.875', 'counters/examples': 205184, 'counters/updates': 6412}
train stats after 205216 examples: {'rewards_train/chosen': '0.18327', 'rewards_train/rejected': '0.040202', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14307', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-123.95', 'loss/train': '0.63549', 'examples_per_second': '30.859', 'grad_norm': '19.125', 'counters/examples': 205216, 'counters/updates': 6413}
train stats after 205248 examples: {'rewards_train/chosen': '0.21349', 'rewards_train/rejected': '0.022972', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19052', 'logps_train/rejected': '-121.32', 'logps_train/chosen': '-148.71', 'loss/train': '0.62454', 'examples_per_second': '31.518', 'grad_norm': '18.75', 'counters/examples': 205248, 'counters/updates': 6414}
train stats after 205280 examples: {'rewards_train/chosen': '0.24181', 'rewards_train/rejected': '0.13173', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11008', 'logps_train/rejected': '-148.56', 'logps_train/chosen': '-144.57', 'loss/train': '0.65119', 'examples_per_second': '32.114', 'grad_norm': '22.125', 'counters/examples': 205280, 'counters/updates': 6415}
train stats after 205312 examples: {'rewards_train/chosen': '0.16599', 'rewards_train/rejected': '0.027331', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13866', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-149.56', 'loss/train': '0.63499', 'examples_per_second': '31.674', 'grad_norm': '18.875', 'counters/examples': 205312, 'counters/updates': 6416}
train stats after 205344 examples: {'rewards_train/chosen': '0.15349', 'rewards_train/rejected': '-0.017749', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17124', 'logps_train/rejected': '-116.27', 'logps_train/chosen': '-165.04', 'loss/train': '0.63479', 'examples_per_second': '31.778', 'grad_norm': '20.875', 'counters/examples': 205344, 'counters/updates': 6417}
train stats after 205376 examples: {'rewards_train/chosen': '0.1582', 'rewards_train/rejected': '0.0036752', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15453', 'logps_train/rejected': '-104.19', 'logps_train/chosen': '-150.84', 'loss/train': '0.64291', 'examples_per_second': '31.974', 'grad_norm': '20.125', 'counters/examples': 205376, 'counters/updates': 6418}
train stats after 205408 examples: {'rewards_train/chosen': '0.16886', 'rewards_train/rejected': '0.11095', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057906', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-141.57', 'loss/train': '0.68541', 'examples_per_second': '30.098', 'grad_norm': '21.375', 'counters/examples': 205408, 'counters/updates': 6419}
train stats after 205440 examples: {'rewards_train/chosen': '0.23873', 'rewards_train/rejected': '0.12775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11098', 'logps_train/rejected': '-92.637', 'logps_train/chosen': '-130.13', 'loss/train': '0.6534', 'examples_per_second': '31.866', 'grad_norm': '19.125', 'counters/examples': 205440, 'counters/updates': 6420}
train stats after 205472 examples: {'rewards_train/chosen': '0.20124', 'rewards_train/rejected': '0.063557', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13769', 'logps_train/rejected': '-104.63', 'logps_train/chosen': '-111.42', 'loss/train': '0.64209', 'examples_per_second': '31.671', 'grad_norm': '17.625', 'counters/examples': 205472, 'counters/updates': 6421}
train stats after 205504 examples: {'rewards_train/chosen': '0.20582', 'rewards_train/rejected': '0.14769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058131', 'logps_train/rejected': '-151.25', 'logps_train/chosen': '-167.86', 'loss/train': '0.68417', 'examples_per_second': '30.47', 'grad_norm': '23.5', 'counters/examples': 205504, 'counters/updates': 6422}
skipping logging after 205536 examples to avoid logging too frequently
train stats after 205568 examples: {'rewards_train/chosen': '0.16742', 'rewards_train/rejected': '0.091334', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07609', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-140.81', 'loss/train': '0.66726', 'examples_per_second': '31.705', 'grad_norm': '20.5', 'counters/examples': 205568, 'counters/updates': 6424}
train stats after 205600 examples: {'rewards_train/chosen': '0.27232', 'rewards_train/rejected': '0.11506', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15727', 'logps_train/rejected': '-133.29', 'logps_train/chosen': '-150.61', 'loss/train': '0.63709', 'examples_per_second': '30.302', 'grad_norm': '20', 'counters/examples': 205600, 'counters/updates': 6425}
train stats after 205632 examples: {'rewards_train/chosen': '0.16165', 'rewards_train/rejected': '0.060938', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10071', 'logps_train/rejected': '-143.04', 'logps_train/chosen': '-131.32', 'loss/train': '0.65983', 'examples_per_second': '30.609', 'grad_norm': '19.875', 'counters/examples': 205632, 'counters/updates': 6426}
train stats after 205664 examples: {'rewards_train/chosen': '0.10764', 'rewards_train/rejected': '0.058572', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049072', 'logps_train/rejected': '-137.35', 'logps_train/chosen': '-148.58', 'loss/train': '0.69082', 'examples_per_second': '31.684', 'grad_norm': '22', 'counters/examples': 205664, 'counters/updates': 6427}
train stats after 205696 examples: {'rewards_train/chosen': '0.16476', 'rewards_train/rejected': '0.0211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14366', 'logps_train/rejected': '-84.789', 'logps_train/chosen': '-112.03', 'loss/train': '0.6435', 'examples_per_second': '30.286', 'grad_norm': '16.625', 'counters/examples': 205696, 'counters/updates': 6428}
train stats after 205728 examples: {'rewards_train/chosen': '0.15206', 'rewards_train/rejected': '0.083839', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06822', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-130.73', 'loss/train': '0.66955', 'examples_per_second': '30.079', 'grad_norm': '21.375', 'counters/examples': 205728, 'counters/updates': 6429}
train stats after 205760 examples: {'rewards_train/chosen': '0.2024', 'rewards_train/rejected': '0.0022803', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20012', 'logps_train/rejected': '-120.32', 'logps_train/chosen': '-140.96', 'loss/train': '0.61474', 'examples_per_second': '31.672', 'grad_norm': '18.5', 'counters/examples': 205760, 'counters/updates': 6430}
skipping logging after 205792 examples to avoid logging too frequently
train stats after 205824 examples: {'rewards_train/chosen': '0.23481', 'rewards_train/rejected': '0.071101', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16371', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-156.27', 'loss/train': '0.63702', 'examples_per_second': '30.837', 'grad_norm': '22.125', 'counters/examples': 205824, 'counters/updates': 6432}
train stats after 205856 examples: {'rewards_train/chosen': '0.14511', 'rewards_train/rejected': '0.032274', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11284', 'logps_train/rejected': '-108.48', 'logps_train/chosen': '-126.92', 'loss/train': '0.64681', 'examples_per_second': '31.126', 'grad_norm': '18.75', 'counters/examples': 205856, 'counters/updates': 6433}
train stats after 205888 examples: {'rewards_train/chosen': '0.1781', 'rewards_train/rejected': '0.031878', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14622', 'logps_train/rejected': '-109.45', 'logps_train/chosen': '-137.05', 'loss/train': '0.63565', 'examples_per_second': '30.981', 'grad_norm': '19.125', 'counters/examples': 205888, 'counters/updates': 6434}
train stats after 205920 examples: {'rewards_train/chosen': '0.25085', 'rewards_train/rejected': '0.15764', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093202', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-115.76', 'loss/train': '0.66603', 'examples_per_second': '32.219', 'grad_norm': '19.625', 'counters/examples': 205920, 'counters/updates': 6435}
train stats after 205952 examples: {'rewards_train/chosen': '0.2338', 'rewards_train/rejected': '-0.060698', 'rewards_train/accuracies': '0.96875', 'rewards_train/margins': '0.2945', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-153.72', 'loss/train': '0.56769', 'examples_per_second': '32.879', 'grad_norm': '18.5', 'counters/examples': 205952, 'counters/updates': 6436}
train stats after 205984 examples: {'rewards_train/chosen': '0.13789', 'rewards_train/rejected': '-0.018719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15661', 'logps_train/rejected': '-119', 'logps_train/chosen': '-161.47', 'loss/train': '0.63264', 'examples_per_second': '31.662', 'grad_norm': '20', 'counters/examples': 205984, 'counters/updates': 6437}
skipping logging after 206016 examples to avoid logging too frequently
train stats after 206048 examples: {'rewards_train/chosen': '0.19395', 'rewards_train/rejected': '0.16191', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032039', 'logps_train/rejected': '-150.75', 'logps_train/chosen': '-136.38', 'loss/train': '0.69497', 'examples_per_second': '31.642', 'grad_norm': '22.375', 'counters/examples': 206048, 'counters/updates': 6439}
train stats after 206080 examples: {'rewards_train/chosen': '0.17429', 'rewards_train/rejected': '0.019577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15471', 'logps_train/rejected': '-138.45', 'logps_train/chosen': '-149.12', 'loss/train': '0.63762', 'examples_per_second': '31.619', 'grad_norm': '21', 'counters/examples': 206080, 'counters/updates': 6440}
train stats after 206112 examples: {'rewards_train/chosen': '0.16172', 'rewards_train/rejected': '0.086734', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074988', 'logps_train/rejected': '-115.75', 'logps_train/chosen': '-128.56', 'loss/train': '0.67324', 'examples_per_second': '32.324', 'grad_norm': '20.375', 'counters/examples': 206112, 'counters/updates': 6441}
train stats after 206144 examples: {'rewards_train/chosen': '0.21421', 'rewards_train/rejected': '0.12377', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090444', 'logps_train/rejected': '-120.88', 'logps_train/chosen': '-148.08', 'loss/train': '0.66212', 'examples_per_second': '30.138', 'grad_norm': '22.125', 'counters/examples': 206144, 'counters/updates': 6442}
train stats after 206176 examples: {'rewards_train/chosen': '0.19398', 'rewards_train/rejected': '0.024986', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.169', 'logps_train/rejected': '-93.554', 'logps_train/chosen': '-160.78', 'loss/train': '0.62524', 'examples_per_second': '31.757', 'grad_norm': '18.75', 'counters/examples': 206176, 'counters/updates': 6443}
train stats after 206208 examples: {'rewards_train/chosen': '0.19981', 'rewards_train/rejected': '0.14281', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056999', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-121.98', 'loss/train': '0.6864', 'examples_per_second': '30.802', 'grad_norm': '19.25', 'counters/examples': 206208, 'counters/updates': 6444}
train stats after 206240 examples: {'rewards_train/chosen': '0.095632', 'rewards_train/rejected': '-0.026567', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1222', 'logps_train/rejected': '-88.861', 'logps_train/chosen': '-92.656', 'loss/train': '0.65293', 'examples_per_second': '31.701', 'grad_norm': '16.875', 'counters/examples': 206240, 'counters/updates': 6445}
train stats after 206272 examples: {'rewards_train/chosen': '0.17854', 'rewards_train/rejected': '0.041785', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13676', 'logps_train/rejected': '-91.493', 'logps_train/chosen': '-144.48', 'loss/train': '0.63988', 'examples_per_second': '30.184', 'grad_norm': '17.875', 'counters/examples': 206272, 'counters/updates': 6446}
skipping logging after 206304 examples to avoid logging too frequently
train stats after 206336 examples: {'rewards_train/chosen': '0.17995', 'rewards_train/rejected': '0.010665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16929', 'logps_train/rejected': '-110.23', 'logps_train/chosen': '-142.95', 'loss/train': '0.62902', 'examples_per_second': '31.616', 'grad_norm': '23.375', 'counters/examples': 206336, 'counters/updates': 6448}
train stats after 206368 examples: {'rewards_train/chosen': '0.25242', 'rewards_train/rejected': '0.021845', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23058', 'logps_train/rejected': '-98.436', 'logps_train/chosen': '-139.02', 'loss/train': '0.59917', 'examples_per_second': '32.237', 'grad_norm': '19.625', 'counters/examples': 206368, 'counters/updates': 6449}
train stats after 206400 examples: {'rewards_train/chosen': '0.25933', 'rewards_train/rejected': '-0.0046742', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.264', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-137.47', 'loss/train': '0.58705', 'examples_per_second': '31.525', 'grad_norm': '18.875', 'counters/examples': 206400, 'counters/updates': 6450}
train stats after 206432 examples: {'rewards_train/chosen': '0.12654', 'rewards_train/rejected': '0.038478', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088057', 'logps_train/rejected': '-177.48', 'logps_train/chosen': '-151.62', 'loss/train': '0.66406', 'examples_per_second': '30.821', 'grad_norm': '23', 'counters/examples': 206432, 'counters/updates': 6451}
skipping logging after 206464 examples to avoid logging too frequently
train stats after 206496 examples: {'rewards_train/chosen': '0.15839', 'rewards_train/rejected': '-0.012847', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17124', 'logps_train/rejected': '-132.08', 'logps_train/chosen': '-134.34', 'loss/train': '0.6234', 'examples_per_second': '25.747', 'grad_norm': '18.875', 'counters/examples': 206496, 'counters/updates': 6453}
train stats after 206528 examples: {'rewards_train/chosen': '0.1219', 'rewards_train/rejected': '-0.041836', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16374', 'logps_train/rejected': '-103.77', 'logps_train/chosen': '-104.16', 'loss/train': '0.62471', 'examples_per_second': '32.386', 'grad_norm': '16', 'counters/examples': 206528, 'counters/updates': 6454}
train stats after 206560 examples: {'rewards_train/chosen': '0.26313', 'rewards_train/rejected': '0.031107', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23202', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-143.61', 'loss/train': '0.60899', 'examples_per_second': '33.08', 'grad_norm': '21.125', 'counters/examples': 206560, 'counters/updates': 6455}
train stats after 206592 examples: {'rewards_train/chosen': '0.18309', 'rewards_train/rejected': '0.003017', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18008', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-169.94', 'loss/train': '0.6352', 'examples_per_second': '31.227', 'grad_norm': '22.5', 'counters/examples': 206592, 'counters/updates': 6456}
skipping logging after 206624 examples to avoid logging too frequently
train stats after 206656 examples: {'rewards_train/chosen': '0.15377', 'rewards_train/rejected': '0.0076437', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14613', 'logps_train/rejected': '-99.401', 'logps_train/chosen': '-114.03', 'loss/train': '0.6381', 'examples_per_second': '39.311', 'grad_norm': '17.875', 'counters/examples': 206656, 'counters/updates': 6458}
train stats after 206688 examples: {'rewards_train/chosen': '0.2449', 'rewards_train/rejected': '0.04303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20187', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-132.1', 'loss/train': '0.6155', 'examples_per_second': '30.629', 'grad_norm': '18.875', 'counters/examples': 206688, 'counters/updates': 6459}
train stats after 206720 examples: {'rewards_train/chosen': '0.2284', 'rewards_train/rejected': '0.015143', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21325', 'logps_train/rejected': '-89.915', 'logps_train/chosen': '-127.96', 'loss/train': '0.6066', 'examples_per_second': '32.686', 'grad_norm': '17.5', 'counters/examples': 206720, 'counters/updates': 6460}
train stats after 206752 examples: {'rewards_train/chosen': '0.2277', 'rewards_train/rejected': '0.12437', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10333', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-161.12', 'loss/train': '0.65833', 'examples_per_second': '32.253', 'grad_norm': '21.125', 'counters/examples': 206752, 'counters/updates': 6461}
skipping logging after 206784 examples to avoid logging too frequently
train stats after 206816 examples: {'rewards_train/chosen': '0.14611', 'rewards_train/rejected': '0.022994', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12312', 'logps_train/rejected': '-110.56', 'logps_train/chosen': '-101.64', 'loss/train': '0.64835', 'examples_per_second': '34.297', 'grad_norm': '19', 'counters/examples': 206816, 'counters/updates': 6463}
train stats after 206848 examples: {'rewards_train/chosen': '0.15111', 'rewards_train/rejected': '-0.014875', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16598', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-145.05', 'loss/train': '0.62771', 'examples_per_second': '31.648', 'grad_norm': '20.625', 'counters/examples': 206848, 'counters/updates': 6464}
train stats after 206880 examples: {'rewards_train/chosen': '0.2624', 'rewards_train/rejected': '0.095859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16654', 'logps_train/rejected': '-128.69', 'logps_train/chosen': '-134.37', 'loss/train': '0.62754', 'examples_per_second': '31.091', 'grad_norm': '18.75', 'counters/examples': 206880, 'counters/updates': 6465}
train stats after 206912 examples: {'rewards_train/chosen': '0.19594', 'rewards_train/rejected': '0.00085897', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19508', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-135.76', 'loss/train': '0.61156', 'examples_per_second': '32.344', 'grad_norm': '19.875', 'counters/examples': 206912, 'counters/updates': 6466}
train stats after 206944 examples: {'rewards_train/chosen': '0.19449', 'rewards_train/rejected': '0.097062', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097426', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-127.84', 'loss/train': '0.66781', 'examples_per_second': '32.302', 'grad_norm': '20.125', 'counters/examples': 206944, 'counters/updates': 6467}
train stats after 206976 examples: {'rewards_train/chosen': '0.19845', 'rewards_train/rejected': '-0.0060371', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20449', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-142.27', 'loss/train': '0.6171', 'examples_per_second': '32.176', 'grad_norm': '20.25', 'counters/examples': 206976, 'counters/updates': 6468}
train stats after 207008 examples: {'rewards_train/chosen': '0.25556', 'rewards_train/rejected': '0.083165', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17239', 'logps_train/rejected': '-137.98', 'logps_train/chosen': '-157.7', 'loss/train': '0.64419', 'examples_per_second': '31.488', 'grad_norm': '21.375', 'counters/examples': 207008, 'counters/updates': 6469}
train stats after 207040 examples: {'rewards_train/chosen': '0.19024', 'rewards_train/rejected': '0.091895', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.098341', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-123.11', 'loss/train': '0.65983', 'examples_per_second': '31.651', 'grad_norm': '19.875', 'counters/examples': 207040, 'counters/updates': 6470}
train stats after 207072 examples: {'rewards_train/chosen': '0.15193', 'rewards_train/rejected': '0.044877', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10706', 'logps_train/rejected': '-104', 'logps_train/chosen': '-133.33', 'loss/train': '0.65493', 'examples_per_second': '32.357', 'grad_norm': '19.25', 'counters/examples': 207072, 'counters/updates': 6471}
train stats after 207104 examples: {'rewards_train/chosen': '0.1308', 'rewards_train/rejected': '0.042318', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088486', 'logps_train/rejected': '-165.03', 'logps_train/chosen': '-158.58', 'loss/train': '0.66475', 'examples_per_second': '31.674', 'grad_norm': '22.25', 'counters/examples': 207104, 'counters/updates': 6472}
train stats after 207136 examples: {'rewards_train/chosen': '0.18183', 'rewards_train/rejected': '0.088372', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093453', 'logps_train/rejected': '-106.81', 'logps_train/chosen': '-109.03', 'loss/train': '0.66045', 'examples_per_second': '32.771', 'grad_norm': '17.875', 'counters/examples': 207136, 'counters/updates': 6473}
train stats after 207168 examples: {'rewards_train/chosen': '0.12641', 'rewards_train/rejected': '0.051836', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074579', 'logps_train/rejected': '-142.67', 'logps_train/chosen': '-136.59', 'loss/train': '0.67657', 'examples_per_second': '31.598', 'grad_norm': '21.75', 'counters/examples': 207168, 'counters/updates': 6474}
train stats after 207200 examples: {'rewards_train/chosen': '0.19137', 'rewards_train/rejected': '-0.00043002', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1918', 'logps_train/rejected': '-104.98', 'logps_train/chosen': '-147.38', 'loss/train': '0.61527', 'examples_per_second': '32.268', 'grad_norm': '19', 'counters/examples': 207200, 'counters/updates': 6475}
train stats after 207232 examples: {'rewards_train/chosen': '0.18204', 'rewards_train/rejected': '0.052812', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12923', 'logps_train/rejected': '-119.4', 'logps_train/chosen': '-124.75', 'loss/train': '0.6452', 'examples_per_second': '32.363', 'grad_norm': '19.375', 'counters/examples': 207232, 'counters/updates': 6476}
train stats after 207264 examples: {'rewards_train/chosen': '0.07951', 'rewards_train/rejected': '0.1065', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026994', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-133.84', 'loss/train': '0.71996', 'examples_per_second': '30.699', 'grad_norm': '21.875', 'counters/examples': 207264, 'counters/updates': 6477}
train stats after 207296 examples: {'rewards_train/chosen': '0.18237', 'rewards_train/rejected': '0.1013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081068', 'logps_train/rejected': '-131.17', 'logps_train/chosen': '-126.87', 'loss/train': '0.6658', 'examples_per_second': '33.094', 'grad_norm': '20.375', 'counters/examples': 207296, 'counters/updates': 6478}
skipping logging after 207328 examples to avoid logging too frequently
train stats after 207360 examples: {'rewards_train/chosen': '0.2204', 'rewards_train/rejected': '0.10118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11923', 'logps_train/rejected': '-125.48', 'logps_train/chosen': '-156.86', 'loss/train': '0.65121', 'examples_per_second': '31.642', 'grad_norm': '21.25', 'counters/examples': 207360, 'counters/updates': 6480}
train stats after 207392 examples: {'rewards_train/chosen': '0.14906', 'rewards_train/rejected': '0.013638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13542', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-176.98', 'loss/train': '0.64425', 'examples_per_second': '30.694', 'grad_norm': '22.625', 'counters/examples': 207392, 'counters/updates': 6481}
skipping logging after 207424 examples to avoid logging too frequently
train stats after 207456 examples: {'rewards_train/chosen': '0.19973', 'rewards_train/rejected': '0.11013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0896', 'logps_train/rejected': '-120.37', 'logps_train/chosen': '-132.06', 'loss/train': '0.6642', 'examples_per_second': '30.808', 'grad_norm': '21.5', 'counters/examples': 207456, 'counters/updates': 6483}
train stats after 207488 examples: {'rewards_train/chosen': '0.18104', 'rewards_train/rejected': '0.099051', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081991', 'logps_train/rejected': '-108.2', 'logps_train/chosen': '-96.781', 'loss/train': '0.66483', 'examples_per_second': '33.057', 'grad_norm': '18.375', 'counters/examples': 207488, 'counters/updates': 6484}
train stats after 207520 examples: {'rewards_train/chosen': '0.14645', 'rewards_train/rejected': '0.021244', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1252', 'logps_train/rejected': '-113.37', 'logps_train/chosen': '-139.01', 'loss/train': '0.64664', 'examples_per_second': '31.697', 'grad_norm': '19.625', 'counters/examples': 207520, 'counters/updates': 6485}
train stats after 207552 examples: {'rewards_train/chosen': '0.11916', 'rewards_train/rejected': '0.024123', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095041', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-139.37', 'loss/train': '0.65594', 'examples_per_second': '30.664', 'grad_norm': '20.625', 'counters/examples': 207552, 'counters/updates': 6486}
train stats after 207584 examples: {'rewards_train/chosen': '0.15203', 'rewards_train/rejected': '-0.055761', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20779', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-133.13', 'loss/train': '0.60891', 'examples_per_second': '32.488', 'grad_norm': '18.25', 'counters/examples': 207584, 'counters/updates': 6487}
train stats after 207616 examples: {'rewards_train/chosen': '0.099812', 'rewards_train/rejected': '0.027119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072693', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-125.97', 'loss/train': '0.67125', 'examples_per_second': '30.638', 'grad_norm': '19.5', 'counters/examples': 207616, 'counters/updates': 6488}
train stats after 207648 examples: {'rewards_train/chosen': '0.18677', 'rewards_train/rejected': '0.05167', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1351', 'logps_train/rejected': '-122.42', 'logps_train/chosen': '-158.54', 'loss/train': '0.64805', 'examples_per_second': '32.751', 'grad_norm': '20.25', 'counters/examples': 207648, 'counters/updates': 6489}
train stats after 207680 examples: {'rewards_train/chosen': '0.12495', 'rewards_train/rejected': '0.15543', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030474', 'logps_train/rejected': '-101.9', 'logps_train/chosen': '-108.52', 'loss/train': '0.72148', 'examples_per_second': '31.644', 'grad_norm': '19.625', 'counters/examples': 207680, 'counters/updates': 6490}
train stats after 207712 examples: {'rewards_train/chosen': '0.16804', 'rewards_train/rejected': '-0.023', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19104', 'logps_train/rejected': '-107.39', 'logps_train/chosen': '-137.88', 'loss/train': '0.63058', 'examples_per_second': '30.389', 'grad_norm': '20.25', 'counters/examples': 207712, 'counters/updates': 6491}
train stats after 207744 examples: {'rewards_train/chosen': '0.21652', 'rewards_train/rejected': '0.11317', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10334', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-162.84', 'loss/train': '0.65252', 'examples_per_second': '32.38', 'grad_norm': '22.25', 'counters/examples': 207744, 'counters/updates': 6492}
train stats after 207776 examples: {'rewards_train/chosen': '0.1724', 'rewards_train/rejected': '0.022486', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14991', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-156.61', 'loss/train': '0.63032', 'examples_per_second': '30.619', 'grad_norm': '20.125', 'counters/examples': 207776, 'counters/updates': 6493}
train stats after 207808 examples: {'rewards_train/chosen': '0.19108', 'rewards_train/rejected': '0.085222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10585', 'logps_train/rejected': '-119.13', 'logps_train/chosen': '-139.06', 'loss/train': '0.65707', 'examples_per_second': '24.362', 'grad_norm': '19.75', 'counters/examples': 207808, 'counters/updates': 6494}
skipping logging after 207840 examples to avoid logging too frequently
train stats after 207872 examples: {'rewards_train/chosen': '0.23053', 'rewards_train/rejected': '0.10564', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12488', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-143.59', 'loss/train': '0.64236', 'examples_per_second': '31.329', 'grad_norm': '19.75', 'counters/examples': 207872, 'counters/updates': 6496}
train stats after 207904 examples: {'rewards_train/chosen': '0.11874', 'rewards_train/rejected': '-0.0035138', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12225', 'logps_train/rejected': '-137.19', 'logps_train/chosen': '-156.34', 'loss/train': '0.64501', 'examples_per_second': '24.43', 'grad_norm': '22', 'counters/examples': 207904, 'counters/updates': 6497}
train stats after 207936 examples: {'rewards_train/chosen': '0.36848', 'rewards_train/rejected': '0.11882', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24966', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-209.81', 'loss/train': '0.59082', 'examples_per_second': '31.645', 'grad_norm': '22.875', 'counters/examples': 207936, 'counters/updates': 6498}
train stats after 207968 examples: {'rewards_train/chosen': '0.20523', 'rewards_train/rejected': '0.051626', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1536', 'logps_train/rejected': '-114.41', 'logps_train/chosen': '-141.51', 'loss/train': '0.63434', 'examples_per_second': '31.649', 'grad_norm': '20.375', 'counters/examples': 207968, 'counters/updates': 6499}
skipping logging after 208000 examples to avoid logging too frequently
train stats after 208032 examples: {'rewards_train/chosen': '0.19313', 'rewards_train/rejected': '0.086851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10627', 'logps_train/rejected': '-154.6', 'logps_train/chosen': '-132.58', 'loss/train': '0.67336', 'examples_per_second': '31.89', 'grad_norm': '21.25', 'counters/examples': 208032, 'counters/updates': 6501}
train stats after 208064 examples: {'rewards_train/chosen': '0.15633', 'rewards_train/rejected': '-0.044641', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20097', 'logps_train/rejected': '-135.86', 'logps_train/chosen': '-120.59', 'loss/train': '0.61137', 'examples_per_second': '31.666', 'grad_norm': '20.5', 'counters/examples': 208064, 'counters/updates': 6502}
train stats after 208096 examples: {'rewards_train/chosen': '0.27678', 'rewards_train/rejected': '0.064848', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21193', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-135.23', 'loss/train': '0.61033', 'examples_per_second': '31.661', 'grad_norm': '19.625', 'counters/examples': 208096, 'counters/updates': 6503}
train stats after 208128 examples: {'rewards_train/chosen': '0.21653', 'rewards_train/rejected': '0.054736', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1618', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-167.56', 'loss/train': '0.62959', 'examples_per_second': '31.331', 'grad_norm': '21.5', 'counters/examples': 208128, 'counters/updates': 6504}
train stats after 208160 examples: {'rewards_train/chosen': '0.18399', 'rewards_train/rejected': '0.023747', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16024', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-135.12', 'loss/train': '0.62935', 'examples_per_second': '31.135', 'grad_norm': '20', 'counters/examples': 208160, 'counters/updates': 6505}
skipping logging after 208192 examples to avoid logging too frequently
train stats after 208224 examples: {'rewards_train/chosen': '0.2622', 'rewards_train/rejected': '0.10946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15274', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-166.88', 'loss/train': '0.63627', 'examples_per_second': '34.711', 'grad_norm': '20.75', 'counters/examples': 208224, 'counters/updates': 6507}
skipping logging after 208256 examples to avoid logging too frequently
train stats after 208288 examples: {'rewards_train/chosen': '0.18973', 'rewards_train/rejected': '0.099938', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089789', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-115.91', 'loss/train': '0.66373', 'examples_per_second': '30.571', 'grad_norm': '20.5', 'counters/examples': 208288, 'counters/updates': 6509}
skipping logging after 208320 examples to avoid logging too frequently
train stats after 208352 examples: {'rewards_train/chosen': '0.088957', 'rewards_train/rejected': '0.12568', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.036723', 'logps_train/rejected': '-137.15', 'logps_train/chosen': '-80.762', 'loss/train': '0.72003', 'examples_per_second': '31.334', 'grad_norm': '20', 'counters/examples': 208352, 'counters/updates': 6511}
train stats after 208384 examples: {'rewards_train/chosen': '0.12375', 'rewards_train/rejected': '0.054943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068807', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-109.33', 'loss/train': '0.6716', 'examples_per_second': '31.671', 'grad_norm': '19.25', 'counters/examples': 208384, 'counters/updates': 6512}
train stats after 208416 examples: {'rewards_train/chosen': '0.20247', 'rewards_train/rejected': '-0.043379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24585', 'logps_train/rejected': '-139.12', 'logps_train/chosen': '-127.98', 'loss/train': '0.60273', 'examples_per_second': '31.708', 'grad_norm': '18.75', 'counters/examples': 208416, 'counters/updates': 6513}
train stats after 208448 examples: {'rewards_train/chosen': '0.20773', 'rewards_train/rejected': '0.077073', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13065', 'logps_train/rejected': '-94.848', 'logps_train/chosen': '-168.6', 'loss/train': '0.64672', 'examples_per_second': '31.036', 'grad_norm': '19.875', 'counters/examples': 208448, 'counters/updates': 6514}
train stats after 208480 examples: {'rewards_train/chosen': '0.19924', 'rewards_train/rejected': '0.046338', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15291', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-141.98', 'loss/train': '0.63765', 'examples_per_second': '30.153', 'grad_norm': '20.375', 'counters/examples': 208480, 'counters/updates': 6515}
train stats after 208512 examples: {'rewards_train/chosen': '0.1298', 'rewards_train/rejected': '0.060864', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068937', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-136.3', 'loss/train': '0.6772', 'examples_per_second': '30.699', 'grad_norm': '20.125', 'counters/examples': 208512, 'counters/updates': 6516}
skipping logging after 208544 examples to avoid logging too frequently
train stats after 208576 examples: {'rewards_train/chosen': '0.17132', 'rewards_train/rejected': '0.018121', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1532', 'logps_train/rejected': '-97.913', 'logps_train/chosen': '-127.07', 'loss/train': '0.63664', 'examples_per_second': '31.332', 'grad_norm': '18.75', 'counters/examples': 208576, 'counters/updates': 6518}
train stats after 208608 examples: {'rewards_train/chosen': '0.22801', 'rewards_train/rejected': '0.14197', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086033', 'logps_train/rejected': '-154.67', 'logps_train/chosen': '-154.46', 'loss/train': '0.66696', 'examples_per_second': '32.535', 'grad_norm': '23', 'counters/examples': 208608, 'counters/updates': 6519}
train stats after 208640 examples: {'rewards_train/chosen': '0.12284', 'rewards_train/rejected': '0.03414', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088703', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-118.06', 'loss/train': '0.6631', 'examples_per_second': '33.103', 'grad_norm': '18.75', 'counters/examples': 208640, 'counters/updates': 6520}
train stats after 208672 examples: {'rewards_train/chosen': '0.15247', 'rewards_train/rejected': '0.12993', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022545', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-158.83', 'loss/train': '0.69416', 'examples_per_second': '32.74', 'grad_norm': '22.375', 'counters/examples': 208672, 'counters/updates': 6521}
skipping logging after 208704 examples to avoid logging too frequently
train stats after 208736 examples: {'rewards_train/chosen': '0.14017', 'rewards_train/rejected': '-0.067071', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20724', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-133.66', 'loss/train': '0.61206', 'examples_per_second': '34.302', 'grad_norm': '19', 'counters/examples': 208736, 'counters/updates': 6523}
train stats after 208768 examples: {'rewards_train/chosen': '0.11912', 'rewards_train/rejected': '0.033905', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085219', 'logps_train/rejected': '-109.46', 'logps_train/chosen': '-111.26', 'loss/train': '0.661', 'examples_per_second': '30.672', 'grad_norm': '18.375', 'counters/examples': 208768, 'counters/updates': 6524}
skipping logging after 208800 examples to avoid logging too frequently
train stats after 208832 examples: {'rewards_train/chosen': '0.18195', 'rewards_train/rejected': '-0.009024', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19097', 'logps_train/rejected': '-97.136', 'logps_train/chosen': '-111.99', 'loss/train': '0.62108', 'examples_per_second': '36.163', 'grad_norm': '18.125', 'counters/examples': 208832, 'counters/updates': 6526}
train stats after 208864 examples: {'rewards_train/chosen': '0.061253', 'rewards_train/rejected': '-0.025201', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086453', 'logps_train/rejected': '-132.67', 'logps_train/chosen': '-141.96', 'loss/train': '0.66433', 'examples_per_second': '31.69', 'grad_norm': '21.25', 'counters/examples': 208864, 'counters/updates': 6527}
train stats after 208896 examples: {'rewards_train/chosen': '0.20053', 'rewards_train/rejected': '-0.0038232', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20435', 'logps_train/rejected': '-108', 'logps_train/chosen': '-118.93', 'loss/train': '0.6093', 'examples_per_second': '31.642', 'grad_norm': '17.875', 'counters/examples': 208896, 'counters/updates': 6528}
skipping logging after 208928 examples to avoid logging too frequently
train stats after 208960 examples: {'rewards_train/chosen': '0.21062', 'rewards_train/rejected': '0.083313', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12731', 'logps_train/rejected': '-107.91', 'logps_train/chosen': '-124.3', 'loss/train': '0.63864', 'examples_per_second': '31.689', 'grad_norm': '17.75', 'counters/examples': 208960, 'counters/updates': 6530}
train stats after 208992 examples: {'rewards_train/chosen': '0.17225', 'rewards_train/rejected': '0.11135', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060903', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-166.86', 'loss/train': '0.68663', 'examples_per_second': '30.066', 'grad_norm': '22.875', 'counters/examples': 208992, 'counters/updates': 6531}
train stats after 209024 examples: {'rewards_train/chosen': '0.15922', 'rewards_train/rejected': '-0.00011537', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15934', 'logps_train/rejected': '-107.55', 'logps_train/chosen': '-122.09', 'loss/train': '0.63945', 'examples_per_second': '31.689', 'grad_norm': '18.625', 'counters/examples': 209024, 'counters/updates': 6532}
train stats after 209056 examples: {'rewards_train/chosen': '0.22745', 'rewards_train/rejected': '-0.048138', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.27559', 'logps_train/rejected': '-101.71', 'logps_train/chosen': '-162.89', 'loss/train': '0.57797', 'examples_per_second': '32.784', 'grad_norm': '18.875', 'counters/examples': 209056, 'counters/updates': 6533}
skipping logging after 209088 examples to avoid logging too frequently
train stats after 209120 examples: {'rewards_train/chosen': '0.14555', 'rewards_train/rejected': '-0.009203', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15475', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-136.97', 'loss/train': '0.62967', 'examples_per_second': '32.922', 'grad_norm': '19.375', 'counters/examples': 209120, 'counters/updates': 6535}
train stats after 209152 examples: {'rewards_train/chosen': '0.16212', 'rewards_train/rejected': '0.031606', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13052', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-160.5', 'loss/train': '0.64528', 'examples_per_second': '31.601', 'grad_norm': '20.625', 'counters/examples': 209152, 'counters/updates': 6536}
train stats after 209184 examples: {'rewards_train/chosen': '0.147', 'rewards_train/rejected': '0.01199', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13501', 'logps_train/rejected': '-136.92', 'logps_train/chosen': '-134.96', 'loss/train': '0.64484', 'examples_per_second': '31.004', 'grad_norm': '20.875', 'counters/examples': 209184, 'counters/updates': 6537}
train stats after 209216 examples: {'rewards_train/chosen': '0.15842', 'rewards_train/rejected': '0.11526', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043162', 'logps_train/rejected': '-145.39', 'logps_train/chosen': '-117.81', 'loss/train': '0.68684', 'examples_per_second': '31.511', 'grad_norm': '21.125', 'counters/examples': 209216, 'counters/updates': 6538}
train stats after 209248 examples: {'rewards_train/chosen': '0.057893', 'rewards_train/rejected': '0.021541', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036352', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-148.22', 'loss/train': '0.68361', 'examples_per_second': '31.674', 'grad_norm': '21.5', 'counters/examples': 209248, 'counters/updates': 6539}
train stats after 209280 examples: {'rewards_train/chosen': '0.34647', 'rewards_train/rejected': '0.064553', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.28191', 'logps_train/rejected': '-108.6', 'logps_train/chosen': '-157.64', 'loss/train': '0.58155', 'examples_per_second': '31.702', 'grad_norm': '18.375', 'counters/examples': 209280, 'counters/updates': 6540}
skipping logging after 209312 examples to avoid logging too frequently
train stats after 209344 examples: {'rewards_train/chosen': '0.25621', 'rewards_train/rejected': '-0.0041178', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26033', 'logps_train/rejected': '-94.26', 'logps_train/chosen': '-160.73', 'loss/train': '0.58822', 'examples_per_second': '33.502', 'grad_norm': '17.875', 'counters/examples': 209344, 'counters/updates': 6542}
skipping logging after 209376 examples to avoid logging too frequently
train stats after 209408 examples: {'rewards_train/chosen': '0.12545', 'rewards_train/rejected': '0.0030107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12244', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-115.24', 'loss/train': '0.64963', 'examples_per_second': '31.789', 'grad_norm': '18.75', 'counters/examples': 209408, 'counters/updates': 6544}
train stats after 209440 examples: {'rewards_train/chosen': '0.17646', 'rewards_train/rejected': '0.020139', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15632', 'logps_train/rejected': '-90.343', 'logps_train/chosen': '-108.77', 'loss/train': '0.62927', 'examples_per_second': '32.2', 'grad_norm': '18.625', 'counters/examples': 209440, 'counters/updates': 6545}
train stats after 209472 examples: {'rewards_train/chosen': '0.13504', 'rewards_train/rejected': '0.090843', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044195', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-90.951', 'loss/train': '0.68838', 'examples_per_second': '30.155', 'grad_norm': '17', 'counters/examples': 209472, 'counters/updates': 6546}
skipping logging after 209504 examples to avoid logging too frequently
train stats after 209536 examples: {'rewards_train/chosen': '0.18056', 'rewards_train/rejected': '0.031595', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14896', 'logps_train/rejected': '-103.57', 'logps_train/chosen': '-137.98', 'loss/train': '0.63848', 'examples_per_second': '34.349', 'grad_norm': '18.75', 'counters/examples': 209536, 'counters/updates': 6548}
train stats after 209568 examples: {'rewards_train/chosen': '0.19816', 'rewards_train/rejected': '0.093074', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10508', 'logps_train/rejected': '-146.71', 'logps_train/chosen': '-181.58', 'loss/train': '0.65553', 'examples_per_second': '31.685', 'grad_norm': '22.75', 'counters/examples': 209568, 'counters/updates': 6549}
train stats after 209600 examples: {'rewards_train/chosen': '0.16025', 'rewards_train/rejected': '0.049533', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11072', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-128.9', 'loss/train': '0.64961', 'examples_per_second': '31.497', 'grad_norm': '20.25', 'counters/examples': 209600, 'counters/updates': 6550}
train stats after 209632 examples: {'rewards_train/chosen': '0.15137', 'rewards_train/rejected': '0.06961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081765', 'logps_train/rejected': '-93.577', 'logps_train/chosen': '-113.89', 'loss/train': '0.65989', 'examples_per_second': '30.1', 'grad_norm': '18.625', 'counters/examples': 209632, 'counters/updates': 6551}
skipping logging after 209664 examples to avoid logging too frequently
train stats after 209696 examples: {'rewards_train/chosen': '0.26053', 'rewards_train/rejected': '0.049232', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21129', 'logps_train/rejected': '-135.76', 'logps_train/chosen': '-155.21', 'loss/train': '0.60613', 'examples_per_second': '30.179', 'grad_norm': '19.375', 'counters/examples': 209696, 'counters/updates': 6553}
train stats after 209728 examples: {'rewards_train/chosen': '0.10969', 'rewards_train/rejected': '-0.042565', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15225', 'logps_train/rejected': '-137.6', 'logps_train/chosen': '-110.78', 'loss/train': '0.62897', 'examples_per_second': '31.672', 'grad_norm': '19.875', 'counters/examples': 209728, 'counters/updates': 6554}
skipping logging after 209760 examples to avoid logging too frequently
train stats after 209792 examples: {'rewards_train/chosen': '0.21511', 'rewards_train/rejected': '0.020141', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19496', 'logps_train/rejected': '-135.64', 'logps_train/chosen': '-136.9', 'loss/train': '0.61697', 'examples_per_second': '31.676', 'grad_norm': '19.5', 'counters/examples': 209792, 'counters/updates': 6556}
train stats after 209824 examples: {'rewards_train/chosen': '0.1135', 'rewards_train/rejected': '0.027499', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086005', 'logps_train/rejected': '-106.38', 'logps_train/chosen': '-139.23', 'loss/train': '0.66561', 'examples_per_second': '30.874', 'grad_norm': '19.5', 'counters/examples': 209824, 'counters/updates': 6557}
train stats after 209856 examples: {'rewards_train/chosen': '0.18412', 'rewards_train/rejected': '0.062706', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12142', 'logps_train/rejected': '-117.11', 'logps_train/chosen': '-135.69', 'loss/train': '0.65524', 'examples_per_second': '31.773', 'grad_norm': '20.25', 'counters/examples': 209856, 'counters/updates': 6558}
train stats after 209888 examples: {'rewards_train/chosen': '0.15985', 'rewards_train/rejected': '0.033666', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12618', 'logps_train/rejected': '-105.06', 'logps_train/chosen': '-167.38', 'loss/train': '0.6512', 'examples_per_second': '31.456', 'grad_norm': '22.5', 'counters/examples': 209888, 'counters/updates': 6559}
train stats after 209920 examples: {'rewards_train/chosen': '0.20451', 'rewards_train/rejected': '-0.010157', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21467', 'logps_train/rejected': '-146.71', 'logps_train/chosen': '-115.94', 'loss/train': '0.60745', 'examples_per_second': '32.425', 'grad_norm': '19.75', 'counters/examples': 209920, 'counters/updates': 6560}
train stats after 209952 examples: {'rewards_train/chosen': '0.143', 'rewards_train/rejected': '0.11827', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.024724', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-126.62', 'loss/train': '0.69794', 'examples_per_second': '32.39', 'grad_norm': '20.25', 'counters/examples': 209952, 'counters/updates': 6561}
train stats after 209984 examples: {'rewards_train/chosen': '0.13657', 'rewards_train/rejected': '0.010587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12598', 'logps_train/rejected': '-149.15', 'logps_train/chosen': '-146.73', 'loss/train': '0.65095', 'examples_per_second': '32.378', 'grad_norm': '20.125', 'counters/examples': 209984, 'counters/updates': 6562}
skipping logging after 210016 examples to avoid logging too frequently
train stats after 210048 examples: {'rewards_train/chosen': '0.24117', 'rewards_train/rejected': '0.026432', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21474', 'logps_train/rejected': '-113.14', 'logps_train/chosen': '-173.04', 'loss/train': '0.61164', 'examples_per_second': '31.535', 'grad_norm': '20.125', 'counters/examples': 210048, 'counters/updates': 6564}
train stats after 210080 examples: {'rewards_train/chosen': '0.16507', 'rewards_train/rejected': '0.048916', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11615', 'logps_train/rejected': '-108.32', 'logps_train/chosen': '-113.62', 'loss/train': '0.64151', 'examples_per_second': '32.456', 'grad_norm': '18.375', 'counters/examples': 210080, 'counters/updates': 6565}
train stats after 210112 examples: {'rewards_train/chosen': '0.17769', 'rewards_train/rejected': '0.053753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12394', 'logps_train/rejected': '-146.06', 'logps_train/chosen': '-162.47', 'loss/train': '0.64899', 'examples_per_second': '31.642', 'grad_norm': '22', 'counters/examples': 210112, 'counters/updates': 6566}
train stats after 210144 examples: {'rewards_train/chosen': '0.23679', 'rewards_train/rejected': '0.10608', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13071', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-124.27', 'loss/train': '0.64034', 'examples_per_second': '31.942', 'grad_norm': '18.75', 'counters/examples': 210144, 'counters/updates': 6567}
train stats after 210176 examples: {'rewards_train/chosen': '0.18154', 'rewards_train/rejected': '-0.033043', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21459', 'logps_train/rejected': '-142.41', 'logps_train/chosen': '-147.24', 'loss/train': '0.60738', 'examples_per_second': '30.679', 'grad_norm': '20.25', 'counters/examples': 210176, 'counters/updates': 6568}
train stats after 210208 examples: {'rewards_train/chosen': '0.29794', 'rewards_train/rejected': '0.055547', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.24239', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-171.51', 'loss/train': '0.58726', 'examples_per_second': '30.232', 'grad_norm': '21.25', 'counters/examples': 210208, 'counters/updates': 6569}
train stats after 210240 examples: {'rewards_train/chosen': '0.14905', 'rewards_train/rejected': '0.036003', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11305', 'logps_train/rejected': '-91.07', 'logps_train/chosen': '-131.04', 'loss/train': '0.64627', 'examples_per_second': '30.724', 'grad_norm': '20.875', 'counters/examples': 210240, 'counters/updates': 6570}
train stats after 210272 examples: {'rewards_train/chosen': '0.24769', 'rewards_train/rejected': '-0.015618', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26331', 'logps_train/rejected': '-94.998', 'logps_train/chosen': '-117.88', 'loss/train': '0.59352', 'examples_per_second': '32.258', 'grad_norm': '17', 'counters/examples': 210272, 'counters/updates': 6571}
train stats after 210304 examples: {'rewards_train/chosen': '0.16548', 'rewards_train/rejected': '-0.041729', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20721', 'logps_train/rejected': '-87.02', 'logps_train/chosen': '-102.93', 'loss/train': '0.60266', 'examples_per_second': '30.367', 'grad_norm': '16.5', 'counters/examples': 210304, 'counters/updates': 6572}
train stats after 210336 examples: {'rewards_train/chosen': '0.15317', 'rewards_train/rejected': '-0.022698', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17586', 'logps_train/rejected': '-116.07', 'logps_train/chosen': '-122.83', 'loss/train': '0.62386', 'examples_per_second': '32.379', 'grad_norm': '18.75', 'counters/examples': 210336, 'counters/updates': 6573}
train stats after 210368 examples: {'rewards_train/chosen': '0.27907', 'rewards_train/rejected': '0.22331', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05576', 'logps_train/rejected': '-144.28', 'logps_train/chosen': '-173.54', 'loss/train': '0.70879', 'examples_per_second': '30.119', 'grad_norm': '24.5', 'counters/examples': 210368, 'counters/updates': 6574}
train stats after 210400 examples: {'rewards_train/chosen': '0.094441', 'rewards_train/rejected': '0.049951', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04449', 'logps_train/rejected': '-124.99', 'logps_train/chosen': '-111.09', 'loss/train': '0.68719', 'examples_per_second': '31.165', 'grad_norm': '19.625', 'counters/examples': 210400, 'counters/updates': 6575}
train stats after 210432 examples: {'rewards_train/chosen': '0.12943', 'rewards_train/rejected': '0.093813', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035612', 'logps_train/rejected': '-101.54', 'logps_train/chosen': '-125.37', 'loss/train': '0.68785', 'examples_per_second': '31.004', 'grad_norm': '20.5', 'counters/examples': 210432, 'counters/updates': 6576}
skipping logging after 210464 examples to avoid logging too frequently
train stats after 210496 examples: {'rewards_train/chosen': '0.16525', 'rewards_train/rejected': '0.033811', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13144', 'logps_train/rejected': '-94.994', 'logps_train/chosen': '-158.26', 'loss/train': '0.64655', 'examples_per_second': '35.682', 'grad_norm': '19.5', 'counters/examples': 210496, 'counters/updates': 6578}
train stats after 210528 examples: {'rewards_train/chosen': '0.21847', 'rewards_train/rejected': '0.046071', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1724', 'logps_train/rejected': '-132.43', 'logps_train/chosen': '-129.59', 'loss/train': '0.61733', 'examples_per_second': '31.669', 'grad_norm': '19.75', 'counters/examples': 210528, 'counters/updates': 6579}
train stats after 210560 examples: {'rewards_train/chosen': '0.19059', 'rewards_train/rejected': '0.1046', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085983', 'logps_train/rejected': '-151.74', 'logps_train/chosen': '-134.27', 'loss/train': '0.66828', 'examples_per_second': '31.762', 'grad_norm': '21.75', 'counters/examples': 210560, 'counters/updates': 6580}
train stats after 210592 examples: {'rewards_train/chosen': '0.29272', 'rewards_train/rejected': '0.015819', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2769', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-138.82', 'loss/train': '0.58294', 'examples_per_second': '30.713', 'grad_norm': '17.625', 'counters/examples': 210592, 'counters/updates': 6581}
train stats after 210624 examples: {'rewards_train/chosen': '0.075472', 'rewards_train/rejected': '0.030825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044647', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-132.95', 'loss/train': '0.6883', 'examples_per_second': '31.657', 'grad_norm': '21.375', 'counters/examples': 210624, 'counters/updates': 6582}
train stats after 210656 examples: {'rewards_train/chosen': '0.082762', 'rewards_train/rejected': '-0.049208', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13197', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-119.18', 'loss/train': '0.64083', 'examples_per_second': '32.395', 'grad_norm': '18.75', 'counters/examples': 210656, 'counters/updates': 6583}
train stats after 210688 examples: {'rewards_train/chosen': '0.14104', 'rewards_train/rejected': '-0.0097372', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15078', 'logps_train/rejected': '-101.34', 'logps_train/chosen': '-144.45', 'loss/train': '0.64076', 'examples_per_second': '31.155', 'grad_norm': '20.125', 'counters/examples': 210688, 'counters/updates': 6584}
train stats after 210720 examples: {'rewards_train/chosen': '0.20367', 'rewards_train/rejected': '-0.071584', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.27525', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-135.25', 'loss/train': '0.58137', 'examples_per_second': '31.639', 'grad_norm': '18.125', 'counters/examples': 210720, 'counters/updates': 6585}
train stats after 210752 examples: {'rewards_train/chosen': '0.24549', 'rewards_train/rejected': '0.057183', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18831', 'logps_train/rejected': '-110.81', 'logps_train/chosen': '-149.69', 'loss/train': '0.62163', 'examples_per_second': '31.689', 'grad_norm': '19.75', 'counters/examples': 210752, 'counters/updates': 6586}
skipping logging after 210784 examples to avoid logging too frequently
train stats after 210816 examples: {'rewards_train/chosen': '0.14128', 'rewards_train/rejected': '-0.0027371', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14402', 'logps_train/rejected': '-153.19', 'logps_train/chosen': '-114.36', 'loss/train': '0.63228', 'examples_per_second': '31.544', 'grad_norm': '19.75', 'counters/examples': 210816, 'counters/updates': 6588}
skipping logging after 210848 examples to avoid logging too frequently
train stats after 210880 examples: {'rewards_train/chosen': '0.22701', 'rewards_train/rejected': '0.060086', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16692', 'logps_train/rejected': '-157.36', 'logps_train/chosen': '-143', 'loss/train': '0.63538', 'examples_per_second': '33.119', 'grad_norm': '21.5', 'counters/examples': 210880, 'counters/updates': 6590}
train stats after 210912 examples: {'rewards_train/chosen': '0.14422', 'rewards_train/rejected': '0.067476', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.076749', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-123.41', 'loss/train': '0.66633', 'examples_per_second': '31.093', 'grad_norm': '20.625', 'counters/examples': 210912, 'counters/updates': 6591}
train stats after 210944 examples: {'rewards_train/chosen': '0.26587', 'rewards_train/rejected': '0.029567', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2363', 'logps_train/rejected': '-101.12', 'logps_train/chosen': '-130.41', 'loss/train': '0.6067', 'examples_per_second': '31.628', 'grad_norm': '18.625', 'counters/examples': 210944, 'counters/updates': 6592}
train stats after 210976 examples: {'rewards_train/chosen': '0.19002', 'rewards_train/rejected': '0.069493', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12053', 'logps_train/rejected': '-153.73', 'logps_train/chosen': '-148.99', 'loss/train': '0.64587', 'examples_per_second': '31.53', 'grad_norm': '22.625', 'counters/examples': 210976, 'counters/updates': 6593}
train stats after 211008 examples: {'rewards_train/chosen': '0.15878', 'rewards_train/rejected': '0.065546', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.093235', 'logps_train/rejected': '-107.74', 'logps_train/chosen': '-143.11', 'loss/train': '0.67649', 'examples_per_second': '32.368', 'grad_norm': '19.25', 'counters/examples': 211008, 'counters/updates': 6594}
train stats after 211040 examples: {'rewards_train/chosen': '0.25672', 'rewards_train/rejected': '0.06139', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19533', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-146.76', 'loss/train': '0.6225', 'examples_per_second': '31.686', 'grad_norm': '20.5', 'counters/examples': 211040, 'counters/updates': 6595}
train stats after 211072 examples: {'rewards_train/chosen': '0.17485', 'rewards_train/rejected': '0.074774', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10008', 'logps_train/rejected': '-145.26', 'logps_train/chosen': '-132.95', 'loss/train': '0.6566', 'examples_per_second': '30.514', 'grad_norm': '21.125', 'counters/examples': 211072, 'counters/updates': 6596}
train stats after 211104 examples: {'rewards_train/chosen': '0.21496', 'rewards_train/rejected': '0.036658', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.1783', 'logps_train/rejected': '-150.67', 'logps_train/chosen': '-127.1', 'loss/train': '0.62304', 'examples_per_second': '31.483', 'grad_norm': '20', 'counters/examples': 211104, 'counters/updates': 6597}
train stats after 211136 examples: {'rewards_train/chosen': '0.11076', 'rewards_train/rejected': '-0.05143', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16219', 'logps_train/rejected': '-92.91', 'logps_train/chosen': '-128.97', 'loss/train': '0.63166', 'examples_per_second': '31.105', 'grad_norm': '18.625', 'counters/examples': 211136, 'counters/updates': 6598}
train stats after 211168 examples: {'rewards_train/chosen': '0.28507', 'rewards_train/rejected': '0.047067', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.238', 'logps_train/rejected': '-99.193', 'logps_train/chosen': '-138.92', 'loss/train': '0.59564', 'examples_per_second': '30.582', 'grad_norm': '18.25', 'counters/examples': 211168, 'counters/updates': 6599}
skipping logging after 211200 examples to avoid logging too frequently
train stats after 211232 examples: {'rewards_train/chosen': '0.095206', 'rewards_train/rejected': '-0.00054737', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.095753', 'logps_train/rejected': '-120.57', 'logps_train/chosen': '-124.37', 'loss/train': '0.65712', 'examples_per_second': '31.54', 'grad_norm': '18.5', 'counters/examples': 211232, 'counters/updates': 6601}
skipping logging after 211264 examples to avoid logging too frequently
train stats after 211296 examples: {'rewards_train/chosen': '0.1607', 'rewards_train/rejected': '0.042838', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11786', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-102.59', 'loss/train': '0.65381', 'examples_per_second': '31.613', 'grad_norm': '19.75', 'counters/examples': 211296, 'counters/updates': 6603}
skipping logging after 211328 examples to avoid logging too frequently
train stats after 211360 examples: {'rewards_train/chosen': '0.19121', 'rewards_train/rejected': '0.014637', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17657', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-150.5', 'loss/train': '0.6261', 'examples_per_second': '31.992', 'grad_norm': '20', 'counters/examples': 211360, 'counters/updates': 6605}
train stats after 211392 examples: {'rewards_train/chosen': '0.20538', 'rewards_train/rejected': '-0.027595', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23298', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-152.75', 'loss/train': '0.59726', 'examples_per_second': '31.687', 'grad_norm': '20.125', 'counters/examples': 211392, 'counters/updates': 6606}
train stats after 211424 examples: {'rewards_train/chosen': '0.2151', 'rewards_train/rejected': '0.089812', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12528', 'logps_train/rejected': '-110.05', 'logps_train/chosen': '-146.26', 'loss/train': '0.64789', 'examples_per_second': '30.306', 'grad_norm': '20.5', 'counters/examples': 211424, 'counters/updates': 6607}
train stats after 211456 examples: {'rewards_train/chosen': '0.1628', 'rewards_train/rejected': '0.10232', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060482', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-144.82', 'loss/train': '0.67281', 'examples_per_second': '31.756', 'grad_norm': '22.5', 'counters/examples': 211456, 'counters/updates': 6608}
train stats after 211488 examples: {'rewards_train/chosen': '0.16068', 'rewards_train/rejected': '0.041379', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11931', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-134.08', 'loss/train': '0.65166', 'examples_per_second': '31.768', 'grad_norm': '19.375', 'counters/examples': 211488, 'counters/updates': 6609}
train stats after 211520 examples: {'rewards_train/chosen': '0.14566', 'rewards_train/rejected': '0.17211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026449', 'logps_train/rejected': '-142.89', 'logps_train/chosen': '-177.49', 'loss/train': '0.72062', 'examples_per_second': '30.425', 'grad_norm': '23.875', 'counters/examples': 211520, 'counters/updates': 6610}
train stats after 211552 examples: {'rewards_train/chosen': '0.15052', 'rewards_train/rejected': '0.098061', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052459', 'logps_train/rejected': '-124.5', 'logps_train/chosen': '-154.83', 'loss/train': '0.68783', 'examples_per_second': '30.082', 'grad_norm': '21.75', 'counters/examples': 211552, 'counters/updates': 6611}
train stats after 211584 examples: {'rewards_train/chosen': '0.26481', 'rewards_train/rejected': '0.052545', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21226', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-151.45', 'loss/train': '0.6188', 'examples_per_second': '30.502', 'grad_norm': '20.5', 'counters/examples': 211584, 'counters/updates': 6612}
train stats after 211616 examples: {'rewards_train/chosen': '0.16298', 'rewards_train/rejected': '0.030786', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1322', 'logps_train/rejected': '-132.06', 'logps_train/chosen': '-134.63', 'loss/train': '0.64546', 'examples_per_second': '31.653', 'grad_norm': '20', 'counters/examples': 211616, 'counters/updates': 6613}
train stats after 211648 examples: {'rewards_train/chosen': '0.24968', 'rewards_train/rejected': '0.18251', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067174', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-147.3', 'loss/train': '0.67109', 'examples_per_second': '31.616', 'grad_norm': '20.25', 'counters/examples': 211648, 'counters/updates': 6614}
train stats after 211680 examples: {'rewards_train/chosen': '0.1484', 'rewards_train/rejected': '-0.06516', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21356', 'logps_train/rejected': '-117.95', 'logps_train/chosen': '-137.07', 'loss/train': '0.60709', 'examples_per_second': '30.688', 'grad_norm': '18.5', 'counters/examples': 211680, 'counters/updates': 6615}
train stats after 211712 examples: {'rewards_train/chosen': '0.20503', 'rewards_train/rejected': '0.032465', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17257', 'logps_train/rejected': '-125.6', 'logps_train/chosen': '-190.1', 'loss/train': '0.62036', 'examples_per_second': '30.232', 'grad_norm': '22.125', 'counters/examples': 211712, 'counters/updates': 6616}
train stats after 211744 examples: {'rewards_train/chosen': '0.20877', 'rewards_train/rejected': '0.067382', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14139', 'logps_train/rejected': '-145.19', 'logps_train/chosen': '-118.63', 'loss/train': '0.64087', 'examples_per_second': '31.057', 'grad_norm': '19.5', 'counters/examples': 211744, 'counters/updates': 6617}
train stats after 211776 examples: {'rewards_train/chosen': '0.19509', 'rewards_train/rejected': '0.192', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.003089', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-143.9', 'loss/train': '0.7089', 'examples_per_second': '31.576', 'grad_norm': '22.5', 'counters/examples': 211776, 'counters/updates': 6618}
train stats after 211808 examples: {'rewards_train/chosen': '0.21294', 'rewards_train/rejected': '0.017027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19591', 'logps_train/rejected': '-89.687', 'logps_train/chosen': '-125.72', 'loss/train': '0.62401', 'examples_per_second': '31.567', 'grad_norm': '17.75', 'counters/examples': 211808, 'counters/updates': 6619}
skipping logging after 211840 examples to avoid logging too frequently
train stats after 211872 examples: {'rewards_train/chosen': '0.20936', 'rewards_train/rejected': '0.13972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069634', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-136.27', 'loss/train': '0.6783', 'examples_per_second': '33.929', 'grad_norm': '21.75', 'counters/examples': 211872, 'counters/updates': 6621}
train stats after 211904 examples: {'rewards_train/chosen': '0.15029', 'rewards_train/rejected': '0.04921', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10108', 'logps_train/rejected': '-95.507', 'logps_train/chosen': '-98.934', 'loss/train': '0.65495', 'examples_per_second': '31.618', 'grad_norm': '17.25', 'counters/examples': 211904, 'counters/updates': 6622}
train stats after 211936 examples: {'rewards_train/chosen': '0.22936', 'rewards_train/rejected': '0.077495', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15186', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-141.43', 'loss/train': '0.63804', 'examples_per_second': '30.173', 'grad_norm': '20.75', 'counters/examples': 211936, 'counters/updates': 6623}
train stats after 211968 examples: {'rewards_train/chosen': '0.15081', 'rewards_train/rejected': '-0.046178', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19699', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-132.79', 'loss/train': '0.61632', 'examples_per_second': '32.622', 'grad_norm': '19.25', 'counters/examples': 211968, 'counters/updates': 6624}
train stats after 212000 examples: {'rewards_train/chosen': '0.2325', 'rewards_train/rejected': '0.077926', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15458', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-167.33', 'loss/train': '0.63852', 'examples_per_second': '30.328', 'grad_norm': '21.375', 'counters/examples': 212000, 'counters/updates': 6625}
train stats after 212032 examples: {'rewards_train/chosen': '0.15575', 'rewards_train/rejected': '-0.055656', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21141', 'logps_train/rejected': '-116.29', 'logps_train/chosen': '-161.81', 'loss/train': '0.60896', 'examples_per_second': '31.68', 'grad_norm': '20', 'counters/examples': 212032, 'counters/updates': 6626}
train stats after 212064 examples: {'rewards_train/chosen': '0.053537', 'rewards_train/rejected': '0.023241', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030296', 'logps_train/rejected': '-85.597', 'logps_train/chosen': '-116.06', 'loss/train': '0.68955', 'examples_per_second': '27.329', 'grad_norm': '17.75', 'counters/examples': 212064, 'counters/updates': 6627}
skipping logging after 212096 examples to avoid logging too frequently
train stats after 212128 examples: {'rewards_train/chosen': '0.16947', 'rewards_train/rejected': '0.00050034', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16897', 'logps_train/rejected': '-86.953', 'logps_train/chosen': '-116.06', 'loss/train': '0.62404', 'examples_per_second': '35.863', 'grad_norm': '16.25', 'counters/examples': 212128, 'counters/updates': 6629}
train stats after 212160 examples: {'rewards_train/chosen': '0.23579', 'rewards_train/rejected': '0.12429', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1115', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-171.9', 'loss/train': '0.6536', 'examples_per_second': '31.606', 'grad_norm': '23.125', 'counters/examples': 212160, 'counters/updates': 6630}
train stats after 212192 examples: {'rewards_train/chosen': '0.20486', 'rewards_train/rejected': '0.097639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10722', 'logps_train/rejected': '-146.22', 'logps_train/chosen': '-175.55', 'loss/train': '0.66753', 'examples_per_second': '31.453', 'grad_norm': '23.875', 'counters/examples': 212192, 'counters/updates': 6631}
train stats after 212224 examples: {'rewards_train/chosen': '0.18071', 'rewards_train/rejected': '0.063267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11744', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-127.63', 'loss/train': '0.64831', 'examples_per_second': '30.304', 'grad_norm': '19.75', 'counters/examples': 212224, 'counters/updates': 6632}
train stats after 212256 examples: {'rewards_train/chosen': '0.1451', 'rewards_train/rejected': '0.025028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12007', 'logps_train/rejected': '-118.97', 'logps_train/chosen': '-134.89', 'loss/train': '0.65078', 'examples_per_second': '30.996', 'grad_norm': '20.25', 'counters/examples': 212256, 'counters/updates': 6633}
train stats after 212288 examples: {'rewards_train/chosen': '0.30347', 'rewards_train/rejected': '0.02023', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28324', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-174.49', 'loss/train': '0.58867', 'examples_per_second': '32.535', 'grad_norm': '20.875', 'counters/examples': 212288, 'counters/updates': 6634}
train stats after 212320 examples: {'rewards_train/chosen': '0.1646', 'rewards_train/rejected': '0.077721', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086881', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-110.6', 'loss/train': '0.66129', 'examples_per_second': '30.98', 'grad_norm': '18.5', 'counters/examples': 212320, 'counters/updates': 6635}
train stats after 212352 examples: {'rewards_train/chosen': '0.14543', 'rewards_train/rejected': '0.040097', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10534', 'logps_train/rejected': '-100.42', 'logps_train/chosen': '-121.04', 'loss/train': '0.66565', 'examples_per_second': '31.383', 'grad_norm': '18.5', 'counters/examples': 212352, 'counters/updates': 6636}
train stats after 212384 examples: {'rewards_train/chosen': '0.1154', 'rewards_train/rejected': '0.023169', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092229', 'logps_train/rejected': '-118', 'logps_train/chosen': '-126.57', 'loss/train': '0.65687', 'examples_per_second': '31.454', 'grad_norm': '18.5', 'counters/examples': 212384, 'counters/updates': 6637}
train stats after 212416 examples: {'rewards_train/chosen': '0.27863', 'rewards_train/rejected': '0.075706', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20293', 'logps_train/rejected': '-114.89', 'logps_train/chosen': '-149.03', 'loss/train': '0.62088', 'examples_per_second': '30.811', 'grad_norm': '19.375', 'counters/examples': 212416, 'counters/updates': 6638}
train stats after 212448 examples: {'rewards_train/chosen': '0.216', 'rewards_train/rejected': '0.10475', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11124', 'logps_train/rejected': '-130.3', 'logps_train/chosen': '-151.44', 'loss/train': '0.65094', 'examples_per_second': '31.535', 'grad_norm': '20.875', 'counters/examples': 212448, 'counters/updates': 6639}
train stats after 212480 examples: {'rewards_train/chosen': '0.14738', 'rewards_train/rejected': '0.010858', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13652', 'logps_train/rejected': '-104.02', 'logps_train/chosen': '-119.1', 'loss/train': '0.63649', 'examples_per_second': '32.087', 'grad_norm': '18', 'counters/examples': 212480, 'counters/updates': 6640}
train stats after 212512 examples: {'rewards_train/chosen': '0.17176', 'rewards_train/rejected': '0.079019', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092745', 'logps_train/rejected': '-105.5', 'logps_train/chosen': '-144.86', 'loss/train': '0.66127', 'examples_per_second': '31.601', 'grad_norm': '19.5', 'counters/examples': 212512, 'counters/updates': 6641}
train stats after 212544 examples: {'rewards_train/chosen': '0.20034', 'rewards_train/rejected': '0.01644', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1839', 'logps_train/rejected': '-104.38', 'logps_train/chosen': '-124.33', 'loss/train': '0.61849', 'examples_per_second': '30.043', 'grad_norm': '16.625', 'counters/examples': 212544, 'counters/updates': 6642}
train stats after 212576 examples: {'rewards_train/chosen': '0.22598', 'rewards_train/rejected': '0.14932', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076658', 'logps_train/rejected': '-128.84', 'logps_train/chosen': '-119.23', 'loss/train': '0.66102', 'examples_per_second': '30.34', 'grad_norm': '20.625', 'counters/examples': 212576, 'counters/updates': 6643}
train stats after 212608 examples: {'rewards_train/chosen': '0.030531', 'rewards_train/rejected': '0.086966', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.056435', 'logps_train/rejected': '-130.84', 'logps_train/chosen': '-115.81', 'loss/train': '0.73791', 'examples_per_second': '31.238', 'grad_norm': '20.625', 'counters/examples': 212608, 'counters/updates': 6644}
train stats after 212640 examples: {'rewards_train/chosen': '0.17946', 'rewards_train/rejected': '0.07755', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10191', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-123.01', 'loss/train': '0.65836', 'examples_per_second': '30.856', 'grad_norm': '20.875', 'counters/examples': 212640, 'counters/updates': 6645}
train stats after 212672 examples: {'rewards_train/chosen': '0.14684', 'rewards_train/rejected': '0.021601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12523', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-146.31', 'loss/train': '0.64205', 'examples_per_second': '32.505', 'grad_norm': '20.25', 'counters/examples': 212672, 'counters/updates': 6646}
train stats after 212704 examples: {'rewards_train/chosen': '0.066724', 'rewards_train/rejected': '0.022832', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043892', 'logps_train/rejected': '-118.52', 'logps_train/chosen': '-157', 'loss/train': '0.68135', 'examples_per_second': '32.658', 'grad_norm': '21.5', 'counters/examples': 212704, 'counters/updates': 6647}
train stats after 212736 examples: {'rewards_train/chosen': '0.13506', 'rewards_train/rejected': '0.061662', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073395', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-142.35', 'loss/train': '0.67588', 'examples_per_second': '32.141', 'grad_norm': '21.5', 'counters/examples': 212736, 'counters/updates': 6648}
skipping logging after 212768 examples to avoid logging too frequently
train stats after 212800 examples: {'rewards_train/chosen': '0.24712', 'rewards_train/rejected': '0.10038', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14673', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-153.48', 'loss/train': '0.64718', 'examples_per_second': '31.27', 'grad_norm': '22', 'counters/examples': 212800, 'counters/updates': 6650}
skipping logging after 212832 examples to avoid logging too frequently
train stats after 212864 examples: {'rewards_train/chosen': '0.25793', 'rewards_train/rejected': '0.047971', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20996', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-131.65', 'loss/train': '0.60565', 'examples_per_second': '30.87', 'grad_norm': '18.75', 'counters/examples': 212864, 'counters/updates': 6652}
skipping logging after 212896 examples to avoid logging too frequently
train stats after 212928 examples: {'rewards_train/chosen': '0.1859', 'rewards_train/rejected': '0.046381', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13952', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-122.98', 'loss/train': '0.65154', 'examples_per_second': '31.212', 'grad_norm': '19.875', 'counters/examples': 212928, 'counters/updates': 6654}
train stats after 212960 examples: {'rewards_train/chosen': '0.14464', 'rewards_train/rejected': '-0.00095975', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14559', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-131.84', 'loss/train': '0.64605', 'examples_per_second': '30.967', 'grad_norm': '18.875', 'counters/examples': 212960, 'counters/updates': 6655}
train stats after 212992 examples: {'rewards_train/chosen': '0.12292', 'rewards_train/rejected': '-0.069667', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19259', 'logps_train/rejected': '-74.959', 'logps_train/chosen': '-150.84', 'loss/train': '0.62091', 'examples_per_second': '31.386', 'grad_norm': '18', 'counters/examples': 212992, 'counters/updates': 6656}
skipping logging after 213024 examples to avoid logging too frequently
train stats after 213056 examples: {'rewards_train/chosen': '0.031172', 'rewards_train/rejected': '-0.034019', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065191', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-118.51', 'loss/train': '0.66947', 'examples_per_second': '31.373', 'grad_norm': '20.75', 'counters/examples': 213056, 'counters/updates': 6658}
train stats after 213088 examples: {'rewards_train/chosen': '0.26921', 'rewards_train/rejected': '0.069569', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19964', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-132.11', 'loss/train': '0.61381', 'examples_per_second': '31.365', 'grad_norm': '17.875', 'counters/examples': 213088, 'counters/updates': 6659}
train stats after 213120 examples: {'rewards_train/chosen': '0.20201', 'rewards_train/rejected': '-0.021573', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22359', 'logps_train/rejected': '-103.38', 'logps_train/chosen': '-137.02', 'loss/train': '0.60671', 'examples_per_second': '30.141', 'grad_norm': '18', 'counters/examples': 213120, 'counters/updates': 6660}
train stats after 213152 examples: {'rewards_train/chosen': '0.10775', 'rewards_train/rejected': '-0.018763', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12651', 'logps_train/rejected': '-93.395', 'logps_train/chosen': '-131.38', 'loss/train': '0.63933', 'examples_per_second': '31.268', 'grad_norm': '18.75', 'counters/examples': 213152, 'counters/updates': 6661}
train stats after 213184 examples: {'rewards_train/chosen': '0.13225', 'rewards_train/rejected': '0.043187', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089064', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-139.83', 'loss/train': '0.66852', 'examples_per_second': '30.628', 'grad_norm': '21.5', 'counters/examples': 213184, 'counters/updates': 6662}
train stats after 213216 examples: {'rewards_train/chosen': '0.2465', 'rewards_train/rejected': '0.079099', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1674', 'logps_train/rejected': '-122.65', 'logps_train/chosen': '-147.79', 'loss/train': '0.62758', 'examples_per_second': '32.695', 'grad_norm': '20.25', 'counters/examples': 213216, 'counters/updates': 6663}
train stats after 213248 examples: {'rewards_train/chosen': '0.17141', 'rewards_train/rejected': '0.022594', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14881', 'logps_train/rejected': '-125.03', 'logps_train/chosen': '-140.84', 'loss/train': '0.6441', 'examples_per_second': '31.814', 'grad_norm': '20', 'counters/examples': 213248, 'counters/updates': 6664}
train stats after 213280 examples: {'rewards_train/chosen': '0.16856', 'rewards_train/rejected': '0.093237', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075318', 'logps_train/rejected': '-114.95', 'logps_train/chosen': '-120.58', 'loss/train': '0.68139', 'examples_per_second': '25.397', 'grad_norm': '20.625', 'counters/examples': 213280, 'counters/updates': 6665}
train stats after 213312 examples: {'rewards_train/chosen': '0.16989', 'rewards_train/rejected': '0.11476', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055132', 'logps_train/rejected': '-107.51', 'logps_train/chosen': '-131.44', 'loss/train': '0.67595', 'examples_per_second': '30.554', 'grad_norm': '20.375', 'counters/examples': 213312, 'counters/updates': 6666}
train stats after 213344 examples: {'rewards_train/chosen': '0.21459', 'rewards_train/rejected': '0.098799', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11579', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-121.11', 'loss/train': '0.65573', 'examples_per_second': '31.106', 'grad_norm': '19.875', 'counters/examples': 213344, 'counters/updates': 6667}
train stats after 213376 examples: {'rewards_train/chosen': '0.11661', 'rewards_train/rejected': '0.022816', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093798', 'logps_train/rejected': '-118.37', 'logps_train/chosen': '-165.33', 'loss/train': '0.65593', 'examples_per_second': '24.812', 'grad_norm': '21.125', 'counters/examples': 213376, 'counters/updates': 6668}
skipping logging after 213408 examples to avoid logging too frequently
train stats after 213440 examples: {'rewards_train/chosen': '0.26155', 'rewards_train/rejected': '0.11283', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14871', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-143.56', 'loss/train': '0.64121', 'examples_per_second': '30.646', 'grad_norm': '21', 'counters/examples': 213440, 'counters/updates': 6670}
skipping logging after 213472 examples to avoid logging too frequently
train stats after 213504 examples: {'rewards_train/chosen': '0.22325', 'rewards_train/rejected': '0.096791', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12646', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-140.78', 'loss/train': '0.65031', 'examples_per_second': '33.983', 'grad_norm': '20.625', 'counters/examples': 213504, 'counters/updates': 6672}
skipping logging after 213536 examples to avoid logging too frequently
train stats after 213568 examples: {'rewards_train/chosen': '0.29593', 'rewards_train/rejected': '0.027262', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.26866', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-125.91', 'loss/train': '0.58915', 'examples_per_second': '33.719', 'grad_norm': '18.75', 'counters/examples': 213568, 'counters/updates': 6674}
skipping logging after 213600 examples to avoid logging too frequently
train stats after 213632 examples: {'rewards_train/chosen': '0.082493', 'rewards_train/rejected': '0.085805', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0033116', 'logps_train/rejected': '-132.23', 'logps_train/chosen': '-148.38', 'loss/train': '0.71273', 'examples_per_second': '31.006', 'grad_norm': '23.375', 'counters/examples': 213632, 'counters/updates': 6676}
skipping logging after 213664 examples to avoid logging too frequently
train stats after 213696 examples: {'rewards_train/chosen': '0.21535', 'rewards_train/rejected': '0.082484', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13287', 'logps_train/rejected': '-146.4', 'logps_train/chosen': '-137.91', 'loss/train': '0.64378', 'examples_per_second': '30.195', 'grad_norm': '21.625', 'counters/examples': 213696, 'counters/updates': 6678}
train stats after 213728 examples: {'rewards_train/chosen': '0.26223', 'rewards_train/rejected': '0.0093146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25292', 'logps_train/rejected': '-104', 'logps_train/chosen': '-137.06', 'loss/train': '0.59407', 'examples_per_second': '31.577', 'grad_norm': '18.25', 'counters/examples': 213728, 'counters/updates': 6679}
train stats after 213760 examples: {'rewards_train/chosen': '0.11829', 'rewards_train/rejected': '0.084062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034226', 'logps_train/rejected': '-150.19', 'logps_train/chosen': '-172.26', 'loss/train': '0.68844', 'examples_per_second': '30.053', 'grad_norm': '23.5', 'counters/examples': 213760, 'counters/updates': 6680}
train stats after 213792 examples: {'rewards_train/chosen': '0.13953', 'rewards_train/rejected': '0.057799', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081726', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-119.62', 'loss/train': '0.6711', 'examples_per_second': '30.124', 'grad_norm': '20.375', 'counters/examples': 213792, 'counters/updates': 6681}
train stats after 213824 examples: {'rewards_train/chosen': '0.33561', 'rewards_train/rejected': '0.013539', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.32207', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-164.35', 'loss/train': '0.56723', 'examples_per_second': '30.832', 'grad_norm': '19.25', 'counters/examples': 213824, 'counters/updates': 6682}
train stats after 213856 examples: {'rewards_train/chosen': '0.16526', 'rewards_train/rejected': '0.071378', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.093887', 'logps_train/rejected': '-102.35', 'logps_train/chosen': '-117.44', 'loss/train': '0.65642', 'examples_per_second': '32.454', 'grad_norm': '18.375', 'counters/examples': 213856, 'counters/updates': 6683}
train stats after 213888 examples: {'rewards_train/chosen': '0.18929', 'rewards_train/rejected': '0.051693', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13759', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-146.43', 'loss/train': '0.63851', 'examples_per_second': '31.636', 'grad_norm': '19.375', 'counters/examples': 213888, 'counters/updates': 6684}
train stats after 213920 examples: {'rewards_train/chosen': '0.1667', 'rewards_train/rejected': '0.023272', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14343', 'logps_train/rejected': '-106.32', 'logps_train/chosen': '-160.36', 'loss/train': '0.64294', 'examples_per_second': '31.79', 'grad_norm': '19.5', 'counters/examples': 213920, 'counters/updates': 6685}
train stats after 213952 examples: {'rewards_train/chosen': '0.12833', 'rewards_train/rejected': '0.021154', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10717', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-133.09', 'loss/train': '0.65554', 'examples_per_second': '31.552', 'grad_norm': '19.75', 'counters/examples': 213952, 'counters/updates': 6686}
train stats after 213984 examples: {'rewards_train/chosen': '0.16785', 'rewards_train/rejected': '0.04614', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12171', 'logps_train/rejected': '-109.46', 'logps_train/chosen': '-131.04', 'loss/train': '0.65336', 'examples_per_second': '31.48', 'grad_norm': '19.5', 'counters/examples': 213984, 'counters/updates': 6687}
train stats after 214016 examples: {'rewards_train/chosen': '0.1811', 'rewards_train/rejected': '0.048801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1323', 'logps_train/rejected': '-113.4', 'logps_train/chosen': '-128.94', 'loss/train': '0.64966', 'examples_per_second': '31.59', 'grad_norm': '19.75', 'counters/examples': 214016, 'counters/updates': 6688}
train stats after 214048 examples: {'rewards_train/chosen': '0.14293', 'rewards_train/rejected': '0.044776', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098152', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-131.64', 'loss/train': '0.65975', 'examples_per_second': '31.041', 'grad_norm': '21.25', 'counters/examples': 214048, 'counters/updates': 6689}
train stats after 214080 examples: {'rewards_train/chosen': '0.19431', 'rewards_train/rejected': '0.016979', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17733', 'logps_train/rejected': '-145.71', 'logps_train/chosen': '-137.38', 'loss/train': '0.6276', 'examples_per_second': '31.541', 'grad_norm': '20.25', 'counters/examples': 214080, 'counters/updates': 6690}
train stats after 214112 examples: {'rewards_train/chosen': '0.11441', 'rewards_train/rejected': '0.02677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087636', 'logps_train/rejected': '-102.97', 'logps_train/chosen': '-121', 'loss/train': '0.66833', 'examples_per_second': '32.113', 'grad_norm': '18.5', 'counters/examples': 214112, 'counters/updates': 6691}
train stats after 214144 examples: {'rewards_train/chosen': '0.052198', 'rewards_train/rejected': '-0.011653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063852', 'logps_train/rejected': '-84.485', 'logps_train/chosen': '-87.931', 'loss/train': '0.67367', 'examples_per_second': '31.582', 'grad_norm': '16.75', 'counters/examples': 214144, 'counters/updates': 6692}
train stats after 214176 examples: {'rewards_train/chosen': '0.086286', 'rewards_train/rejected': '0.018451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067835', 'logps_train/rejected': '-110.86', 'logps_train/chosen': '-135.19', 'loss/train': '0.67654', 'examples_per_second': '31.104', 'grad_norm': '19.5', 'counters/examples': 214176, 'counters/updates': 6693}
train stats after 214208 examples: {'rewards_train/chosen': '0.29464', 'rewards_train/rejected': '-0.0017533', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.29639', 'logps_train/rejected': '-83.201', 'logps_train/chosen': '-120.68', 'loss/train': '0.56781', 'examples_per_second': '32.435', 'grad_norm': '15.938', 'counters/examples': 214208, 'counters/updates': 6694}
train stats after 214240 examples: {'rewards_train/chosen': '0.086445', 'rewards_train/rejected': '0.016145', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0703', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-133.35', 'loss/train': '0.6737', 'examples_per_second': '31.481', 'grad_norm': '21', 'counters/examples': 214240, 'counters/updates': 6695}
train stats after 214272 examples: {'rewards_train/chosen': '0.19326', 'rewards_train/rejected': '-0.014653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20791', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-174.09', 'loss/train': '0.62025', 'examples_per_second': '29.963', 'grad_norm': '19.875', 'counters/examples': 214272, 'counters/updates': 6696}
train stats after 214304 examples: {'rewards_train/chosen': '0.15624', 'rewards_train/rejected': '0.082425', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073816', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-155.51', 'loss/train': '0.67231', 'examples_per_second': '31.926', 'grad_norm': '22', 'counters/examples': 214304, 'counters/updates': 6697}
train stats after 214336 examples: {'rewards_train/chosen': '0.23943', 'rewards_train/rejected': '0.02268', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21675', 'logps_train/rejected': '-123.73', 'logps_train/chosen': '-119.26', 'loss/train': '0.61154', 'examples_per_second': '32.505', 'grad_norm': '18.125', 'counters/examples': 214336, 'counters/updates': 6698}
train stats after 214368 examples: {'rewards_train/chosen': '0.11235', 'rewards_train/rejected': '-0.027285', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13963', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-132.19', 'loss/train': '0.63966', 'examples_per_second': '31.071', 'grad_norm': '18', 'counters/examples': 214368, 'counters/updates': 6699}
train stats after 214400 examples: {'rewards_train/chosen': '0.11451', 'rewards_train/rejected': '-0.021731', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13624', 'logps_train/rejected': '-107.58', 'logps_train/chosen': '-139.62', 'loss/train': '0.64553', 'examples_per_second': '31.936', 'grad_norm': '20', 'counters/examples': 214400, 'counters/updates': 6700}
train stats after 214432 examples: {'rewards_train/chosen': '0.16543', 'rewards_train/rejected': '0.030953', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13447', 'logps_train/rejected': '-101.62', 'logps_train/chosen': '-146.07', 'loss/train': '0.65187', 'examples_per_second': '30.543', 'grad_norm': '21.25', 'counters/examples': 214432, 'counters/updates': 6701}
train stats after 214464 examples: {'rewards_train/chosen': '0.16668', 'rewards_train/rejected': '0.0049815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1617', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-134.82', 'loss/train': '0.63008', 'examples_per_second': '31.533', 'grad_norm': '19.25', 'counters/examples': 214464, 'counters/updates': 6702}
skipping logging after 214496 examples to avoid logging too frequently
train stats after 214528 examples: {'rewards_train/chosen': '0.16017', 'rewards_train/rejected': '0.037641', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12253', 'logps_train/rejected': '-105.24', 'logps_train/chosen': '-143', 'loss/train': '0.65275', 'examples_per_second': '31.551', 'grad_norm': '19', 'counters/examples': 214528, 'counters/updates': 6704}
train stats after 214560 examples: {'rewards_train/chosen': '0.20655', 'rewards_train/rejected': '0.092307', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11425', 'logps_train/rejected': '-105.95', 'logps_train/chosen': '-116.11', 'loss/train': '0.65176', 'examples_per_second': '31.694', 'grad_norm': '18.375', 'counters/examples': 214560, 'counters/updates': 6705}
train stats after 214592 examples: {'rewards_train/chosen': '0.13384', 'rewards_train/rejected': '0.038274', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095562', 'logps_train/rejected': '-124.69', 'logps_train/chosen': '-171', 'loss/train': '0.66703', 'examples_per_second': '31.647', 'grad_norm': '21.875', 'counters/examples': 214592, 'counters/updates': 6706}
skipping logging after 214624 examples to avoid logging too frequently
train stats after 214656 examples: {'rewards_train/chosen': '0.1513', 'rewards_train/rejected': '0.023223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12808', 'logps_train/rejected': '-101.29', 'logps_train/chosen': '-108.33', 'loss/train': '0.64464', 'examples_per_second': '35.745', 'grad_norm': '17.125', 'counters/examples': 214656, 'counters/updates': 6708}
train stats after 214688 examples: {'rewards_train/chosen': '0.096852', 'rewards_train/rejected': '-0.012604', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10946', 'logps_train/rejected': '-79.47', 'logps_train/chosen': '-127.88', 'loss/train': '0.66045', 'examples_per_second': '32.569', 'grad_norm': '17', 'counters/examples': 214688, 'counters/updates': 6709}
train stats after 214720 examples: {'rewards_train/chosen': '0.17545', 'rewards_train/rejected': '0.09438', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081075', 'logps_train/rejected': '-125.94', 'logps_train/chosen': '-145.22', 'loss/train': '0.67205', 'examples_per_second': '29.95', 'grad_norm': '19.125', 'counters/examples': 214720, 'counters/updates': 6710}
train stats after 214752 examples: {'rewards_train/chosen': '0.20787', 'rewards_train/rejected': '-0.039455', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24733', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-186.18', 'loss/train': '0.60087', 'examples_per_second': '31.528', 'grad_norm': '20', 'counters/examples': 214752, 'counters/updates': 6711}
skipping logging after 214784 examples to avoid logging too frequently
train stats after 214816 examples: {'rewards_train/chosen': '0.099664', 'rewards_train/rejected': '0.086046', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013618', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-92.474', 'loss/train': '0.70611', 'examples_per_second': '31.692', 'grad_norm': '21', 'counters/examples': 214816, 'counters/updates': 6713}
train stats after 214848 examples: {'rewards_train/chosen': '0.19595', 'rewards_train/rejected': '0.0016045', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19435', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-120.12', 'loss/train': '0.62029', 'examples_per_second': '31.088', 'grad_norm': '18.875', 'counters/examples': 214848, 'counters/updates': 6714}
train stats after 214880 examples: {'rewards_train/chosen': '0.24218', 'rewards_train/rejected': '0.097435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14474', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-135.32', 'loss/train': '0.63566', 'examples_per_second': '32.929', 'grad_norm': '19.625', 'counters/examples': 214880, 'counters/updates': 6715}
train stats after 214912 examples: {'rewards_train/chosen': '0.095753', 'rewards_train/rejected': '0.064302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03145', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-132.53', 'loss/train': '0.69297', 'examples_per_second': '32.923', 'grad_norm': '21.375', 'counters/examples': 214912, 'counters/updates': 6716}
train stats after 214944 examples: {'rewards_train/chosen': '0.13279', 'rewards_train/rejected': '0.017008', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11578', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-137.38', 'loss/train': '0.64888', 'examples_per_second': '30.548', 'grad_norm': '19.625', 'counters/examples': 214944, 'counters/updates': 6717}
train stats after 214976 examples: {'rewards_train/chosen': '0.18126', 'rewards_train/rejected': '0.074887', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10637', 'logps_train/rejected': '-122.24', 'logps_train/chosen': '-134.98', 'loss/train': '0.65435', 'examples_per_second': '31.422', 'grad_norm': '20.375', 'counters/examples': 214976, 'counters/updates': 6718}
skipping logging after 215008 examples to avoid logging too frequently
train stats after 215040 examples: {'rewards_train/chosen': '0.13897', 'rewards_train/rejected': '0.10391', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03506', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-142.71', 'loss/train': '0.69116', 'examples_per_second': '30.258', 'grad_norm': '21.25', 'counters/examples': 215040, 'counters/updates': 6720}
skipping logging after 215072 examples to avoid logging too frequently
train stats after 215104 examples: {'rewards_train/chosen': '0.21585', 'rewards_train/rejected': '0.064604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15125', 'logps_train/rejected': '-102.56', 'logps_train/chosen': '-177.51', 'loss/train': '0.64185', 'examples_per_second': '33.235', 'grad_norm': '21.375', 'counters/examples': 215104, 'counters/updates': 6722}
train stats after 215136 examples: {'rewards_train/chosen': '0.2035', 'rewards_train/rejected': '-0.021553', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22506', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-165.2', 'loss/train': '0.59834', 'examples_per_second': '31.338', 'grad_norm': '20.25', 'counters/examples': 215136, 'counters/updates': 6723}
train stats after 215168 examples: {'rewards_train/chosen': '0.17516', 'rewards_train/rejected': '0.103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072157', 'logps_train/rejected': '-142.47', 'logps_train/chosen': '-109.8', 'loss/train': '0.67221', 'examples_per_second': '30.11', 'grad_norm': '20.375', 'counters/examples': 215168, 'counters/updates': 6724}
train stats after 215200 examples: {'rewards_train/chosen': '0.12782', 'rewards_train/rejected': '0.067372', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06045', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-117.03', 'loss/train': '0.67536', 'examples_per_second': '31.591', 'grad_norm': '19.5', 'counters/examples': 215200, 'counters/updates': 6725}
train stats after 215232 examples: {'rewards_train/chosen': '0.14854', 'rewards_train/rejected': '0.027543', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.121', 'logps_train/rejected': '-104.87', 'logps_train/chosen': '-124.49', 'loss/train': '0.64558', 'examples_per_second': '31.515', 'grad_norm': '18.125', 'counters/examples': 215232, 'counters/updates': 6726}
train stats after 215264 examples: {'rewards_train/chosen': '0.20212', 'rewards_train/rejected': '0.076945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12517', 'logps_train/rejected': '-183.83', 'logps_train/chosen': '-160', 'loss/train': '0.65785', 'examples_per_second': '30.496', 'grad_norm': '23.875', 'counters/examples': 215264, 'counters/updates': 6727}
train stats after 215296 examples: {'rewards_train/chosen': '0.29378', 'rewards_train/rejected': '-0.0032158', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.29699', 'logps_train/rejected': '-103.44', 'logps_train/chosen': '-146.84', 'loss/train': '0.57815', 'examples_per_second': '30.649', 'grad_norm': '18.125', 'counters/examples': 215296, 'counters/updates': 6728}
train stats after 215328 examples: {'rewards_train/chosen': '0.13851', 'rewards_train/rejected': '-0.01884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15735', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-137.38', 'loss/train': '0.6283', 'examples_per_second': '30.815', 'grad_norm': '20.125', 'counters/examples': 215328, 'counters/updates': 6729}
train stats after 215360 examples: {'rewards_train/chosen': '0.068638', 'rewards_train/rejected': '0.019375', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049263', 'logps_train/rejected': '-93.59', 'logps_train/chosen': '-90.105', 'loss/train': '0.68204', 'examples_per_second': '33.076', 'grad_norm': '17.375', 'counters/examples': 215360, 'counters/updates': 6730}
skipping logging after 215392 examples to avoid logging too frequently
train stats after 215424 examples: {'rewards_train/chosen': '0.18062', 'rewards_train/rejected': '-0.043769', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22439', 'logps_train/rejected': '-120.52', 'logps_train/chosen': '-146.93', 'loss/train': '0.60589', 'examples_per_second': '32.173', 'grad_norm': '19.75', 'counters/examples': 215424, 'counters/updates': 6732}
train stats after 215456 examples: {'rewards_train/chosen': '0.18873', 'rewards_train/rejected': '0.012764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17597', 'logps_train/rejected': '-118.49', 'logps_train/chosen': '-125.44', 'loss/train': '0.6236', 'examples_per_second': '31.025', 'grad_norm': '19.375', 'counters/examples': 215456, 'counters/updates': 6733}
train stats after 215488 examples: {'rewards_train/chosen': '0.076805', 'rewards_train/rejected': '0.077633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00082816', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-127.84', 'loss/train': '0.70545', 'examples_per_second': '31.716', 'grad_norm': '21.875', 'counters/examples': 215488, 'counters/updates': 6734}
skipping logging after 215520 examples to avoid logging too frequently
train stats after 215552 examples: {'rewards_train/chosen': '0.11933', 'rewards_train/rejected': '0.049607', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.069724', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-115.74', 'loss/train': '0.67561', 'examples_per_second': '33.318', 'grad_norm': '19.625', 'counters/examples': 215552, 'counters/updates': 6736}
train stats after 215584 examples: {'rewards_train/chosen': '0.13133', 'rewards_train/rejected': '0.089018', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042307', 'logps_train/rejected': '-105.73', 'logps_train/chosen': '-140.07', 'loss/train': '0.68951', 'examples_per_second': '31.429', 'grad_norm': '20.25', 'counters/examples': 215584, 'counters/updates': 6737}
train stats after 215616 examples: {'rewards_train/chosen': '0.18744', 'rewards_train/rejected': '0.095113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092328', 'logps_train/rejected': '-137.24', 'logps_train/chosen': '-127.43', 'loss/train': '0.66743', 'examples_per_second': '30.511', 'grad_norm': '19.75', 'counters/examples': 215616, 'counters/updates': 6738}
train stats after 215648 examples: {'rewards_train/chosen': '0.21972', 'rewards_train/rejected': '0.06203', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1577', 'logps_train/rejected': '-113.92', 'logps_train/chosen': '-147.47', 'loss/train': '0.63337', 'examples_per_second': '30.746', 'grad_norm': '18.25', 'counters/examples': 215648, 'counters/updates': 6739}
skipping logging after 215680 examples to avoid logging too frequently
train stats after 215712 examples: {'rewards_train/chosen': '0.13281', 'rewards_train/rejected': '-0.025101', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15791', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-161.79', 'loss/train': '0.62646', 'examples_per_second': '33.356', 'grad_norm': '21', 'counters/examples': 215712, 'counters/updates': 6741}
train stats after 215744 examples: {'rewards_train/chosen': '0.18115', 'rewards_train/rejected': '0.05416', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12699', 'logps_train/rejected': '-102.32', 'logps_train/chosen': '-135.37', 'loss/train': '0.64345', 'examples_per_second': '30.428', 'grad_norm': '18.25', 'counters/examples': 215744, 'counters/updates': 6742}
skipping logging after 215776 examples to avoid logging too frequently
train stats after 215808 examples: {'rewards_train/chosen': '0.15307', 'rewards_train/rejected': '0.048965', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1041', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-152.12', 'loss/train': '0.66046', 'examples_per_second': '31.849', 'grad_norm': '21.375', 'counters/examples': 215808, 'counters/updates': 6744}
train stats after 215840 examples: {'rewards_train/chosen': '0.07943', 'rewards_train/rejected': '-0.017158', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096588', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-135.08', 'loss/train': '0.65777', 'examples_per_second': '30.096', 'grad_norm': '21.375', 'counters/examples': 215840, 'counters/updates': 6745}
train stats after 215872 examples: {'rewards_train/chosen': '0.17147', 'rewards_train/rejected': '0.025982', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14549', 'logps_train/rejected': '-109.5', 'logps_train/chosen': '-138.03', 'loss/train': '0.62918', 'examples_per_second': '31.412', 'grad_norm': '18.75', 'counters/examples': 215872, 'counters/updates': 6746}
train stats after 215904 examples: {'rewards_train/chosen': '0.19165', 'rewards_train/rejected': '0.044502', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14715', 'logps_train/rejected': '-151.57', 'logps_train/chosen': '-140.83', 'loss/train': '0.6381', 'examples_per_second': '31.272', 'grad_norm': '21', 'counters/examples': 215904, 'counters/updates': 6747}
skipping logging after 215936 examples to avoid logging too frequently
train stats after 215968 examples: {'rewards_train/chosen': '0.19864', 'rewards_train/rejected': '0.024573', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17407', 'logps_train/rejected': '-90.421', 'logps_train/chosen': '-139.4', 'loss/train': '0.62654', 'examples_per_second': '35.734', 'grad_norm': '19.375', 'counters/examples': 215968, 'counters/updates': 6749}
train stats after 216000 examples: {'rewards_train/chosen': '0.19537', 'rewards_train/rejected': '0.09464', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10073', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-135.73', 'loss/train': '0.66059', 'examples_per_second': '30.136', 'grad_norm': '22.625', 'counters/examples': 216000, 'counters/updates': 6750}
Running evaluation after 216000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.96it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.92it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.88it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 216000: {'rewards_eval/chosen': '0.18405', 'rewards_eval/rejected': '0.043084', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.14097', 'logps_eval/rejected': '-114.96', 'logps_eval/chosen': '-133.71', 'loss/eval': '0.64417'}
skipping save for non epoch
train stats after 216032 examples: {'rewards_train/chosen': '0.21481', 'rewards_train/rejected': '0.073117', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1417', 'logps_train/rejected': '-122.18', 'logps_train/chosen': '-139.13', 'loss/train': '0.65424', 'examples_per_second': '34.214', 'grad_norm': '20.125', 'counters/examples': 216032, 'counters/updates': 6751}
train stats after 216064 examples: {'rewards_train/chosen': '0.21415', 'rewards_train/rejected': '0.063754', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15039', 'logps_train/rejected': '-129.82', 'logps_train/chosen': '-132.73', 'loss/train': '0.63081', 'examples_per_second': '32.445', 'grad_norm': '19.625', 'counters/examples': 216064, 'counters/updates': 6752}
skipping logging after 216096 examples to avoid logging too frequently
train stats after 216128 examples: {'rewards_train/chosen': '0.24363', 'rewards_train/rejected': '-0.011248', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25487', 'logps_train/rejected': '-138.06', 'logps_train/chosen': '-196.12', 'loss/train': '0.58445', 'examples_per_second': '30.119', 'grad_norm': '21.375', 'counters/examples': 216128, 'counters/updates': 6754}
train stats after 216160 examples: {'rewards_train/chosen': '0.076672', 'rewards_train/rejected': '0.059358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017314', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-130.29', 'loss/train': '0.70134', 'examples_per_second': '32.075', 'grad_norm': '21.125', 'counters/examples': 216160, 'counters/updates': 6755}
train stats after 216192 examples: {'rewards_train/chosen': '0.23081', 'rewards_train/rejected': '0.085093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14572', 'logps_train/rejected': '-164.51', 'logps_train/chosen': '-151.3', 'loss/train': '0.6446', 'examples_per_second': '31.281', 'grad_norm': '22.375', 'counters/examples': 216192, 'counters/updates': 6756}
train stats after 216224 examples: {'rewards_train/chosen': '0.14816', 'rewards_train/rejected': '0.034111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11404', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-114.42', 'loss/train': '0.6541', 'examples_per_second': '30.845', 'grad_norm': '19.875', 'counters/examples': 216224, 'counters/updates': 6757}
train stats after 216256 examples: {'rewards_train/chosen': '0.21732', 'rewards_train/rejected': '0.05599', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16133', 'logps_train/rejected': '-128.37', 'logps_train/chosen': '-171.77', 'loss/train': '0.63716', 'examples_per_second': '32.257', 'grad_norm': '21.375', 'counters/examples': 216256, 'counters/updates': 6758}
train stats after 216288 examples: {'rewards_train/chosen': '0.14913', 'rewards_train/rejected': '0.075731', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073403', 'logps_train/rejected': '-155.82', 'logps_train/chosen': '-118.25', 'loss/train': '0.67462', 'examples_per_second': '31.23', 'grad_norm': '20.625', 'counters/examples': 216288, 'counters/updates': 6759}
train stats after 216320 examples: {'rewards_train/chosen': '0.21757', 'rewards_train/rejected': '0.030992', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18658', 'logps_train/rejected': '-142.38', 'logps_train/chosen': '-182.95', 'loss/train': '0.61648', 'examples_per_second': '31.428', 'grad_norm': '21.75', 'counters/examples': 216320, 'counters/updates': 6760}
train stats after 216352 examples: {'rewards_train/chosen': '0.068944', 'rewards_train/rejected': '0.058943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010001', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-154.48', 'loss/train': '0.70062', 'examples_per_second': '32.949', 'grad_norm': '23', 'counters/examples': 216352, 'counters/updates': 6761}
train stats after 216384 examples: {'rewards_train/chosen': '0.19498', 'rewards_train/rejected': '-0.004212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19919', 'logps_train/rejected': '-95.636', 'logps_train/chosen': '-150.92', 'loss/train': '0.61127', 'examples_per_second': '31.501', 'grad_norm': '18', 'counters/examples': 216384, 'counters/updates': 6762}
train stats after 216416 examples: {'rewards_train/chosen': '0.15985', 'rewards_train/rejected': '0.097691', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062163', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-128.19', 'loss/train': '0.68213', 'examples_per_second': '30.072', 'grad_norm': '20.75', 'counters/examples': 216416, 'counters/updates': 6763}
skipping logging after 216448 examples to avoid logging too frequently
train stats after 216480 examples: {'rewards_train/chosen': '0.20374', 'rewards_train/rejected': '0.043844', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15989', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-141.44', 'loss/train': '0.63563', 'examples_per_second': '33.68', 'grad_norm': '20.25', 'counters/examples': 216480, 'counters/updates': 6765}
train stats after 216512 examples: {'rewards_train/chosen': '0.21723', 'rewards_train/rejected': '-0.0054669', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2227', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-129.88', 'loss/train': '0.60039', 'examples_per_second': '31.765', 'grad_norm': '19.125', 'counters/examples': 216512, 'counters/updates': 6766}
train stats after 216544 examples: {'rewards_train/chosen': '0.16422', 'rewards_train/rejected': '0.11156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052659', 'logps_train/rejected': '-156.03', 'logps_train/chosen': '-169.05', 'loss/train': '0.68545', 'examples_per_second': '31.588', 'grad_norm': '24.125', 'counters/examples': 216544, 'counters/updates': 6767}
train stats after 216576 examples: {'rewards_train/chosen': '0.23488', 'rewards_train/rejected': '0.065403', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16948', 'logps_train/rejected': '-143.21', 'logps_train/chosen': '-134.22', 'loss/train': '0.63522', 'examples_per_second': '30.574', 'grad_norm': '21.375', 'counters/examples': 216576, 'counters/updates': 6768}
train stats after 216608 examples: {'rewards_train/chosen': '0.15754', 'rewards_train/rejected': '-0.038789', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19633', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-112.8', 'loss/train': '0.60893', 'examples_per_second': '31.776', 'grad_norm': '18', 'counters/examples': 216608, 'counters/updates': 6769}
train stats after 216640 examples: {'rewards_train/chosen': '0.036157', 'rewards_train/rejected': '0.033815', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0023427', 'logps_train/rejected': '-142.58', 'logps_train/chosen': '-121.04', 'loss/train': '0.7059', 'examples_per_second': '30.076', 'grad_norm': '22.5', 'counters/examples': 216640, 'counters/updates': 6770}
skipping logging after 216672 examples to avoid logging too frequently
train stats after 216704 examples: {'rewards_train/chosen': '0.17914', 'rewards_train/rejected': '-0.036804', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21594', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-158.19', 'loss/train': '0.61563', 'examples_per_second': '32.364', 'grad_norm': '21.625', 'counters/examples': 216704, 'counters/updates': 6772}
train stats after 216736 examples: {'rewards_train/chosen': '0.16697', 'rewards_train/rejected': '0.02669', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14028', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-149.26', 'loss/train': '0.63874', 'examples_per_second': '30.977', 'grad_norm': '21.25', 'counters/examples': 216736, 'counters/updates': 6773}
train stats after 216768 examples: {'rewards_train/chosen': '0.17971', 'rewards_train/rejected': '0.010756', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16896', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-112.05', 'loss/train': '0.63593', 'examples_per_second': '31.372', 'grad_norm': '18.75', 'counters/examples': 216768, 'counters/updates': 6774}
skipping logging after 216800 examples to avoid logging too frequently
train stats after 216832 examples: {'rewards_train/chosen': '0.083266', 'rewards_train/rejected': '-0.02366', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10693', 'logps_train/rejected': '-91.884', 'logps_train/chosen': '-124.99', 'loss/train': '0.65529', 'examples_per_second': '30.291', 'grad_norm': '19.25', 'counters/examples': 216832, 'counters/updates': 6776}
skipping logging after 216864 examples to avoid logging too frequently
train stats after 216896 examples: {'rewards_train/chosen': '0.18498', 'rewards_train/rejected': '0.063149', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12183', 'logps_train/rejected': '-85.045', 'logps_train/chosen': '-119.93', 'loss/train': '0.65081', 'examples_per_second': '35.569', 'grad_norm': '17.5', 'counters/examples': 216896, 'counters/updates': 6778}
train stats after 216928 examples: {'rewards_train/chosen': '0.23003', 'rewards_train/rejected': '0.05956', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17047', 'logps_train/rejected': '-100.18', 'logps_train/chosen': '-126.5', 'loss/train': '0.62123', 'examples_per_second': '33.143', 'grad_norm': '18.5', 'counters/examples': 216928, 'counters/updates': 6779}
train stats after 216960 examples: {'rewards_train/chosen': '0.18363', 'rewards_train/rejected': '0.10214', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08149', 'logps_train/rejected': '-100.02', 'logps_train/chosen': '-136.94', 'loss/train': '0.66128', 'examples_per_second': '31.631', 'grad_norm': '20.75', 'counters/examples': 216960, 'counters/updates': 6780}
train stats after 216992 examples: {'rewards_train/chosen': '0.11432', 'rewards_train/rejected': '-0.001789', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11611', 'logps_train/rejected': '-132.24', 'logps_train/chosen': '-105.93', 'loss/train': '0.65761', 'examples_per_second': '32.937', 'grad_norm': '19', 'counters/examples': 216992, 'counters/updates': 6781}
train stats after 217024 examples: {'rewards_train/chosen': '0.25006', 'rewards_train/rejected': '0.1192', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13086', 'logps_train/rejected': '-108.51', 'logps_train/chosen': '-142.08', 'loss/train': '0.6438', 'examples_per_second': '30.949', 'grad_norm': '19.25', 'counters/examples': 217024, 'counters/updates': 6782}
train stats after 217056 examples: {'rewards_train/chosen': '0.15284', 'rewards_train/rejected': '0.028184', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12466', 'logps_train/rejected': '-118.22', 'logps_train/chosen': '-156.72', 'loss/train': '0.6509', 'examples_per_second': '31.792', 'grad_norm': '21.125', 'counters/examples': 217056, 'counters/updates': 6783}
skipping logging after 217088 examples to avoid logging too frequently
train stats after 217120 examples: {'rewards_train/chosen': '0.155', 'rewards_train/rejected': '0.034917', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12008', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-98.6', 'loss/train': '0.65521', 'examples_per_second': '34.258', 'grad_norm': '18', 'counters/examples': 217120, 'counters/updates': 6785}
train stats after 217152 examples: {'rewards_train/chosen': '0.097658', 'rewards_train/rejected': '0.05924', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038417', 'logps_train/rejected': '-114.12', 'logps_train/chosen': '-145.67', 'loss/train': '0.69189', 'examples_per_second': '31.032', 'grad_norm': '21.375', 'counters/examples': 217152, 'counters/updates': 6786}
train stats after 217184 examples: {'rewards_train/chosen': '0.22125', 'rewards_train/rejected': '0.086433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13482', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-145.11', 'loss/train': '0.64793', 'examples_per_second': '31.652', 'grad_norm': '21.75', 'counters/examples': 217184, 'counters/updates': 6787}
train stats after 217216 examples: {'rewards_train/chosen': '0.11263', 'rewards_train/rejected': '0.062915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049717', 'logps_train/rejected': '-129.37', 'logps_train/chosen': '-138.03', 'loss/train': '0.68142', 'examples_per_second': '31.685', 'grad_norm': '21.625', 'counters/examples': 217216, 'counters/updates': 6788}
train stats after 217248 examples: {'rewards_train/chosen': '0.13168', 'rewards_train/rejected': '-0.045007', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17669', 'logps_train/rejected': '-166.91', 'logps_train/chosen': '-160.66', 'loss/train': '0.62692', 'examples_per_second': '32.426', 'grad_norm': '22.25', 'counters/examples': 217248, 'counters/updates': 6789}
train stats after 217280 examples: {'rewards_train/chosen': '0.20753', 'rewards_train/rejected': '-0.022717', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23025', 'logps_train/rejected': '-171.23', 'logps_train/chosen': '-171.28', 'loss/train': '0.61759', 'examples_per_second': '31.676', 'grad_norm': '22.25', 'counters/examples': 217280, 'counters/updates': 6790}
train stats after 217312 examples: {'rewards_train/chosen': '0.13669', 'rewards_train/rejected': '-0.015601', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15229', 'logps_train/rejected': '-108.12', 'logps_train/chosen': '-118.1', 'loss/train': '0.63064', 'examples_per_second': '31.528', 'grad_norm': '17.875', 'counters/examples': 217312, 'counters/updates': 6791}
train stats after 217344 examples: {'rewards_train/chosen': '0.19636', 'rewards_train/rejected': '0.026348', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17002', 'logps_train/rejected': '-112.74', 'logps_train/chosen': '-154.94', 'loss/train': '0.62287', 'examples_per_second': '31.284', 'grad_norm': '20.5', 'counters/examples': 217344, 'counters/updates': 6792}
train stats after 217376 examples: {'rewards_train/chosen': '0.14778', 'rewards_train/rejected': '0.034911', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11287', 'logps_train/rejected': '-117.27', 'logps_train/chosen': '-117.24', 'loss/train': '0.65028', 'examples_per_second': '30.559', 'grad_norm': '18.5', 'counters/examples': 217376, 'counters/updates': 6793}
train stats after 217408 examples: {'rewards_train/chosen': '0.14731', 'rewards_train/rejected': '0.029427', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11789', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-148.8', 'loss/train': '0.6507', 'examples_per_second': '31.306', 'grad_norm': '20.25', 'counters/examples': 217408, 'counters/updates': 6794}
train stats after 217440 examples: {'rewards_train/chosen': '0.24659', 'rewards_train/rejected': '0.14161', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10497', 'logps_train/rejected': '-107.33', 'logps_train/chosen': '-126.17', 'loss/train': '0.66858', 'examples_per_second': '30.672', 'grad_norm': '20.25', 'counters/examples': 217440, 'counters/updates': 6795}
skipping logging after 217472 examples to avoid logging too frequently
train stats after 217504 examples: {'rewards_train/chosen': '0.30779', 'rewards_train/rejected': '0.081826', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22596', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-162.75', 'loss/train': '0.60992', 'examples_per_second': '33.207', 'grad_norm': '20', 'counters/examples': 217504, 'counters/updates': 6797}
train stats after 217536 examples: {'rewards_train/chosen': '0.1242', 'rewards_train/rejected': '0.022754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10144', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-101.18', 'loss/train': '0.65335', 'examples_per_second': '30.943', 'grad_norm': '18.875', 'counters/examples': 217536, 'counters/updates': 6798}
train stats after 217568 examples: {'rewards_train/chosen': '0.19089', 'rewards_train/rejected': '0.0066451', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18425', 'logps_train/rejected': '-126.76', 'logps_train/chosen': '-108.58', 'loss/train': '0.61997', 'examples_per_second': '32.603', 'grad_norm': '18.625', 'counters/examples': 217568, 'counters/updates': 6799}
skipping logging after 217600 examples to avoid logging too frequently
train stats after 217632 examples: {'rewards_train/chosen': '0.11814', 'rewards_train/rejected': '0.012177', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10596', 'logps_train/rejected': '-119.34', 'logps_train/chosen': '-115.36', 'loss/train': '0.66422', 'examples_per_second': '26.307', 'grad_norm': '19.625', 'counters/examples': 217632, 'counters/updates': 6801}
train stats after 217664 examples: {'rewards_train/chosen': '0.15734', 'rewards_train/rejected': '0.039203', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11814', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-176.05', 'loss/train': '0.65477', 'examples_per_second': '30.187', 'grad_norm': '23.25', 'counters/examples': 217664, 'counters/updates': 6802}
train stats after 217696 examples: {'rewards_train/chosen': '0.092674', 'rewards_train/rejected': '0.0074652', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085209', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-174.54', 'loss/train': '0.66802', 'examples_per_second': '31.643', 'grad_norm': '22.25', 'counters/examples': 217696, 'counters/updates': 6803}
train stats after 217728 examples: {'rewards_train/chosen': '0.27437', 'rewards_train/rejected': '0.007886', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26648', 'logps_train/rejected': '-100.85', 'logps_train/chosen': '-143.96', 'loss/train': '0.59353', 'examples_per_second': '30.159', 'grad_norm': '17.875', 'counters/examples': 217728, 'counters/updates': 6804}
skipping logging after 217760 examples to avoid logging too frequently
train stats after 217792 examples: {'rewards_train/chosen': '0.21659', 'rewards_train/rejected': '0.093567', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12302', 'logps_train/rejected': '-141.98', 'logps_train/chosen': '-152.29', 'loss/train': '0.64774', 'examples_per_second': '32.295', 'grad_norm': '22.5', 'counters/examples': 217792, 'counters/updates': 6806}
train stats after 217824 examples: {'rewards_train/chosen': '0.15701', 'rewards_train/rejected': '-0.038716', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19573', 'logps_train/rejected': '-109.83', 'logps_train/chosen': '-135.27', 'loss/train': '0.6285', 'examples_per_second': '30.727', 'grad_norm': '20.125', 'counters/examples': 217824, 'counters/updates': 6807}
train stats after 217856 examples: {'rewards_train/chosen': '0.1458', 'rewards_train/rejected': '0.052954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092844', 'logps_train/rejected': '-131.04', 'logps_train/chosen': '-156.91', 'loss/train': '0.66282', 'examples_per_second': '30.739', 'grad_norm': '21.5', 'counters/examples': 217856, 'counters/updates': 6808}
train stats after 217888 examples: {'rewards_train/chosen': '0.13312', 'rewards_train/rejected': '0.023064', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11005', 'logps_train/rejected': '-94.828', 'logps_train/chosen': '-124.12', 'loss/train': '0.64548', 'examples_per_second': '31.504', 'grad_norm': '17.75', 'counters/examples': 217888, 'counters/updates': 6809}
skipping logging after 217920 examples to avoid logging too frequently
train stats after 217952 examples: {'rewards_train/chosen': '0.13203', 'rewards_train/rejected': '0.069344', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062682', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-120.95', 'loss/train': '0.67377', 'examples_per_second': '32.112', 'grad_norm': '19.875', 'counters/examples': 217952, 'counters/updates': 6811}
train stats after 217984 examples: {'rewards_train/chosen': '0.18075', 'rewards_train/rejected': '0.066409', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11434', 'logps_train/rejected': '-111.87', 'logps_train/chosen': '-123.35', 'loss/train': '0.65336', 'examples_per_second': '30.349', 'grad_norm': '18.875', 'counters/examples': 217984, 'counters/updates': 6812}
train stats after 218016 examples: {'rewards_train/chosen': '0.17176', 'rewards_train/rejected': '0.10355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068218', 'logps_train/rejected': '-122.92', 'logps_train/chosen': '-153.12', 'loss/train': '0.67243', 'examples_per_second': '31.51', 'grad_norm': '21.375', 'counters/examples': 218016, 'counters/updates': 6813}
train stats after 218048 examples: {'rewards_train/chosen': '0.17779', 'rewards_train/rejected': '0.082817', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094973', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-137.58', 'loss/train': '0.65957', 'examples_per_second': '31.225', 'grad_norm': '20.5', 'counters/examples': 218048, 'counters/updates': 6814}
train stats after 218080 examples: {'rewards_train/chosen': '0.2327', 'rewards_train/rejected': '0.11413', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11857', 'logps_train/rejected': '-152.94', 'logps_train/chosen': '-204.95', 'loss/train': '0.65852', 'examples_per_second': '31.667', 'grad_norm': '23.75', 'counters/examples': 218080, 'counters/updates': 6815}
train stats after 218112 examples: {'rewards_train/chosen': '0.19678', 'rewards_train/rejected': '0.063148', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13363', 'logps_train/rejected': '-104.5', 'logps_train/chosen': '-123.29', 'loss/train': '0.65171', 'examples_per_second': '31.964', 'grad_norm': '18.25', 'counters/examples': 218112, 'counters/updates': 6816}
train stats after 218144 examples: {'rewards_train/chosen': '0.22862', 'rewards_train/rejected': '0.070365', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15826', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-124.35', 'loss/train': '0.62535', 'examples_per_second': '31.649', 'grad_norm': '19.25', 'counters/examples': 218144, 'counters/updates': 6817}
train stats after 218176 examples: {'rewards_train/chosen': '0.090996', 'rewards_train/rejected': '0.058445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032551', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-124.34', 'loss/train': '0.68796', 'examples_per_second': '30.118', 'grad_norm': '21', 'counters/examples': 218176, 'counters/updates': 6818}
train stats after 218208 examples: {'rewards_train/chosen': '0.27712', 'rewards_train/rejected': '-0.0117', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.28882', 'logps_train/rejected': '-140.55', 'logps_train/chosen': '-153.62', 'loss/train': '0.57895', 'examples_per_second': '32.217', 'grad_norm': '18.625', 'counters/examples': 218208, 'counters/updates': 6819}
train stats after 218240 examples: {'rewards_train/chosen': '0.19329', 'rewards_train/rejected': '0.070109', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12318', 'logps_train/rejected': '-148.4', 'logps_train/chosen': '-138.6', 'loss/train': '0.65063', 'examples_per_second': '32.426', 'grad_norm': '22', 'counters/examples': 218240, 'counters/updates': 6820}
train stats after 218272 examples: {'rewards_train/chosen': '0.098111', 'rewards_train/rejected': '-0.049681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14779', 'logps_train/rejected': '-91.942', 'logps_train/chosen': '-105.74', 'loss/train': '0.64066', 'examples_per_second': '31.755', 'grad_norm': '17.75', 'counters/examples': 218272, 'counters/updates': 6821}
train stats after 218304 examples: {'rewards_train/chosen': '0.15245', 'rewards_train/rejected': '0.024818', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12763', 'logps_train/rejected': '-89.825', 'logps_train/chosen': '-140.99', 'loss/train': '0.64658', 'examples_per_second': '31.616', 'grad_norm': '18.25', 'counters/examples': 218304, 'counters/updates': 6822}
train stats after 218336 examples: {'rewards_train/chosen': '0.22045', 'rewards_train/rejected': '0.062353', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1581', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-128.72', 'loss/train': '0.62776', 'examples_per_second': '31.037', 'grad_norm': '18.875', 'counters/examples': 218336, 'counters/updates': 6823}
train stats after 218368 examples: {'rewards_train/chosen': '0.15568', 'rewards_train/rejected': '-0.047412', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20309', 'logps_train/rejected': '-109.25', 'logps_train/chosen': '-113.18', 'loss/train': '0.61009', 'examples_per_second': '31.673', 'grad_norm': '18.625', 'counters/examples': 218368, 'counters/updates': 6824}
train stats after 218400 examples: {'rewards_train/chosen': '0.1035', 'rewards_train/rejected': '0.063201', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040299', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-139.46', 'loss/train': '0.69827', 'examples_per_second': '30.803', 'grad_norm': '22.875', 'counters/examples': 218400, 'counters/updates': 6825}
skipping logging after 218432 examples to avoid logging too frequently
train stats after 218464 examples: {'rewards_train/chosen': '0.28436', 'rewards_train/rejected': '0.043902', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24046', 'logps_train/rejected': '-122.57', 'logps_train/chosen': '-143.03', 'loss/train': '0.59481', 'examples_per_second': '31.405', 'grad_norm': '18.875', 'counters/examples': 218464, 'counters/updates': 6827}
train stats after 218496 examples: {'rewards_train/chosen': '0.23264', 'rewards_train/rejected': '-0.017946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25059', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-149.45', 'loss/train': '0.59418', 'examples_per_second': '31.37', 'grad_norm': '19', 'counters/examples': 218496, 'counters/updates': 6828}
train stats after 218528 examples: {'rewards_train/chosen': '0.18812', 'rewards_train/rejected': '-0.02093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20905', 'logps_train/rejected': '-89.031', 'logps_train/chosen': '-148.56', 'loss/train': '0.61571', 'examples_per_second': '32.14', 'grad_norm': '17', 'counters/examples': 218528, 'counters/updates': 6829}
skipping logging after 218560 examples to avoid logging too frequently
train stats after 218592 examples: {'rewards_train/chosen': '0.16757', 'rewards_train/rejected': '-0.011623', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17919', 'logps_train/rejected': '-124.12', 'logps_train/chosen': '-135.39', 'loss/train': '0.61865', 'examples_per_second': '30.33', 'grad_norm': '19.5', 'counters/examples': 218592, 'counters/updates': 6831}
skipping logging after 218624 examples to avoid logging too frequently
train stats after 218656 examples: {'rewards_train/chosen': '0.28671', 'rewards_train/rejected': '0.012135', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.27457', 'logps_train/rejected': '-105.75', 'logps_train/chosen': '-143.41', 'loss/train': '0.5786', 'examples_per_second': '34.126', 'grad_norm': '17.875', 'counters/examples': 218656, 'counters/updates': 6833}
train stats after 218688 examples: {'rewards_train/chosen': '0.25609', 'rewards_train/rejected': '0.064943', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19114', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-158.95', 'loss/train': '0.62487', 'examples_per_second': '31.853', 'grad_norm': '20.625', 'counters/examples': 218688, 'counters/updates': 6834}
train stats after 218720 examples: {'rewards_train/chosen': '0.19165', 'rewards_train/rejected': '0.12822', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063429', 'logps_train/rejected': '-133.95', 'logps_train/chosen': '-152.74', 'loss/train': '0.67007', 'examples_per_second': '31.36', 'grad_norm': '22.25', 'counters/examples': 218720, 'counters/updates': 6835}
train stats after 218752 examples: {'rewards_train/chosen': '0.2079', 'rewards_train/rejected': '0.020958', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18694', 'logps_train/rejected': '-119.68', 'logps_train/chosen': '-149.52', 'loss/train': '0.61644', 'examples_per_second': '24.566', 'grad_norm': '19.25', 'counters/examples': 218752, 'counters/updates': 6836}
train stats after 218784 examples: {'rewards_train/chosen': '0.13993', 'rewards_train/rejected': '0.0087969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13114', 'logps_train/rejected': '-98.653', 'logps_train/chosen': '-117.46', 'loss/train': '0.64423', 'examples_per_second': '32.367', 'grad_norm': '17.625', 'counters/examples': 218784, 'counters/updates': 6837}
skipping logging after 218816 examples to avoid logging too frequently
train stats after 218848 examples: {'rewards_train/chosen': '0.20816', 'rewards_train/rejected': '-0.011108', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21927', 'logps_train/rejected': '-101.75', 'logps_train/chosen': '-133', 'loss/train': '0.60274', 'examples_per_second': '24.516', 'grad_norm': '17.5', 'counters/examples': 218848, 'counters/updates': 6839}
skipping logging after 218880 examples to avoid logging too frequently
train stats after 218912 examples: {'rewards_train/chosen': '0.081131', 'rewards_train/rejected': '0.0072148', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073916', 'logps_train/rejected': '-88.268', 'logps_train/chosen': '-112.11', 'loss/train': '0.66962', 'examples_per_second': '32.983', 'grad_norm': '17.375', 'counters/examples': 218912, 'counters/updates': 6841}
train stats after 218944 examples: {'rewards_train/chosen': '0.2409', 'rewards_train/rejected': '0.16437', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076532', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-137.27', 'loss/train': '0.6828', 'examples_per_second': '31.509', 'grad_norm': '21.625', 'counters/examples': 218944, 'counters/updates': 6842}
train stats after 218976 examples: {'rewards_train/chosen': '0.1404', 'rewards_train/rejected': '0.10171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038687', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-108.57', 'loss/train': '0.68555', 'examples_per_second': '30.126', 'grad_norm': '20.125', 'counters/examples': 218976, 'counters/updates': 6843}
train stats after 219008 examples: {'rewards_train/chosen': '0.16673', 'rewards_train/rejected': '0.13391', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.032819', 'logps_train/rejected': '-149.69', 'logps_train/chosen': '-149.16', 'loss/train': '0.69058', 'examples_per_second': '31.606', 'grad_norm': '23.125', 'counters/examples': 219008, 'counters/updates': 6844}
train stats after 219040 examples: {'rewards_train/chosen': '0.23032', 'rewards_train/rejected': '-0.013351', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24367', 'logps_train/rejected': '-142.57', 'logps_train/chosen': '-177.96', 'loss/train': '0.59903', 'examples_per_second': '33.164', 'grad_norm': '22.75', 'counters/examples': 219040, 'counters/updates': 6845}
train stats after 219072 examples: {'rewards_train/chosen': '0.2227', 'rewards_train/rejected': '0.0080519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21465', 'logps_train/rejected': '-99.81', 'logps_train/chosen': '-134.67', 'loss/train': '0.60737', 'examples_per_second': '31.509', 'grad_norm': '18.75', 'counters/examples': 219072, 'counters/updates': 6846}
skipping logging after 219104 examples to avoid logging too frequently
train stats after 219136 examples: {'rewards_train/chosen': '0.16134', 'rewards_train/rejected': '0.072123', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089221', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-103.61', 'loss/train': '0.65544', 'examples_per_second': '31.67', 'grad_norm': '19.875', 'counters/examples': 219136, 'counters/updates': 6848}
skipping logging after 219168 examples to avoid logging too frequently
train stats after 219200 examples: {'rewards_train/chosen': '0.26056', 'rewards_train/rejected': '0.10963', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15093', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-152.11', 'loss/train': '0.63008', 'examples_per_second': '31.076', 'grad_norm': '19.5', 'counters/examples': 219200, 'counters/updates': 6850}
train stats after 219232 examples: {'rewards_train/chosen': '0.097764', 'rewards_train/rejected': '0.0064356', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091329', 'logps_train/rejected': '-110.12', 'logps_train/chosen': '-104.15', 'loss/train': '0.6637', 'examples_per_second': '30.59', 'grad_norm': '18.875', 'counters/examples': 219232, 'counters/updates': 6851}
skipping logging after 219264 examples to avoid logging too frequently
train stats after 219296 examples: {'rewards_train/chosen': '0.30333', 'rewards_train/rejected': '-0.017724', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.32105', 'logps_train/rejected': '-116.81', 'logps_train/chosen': '-131.18', 'loss/train': '0.56989', 'examples_per_second': '32.26', 'grad_norm': '18.625', 'counters/examples': 219296, 'counters/updates': 6853}
train stats after 219328 examples: {'rewards_train/chosen': '0.12059', 'rewards_train/rejected': '0.028807', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09178', 'logps_train/rejected': '-123.15', 'logps_train/chosen': '-145.43', 'loss/train': '0.66496', 'examples_per_second': '31.507', 'grad_norm': '21.125', 'counters/examples': 219328, 'counters/updates': 6854}
train stats after 219360 examples: {'rewards_train/chosen': '0.15491', 'rewards_train/rejected': '-0.058775', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21368', 'logps_train/rejected': '-131.35', 'logps_train/chosen': '-122.02', 'loss/train': '0.60986', 'examples_per_second': '32.455', 'grad_norm': '18.625', 'counters/examples': 219360, 'counters/updates': 6855}
skipping logging after 219392 examples to avoid logging too frequently
train stats after 219424 examples: {'rewards_train/chosen': '0.2133', 'rewards_train/rejected': '-0.052021', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26532', 'logps_train/rejected': '-99.008', 'logps_train/chosen': '-153.87', 'loss/train': '0.5949', 'examples_per_second': '32.508', 'grad_norm': '19.875', 'counters/examples': 219424, 'counters/updates': 6857}
train stats after 219456 examples: {'rewards_train/chosen': '0.16794', 'rewards_train/rejected': '0.056744', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1112', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-147.73', 'loss/train': '0.65153', 'examples_per_second': '30.729', 'grad_norm': '19.375', 'counters/examples': 219456, 'counters/updates': 6858}
train stats after 219488 examples: {'rewards_train/chosen': '0.13109', 'rewards_train/rejected': '0.15319', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0221', 'logps_train/rejected': '-146.63', 'logps_train/chosen': '-155.4', 'loss/train': '0.71708', 'examples_per_second': '31.689', 'grad_norm': '23.625', 'counters/examples': 219488, 'counters/updates': 6859}
skipping logging after 219520 examples to avoid logging too frequently
train stats after 219552 examples: {'rewards_train/chosen': '0.20611', 'rewards_train/rejected': '-0.021024', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22713', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-129.9', 'loss/train': '0.60411', 'examples_per_second': '32.34', 'grad_norm': '18.375', 'counters/examples': 219552, 'counters/updates': 6861}
train stats after 219584 examples: {'rewards_train/chosen': '0.27414', 'rewards_train/rejected': '0.014853', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25929', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-159.6', 'loss/train': '0.59203', 'examples_per_second': '30.602', 'grad_norm': '21', 'counters/examples': 219584, 'counters/updates': 6862}
train stats after 219616 examples: {'rewards_train/chosen': '0.13935', 'rewards_train/rejected': '0.036065', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10329', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-157.28', 'loss/train': '0.65899', 'examples_per_second': '30.93', 'grad_norm': '21.5', 'counters/examples': 219616, 'counters/updates': 6863}
train stats after 219648 examples: {'rewards_train/chosen': '0.15441', 'rewards_train/rejected': '0.026448', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12797', 'logps_train/rejected': '-101.87', 'logps_train/chosen': '-128.92', 'loss/train': '0.65118', 'examples_per_second': '32.221', 'grad_norm': '19.375', 'counters/examples': 219648, 'counters/updates': 6864}
train stats after 219680 examples: {'rewards_train/chosen': '0.15851', 'rewards_train/rejected': '-0.043524', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20203', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-143.41', 'loss/train': '0.61095', 'examples_per_second': '31.587', 'grad_norm': '19.5', 'counters/examples': 219680, 'counters/updates': 6865}
train stats after 219712 examples: {'rewards_train/chosen': '0.19348', 'rewards_train/rejected': '0.076554', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11693', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-154.23', 'loss/train': '0.65272', 'examples_per_second': '30.843', 'grad_norm': '20.625', 'counters/examples': 219712, 'counters/updates': 6866}
train stats after 219744 examples: {'rewards_train/chosen': '0.22398', 'rewards_train/rejected': '0.052769', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17121', 'logps_train/rejected': '-100.66', 'logps_train/chosen': '-128.56', 'loss/train': '0.63099', 'examples_per_second': '31.375', 'grad_norm': '18.375', 'counters/examples': 219744, 'counters/updates': 6867}
train stats after 219776 examples: {'rewards_train/chosen': '0.20058', 'rewards_train/rejected': '0.039692', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16088', 'logps_train/rejected': '-137.74', 'logps_train/chosen': '-140.27', 'loss/train': '0.62974', 'examples_per_second': '31.829', 'grad_norm': '20.5', 'counters/examples': 219776, 'counters/updates': 6868}
train stats after 219808 examples: {'rewards_train/chosen': '0.10419', 'rewards_train/rejected': '0.090025', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014165', 'logps_train/rejected': '-102.9', 'logps_train/chosen': '-141.1', 'loss/train': '0.69899', 'examples_per_second': '31.743', 'grad_norm': '20.5', 'counters/examples': 219808, 'counters/updates': 6869}
train stats after 219840 examples: {'rewards_train/chosen': '0.24447', 'rewards_train/rejected': '0.20336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041102', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-116.14', 'loss/train': '0.68961', 'examples_per_second': '31.554', 'grad_norm': '19.875', 'counters/examples': 219840, 'counters/updates': 6870}
train stats after 219872 examples: {'rewards_train/chosen': '0.19624', 'rewards_train/rejected': '0.028525', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16771', 'logps_train/rejected': '-113.73', 'logps_train/chosen': '-143.97', 'loss/train': '0.6303', 'examples_per_second': '31.533', 'grad_norm': '19.5', 'counters/examples': 219872, 'counters/updates': 6871}
skipping logging after 219904 examples to avoid logging too frequently
train stats after 219936 examples: {'rewards_train/chosen': '0.12682', 'rewards_train/rejected': '0.010714', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11611', 'logps_train/rejected': '-150.37', 'logps_train/chosen': '-152.26', 'loss/train': '0.65278', 'examples_per_second': '30.208', 'grad_norm': '21.75', 'counters/examples': 219936, 'counters/updates': 6873}
skipping logging after 219968 examples to avoid logging too frequently
train stats after 220000 examples: {'rewards_train/chosen': '0.13784', 'rewards_train/rejected': '-0.041382', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17922', 'logps_train/rejected': '-141.35', 'logps_train/chosen': '-156.99', 'loss/train': '0.62054', 'examples_per_second': '30.452', 'grad_norm': '22.25', 'counters/examples': 220000, 'counters/updates': 6875}
train stats after 220032 examples: {'rewards_train/chosen': '0.12606', 'rewards_train/rejected': '0.053296', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072767', 'logps_train/rejected': '-135.14', 'logps_train/chosen': '-144.18', 'loss/train': '0.66707', 'examples_per_second': '32.529', 'grad_norm': '21.75', 'counters/examples': 220032, 'counters/updates': 6876}
train stats after 220064 examples: {'rewards_train/chosen': '0.12012', 'rewards_train/rejected': '-0.037065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15719', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-92.228', 'loss/train': '0.63575', 'examples_per_second': '31.605', 'grad_norm': '18.375', 'counters/examples': 220064, 'counters/updates': 6877}
train stats after 220096 examples: {'rewards_train/chosen': '0.069543', 'rewards_train/rejected': '-0.017554', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087097', 'logps_train/rejected': '-90.736', 'logps_train/chosen': '-103.28', 'loss/train': '0.66044', 'examples_per_second': '31.399', 'grad_norm': '18.25', 'counters/examples': 220096, 'counters/updates': 6878}
train stats after 220128 examples: {'rewards_train/chosen': '0.1388', 'rewards_train/rejected': '0.041395', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0974', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-131.01', 'loss/train': '0.66087', 'examples_per_second': '31.388', 'grad_norm': '21', 'counters/examples': 220128, 'counters/updates': 6879}
train stats after 220160 examples: {'rewards_train/chosen': '0.22165', 'rewards_train/rejected': '0.070874', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15077', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-126.29', 'loss/train': '0.62609', 'examples_per_second': '31.004', 'grad_norm': '18', 'counters/examples': 220160, 'counters/updates': 6880}
skipping logging after 220192 examples to avoid logging too frequently
train stats after 220224 examples: {'rewards_train/chosen': '0.10731', 'rewards_train/rejected': '0.024079', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083236', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-128.52', 'loss/train': '0.66423', 'examples_per_second': '34.07', 'grad_norm': '20.875', 'counters/examples': 220224, 'counters/updates': 6882}
skipping logging after 220256 examples to avoid logging too frequently
train stats after 220288 examples: {'rewards_train/chosen': '0.18173', 'rewards_train/rejected': '-0.0056519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18739', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-138.63', 'loss/train': '0.62278', 'examples_per_second': '30.802', 'grad_norm': '20.75', 'counters/examples': 220288, 'counters/updates': 6884}
skipping logging after 220320 examples to avoid logging too frequently
train stats after 220352 examples: {'rewards_train/chosen': '0.095023', 'rewards_train/rejected': '0.00047321', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09455', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-124.33', 'loss/train': '0.66358', 'examples_per_second': '31.575', 'grad_norm': '19.625', 'counters/examples': 220352, 'counters/updates': 6886}
skipping logging after 220384 examples to avoid logging too frequently
train stats after 220416 examples: {'rewards_train/chosen': '0.23935', 'rewards_train/rejected': '0.016809', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22254', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-147.75', 'loss/train': '0.60008', 'examples_per_second': '30.097', 'grad_norm': '19.25', 'counters/examples': 220416, 'counters/updates': 6888}
train stats after 220448 examples: {'rewards_train/chosen': '0.20891', 'rewards_train/rejected': '0.13689', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.072022', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-130.24', 'loss/train': '0.67985', 'examples_per_second': '31.619', 'grad_norm': '20.875', 'counters/examples': 220448, 'counters/updates': 6889}
train stats after 220480 examples: {'rewards_train/chosen': '0.18411', 'rewards_train/rejected': '0.044059', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14005', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-148.21', 'loss/train': '0.64403', 'examples_per_second': '31.6', 'grad_norm': '20.75', 'counters/examples': 220480, 'counters/updates': 6890}
train stats after 220512 examples: {'rewards_train/chosen': '0.18182', 'rewards_train/rejected': '0.057685', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12413', 'logps_train/rejected': '-107.48', 'logps_train/chosen': '-101.8', 'loss/train': '0.65431', 'examples_per_second': '30.778', 'grad_norm': '18.75', 'counters/examples': 220512, 'counters/updates': 6891}
train stats after 220544 examples: {'rewards_train/chosen': '0.19142', 'rewards_train/rejected': '0.12903', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062396', 'logps_train/rejected': '-135.61', 'logps_train/chosen': '-145.01', 'loss/train': '0.68102', 'examples_per_second': '32.274', 'grad_norm': '22.125', 'counters/examples': 220544, 'counters/updates': 6892}
train stats after 220576 examples: {'rewards_train/chosen': '0.23871', 'rewards_train/rejected': '0.022831', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21588', 'logps_train/rejected': '-158.33', 'logps_train/chosen': '-165.61', 'loss/train': '0.61051', 'examples_per_second': '31.072', 'grad_norm': '20.625', 'counters/examples': 220576, 'counters/updates': 6893}
train stats after 220608 examples: {'rewards_train/chosen': '0.19163', 'rewards_train/rejected': '-0.025892', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21752', 'logps_train/rejected': '-98.317', 'logps_train/chosen': '-119.91', 'loss/train': '0.60715', 'examples_per_second': '31.522', 'grad_norm': '18', 'counters/examples': 220608, 'counters/updates': 6894}
train stats after 220640 examples: {'rewards_train/chosen': '0.065128', 'rewards_train/rejected': '0.029271', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035858', 'logps_train/rejected': '-133.13', 'logps_train/chosen': '-125.05', 'loss/train': '0.69026', 'examples_per_second': '32.2', 'grad_norm': '21.125', 'counters/examples': 220640, 'counters/updates': 6895}
train stats after 220672 examples: {'rewards_train/chosen': '0.35903', 'rewards_train/rejected': '0.04893', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3101', 'logps_train/rejected': '-183.07', 'logps_train/chosen': '-189.26', 'loss/train': '0.58334', 'examples_per_second': '32.506', 'grad_norm': '22', 'counters/examples': 220672, 'counters/updates': 6896}
train stats after 220704 examples: {'rewards_train/chosen': '0.1798', 'rewards_train/rejected': '-0.072768', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.25257', 'logps_train/rejected': '-109.28', 'logps_train/chosen': '-129', 'loss/train': '0.59752', 'examples_per_second': '30.81', 'grad_norm': '17.5', 'counters/examples': 220704, 'counters/updates': 6897}
train stats after 220736 examples: {'rewards_train/chosen': '0.32686', 'rewards_train/rejected': '0.095454', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2314', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-148.91', 'loss/train': '0.61027', 'examples_per_second': '30.124', 'grad_norm': '19.5', 'counters/examples': 220736, 'counters/updates': 6898}
train stats after 220768 examples: {'rewards_train/chosen': '0.19685', 'rewards_train/rejected': '0.030294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16655', 'logps_train/rejected': '-115.1', 'logps_train/chosen': '-138.96', 'loss/train': '0.63859', 'examples_per_second': '33.16', 'grad_norm': '19', 'counters/examples': 220768, 'counters/updates': 6899}
skipping logging after 220800 examples to avoid logging too frequently
train stats after 220832 examples: {'rewards_train/chosen': '0.27145', 'rewards_train/rejected': '0.10273', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16872', 'logps_train/rejected': '-94.864', 'logps_train/chosen': '-126.46', 'loss/train': '0.63351', 'examples_per_second': '32.64', 'grad_norm': '17', 'counters/examples': 220832, 'counters/updates': 6901}
train stats after 220864 examples: {'rewards_train/chosen': '0.10789', 'rewards_train/rejected': '0.0076772', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10021', 'logps_train/rejected': '-107.31', 'logps_train/chosen': '-121.04', 'loss/train': '0.6599', 'examples_per_second': '31.511', 'grad_norm': '19.25', 'counters/examples': 220864, 'counters/updates': 6902}
skipping logging after 220896 examples to avoid logging too frequently
train stats after 220928 examples: {'rewards_train/chosen': '0.11597', 'rewards_train/rejected': '0.13572', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.019749', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-92.985', 'loss/train': '0.71595', 'examples_per_second': '31.157', 'grad_norm': '20.75', 'counters/examples': 220928, 'counters/updates': 6904}
train stats after 220960 examples: {'rewards_train/chosen': '0.17688', 'rewards_train/rejected': '-0.0101', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18698', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-127.82', 'loss/train': '0.62378', 'examples_per_second': '32.871', 'grad_norm': '17.75', 'counters/examples': 220960, 'counters/updates': 6905}
train stats after 220992 examples: {'rewards_train/chosen': '0.17267', 'rewards_train/rejected': '-0.019063', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19173', 'logps_train/rejected': '-150.15', 'logps_train/chosen': '-145.01', 'loss/train': '0.61148', 'examples_per_second': '31.534', 'grad_norm': '20.375', 'counters/examples': 220992, 'counters/updates': 6906}
train stats after 221024 examples: {'rewards_train/chosen': '0.086099', 'rewards_train/rejected': '-0.011957', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098056', 'logps_train/rejected': '-110.67', 'logps_train/chosen': '-139.93', 'loss/train': '0.66235', 'examples_per_second': '31.539', 'grad_norm': '20.25', 'counters/examples': 221024, 'counters/updates': 6907}
train stats after 221056 examples: {'rewards_train/chosen': '0.15797', 'rewards_train/rejected': '-0.024422', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18239', 'logps_train/rejected': '-117.12', 'logps_train/chosen': '-127.5', 'loss/train': '0.62582', 'examples_per_second': '31.381', 'grad_norm': '19.25', 'counters/examples': 221056, 'counters/updates': 6908}
train stats after 221088 examples: {'rewards_train/chosen': '0.17515', 'rewards_train/rejected': '0.077231', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097916', 'logps_train/rejected': '-110.83', 'logps_train/chosen': '-136.69', 'loss/train': '0.67041', 'examples_per_second': '32.449', 'grad_norm': '19.375', 'counters/examples': 221088, 'counters/updates': 6909}
train stats after 221120 examples: {'rewards_train/chosen': '0.19865', 'rewards_train/rejected': '-0.068626', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.26728', 'logps_train/rejected': '-120.71', 'logps_train/chosen': '-127.98', 'loss/train': '0.58489', 'examples_per_second': '31.64', 'grad_norm': '19.375', 'counters/examples': 221120, 'counters/updates': 6910}
train stats after 221152 examples: {'rewards_train/chosen': '0.11354', 'rewards_train/rejected': '0.059267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054272', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-88.727', 'loss/train': '0.67789', 'examples_per_second': '30.767', 'grad_norm': '19.125', 'counters/examples': 221152, 'counters/updates': 6911}
train stats after 221184 examples: {'rewards_train/chosen': '0.1545', 'rewards_train/rejected': '0.019745', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13475', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-144.63', 'loss/train': '0.64159', 'examples_per_second': '30.428', 'grad_norm': '19.75', 'counters/examples': 221184, 'counters/updates': 6912}
train stats after 221216 examples: {'rewards_train/chosen': '0.11045', 'rewards_train/rejected': '0.068579', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041875', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-127.91', 'loss/train': '0.68612', 'examples_per_second': '32.217', 'grad_norm': '20.375', 'counters/examples': 221216, 'counters/updates': 6913}
train stats after 221248 examples: {'rewards_train/chosen': '0.087238', 'rewards_train/rejected': '0.018388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068849', 'logps_train/rejected': '-111.94', 'logps_train/chosen': '-127.98', 'loss/train': '0.67051', 'examples_per_second': '31.611', 'grad_norm': '19.625', 'counters/examples': 221248, 'counters/updates': 6914}
skipping logging after 221280 examples to avoid logging too frequently
train stats after 221312 examples: {'rewards_train/chosen': '0.14901', 'rewards_train/rejected': '0.07337', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075643', 'logps_train/rejected': '-92.029', 'logps_train/chosen': '-128.37', 'loss/train': '0.66788', 'examples_per_second': '35.312', 'grad_norm': '19.25', 'counters/examples': 221312, 'counters/updates': 6916}
train stats after 221344 examples: {'rewards_train/chosen': '0.16642', 'rewards_train/rejected': '0.081946', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084475', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-109.05', 'loss/train': '0.66286', 'examples_per_second': '31.578', 'grad_norm': '19.875', 'counters/examples': 221344, 'counters/updates': 6917}
skipping logging after 221376 examples to avoid logging too frequently
train stats after 221408 examples: {'rewards_train/chosen': '0.20225', 'rewards_train/rejected': '-0.017622', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21987', 'logps_train/rejected': '-91.956', 'logps_train/chosen': '-133.22', 'loss/train': '0.60418', 'examples_per_second': '33.343', 'grad_norm': '16.125', 'counters/examples': 221408, 'counters/updates': 6919}
skipping logging after 221440 examples to avoid logging too frequently
train stats after 221472 examples: {'rewards_train/chosen': '0.19885', 'rewards_train/rejected': '-0.015135', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21399', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-124.21', 'loss/train': '0.60682', 'examples_per_second': '31.543', 'grad_norm': '18.25', 'counters/examples': 221472, 'counters/updates': 6921}
skipping logging after 221504 examples to avoid logging too frequently
train stats after 221536 examples: {'rewards_train/chosen': '0.27156', 'rewards_train/rejected': '0.16259', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10897', 'logps_train/rejected': '-149.53', 'logps_train/chosen': '-146.34', 'loss/train': '0.66098', 'examples_per_second': '30.121', 'grad_norm': '22.25', 'counters/examples': 221536, 'counters/updates': 6923}
skipping logging after 221568 examples to avoid logging too frequently
train stats after 221600 examples: {'rewards_train/chosen': '0.2079', 'rewards_train/rejected': '0.038995', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16891', 'logps_train/rejected': '-107.9', 'logps_train/chosen': '-129.97', 'loss/train': '0.61913', 'examples_per_second': '34.719', 'grad_norm': '19', 'counters/examples': 221600, 'counters/updates': 6925}
train stats after 221632 examples: {'rewards_train/chosen': '0.20126', 'rewards_train/rejected': '0.021', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18026', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-172.35', 'loss/train': '0.62974', 'examples_per_second': '30.985', 'grad_norm': '19.875', 'counters/examples': 221632, 'counters/updates': 6926}
skipping logging after 221664 examples to avoid logging too frequently
train stats after 221696 examples: {'rewards_train/chosen': '0.19398', 'rewards_train/rejected': '0.15576', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038224', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-105.77', 'loss/train': '0.69335', 'examples_per_second': '32.173', 'grad_norm': '21', 'counters/examples': 221696, 'counters/updates': 6928}
train stats after 221728 examples: {'rewards_train/chosen': '0.18962', 'rewards_train/rejected': '0.050961', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13866', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-136.08', 'loss/train': '0.6373', 'examples_per_second': '33.087', 'grad_norm': '19', 'counters/examples': 221728, 'counters/updates': 6929}
train stats after 221760 examples: {'rewards_train/chosen': '0.16195', 'rewards_train/rejected': '0.015767', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14618', 'logps_train/rejected': '-95.139', 'logps_train/chosen': '-114.4', 'loss/train': '0.63131', 'examples_per_second': '32.638', 'grad_norm': '18', 'counters/examples': 221760, 'counters/updates': 6930}
train stats after 221792 examples: {'rewards_train/chosen': '0.13397', 'rewards_train/rejected': '0.029882', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10409', 'logps_train/rejected': '-118.8', 'logps_train/chosen': '-126.62', 'loss/train': '0.65762', 'examples_per_second': '30.088', 'grad_norm': '19.375', 'counters/examples': 221792, 'counters/updates': 6931}
train stats after 221824 examples: {'rewards_train/chosen': '0.20954', 'rewards_train/rejected': '0.12174', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087795', 'logps_train/rejected': '-151.88', 'logps_train/chosen': '-146.96', 'loss/train': '0.66718', 'examples_per_second': '31.594', 'grad_norm': '20.75', 'counters/examples': 221824, 'counters/updates': 6932}
skipping logging after 221856 examples to avoid logging too frequently
train stats after 221888 examples: {'rewards_train/chosen': '0.24334', 'rewards_train/rejected': '0.1221', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12124', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-134.96', 'loss/train': '0.6507', 'examples_per_second': '30.994', 'grad_norm': '19.625', 'counters/examples': 221888, 'counters/updates': 6934}
skipping logging after 221920 examples to avoid logging too frequently
train stats after 221952 examples: {'rewards_train/chosen': '0.13494', 'rewards_train/rejected': '0.084008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050934', 'logps_train/rejected': '-136.8', 'logps_train/chosen': '-165.09', 'loss/train': '0.69088', 'examples_per_second': '30.441', 'grad_norm': '23.625', 'counters/examples': 221952, 'counters/updates': 6936}
train stats after 221984 examples: {'rewards_train/chosen': '0.1263', 'rewards_train/rejected': '0.013648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11265', 'logps_train/rejected': '-92.764', 'logps_train/chosen': '-113.45', 'loss/train': '0.66437', 'examples_per_second': '31.785', 'grad_norm': '18.875', 'counters/examples': 221984, 'counters/updates': 6937}
train stats after 222016 examples: {'rewards_train/chosen': '0.23406', 'rewards_train/rejected': '0.021496', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21257', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-114.73', 'loss/train': '0.60375', 'examples_per_second': '30.895', 'grad_norm': '18.875', 'counters/examples': 222016, 'counters/updates': 6938}
train stats after 222048 examples: {'rewards_train/chosen': '0.086093', 'rewards_train/rejected': '0.030543', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05555', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-112.58', 'loss/train': '0.67965', 'examples_per_second': '32.862', 'grad_norm': '19.5', 'counters/examples': 222048, 'counters/updates': 6939}
train stats after 222080 examples: {'rewards_train/chosen': '0.21782', 'rewards_train/rejected': '0.022185', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19564', 'logps_train/rejected': '-118.93', 'logps_train/chosen': '-178.19', 'loss/train': '0.61632', 'examples_per_second': '32.329', 'grad_norm': '21.75', 'counters/examples': 222080, 'counters/updates': 6940}
train stats after 222112 examples: {'rewards_train/chosen': '0.1842', 'rewards_train/rejected': '0.077927', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10627', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-135.53', 'loss/train': '0.65585', 'examples_per_second': '31.531', 'grad_norm': '21.125', 'counters/examples': 222112, 'counters/updates': 6941}
train stats after 222144 examples: {'rewards_train/chosen': '0.13624', 'rewards_train/rejected': '0.030474', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10577', 'logps_train/rejected': '-134.07', 'logps_train/chosen': '-184.16', 'loss/train': '0.66393', 'examples_per_second': '31.267', 'grad_norm': '24.25', 'counters/examples': 222144, 'counters/updates': 6942}
skipping logging after 222176 examples to avoid logging too frequently
train stats after 222208 examples: {'rewards_train/chosen': '0.18313', 'rewards_train/rejected': '0.063355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11978', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-146.53', 'loss/train': '0.66555', 'examples_per_second': '30.616', 'grad_norm': '21.5', 'counters/examples': 222208, 'counters/updates': 6944}
train stats after 222240 examples: {'rewards_train/chosen': '0.19961', 'rewards_train/rejected': '0.039221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16039', 'logps_train/rejected': '-109.97', 'logps_train/chosen': '-128.3', 'loss/train': '0.62866', 'examples_per_second': '32.219', 'grad_norm': '18.125', 'counters/examples': 222240, 'counters/updates': 6945}
train stats after 222272 examples: {'rewards_train/chosen': '0.14199', 'rewards_train/rejected': '0.027433', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11455', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-123.31', 'loss/train': '0.65398', 'examples_per_second': '30.926', 'grad_norm': '18.5', 'counters/examples': 222272, 'counters/updates': 6946}
train stats after 222304 examples: {'rewards_train/chosen': '0.09471', 'rewards_train/rejected': '-0.027616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12233', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-112.71', 'loss/train': '0.64594', 'examples_per_second': '30.1', 'grad_norm': '19.875', 'counters/examples': 222304, 'counters/updates': 6947}
train stats after 222336 examples: {'rewards_train/chosen': '0.056665', 'rewards_train/rejected': '0.052194', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0044712', 'logps_train/rejected': '-100.9', 'logps_train/chosen': '-132.39', 'loss/train': '0.69945', 'examples_per_second': '31.578', 'grad_norm': '19.375', 'counters/examples': 222336, 'counters/updates': 6948}
train stats after 222368 examples: {'rewards_train/chosen': '0.22463', 'rewards_train/rejected': '0.048976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17565', 'logps_train/rejected': '-99.022', 'logps_train/chosen': '-114.85', 'loss/train': '0.6276', 'examples_per_second': '31.507', 'grad_norm': '17.25', 'counters/examples': 222368, 'counters/updates': 6949}
train stats after 222400 examples: {'rewards_train/chosen': '0.17434', 'rewards_train/rejected': '-0.0037317', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17808', 'logps_train/rejected': '-83.017', 'logps_train/chosen': '-161.08', 'loss/train': '0.62642', 'examples_per_second': '31.566', 'grad_norm': '19.375', 'counters/examples': 222400, 'counters/updates': 6950}
train stats after 222432 examples: {'rewards_train/chosen': '0.063072', 'rewards_train/rejected': '-0.012001', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075072', 'logps_train/rejected': '-111.91', 'logps_train/chosen': '-116.11', 'loss/train': '0.66534', 'examples_per_second': '30.594', 'grad_norm': '19.125', 'counters/examples': 222432, 'counters/updates': 6951}
train stats after 222464 examples: {'rewards_train/chosen': '0.23023', 'rewards_train/rejected': '0.13631', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.093924', 'logps_train/rejected': '-135.29', 'logps_train/chosen': '-156.44', 'loss/train': '0.6619', 'examples_per_second': '31.256', 'grad_norm': '22.375', 'counters/examples': 222464, 'counters/updates': 6952}
train stats after 222496 examples: {'rewards_train/chosen': '0.14641', 'rewards_train/rejected': '0.014102', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13231', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-121.01', 'loss/train': '0.63852', 'examples_per_second': '31.582', 'grad_norm': '20', 'counters/examples': 222496, 'counters/updates': 6953}
train stats after 222528 examples: {'rewards_train/chosen': '0.096236', 'rewards_train/rejected': '-0.0074403', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10368', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-158.09', 'loss/train': '0.66192', 'examples_per_second': '32.65', 'grad_norm': '29', 'counters/examples': 222528, 'counters/updates': 6954}
train stats after 222560 examples: {'rewards_train/chosen': '0.17522', 'rewards_train/rejected': '0.077627', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097597', 'logps_train/rejected': '-103.54', 'logps_train/chosen': '-131.87', 'loss/train': '0.65859', 'examples_per_second': '32.641', 'grad_norm': '19.5', 'counters/examples': 222560, 'counters/updates': 6955}
train stats after 222592 examples: {'rewards_train/chosen': '0.19797', 'rewards_train/rejected': '0.0085963', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18938', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-142.24', 'loss/train': '0.61223', 'examples_per_second': '30.594', 'grad_norm': '18.875', 'counters/examples': 222592, 'counters/updates': 6956}
train stats after 222624 examples: {'rewards_train/chosen': '0.12792', 'rewards_train/rejected': '0.11191', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016011', 'logps_train/rejected': '-166.99', 'logps_train/chosen': '-147.66', 'loss/train': '0.70455', 'examples_per_second': '31.037', 'grad_norm': '25.125', 'counters/examples': 222624, 'counters/updates': 6957}
skipping logging after 222656 examples to avoid logging too frequently
train stats after 222688 examples: {'rewards_train/chosen': '0.21042', 'rewards_train/rejected': '0.075203', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13521', 'logps_train/rejected': '-99.857', 'logps_train/chosen': '-152.82', 'loss/train': '0.64719', 'examples_per_second': '30.926', 'grad_norm': '19.5', 'counters/examples': 222688, 'counters/updates': 6959}
skipping logging after 222720 examples to avoid logging too frequently
train stats after 222752 examples: {'rewards_train/chosen': '0.11074', 'rewards_train/rejected': '0.003158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10758', 'logps_train/rejected': '-115.51', 'logps_train/chosen': '-129.67', 'loss/train': '0.6611', 'examples_per_second': '31.237', 'grad_norm': '19.25', 'counters/examples': 222752, 'counters/updates': 6961}
train stats after 222784 examples: {'rewards_train/chosen': '0.14744', 'rewards_train/rejected': '-0.025999', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17344', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-108.36', 'loss/train': '0.62125', 'examples_per_second': '32.045', 'grad_norm': '18', 'counters/examples': 222784, 'counters/updates': 6962}
train stats after 222816 examples: {'rewards_train/chosen': '0.27087', 'rewards_train/rejected': '0.039163', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2317', 'logps_train/rejected': '-106.94', 'logps_train/chosen': '-147.88', 'loss/train': '0.59713', 'examples_per_second': '31.956', 'grad_norm': '20.75', 'counters/examples': 222816, 'counters/updates': 6963}
train stats after 222848 examples: {'rewards_train/chosen': '0.077287', 'rewards_train/rejected': '0.069055', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0082311', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-140.24', 'loss/train': '0.69802', 'examples_per_second': '31.575', 'grad_norm': '22', 'counters/examples': 222848, 'counters/updates': 6964}
train stats after 222880 examples: {'rewards_train/chosen': '0.25369', 'rewards_train/rejected': '0.076532', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17716', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-182.22', 'loss/train': '0.62401', 'examples_per_second': '32.25', 'grad_norm': '21.75', 'counters/examples': 222880, 'counters/updates': 6965}
train stats after 222912 examples: {'rewards_train/chosen': '0.17316', 'rewards_train/rejected': '0.054258', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1189', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-159.94', 'loss/train': '0.64976', 'examples_per_second': '32.447', 'grad_norm': '21.625', 'counters/examples': 222912, 'counters/updates': 6966}
train stats after 222944 examples: {'rewards_train/chosen': '0.17136', 'rewards_train/rejected': '0.075671', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095686', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-143.96', 'loss/train': '0.66337', 'examples_per_second': '31.081', 'grad_norm': '21.375', 'counters/examples': 222944, 'counters/updates': 6967}
train stats after 222976 examples: {'rewards_train/chosen': '0.20329', 'rewards_train/rejected': '0.14053', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062754', 'logps_train/rejected': '-137.73', 'logps_train/chosen': '-133.74', 'loss/train': '0.68886', 'examples_per_second': '30.61', 'grad_norm': '22.5', 'counters/examples': 222976, 'counters/updates': 6968}
train stats after 223008 examples: {'rewards_train/chosen': '0.085259', 'rewards_train/rejected': '0.030467', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054792', 'logps_train/rejected': '-149.65', 'logps_train/chosen': '-120.04', 'loss/train': '0.68152', 'examples_per_second': '33.174', 'grad_norm': '21', 'counters/examples': 223008, 'counters/updates': 6969}
train stats after 223040 examples: {'rewards_train/chosen': '0.1249', 'rewards_train/rejected': '0.10502', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019885', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-124.39', 'loss/train': '0.70829', 'examples_per_second': '31.519', 'grad_norm': '22', 'counters/examples': 223040, 'counters/updates': 6970}
train stats after 223072 examples: {'rewards_train/chosen': '0.10964', 'rewards_train/rejected': '0.077726', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031915', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-142.33', 'loss/train': '0.6883', 'examples_per_second': '32.622', 'grad_norm': '21.25', 'counters/examples': 223072, 'counters/updates': 6971}
skipping logging after 223104 examples to avoid logging too frequently
train stats after 223136 examples: {'rewards_train/chosen': '0.15628', 'rewards_train/rejected': '0.021655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13463', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-138.01', 'loss/train': '0.64937', 'examples_per_second': '31.606', 'grad_norm': '20.375', 'counters/examples': 223136, 'counters/updates': 6973}
train stats after 223168 examples: {'rewards_train/chosen': '0.23787', 'rewards_train/rejected': '0.048733', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18914', 'logps_train/rejected': '-99.605', 'logps_train/chosen': '-141.69', 'loss/train': '0.61367', 'examples_per_second': '31.655', 'grad_norm': '18.5', 'counters/examples': 223168, 'counters/updates': 6974}
train stats after 223200 examples: {'rewards_train/chosen': '0.24806', 'rewards_train/rejected': '0.084393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16367', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-136.03', 'loss/train': '0.63022', 'examples_per_second': '22.025', 'grad_norm': '19', 'counters/examples': 223200, 'counters/updates': 6975}
train stats after 223232 examples: {'rewards_train/chosen': '0.20197', 'rewards_train/rejected': '-0.0413', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24327', 'logps_train/rejected': '-81.494', 'logps_train/chosen': '-114.78', 'loss/train': '0.60391', 'examples_per_second': '31.299', 'grad_norm': '16', 'counters/examples': 223232, 'counters/updates': 6976}
train stats after 223264 examples: {'rewards_train/chosen': '0.19877', 'rewards_train/rejected': '0.0075311', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19124', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-112.62', 'loss/train': '0.61169', 'examples_per_second': '32.09', 'grad_norm': '18.25', 'counters/examples': 223264, 'counters/updates': 6977}
skipping logging after 223296 examples to avoid logging too frequently
train stats after 223328 examples: {'rewards_train/chosen': '0.20105', 'rewards_train/rejected': '0.072061', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12899', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-145.01', 'loss/train': '0.64918', 'examples_per_second': '30.222', 'grad_norm': '20.625', 'counters/examples': 223328, 'counters/updates': 6979}
train stats after 223360 examples: {'rewards_train/chosen': '0.19175', 'rewards_train/rejected': '-0.02668', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21843', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-150.47', 'loss/train': '0.60539', 'examples_per_second': '32.195', 'grad_norm': '20', 'counters/examples': 223360, 'counters/updates': 6980}
skipping logging after 223392 examples to avoid logging too frequently
train stats after 223424 examples: {'rewards_train/chosen': '0.19987', 'rewards_train/rejected': '-0.060937', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26081', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-146.81', 'loss/train': '0.59121', 'examples_per_second': '30.064', 'grad_norm': '20.25', 'counters/examples': 223424, 'counters/updates': 6982}
train stats after 223456 examples: {'rewards_train/chosen': '0.24405', 'rewards_train/rejected': '0.12456', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11948', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-132.32', 'loss/train': '0.65466', 'examples_per_second': '30.612', 'grad_norm': '20.5', 'counters/examples': 223456, 'counters/updates': 6983}
train stats after 223488 examples: {'rewards_train/chosen': '0.17523', 'rewards_train/rejected': '-0.059862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23509', 'logps_train/rejected': '-105.13', 'logps_train/chosen': '-126.54', 'loss/train': '0.62161', 'examples_per_second': '31.3', 'grad_norm': '17.875', 'counters/examples': 223488, 'counters/updates': 6984}
train stats after 223520 examples: {'rewards_train/chosen': '0.29546', 'rewards_train/rejected': '0.098063', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1974', 'logps_train/rejected': '-88.46', 'logps_train/chosen': '-139.77', 'loss/train': '0.6188', 'examples_per_second': '30.292', 'grad_norm': '17.5', 'counters/examples': 223520, 'counters/updates': 6985}
train stats after 223552 examples: {'rewards_train/chosen': '0.20299', 'rewards_train/rejected': '-0.0075221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21052', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-141.93', 'loss/train': '0.61414', 'examples_per_second': '30.827', 'grad_norm': '19.5', 'counters/examples': 223552, 'counters/updates': 6986}
train stats after 223584 examples: {'rewards_train/chosen': '0.22619', 'rewards_train/rejected': '0.05924', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16695', 'logps_train/rejected': '-132.89', 'logps_train/chosen': '-179.45', 'loss/train': '0.62759', 'examples_per_second': '31.456', 'grad_norm': '22.25', 'counters/examples': 223584, 'counters/updates': 6987}
train stats after 223616 examples: {'rewards_train/chosen': '0.36964', 'rewards_train/rejected': '0.18643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18322', 'logps_train/rejected': '-141.02', 'logps_train/chosen': '-167.41', 'loss/train': '0.62779', 'examples_per_second': '31.661', 'grad_norm': '20.875', 'counters/examples': 223616, 'counters/updates': 6988}
skipping logging after 223648 examples to avoid logging too frequently
train stats after 223680 examples: {'rewards_train/chosen': '0.25076', 'rewards_train/rejected': '-0.061162', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.31192', 'logps_train/rejected': '-119.3', 'logps_train/chosen': '-150.8', 'loss/train': '0.56997', 'examples_per_second': '32.836', 'grad_norm': '19.25', 'counters/examples': 223680, 'counters/updates': 6990}
train stats after 223712 examples: {'rewards_train/chosen': '0.1541', 'rewards_train/rejected': '0.087022', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067077', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-131.6', 'loss/train': '0.68027', 'examples_per_second': '31.748', 'grad_norm': '19.5', 'counters/examples': 223712, 'counters/updates': 6991}
skipping logging after 223744 examples to avoid logging too frequently
train stats after 223776 examples: {'rewards_train/chosen': '0.14248', 'rewards_train/rejected': '0.027086', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1154', 'logps_train/rejected': '-117.07', 'logps_train/chosen': '-142.5', 'loss/train': '0.6474', 'examples_per_second': '30.953', 'grad_norm': '19.375', 'counters/examples': 223776, 'counters/updates': 6993}
skipping logging after 223808 examples to avoid logging too frequently
train stats after 223840 examples: {'rewards_train/chosen': '0.22426', 'rewards_train/rejected': '0.12305', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10121', 'logps_train/rejected': '-103.71', 'logps_train/chosen': '-132.17', 'loss/train': '0.66696', 'examples_per_second': '30.716', 'grad_norm': '18.875', 'counters/examples': 223840, 'counters/updates': 6995}
skipping logging after 223872 examples to avoid logging too frequently
train stats after 223904 examples: {'rewards_train/chosen': '0.10763', 'rewards_train/rejected': '0.053239', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.054396', 'logps_train/rejected': '-99.438', 'logps_train/chosen': '-114.53', 'loss/train': '0.67269', 'examples_per_second': '34.007', 'grad_norm': '19.375', 'counters/examples': 223904, 'counters/updates': 6997}
train stats after 223936 examples: {'rewards_train/chosen': '0.12743', 'rewards_train/rejected': '-0.025292', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15272', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-120.95', 'loss/train': '0.62881', 'examples_per_second': '32.506', 'grad_norm': '18.125', 'counters/examples': 223936, 'counters/updates': 6998}
train stats after 223968 examples: {'rewards_train/chosen': '0.15494', 'rewards_train/rejected': '0.085788', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069153', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-161.46', 'loss/train': '0.68381', 'examples_per_second': '31.433', 'grad_norm': '23.75', 'counters/examples': 223968, 'counters/updates': 6999}
train stats after 224000 examples: {'rewards_train/chosen': '0.23419', 'rewards_train/rejected': '-0.10597', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.34016', 'logps_train/rejected': '-87.215', 'logps_train/chosen': '-130.92', 'loss/train': '0.55934', 'examples_per_second': '31.569', 'grad_norm': '15.375', 'counters/examples': 224000, 'counters/updates': 7000}
skipping logging after 224032 examples to avoid logging too frequently
train stats after 224064 examples: {'rewards_train/chosen': '0.1744', 'rewards_train/rejected': '-0.022521', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19692', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-174.36', 'loss/train': '0.61819', 'examples_per_second': '30.351', 'grad_norm': '21', 'counters/examples': 224064, 'counters/updates': 7002}
train stats after 224096 examples: {'rewards_train/chosen': '0.13047', 'rewards_train/rejected': '-0.0026035', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13307', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-141.2', 'loss/train': '0.64298', 'examples_per_second': '31.85', 'grad_norm': '20.5', 'counters/examples': 224096, 'counters/updates': 7003}
train stats after 224128 examples: {'rewards_train/chosen': '0.20028', 'rewards_train/rejected': '0.12959', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070693', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-140.69', 'loss/train': '0.67314', 'examples_per_second': '30.481', 'grad_norm': '19.875', 'counters/examples': 224128, 'counters/updates': 7004}
train stats after 224160 examples: {'rewards_train/chosen': '0.1897', 'rewards_train/rejected': '0.019794', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16991', 'logps_train/rejected': '-102.84', 'logps_train/chosen': '-138.87', 'loss/train': '0.62574', 'examples_per_second': '32.082', 'grad_norm': '18', 'counters/examples': 224160, 'counters/updates': 7005}
train stats after 224192 examples: {'rewards_train/chosen': '0.17661', 'rewards_train/rejected': '0.081751', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094857', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-123.51', 'loss/train': '0.65751', 'examples_per_second': '32.623', 'grad_norm': '19.75', 'counters/examples': 224192, 'counters/updates': 7006}
train stats after 224224 examples: {'rewards_train/chosen': '0.17788', 'rewards_train/rejected': '-0.014752', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19263', 'logps_train/rejected': '-108.84', 'logps_train/chosen': '-140.68', 'loss/train': '0.61735', 'examples_per_second': '24.503', 'grad_norm': '18.5', 'counters/examples': 224224, 'counters/updates': 7007}
train stats after 224256 examples: {'rewards_train/chosen': '0.21169', 'rewards_train/rejected': '0.059137', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15255', 'logps_train/rejected': '-134.5', 'logps_train/chosen': '-148.29', 'loss/train': '0.63638', 'examples_per_second': '31.752', 'grad_norm': '20', 'counters/examples': 224256, 'counters/updates': 7008}
train stats after 224288 examples: {'rewards_train/chosen': '0.17752', 'rewards_train/rejected': '-0.0019357', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17945', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-141.48', 'loss/train': '0.6288', 'examples_per_second': '31.558', 'grad_norm': '20.25', 'counters/examples': 224288, 'counters/updates': 7009}
train stats after 224320 examples: {'rewards_train/chosen': '0.15624', 'rewards_train/rejected': '0.12291', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033325', 'logps_train/rejected': '-123.86', 'logps_train/chosen': '-133.52', 'loss/train': '0.69994', 'examples_per_second': '24.487', 'grad_norm': '21.375', 'counters/examples': 224320, 'counters/updates': 7010}
train stats after 224352 examples: {'rewards_train/chosen': '0.18232', 'rewards_train/rejected': '0.099489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082833', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-138.83', 'loss/train': '0.67734', 'examples_per_second': '31.662', 'grad_norm': '21', 'counters/examples': 224352, 'counters/updates': 7011}
train stats after 224384 examples: {'rewards_train/chosen': '0.15811', 'rewards_train/rejected': '-0.017504', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17562', 'logps_train/rejected': '-92.807', 'logps_train/chosen': '-119.64', 'loss/train': '0.62042', 'examples_per_second': '31.996', 'grad_norm': '17.375', 'counters/examples': 224384, 'counters/updates': 7012}
train stats after 224416 examples: {'rewards_train/chosen': '0.29595', 'rewards_train/rejected': '0.09271', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20324', 'logps_train/rejected': '-142.02', 'logps_train/chosen': '-133.59', 'loss/train': '0.6212', 'examples_per_second': '30.808', 'grad_norm': '19.875', 'counters/examples': 224416, 'counters/updates': 7013}
train stats after 224448 examples: {'rewards_train/chosen': '0.29724', 'rewards_train/rejected': '0.073039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2242', 'logps_train/rejected': '-126.95', 'logps_train/chosen': '-130.68', 'loss/train': '0.60647', 'examples_per_second': '32.201', 'grad_norm': '19.375', 'counters/examples': 224448, 'counters/updates': 7014}
train stats after 224480 examples: {'rewards_train/chosen': '0.11432', 'rewards_train/rejected': '-0.015386', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1297', 'logps_train/rejected': '-141.24', 'logps_train/chosen': '-136.76', 'loss/train': '0.64403', 'examples_per_second': '32.417', 'grad_norm': '21', 'counters/examples': 224480, 'counters/updates': 7015}
train stats after 224512 examples: {'rewards_train/chosen': '0.12323', 'rewards_train/rejected': '0.0079039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11533', 'logps_train/rejected': '-146.7', 'logps_train/chosen': '-133.6', 'loss/train': '0.65061', 'examples_per_second': '31.484', 'grad_norm': '21', 'counters/examples': 224512, 'counters/updates': 7016}
train stats after 224544 examples: {'rewards_train/chosen': '0.10823', 'rewards_train/rejected': '-0.027872', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13611', 'logps_train/rejected': '-104.06', 'logps_train/chosen': '-146.03', 'loss/train': '0.65158', 'examples_per_second': '30.989', 'grad_norm': '20.5', 'counters/examples': 224544, 'counters/updates': 7017}
train stats after 224576 examples: {'rewards_train/chosen': '0.16763', 'rewards_train/rejected': '-0.020403', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18804', 'logps_train/rejected': '-94.271', 'logps_train/chosen': '-173.34', 'loss/train': '0.61645', 'examples_per_second': '31.683', 'grad_norm': '19.75', 'counters/examples': 224576, 'counters/updates': 7018}
skipping logging after 224608 examples to avoid logging too frequently
train stats after 224640 examples: {'rewards_train/chosen': '0.14878', 'rewards_train/rejected': '-0.03499', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18377', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-151.44', 'loss/train': '0.61918', 'examples_per_second': '31.643', 'grad_norm': '20.625', 'counters/examples': 224640, 'counters/updates': 7020}
skipping logging after 224672 examples to avoid logging too frequently
train stats after 224704 examples: {'rewards_train/chosen': '0.16597', 'rewards_train/rejected': '0.047047', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11892', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-135.44', 'loss/train': '0.65134', 'examples_per_second': '31.573', 'grad_norm': '20.375', 'counters/examples': 224704, 'counters/updates': 7022}
train stats after 224736 examples: {'rewards_train/chosen': '0.2722', 'rewards_train/rejected': '0.054904', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2173', 'logps_train/rejected': '-147.84', 'logps_train/chosen': '-134.84', 'loss/train': '0.61211', 'examples_per_second': '31.552', 'grad_norm': '19.75', 'counters/examples': 224736, 'counters/updates': 7023}
skipping logging after 224768 examples to avoid logging too frequently
train stats after 224800 examples: {'rewards_train/chosen': '0.14195', 'rewards_train/rejected': '0.051729', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090225', 'logps_train/rejected': '-93.268', 'logps_train/chosen': '-98.938', 'loss/train': '0.66276', 'examples_per_second': '32.707', 'grad_norm': '17.875', 'counters/examples': 224800, 'counters/updates': 7025}
train stats after 224832 examples: {'rewards_train/chosen': '0.21491', 'rewards_train/rejected': '0.014849', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20006', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-147.34', 'loss/train': '0.61565', 'examples_per_second': '31.313', 'grad_norm': '20.875', 'counters/examples': 224832, 'counters/updates': 7026}
skipping logging after 224864 examples to avoid logging too frequently
train stats after 224896 examples: {'rewards_train/chosen': '0.16971', 'rewards_train/rejected': '0.14737', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022336', 'logps_train/rejected': '-108.89', 'logps_train/chosen': '-120.93', 'loss/train': '0.69674', 'examples_per_second': '36.401', 'grad_norm': '19', 'counters/examples': 224896, 'counters/updates': 7028}
train stats after 224928 examples: {'rewards_train/chosen': '0.21891', 'rewards_train/rejected': '0.019104', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19981', 'logps_train/rejected': '-95.243', 'logps_train/chosen': '-110.46', 'loss/train': '0.61601', 'examples_per_second': '31.348', 'grad_norm': '17.875', 'counters/examples': 224928, 'counters/updates': 7029}
skipping logging after 224960 examples to avoid logging too frequently
train stats after 224992 examples: {'rewards_train/chosen': '0.13872', 'rewards_train/rejected': '-0.05304', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19176', 'logps_train/rejected': '-87.564', 'logps_train/chosen': '-116.5', 'loss/train': '0.61727', 'examples_per_second': '34.125', 'grad_norm': '17.5', 'counters/examples': 224992, 'counters/updates': 7031}
skipping logging after 225024 examples to avoid logging too frequently
train stats after 225056 examples: {'rewards_train/chosen': '0.17666', 'rewards_train/rejected': '0.081137', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095525', 'logps_train/rejected': '-88.35', 'logps_train/chosen': '-132.98', 'loss/train': '0.65977', 'examples_per_second': '32.761', 'grad_norm': '18.5', 'counters/examples': 225056, 'counters/updates': 7033}
train stats after 225088 examples: {'rewards_train/chosen': '0.20564', 'rewards_train/rejected': '-0.09938', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.30502', 'logps_train/rejected': '-77.497', 'logps_train/chosen': '-152.14', 'loss/train': '0.5672', 'examples_per_second': '31.412', 'grad_norm': '16.75', 'counters/examples': 225088, 'counters/updates': 7034}
train stats after 225120 examples: {'rewards_train/chosen': '0.19635', 'rewards_train/rejected': '0.067512', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12884', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-115.83', 'loss/train': '0.64466', 'examples_per_second': '31.04', 'grad_norm': '19.25', 'counters/examples': 225120, 'counters/updates': 7035}
skipping logging after 225152 examples to avoid logging too frequently
train stats after 225184 examples: {'rewards_train/chosen': '0.18148', 'rewards_train/rejected': '-0.037922', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21941', 'logps_train/rejected': '-100.48', 'logps_train/chosen': '-113.35', 'loss/train': '0.60985', 'examples_per_second': '30.325', 'grad_norm': '18.5', 'counters/examples': 225184, 'counters/updates': 7037}
train stats after 225216 examples: {'rewards_train/chosen': '0.22897', 'rewards_train/rejected': '0.01361', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21536', 'logps_train/rejected': '-117.3', 'logps_train/chosen': '-146.75', 'loss/train': '0.60309', 'examples_per_second': '32.922', 'grad_norm': '19.25', 'counters/examples': 225216, 'counters/updates': 7038}
train stats after 225248 examples: {'rewards_train/chosen': '0.11979', 'rewards_train/rejected': '0.11123', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0085634', 'logps_train/rejected': '-125.99', 'logps_train/chosen': '-137.61', 'loss/train': '0.71629', 'examples_per_second': '31.437', 'grad_norm': '20.25', 'counters/examples': 225248, 'counters/updates': 7039}
train stats after 225280 examples: {'rewards_train/chosen': '0.15821', 'rewards_train/rejected': '0.095834', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062379', 'logps_train/rejected': '-103.19', 'logps_train/chosen': '-111.91', 'loss/train': '0.67554', 'examples_per_second': '31.694', 'grad_norm': '18.75', 'counters/examples': 225280, 'counters/updates': 7040}
skipping logging after 225312 examples to avoid logging too frequently
train stats after 225344 examples: {'rewards_train/chosen': '0.097277', 'rewards_train/rejected': '0.030102', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067175', 'logps_train/rejected': '-96.864', 'logps_train/chosen': '-89.943', 'loss/train': '0.66825', 'examples_per_second': '30.531', 'grad_norm': '17.75', 'counters/examples': 225344, 'counters/updates': 7042}
train stats after 225376 examples: {'rewards_train/chosen': '0.21107', 'rewards_train/rejected': '-0.047058', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25813', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-135.43', 'loss/train': '0.58763', 'examples_per_second': '31.21', 'grad_norm': '18.875', 'counters/examples': 225376, 'counters/updates': 7043}
train stats after 225408 examples: {'rewards_train/chosen': '0.23914', 'rewards_train/rejected': '0.12393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11522', 'logps_train/rejected': '-147.73', 'logps_train/chosen': '-140.8', 'loss/train': '0.65383', 'examples_per_second': '30.157', 'grad_norm': '21.25', 'counters/examples': 225408, 'counters/updates': 7044}
train stats after 225440 examples: {'rewards_train/chosen': '0.17501', 'rewards_train/rejected': '-0.026292', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2013', 'logps_train/rejected': '-125.88', 'logps_train/chosen': '-144.54', 'loss/train': '0.62288', 'examples_per_second': '30.205', 'grad_norm': '21.625', 'counters/examples': 225440, 'counters/updates': 7045}
train stats after 225472 examples: {'rewards_train/chosen': '0.13745', 'rewards_train/rejected': '0.038096', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099353', 'logps_train/rejected': '-90.296', 'logps_train/chosen': '-106.43', 'loss/train': '0.66654', 'examples_per_second': '31.803', 'grad_norm': '17', 'counters/examples': 225472, 'counters/updates': 7046}
train stats after 225504 examples: {'rewards_train/chosen': '0.25484', 'rewards_train/rejected': '0.104', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15084', 'logps_train/rejected': '-162.64', 'logps_train/chosen': '-159.8', 'loss/train': '0.63535', 'examples_per_second': '30.946', 'grad_norm': '22', 'counters/examples': 225504, 'counters/updates': 7047}
skipping logging after 225536 examples to avoid logging too frequently
train stats after 225568 examples: {'rewards_train/chosen': '0.052595', 'rewards_train/rejected': '-0.02169', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074285', 'logps_train/rejected': '-102.43', 'logps_train/chosen': '-112.72', 'loss/train': '0.67645', 'examples_per_second': '31.014', 'grad_norm': '18.75', 'counters/examples': 225568, 'counters/updates': 7049}
train stats after 225600 examples: {'rewards_train/chosen': '0.13048', 'rewards_train/rejected': '0.10874', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021745', 'logps_train/rejected': '-141.74', 'logps_train/chosen': '-140.42', 'loss/train': '0.70432', 'examples_per_second': '31.639', 'grad_norm': '24', 'counters/examples': 225600, 'counters/updates': 7050}
train stats after 225632 examples: {'rewards_train/chosen': '0.1666', 'rewards_train/rejected': '0.044153', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12244', 'logps_train/rejected': '-97.428', 'logps_train/chosen': '-115.86', 'loss/train': '0.64642', 'examples_per_second': '31.615', 'grad_norm': '18.25', 'counters/examples': 225632, 'counters/updates': 7051}
train stats after 225664 examples: {'rewards_train/chosen': '0.16692', 'rewards_train/rejected': '-0.083516', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25043', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-125.57', 'loss/train': '0.59527', 'examples_per_second': '31.333', 'grad_norm': '18', 'counters/examples': 225664, 'counters/updates': 7052}
train stats after 225696 examples: {'rewards_train/chosen': '0.26563', 'rewards_train/rejected': '0.1098', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15583', 'logps_train/rejected': '-149.41', 'logps_train/chosen': '-113.2', 'loss/train': '0.63333', 'examples_per_second': '31.347', 'grad_norm': '20.375', 'counters/examples': 225696, 'counters/updates': 7053}
train stats after 225728 examples: {'rewards_train/chosen': '0.2072', 'rewards_train/rejected': '0.05069', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15651', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-168.44', 'loss/train': '0.63405', 'examples_per_second': '31.656', 'grad_norm': '20.75', 'counters/examples': 225728, 'counters/updates': 7054}
train stats after 225760 examples: {'rewards_train/chosen': '0.13594', 'rewards_train/rejected': '0.026859', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10908', 'logps_train/rejected': '-100.9', 'logps_train/chosen': '-91.575', 'loss/train': '0.65166', 'examples_per_second': '32.065', 'grad_norm': '19', 'counters/examples': 225760, 'counters/updates': 7055}
train stats after 225792 examples: {'rewards_train/chosen': '0.16229', 'rewards_train/rejected': '-0.009205', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1715', 'logps_train/rejected': '-90.647', 'logps_train/chosen': '-134.77', 'loss/train': '0.62505', 'examples_per_second': '30.544', 'grad_norm': '18.875', 'counters/examples': 225792, 'counters/updates': 7056}
train stats after 225824 examples: {'rewards_train/chosen': '0.15905', 'rewards_train/rejected': '0.077074', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081975', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-145.99', 'loss/train': '0.67066', 'examples_per_second': '32.501', 'grad_norm': '21.875', 'counters/examples': 225824, 'counters/updates': 7057}
skipping logging after 225856 examples to avoid logging too frequently
train stats after 225888 examples: {'rewards_train/chosen': '0.19625', 'rewards_train/rejected': '0.03614', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16011', 'logps_train/rejected': '-113.03', 'logps_train/chosen': '-144.11', 'loss/train': '0.63239', 'examples_per_second': '30.435', 'grad_norm': '20.625', 'counters/examples': 225888, 'counters/updates': 7059}
train stats after 225920 examples: {'rewards_train/chosen': '0.1345', 'rewards_train/rejected': '0.0084464', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12605', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-137.93', 'loss/train': '0.64437', 'examples_per_second': '30.172', 'grad_norm': '20', 'counters/examples': 225920, 'counters/updates': 7060}
train stats after 225952 examples: {'rewards_train/chosen': '0.077545', 'rewards_train/rejected': '0.074324', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0032211', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-96.014', 'loss/train': '0.70342', 'examples_per_second': '32.472', 'grad_norm': '20.125', 'counters/examples': 225952, 'counters/updates': 7061}
train stats after 225984 examples: {'rewards_train/chosen': '0.13379', 'rewards_train/rejected': '-0.067661', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20145', 'logps_train/rejected': '-112.37', 'logps_train/chosen': '-140.47', 'loss/train': '0.60683', 'examples_per_second': '31.667', 'grad_norm': '18.75', 'counters/examples': 225984, 'counters/updates': 7062}
train stats after 226016 examples: {'rewards_train/chosen': '0.23893', 'rewards_train/rejected': '0.07573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1632', 'logps_train/rejected': '-149.16', 'logps_train/chosen': '-155.8', 'loss/train': '0.62876', 'examples_per_second': '32.004', 'grad_norm': '22', 'counters/examples': 226016, 'counters/updates': 7063}
skipping logging after 226048 examples to avoid logging too frequently
train stats after 226080 examples: {'rewards_train/chosen': '0.12972', 'rewards_train/rejected': '0.0019374', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12778', 'logps_train/rejected': '-93.529', 'logps_train/chosen': '-132.68', 'loss/train': '0.64669', 'examples_per_second': '31.147', 'grad_norm': '18.625', 'counters/examples': 226080, 'counters/updates': 7065}
skipping logging after 226112 examples to avoid logging too frequently
train stats after 226144 examples: {'rewards_train/chosen': '0.10672', 'rewards_train/rejected': '0.0012687', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10545', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-133.35', 'loss/train': '0.65613', 'examples_per_second': '33.88', 'grad_norm': '20.625', 'counters/examples': 226144, 'counters/updates': 7067}
train stats after 226176 examples: {'rewards_train/chosen': '0.15317', 'rewards_train/rejected': '0.087137', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066036', 'logps_train/rejected': '-150.32', 'logps_train/chosen': '-112.83', 'loss/train': '0.67708', 'examples_per_second': '31.454', 'grad_norm': '20.875', 'counters/examples': 226176, 'counters/updates': 7068}
train stats after 226208 examples: {'rewards_train/chosen': '0.20496', 'rewards_train/rejected': '0.051144', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15382', 'logps_train/rejected': '-116.61', 'logps_train/chosen': '-103.69', 'loss/train': '0.6317', 'examples_per_second': '32.444', 'grad_norm': '18.375', 'counters/examples': 226208, 'counters/updates': 7069}
train stats after 226240 examples: {'rewards_train/chosen': '0.25314', 'rewards_train/rejected': '0.12093', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13221', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-171.28', 'loss/train': '0.64516', 'examples_per_second': '31.374', 'grad_norm': '21', 'counters/examples': 226240, 'counters/updates': 7070}
train stats after 226272 examples: {'rewards_train/chosen': '0.23998', 'rewards_train/rejected': '0.10386', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13613', 'logps_train/rejected': '-106.51', 'logps_train/chosen': '-105.03', 'loss/train': '0.64361', 'examples_per_second': '30.194', 'grad_norm': '17.25', 'counters/examples': 226272, 'counters/updates': 7071}
train stats after 226304 examples: {'rewards_train/chosen': '0.14714', 'rewards_train/rejected': '0.063344', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083793', 'logps_train/rejected': '-140.39', 'logps_train/chosen': '-114.5', 'loss/train': '0.66142', 'examples_per_second': '32.778', 'grad_norm': '20.375', 'counters/examples': 226304, 'counters/updates': 7072}
skipping logging after 226336 examples to avoid logging too frequently
train stats after 226368 examples: {'rewards_train/chosen': '0.24839', 'rewards_train/rejected': '0.055068', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19333', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-149.13', 'loss/train': '0.62543', 'examples_per_second': '31.565', 'grad_norm': '20.625', 'counters/examples': 226368, 'counters/updates': 7074}
train stats after 226400 examples: {'rewards_train/chosen': '0.10355', 'rewards_train/rejected': '0.010933', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09262', 'logps_train/rejected': '-114.61', 'logps_train/chosen': '-121.02', 'loss/train': '0.66947', 'examples_per_second': '30.082', 'grad_norm': '20.375', 'counters/examples': 226400, 'counters/updates': 7075}
train stats after 226432 examples: {'rewards_train/chosen': '0.1437', 'rewards_train/rejected': '0.10756', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036145', 'logps_train/rejected': '-147.38', 'logps_train/chosen': '-145.36', 'loss/train': '0.69242', 'examples_per_second': '31.409', 'grad_norm': '23', 'counters/examples': 226432, 'counters/updates': 7076}
train stats after 226464 examples: {'rewards_train/chosen': '0.17135', 'rewards_train/rejected': '0.0031393', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16821', 'logps_train/rejected': '-90.176', 'logps_train/chosen': '-111.39', 'loss/train': '0.62613', 'examples_per_second': '31.989', 'grad_norm': '17.25', 'counters/examples': 226464, 'counters/updates': 7077}
train stats after 226496 examples: {'rewards_train/chosen': '0.21508', 'rewards_train/rejected': '-0.05676', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27184', 'logps_train/rejected': '-121.73', 'logps_train/chosen': '-138.25', 'loss/train': '0.59025', 'examples_per_second': '30.907', 'grad_norm': '19.5', 'counters/examples': 226496, 'counters/updates': 7078}
train stats after 226528 examples: {'rewards_train/chosen': '0.16833', 'rewards_train/rejected': '0.12788', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040448', 'logps_train/rejected': '-111.13', 'logps_train/chosen': '-128.21', 'loss/train': '0.69091', 'examples_per_second': '30.647', 'grad_norm': '21.875', 'counters/examples': 226528, 'counters/updates': 7079}
train stats after 226560 examples: {'rewards_train/chosen': '0.10779', 'rewards_train/rejected': '0.11677', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0089774', 'logps_train/rejected': '-122.74', 'logps_train/chosen': '-134.7', 'loss/train': '0.70653', 'examples_per_second': '30.832', 'grad_norm': '20.375', 'counters/examples': 226560, 'counters/updates': 7080}
train stats after 226592 examples: {'rewards_train/chosen': '0.1405', 'rewards_train/rejected': '0.064319', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076179', 'logps_train/rejected': '-156.39', 'logps_train/chosen': '-164.17', 'loss/train': '0.67824', 'examples_per_second': '31.649', 'grad_norm': '25.5', 'counters/examples': 226592, 'counters/updates': 7081}
skipping logging after 226624 examples to avoid logging too frequently
train stats after 226656 examples: {'rewards_train/chosen': '0.13765', 'rewards_train/rejected': '0.023585', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11406', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-145.58', 'loss/train': '0.65615', 'examples_per_second': '31.641', 'grad_norm': '21', 'counters/examples': 226656, 'counters/updates': 7083}
train stats after 226688 examples: {'rewards_train/chosen': '0.13662', 'rewards_train/rejected': '0.11215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024469', 'logps_train/rejected': '-124.5', 'logps_train/chosen': '-138.92', 'loss/train': '0.695', 'examples_per_second': '32.276', 'grad_norm': '23.125', 'counters/examples': 226688, 'counters/updates': 7084}
skipping logging after 226720 examples to avoid logging too frequently
train stats after 226752 examples: {'rewards_train/chosen': '0.15711', 'rewards_train/rejected': '0.13255', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024561', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-128.52', 'loss/train': '0.69886', 'examples_per_second': '30.975', 'grad_norm': '21.875', 'counters/examples': 226752, 'counters/updates': 7086}
train stats after 226784 examples: {'rewards_train/chosen': '0.19492', 'rewards_train/rejected': '-0.03052', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22544', 'logps_train/rejected': '-111.2', 'logps_train/chosen': '-169.68', 'loss/train': '0.60948', 'examples_per_second': '32.668', 'grad_norm': '19.5', 'counters/examples': 226784, 'counters/updates': 7087}
train stats after 226816 examples: {'rewards_train/chosen': '0.20562', 'rewards_train/rejected': '0.061592', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14402', 'logps_train/rejected': '-101.99', 'logps_train/chosen': '-135.62', 'loss/train': '0.64699', 'examples_per_second': '30.109', 'grad_norm': '19.875', 'counters/examples': 226816, 'counters/updates': 7088}
train stats after 226848 examples: {'rewards_train/chosen': '0.22921', 'rewards_train/rejected': '0.033113', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1961', 'logps_train/rejected': '-119.05', 'logps_train/chosen': '-139.09', 'loss/train': '0.61604', 'examples_per_second': '32.403', 'grad_norm': '19.875', 'counters/examples': 226848, 'counters/updates': 7089}
train stats after 226880 examples: {'rewards_train/chosen': '0.20711', 'rewards_train/rejected': '0.040978', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16614', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-173.11', 'loss/train': '0.62539', 'examples_per_second': '30.168', 'grad_norm': '21.875', 'counters/examples': 226880, 'counters/updates': 7090}
train stats after 226912 examples: {'rewards_train/chosen': '0.16096', 'rewards_train/rejected': '0.031223', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12974', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-134.71', 'loss/train': '0.65034', 'examples_per_second': '32.951', 'grad_norm': '19.75', 'counters/examples': 226912, 'counters/updates': 7091}
train stats after 226944 examples: {'rewards_train/chosen': '0.21832', 'rewards_train/rejected': '0.029418', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1889', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-159.74', 'loss/train': '0.62466', 'examples_per_second': '30.454', 'grad_norm': '20.5', 'counters/examples': 226944, 'counters/updates': 7092}
train stats after 226976 examples: {'rewards_train/chosen': '0.11034', 'rewards_train/rejected': '0.00073021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10961', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-186.03', 'loss/train': '0.65271', 'examples_per_second': '31.6', 'grad_norm': '23.125', 'counters/examples': 226976, 'counters/updates': 7093}
skipping logging after 227008 examples to avoid logging too frequently
train stats after 227040 examples: {'rewards_train/chosen': '0.23603', 'rewards_train/rejected': '-0.0021871', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23822', 'logps_train/rejected': '-135.4', 'logps_train/chosen': '-175.77', 'loss/train': '0.59412', 'examples_per_second': '31.62', 'grad_norm': '21.375', 'counters/examples': 227040, 'counters/updates': 7095}
train stats after 227072 examples: {'rewards_train/chosen': '0.13287', 'rewards_train/rejected': '-0.011172', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14404', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-144.52', 'loss/train': '0.63782', 'examples_per_second': '31.474', 'grad_norm': '20.25', 'counters/examples': 227072, 'counters/updates': 7096}
train stats after 227104 examples: {'rewards_train/chosen': '0.21758', 'rewards_train/rejected': '0.067062', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15051', 'logps_train/rejected': '-116.07', 'logps_train/chosen': '-181.15', 'loss/train': '0.63495', 'examples_per_second': '30.009', 'grad_norm': '21', 'counters/examples': 227104, 'counters/updates': 7097}
train stats after 227136 examples: {'rewards_train/chosen': '0.16069', 'rewards_train/rejected': '-0.0052686', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16596', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-145.03', 'loss/train': '0.63329', 'examples_per_second': '31.613', 'grad_norm': '20.75', 'counters/examples': 227136, 'counters/updates': 7098}
train stats after 227168 examples: {'rewards_train/chosen': '0.16961', 'rewards_train/rejected': '0.14028', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029338', 'logps_train/rejected': '-116.08', 'logps_train/chosen': '-147.23', 'loss/train': '0.68809', 'examples_per_second': '32.212', 'grad_norm': '21', 'counters/examples': 227168, 'counters/updates': 7099}
train stats after 227200 examples: {'rewards_train/chosen': '0.22026', 'rewards_train/rejected': '0.11184', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10842', 'logps_train/rejected': '-154.65', 'logps_train/chosen': '-154.37', 'loss/train': '0.66178', 'examples_per_second': '31.566', 'grad_norm': '22.25', 'counters/examples': 227200, 'counters/updates': 7100}
train stats after 227232 examples: {'rewards_train/chosen': '0.20258', 'rewards_train/rejected': '-0.036799', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23938', 'logps_train/rejected': '-132.11', 'logps_train/chosen': '-152.66', 'loss/train': '0.59907', 'examples_per_second': '31.474', 'grad_norm': '19.125', 'counters/examples': 227232, 'counters/updates': 7101}
skipping logging after 227264 examples to avoid logging too frequently
train stats after 227296 examples: {'rewards_train/chosen': '0.20618', 'rewards_train/rejected': '0.0025458', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20364', 'logps_train/rejected': '-96.709', 'logps_train/chosen': '-161.33', 'loss/train': '0.61161', 'examples_per_second': '30.868', 'grad_norm': '19.25', 'counters/examples': 227296, 'counters/updates': 7103}
skipping logging after 227328 examples to avoid logging too frequently
train stats after 227360 examples: {'rewards_train/chosen': '0.059544', 'rewards_train/rejected': '0.050192', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0093521', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-92.047', 'loss/train': '0.71259', 'examples_per_second': '30.622', 'grad_norm': '21.125', 'counters/examples': 227360, 'counters/updates': 7105}
skipping logging after 227392 examples to avoid logging too frequently
train stats after 227424 examples: {'rewards_train/chosen': '0.3016', 'rewards_train/rejected': '0.11705', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18456', 'logps_train/rejected': '-135.89', 'logps_train/chosen': '-145.64', 'loss/train': '0.62065', 'examples_per_second': '34.27', 'grad_norm': '19.375', 'counters/examples': 227424, 'counters/updates': 7107}
train stats after 227456 examples: {'rewards_train/chosen': '0.31413', 'rewards_train/rejected': '0.04414', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26999', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-154.81', 'loss/train': '0.58627', 'examples_per_second': '31.628', 'grad_norm': '19.375', 'counters/examples': 227456, 'counters/updates': 7108}
train stats after 227488 examples: {'rewards_train/chosen': '0.20757', 'rewards_train/rejected': '-0.0092744', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21685', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-135.5', 'loss/train': '0.60607', 'examples_per_second': '31.658', 'grad_norm': '18.5', 'counters/examples': 227488, 'counters/updates': 7109}
train stats after 227520 examples: {'rewards_train/chosen': '0.23485', 'rewards_train/rejected': '-0.004387', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23924', 'logps_train/rejected': '-98.261', 'logps_train/chosen': '-113.38', 'loss/train': '0.60323', 'examples_per_second': '31.473', 'grad_norm': '17', 'counters/examples': 227520, 'counters/updates': 7110}
train stats after 227552 examples: {'rewards_train/chosen': '0.12529', 'rewards_train/rejected': '0.070981', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054308', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-143.5', 'loss/train': '0.67914', 'examples_per_second': '31.56', 'grad_norm': '22.25', 'counters/examples': 227552, 'counters/updates': 7111}
skipping logging after 227584 examples to avoid logging too frequently
train stats after 227616 examples: {'rewards_train/chosen': '0.25335', 'rewards_train/rejected': '0.10682', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14653', 'logps_train/rejected': '-124.75', 'logps_train/chosen': '-131.47', 'loss/train': '0.6465', 'examples_per_second': '36.15', 'grad_norm': '20.75', 'counters/examples': 227616, 'counters/updates': 7113}
train stats after 227648 examples: {'rewards_train/chosen': '0.11983', 'rewards_train/rejected': '-0.04573', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16556', 'logps_train/rejected': '-95.266', 'logps_train/chosen': '-119.67', 'loss/train': '0.63078', 'examples_per_second': '30.903', 'grad_norm': '17.625', 'counters/examples': 227648, 'counters/updates': 7114}
train stats after 227680 examples: {'rewards_train/chosen': '0.11491', 'rewards_train/rejected': '0.10708', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0078301', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-139.81', 'loss/train': '0.70834', 'examples_per_second': '32.76', 'grad_norm': '21.5', 'counters/examples': 227680, 'counters/updates': 7115}
train stats after 227712 examples: {'rewards_train/chosen': '0.073427', 'rewards_train/rejected': '0.061101', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012327', 'logps_train/rejected': '-172.84', 'logps_train/chosen': '-112.98', 'loss/train': '0.7011', 'examples_per_second': '30.21', 'grad_norm': '22.5', 'counters/examples': 227712, 'counters/updates': 7116}
train stats after 227744 examples: {'rewards_train/chosen': '0.17454', 'rewards_train/rejected': '0.035879', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13866', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-152.85', 'loss/train': '0.65026', 'examples_per_second': '30.192', 'grad_norm': '22.125', 'counters/examples': 227744, 'counters/updates': 7117}
train stats after 227776 examples: {'rewards_train/chosen': '0.13873', 'rewards_train/rejected': '-0.030683', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16941', 'logps_train/rejected': '-119.99', 'logps_train/chosen': '-141.54', 'loss/train': '0.62422', 'examples_per_second': '30.169', 'grad_norm': '19.875', 'counters/examples': 227776, 'counters/updates': 7118}
train stats after 227808 examples: {'rewards_train/chosen': '0.20088', 'rewards_train/rejected': '0.14803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05285', 'logps_train/rejected': '-155.3', 'logps_train/chosen': '-145.97', 'loss/train': '0.68202', 'examples_per_second': '31.57', 'grad_norm': '21.5', 'counters/examples': 227808, 'counters/updates': 7119}
train stats after 227840 examples: {'rewards_train/chosen': '0.26062', 'rewards_train/rejected': '0.14056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12006', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-155.48', 'loss/train': '0.65147', 'examples_per_second': '31.644', 'grad_norm': '20.5', 'counters/examples': 227840, 'counters/updates': 7120}
skipping logging after 227872 examples to avoid logging too frequently
train stats after 227904 examples: {'rewards_train/chosen': '0.1945', 'rewards_train/rejected': '0.11779', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076712', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-154.95', 'loss/train': '0.67049', 'examples_per_second': '30.59', 'grad_norm': '21.125', 'counters/examples': 227904, 'counters/updates': 7122}
skipping logging after 227936 examples to avoid logging too frequently
train stats after 227968 examples: {'rewards_train/chosen': '0.21567', 'rewards_train/rejected': '-0.044102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25977', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-148.02', 'loss/train': '0.59143', 'examples_per_second': '31.495', 'grad_norm': '20.25', 'counters/examples': 227968, 'counters/updates': 7124}
train stats after 228000 examples: {'rewards_train/chosen': '0.1186', 'rewards_train/rejected': '0.065928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052669', 'logps_train/rejected': '-112.18', 'logps_train/chosen': '-144.45', 'loss/train': '0.68586', 'examples_per_second': '31.376', 'grad_norm': '21.25', 'counters/examples': 228000, 'counters/updates': 7125}
Running evaluation after 228000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.92it/s]
eval after 228000: {'rewards_eval/chosen': '0.18518', 'rewards_eval/rejected': '0.043823', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.14136', 'logps_eval/rejected': '-114.95', 'logps_eval/chosen': '-133.7', 'loss/eval': '0.64519'}
skipping save for non epoch
train stats after 228032 examples: {'rewards_train/chosen': '0.14905', 'rewards_train/rejected': '0.088914', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060137', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-123.54', 'loss/train': '0.68905', 'examples_per_second': '33.057', 'grad_norm': '19.875', 'counters/examples': 228032, 'counters/updates': 7126}
train stats after 228064 examples: {'rewards_train/chosen': '0.30705', 'rewards_train/rejected': '-0.023577', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.33063', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-176.31', 'loss/train': '0.55821', 'examples_per_second': '31.704', 'grad_norm': '19.125', 'counters/examples': 228064, 'counters/updates': 7127}
train stats after 228096 examples: {'rewards_train/chosen': '0.10555', 'rewards_train/rejected': '0.097869', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00768', 'logps_train/rejected': '-132.65', 'logps_train/chosen': '-149.41', 'loss/train': '0.70463', 'examples_per_second': '30.176', 'grad_norm': '23.5', 'counters/examples': 228096, 'counters/updates': 7128}
train stats after 228128 examples: {'rewards_train/chosen': '0.13843', 'rewards_train/rejected': '0.021499', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11694', 'logps_train/rejected': '-180.85', 'logps_train/chosen': '-157.01', 'loss/train': '0.64993', 'examples_per_second': '30.169', 'grad_norm': '23.5', 'counters/examples': 228128, 'counters/updates': 7129}
skipping logging after 228160 examples to avoid logging too frequently
train stats after 228192 examples: {'rewards_train/chosen': '0.09062', 'rewards_train/rejected': '0.099914', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0092936', 'logps_train/rejected': '-113.12', 'logps_train/chosen': '-119.08', 'loss/train': '0.71035', 'examples_per_second': '33.748', 'grad_norm': '19.5', 'counters/examples': 228192, 'counters/updates': 7131}
skipping logging after 228224 examples to avoid logging too frequently
train stats after 228256 examples: {'rewards_train/chosen': '0.15033', 'rewards_train/rejected': '0.012014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13832', 'logps_train/rejected': '-89.282', 'logps_train/chosen': '-130.92', 'loss/train': '0.644', 'examples_per_second': '31.428', 'grad_norm': '17.75', 'counters/examples': 228256, 'counters/updates': 7133}
train stats after 228288 examples: {'rewards_train/chosen': '0.22327', 'rewards_train/rejected': '-0.0040058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22727', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-154.24', 'loss/train': '0.60601', 'examples_per_second': '32.761', 'grad_norm': '19.875', 'counters/examples': 228288, 'counters/updates': 7134}
train stats after 228320 examples: {'rewards_train/chosen': '0.19884', 'rewards_train/rejected': '0.022642', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1762', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-135.24', 'loss/train': '0.61812', 'examples_per_second': '32.958', 'grad_norm': '17.875', 'counters/examples': 228320, 'counters/updates': 7135}
train stats after 228352 examples: {'rewards_train/chosen': '0.21449', 'rewards_train/rejected': '0.054657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15983', 'logps_train/rejected': '-132.51', 'logps_train/chosen': '-159.99', 'loss/train': '0.63497', 'examples_per_second': '30.512', 'grad_norm': '21.375', 'counters/examples': 228352, 'counters/updates': 7136}
train stats after 228384 examples: {'rewards_train/chosen': '0.14736', 'rewards_train/rejected': '0.035693', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11167', 'logps_train/rejected': '-99.391', 'logps_train/chosen': '-133.04', 'loss/train': '0.66159', 'examples_per_second': '31.673', 'grad_norm': '19.5', 'counters/examples': 228384, 'counters/updates': 7137}
train stats after 228416 examples: {'rewards_train/chosen': '0.19256', 'rewards_train/rejected': '0.066514', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12605', 'logps_train/rejected': '-137.98', 'logps_train/chosen': '-189.74', 'loss/train': '0.65549', 'examples_per_second': '31.593', 'grad_norm': '22.125', 'counters/examples': 228416, 'counters/updates': 7138}
train stats after 228448 examples: {'rewards_train/chosen': '0.17949', 'rewards_train/rejected': '0.12405', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055433', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-155.13', 'loss/train': '0.68269', 'examples_per_second': '31.334', 'grad_norm': '23.125', 'counters/examples': 228448, 'counters/updates': 7139}
skipping logging after 228480 examples to avoid logging too frequently
train stats after 228512 examples: {'rewards_train/chosen': '0.18755', 'rewards_train/rejected': '0.10382', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083735', 'logps_train/rejected': '-138.27', 'logps_train/chosen': '-174.66', 'loss/train': '0.67235', 'examples_per_second': '31.664', 'grad_norm': '22', 'counters/examples': 228512, 'counters/updates': 7141}
train stats after 228544 examples: {'rewards_train/chosen': '0.18763', 'rewards_train/rejected': '0.022715', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16492', 'logps_train/rejected': '-111.68', 'logps_train/chosen': '-127.83', 'loss/train': '0.63337', 'examples_per_second': '30.071', 'grad_norm': '17.75', 'counters/examples': 228544, 'counters/updates': 7142}
skipping logging after 228576 examples to avoid logging too frequently
train stats after 228608 examples: {'rewards_train/chosen': '0.15761', 'rewards_train/rejected': '0.016232', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14138', 'logps_train/rejected': '-95.536', 'logps_train/chosen': '-120.41', 'loss/train': '0.63864', 'examples_per_second': '32.203', 'grad_norm': '17.5', 'counters/examples': 228608, 'counters/updates': 7144}
train stats after 228640 examples: {'rewards_train/chosen': '0.12949', 'rewards_train/rejected': '-0.019989', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14948', 'logps_train/rejected': '-103.92', 'logps_train/chosen': '-133.51', 'loss/train': '0.64202', 'examples_per_second': '31.712', 'grad_norm': '19.5', 'counters/examples': 228640, 'counters/updates': 7145}
skipping logging after 228672 examples to avoid logging too frequently
train stats after 228704 examples: {'rewards_train/chosen': '0.10453', 'rewards_train/rejected': '-0.045917', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15044', 'logps_train/rejected': '-91.143', 'logps_train/chosen': '-135.24', 'loss/train': '0.64249', 'examples_per_second': '31.969', 'grad_norm': '18.75', 'counters/examples': 228704, 'counters/updates': 7147}
train stats after 228736 examples: {'rewards_train/chosen': '0.10556', 'rewards_train/rejected': '-0.042422', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14799', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-128.4', 'loss/train': '0.64', 'examples_per_second': '31.397', 'grad_norm': '19.625', 'counters/examples': 228736, 'counters/updates': 7148}
train stats after 228768 examples: {'rewards_train/chosen': '0.17371', 'rewards_train/rejected': '0.043242', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13046', 'logps_train/rejected': '-122.71', 'logps_train/chosen': '-150.88', 'loss/train': '0.64768', 'examples_per_second': '24.527', 'grad_norm': '21.75', 'counters/examples': 228768, 'counters/updates': 7149}
train stats after 228800 examples: {'rewards_train/chosen': '0.14156', 'rewards_train/rejected': '0.12532', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016246', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-135.47', 'loss/train': '0.70997', 'examples_per_second': '30.84', 'grad_norm': '21.75', 'counters/examples': 228800, 'counters/updates': 7150}
skipping logging after 228832 examples to avoid logging too frequently
train stats after 228864 examples: {'rewards_train/chosen': '0.18056', 'rewards_train/rejected': '0.11206', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068499', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-145.66', 'loss/train': '0.67368', 'examples_per_second': '33.253', 'grad_norm': '22.625', 'counters/examples': 228864, 'counters/updates': 7152}
skipping logging after 228896 examples to avoid logging too frequently
train stats after 228928 examples: {'rewards_train/chosen': '0.26697', 'rewards_train/rejected': '0.10129', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16568', 'logps_train/rejected': '-110.49', 'logps_train/chosen': '-118.8', 'loss/train': '0.62392', 'examples_per_second': '32.566', 'grad_norm': '17.875', 'counters/examples': 228928, 'counters/updates': 7154}
skipping logging after 228960 examples to avoid logging too frequently
train stats after 228992 examples: {'rewards_train/chosen': '0.19294', 'rewards_train/rejected': '-0.0040041', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19694', 'logps_train/rejected': '-115.49', 'logps_train/chosen': '-135.45', 'loss/train': '0.61951', 'examples_per_second': '31.499', 'grad_norm': '19.25', 'counters/examples': 228992, 'counters/updates': 7156}
train stats after 229024 examples: {'rewards_train/chosen': '0.16073', 'rewards_train/rejected': '0.01612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14461', 'logps_train/rejected': '-99.673', 'logps_train/chosen': '-145.65', 'loss/train': '0.64708', 'examples_per_second': '32.162', 'grad_norm': '18.25', 'counters/examples': 229024, 'counters/updates': 7157}
train stats after 229056 examples: {'rewards_train/chosen': '0.14914', 'rewards_train/rejected': '0.039791', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10935', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-116.59', 'loss/train': '0.65236', 'examples_per_second': '31.661', 'grad_norm': '20.125', 'counters/examples': 229056, 'counters/updates': 7158}
train stats after 229088 examples: {'rewards_train/chosen': '0.11219', 'rewards_train/rejected': '0.13983', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02764', 'logps_train/rejected': '-132.23', 'logps_train/chosen': '-131.26', 'loss/train': '0.71945', 'examples_per_second': '31.397', 'grad_norm': '22.25', 'counters/examples': 229088, 'counters/updates': 7159}
train stats after 229120 examples: {'rewards_train/chosen': '0.17525', 'rewards_train/rejected': '-0.0078169', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18307', 'logps_train/rejected': '-111.42', 'logps_train/chosen': '-162.46', 'loss/train': '0.61857', 'examples_per_second': '30.358', 'grad_norm': '20.25', 'counters/examples': 229120, 'counters/updates': 7160}
train stats after 229152 examples: {'rewards_train/chosen': '0.15555', 'rewards_train/rejected': '0.068796', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086758', 'logps_train/rejected': '-111.37', 'logps_train/chosen': '-133.86', 'loss/train': '0.66028', 'examples_per_second': '30.068', 'grad_norm': '19.75', 'counters/examples': 229152, 'counters/updates': 7161}
train stats after 229184 examples: {'rewards_train/chosen': '0.16465', 'rewards_train/rejected': '0.0088192', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15583', 'logps_train/rejected': '-96.918', 'logps_train/chosen': '-118.25', 'loss/train': '0.63876', 'examples_per_second': '31.327', 'grad_norm': '17.625', 'counters/examples': 229184, 'counters/updates': 7162}
train stats after 229216 examples: {'rewards_train/chosen': '0.16077', 'rewards_train/rejected': '0.082861', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.077908', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-152.18', 'loss/train': '0.67514', 'examples_per_second': '31.316', 'grad_norm': '22.125', 'counters/examples': 229216, 'counters/updates': 7163}
train stats after 229248 examples: {'rewards_train/chosen': '0.17997', 'rewards_train/rejected': '0.0025928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17738', 'logps_train/rejected': '-172.37', 'logps_train/chosen': '-161.61', 'loss/train': '0.63471', 'examples_per_second': '30.158', 'grad_norm': '23.125', 'counters/examples': 229248, 'counters/updates': 7164}
train stats after 229280 examples: {'rewards_train/chosen': '0.24742', 'rewards_train/rejected': '-0.0084226', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25585', 'logps_train/rejected': '-155.84', 'logps_train/chosen': '-150.71', 'loss/train': '0.59509', 'examples_per_second': '33.311', 'grad_norm': '20.75', 'counters/examples': 229280, 'counters/updates': 7165}
train stats after 229312 examples: {'rewards_train/chosen': '0.18183', 'rewards_train/rejected': '0.1702', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011626', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-126.47', 'loss/train': '0.69919', 'examples_per_second': '31.685', 'grad_norm': '21.125', 'counters/examples': 229312, 'counters/updates': 7166}
skipping logging after 229344 examples to avoid logging too frequently
train stats after 229376 examples: {'rewards_train/chosen': '0.033427', 'rewards_train/rejected': '-0.053725', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087152', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-153.17', 'loss/train': '0.66749', 'examples_per_second': '30.483', 'grad_norm': '20', 'counters/examples': 229376, 'counters/updates': 7168}
train stats after 229408 examples: {'rewards_train/chosen': '0.18819', 'rewards_train/rejected': '0.0027922', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1854', 'logps_train/rejected': '-129.43', 'logps_train/chosen': '-161.18', 'loss/train': '0.62275', 'examples_per_second': '32.437', 'grad_norm': '21.125', 'counters/examples': 229408, 'counters/updates': 7169}
train stats after 229440 examples: {'rewards_train/chosen': '0.14532', 'rewards_train/rejected': '-0.010973', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15629', 'logps_train/rejected': '-101.48', 'logps_train/chosen': '-142.35', 'loss/train': '0.63339', 'examples_per_second': '30.095', 'grad_norm': '19.125', 'counters/examples': 229440, 'counters/updates': 7170}
train stats after 229472 examples: {'rewards_train/chosen': '0.11184', 'rewards_train/rejected': '0.026401', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085439', 'logps_train/rejected': '-103.7', 'logps_train/chosen': '-115.09', 'loss/train': '0.67041', 'examples_per_second': '31.684', 'grad_norm': '19.375', 'counters/examples': 229472, 'counters/updates': 7171}
train stats after 229504 examples: {'rewards_train/chosen': '0.20426', 'rewards_train/rejected': '0.086831', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11743', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-163.44', 'loss/train': '0.65962', 'examples_per_second': '32.411', 'grad_norm': '22.5', 'counters/examples': 229504, 'counters/updates': 7172}
train stats after 229536 examples: {'rewards_train/chosen': '0.23389', 'rewards_train/rejected': '0.064013', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16988', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-120.16', 'loss/train': '0.6254', 'examples_per_second': '32.466', 'grad_norm': '19.5', 'counters/examples': 229536, 'counters/updates': 7173}
train stats after 229568 examples: {'rewards_train/chosen': '0.23416', 'rewards_train/rejected': '0.075648', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15851', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-144.42', 'loss/train': '0.63336', 'examples_per_second': '31.339', 'grad_norm': '21.75', 'counters/examples': 229568, 'counters/updates': 7174}
train stats after 229600 examples: {'rewards_train/chosen': '0.089574', 'rewards_train/rejected': '0.041226', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048348', 'logps_train/rejected': '-129.13', 'logps_train/chosen': '-113.92', 'loss/train': '0.68616', 'examples_per_second': '32.54', 'grad_norm': '20.125', 'counters/examples': 229600, 'counters/updates': 7175}
train stats after 229632 examples: {'rewards_train/chosen': '0.25874', 'rewards_train/rejected': '0.073866', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18487', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-138.4', 'loss/train': '0.62055', 'examples_per_second': '32.379', 'grad_norm': '19.25', 'counters/examples': 229632, 'counters/updates': 7176}
train stats after 229664 examples: {'rewards_train/chosen': '0.25946', 'rewards_train/rejected': '0.035747', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22371', 'logps_train/rejected': '-121.39', 'logps_train/chosen': '-164.59', 'loss/train': '0.61099', 'examples_per_second': '30.517', 'grad_norm': '20.25', 'counters/examples': 229664, 'counters/updates': 7177}
train stats after 229696 examples: {'rewards_train/chosen': '0.23982', 'rewards_train/rejected': '0.09218', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14764', 'logps_train/rejected': '-94.226', 'logps_train/chosen': '-164.56', 'loss/train': '0.63596', 'examples_per_second': '24.695', 'grad_norm': '20.75', 'counters/examples': 229696, 'counters/updates': 7178}
train stats after 229728 examples: {'rewards_train/chosen': '0.21048', 'rewards_train/rejected': '0.016765', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19372', 'logps_train/rejected': '-108.96', 'logps_train/chosen': '-151', 'loss/train': '0.62473', 'examples_per_second': '31.242', 'grad_norm': '20.375', 'counters/examples': 229728, 'counters/updates': 7179}
train stats after 229760 examples: {'rewards_train/chosen': '0.19624', 'rewards_train/rejected': '0.0040206', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19222', 'logps_train/rejected': '-107.57', 'logps_train/chosen': '-160.43', 'loss/train': '0.6163', 'examples_per_second': '31.585', 'grad_norm': '19.125', 'counters/examples': 229760, 'counters/updates': 7180}
train stats after 229792 examples: {'rewards_train/chosen': '0.086928', 'rewards_train/rejected': '0.032854', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054074', 'logps_train/rejected': '-118.44', 'logps_train/chosen': '-132.12', 'loss/train': '0.68147', 'examples_per_second': '24.475', 'grad_norm': '20.25', 'counters/examples': 229792, 'counters/updates': 7181}
train stats after 229824 examples: {'rewards_train/chosen': '0.12626', 'rewards_train/rejected': '-0.0036151', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12988', 'logps_train/rejected': '-88.55', 'logps_train/chosen': '-123.05', 'loss/train': '0.64829', 'examples_per_second': '32.614', 'grad_norm': '18.75', 'counters/examples': 229824, 'counters/updates': 7182}
skipping logging after 229856 examples to avoid logging too frequently
train stats after 229888 examples: {'rewards_train/chosen': '0.2167', 'rewards_train/rejected': '0.03459', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18211', 'logps_train/rejected': '-126.79', 'logps_train/chosen': '-157.53', 'loss/train': '0.62604', 'examples_per_second': '30.172', 'grad_norm': '22.875', 'counters/examples': 229888, 'counters/updates': 7184}
train stats after 229920 examples: {'rewards_train/chosen': '0.099039', 'rewards_train/rejected': '-0.085939', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18498', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-147.05', 'loss/train': '0.6291', 'examples_per_second': '30.709', 'grad_norm': '19.5', 'counters/examples': 229920, 'counters/updates': 7185}
train stats after 229952 examples: {'rewards_train/chosen': '0.17726', 'rewards_train/rejected': '-0.0069757', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18424', 'logps_train/rejected': '-93.767', 'logps_train/chosen': '-142.4', 'loss/train': '0.62518', 'examples_per_second': '31.729', 'grad_norm': '17.375', 'counters/examples': 229952, 'counters/updates': 7186}
skipping logging after 229984 examples to avoid logging too frequently
train stats after 230016 examples: {'rewards_train/chosen': '0.063635', 'rewards_train/rejected': '-0.11503', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17867', 'logps_train/rejected': '-89.273', 'logps_train/chosen': '-101.08', 'loss/train': '0.62477', 'examples_per_second': '31.717', 'grad_norm': '17.25', 'counters/examples': 230016, 'counters/updates': 7188}
train stats after 230048 examples: {'rewards_train/chosen': '0.12986', 'rewards_train/rejected': '0.1205', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0093593', 'logps_train/rejected': '-116.12', 'logps_train/chosen': '-158.73', 'loss/train': '0.70499', 'examples_per_second': '31.942', 'grad_norm': '21.75', 'counters/examples': 230048, 'counters/updates': 7189}
train stats after 230080 examples: {'rewards_train/chosen': '0.21068', 'rewards_train/rejected': '0.084334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12635', 'logps_train/rejected': '-94.963', 'logps_train/chosen': '-112.59', 'loss/train': '0.65045', 'examples_per_second': '31.066', 'grad_norm': '17.75', 'counters/examples': 230080, 'counters/updates': 7190}
skipping logging after 230112 examples to avoid logging too frequently
train stats after 230144 examples: {'rewards_train/chosen': '0.19083', 'rewards_train/rejected': '0.14255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048276', 'logps_train/rejected': '-146.77', 'logps_train/chosen': '-159.62', 'loss/train': '0.69147', 'examples_per_second': '33.95', 'grad_norm': '23.75', 'counters/examples': 230144, 'counters/updates': 7192}
train stats after 230176 examples: {'rewards_train/chosen': '0.19067', 'rewards_train/rejected': '0.02331', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16736', 'logps_train/rejected': '-140.25', 'logps_train/chosen': '-147.7', 'loss/train': '0.63245', 'examples_per_second': '32.411', 'grad_norm': '22', 'counters/examples': 230176, 'counters/updates': 7193}
train stats after 230208 examples: {'rewards_train/chosen': '0.21729', 'rewards_train/rejected': '0.016444', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20084', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-140.55', 'loss/train': '0.61631', 'examples_per_second': '30.826', 'grad_norm': '19.375', 'counters/examples': 230208, 'counters/updates': 7194}
train stats after 230240 examples: {'rewards_train/chosen': '0.19587', 'rewards_train/rejected': '0.023796', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17208', 'logps_train/rejected': '-154.16', 'logps_train/chosen': '-128.81', 'loss/train': '0.62535', 'examples_per_second': '31.724', 'grad_norm': '20.75', 'counters/examples': 230240, 'counters/updates': 7195}
train stats after 230272 examples: {'rewards_train/chosen': '0.20702', 'rewards_train/rejected': '0.075471', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13155', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-118.81', 'loss/train': '0.63946', 'examples_per_second': '31.021', 'grad_norm': '18.625', 'counters/examples': 230272, 'counters/updates': 7196}
train stats after 230304 examples: {'rewards_train/chosen': '0.16766', 'rewards_train/rejected': '0.0056309', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16203', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-108.22', 'loss/train': '0.63223', 'examples_per_second': '32.566', 'grad_norm': '17.5', 'counters/examples': 230304, 'counters/updates': 7197}
train stats after 230336 examples: {'rewards_train/chosen': '0.15401', 'rewards_train/rejected': '0.044058', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10995', 'logps_train/rejected': '-115.19', 'logps_train/chosen': '-147.88', 'loss/train': '0.65392', 'examples_per_second': '30.955', 'grad_norm': '20.375', 'counters/examples': 230336, 'counters/updates': 7198}
train stats after 230368 examples: {'rewards_train/chosen': '0.16402', 'rewards_train/rejected': '0.056474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10754', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-121.34', 'loss/train': '0.65928', 'examples_per_second': '30.994', 'grad_norm': '19.75', 'counters/examples': 230368, 'counters/updates': 7199}
train stats after 230400 examples: {'rewards_train/chosen': '0.11263', 'rewards_train/rejected': '-0.011731', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12436', 'logps_train/rejected': '-136.65', 'logps_train/chosen': '-119.25', 'loss/train': '0.64999', 'examples_per_second': '31.319', 'grad_norm': '21.125', 'counters/examples': 230400, 'counters/updates': 7200}
train stats after 230432 examples: {'rewards_train/chosen': '0.21097', 'rewards_train/rejected': '0.045305', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16566', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-149.64', 'loss/train': '0.62978', 'examples_per_second': '31.623', 'grad_norm': '20.75', 'counters/examples': 230432, 'counters/updates': 7201}
train stats after 230464 examples: {'rewards_train/chosen': '0.057298', 'rewards_train/rejected': '-0.010795', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068093', 'logps_train/rejected': '-168.66', 'logps_train/chosen': '-139.86', 'loss/train': '0.67022', 'examples_per_second': '31.673', 'grad_norm': '23', 'counters/examples': 230464, 'counters/updates': 7202}
train stats after 230496 examples: {'rewards_train/chosen': '0.20896', 'rewards_train/rejected': '0.066667', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14229', 'logps_train/rejected': '-96.265', 'logps_train/chosen': '-115.86', 'loss/train': '0.63722', 'examples_per_second': '30.973', 'grad_norm': '16.75', 'counters/examples': 230496, 'counters/updates': 7203}
skipping logging after 230528 examples to avoid logging too frequently
train stats after 230560 examples: {'rewards_train/chosen': '0.14816', 'rewards_train/rejected': '0.067399', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080763', 'logps_train/rejected': '-124.07', 'logps_train/chosen': '-109.8', 'loss/train': '0.66121', 'examples_per_second': '31.663', 'grad_norm': '19.75', 'counters/examples': 230560, 'counters/updates': 7205}
train stats after 230592 examples: {'rewards_train/chosen': '0.18404', 'rewards_train/rejected': '0.043876', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14017', 'logps_train/rejected': '-117.23', 'logps_train/chosen': '-126.82', 'loss/train': '0.64175', 'examples_per_second': '30.979', 'grad_norm': '19.75', 'counters/examples': 230592, 'counters/updates': 7206}
train stats after 230624 examples: {'rewards_train/chosen': '0.13911', 'rewards_train/rejected': '-0.0071792', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14629', 'logps_train/rejected': '-113.75', 'logps_train/chosen': '-108.26', 'loss/train': '0.63715', 'examples_per_second': '31.833', 'grad_norm': '18.5', 'counters/examples': 230624, 'counters/updates': 7207}
skipping logging after 230656 examples to avoid logging too frequently
train stats after 230688 examples: {'rewards_train/chosen': '0.060609', 'rewards_train/rejected': '0.0082061', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052403', 'logps_train/rejected': '-97.748', 'logps_train/chosen': '-143.24', 'loss/train': '0.67669', 'examples_per_second': '31.647', 'grad_norm': '20.25', 'counters/examples': 230688, 'counters/updates': 7209}
train stats after 230720 examples: {'rewards_train/chosen': '0.22914', 'rewards_train/rejected': '0.10945', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11969', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-131.17', 'loss/train': '0.64747', 'examples_per_second': '30.713', 'grad_norm': '18.25', 'counters/examples': 230720, 'counters/updates': 7210}
train stats after 230752 examples: {'rewards_train/chosen': '0.14432', 'rewards_train/rejected': '-0.003412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14773', 'logps_train/rejected': '-146.82', 'logps_train/chosen': '-132.71', 'loss/train': '0.64616', 'examples_per_second': '31.686', 'grad_norm': '21.25', 'counters/examples': 230752, 'counters/updates': 7211}
skipping logging after 230784 examples to avoid logging too frequently
train stats after 230816 examples: {'rewards_train/chosen': '0.12694', 'rewards_train/rejected': '0.12188', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0050548', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-155.27', 'loss/train': '0.69922', 'examples_per_second': '32.569', 'grad_norm': '21.5', 'counters/examples': 230816, 'counters/updates': 7213}
skipping logging after 230848 examples to avoid logging too frequently
train stats after 230880 examples: {'rewards_train/chosen': '0.13723', 'rewards_train/rejected': '-0.016051', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15328', 'logps_train/rejected': '-104.85', 'logps_train/chosen': '-139.85', 'loss/train': '0.63715', 'examples_per_second': '30.161', 'grad_norm': '18.75', 'counters/examples': 230880, 'counters/updates': 7215}
train stats after 230912 examples: {'rewards_train/chosen': '0.23476', 'rewards_train/rejected': '0.059587', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17517', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-128.04', 'loss/train': '0.62403', 'examples_per_second': '32.021', 'grad_norm': '19', 'counters/examples': 230912, 'counters/updates': 7216}
train stats after 230944 examples: {'rewards_train/chosen': '0.1673', 'rewards_train/rejected': '0.082501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0848', 'logps_train/rejected': '-163.1', 'logps_train/chosen': '-155.31', 'loss/train': '0.66527', 'examples_per_second': '32.435', 'grad_norm': '23.125', 'counters/examples': 230944, 'counters/updates': 7217}
train stats after 230976 examples: {'rewards_train/chosen': '0.11166', 'rewards_train/rejected': '0.10994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0017288', 'logps_train/rejected': '-118.85', 'logps_train/chosen': '-153.17', 'loss/train': '0.70798', 'examples_per_second': '31.685', 'grad_norm': '22.375', 'counters/examples': 230976, 'counters/updates': 7218}
train stats after 231008 examples: {'rewards_train/chosen': '0.17184', 'rewards_train/rejected': '0.083968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087867', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-130.98', 'loss/train': '0.66278', 'examples_per_second': '31.374', 'grad_norm': '20.25', 'counters/examples': 231008, 'counters/updates': 7219}
train stats after 231040 examples: {'rewards_train/chosen': '0.17952', 'rewards_train/rejected': '0.042289', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13723', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-120.41', 'loss/train': '0.644', 'examples_per_second': '31.668', 'grad_norm': '19.625', 'counters/examples': 231040, 'counters/updates': 7220}
skipping logging after 231072 examples to avoid logging too frequently
train stats after 231104 examples: {'rewards_train/chosen': '0.18978', 'rewards_train/rejected': '0.023678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16611', 'logps_train/rejected': '-112.45', 'logps_train/chosen': '-124.33', 'loss/train': '0.63519', 'examples_per_second': '31.996', 'grad_norm': '18.875', 'counters/examples': 231104, 'counters/updates': 7222}
skipping logging after 231136 examples to avoid logging too frequently
train stats after 231168 examples: {'rewards_train/chosen': '0.23013', 'rewards_train/rejected': '0.013477', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21665', 'logps_train/rejected': '-99.993', 'logps_train/chosen': '-145.27', 'loss/train': '0.61651', 'examples_per_second': '30.319', 'grad_norm': '18.375', 'counters/examples': 231168, 'counters/updates': 7224}
skipping logging after 231200 examples to avoid logging too frequently
train stats after 231232 examples: {'rewards_train/chosen': '0.15577', 'rewards_train/rejected': '0.026783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12898', 'logps_train/rejected': '-103.45', 'logps_train/chosen': '-134.36', 'loss/train': '0.64367', 'examples_per_second': '30.175', 'grad_norm': '20.375', 'counters/examples': 231232, 'counters/updates': 7226}
train stats after 231264 examples: {'rewards_train/chosen': '0.23597', 'rewards_train/rejected': '0.081488', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15448', 'logps_train/rejected': '-101.22', 'logps_train/chosen': '-159.95', 'loss/train': '0.62948', 'examples_per_second': '31.665', 'grad_norm': '19.25', 'counters/examples': 231264, 'counters/updates': 7227}
train stats after 231296 examples: {'rewards_train/chosen': '0.093201', 'rewards_train/rejected': '-0.045861', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13906', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-148.33', 'loss/train': '0.64039', 'examples_per_second': '31.52', 'grad_norm': '20.375', 'counters/examples': 231296, 'counters/updates': 7228}
skipping logging after 231328 examples to avoid logging too frequently
train stats after 231360 examples: {'rewards_train/chosen': '0.23418', 'rewards_train/rejected': '0.11473', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11946', 'logps_train/rejected': '-104.58', 'logps_train/chosen': '-137.59', 'loss/train': '0.64754', 'examples_per_second': '31.665', 'grad_norm': '19.75', 'counters/examples': 231360, 'counters/updates': 7230}
train stats after 231392 examples: {'rewards_train/chosen': '0.12442', 'rewards_train/rejected': '0.015646', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10877', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-121.21', 'loss/train': '0.65616', 'examples_per_second': '31.823', 'grad_norm': '18.875', 'counters/examples': 231392, 'counters/updates': 7231}
train stats after 231424 examples: {'rewards_train/chosen': '0.22377', 'rewards_train/rejected': '0.082718', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14105', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-113.06', 'loss/train': '0.63701', 'examples_per_second': '30.568', 'grad_norm': '19.25', 'counters/examples': 231424, 'counters/updates': 7232}
skipping logging after 231456 examples to avoid logging too frequently
train stats after 231488 examples: {'rewards_train/chosen': '0.21689', 'rewards_train/rejected': '0.032054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18484', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-149.77', 'loss/train': '0.62574', 'examples_per_second': '33.538', 'grad_norm': '19.875', 'counters/examples': 231488, 'counters/updates': 7234}
train stats after 231520 examples: {'rewards_train/chosen': '0.23633', 'rewards_train/rejected': '0.055726', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1806', 'logps_train/rejected': '-117.7', 'logps_train/chosen': '-132.98', 'loss/train': '0.6148', 'examples_per_second': '31.423', 'grad_norm': '19.75', 'counters/examples': 231520, 'counters/updates': 7235}
skipping logging after 231552 examples to avoid logging too frequently
train stats after 231584 examples: {'rewards_train/chosen': '0.26858', 'rewards_train/rejected': '0.083324', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18526', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-147.92', 'loss/train': '0.6228', 'examples_per_second': '30.984', 'grad_norm': '20.25', 'counters/examples': 231584, 'counters/updates': 7237}
train stats after 231616 examples: {'rewards_train/chosen': '0.21302', 'rewards_train/rejected': '-0.029568', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24259', 'logps_train/rejected': '-95.761', 'logps_train/chosen': '-127.62', 'loss/train': '0.59845', 'examples_per_second': '32.747', 'grad_norm': '17.375', 'counters/examples': 231616, 'counters/updates': 7238}
train stats after 231648 examples: {'rewards_train/chosen': '0.11637', 'rewards_train/rejected': '-0.010595', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12696', 'logps_train/rejected': '-108.02', 'logps_train/chosen': '-150.63', 'loss/train': '0.6441', 'examples_per_second': '32.742', 'grad_norm': '20.25', 'counters/examples': 231648, 'counters/updates': 7239}
train stats after 231680 examples: {'rewards_train/chosen': '0.27231', 'rewards_train/rejected': '0.06898', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20333', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-145.91', 'loss/train': '0.61581', 'examples_per_second': '30.149', 'grad_norm': '18.875', 'counters/examples': 231680, 'counters/updates': 7240}
skipping logging after 231712 examples to avoid logging too frequently
train stats after 231744 examples: {'rewards_train/chosen': '0.23599', 'rewards_train/rejected': '0.046304', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18969', 'logps_train/rejected': '-94.323', 'logps_train/chosen': '-148.3', 'loss/train': '0.62874', 'examples_per_second': '31.542', 'grad_norm': '19.875', 'counters/examples': 231744, 'counters/updates': 7242}
train stats after 231776 examples: {'rewards_train/chosen': '0.12668', 'rewards_train/rejected': '-0.010114', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13679', 'logps_train/rejected': '-123.73', 'logps_train/chosen': '-123.3', 'loss/train': '0.64356', 'examples_per_second': '31.558', 'grad_norm': '19.875', 'counters/examples': 231776, 'counters/updates': 7243}
train stats after 231808 examples: {'rewards_train/chosen': '0.17047', 'rewards_train/rejected': '0.050604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11987', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-119.85', 'loss/train': '0.64988', 'examples_per_second': '30.243', 'grad_norm': '19', 'counters/examples': 231808, 'counters/updates': 7244}
train stats after 231840 examples: {'rewards_train/chosen': '0.088147', 'rewards_train/rejected': '0.046532', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041615', 'logps_train/rejected': '-152.8', 'logps_train/chosen': '-148.04', 'loss/train': '0.68396', 'examples_per_second': '32.184', 'grad_norm': '22.375', 'counters/examples': 231840, 'counters/updates': 7245}
train stats after 231872 examples: {'rewards_train/chosen': '0.14521', 'rewards_train/rejected': '0.10597', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039242', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-149.67', 'loss/train': '0.70631', 'examples_per_second': '30.573', 'grad_norm': '25.375', 'counters/examples': 231872, 'counters/updates': 7246}
skipping logging after 231904 examples to avoid logging too frequently
train stats after 231936 examples: {'rewards_train/chosen': '0.15869', 'rewards_train/rejected': '-0.0025965', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16129', 'logps_train/rejected': '-84.145', 'logps_train/chosen': '-131.62', 'loss/train': '0.62948', 'examples_per_second': '34.427', 'grad_norm': '17.5', 'counters/examples': 231936, 'counters/updates': 7248}
train stats after 231968 examples: {'rewards_train/chosen': '0.21117', 'rewards_train/rejected': '-0.080265', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.29144', 'logps_train/rejected': '-113.13', 'logps_train/chosen': '-136.03', 'loss/train': '0.58338', 'examples_per_second': '30.863', 'grad_norm': '17.5', 'counters/examples': 231968, 'counters/updates': 7249}
train stats after 232000 examples: {'rewards_train/chosen': '0.19177', 'rewards_train/rejected': '0.082021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10975', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-118.88', 'loss/train': '0.65508', 'examples_per_second': '31.665', 'grad_norm': '20.75', 'counters/examples': 232000, 'counters/updates': 7250}
skipping logging after 232032 examples to avoid logging too frequently
train stats after 232064 examples: {'rewards_train/chosen': '0.1214', 'rewards_train/rejected': '-0.012216', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13362', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-126.81', 'loss/train': '0.64387', 'examples_per_second': '32.291', 'grad_norm': '18.5', 'counters/examples': 232064, 'counters/updates': 7252}
train stats after 232096 examples: {'rewards_train/chosen': '0.07192', 'rewards_train/rejected': '0.018831', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053089', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-150.89', 'loss/train': '0.68948', 'examples_per_second': '32.965', 'grad_norm': '21.875', 'counters/examples': 232096, 'counters/updates': 7253}
train stats after 232128 examples: {'rewards_train/chosen': '0.15706', 'rewards_train/rejected': '-0.00048944', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15755', 'logps_train/rejected': '-88.285', 'logps_train/chosen': '-129.22', 'loss/train': '0.6266', 'examples_per_second': '31.789', 'grad_norm': '17.625', 'counters/examples': 232128, 'counters/updates': 7254}
train stats after 232160 examples: {'rewards_train/chosen': '0.22174', 'rewards_train/rejected': '0.069575', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15217', 'logps_train/rejected': '-95.804', 'logps_train/chosen': '-110.36', 'loss/train': '0.64084', 'examples_per_second': '31.338', 'grad_norm': '17.375', 'counters/examples': 232160, 'counters/updates': 7255}
train stats after 232192 examples: {'rewards_train/chosen': '0.075372', 'rewards_train/rejected': '-0.045399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12077', 'logps_train/rejected': '-115.7', 'logps_train/chosen': '-113.97', 'loss/train': '0.64901', 'examples_per_second': '30.655', 'grad_norm': '18.25', 'counters/examples': 232192, 'counters/updates': 7256}
skipping logging after 232224 examples to avoid logging too frequently
train stats after 232256 examples: {'rewards_train/chosen': '0.14178', 'rewards_train/rejected': '0.0024027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13938', 'logps_train/rejected': '-108.37', 'logps_train/chosen': '-133.53', 'loss/train': '0.65334', 'examples_per_second': '32.065', 'grad_norm': '19.375', 'counters/examples': 232256, 'counters/updates': 7258}
train stats after 232288 examples: {'rewards_train/chosen': '0.18798', 'rewards_train/rejected': '0.079672', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10831', 'logps_train/rejected': '-131.11', 'logps_train/chosen': '-115.67', 'loss/train': '0.65', 'examples_per_second': '31.571', 'grad_norm': '20.125', 'counters/examples': 232288, 'counters/updates': 7259}
train stats after 232320 examples: {'rewards_train/chosen': '0.23997', 'rewards_train/rejected': '-0.057678', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.29765', 'logps_train/rejected': '-150.54', 'logps_train/chosen': '-158.68', 'loss/train': '0.57602', 'examples_per_second': '30.718', 'grad_norm': '20.875', 'counters/examples': 232320, 'counters/updates': 7260}
skipping logging after 232352 examples to avoid logging too frequently
train stats after 232384 examples: {'rewards_train/chosen': '0.13238', 'rewards_train/rejected': '0.05907', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.073307', 'logps_train/rejected': '-100.71', 'logps_train/chosen': '-157.23', 'loss/train': '0.67184', 'examples_per_second': '30.684', 'grad_norm': '20.875', 'counters/examples': 232384, 'counters/updates': 7262}
train stats after 232416 examples: {'rewards_train/chosen': '0.18061', 'rewards_train/rejected': '0.10524', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075374', 'logps_train/rejected': '-116.5', 'logps_train/chosen': '-136.03', 'loss/train': '0.67339', 'examples_per_second': '31.639', 'grad_norm': '19.75', 'counters/examples': 232416, 'counters/updates': 7263}
train stats after 232448 examples: {'rewards_train/chosen': '0.10033', 'rewards_train/rejected': '-0.0012225', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10155', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-142.62', 'loss/train': '0.66033', 'examples_per_second': '31.96', 'grad_norm': '20.25', 'counters/examples': 232448, 'counters/updates': 7264}
train stats after 232480 examples: {'rewards_train/chosen': '0.25302', 'rewards_train/rejected': '0.084931', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16808', 'logps_train/rejected': '-124.88', 'logps_train/chosen': '-139.17', 'loss/train': '0.63418', 'examples_per_second': '32.195', 'grad_norm': '22', 'counters/examples': 232480, 'counters/updates': 7265}
train stats after 232512 examples: {'rewards_train/chosen': '0.24027', 'rewards_train/rejected': '0.031899', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20837', 'logps_train/rejected': '-133.13', 'logps_train/chosen': '-159.67', 'loss/train': '0.60385', 'examples_per_second': '30.531', 'grad_norm': '20.25', 'counters/examples': 232512, 'counters/updates': 7266}
train stats after 232544 examples: {'rewards_train/chosen': '0.14243', 'rewards_train/rejected': '0.0050996', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13733', 'logps_train/rejected': '-138.79', 'logps_train/chosen': '-130.37', 'loss/train': '0.63725', 'examples_per_second': '30.169', 'grad_norm': '20.5', 'counters/examples': 232544, 'counters/updates': 7267}
train stats after 232576 examples: {'rewards_train/chosen': '0.081285', 'rewards_train/rejected': '-0.041467', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12275', 'logps_train/rejected': '-94.431', 'logps_train/chosen': '-128.78', 'loss/train': '0.64741', 'examples_per_second': '32.437', 'grad_norm': '18.375', 'counters/examples': 232576, 'counters/updates': 7268}
train stats after 232608 examples: {'rewards_train/chosen': '0.17095', 'rewards_train/rejected': '-0.015524', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18647', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-130.79', 'loss/train': '0.62128', 'examples_per_second': '31.657', 'grad_norm': '19.75', 'counters/examples': 232608, 'counters/updates': 7269}
train stats after 232640 examples: {'rewards_train/chosen': '0.17798', 'rewards_train/rejected': '0.08805', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089927', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-131.47', 'loss/train': '0.65571', 'examples_per_second': '32.969', 'grad_norm': '20.375', 'counters/examples': 232640, 'counters/updates': 7270}
train stats after 232672 examples: {'rewards_train/chosen': '0.18934', 'rewards_train/rejected': '0.008208', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18113', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-152.87', 'loss/train': '0.62208', 'examples_per_second': '30.551', 'grad_norm': '19.75', 'counters/examples': 232672, 'counters/updates': 7271}
train stats after 232704 examples: {'rewards_train/chosen': '0.13577', 'rewards_train/rejected': '0.039361', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096407', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-152.84', 'loss/train': '0.67172', 'examples_per_second': '32.931', 'grad_norm': '21.875', 'counters/examples': 232704, 'counters/updates': 7272}
train stats after 232736 examples: {'rewards_train/chosen': '0.21848', 'rewards_train/rejected': '0.056085', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1624', 'logps_train/rejected': '-177.97', 'logps_train/chosen': '-160.91', 'loss/train': '0.62697', 'examples_per_second': '31.629', 'grad_norm': '22.375', 'counters/examples': 232736, 'counters/updates': 7273}
train stats after 232768 examples: {'rewards_train/chosen': '0.062577', 'rewards_train/rejected': '0.0092683', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053309', 'logps_train/rejected': '-112.2', 'logps_train/chosen': '-122.19', 'loss/train': '0.68162', 'examples_per_second': '32.735', 'grad_norm': '18.875', 'counters/examples': 232768, 'counters/updates': 7274}
train stats after 232800 examples: {'rewards_train/chosen': '0.29256', 'rewards_train/rejected': '-0.0059078', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.29847', 'logps_train/rejected': '-85.558', 'logps_train/chosen': '-108.44', 'loss/train': '0.57498', 'examples_per_second': '31.612', 'grad_norm': '15.625', 'counters/examples': 232800, 'counters/updates': 7275}
train stats after 232832 examples: {'rewards_train/chosen': '0.05845', 'rewards_train/rejected': '-0.0022765', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.060726', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-174.68', 'loss/train': '0.67741', 'examples_per_second': '30.204', 'grad_norm': '23.125', 'counters/examples': 232832, 'counters/updates': 7276}
skipping logging after 232864 examples to avoid logging too frequently
train stats after 232896 examples: {'rewards_train/chosen': '0.18749', 'rewards_train/rejected': '0.053231', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13426', 'logps_train/rejected': '-117.24', 'logps_train/chosen': '-134.2', 'loss/train': '0.65469', 'examples_per_second': '31.678', 'grad_norm': '21', 'counters/examples': 232896, 'counters/updates': 7278}
train stats after 232928 examples: {'rewards_train/chosen': '0.086304', 'rewards_train/rejected': '0.032561', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053743', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-135.79', 'loss/train': '0.68957', 'examples_per_second': '31.603', 'grad_norm': '20.75', 'counters/examples': 232928, 'counters/updates': 7279}
train stats after 232960 examples: {'rewards_train/chosen': '0.087493', 'rewards_train/rejected': '0.0078492', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079644', 'logps_train/rejected': '-108.16', 'logps_train/chosen': '-124.82', 'loss/train': '0.66957', 'examples_per_second': '32.926', 'grad_norm': '18.75', 'counters/examples': 232960, 'counters/updates': 7280}
train stats after 232992 examples: {'rewards_train/chosen': '0.15915', 'rewards_train/rejected': '0.025582', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13357', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-133.44', 'loss/train': '0.64321', 'examples_per_second': '31.638', 'grad_norm': '20.625', 'counters/examples': 232992, 'counters/updates': 7281}
train stats after 233024 examples: {'rewards_train/chosen': '0.24763', 'rewards_train/rejected': '0.0054557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24218', 'logps_train/rejected': '-114.33', 'logps_train/chosen': '-133.48', 'loss/train': '0.61014', 'examples_per_second': '31.561', 'grad_norm': '19.5', 'counters/examples': 233024, 'counters/updates': 7282}
train stats after 233056 examples: {'rewards_train/chosen': '0.21641', 'rewards_train/rejected': '0.14601', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0704', 'logps_train/rejected': '-128.56', 'logps_train/chosen': '-145.79', 'loss/train': '0.67723', 'examples_per_second': '31.93', 'grad_norm': '20.75', 'counters/examples': 233056, 'counters/updates': 7283}
train stats after 233088 examples: {'rewards_train/chosen': '0.19358', 'rewards_train/rejected': '0.028866', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16471', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-135.14', 'loss/train': '0.62168', 'examples_per_second': '31.98', 'grad_norm': '19.25', 'counters/examples': 233088, 'counters/updates': 7284}
train stats after 233120 examples: {'rewards_train/chosen': '0.14811', 'rewards_train/rejected': '0.030314', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1178', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-98.873', 'loss/train': '0.64195', 'examples_per_second': '31.967', 'grad_norm': '17.875', 'counters/examples': 233120, 'counters/updates': 7285}
train stats after 233152 examples: {'rewards_train/chosen': '0.23569', 'rewards_train/rejected': '-0.0027026', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2384', 'logps_train/rejected': '-98.847', 'logps_train/chosen': '-136.49', 'loss/train': '0.59529', 'examples_per_second': '30.223', 'grad_norm': '18', 'counters/examples': 233152, 'counters/updates': 7286}
train stats after 233184 examples: {'rewards_train/chosen': '0.16497', 'rewards_train/rejected': '0.078271', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086701', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-132.58', 'loss/train': '0.67119', 'examples_per_second': '31.917', 'grad_norm': '20.25', 'counters/examples': 233184, 'counters/updates': 7287}
train stats after 233216 examples: {'rewards_train/chosen': '0.2601', 'rewards_train/rejected': '-0.052705', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.3128', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-168.94', 'loss/train': '0.57634', 'examples_per_second': '32.122', 'grad_norm': '19.125', 'counters/examples': 233216, 'counters/updates': 7288}
train stats after 233248 examples: {'rewards_train/chosen': '0.095442', 'rewards_train/rejected': '0.055865', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039576', 'logps_train/rejected': '-95.791', 'logps_train/chosen': '-113.13', 'loss/train': '0.68371', 'examples_per_second': '31.631', 'grad_norm': '18.5', 'counters/examples': 233248, 'counters/updates': 7289}
train stats after 233280 examples: {'rewards_train/chosen': '0.19787', 'rewards_train/rejected': '0.13085', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067019', 'logps_train/rejected': '-106.27', 'logps_train/chosen': '-146.56', 'loss/train': '0.67529', 'examples_per_second': '31.675', 'grad_norm': '21.625', 'counters/examples': 233280, 'counters/updates': 7290}
train stats after 233312 examples: {'rewards_train/chosen': '0.17098', 'rewards_train/rejected': '-0.020695', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19167', 'logps_train/rejected': '-113.58', 'logps_train/chosen': '-113.96', 'loss/train': '0.61981', 'examples_per_second': '31.339', 'grad_norm': '17.625', 'counters/examples': 233312, 'counters/updates': 7291}
train stats after 233344 examples: {'rewards_train/chosen': '0.19795', 'rewards_train/rejected': '0.07558', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12237', 'logps_train/rejected': '-151.65', 'logps_train/chosen': '-132.25', 'loss/train': '0.6475', 'examples_per_second': '31.395', 'grad_norm': '22.25', 'counters/examples': 233344, 'counters/updates': 7292}
train stats after 233376 examples: {'rewards_train/chosen': '0.16482', 'rewards_train/rejected': '0.092997', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071821', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-141.61', 'loss/train': '0.6775', 'examples_per_second': '31.778', 'grad_norm': '21', 'counters/examples': 233376, 'counters/updates': 7293}
train stats after 233408 examples: {'rewards_train/chosen': '0.20005', 'rewards_train/rejected': '0.10048', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099564', 'logps_train/rejected': '-130.21', 'logps_train/chosen': '-142.35', 'loss/train': '0.65891', 'examples_per_second': '31.62', 'grad_norm': '20.375', 'counters/examples': 233408, 'counters/updates': 7294}
train stats after 233440 examples: {'rewards_train/chosen': '0.21022', 'rewards_train/rejected': '0.071152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13907', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-148.71', 'loss/train': '0.64241', 'examples_per_second': '30.739', 'grad_norm': '20.125', 'counters/examples': 233440, 'counters/updates': 7295}
train stats after 233472 examples: {'rewards_train/chosen': '0.16661', 'rewards_train/rejected': '0.040363', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12624', 'logps_train/rejected': '-79.251', 'logps_train/chosen': '-124.95', 'loss/train': '0.64597', 'examples_per_second': '32.892', 'grad_norm': '17.375', 'counters/examples': 233472, 'counters/updates': 7296}
train stats after 233504 examples: {'rewards_train/chosen': '0.1644', 'rewards_train/rejected': '0.056142', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10826', 'logps_train/rejected': '-148.45', 'logps_train/chosen': '-113.08', 'loss/train': '0.6644', 'examples_per_second': '31.603', 'grad_norm': '21.875', 'counters/examples': 233504, 'counters/updates': 7297}
skipping logging after 233536 examples to avoid logging too frequently
train stats after 233568 examples: {'rewards_train/chosen': '0.15395', 'rewards_train/rejected': '0.081113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072833', 'logps_train/rejected': '-109.59', 'logps_train/chosen': '-117.61', 'loss/train': '0.67407', 'examples_per_second': '33.749', 'grad_norm': '20.875', 'counters/examples': 233568, 'counters/updates': 7299}
train stats after 233600 examples: {'rewards_train/chosen': '0.12357', 'rewards_train/rejected': '0.023299', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10027', 'logps_train/rejected': '-140.84', 'logps_train/chosen': '-133.92', 'loss/train': '0.66424', 'examples_per_second': '32.539', 'grad_norm': '21.125', 'counters/examples': 233600, 'counters/updates': 7300}
train stats after 233632 examples: {'rewards_train/chosen': '0.28786', 'rewards_train/rejected': '0.017909', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.26995', 'logps_train/rejected': '-118.12', 'logps_train/chosen': '-119.24', 'loss/train': '0.582', 'examples_per_second': '30.462', 'grad_norm': '17.5', 'counters/examples': 233632, 'counters/updates': 7301}
train stats after 233664 examples: {'rewards_train/chosen': '0.13583', 'rewards_train/rejected': '-0.097525', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23336', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-136.43', 'loss/train': '0.60057', 'examples_per_second': '32.138', 'grad_norm': '18.25', 'counters/examples': 233664, 'counters/updates': 7302}
train stats after 233696 examples: {'rewards_train/chosen': '0.20101', 'rewards_train/rejected': '0.029745', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17126', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-132', 'loss/train': '0.62832', 'examples_per_second': '30.17', 'grad_norm': '18.625', 'counters/examples': 233696, 'counters/updates': 7303}
train stats after 233728 examples: {'rewards_train/chosen': '0.086707', 'rewards_train/rejected': '0.11137', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024659', 'logps_train/rejected': '-161.21', 'logps_train/chosen': '-163.03', 'loss/train': '0.72259', 'examples_per_second': '31.627', 'grad_norm': '23.75', 'counters/examples': 233728, 'counters/updates': 7304}
skipping logging after 233760 examples to avoid logging too frequently
train stats after 233792 examples: {'rewards_train/chosen': '0.19824', 'rewards_train/rejected': '0.070041', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1282', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-149.29', 'loss/train': '0.65169', 'examples_per_second': '31.306', 'grad_norm': '20.25', 'counters/examples': 233792, 'counters/updates': 7306}
train stats after 233824 examples: {'rewards_train/chosen': '0.21808', 'rewards_train/rejected': '0.056068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16201', 'logps_train/rejected': '-147.84', 'logps_train/chosen': '-151.58', 'loss/train': '0.62728', 'examples_per_second': '30.819', 'grad_norm': '20.875', 'counters/examples': 233824, 'counters/updates': 7307}
train stats after 233856 examples: {'rewards_train/chosen': '0.20496', 'rewards_train/rejected': '-0.0014695', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20643', 'logps_train/rejected': '-104.12', 'logps_train/chosen': '-133.92', 'loss/train': '0.60621', 'examples_per_second': '30.95', 'grad_norm': '18.875', 'counters/examples': 233856, 'counters/updates': 7308}
train stats after 233888 examples: {'rewards_train/chosen': '0.25261', 'rewards_train/rejected': '-0.046451', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.29906', 'logps_train/rejected': '-104.67', 'logps_train/chosen': '-135.41', 'loss/train': '0.57685', 'examples_per_second': '32.605', 'grad_norm': '17.25', 'counters/examples': 233888, 'counters/updates': 7309}
train stats after 233920 examples: {'rewards_train/chosen': '0.2815', 'rewards_train/rejected': '0.13088', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15063', 'logps_train/rejected': '-157.3', 'logps_train/chosen': '-174.67', 'loss/train': '0.659', 'examples_per_second': '33.082', 'grad_norm': '24.25', 'counters/examples': 233920, 'counters/updates': 7310}
train stats after 233952 examples: {'rewards_train/chosen': '0.17225', 'rewards_train/rejected': '-0.0096424', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1819', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-138.12', 'loss/train': '0.62035', 'examples_per_second': '31.576', 'grad_norm': '21.625', 'counters/examples': 233952, 'counters/updates': 7311}
train stats after 233984 examples: {'rewards_train/chosen': '0.17417', 'rewards_train/rejected': '0.0086427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16553', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-143.85', 'loss/train': '0.6241', 'examples_per_second': '32.417', 'grad_norm': '18.75', 'counters/examples': 233984, 'counters/updates': 7312}
train stats after 234016 examples: {'rewards_train/chosen': '0.19121', 'rewards_train/rejected': '0.079345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11186', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-171.5', 'loss/train': '0.66044', 'examples_per_second': '31.157', 'grad_norm': '22.25', 'counters/examples': 234016, 'counters/updates': 7313}
train stats after 234048 examples: {'rewards_train/chosen': '0.24146', 'rewards_train/rejected': '0.022593', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21886', 'logps_train/rejected': '-154.62', 'logps_train/chosen': '-147.07', 'loss/train': '0.60698', 'examples_per_second': '31.352', 'grad_norm': '20.625', 'counters/examples': 234048, 'counters/updates': 7314}
train stats after 234080 examples: {'rewards_train/chosen': '0.1393', 'rewards_train/rejected': '0.083476', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055821', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-171.07', 'loss/train': '0.6837', 'examples_per_second': '30.036', 'grad_norm': '23.625', 'counters/examples': 234080, 'counters/updates': 7315}
train stats after 234112 examples: {'rewards_train/chosen': '0.2953', 'rewards_train/rejected': '0.11432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18098', 'logps_train/rejected': '-123.63', 'logps_train/chosen': '-164.06', 'loss/train': '0.63774', 'examples_per_second': '30.53', 'grad_norm': '22.25', 'counters/examples': 234112, 'counters/updates': 7316}
train stats after 234144 examples: {'rewards_train/chosen': '0.2004', 'rewards_train/rejected': '-0.034786', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23518', 'logps_train/rejected': '-96.348', 'logps_train/chosen': '-150', 'loss/train': '0.59907', 'examples_per_second': '31.744', 'grad_norm': '18', 'counters/examples': 234144, 'counters/updates': 7317}
train stats after 234176 examples: {'rewards_train/chosen': '0.21359', 'rewards_train/rejected': '0.02419', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1894', 'logps_train/rejected': '-128.55', 'logps_train/chosen': '-143.22', 'loss/train': '0.61545', 'examples_per_second': '31.989', 'grad_norm': '19.75', 'counters/examples': 234176, 'counters/updates': 7318}
train stats after 234208 examples: {'rewards_train/chosen': '0.18183', 'rewards_train/rejected': '0.030434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1514', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-117.98', 'loss/train': '0.63895', 'examples_per_second': '30.881', 'grad_norm': '18.25', 'counters/examples': 234208, 'counters/updates': 7319}
skipping logging after 234240 examples to avoid logging too frequently
train stats after 234272 examples: {'rewards_train/chosen': '0.17384', 'rewards_train/rejected': '0.1207', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053144', 'logps_train/rejected': '-112.07', 'logps_train/chosen': '-109', 'loss/train': '0.67298', 'examples_per_second': '31.606', 'grad_norm': '19.25', 'counters/examples': 234272, 'counters/updates': 7321}
skipping logging after 234304 examples to avoid logging too frequently
train stats after 234336 examples: {'rewards_train/chosen': '0.21214', 'rewards_train/rejected': '-0.013096', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22523', 'logps_train/rejected': '-97.322', 'logps_train/chosen': '-149.47', 'loss/train': '0.60189', 'examples_per_second': '25.186', 'grad_norm': '18.25', 'counters/examples': 234336, 'counters/updates': 7323}
train stats after 234368 examples: {'rewards_train/chosen': '0.17722', 'rewards_train/rejected': '0.069545', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10767', 'logps_train/rejected': '-106.91', 'logps_train/chosen': '-151.1', 'loss/train': '0.64667', 'examples_per_second': '30.676', 'grad_norm': '20.125', 'counters/examples': 234368, 'counters/updates': 7324}
skipping logging after 234400 examples to avoid logging too frequently
train stats after 234432 examples: {'rewards_train/chosen': '0.12135', 'rewards_train/rejected': '0.099011', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022337', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-135.04', 'loss/train': '0.69664', 'examples_per_second': '36.142', 'grad_norm': '21.75', 'counters/examples': 234432, 'counters/updates': 7326}
train stats after 234464 examples: {'rewards_train/chosen': '0.18548', 'rewards_train/rejected': '0.04395', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14153', 'logps_train/rejected': '-138.53', 'logps_train/chosen': '-125.55', 'loss/train': '0.64849', 'examples_per_second': '31.559', 'grad_norm': '21.375', 'counters/examples': 234464, 'counters/updates': 7327}
train stats after 234496 examples: {'rewards_train/chosen': '0.19549', 'rewards_train/rejected': '-0.028145', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22363', 'logps_train/rejected': '-135.16', 'logps_train/chosen': '-162.32', 'loss/train': '0.60144', 'examples_per_second': '31.573', 'grad_norm': '20', 'counters/examples': 234496, 'counters/updates': 7328}
train stats after 234528 examples: {'rewards_train/chosen': '0.19046', 'rewards_train/rejected': '0.075739', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11472', 'logps_train/rejected': '-162.53', 'logps_train/chosen': '-157.98', 'loss/train': '0.65616', 'examples_per_second': '30.247', 'grad_norm': '23', 'counters/examples': 234528, 'counters/updates': 7329}
train stats after 234560 examples: {'rewards_train/chosen': '0.060195', 'rewards_train/rejected': '-0.0054104', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065605', 'logps_train/rejected': '-151.52', 'logps_train/chosen': '-137.49', 'loss/train': '0.67598', 'examples_per_second': '31.6', 'grad_norm': '21.75', 'counters/examples': 234560, 'counters/updates': 7330}
train stats after 234592 examples: {'rewards_train/chosen': '0.17037', 'rewards_train/rejected': '0.090614', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079761', 'logps_train/rejected': '-122.33', 'logps_train/chosen': '-142.98', 'loss/train': '0.6704', 'examples_per_second': '30.411', 'grad_norm': '21.375', 'counters/examples': 234592, 'counters/updates': 7331}
train stats after 234624 examples: {'rewards_train/chosen': '0.20942', 'rewards_train/rejected': '0.057553', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15186', 'logps_train/rejected': '-135.09', 'logps_train/chosen': '-130.74', 'loss/train': '0.64542', 'examples_per_second': '31.583', 'grad_norm': '20.75', 'counters/examples': 234624, 'counters/updates': 7332}
train stats after 234656 examples: {'rewards_train/chosen': '0.1101', 'rewards_train/rejected': '0.023444', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08666', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-151.98', 'loss/train': '0.66108', 'examples_per_second': '30.887', 'grad_norm': '21', 'counters/examples': 234656, 'counters/updates': 7333}
train stats after 234688 examples: {'rewards_train/chosen': '0.2689', 'rewards_train/rejected': '0.11948', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14942', 'logps_train/rejected': '-122.18', 'logps_train/chosen': '-171.14', 'loss/train': '0.63591', 'examples_per_second': '31.272', 'grad_norm': '20.375', 'counters/examples': 234688, 'counters/updates': 7334}
train stats after 234720 examples: {'rewards_train/chosen': '0.065666', 'rewards_train/rejected': '-0.14145', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20711', 'logps_train/rejected': '-102.81', 'logps_train/chosen': '-130.94', 'loss/train': '0.62053', 'examples_per_second': '30.482', 'grad_norm': '23.5', 'counters/examples': 234720, 'counters/updates': 7335}
skipping logging after 234752 examples to avoid logging too frequently
train stats after 234784 examples: {'rewards_train/chosen': '0.2481', 'rewards_train/rejected': '0.021088', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22701', 'logps_train/rejected': '-103.74', 'logps_train/chosen': '-160.11', 'loss/train': '0.59849', 'examples_per_second': '31.005', 'grad_norm': '19.5', 'counters/examples': 234784, 'counters/updates': 7337}
train stats after 234816 examples: {'rewards_train/chosen': '0.14567', 'rewards_train/rejected': '0.12946', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016217', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-140.26', 'loss/train': '0.70744', 'examples_per_second': '30.226', 'grad_norm': '21.25', 'counters/examples': 234816, 'counters/updates': 7338}
train stats after 234848 examples: {'rewards_train/chosen': '0.013244', 'rewards_train/rejected': '-0.014267', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027511', 'logps_train/rejected': '-96.583', 'logps_train/chosen': '-113.74', 'loss/train': '0.6909', 'examples_per_second': '31.613', 'grad_norm': '18.75', 'counters/examples': 234848, 'counters/updates': 7339}
train stats after 234880 examples: {'rewards_train/chosen': '0.20142', 'rewards_train/rejected': '0.058877', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14255', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-152.19', 'loss/train': '0.65189', 'examples_per_second': '30.624', 'grad_norm': '22.125', 'counters/examples': 234880, 'counters/updates': 7340}
train stats after 234912 examples: {'rewards_train/chosen': '0.12037', 'rewards_train/rejected': '-0.040031', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1604', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-146.42', 'loss/train': '0.63173', 'examples_per_second': '31.585', 'grad_norm': '19.875', 'counters/examples': 234912, 'counters/updates': 7341}
train stats after 234944 examples: {'rewards_train/chosen': '0.21507', 'rewards_train/rejected': '0.058822', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15624', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-149.15', 'loss/train': '0.63122', 'examples_per_second': '31.587', 'grad_norm': '20.75', 'counters/examples': 234944, 'counters/updates': 7342}
train stats after 234976 examples: {'rewards_train/chosen': '0.23844', 'rewards_train/rejected': '0.072584', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16586', 'logps_train/rejected': '-96.346', 'logps_train/chosen': '-124.32', 'loss/train': '0.63194', 'examples_per_second': '31.498', 'grad_norm': '18', 'counters/examples': 234976, 'counters/updates': 7343}
train stats after 235008 examples: {'rewards_train/chosen': '0.17211', 'rewards_train/rejected': '-0.0033223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17543', 'logps_train/rejected': '-117.23', 'logps_train/chosen': '-132.51', 'loss/train': '0.62536', 'examples_per_second': '32.469', 'grad_norm': '18.375', 'counters/examples': 235008, 'counters/updates': 7344}
train stats after 235040 examples: {'rewards_train/chosen': '0.17107', 'rewards_train/rejected': '-0.01532', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18639', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-135.35', 'loss/train': '0.62575', 'examples_per_second': '32.508', 'grad_norm': '20.25', 'counters/examples': 235040, 'counters/updates': 7345}
skipping logging after 235072 examples to avoid logging too frequently
train stats after 235104 examples: {'rewards_train/chosen': '0.11841', 'rewards_train/rejected': '-0.055697', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17411', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-133.44', 'loss/train': '0.63208', 'examples_per_second': '31.252', 'grad_norm': '19', 'counters/examples': 235104, 'counters/updates': 7347}
train stats after 235136 examples: {'rewards_train/chosen': '0.16761', 'rewards_train/rejected': '0.085131', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.082479', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-134.58', 'loss/train': '0.66747', 'examples_per_second': '33.157', 'grad_norm': '20.125', 'counters/examples': 235136, 'counters/updates': 7348}
train stats after 235168 examples: {'rewards_train/chosen': '0.20902', 'rewards_train/rejected': '0.12919', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079833', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-163.46', 'loss/train': '0.66552', 'examples_per_second': '23.67', 'grad_norm': '22.25', 'counters/examples': 235168, 'counters/updates': 7349}
train stats after 235200 examples: {'rewards_train/chosen': '0.090202', 'rewards_train/rejected': '-0.013855', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10406', 'logps_train/rejected': '-119.34', 'logps_train/chosen': '-131.08', 'loss/train': '0.66404', 'examples_per_second': '31.523', 'grad_norm': '20.125', 'counters/examples': 235200, 'counters/updates': 7350}
train stats after 235232 examples: {'rewards_train/chosen': '0.24354', 'rewards_train/rejected': '0.039051', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20449', 'logps_train/rejected': '-139.39', 'logps_train/chosen': '-167.35', 'loss/train': '0.62049', 'examples_per_second': '32.12', 'grad_norm': '21.75', 'counters/examples': 235232, 'counters/updates': 7351}
train stats after 235264 examples: {'rewards_train/chosen': '0.16499', 'rewards_train/rejected': '-0.0049795', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16997', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-117.23', 'loss/train': '0.62656', 'examples_per_second': '23.743', 'grad_norm': '18.875', 'counters/examples': 235264, 'counters/updates': 7352}
skipping logging after 235296 examples to avoid logging too frequently
train stats after 235328 examples: {'rewards_train/chosen': '0.13748', 'rewards_train/rejected': '-0.083007', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22049', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-156.24', 'loss/train': '0.60689', 'examples_per_second': '30.461', 'grad_norm': '18.875', 'counters/examples': 235328, 'counters/updates': 7354}
train stats after 235360 examples: {'rewards_train/chosen': '0.21831', 'rewards_train/rejected': '0.084795', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13351', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-102.13', 'loss/train': '0.64897', 'examples_per_second': '32.838', 'grad_norm': '16.875', 'counters/examples': 235360, 'counters/updates': 7355}
train stats after 235392 examples: {'rewards_train/chosen': '0.15414', 'rewards_train/rejected': '-0.061117', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21526', 'logps_train/rejected': '-113.57', 'logps_train/chosen': '-103.04', 'loss/train': '0.61098', 'examples_per_second': '31.558', 'grad_norm': '17.5', 'counters/examples': 235392, 'counters/updates': 7356}
train stats after 235424 examples: {'rewards_train/chosen': '0.2431', 'rewards_train/rejected': '0.087994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15511', 'logps_train/rejected': '-108.33', 'logps_train/chosen': '-141.48', 'loss/train': '0.63915', 'examples_per_second': '32.266', 'grad_norm': '20.25', 'counters/examples': 235424, 'counters/updates': 7357}
train stats after 235456 examples: {'rewards_train/chosen': '0.21954', 'rewards_train/rejected': '0.063238', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15631', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-141.84', 'loss/train': '0.63388', 'examples_per_second': '31.394', 'grad_norm': '20.25', 'counters/examples': 235456, 'counters/updates': 7358}
train stats after 235488 examples: {'rewards_train/chosen': '0.1474', 'rewards_train/rejected': '-0.0040608', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15146', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-148.24', 'loss/train': '0.63223', 'examples_per_second': '31.26', 'grad_norm': '20.25', 'counters/examples': 235488, 'counters/updates': 7359}
train stats after 235520 examples: {'rewards_train/chosen': '0.16934', 'rewards_train/rejected': '0.15413', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015208', 'logps_train/rejected': '-132.99', 'logps_train/chosen': '-137.64', 'loss/train': '0.69702', 'examples_per_second': '31.559', 'grad_norm': '23.375', 'counters/examples': 235520, 'counters/updates': 7360}
train stats after 235552 examples: {'rewards_train/chosen': '0.14583', 'rewards_train/rejected': '-0.013129', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15896', 'logps_train/rejected': '-84.758', 'logps_train/chosen': '-145.29', 'loss/train': '0.63183', 'examples_per_second': '30.46', 'grad_norm': '18.25', 'counters/examples': 235552, 'counters/updates': 7361}
skipping logging after 235584 examples to avoid logging too frequently
train stats after 235616 examples: {'rewards_train/chosen': '0.17016', 'rewards_train/rejected': '0.0086001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16156', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-128.46', 'loss/train': '0.63782', 'examples_per_second': '30.835', 'grad_norm': '20', 'counters/examples': 235616, 'counters/updates': 7363}
train stats after 235648 examples: {'rewards_train/chosen': '0.13337', 'rewards_train/rejected': '0.015523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11785', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-113.55', 'loss/train': '0.65047', 'examples_per_second': '31.587', 'grad_norm': '19.125', 'counters/examples': 235648, 'counters/updates': 7364}
skipping logging after 235680 examples to avoid logging too frequently
train stats after 235712 examples: {'rewards_train/chosen': '0.18202', 'rewards_train/rejected': '0.0058352', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17619', 'logps_train/rejected': '-148.42', 'logps_train/chosen': '-161.23', 'loss/train': '0.64774', 'examples_per_second': '30.624', 'grad_norm': '21.75', 'counters/examples': 235712, 'counters/updates': 7366}
train stats after 235744 examples: {'rewards_train/chosen': '0.17987', 'rewards_train/rejected': '0.027753', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15212', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-120.52', 'loss/train': '0.64201', 'examples_per_second': '32.464', 'grad_norm': '19.625', 'counters/examples': 235744, 'counters/updates': 7367}
skipping logging after 235776 examples to avoid logging too frequently
train stats after 235808 examples: {'rewards_train/chosen': '0.10598', 'rewards_train/rejected': '-0.032402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13838', 'logps_train/rejected': '-93.455', 'logps_train/chosen': '-130.32', 'loss/train': '0.64176', 'examples_per_second': '30.711', 'grad_norm': '17.5', 'counters/examples': 235808, 'counters/updates': 7369}
train stats after 235840 examples: {'rewards_train/chosen': '0.1746', 'rewards_train/rejected': '0.086539', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.088065', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-156.92', 'loss/train': '0.65791', 'examples_per_second': '32.187', 'grad_norm': '20.25', 'counters/examples': 235840, 'counters/updates': 7370}
train stats after 235872 examples: {'rewards_train/chosen': '0.27429', 'rewards_train/rejected': '0.066577', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20771', 'logps_train/rejected': '-141.9', 'logps_train/chosen': '-150.77', 'loss/train': '0.61428', 'examples_per_second': '32.047', 'grad_norm': '19.875', 'counters/examples': 235872, 'counters/updates': 7371}
skipping logging after 235904 examples to avoid logging too frequently
train stats after 235936 examples: {'rewards_train/chosen': '0.20442', 'rewards_train/rejected': '0.11107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093353', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-159.56', 'loss/train': '0.67876', 'examples_per_second': '33.149', 'grad_norm': '22.25', 'counters/examples': 235936, 'counters/updates': 7373}
skipping logging after 235968 examples to avoid logging too frequently
train stats after 236000 examples: {'rewards_train/chosen': '0.088699', 'rewards_train/rejected': '0.038712', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049987', 'logps_train/rejected': '-85.215', 'logps_train/chosen': '-106.17', 'loss/train': '0.68906', 'examples_per_second': '30.515', 'grad_norm': '18.75', 'counters/examples': 236000, 'counters/updates': 7375}
skipping logging after 236032 examples to avoid logging too frequently
train stats after 236064 examples: {'rewards_train/chosen': '0.1669', 'rewards_train/rejected': '0.046827', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12007', 'logps_train/rejected': '-94.126', 'logps_train/chosen': '-143.11', 'loss/train': '0.65104', 'examples_per_second': '30.793', 'grad_norm': '20.25', 'counters/examples': 236064, 'counters/updates': 7377}
train stats after 236096 examples: {'rewards_train/chosen': '0.16641', 'rewards_train/rejected': '-0.052657', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21906', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-120.18', 'loss/train': '0.60928', 'examples_per_second': '32.44', 'grad_norm': '18.25', 'counters/examples': 236096, 'counters/updates': 7378}
train stats after 236128 examples: {'rewards_train/chosen': '0.1473', 'rewards_train/rejected': '0.037327', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10997', 'logps_train/rejected': '-108.35', 'logps_train/chosen': '-186.71', 'loss/train': '0.65165', 'examples_per_second': '31.334', 'grad_norm': '21.75', 'counters/examples': 236128, 'counters/updates': 7379}
skipping logging after 236160 examples to avoid logging too frequently
train stats after 236192 examples: {'rewards_train/chosen': '0.20229', 'rewards_train/rejected': '0.049139', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15316', 'logps_train/rejected': '-109.51', 'logps_train/chosen': '-155.37', 'loss/train': '0.62925', 'examples_per_second': '31.484', 'grad_norm': '19.125', 'counters/examples': 236192, 'counters/updates': 7381}
skipping logging after 236224 examples to avoid logging too frequently
train stats after 236256 examples: {'rewards_train/chosen': '0.19897', 'rewards_train/rejected': '0.18616', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01281', 'logps_train/rejected': '-143.21', 'logps_train/chosen': '-124.39', 'loss/train': '0.70571', 'examples_per_second': '29.54', 'grad_norm': '22.375', 'counters/examples': 236256, 'counters/updates': 7383}
train stats after 236288 examples: {'rewards_train/chosen': '0.23459', 'rewards_train/rejected': '-0.014905', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24949', 'logps_train/rejected': '-107.18', 'logps_train/chosen': '-123.94', 'loss/train': '0.59582', 'examples_per_second': '30.464', 'grad_norm': '17.5', 'counters/examples': 236288, 'counters/updates': 7384}
train stats after 236320 examples: {'rewards_train/chosen': '0.19773', 'rewards_train/rejected': '0.11083', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086894', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-139.82', 'loss/train': '0.66553', 'examples_per_second': '30.313', 'grad_norm': '20.875', 'counters/examples': 236320, 'counters/updates': 7385}
train stats after 236352 examples: {'rewards_train/chosen': '0.033656', 'rewards_train/rejected': '0.020938', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012719', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-116.26', 'loss/train': '0.70509', 'examples_per_second': '32.587', 'grad_norm': '19.875', 'counters/examples': 236352, 'counters/updates': 7386}
train stats after 236384 examples: {'rewards_train/chosen': '0.090191', 'rewards_train/rejected': '-0.068498', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15869', 'logps_train/rejected': '-144.66', 'logps_train/chosen': '-138.9', 'loss/train': '0.62999', 'examples_per_second': '31.311', 'grad_norm': '20.75', 'counters/examples': 236384, 'counters/updates': 7387}
train stats after 236416 examples: {'rewards_train/chosen': '0.18677', 'rewards_train/rejected': '0.046236', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14053', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-120.52', 'loss/train': '0.64321', 'examples_per_second': '32.072', 'grad_norm': '19.75', 'counters/examples': 236416, 'counters/updates': 7388}
train stats after 236448 examples: {'rewards_train/chosen': '0.11827', 'rewards_train/rejected': '0.08081', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037463', 'logps_train/rejected': '-137.74', 'logps_train/chosen': '-155.82', 'loss/train': '0.69005', 'examples_per_second': '31.503', 'grad_norm': '22.25', 'counters/examples': 236448, 'counters/updates': 7389}
train stats after 236480 examples: {'rewards_train/chosen': '0.21438', 'rewards_train/rejected': '0.12678', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0876', 'logps_train/rejected': '-145.48', 'logps_train/chosen': '-152.48', 'loss/train': '0.66369', 'examples_per_second': '31.512', 'grad_norm': '22.875', 'counters/examples': 236480, 'counters/updates': 7390}
train stats after 236512 examples: {'rewards_train/chosen': '0.17571', 'rewards_train/rejected': '0.10066', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.07505', 'logps_train/rejected': '-92.132', 'logps_train/chosen': '-146.59', 'loss/train': '0.67164', 'examples_per_second': '30.089', 'grad_norm': '19.125', 'counters/examples': 236512, 'counters/updates': 7391}
train stats after 236544 examples: {'rewards_train/chosen': '0.13243', 'rewards_train/rejected': '0.01814', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11429', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-161.26', 'loss/train': '0.65206', 'examples_per_second': '31.07', 'grad_norm': '22.375', 'counters/examples': 236544, 'counters/updates': 7392}
train stats after 236576 examples: {'rewards_train/chosen': '0.033566', 'rewards_train/rejected': '0.028889', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0046772', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-153.53', 'loss/train': '0.70377', 'examples_per_second': '30.445', 'grad_norm': '21.625', 'counters/examples': 236576, 'counters/updates': 7393}
train stats after 236608 examples: {'rewards_train/chosen': '0.21674', 'rewards_train/rejected': '0.070671', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14607', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-147.53', 'loss/train': '0.63907', 'examples_per_second': '31.511', 'grad_norm': '21.625', 'counters/examples': 236608, 'counters/updates': 7394}
train stats after 236640 examples: {'rewards_train/chosen': '0.20612', 'rewards_train/rejected': '-0.059699', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26582', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-126.88', 'loss/train': '0.60054', 'examples_per_second': '32.386', 'grad_norm': '18.5', 'counters/examples': 236640, 'counters/updates': 7395}
skipping logging after 236672 examples to avoid logging too frequently
train stats after 236704 examples: {'rewards_train/chosen': '0.13324', 'rewards_train/rejected': '0.087702', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045538', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-121.1', 'loss/train': '0.6846', 'examples_per_second': '31.506', 'grad_norm': '20.125', 'counters/examples': 236704, 'counters/updates': 7397}
skipping logging after 236736 examples to avoid logging too frequently
train stats after 236768 examples: {'rewards_train/chosen': '0.14033', 'rewards_train/rejected': '0.032831', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1075', 'logps_train/rejected': '-102.57', 'logps_train/chosen': '-139.95', 'loss/train': '0.65358', 'examples_per_second': '31.456', 'grad_norm': '20', 'counters/examples': 236768, 'counters/updates': 7399}
skipping logging after 236800 examples to avoid logging too frequently
train stats after 236832 examples: {'rewards_train/chosen': '0.16145', 'rewards_train/rejected': '0.094011', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067435', 'logps_train/rejected': '-115.63', 'logps_train/chosen': '-134.79', 'loss/train': '0.67728', 'examples_per_second': '31.642', 'grad_norm': '20.125', 'counters/examples': 236832, 'counters/updates': 7401}
train stats after 236864 examples: {'rewards_train/chosen': '0.17744', 'rewards_train/rejected': '0.058901', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11854', 'logps_train/rejected': '-83.208', 'logps_train/chosen': '-145.92', 'loss/train': '0.66849', 'examples_per_second': '32.355', 'grad_norm': '19.875', 'counters/examples': 236864, 'counters/updates': 7402}
skipping logging after 236896 examples to avoid logging too frequently
train stats after 236928 examples: {'rewards_train/chosen': '0.17093', 'rewards_train/rejected': '0.043293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12764', 'logps_train/rejected': '-118.47', 'logps_train/chosen': '-130.65', 'loss/train': '0.64515', 'examples_per_second': '30.13', 'grad_norm': '19.625', 'counters/examples': 236928, 'counters/updates': 7404}
train stats after 236960 examples: {'rewards_train/chosen': '0.19167', 'rewards_train/rejected': '0.04836', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14331', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-132.69', 'loss/train': '0.63981', 'examples_per_second': '30.595', 'grad_norm': '19.25', 'counters/examples': 236960, 'counters/updates': 7405}
train stats after 236992 examples: {'rewards_train/chosen': '0.014164', 'rewards_train/rejected': '0.054129', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.039965', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-130.88', 'loss/train': '0.72512', 'examples_per_second': '32.817', 'grad_norm': '20.875', 'counters/examples': 236992, 'counters/updates': 7406}
train stats after 237024 examples: {'rewards_train/chosen': '0.10612', 'rewards_train/rejected': '0.020756', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085363', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-174.53', 'loss/train': '0.6697', 'examples_per_second': '31.896', 'grad_norm': '23', 'counters/examples': 237024, 'counters/updates': 7407}
train stats after 237056 examples: {'rewards_train/chosen': '0.18533', 'rewards_train/rejected': '0.0099974', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17533', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-144.26', 'loss/train': '0.63593', 'examples_per_second': '31.919', 'grad_norm': '23', 'counters/examples': 237056, 'counters/updates': 7408}
train stats after 237088 examples: {'rewards_train/chosen': '0.13467', 'rewards_train/rejected': '0.014187', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12048', 'logps_train/rejected': '-108.77', 'logps_train/chosen': '-141.52', 'loss/train': '0.65352', 'examples_per_second': '32.642', 'grad_norm': '19.375', 'counters/examples': 237088, 'counters/updates': 7409}
skipping logging after 237120 examples to avoid logging too frequently
train stats after 237152 examples: {'rewards_train/chosen': '0.16484', 'rewards_train/rejected': '0.08025', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084587', 'logps_train/rejected': '-119.98', 'logps_train/chosen': '-117.64', 'loss/train': '0.66882', 'examples_per_second': '31.328', 'grad_norm': '21.875', 'counters/examples': 237152, 'counters/updates': 7411}
train stats after 237184 examples: {'rewards_train/chosen': '0.14994', 'rewards_train/rejected': '0.090669', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059271', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-126.36', 'loss/train': '0.68163', 'examples_per_second': '30.543', 'grad_norm': '20.125', 'counters/examples': 237184, 'counters/updates': 7412}
train stats after 237216 examples: {'rewards_train/chosen': '0.1692', 'rewards_train/rejected': '-0.012114', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18132', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-117.11', 'loss/train': '0.63021', 'examples_per_second': '30.628', 'grad_norm': '17.875', 'counters/examples': 237216, 'counters/updates': 7413}
train stats after 237248 examples: {'rewards_train/chosen': '0.23038', 'rewards_train/rejected': '0.026275', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20411', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-137.92', 'loss/train': '0.61136', 'examples_per_second': '33.342', 'grad_norm': '19.75', 'counters/examples': 237248, 'counters/updates': 7414}
train stats after 237280 examples: {'rewards_train/chosen': '0.12719', 'rewards_train/rejected': '-0.0015329', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12872', 'logps_train/rejected': '-88.934', 'logps_train/chosen': '-78.742', 'loss/train': '0.64113', 'examples_per_second': '30.345', 'grad_norm': '15.562', 'counters/examples': 237280, 'counters/updates': 7415}
train stats after 237312 examples: {'rewards_train/chosen': '0.19059', 'rewards_train/rejected': '-0.012031', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20263', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-156.81', 'loss/train': '0.6199', 'examples_per_second': '31.926', 'grad_norm': '21.125', 'counters/examples': 237312, 'counters/updates': 7416}
train stats after 237344 examples: {'rewards_train/chosen': '0.2709', 'rewards_train/rejected': '0.10995', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16095', 'logps_train/rejected': '-93.495', 'logps_train/chosen': '-134.46', 'loss/train': '0.62886', 'examples_per_second': '31.749', 'grad_norm': '17', 'counters/examples': 237344, 'counters/updates': 7417}
skipping logging after 237376 examples to avoid logging too frequently
train stats after 237408 examples: {'rewards_train/chosen': '0.19294', 'rewards_train/rejected': '-0.013843', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20679', 'logps_train/rejected': '-102.71', 'logps_train/chosen': '-129.95', 'loss/train': '0.61043', 'examples_per_second': '30.14', 'grad_norm': '17.75', 'counters/examples': 237408, 'counters/updates': 7419}
train stats after 237440 examples: {'rewards_train/chosen': '0.27449', 'rewards_train/rejected': '0.057012', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21748', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-145.43', 'loss/train': '0.61533', 'examples_per_second': '31.424', 'grad_norm': '20.125', 'counters/examples': 237440, 'counters/updates': 7420}
train stats after 237472 examples: {'rewards_train/chosen': '0.21773', 'rewards_train/rejected': '0.03327', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18446', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-167.66', 'loss/train': '0.62002', 'examples_per_second': '30.68', 'grad_norm': '21.25', 'counters/examples': 237472, 'counters/updates': 7421}
train stats after 237504 examples: {'rewards_train/chosen': '0.11329', 'rewards_train/rejected': '0.035264', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.078023', 'logps_train/rejected': '-92.798', 'logps_train/chosen': '-107.79', 'loss/train': '0.66925', 'examples_per_second': '32.62', 'grad_norm': '19.375', 'counters/examples': 237504, 'counters/updates': 7422}
train stats after 237536 examples: {'rewards_train/chosen': '0.1789', 'rewards_train/rejected': '-0.0084369', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18734', 'logps_train/rejected': '-99.52', 'logps_train/chosen': '-141.73', 'loss/train': '0.61387', 'examples_per_second': '31.091', 'grad_norm': '18.25', 'counters/examples': 237536, 'counters/updates': 7423}
train stats after 237568 examples: {'rewards_train/chosen': '0.20436', 'rewards_train/rejected': '-0.027425', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23178', 'logps_train/rejected': '-103.71', 'logps_train/chosen': '-119.52', 'loss/train': '0.60429', 'examples_per_second': '31.064', 'grad_norm': '18.125', 'counters/examples': 237568, 'counters/updates': 7424}
train stats after 237600 examples: {'rewards_train/chosen': '0.16259', 'rewards_train/rejected': '-0.014145', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17674', 'logps_train/rejected': '-103.41', 'logps_train/chosen': '-135.76', 'loss/train': '0.61897', 'examples_per_second': '30.08', 'grad_norm': '19.125', 'counters/examples': 237600, 'counters/updates': 7425}
train stats after 237632 examples: {'rewards_train/chosen': '0.16728', 'rewards_train/rejected': '0.014189', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15309', 'logps_train/rejected': '-109.51', 'logps_train/chosen': '-149.38', 'loss/train': '0.63285', 'examples_per_second': '30.224', 'grad_norm': '18.5', 'counters/examples': 237632, 'counters/updates': 7426}
skipping logging after 237664 examples to avoid logging too frequently
train stats after 237696 examples: {'rewards_train/chosen': '0.24029', 'rewards_train/rejected': '0.078137', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16215', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-109.8', 'loss/train': '0.63518', 'examples_per_second': '31.502', 'grad_norm': '18.625', 'counters/examples': 237696, 'counters/updates': 7428}
train stats after 237728 examples: {'rewards_train/chosen': '0.2239', 'rewards_train/rejected': '0.011534', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21237', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-98.657', 'loss/train': '0.60666', 'examples_per_second': '31.564', 'grad_norm': '17.375', 'counters/examples': 237728, 'counters/updates': 7429}
skipping logging after 237760 examples to avoid logging too frequently
train stats after 237792 examples: {'rewards_train/chosen': '0.18071', 'rewards_train/rejected': '0.0141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16661', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-141.23', 'loss/train': '0.64214', 'examples_per_second': '31.101', 'grad_norm': '20.375', 'counters/examples': 237792, 'counters/updates': 7431}
train stats after 237824 examples: {'rewards_train/chosen': '0.30637', 'rewards_train/rejected': '0.14631', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16005', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-137.99', 'loss/train': '0.63255', 'examples_per_second': '31.165', 'grad_norm': '19.625', 'counters/examples': 237824, 'counters/updates': 7432}
train stats after 237856 examples: {'rewards_train/chosen': '0.16831', 'rewards_train/rejected': '0.008772', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15954', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-149.01', 'loss/train': '0.63842', 'examples_per_second': '31.514', 'grad_norm': '20.625', 'counters/examples': 237856, 'counters/updates': 7433}
train stats after 237888 examples: {'rewards_train/chosen': '0.17966', 'rewards_train/rejected': '0.088492', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091166', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-153.61', 'loss/train': '0.67018', 'examples_per_second': '31.526', 'grad_norm': '21.375', 'counters/examples': 237888, 'counters/updates': 7434}
train stats after 237920 examples: {'rewards_train/chosen': '0.20686', 'rewards_train/rejected': '0.048754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15811', 'logps_train/rejected': '-114.22', 'logps_train/chosen': '-156.4', 'loss/train': '0.64035', 'examples_per_second': '31.766', 'grad_norm': '19.625', 'counters/examples': 237920, 'counters/updates': 7435}
skipping logging after 237952 examples to avoid logging too frequently
train stats after 237984 examples: {'rewards_train/chosen': '0.19327', 'rewards_train/rejected': '0.080998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11227', 'logps_train/rejected': '-146.4', 'logps_train/chosen': '-116.87', 'loss/train': '0.65464', 'examples_per_second': '33.101', 'grad_norm': '20.5', 'counters/examples': 237984, 'counters/updates': 7437}
skipping logging after 238016 examples to avoid logging too frequently
train stats after 238048 examples: {'rewards_train/chosen': '0.23134', 'rewards_train/rejected': '0.051182', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18016', 'logps_train/rejected': '-92.815', 'logps_train/chosen': '-97.714', 'loss/train': '0.62577', 'examples_per_second': '33.835', 'grad_norm': '15.75', 'counters/examples': 238048, 'counters/updates': 7439}
train stats after 238080 examples: {'rewards_train/chosen': '0.18114', 'rewards_train/rejected': '0.067766', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11337', 'logps_train/rejected': '-95.093', 'logps_train/chosen': '-140.53', 'loss/train': '0.65277', 'examples_per_second': '33.162', 'grad_norm': '19.125', 'counters/examples': 238080, 'counters/updates': 7440}
train stats after 238112 examples: {'rewards_train/chosen': '0.11916', 'rewards_train/rejected': '0.064255', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054903', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-135.75', 'loss/train': '0.68547', 'examples_per_second': '32.408', 'grad_norm': '21.875', 'counters/examples': 238112, 'counters/updates': 7441}
train stats after 238144 examples: {'rewards_train/chosen': '0.1403', 'rewards_train/rejected': '-0.0053414', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14564', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-139.98', 'loss/train': '0.6329', 'examples_per_second': '29.898', 'grad_norm': '21.25', 'counters/examples': 238144, 'counters/updates': 7442}
train stats after 238176 examples: {'rewards_train/chosen': '0.25742', 'rewards_train/rejected': '0.070565', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18685', 'logps_train/rejected': '-106.8', 'logps_train/chosen': '-151.72', 'loss/train': '0.61815', 'examples_per_second': '31.334', 'grad_norm': '20.75', 'counters/examples': 238176, 'counters/updates': 7443}
train stats after 238208 examples: {'rewards_train/chosen': '0.20207', 'rewards_train/rejected': '0.083164', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11891', 'logps_train/rejected': '-116.7', 'logps_train/chosen': '-122.24', 'loss/train': '0.65162', 'examples_per_second': '30.792', 'grad_norm': '20.5', 'counters/examples': 238208, 'counters/updates': 7444}
train stats after 238240 examples: {'rewards_train/chosen': '0.17849', 'rewards_train/rejected': '0.0041352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17436', 'logps_train/rejected': '-124.99', 'logps_train/chosen': '-175.08', 'loss/train': '0.63294', 'examples_per_second': '30.394', 'grad_norm': '21.625', 'counters/examples': 238240, 'counters/updates': 7445}
train stats after 238272 examples: {'rewards_train/chosen': '0.14686', 'rewards_train/rejected': '0.013134', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13373', 'logps_train/rejected': '-97.238', 'logps_train/chosen': '-150.67', 'loss/train': '0.64819', 'examples_per_second': '30.835', 'grad_norm': '19', 'counters/examples': 238272, 'counters/updates': 7446}
train stats after 238304 examples: {'rewards_train/chosen': '0.21952', 'rewards_train/rejected': '0.042723', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1768', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-143.39', 'loss/train': '0.62547', 'examples_per_second': '31.454', 'grad_norm': '19.125', 'counters/examples': 238304, 'counters/updates': 7447}
train stats after 238336 examples: {'rewards_train/chosen': '0.21692', 'rewards_train/rejected': '0.015457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20146', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-162.72', 'loss/train': '0.62021', 'examples_per_second': '31.97', 'grad_norm': '20.5', 'counters/examples': 238336, 'counters/updates': 7448}
train stats after 238368 examples: {'rewards_train/chosen': '0.24427', 'rewards_train/rejected': '0.003733', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24053', 'logps_train/rejected': '-146.39', 'logps_train/chosen': '-158.95', 'loss/train': '0.5985', 'examples_per_second': '31.928', 'grad_norm': '19.875', 'counters/examples': 238368, 'counters/updates': 7449}
train stats after 238400 examples: {'rewards_train/chosen': '0.1857', 'rewards_train/rejected': '0.24403', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.058339', 'logps_train/rejected': '-131.97', 'logps_train/chosen': '-101.55', 'loss/train': '0.73537', 'examples_per_second': '32.247', 'grad_norm': '21.125', 'counters/examples': 238400, 'counters/updates': 7450}
train stats after 238432 examples: {'rewards_train/chosen': '0.27496', 'rewards_train/rejected': '0.098943', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17602', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-151.86', 'loss/train': '0.61898', 'examples_per_second': '30.69', 'grad_norm': '20.75', 'counters/examples': 238432, 'counters/updates': 7451}
train stats after 238464 examples: {'rewards_train/chosen': '0.16591', 'rewards_train/rejected': '0.085546', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080361', 'logps_train/rejected': '-112.72', 'logps_train/chosen': '-116.68', 'loss/train': '0.66795', 'examples_per_second': '32.142', 'grad_norm': '19.5', 'counters/examples': 238464, 'counters/updates': 7452}
train stats after 238496 examples: {'rewards_train/chosen': '0.16918', 'rewards_train/rejected': '-0.085758', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25494', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-119.12', 'loss/train': '0.59589', 'examples_per_second': '31.487', 'grad_norm': '19', 'counters/examples': 238496, 'counters/updates': 7453}
train stats after 238528 examples: {'rewards_train/chosen': '0.21808', 'rewards_train/rejected': '0.0015717', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21651', 'logps_train/rejected': '-117.31', 'logps_train/chosen': '-120.59', 'loss/train': '0.61356', 'examples_per_second': '30.689', 'grad_norm': '18.375', 'counters/examples': 238528, 'counters/updates': 7454}
train stats after 238560 examples: {'rewards_train/chosen': '0.12948', 'rewards_train/rejected': '-0.058819', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1883', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-147.49', 'loss/train': '0.6217', 'examples_per_second': '31.52', 'grad_norm': '20.125', 'counters/examples': 238560, 'counters/updates': 7455}
train stats after 238592 examples: {'rewards_train/chosen': '0.18959', 'rewards_train/rejected': '-0.013348', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20294', 'logps_train/rejected': '-116.06', 'logps_train/chosen': '-128.64', 'loss/train': '0.61362', 'examples_per_second': '32.381', 'grad_norm': '18.375', 'counters/examples': 238592, 'counters/updates': 7456}
train stats after 238624 examples: {'rewards_train/chosen': '0.18123', 'rewards_train/rejected': '-8.7376e-05', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18131', 'logps_train/rejected': '-107.03', 'logps_train/chosen': '-142.52', 'loss/train': '0.62057', 'examples_per_second': '31.478', 'grad_norm': '19.75', 'counters/examples': 238624, 'counters/updates': 7457}
train stats after 238656 examples: {'rewards_train/chosen': '0.18052', 'rewards_train/rejected': '0.054738', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12578', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-149.93', 'loss/train': '0.65657', 'examples_per_second': '30.835', 'grad_norm': '20.25', 'counters/examples': 238656, 'counters/updates': 7458}
train stats after 238688 examples: {'rewards_train/chosen': '0.17037', 'rewards_train/rejected': '0.10603', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064333', 'logps_train/rejected': '-115.74', 'logps_train/chosen': '-114.56', 'loss/train': '0.67859', 'examples_per_second': '31.679', 'grad_norm': '19.375', 'counters/examples': 238688, 'counters/updates': 7459}
train stats after 238720 examples: {'rewards_train/chosen': '0.10714', 'rewards_train/rejected': '0.059389', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047754', 'logps_train/rejected': '-148.82', 'logps_train/chosen': '-127.37', 'loss/train': '0.68548', 'examples_per_second': '31.509', 'grad_norm': '23.5', 'counters/examples': 238720, 'counters/updates': 7460}
train stats after 238752 examples: {'rewards_train/chosen': '0.20264', 'rewards_train/rejected': '-0.02082', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22346', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-124.95', 'loss/train': '0.61209', 'examples_per_second': '31.479', 'grad_norm': '19.375', 'counters/examples': 238752, 'counters/updates': 7461}
train stats after 238784 examples: {'rewards_train/chosen': '0.093411', 'rewards_train/rejected': '-0.059109', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15252', 'logps_train/rejected': '-121.16', 'logps_train/chosen': '-146.89', 'loss/train': '0.6326', 'examples_per_second': '31.072', 'grad_norm': '19', 'counters/examples': 238784, 'counters/updates': 7462}
train stats after 238816 examples: {'rewards_train/chosen': '0.22543', 'rewards_train/rejected': '0.049349', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17608', 'logps_train/rejected': '-125.05', 'logps_train/chosen': '-139.63', 'loss/train': '0.62251', 'examples_per_second': '31.48', 'grad_norm': '20', 'counters/examples': 238816, 'counters/updates': 7463}
train stats after 238848 examples: {'rewards_train/chosen': '0.15066', 'rewards_train/rejected': '0.020182', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13048', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-157.56', 'loss/train': '0.64732', 'examples_per_second': '30.546', 'grad_norm': '21.875', 'counters/examples': 238848, 'counters/updates': 7464}
train stats after 238880 examples: {'rewards_train/chosen': '0.13192', 'rewards_train/rejected': '0.061781', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070141', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-105.8', 'loss/train': '0.66692', 'examples_per_second': '31.482', 'grad_norm': '19.75', 'counters/examples': 238880, 'counters/updates': 7465}
skipping logging after 238912 examples to avoid logging too frequently
train stats after 238944 examples: {'rewards_train/chosen': '0.13291', 'rewards_train/rejected': '-0.066617', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.19952', 'logps_train/rejected': '-98.914', 'logps_train/chosen': '-112.26', 'loss/train': '0.60621', 'examples_per_second': '30.582', 'grad_norm': '17.125', 'counters/examples': 238944, 'counters/updates': 7467}
train stats after 238976 examples: {'rewards_train/chosen': '0.29289', 'rewards_train/rejected': '0.045889', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.247', 'logps_train/rejected': '-128.01', 'logps_train/chosen': '-183.13', 'loss/train': '0.60455', 'examples_per_second': '31.494', 'grad_norm': '20.5', 'counters/examples': 238976, 'counters/updates': 7468}
train stats after 239008 examples: {'rewards_train/chosen': '0.013117', 'rewards_train/rejected': '0.062146', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.049028', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-144.45', 'loss/train': '0.73371', 'examples_per_second': '31.906', 'grad_norm': '21.875', 'counters/examples': 239008, 'counters/updates': 7469}
train stats after 239040 examples: {'rewards_train/chosen': '0.25573', 'rewards_train/rejected': '0.0022335', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2535', 'logps_train/rejected': '-67.737', 'logps_train/chosen': '-113.42', 'loss/train': '0.58979', 'examples_per_second': '31.826', 'grad_norm': '15.062', 'counters/examples': 239040, 'counters/updates': 7470}
train stats after 239072 examples: {'rewards_train/chosen': '0.20051', 'rewards_train/rejected': '0.097252', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10326', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-159.96', 'loss/train': '0.66251', 'examples_per_second': '31.494', 'grad_norm': '22.25', 'counters/examples': 239072, 'counters/updates': 7471}
train stats after 239104 examples: {'rewards_train/chosen': '0.19744', 'rewards_train/rejected': '-0.077562', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27501', 'logps_train/rejected': '-101.19', 'logps_train/chosen': '-138.96', 'loss/train': '0.58764', 'examples_per_second': '30.553', 'grad_norm': '17.375', 'counters/examples': 239104, 'counters/updates': 7472}
train stats after 239136 examples: {'rewards_train/chosen': '0.20367', 'rewards_train/rejected': '0.017648', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18602', 'logps_train/rejected': '-145.68', 'logps_train/chosen': '-142.4', 'loss/train': '0.61961', 'examples_per_second': '30.094', 'grad_norm': '20.25', 'counters/examples': 239136, 'counters/updates': 7473}
train stats after 239168 examples: {'rewards_train/chosen': '0.16249', 'rewards_train/rejected': '-0.036846', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19934', 'logps_train/rejected': '-100.13', 'logps_train/chosen': '-144.83', 'loss/train': '0.62815', 'examples_per_second': '32.595', 'grad_norm': '19.75', 'counters/examples': 239168, 'counters/updates': 7474}
train stats after 239200 examples: {'rewards_train/chosen': '0.15554', 'rewards_train/rejected': '-0.024866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18041', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-135.9', 'loss/train': '0.62424', 'examples_per_second': '30.493', 'grad_norm': '19.75', 'counters/examples': 239200, 'counters/updates': 7475}
train stats after 239232 examples: {'rewards_train/chosen': '0.18872', 'rewards_train/rejected': '0.068679', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12004', 'logps_train/rejected': '-164.68', 'logps_train/chosen': '-161.87', 'loss/train': '0.66375', 'examples_per_second': '31.521', 'grad_norm': '24.75', 'counters/examples': 239232, 'counters/updates': 7476}
train stats after 239264 examples: {'rewards_train/chosen': '0.13356', 'rewards_train/rejected': '-0.048147', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18171', 'logps_train/rejected': '-108.59', 'logps_train/chosen': '-151.68', 'loss/train': '0.62549', 'examples_per_second': '30.056', 'grad_norm': '19.5', 'counters/examples': 239264, 'counters/updates': 7477}
train stats after 239296 examples: {'rewards_train/chosen': '0.21896', 'rewards_train/rejected': '0.1114', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10757', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-132.73', 'loss/train': '0.67743', 'examples_per_second': '32.96', 'grad_norm': '20.5', 'counters/examples': 239296, 'counters/updates': 7478}
train stats after 239328 examples: {'rewards_train/chosen': '0.081794', 'rewards_train/rejected': '0.015247', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066547', 'logps_train/rejected': '-94.782', 'logps_train/chosen': '-132.88', 'loss/train': '0.67717', 'examples_per_second': '30.964', 'grad_norm': '19.75', 'counters/examples': 239328, 'counters/updates': 7479}
train stats after 239360 examples: {'rewards_train/chosen': '0.23546', 'rewards_train/rejected': '0.028456', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.207', 'logps_train/rejected': '-152.07', 'logps_train/chosen': '-156.62', 'loss/train': '0.60702', 'examples_per_second': '31.572', 'grad_norm': '21.375', 'counters/examples': 239360, 'counters/updates': 7480}
train stats after 239392 examples: {'rewards_train/chosen': '0.22454', 'rewards_train/rejected': '0.049284', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17525', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-170.73', 'loss/train': '0.63307', 'examples_per_second': '31.527', 'grad_norm': '21.5', 'counters/examples': 239392, 'counters/updates': 7481}
skipping logging after 239424 examples to avoid logging too frequently
train stats after 239456 examples: {'rewards_train/chosen': '0.069298', 'rewards_train/rejected': '0.063183', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0061157', 'logps_train/rejected': '-137.15', 'logps_train/chosen': '-133.18', 'loss/train': '0.71101', 'examples_per_second': '30.116', 'grad_norm': '23', 'counters/examples': 239456, 'counters/updates': 7483}
train stats after 239488 examples: {'rewards_train/chosen': '0.23681', 'rewards_train/rejected': '-0.043937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.28074', 'logps_train/rejected': '-93.748', 'logps_train/chosen': '-127.17', 'loss/train': '0.59888', 'examples_per_second': '31.484', 'grad_norm': '17.625', 'counters/examples': 239488, 'counters/updates': 7484}
train stats after 239520 examples: {'rewards_train/chosen': '0.16651', 'rewards_train/rejected': '-0.012498', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.179', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-146.67', 'loss/train': '0.62582', 'examples_per_second': '31.58', 'grad_norm': '20.5', 'counters/examples': 239520, 'counters/updates': 7485}
train stats after 239552 examples: {'rewards_train/chosen': '0.15567', 'rewards_train/rejected': '-0.013579', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16925', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-127.45', 'loss/train': '0.62681', 'examples_per_second': '31.482', 'grad_norm': '19.625', 'counters/examples': 239552, 'counters/updates': 7486}
train stats after 239584 examples: {'rewards_train/chosen': '0.21596', 'rewards_train/rejected': '-0.070506', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28647', 'logps_train/rejected': '-142.42', 'logps_train/chosen': '-178.26', 'loss/train': '0.58418', 'examples_per_second': '30.473', 'grad_norm': '19.625', 'counters/examples': 239584, 'counters/updates': 7487}
train stats after 239616 examples: {'rewards_train/chosen': '0.20478', 'rewards_train/rejected': '-0.034549', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23933', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-136.32', 'loss/train': '0.59444', 'examples_per_second': '31.465', 'grad_norm': '18.375', 'counters/examples': 239616, 'counters/updates': 7488}
train stats after 239648 examples: {'rewards_train/chosen': '0.15067', 'rewards_train/rejected': '-0.011029', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1617', 'logps_train/rejected': '-101.1', 'logps_train/chosen': '-129.82', 'loss/train': '0.62772', 'examples_per_second': '31.054', 'grad_norm': '18.125', 'counters/examples': 239648, 'counters/updates': 7489}
skipping logging after 239680 examples to avoid logging too frequently
train stats after 239712 examples: {'rewards_train/chosen': '0.23351', 'rewards_train/rejected': '-0.039149', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.27266', 'logps_train/rejected': '-98.86', 'logps_train/chosen': '-154.67', 'loss/train': '0.57931', 'examples_per_second': '30.871', 'grad_norm': '18.75', 'counters/examples': 239712, 'counters/updates': 7491}
train stats after 239744 examples: {'rewards_train/chosen': '0.16642', 'rewards_train/rejected': '0.021517', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1449', 'logps_train/rejected': '-104.65', 'logps_train/chosen': '-121.81', 'loss/train': '0.63617', 'examples_per_second': '32.791', 'grad_norm': '19', 'counters/examples': 239744, 'counters/updates': 7492}
train stats after 239776 examples: {'rewards_train/chosen': '0.22162', 'rewards_train/rejected': '0.090284', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13133', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-116.29', 'loss/train': '0.64321', 'examples_per_second': '31.572', 'grad_norm': '19.75', 'counters/examples': 239776, 'counters/updates': 7493}
skipping logging after 239808 examples to avoid logging too frequently
train stats after 239840 examples: {'rewards_train/chosen': '0.16182', 'rewards_train/rejected': '0.069428', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092396', 'logps_train/rejected': '-107.53', 'logps_train/chosen': '-129.89', 'loss/train': '0.66005', 'examples_per_second': '35.417', 'grad_norm': '20', 'counters/examples': 239840, 'counters/updates': 7495}
train stats after 239872 examples: {'rewards_train/chosen': '0.15465', 'rewards_train/rejected': '0.11991', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034738', 'logps_train/rejected': '-133.01', 'logps_train/chosen': '-137.28', 'loss/train': '0.69236', 'examples_per_second': '31.443', 'grad_norm': '21.375', 'counters/examples': 239872, 'counters/updates': 7496}
train stats after 239904 examples: {'rewards_train/chosen': '0.14743', 'rewards_train/rejected': '0.083454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06398', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-118.94', 'loss/train': '0.67246', 'examples_per_second': '26.255', 'grad_norm': '19.875', 'counters/examples': 239904, 'counters/updates': 7497}
train stats after 239936 examples: {'rewards_train/chosen': '0.12411', 'rewards_train/rejected': '0.043192', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080915', 'logps_train/rejected': '-105.67', 'logps_train/chosen': '-140.34', 'loss/train': '0.66962', 'examples_per_second': '31.38', 'grad_norm': '20.125', 'counters/examples': 239936, 'counters/updates': 7498}
train stats after 239968 examples: {'rewards_train/chosen': '0.063402', 'rewards_train/rejected': '-0.0071905', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070593', 'logps_train/rejected': '-105.46', 'logps_train/chosen': '-109.97', 'loss/train': '0.67229', 'examples_per_second': '31.959', 'grad_norm': '18.875', 'counters/examples': 239968, 'counters/updates': 7499}
train stats after 240000 examples: {'rewards_train/chosen': '0.15123', 'rewards_train/rejected': '0.06362', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087613', 'logps_train/rejected': '-104.44', 'logps_train/chosen': '-117.53', 'loss/train': '0.6665', 'examples_per_second': '32.556', 'grad_norm': '19.625', 'counters/examples': 240000, 'counters/updates': 7500}
Running evaluation after 240000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.16it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.81it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.90it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.90it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.92it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 240000: {'rewards_eval/chosen': '0.18074', 'rewards_eval/rejected': '0.041692', 'rewards_eval/accuracies': '0.59766', 'rewards_eval/margins': '0.13904', 'logps_eval/rejected': '-114.98', 'logps_eval/chosen': '-133.74', 'loss/eval': '0.64758'}
skipping save for non epoch
train stats after 240032 examples: {'rewards_train/chosen': '0.19027', 'rewards_train/rejected': '0.0409', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14937', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-144.03', 'loss/train': '0.63928', 'examples_per_second': '30.574', 'grad_norm': '20.875', 'counters/examples': 240032, 'counters/updates': 7501}
train stats after 240064 examples: {'rewards_train/chosen': '0.073265', 'rewards_train/rejected': '-0.12188', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19515', 'logps_train/rejected': '-94.172', 'logps_train/chosen': '-113.41', 'loss/train': '0.61131', 'examples_per_second': '30.477', 'grad_norm': '17.375', 'counters/examples': 240064, 'counters/updates': 7502}
train stats after 240096 examples: {'rewards_train/chosen': '0.076356', 'rewards_train/rejected': '-0.044582', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12094', 'logps_train/rejected': '-106.3', 'logps_train/chosen': '-117.22', 'loss/train': '0.65232', 'examples_per_second': '31.11', 'grad_norm': '19', 'counters/examples': 240096, 'counters/updates': 7503}
train stats after 240128 examples: {'rewards_train/chosen': '0.29047', 'rewards_train/rejected': '0.13739', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15308', 'logps_train/rejected': '-118.97', 'logps_train/chosen': '-162.01', 'loss/train': '0.63464', 'examples_per_second': '31.108', 'grad_norm': '21.75', 'counters/examples': 240128, 'counters/updates': 7504}
train stats after 240160 examples: {'rewards_train/chosen': '0.15013', 'rewards_train/rejected': '0.062259', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087869', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-150.45', 'loss/train': '0.66273', 'examples_per_second': '30.834', 'grad_norm': '19.875', 'counters/examples': 240160, 'counters/updates': 7505}
train stats after 240192 examples: {'rewards_train/chosen': '0.19908', 'rewards_train/rejected': '0.026561', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17251', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-131.99', 'loss/train': '0.62462', 'examples_per_second': '30.858', 'grad_norm': '20', 'counters/examples': 240192, 'counters/updates': 7506}
train stats after 240224 examples: {'rewards_train/chosen': '0.18818', 'rewards_train/rejected': '0.01369', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17449', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-131.8', 'loss/train': '0.63313', 'examples_per_second': '31.289', 'grad_norm': '19.25', 'counters/examples': 240224, 'counters/updates': 7507}
skipping logging after 240256 examples to avoid logging too frequently
train stats after 240288 examples: {'rewards_train/chosen': '0.15799', 'rewards_train/rejected': '0.022155', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13584', 'logps_train/rejected': '-133.18', 'logps_train/chosen': '-127.46', 'loss/train': '0.63727', 'examples_per_second': '30.428', 'grad_norm': '19.875', 'counters/examples': 240288, 'counters/updates': 7509}
train stats after 240320 examples: {'rewards_train/chosen': '0.081963', 'rewards_train/rejected': '0.041496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040466', 'logps_train/rejected': '-147.97', 'logps_train/chosen': '-150.34', 'loss/train': '0.68461', 'examples_per_second': '31.008', 'grad_norm': '23.75', 'counters/examples': 240320, 'counters/updates': 7510}
train stats after 240352 examples: {'rewards_train/chosen': '0.15861', 'rewards_train/rejected': '0.058408', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.1002', 'logps_train/rejected': '-102.51', 'logps_train/chosen': '-125.05', 'loss/train': '0.66657', 'examples_per_second': '33.108', 'grad_norm': '18.375', 'counters/examples': 240352, 'counters/updates': 7511}
skipping logging after 240384 examples to avoid logging too frequently
train stats after 240416 examples: {'rewards_train/chosen': '0.099614', 'rewards_train/rejected': '0.031558', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068056', 'logps_train/rejected': '-103.47', 'logps_train/chosen': '-101.05', 'loss/train': '0.68041', 'examples_per_second': '30.857', 'grad_norm': '18.5', 'counters/examples': 240416, 'counters/updates': 7513}
train stats after 240448 examples: {'rewards_train/chosen': '0.16917', 'rewards_train/rejected': '0.023693', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14548', 'logps_train/rejected': '-111.41', 'logps_train/chosen': '-144.65', 'loss/train': '0.63899', 'examples_per_second': '31.437', 'grad_norm': '20.25', 'counters/examples': 240448, 'counters/updates': 7514}
train stats after 240480 examples: {'rewards_train/chosen': '0.21652', 'rewards_train/rejected': '0.12009', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096435', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-127.66', 'loss/train': '0.65891', 'examples_per_second': '30.429', 'grad_norm': '19.25', 'counters/examples': 240480, 'counters/updates': 7515}
train stats after 240512 examples: {'rewards_train/chosen': '0.15255', 'rewards_train/rejected': '-0.022797', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17535', 'logps_train/rejected': '-106.98', 'logps_train/chosen': '-150.03', 'loss/train': '0.62631', 'examples_per_second': '30.527', 'grad_norm': '19.625', 'counters/examples': 240512, 'counters/updates': 7516}
skipping logging after 240544 examples to avoid logging too frequently
train stats after 240576 examples: {'rewards_train/chosen': '0.21621', 'rewards_train/rejected': '0.17055', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045661', 'logps_train/rejected': '-115.64', 'logps_train/chosen': '-145.25', 'loss/train': '0.68255', 'examples_per_second': '30.429', 'grad_norm': '21.5', 'counters/examples': 240576, 'counters/updates': 7518}
train stats after 240608 examples: {'rewards_train/chosen': '0.21744', 'rewards_train/rejected': '0.076657', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14078', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-126.39', 'loss/train': '0.63656', 'examples_per_second': '32.831', 'grad_norm': '18.75', 'counters/examples': 240608, 'counters/updates': 7519}
train stats after 240640 examples: {'rewards_train/chosen': '0.13634', 'rewards_train/rejected': '0.0081786', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12816', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-125.99', 'loss/train': '0.64318', 'examples_per_second': '24.74', 'grad_norm': '19.375', 'counters/examples': 240640, 'counters/updates': 7520}
skipping logging after 240672 examples to avoid logging too frequently
train stats after 240704 examples: {'rewards_train/chosen': '0.16217', 'rewards_train/rejected': '0.038189', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12398', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-123.44', 'loss/train': '0.6542', 'examples_per_second': '35.298', 'grad_norm': '19.75', 'counters/examples': 240704, 'counters/updates': 7522}
train stats after 240736 examples: {'rewards_train/chosen': '0.15142', 'rewards_train/rejected': '0.029526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1219', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-160.83', 'loss/train': '0.65213', 'examples_per_second': '24.121', 'grad_norm': '23.25', 'counters/examples': 240736, 'counters/updates': 7523}
train stats after 240768 examples: {'rewards_train/chosen': '0.15234', 'rewards_train/rejected': '0.065976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086363', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-121.02', 'loss/train': '0.67118', 'examples_per_second': '30.796', 'grad_norm': '20.875', 'counters/examples': 240768, 'counters/updates': 7524}
train stats after 240800 examples: {'rewards_train/chosen': '0.076295', 'rewards_train/rejected': '0.057225', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01907', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-109.54', 'loss/train': '0.69696', 'examples_per_second': '33.171', 'grad_norm': '19.75', 'counters/examples': 240800, 'counters/updates': 7525}
train stats after 240832 examples: {'rewards_train/chosen': '0.061816', 'rewards_train/rejected': '0.0036553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058161', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-170.64', 'loss/train': '0.68038', 'examples_per_second': '31.02', 'grad_norm': '23.5', 'counters/examples': 240832, 'counters/updates': 7526}
train stats after 240864 examples: {'rewards_train/chosen': '0.23908', 'rewards_train/rejected': '0.081772', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1573', 'logps_train/rejected': '-97.173', 'logps_train/chosen': '-133.53', 'loss/train': '0.63149', 'examples_per_second': '32.296', 'grad_norm': '19.375', 'counters/examples': 240864, 'counters/updates': 7527}
train stats after 240896 examples: {'rewards_train/chosen': '0.18045', 'rewards_train/rejected': '0.035582', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14487', 'logps_train/rejected': '-112.93', 'logps_train/chosen': '-135.66', 'loss/train': '0.64977', 'examples_per_second': '30.923', 'grad_norm': '21.625', 'counters/examples': 240896, 'counters/updates': 7528}
train stats after 240928 examples: {'rewards_train/chosen': '0.22794', 'rewards_train/rejected': '0.031781', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19616', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-117.3', 'loss/train': '0.6145', 'examples_per_second': '31.886', 'grad_norm': '18', 'counters/examples': 240928, 'counters/updates': 7529}
skipping logging after 240960 examples to avoid logging too frequently
train stats after 240992 examples: {'rewards_train/chosen': '0.14057', 'rewards_train/rejected': '-0.034166', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17474', 'logps_train/rejected': '-92.648', 'logps_train/chosen': '-128.81', 'loss/train': '0.63191', 'examples_per_second': '33.356', 'grad_norm': '22.375', 'counters/examples': 240992, 'counters/updates': 7531}
train stats after 241024 examples: {'rewards_train/chosen': '0.24327', 'rewards_train/rejected': '0.083438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15983', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-133.51', 'loss/train': '0.63438', 'examples_per_second': '30.918', 'grad_norm': '20.125', 'counters/examples': 241024, 'counters/updates': 7532}
skipping logging after 241056 examples to avoid logging too frequently
train stats after 241088 examples: {'rewards_train/chosen': '0.24282', 'rewards_train/rejected': '0.058937', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18388', 'logps_train/rejected': '-129.17', 'logps_train/chosen': '-132.85', 'loss/train': '0.61986', 'examples_per_second': '30.071', 'grad_norm': '19.625', 'counters/examples': 241088, 'counters/updates': 7534}
train stats after 241120 examples: {'rewards_train/chosen': '0.10615', 'rewards_train/rejected': '-0.0024497', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1086', 'logps_train/rejected': '-143.75', 'logps_train/chosen': '-122.92', 'loss/train': '0.65319', 'examples_per_second': '31.473', 'grad_norm': '19.875', 'counters/examples': 241120, 'counters/updates': 7535}
train stats after 241152 examples: {'rewards_train/chosen': '0.22075', 'rewards_train/rejected': '0.095779', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12497', 'logps_train/rejected': '-111.94', 'logps_train/chosen': '-127.73', 'loss/train': '0.64566', 'examples_per_second': '30.588', 'grad_norm': '20.75', 'counters/examples': 241152, 'counters/updates': 7536}
train stats after 241184 examples: {'rewards_train/chosen': '0.2195', 'rewards_train/rejected': '0.15067', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068826', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-126.61', 'loss/train': '0.6697', 'examples_per_second': '30.825', 'grad_norm': '19.125', 'counters/examples': 241184, 'counters/updates': 7537}
train stats after 241216 examples: {'rewards_train/chosen': '0.2276', 'rewards_train/rejected': '0.073131', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15447', 'logps_train/rejected': '-98.658', 'logps_train/chosen': '-108.32', 'loss/train': '0.64555', 'examples_per_second': '30.446', 'grad_norm': '17.375', 'counters/examples': 241216, 'counters/updates': 7538}
skipping logging after 241248 examples to avoid logging too frequently
train stats after 241280 examples: {'rewards_train/chosen': '0.30198', 'rewards_train/rejected': '0.10714', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19483', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-126.09', 'loss/train': '0.61483', 'examples_per_second': '33.272', 'grad_norm': '18.75', 'counters/examples': 241280, 'counters/updates': 7540}
train stats after 241312 examples: {'rewards_train/chosen': '0.16784', 'rewards_train/rejected': '0.019019', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14882', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-130.76', 'loss/train': '0.6487', 'examples_per_second': '31.862', 'grad_norm': '19.25', 'counters/examples': 241312, 'counters/updates': 7541}
train stats after 241344 examples: {'rewards_train/chosen': '0.13282', 'rewards_train/rejected': '0.023745', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10908', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-142.01', 'loss/train': '0.65851', 'examples_per_second': '31.514', 'grad_norm': '22.625', 'counters/examples': 241344, 'counters/updates': 7542}
train stats after 241376 examples: {'rewards_train/chosen': '0.069833', 'rewards_train/rejected': '-0.048297', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11813', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-162.13', 'loss/train': '0.64596', 'examples_per_second': '31.521', 'grad_norm': '22.25', 'counters/examples': 241376, 'counters/updates': 7543}
train stats after 241408 examples: {'rewards_train/chosen': '0.063188', 'rewards_train/rejected': '0.070924', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0077366', 'logps_train/rejected': '-136.64', 'logps_train/chosen': '-146.49', 'loss/train': '0.71998', 'examples_per_second': '31.004', 'grad_norm': '22.875', 'counters/examples': 241408, 'counters/updates': 7544}
train stats after 241440 examples: {'rewards_train/chosen': '0.31972', 'rewards_train/rejected': '0.083349', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23637', 'logps_train/rejected': '-115.25', 'logps_train/chosen': '-168.02', 'loss/train': '0.59718', 'examples_per_second': '31.452', 'grad_norm': '21.25', 'counters/examples': 241440, 'counters/updates': 7545}
train stats after 241472 examples: {'rewards_train/chosen': '0.24884', 'rewards_train/rejected': '0.041097', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20774', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-129.8', 'loss/train': '0.61603', 'examples_per_second': '32.254', 'grad_norm': '19.5', 'counters/examples': 241472, 'counters/updates': 7546}
train stats after 241504 examples: {'rewards_train/chosen': '0.25801', 'rewards_train/rejected': '0.014001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24401', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-175.22', 'loss/train': '0.60053', 'examples_per_second': '31.49', 'grad_norm': '21.625', 'counters/examples': 241504, 'counters/updates': 7547}
train stats after 241536 examples: {'rewards_train/chosen': '0.22824', 'rewards_train/rejected': '0.050097', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17814', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-149.5', 'loss/train': '0.62782', 'examples_per_second': '31.491', 'grad_norm': '21.625', 'counters/examples': 241536, 'counters/updates': 7548}
train stats after 241568 examples: {'rewards_train/chosen': '0.20348', 'rewards_train/rejected': '0.05682', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14666', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-164.97', 'loss/train': '0.64138', 'examples_per_second': '30.021', 'grad_norm': '20.375', 'counters/examples': 241568, 'counters/updates': 7549}
train stats after 241600 examples: {'rewards_train/chosen': '0.24844', 'rewards_train/rejected': '0.1639', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084541', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-131.96', 'loss/train': '0.67877', 'examples_per_second': '31.623', 'grad_norm': '18.75', 'counters/examples': 241600, 'counters/updates': 7550}
train stats after 241632 examples: {'rewards_train/chosen': '0.28313', 'rewards_train/rejected': '0.10854', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17459', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-132.21', 'loss/train': '0.63098', 'examples_per_second': '32.415', 'grad_norm': '19.125', 'counters/examples': 241632, 'counters/updates': 7551}
train stats after 241664 examples: {'rewards_train/chosen': '0.1588', 'rewards_train/rejected': '0.010353', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14845', 'logps_train/rejected': '-96.249', 'logps_train/chosen': '-165.41', 'loss/train': '0.64056', 'examples_per_second': '31.808', 'grad_norm': '20', 'counters/examples': 241664, 'counters/updates': 7552}
train stats after 241696 examples: {'rewards_train/chosen': '0.18611', 'rewards_train/rejected': '-0.070298', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2564', 'logps_train/rejected': '-83.312', 'logps_train/chosen': '-114.97', 'loss/train': '0.5867', 'examples_per_second': '32.101', 'grad_norm': '16', 'counters/examples': 241696, 'counters/updates': 7553}
train stats after 241728 examples: {'rewards_train/chosen': '0.35299', 'rewards_train/rejected': '0.05606', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29693', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-151.34', 'loss/train': '0.58104', 'examples_per_second': '31.507', 'grad_norm': '20.625', 'counters/examples': 241728, 'counters/updates': 7554}
train stats after 241760 examples: {'rewards_train/chosen': '0.1841', 'rewards_train/rejected': '0.046326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13778', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-160.62', 'loss/train': '0.64636', 'examples_per_second': '31.321', 'grad_norm': '21.375', 'counters/examples': 241760, 'counters/updates': 7555}
train stats after 241792 examples: {'rewards_train/chosen': '0.23771', 'rewards_train/rejected': '0.15038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087331', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-133.82', 'loss/train': '0.66293', 'examples_per_second': '31.115', 'grad_norm': '20.125', 'counters/examples': 241792, 'counters/updates': 7556}
train stats after 241824 examples: {'rewards_train/chosen': '0.22239', 'rewards_train/rejected': '0.089091', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13329', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-173.81', 'loss/train': '0.65037', 'examples_per_second': '31.317', 'grad_norm': '22.125', 'counters/examples': 241824, 'counters/updates': 7557}
train stats after 241856 examples: {'rewards_train/chosen': '0.098827', 'rewards_train/rejected': '0.037258', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061569', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-121.97', 'loss/train': '0.67819', 'examples_per_second': '31.452', 'grad_norm': '20.125', 'counters/examples': 241856, 'counters/updates': 7558}
skipping logging after 241888 examples to avoid logging too frequently
train stats after 241920 examples: {'rewards_train/chosen': '0.23379', 'rewards_train/rejected': '0.14571', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088072', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-137.3', 'loss/train': '0.67117', 'examples_per_second': '31.06', 'grad_norm': '20.625', 'counters/examples': 241920, 'counters/updates': 7560}
train stats after 241952 examples: {'rewards_train/chosen': '0.17412', 'rewards_train/rejected': '0.10449', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.069632', 'logps_train/rejected': '-157.03', 'logps_train/chosen': '-159.63', 'loss/train': '0.67753', 'examples_per_second': '31.502', 'grad_norm': '23.625', 'counters/examples': 241952, 'counters/updates': 7561}
train stats after 241984 examples: {'rewards_train/chosen': '0.19676', 'rewards_train/rejected': '0.0645', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13226', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-149.53', 'loss/train': '0.65005', 'examples_per_second': '31.467', 'grad_norm': '21.375', 'counters/examples': 241984, 'counters/updates': 7562}
train stats after 242016 examples: {'rewards_train/chosen': '0.088583', 'rewards_train/rejected': '0.11013', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021547', 'logps_train/rejected': '-100.91', 'logps_train/chosen': '-119.53', 'loss/train': '0.71466', 'examples_per_second': '30.523', 'grad_norm': '20.875', 'counters/examples': 242016, 'counters/updates': 7563}
train stats after 242048 examples: {'rewards_train/chosen': '0.16833', 'rewards_train/rejected': '0.043742', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12459', 'logps_train/rejected': '-116.63', 'logps_train/chosen': '-143.58', 'loss/train': '0.65294', 'examples_per_second': '30.414', 'grad_norm': '21.5', 'counters/examples': 242048, 'counters/updates': 7564}
train stats after 242080 examples: {'rewards_train/chosen': '0.25904', 'rewards_train/rejected': '0.063981', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19506', 'logps_train/rejected': '-151.04', 'logps_train/chosen': '-162.62', 'loss/train': '0.61989', 'examples_per_second': '31.5', 'grad_norm': '21.875', 'counters/examples': 242080, 'counters/updates': 7565}
train stats after 242112 examples: {'rewards_train/chosen': '0.16281', 'rewards_train/rejected': '-0.029352', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19217', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-120.29', 'loss/train': '0.62118', 'examples_per_second': '31.088', 'grad_norm': '18.125', 'counters/examples': 242112, 'counters/updates': 7566}
train stats after 242144 examples: {'rewards_train/chosen': '0.15509', 'rewards_train/rejected': '0.052035', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10305', 'logps_train/rejected': '-150.18', 'logps_train/chosen': '-142.21', 'loss/train': '0.65143', 'examples_per_second': '31.795', 'grad_norm': '22.125', 'counters/examples': 242144, 'counters/updates': 7567}
skipping logging after 242176 examples to avoid logging too frequently
train stats after 242208 examples: {'rewards_train/chosen': '0.16395', 'rewards_train/rejected': '0.07083', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093118', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-140.61', 'loss/train': '0.66989', 'examples_per_second': '33.449', 'grad_norm': '23.375', 'counters/examples': 242208, 'counters/updates': 7569}
skipping logging after 242240 examples to avoid logging too frequently
train stats after 242272 examples: {'rewards_train/chosen': '0.19234', 'rewards_train/rejected': '0.042643', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14969', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-164.27', 'loss/train': '0.64144', 'examples_per_second': '29.99', 'grad_norm': '21.375', 'counters/examples': 242272, 'counters/updates': 7571}
train stats after 242304 examples: {'rewards_train/chosen': '0.28824', 'rewards_train/rejected': '0.14415', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14409', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-165.59', 'loss/train': '0.65136', 'examples_per_second': '29.97', 'grad_norm': '22.875', 'counters/examples': 242304, 'counters/updates': 7572}
train stats after 242336 examples: {'rewards_train/chosen': '0.27369', 'rewards_train/rejected': '0.067428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20626', 'logps_train/rejected': '-108.87', 'logps_train/chosen': '-144.48', 'loss/train': '0.61744', 'examples_per_second': '30.506', 'grad_norm': '19.5', 'counters/examples': 242336, 'counters/updates': 7573}
train stats after 242368 examples: {'rewards_train/chosen': '0.073701', 'rewards_train/rejected': '0.033662', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040039', 'logps_train/rejected': '-99.463', 'logps_train/chosen': '-133.06', 'loss/train': '0.69198', 'examples_per_second': '32.161', 'grad_norm': '20', 'counters/examples': 242368, 'counters/updates': 7574}
train stats after 242400 examples: {'rewards_train/chosen': '0.16566', 'rewards_train/rejected': '0.0029788', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16268', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-146.09', 'loss/train': '0.63504', 'examples_per_second': '31.438', 'grad_norm': '21.25', 'counters/examples': 242400, 'counters/updates': 7575}
train stats after 242432 examples: {'rewards_train/chosen': '0.15638', 'rewards_train/rejected': '0.038205', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11817', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-97.114', 'loss/train': '0.66004', 'examples_per_second': '31.523', 'grad_norm': '18.25', 'counters/examples': 242432, 'counters/updates': 7576}
train stats after 242464 examples: {'rewards_train/chosen': '0.15751', 'rewards_train/rejected': '0.0073855', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15013', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-145.93', 'loss/train': '0.6315', 'examples_per_second': '30.187', 'grad_norm': '20.125', 'counters/examples': 242464, 'counters/updates': 7577}
train stats after 242496 examples: {'rewards_train/chosen': '0.034493', 'rewards_train/rejected': '-0.037982', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072475', 'logps_train/rejected': '-123.93', 'logps_train/chosen': '-131.56', 'loss/train': '0.66393', 'examples_per_second': '30.673', 'grad_norm': '21.25', 'counters/examples': 242496, 'counters/updates': 7578}
train stats after 242528 examples: {'rewards_train/chosen': '0.21993', 'rewards_train/rejected': '-0.045319', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.26525', 'logps_train/rejected': '-107.06', 'logps_train/chosen': '-135.49', 'loss/train': '0.58762', 'examples_per_second': '31.135', 'grad_norm': '18', 'counters/examples': 242528, 'counters/updates': 7579}
skipping logging after 242560 examples to avoid logging too frequently
train stats after 242592 examples: {'rewards_train/chosen': '0.23016', 'rewards_train/rejected': '0.088027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14213', 'logps_train/rejected': '-100.34', 'logps_train/chosen': '-128.33', 'loss/train': '0.6381', 'examples_per_second': '30.102', 'grad_norm': '18.875', 'counters/examples': 242592, 'counters/updates': 7581}
train stats after 242624 examples: {'rewards_train/chosen': '0.18613', 'rewards_train/rejected': '0.028456', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15767', 'logps_train/rejected': '-128.46', 'logps_train/chosen': '-125.43', 'loss/train': '0.63788', 'examples_per_second': '30.771', 'grad_norm': '19.875', 'counters/examples': 242624, 'counters/updates': 7582}
train stats after 242656 examples: {'rewards_train/chosen': '0.16796', 'rewards_train/rejected': '-0.011198', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17915', 'logps_train/rejected': '-96.05', 'logps_train/chosen': '-113.86', 'loss/train': '0.62491', 'examples_per_second': '29.996', 'grad_norm': '17.25', 'counters/examples': 242656, 'counters/updates': 7583}
train stats after 242688 examples: {'rewards_train/chosen': '0.15885', 'rewards_train/rejected': '-0.03967', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19852', 'logps_train/rejected': '-97.946', 'logps_train/chosen': '-121.48', 'loss/train': '0.61621', 'examples_per_second': '31.502', 'grad_norm': '17.875', 'counters/examples': 242688, 'counters/updates': 7584}
train stats after 242720 examples: {'rewards_train/chosen': '0.1212', 'rewards_train/rejected': '-0.015553', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13675', 'logps_train/rejected': '-90.226', 'logps_train/chosen': '-107.35', 'loss/train': '0.6495', 'examples_per_second': '30.739', 'grad_norm': '17', 'counters/examples': 242720, 'counters/updates': 7585}
train stats after 242752 examples: {'rewards_train/chosen': '0.087809', 'rewards_train/rejected': '-0.044794', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1326', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-112', 'loss/train': '0.64474', 'examples_per_second': '32.516', 'grad_norm': '19.5', 'counters/examples': 242752, 'counters/updates': 7586}
train stats after 242784 examples: {'rewards_train/chosen': '0.14578', 'rewards_train/rejected': '0.097237', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04854', 'logps_train/rejected': '-99.104', 'logps_train/chosen': '-130.35', 'loss/train': '0.68268', 'examples_per_second': '32.75', 'grad_norm': '19', 'counters/examples': 242784, 'counters/updates': 7587}
skipping logging after 242816 examples to avoid logging too frequently
train stats after 242848 examples: {'rewards_train/chosen': '0.20633', 'rewards_train/rejected': '0.11727', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08906', 'logps_train/rejected': '-139.57', 'logps_train/chosen': '-153.01', 'loss/train': '0.66471', 'examples_per_second': '31.3', 'grad_norm': '21.625', 'counters/examples': 242848, 'counters/updates': 7589}
train stats after 242880 examples: {'rewards_train/chosen': '0.098155', 'rewards_train/rejected': '0.030541', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067614', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-116.81', 'loss/train': '0.67548', 'examples_per_second': '31.425', 'grad_norm': '20.5', 'counters/examples': 242880, 'counters/updates': 7590}
train stats after 242912 examples: {'rewards_train/chosen': '0.18207', 'rewards_train/rejected': '0.089293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092773', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-154.2', 'loss/train': '0.66929', 'examples_per_second': '31.511', 'grad_norm': '22', 'counters/examples': 242912, 'counters/updates': 7591}
train stats after 242944 examples: {'rewards_train/chosen': '0.15155', 'rewards_train/rejected': '-0.03224', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18379', 'logps_train/rejected': '-132.07', 'logps_train/chosen': '-128.68', 'loss/train': '0.62956', 'examples_per_second': '31.458', 'grad_norm': '20.5', 'counters/examples': 242944, 'counters/updates': 7592}
train stats after 242976 examples: {'rewards_train/chosen': '0.18183', 'rewards_train/rejected': '-0.0066684', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1885', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-139.89', 'loss/train': '0.6127', 'examples_per_second': '31.503', 'grad_norm': '19.5', 'counters/examples': 242976, 'counters/updates': 7593}
train stats after 243008 examples: {'rewards_train/chosen': '0.25775', 'rewards_train/rejected': '0.059793', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19795', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-132.04', 'loss/train': '0.61106', 'examples_per_second': '31.434', 'grad_norm': '18.625', 'counters/examples': 243008, 'counters/updates': 7594}
skipping logging after 243040 examples to avoid logging too frequently
train stats after 243072 examples: {'rewards_train/chosen': '0.10257', 'rewards_train/rejected': '0.015241', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08733', 'logps_train/rejected': '-108.81', 'logps_train/chosen': '-143.35', 'loss/train': '0.66234', 'examples_per_second': '34.046', 'grad_norm': '20', 'counters/examples': 243072, 'counters/updates': 7596}
train stats after 243104 examples: {'rewards_train/chosen': '0.14007', 'rewards_train/rejected': '0.057497', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082574', 'logps_train/rejected': '-116.76', 'logps_train/chosen': '-147.32', 'loss/train': '0.67779', 'examples_per_second': '31.282', 'grad_norm': '23', 'counters/examples': 243104, 'counters/updates': 7597}
train stats after 243136 examples: {'rewards_train/chosen': '0.19022', 'rewards_train/rejected': '0.025277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16494', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-139.5', 'loss/train': '0.63849', 'examples_per_second': '31.415', 'grad_norm': '21.875', 'counters/examples': 243136, 'counters/updates': 7598}
train stats after 243168 examples: {'rewards_train/chosen': '0.18303', 'rewards_train/rejected': '0.085031', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097994', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-151.38', 'loss/train': '0.66578', 'examples_per_second': '32.4', 'grad_norm': '21.625', 'counters/examples': 243168, 'counters/updates': 7599}
train stats after 243200 examples: {'rewards_train/chosen': '0.24553', 'rewards_train/rejected': '0.05626', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18927', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-101.7', 'loss/train': '0.62163', 'examples_per_second': '30.124', 'grad_norm': '18.5', 'counters/examples': 243200, 'counters/updates': 7600}
train stats after 243232 examples: {'rewards_train/chosen': '0.092753', 'rewards_train/rejected': '-0.074769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16752', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-141.64', 'loss/train': '0.63636', 'examples_per_second': '31.096', 'grad_norm': '21', 'counters/examples': 243232, 'counters/updates': 7601}
train stats after 243264 examples: {'rewards_train/chosen': '0.11886', 'rewards_train/rejected': '-0.056967', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17582', 'logps_train/rejected': '-116.71', 'logps_train/chosen': '-120.11', 'loss/train': '0.62014', 'examples_per_second': '32.703', 'grad_norm': '19.625', 'counters/examples': 243264, 'counters/updates': 7602}
train stats after 243296 examples: {'rewards_train/chosen': '0.036412', 'rewards_train/rejected': '0.059423', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023011', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-147.25', 'loss/train': '0.72561', 'examples_per_second': '31.289', 'grad_norm': '22', 'counters/examples': 243296, 'counters/updates': 7603}
train stats after 243328 examples: {'rewards_train/chosen': '0.20723', 'rewards_train/rejected': '0.13097', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.076261', 'logps_train/rejected': '-97.627', 'logps_train/chosen': '-130.53', 'loss/train': '0.68102', 'examples_per_second': '31.525', 'grad_norm': '19.5', 'counters/examples': 243328, 'counters/updates': 7604}
train stats after 243360 examples: {'rewards_train/chosen': '0.24456', 'rewards_train/rejected': '0.082645', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16192', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-134.23', 'loss/train': '0.63595', 'examples_per_second': '31.473', 'grad_norm': '20.125', 'counters/examples': 243360, 'counters/updates': 7605}
train stats after 243392 examples: {'rewards_train/chosen': '0.23884', 'rewards_train/rejected': '0.092963', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14588', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-181.63', 'loss/train': '0.64112', 'examples_per_second': '33.053', 'grad_norm': '22.125', 'counters/examples': 243392, 'counters/updates': 7606}
train stats after 243424 examples: {'rewards_train/chosen': '0.10928', 'rewards_train/rejected': '0.060369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048913', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-120.13', 'loss/train': '0.69469', 'examples_per_second': '31.06', 'grad_norm': '21.5', 'counters/examples': 243424, 'counters/updates': 7607}
train stats after 243456 examples: {'rewards_train/chosen': '0.26523', 'rewards_train/rejected': '0.033682', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23155', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-147.25', 'loss/train': '0.60619', 'examples_per_second': '31.504', 'grad_norm': '20.75', 'counters/examples': 243456, 'counters/updates': 7608}
train stats after 243488 examples: {'rewards_train/chosen': '0.17867', 'rewards_train/rejected': '0.039369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1393', 'logps_train/rejected': '-100.91', 'logps_train/chosen': '-111.92', 'loss/train': '0.6481', 'examples_per_second': '31.683', 'grad_norm': '17.25', 'counters/examples': 243488, 'counters/updates': 7609}
skipping logging after 243520 examples to avoid logging too frequently
train stats after 243552 examples: {'rewards_train/chosen': '0.14074', 'rewards_train/rejected': '0.073758', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066979', 'logps_train/rejected': '-132.84', 'logps_train/chosen': '-158.78', 'loss/train': '0.68786', 'examples_per_second': '31.072', 'grad_norm': '22.125', 'counters/examples': 243552, 'counters/updates': 7611}
train stats after 243584 examples: {'rewards_train/chosen': '0.23196', 'rewards_train/rejected': '0.034195', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19777', 'logps_train/rejected': '-99.712', 'logps_train/chosen': '-121.78', 'loss/train': '0.61654', 'examples_per_second': '31.412', 'grad_norm': '17.625', 'counters/examples': 243584, 'counters/updates': 7612}
train stats after 243616 examples: {'rewards_train/chosen': '0.19295', 'rewards_train/rejected': '0.027553', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1654', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-141.81', 'loss/train': '0.6368', 'examples_per_second': '30.248', 'grad_norm': '21', 'counters/examples': 243616, 'counters/updates': 7613}
train stats after 243648 examples: {'rewards_train/chosen': '0.17425', 'rewards_train/rejected': '0.059902', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11435', 'logps_train/rejected': '-125.47', 'logps_train/chosen': '-148.65', 'loss/train': '0.65255', 'examples_per_second': '30.401', 'grad_norm': '21', 'counters/examples': 243648, 'counters/updates': 7614}
train stats after 243680 examples: {'rewards_train/chosen': '0.17132', 'rewards_train/rejected': '-0.02309', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19441', 'logps_train/rejected': '-101.84', 'logps_train/chosen': '-137.19', 'loss/train': '0.62492', 'examples_per_second': '31.492', 'grad_norm': '18.125', 'counters/examples': 243680, 'counters/updates': 7615}
train stats after 243712 examples: {'rewards_train/chosen': '0.22662', 'rewards_train/rejected': '-0.095941', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.32256', 'logps_train/rejected': '-89.262', 'logps_train/chosen': '-122.15', 'loss/train': '0.56866', 'examples_per_second': '32.47', 'grad_norm': '16.125', 'counters/examples': 243712, 'counters/updates': 7616}
train stats after 243744 examples: {'rewards_train/chosen': '0.16039', 'rewards_train/rejected': '0.014865', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14553', 'logps_train/rejected': '-125.09', 'logps_train/chosen': '-147.9', 'loss/train': '0.6427', 'examples_per_second': '30.026', 'grad_norm': '20.375', 'counters/examples': 243744, 'counters/updates': 7617}
train stats after 243776 examples: {'rewards_train/chosen': '0.15296', 'rewards_train/rejected': '0.047609', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10535', 'logps_train/rejected': '-88.405', 'logps_train/chosen': '-97.31', 'loss/train': '0.6546', 'examples_per_second': '31.28', 'grad_norm': '16.5', 'counters/examples': 243776, 'counters/updates': 7618}
skipping logging after 243808 examples to avoid logging too frequently
train stats after 243840 examples: {'rewards_train/chosen': '0.20881', 'rewards_train/rejected': '0.095862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11295', 'logps_train/rejected': '-141.05', 'logps_train/chosen': '-131.48', 'loss/train': '0.6494', 'examples_per_second': '31.556', 'grad_norm': '20.875', 'counters/examples': 243840, 'counters/updates': 7620}
train stats after 243872 examples: {'rewards_train/chosen': '0.11274', 'rewards_train/rejected': '0.098301', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014436', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-129.44', 'loss/train': '0.71948', 'examples_per_second': '31.485', 'grad_norm': '22.625', 'counters/examples': 243872, 'counters/updates': 7621}
train stats after 243904 examples: {'rewards_train/chosen': '0.18507', 'rewards_train/rejected': '0.028627', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15644', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-137.89', 'loss/train': '0.64217', 'examples_per_second': '31.46', 'grad_norm': '19.75', 'counters/examples': 243904, 'counters/updates': 7622}
train stats after 243936 examples: {'rewards_train/chosen': '0.15147', 'rewards_train/rejected': '-0.026197', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17766', 'logps_train/rejected': '-109.28', 'logps_train/chosen': '-125.6', 'loss/train': '0.62607', 'examples_per_second': '32.868', 'grad_norm': '19.125', 'counters/examples': 243936, 'counters/updates': 7623}
train stats after 243968 examples: {'rewards_train/chosen': '0.21185', 'rewards_train/rejected': '-0.018907', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23076', 'logps_train/rejected': '-117.43', 'logps_train/chosen': '-164.59', 'loss/train': '0.59832', 'examples_per_second': '30.518', 'grad_norm': '20.125', 'counters/examples': 243968, 'counters/updates': 7624}
train stats after 244000 examples: {'rewards_train/chosen': '0.11388', 'rewards_train/rejected': '0.024317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089566', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-136.75', 'loss/train': '0.67429', 'examples_per_second': '31.271', 'grad_norm': '20.875', 'counters/examples': 244000, 'counters/updates': 7625}
train stats after 244032 examples: {'rewards_train/chosen': '0.27214', 'rewards_train/rejected': '0.085343', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18679', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-144.06', 'loss/train': '0.62797', 'examples_per_second': '30.554', 'grad_norm': '20', 'counters/examples': 244032, 'counters/updates': 7626}
skipping logging after 244064 examples to avoid logging too frequently
train stats after 244096 examples: {'rewards_train/chosen': '0.15841', 'rewards_train/rejected': '-0.0080099', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16642', 'logps_train/rejected': '-114.3', 'logps_train/chosen': '-138.77', 'loss/train': '0.62807', 'examples_per_second': '30.878', 'grad_norm': '20', 'counters/examples': 244096, 'counters/updates': 7628}
train stats after 244128 examples: {'rewards_train/chosen': '0.073361', 'rewards_train/rejected': '-0.033376', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10674', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-130.91', 'loss/train': '0.65851', 'examples_per_second': '31.351', 'grad_norm': '19.625', 'counters/examples': 244128, 'counters/updates': 7629}
train stats after 244160 examples: {'rewards_train/chosen': '0.17303', 'rewards_train/rejected': '0.088136', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08489', 'logps_train/rejected': '-102.51', 'logps_train/chosen': '-117.04', 'loss/train': '0.6692', 'examples_per_second': '32.027', 'grad_norm': '18.75', 'counters/examples': 244160, 'counters/updates': 7630}
train stats after 244192 examples: {'rewards_train/chosen': '0.22369', 'rewards_train/rejected': '0.061083', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16261', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-119.05', 'loss/train': '0.62726', 'examples_per_second': '30.614', 'grad_norm': '19.25', 'counters/examples': 244192, 'counters/updates': 7631}
train stats after 244224 examples: {'rewards_train/chosen': '0.15789', 'rewards_train/rejected': '0.07975', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07814', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-143.28', 'loss/train': '0.67269', 'examples_per_second': '30.461', 'grad_norm': '21', 'counters/examples': 244224, 'counters/updates': 7632}
skipping logging after 244256 examples to avoid logging too frequently
train stats after 244288 examples: {'rewards_train/chosen': '0.17461', 'rewards_train/rejected': '0.052311', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1223', 'logps_train/rejected': '-124.71', 'logps_train/chosen': '-129.92', 'loss/train': '0.64458', 'examples_per_second': '32.869', 'grad_norm': '19.375', 'counters/examples': 244288, 'counters/updates': 7634}
skipping logging after 244320 examples to avoid logging too frequently
train stats after 244352 examples: {'rewards_train/chosen': '0.17264', 'rewards_train/rejected': '-0.020838', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19347', 'logps_train/rejected': '-108.31', 'logps_train/chosen': '-115.02', 'loss/train': '0.61704', 'examples_per_second': '33.43', 'grad_norm': '18.125', 'counters/examples': 244352, 'counters/updates': 7636}
skipping logging after 244384 examples to avoid logging too frequently
train stats after 244416 examples: {'rewards_train/chosen': '0.18933', 'rewards_train/rejected': '0.10764', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08169', 'logps_train/rejected': '-112.03', 'logps_train/chosen': '-138.76', 'loss/train': '0.67678', 'examples_per_second': '30.445', 'grad_norm': '20.625', 'counters/examples': 244416, 'counters/updates': 7638}
train stats after 244448 examples: {'rewards_train/chosen': '0.19922', 'rewards_train/rejected': '-0.012646', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21187', 'logps_train/rejected': '-105.02', 'logps_train/chosen': '-121.65', 'loss/train': '0.60347', 'examples_per_second': '29.989', 'grad_norm': '17.5', 'counters/examples': 244448, 'counters/updates': 7639}
train stats after 244480 examples: {'rewards_train/chosen': '0.18322', 'rewards_train/rejected': '-0.030742', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21396', 'logps_train/rejected': '-115.95', 'logps_train/chosen': '-145.61', 'loss/train': '0.6143', 'examples_per_second': '31.43', 'grad_norm': '20.125', 'counters/examples': 244480, 'counters/updates': 7640}
train stats after 244512 examples: {'rewards_train/chosen': '0.23324', 'rewards_train/rejected': '-0.011043', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24428', 'logps_train/rejected': '-108.76', 'logps_train/chosen': '-145.48', 'loss/train': '0.60189', 'examples_per_second': '31.92', 'grad_norm': '18.75', 'counters/examples': 244512, 'counters/updates': 7641}
train stats after 244544 examples: {'rewards_train/chosen': '0.31986', 'rewards_train/rejected': '0.080008', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23985', 'logps_train/rejected': '-105.81', 'logps_train/chosen': '-147.36', 'loss/train': '0.59573', 'examples_per_second': '31.469', 'grad_norm': '20.25', 'counters/examples': 244544, 'counters/updates': 7642}
train stats after 244576 examples: {'rewards_train/chosen': '0.10339', 'rewards_train/rejected': '0.056089', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047299', 'logps_train/rejected': '-118', 'logps_train/chosen': '-126.2', 'loss/train': '0.6843', 'examples_per_second': '32.628', 'grad_norm': '19.5', 'counters/examples': 244576, 'counters/updates': 7643}
train stats after 244608 examples: {'rewards_train/chosen': '0.21495', 'rewards_train/rejected': '0.096118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11883', 'logps_train/rejected': '-135.17', 'logps_train/chosen': '-118.45', 'loss/train': '0.65451', 'examples_per_second': '31.38', 'grad_norm': '21.625', 'counters/examples': 244608, 'counters/updates': 7644}
train stats after 244640 examples: {'rewards_train/chosen': '0.25706', 'rewards_train/rejected': '0.027363', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2297', 'logps_train/rejected': '-127.29', 'logps_train/chosen': '-131.68', 'loss/train': '0.60866', 'examples_per_second': '31.026', 'grad_norm': '19.875', 'counters/examples': 244640, 'counters/updates': 7645}
train stats after 244672 examples: {'rewards_train/chosen': '0.26233', 'rewards_train/rejected': '0.049192', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21314', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-119.36', 'loss/train': '0.61441', 'examples_per_second': '32.848', 'grad_norm': '18.875', 'counters/examples': 244672, 'counters/updates': 7646}
skipping logging after 244704 examples to avoid logging too frequently
train stats after 244736 examples: {'rewards_train/chosen': '0.16657', 'rewards_train/rejected': '0.069907', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096668', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-119.16', 'loss/train': '0.65864', 'examples_per_second': '31.537', 'grad_norm': '18.25', 'counters/examples': 244736, 'counters/updates': 7648}
train stats after 244768 examples: {'rewards_train/chosen': '0.17901', 'rewards_train/rejected': '-0.023517', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20253', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-122.79', 'loss/train': '0.622', 'examples_per_second': '30.854', 'grad_norm': '19.625', 'counters/examples': 244768, 'counters/updates': 7649}
train stats after 244800 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.067887', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.072274', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-117.73', 'loss/train': '0.67544', 'examples_per_second': '32.256', 'grad_norm': '19', 'counters/examples': 244800, 'counters/updates': 7650}
train stats after 244832 examples: {'rewards_train/chosen': '0.02552', 'rewards_train/rejected': '-0.1019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12742', 'logps_train/rejected': '-149.88', 'logps_train/chosen': '-134.74', 'loss/train': '0.64879', 'examples_per_second': '31.583', 'grad_norm': '21.125', 'counters/examples': 244832, 'counters/updates': 7651}
train stats after 244864 examples: {'rewards_train/chosen': '0.18483', 'rewards_train/rejected': '0.038078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14675', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-129.24', 'loss/train': '0.64712', 'examples_per_second': '32.918', 'grad_norm': '20.5', 'counters/examples': 244864, 'counters/updates': 7652}
train stats after 244896 examples: {'rewards_train/chosen': '0.12586', 'rewards_train/rejected': '-0.045243', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17111', 'logps_train/rejected': '-111.77', 'logps_train/chosen': '-154.63', 'loss/train': '0.63766', 'examples_per_second': '30.402', 'grad_norm': '22.625', 'counters/examples': 244896, 'counters/updates': 7653}
train stats after 244928 examples: {'rewards_train/chosen': '0.10526', 'rewards_train/rejected': '-0.0099548', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11522', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-115.93', 'loss/train': '0.65174', 'examples_per_second': '31.131', 'grad_norm': '18.5', 'counters/examples': 244928, 'counters/updates': 7654}
train stats after 244960 examples: {'rewards_train/chosen': '0.22416', 'rewards_train/rejected': '0.037668', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18649', 'logps_train/rejected': '-134.41', 'logps_train/chosen': '-145.7', 'loss/train': '0.61826', 'examples_per_second': '31.391', 'grad_norm': '20', 'counters/examples': 244960, 'counters/updates': 7655}
train stats after 244992 examples: {'rewards_train/chosen': '0.079253', 'rewards_train/rejected': '-0.0092385', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.088492', 'logps_train/rejected': '-91.945', 'logps_train/chosen': '-140.65', 'loss/train': '0.67659', 'examples_per_second': '31.327', 'grad_norm': '18.375', 'counters/examples': 244992, 'counters/updates': 7656}
train stats after 245024 examples: {'rewards_train/chosen': '0.12699', 'rewards_train/rejected': '0.045495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081495', 'logps_train/rejected': '-114.9', 'logps_train/chosen': '-140.96', 'loss/train': '0.66814', 'examples_per_second': '32.248', 'grad_norm': '21', 'counters/examples': 245024, 'counters/updates': 7657}
train stats after 245056 examples: {'rewards_train/chosen': '0.065143', 'rewards_train/rejected': '-0.067971', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13311', 'logps_train/rejected': '-102.04', 'logps_train/chosen': '-138.63', 'loss/train': '0.64937', 'examples_per_second': '32.941', 'grad_norm': '19.5', 'counters/examples': 245056, 'counters/updates': 7658}
train stats after 245088 examples: {'rewards_train/chosen': '0.13813', 'rewards_train/rejected': '0.062978', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075149', 'logps_train/rejected': '-134.6', 'logps_train/chosen': '-140.85', 'loss/train': '0.67899', 'examples_per_second': '31.438', 'grad_norm': '23', 'counters/examples': 245088, 'counters/updates': 7659}
train stats after 245120 examples: {'rewards_train/chosen': '0.16316', 'rewards_train/rejected': '0.048989', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11417', 'logps_train/rejected': '-123.71', 'logps_train/chosen': '-153.22', 'loss/train': '0.6544', 'examples_per_second': '30.879', 'grad_norm': '20.75', 'counters/examples': 245120, 'counters/updates': 7660}
train stats after 245152 examples: {'rewards_train/chosen': '0.19366', 'rewards_train/rejected': '-0.0094662', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20313', 'logps_train/rejected': '-100.57', 'logps_train/chosen': '-142.18', 'loss/train': '0.61391', 'examples_per_second': '31.032', 'grad_norm': '17.875', 'counters/examples': 245152, 'counters/updates': 7661}
train stats after 245184 examples: {'rewards_train/chosen': '0.15168', 'rewards_train/rejected': '0.04935', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10233', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-115.99', 'loss/train': '0.65428', 'examples_per_second': '30.794', 'grad_norm': '18.25', 'counters/examples': 245184, 'counters/updates': 7662}
skipping logging after 245216 examples to avoid logging too frequently
train stats after 245248 examples: {'rewards_train/chosen': '0.16513', 'rewards_train/rejected': '-0.028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19313', 'logps_train/rejected': '-141.36', 'logps_train/chosen': '-140.56', 'loss/train': '0.62854', 'examples_per_second': '30.54', 'grad_norm': '20.875', 'counters/examples': 245248, 'counters/updates': 7664}
train stats after 245280 examples: {'rewards_train/chosen': '0.25583', 'rewards_train/rejected': '0.20474', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051091', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-134.28', 'loss/train': '0.69152', 'examples_per_second': '31.442', 'grad_norm': '21.5', 'counters/examples': 245280, 'counters/updates': 7665}
train stats after 245312 examples: {'rewards_train/chosen': '0.10009', 'rewards_train/rejected': '0.013335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086754', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-155.07', 'loss/train': '0.671', 'examples_per_second': '30.667', 'grad_norm': '21.75', 'counters/examples': 245312, 'counters/updates': 7666}
train stats after 245344 examples: {'rewards_train/chosen': '0.24333', 'rewards_train/rejected': '0.03102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21231', 'logps_train/rejected': '-107.6', 'logps_train/chosen': '-163.79', 'loss/train': '0.60968', 'examples_per_second': '31.177', 'grad_norm': '20.125', 'counters/examples': 245344, 'counters/updates': 7667}
train stats after 245376 examples: {'rewards_train/chosen': '0.21817', 'rewards_train/rejected': '0.025418', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19275', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-132.26', 'loss/train': '0.62011', 'examples_per_second': '31.299', 'grad_norm': '20', 'counters/examples': 245376, 'counters/updates': 7668}
train stats after 245408 examples: {'rewards_train/chosen': '0.05232', 'rewards_train/rejected': '-0.052253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10457', 'logps_train/rejected': '-92.599', 'logps_train/chosen': '-121.63', 'loss/train': '0.65106', 'examples_per_second': '31.356', 'grad_norm': '17.625', 'counters/examples': 245408, 'counters/updates': 7669}
train stats after 245440 examples: {'rewards_train/chosen': '0.17188', 'rewards_train/rejected': '0.040727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13115', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-124.9', 'loss/train': '0.64754', 'examples_per_second': '33.268', 'grad_norm': '18.75', 'counters/examples': 245440, 'counters/updates': 7670}
train stats after 245472 examples: {'rewards_train/chosen': '0.18247', 'rewards_train/rejected': '0.00048488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18199', 'logps_train/rejected': '-104.7', 'logps_train/chosen': '-138.04', 'loss/train': '0.64306', 'examples_per_second': '31.542', 'grad_norm': '18.875', 'counters/examples': 245472, 'counters/updates': 7671}
train stats after 245504 examples: {'rewards_train/chosen': '0.115', 'rewards_train/rejected': '0.010473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10453', 'logps_train/rejected': '-121.69', 'logps_train/chosen': '-120.56', 'loss/train': '0.65729', 'examples_per_second': '31.159', 'grad_norm': '19.125', 'counters/examples': 245504, 'counters/updates': 7672}
train stats after 245536 examples: {'rewards_train/chosen': '0.15596', 'rewards_train/rejected': '0.040239', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11573', 'logps_train/rejected': '-139.77', 'logps_train/chosen': '-145.07', 'loss/train': '0.65057', 'examples_per_second': '31.002', 'grad_norm': '21', 'counters/examples': 245536, 'counters/updates': 7673}
train stats after 245568 examples: {'rewards_train/chosen': '0.15834', 'rewards_train/rejected': '0.098896', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059445', 'logps_train/rejected': '-143.38', 'logps_train/chosen': '-165', 'loss/train': '0.67854', 'examples_per_second': '31.42', 'grad_norm': '23.5', 'counters/examples': 245568, 'counters/updates': 7674}
train stats after 245600 examples: {'rewards_train/chosen': '0.18298', 'rewards_train/rejected': '-0.052391', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23537', 'logps_train/rejected': '-97.078', 'logps_train/chosen': '-119.52', 'loss/train': '0.59962', 'examples_per_second': '32.216', 'grad_norm': '16.625', 'counters/examples': 245600, 'counters/updates': 7675}
train stats after 245632 examples: {'rewards_train/chosen': '0.13778', 'rewards_train/rejected': '0.064841', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072936', 'logps_train/rejected': '-98.484', 'logps_train/chosen': '-129.68', 'loss/train': '0.67549', 'examples_per_second': '31.301', 'grad_norm': '19.5', 'counters/examples': 245632, 'counters/updates': 7676}
train stats after 245664 examples: {'rewards_train/chosen': '0.14474', 'rewards_train/rejected': '0.082779', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061966', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-154.29', 'loss/train': '0.69461', 'examples_per_second': '31.484', 'grad_norm': '23.875', 'counters/examples': 245664, 'counters/updates': 7677}
train stats after 245696 examples: {'rewards_train/chosen': '0.22111', 'rewards_train/rejected': '-0.034999', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25611', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-145.77', 'loss/train': '0.59575', 'examples_per_second': '31.518', 'grad_norm': '18.125', 'counters/examples': 245696, 'counters/updates': 7678}
train stats after 245728 examples: {'rewards_train/chosen': '0.13199', 'rewards_train/rejected': '0.061856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070133', 'logps_train/rejected': '-105.99', 'logps_train/chosen': '-155.79', 'loss/train': '0.67806', 'examples_per_second': '31.516', 'grad_norm': '20.625', 'counters/examples': 245728, 'counters/updates': 7679}
train stats after 245760 examples: {'rewards_train/chosen': '0.185', 'rewards_train/rejected': '-0.065575', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25057', 'logps_train/rejected': '-104.33', 'logps_train/chosen': '-136.12', 'loss/train': '0.58987', 'examples_per_second': '32.031', 'grad_norm': '17.375', 'counters/examples': 245760, 'counters/updates': 7680}
train stats after 245792 examples: {'rewards_train/chosen': '0.13989', 'rewards_train/rejected': '0.047614', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092273', 'logps_train/rejected': '-133.22', 'logps_train/chosen': '-144.62', 'loss/train': '0.66519', 'examples_per_second': '32.36', 'grad_norm': '22.125', 'counters/examples': 245792, 'counters/updates': 7681}
train stats after 245824 examples: {'rewards_train/chosen': '0.084326', 'rewards_train/rejected': '0.053175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031151', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-116.65', 'loss/train': '0.6971', 'examples_per_second': '32.656', 'grad_norm': '20.625', 'counters/examples': 245824, 'counters/updates': 7682}
train stats after 245856 examples: {'rewards_train/chosen': '0.11488', 'rewards_train/rejected': '0.014008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10087', 'logps_train/rejected': '-82.485', 'logps_train/chosen': '-132.84', 'loss/train': '0.66515', 'examples_per_second': '31.497', 'grad_norm': '18.875', 'counters/examples': 245856, 'counters/updates': 7683}
skipping logging after 245888 examples to avoid logging too frequently
train stats after 245920 examples: {'rewards_train/chosen': '0.13789', 'rewards_train/rejected': '-0.011201', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14909', 'logps_train/rejected': '-92.287', 'logps_train/chosen': '-110.17', 'loss/train': '0.6337', 'examples_per_second': '35.484', 'grad_norm': '17', 'counters/examples': 245920, 'counters/updates': 7685}
train stats after 245952 examples: {'rewards_train/chosen': '0.089456', 'rewards_train/rejected': '0.038784', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050671', 'logps_train/rejected': '-129.35', 'logps_train/chosen': '-135.07', 'loss/train': '0.69569', 'examples_per_second': '31.393', 'grad_norm': '22.5', 'counters/examples': 245952, 'counters/updates': 7686}
train stats after 245984 examples: {'rewards_train/chosen': '0.27341', 'rewards_train/rejected': '0.10362', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1698', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-121.97', 'loss/train': '0.62649', 'examples_per_second': '31.524', 'grad_norm': '18.875', 'counters/examples': 245984, 'counters/updates': 7687}
train stats after 246016 examples: {'rewards_train/chosen': '0.15706', 'rewards_train/rejected': '0.024004', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13306', 'logps_train/rejected': '-136.83', 'logps_train/chosen': '-126.36', 'loss/train': '0.64179', 'examples_per_second': '32.434', 'grad_norm': '20.25', 'counters/examples': 246016, 'counters/updates': 7688}
skipping logging after 246048 examples to avoid logging too frequently
train stats after 246080 examples: {'rewards_train/chosen': '0.23937', 'rewards_train/rejected': '0.071687', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16768', 'logps_train/rejected': '-147.66', 'logps_train/chosen': '-139.33', 'loss/train': '0.62417', 'examples_per_second': '30.198', 'grad_norm': '21.25', 'counters/examples': 246080, 'counters/updates': 7690}
train stats after 246112 examples: {'rewards_train/chosen': '0.16582', 'rewards_train/rejected': '0.0014393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16439', 'logps_train/rejected': '-87.573', 'logps_train/chosen': '-133.42', 'loss/train': '0.62796', 'examples_per_second': '22.983', 'grad_norm': '18.75', 'counters/examples': 246112, 'counters/updates': 7691}
train stats after 246144 examples: {'rewards_train/chosen': '0.26298', 'rewards_train/rejected': '0.11529', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14769', 'logps_train/rejected': '-95.625', 'logps_train/chosen': '-124.3', 'loss/train': '0.63629', 'examples_per_second': '31.072', 'grad_norm': '17.375', 'counters/examples': 246144, 'counters/updates': 7692}
skipping logging after 246176 examples to avoid logging too frequently
train stats after 246208 examples: {'rewards_train/chosen': '0.24218', 'rewards_train/rejected': '0.025644', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21653', 'logps_train/rejected': '-112.08', 'logps_train/chosen': '-132.27', 'loss/train': '0.6176', 'examples_per_second': '24.669', 'grad_norm': '18.625', 'counters/examples': 246208, 'counters/updates': 7694}
train stats after 246240 examples: {'rewards_train/chosen': '0.095125', 'rewards_train/rejected': '-0.035589', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13071', 'logps_train/rejected': '-129.07', 'logps_train/chosen': '-101.81', 'loss/train': '0.66252', 'examples_per_second': '32.422', 'grad_norm': '19.5', 'counters/examples': 246240, 'counters/updates': 7695}
train stats after 246272 examples: {'rewards_train/chosen': '0.19288', 'rewards_train/rejected': '-0.065703', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25858', 'logps_train/rejected': '-85.655', 'logps_train/chosen': '-127.97', 'loss/train': '0.59134', 'examples_per_second': '32.167', 'grad_norm': '17', 'counters/examples': 246272, 'counters/updates': 7696}
train stats after 246304 examples: {'rewards_train/chosen': '0.14788', 'rewards_train/rejected': '0.075315', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07257', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-174.78', 'loss/train': '0.68601', 'examples_per_second': '30.498', 'grad_norm': '23.5', 'counters/examples': 246304, 'counters/updates': 7697}
train stats after 246336 examples: {'rewards_train/chosen': '0.33286', 'rewards_train/rejected': '0.066125', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26673', 'logps_train/rejected': '-108.48', 'logps_train/chosen': '-140.42', 'loss/train': '0.59929', 'examples_per_second': '31.01', 'grad_norm': '19.25', 'counters/examples': 246336, 'counters/updates': 7698}
train stats after 246368 examples: {'rewards_train/chosen': '0.056192', 'rewards_train/rejected': '0.060766', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0045734', 'logps_train/rejected': '-142.49', 'logps_train/chosen': '-107.11', 'loss/train': '0.70888', 'examples_per_second': '31.445', 'grad_norm': '22.125', 'counters/examples': 246368, 'counters/updates': 7699}
train stats after 246400 examples: {'rewards_train/chosen': '0.1736', 'rewards_train/rejected': '-0.0022087', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17581', 'logps_train/rejected': '-134.94', 'logps_train/chosen': '-110.52', 'loss/train': '0.62302', 'examples_per_second': '32.907', 'grad_norm': '18.75', 'counters/examples': 246400, 'counters/updates': 7700}
train stats after 246432 examples: {'rewards_train/chosen': '0.2037', 'rewards_train/rejected': '0.0048867', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19881', 'logps_train/rejected': '-122.84', 'logps_train/chosen': '-161.91', 'loss/train': '0.62205', 'examples_per_second': '30.295', 'grad_norm': '20.25', 'counters/examples': 246432, 'counters/updates': 7701}
skipping logging after 246464 examples to avoid logging too frequently
train stats after 246496 examples: {'rewards_train/chosen': '0.15685', 'rewards_train/rejected': '0.093552', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063294', 'logps_train/rejected': '-100.56', 'logps_train/chosen': '-116.16', 'loss/train': '0.68161', 'examples_per_second': '31.44', 'grad_norm': '19.125', 'counters/examples': 246496, 'counters/updates': 7703}
train stats after 246528 examples: {'rewards_train/chosen': '0.16097', 'rewards_train/rejected': '0.13208', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028892', 'logps_train/rejected': '-133.41', 'logps_train/chosen': '-124.79', 'loss/train': '0.69431', 'examples_per_second': '31.379', 'grad_norm': '21.875', 'counters/examples': 246528, 'counters/updates': 7704}
train stats after 246560 examples: {'rewards_train/chosen': '0.16758', 'rewards_train/rejected': '0.067513', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10007', 'logps_train/rejected': '-101.4', 'logps_train/chosen': '-97.576', 'loss/train': '0.66459', 'examples_per_second': '30.322', 'grad_norm': '17.5', 'counters/examples': 246560, 'counters/updates': 7705}
train stats after 246592 examples: {'rewards_train/chosen': '0.174', 'rewards_train/rejected': '0.027974', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14602', 'logps_train/rejected': '-106.29', 'logps_train/chosen': '-117.43', 'loss/train': '0.64416', 'examples_per_second': '31.193', 'grad_norm': '18.125', 'counters/examples': 246592, 'counters/updates': 7706}
train stats after 246624 examples: {'rewards_train/chosen': '0.23589', 'rewards_train/rejected': '0.025157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21073', 'logps_train/rejected': '-96.463', 'logps_train/chosen': '-138.84', 'loss/train': '0.60817', 'examples_per_second': '30.177', 'grad_norm': '17.875', 'counters/examples': 246624, 'counters/updates': 7707}
train stats after 246656 examples: {'rewards_train/chosen': '0.26485', 'rewards_train/rejected': '0.21702', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047832', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-126.59', 'loss/train': '0.68367', 'examples_per_second': '30.543', 'grad_norm': '20.625', 'counters/examples': 246656, 'counters/updates': 7708}
train stats after 246688 examples: {'rewards_train/chosen': '0.14208', 'rewards_train/rejected': '0.083227', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058849', 'logps_train/rejected': '-103.24', 'logps_train/chosen': '-104.86', 'loss/train': '0.68028', 'examples_per_second': '31.728', 'grad_norm': '18.375', 'counters/examples': 246688, 'counters/updates': 7709}
skipping logging after 246720 examples to avoid logging too frequently
train stats after 246752 examples: {'rewards_train/chosen': '0.15532', 'rewards_train/rejected': '0.080469', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074847', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-159.53', 'loss/train': '0.67283', 'examples_per_second': '31.348', 'grad_norm': '21.25', 'counters/examples': 246752, 'counters/updates': 7711}
train stats after 246784 examples: {'rewards_train/chosen': '0.15316', 'rewards_train/rejected': '0.044131', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10902', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-125.64', 'loss/train': '0.66017', 'examples_per_second': '23.923', 'grad_norm': '18.5', 'counters/examples': 246784, 'counters/updates': 7712}
train stats after 246816 examples: {'rewards_train/chosen': '0.15511', 'rewards_train/rejected': '0.063394', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091718', 'logps_train/rejected': '-107.4', 'logps_train/chosen': '-101.58', 'loss/train': '0.67411', 'examples_per_second': '32.166', 'grad_norm': '18.5', 'counters/examples': 246816, 'counters/updates': 7713}
train stats after 246848 examples: {'rewards_train/chosen': '0.21475', 'rewards_train/rejected': '0.12534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08941', 'logps_train/rejected': '-123.43', 'logps_train/chosen': '-128.11', 'loss/train': '0.66908', 'examples_per_second': '31.43', 'grad_norm': '21', 'counters/examples': 246848, 'counters/updates': 7714}
train stats after 246880 examples: {'rewards_train/chosen': '0.19235', 'rewards_train/rejected': '0.10546', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086883', 'logps_train/rejected': '-111.5', 'logps_train/chosen': '-137.26', 'loss/train': '0.66564', 'examples_per_second': '31.606', 'grad_norm': '19.5', 'counters/examples': 246880, 'counters/updates': 7715}
skipping logging after 246912 examples to avoid logging too frequently
train stats after 246944 examples: {'rewards_train/chosen': '0.12415', 'rewards_train/rejected': '0.083094', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041059', 'logps_train/rejected': '-106.3', 'logps_train/chosen': '-132.45', 'loss/train': '0.68072', 'examples_per_second': '31.315', 'grad_norm': '19.75', 'counters/examples': 246944, 'counters/updates': 7717}
train stats after 246976 examples: {'rewards_train/chosen': '0.21928', 'rewards_train/rejected': '0.071307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14798', 'logps_train/rejected': '-135.2', 'logps_train/chosen': '-169.16', 'loss/train': '0.64136', 'examples_per_second': '30.014', 'grad_norm': '22.875', 'counters/examples': 246976, 'counters/updates': 7718}
train stats after 247008 examples: {'rewards_train/chosen': '0.15517', 'rewards_train/rejected': '0.05977', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.095402', 'logps_train/rejected': '-144.34', 'logps_train/chosen': '-137.82', 'loss/train': '0.66305', 'examples_per_second': '31.501', 'grad_norm': '21.875', 'counters/examples': 247008, 'counters/updates': 7719}
skipping logging after 247040 examples to avoid logging too frequently
train stats after 247072 examples: {'rewards_train/chosen': '0.14988', 'rewards_train/rejected': '0.082886', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066998', 'logps_train/rejected': '-132.28', 'logps_train/chosen': '-104.21', 'loss/train': '0.67708', 'examples_per_second': '30.215', 'grad_norm': '20.625', 'counters/examples': 247072, 'counters/updates': 7721}
train stats after 247104 examples: {'rewards_train/chosen': '0.22268', 'rewards_train/rejected': '0.087267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13541', 'logps_train/rejected': '-147.64', 'logps_train/chosen': '-169.49', 'loss/train': '0.643', 'examples_per_second': '30.012', 'grad_norm': '23.375', 'counters/examples': 247104, 'counters/updates': 7722}
train stats after 247136 examples: {'rewards_train/chosen': '0.1318', 'rewards_train/rejected': '0.020171', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11163', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-109.87', 'loss/train': '0.6516', 'examples_per_second': '31.45', 'grad_norm': '20', 'counters/examples': 247136, 'counters/updates': 7723}
train stats after 247168 examples: {'rewards_train/chosen': '0.17364', 'rewards_train/rejected': '-0.052886', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22652', 'logps_train/rejected': '-112.3', 'logps_train/chosen': '-166.46', 'loss/train': '0.60182', 'examples_per_second': '32.881', 'grad_norm': '19', 'counters/examples': 247168, 'counters/updates': 7724}
train stats after 247200 examples: {'rewards_train/chosen': '0.18039', 'rewards_train/rejected': '0.14227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038117', 'logps_train/rejected': '-155.24', 'logps_train/chosen': '-153.56', 'loss/train': '0.69285', 'examples_per_second': '30.764', 'grad_norm': '24', 'counters/examples': 247200, 'counters/updates': 7725}
train stats after 247232 examples: {'rewards_train/chosen': '0.13808', 'rewards_train/rejected': '-0.0033821', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14146', 'logps_train/rejected': '-103.94', 'logps_train/chosen': '-146.58', 'loss/train': '0.64566', 'examples_per_second': '31.268', 'grad_norm': '19.75', 'counters/examples': 247232, 'counters/updates': 7726}
skipping logging after 247264 examples to avoid logging too frequently
train stats after 247296 examples: {'rewards_train/chosen': '0.16764', 'rewards_train/rejected': '0.074777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092867', 'logps_train/rejected': '-156.73', 'logps_train/chosen': '-139.84', 'loss/train': '0.66117', 'examples_per_second': '30.36', 'grad_norm': '22.625', 'counters/examples': 247296, 'counters/updates': 7728}
train stats after 247328 examples: {'rewards_train/chosen': '0.2127', 'rewards_train/rejected': '0.091378', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12132', 'logps_train/rejected': '-143', 'logps_train/chosen': '-157.91', 'loss/train': '0.65282', 'examples_per_second': '30.014', 'grad_norm': '20.75', 'counters/examples': 247328, 'counters/updates': 7729}
skipping logging after 247360 examples to avoid logging too frequently
train stats after 247392 examples: {'rewards_train/chosen': '0.19399', 'rewards_train/rejected': '0.032542', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16144', 'logps_train/rejected': '-115.12', 'logps_train/chosen': '-166.21', 'loss/train': '0.62793', 'examples_per_second': '31.38', 'grad_norm': '20.375', 'counters/examples': 247392, 'counters/updates': 7731}
train stats after 247424 examples: {'rewards_train/chosen': '0.20438', 'rewards_train/rejected': '-0.063742', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26812', 'logps_train/rejected': '-101.64', 'logps_train/chosen': '-114.53', 'loss/train': '0.58468', 'examples_per_second': '31.842', 'grad_norm': '16.625', 'counters/examples': 247424, 'counters/updates': 7732}
train stats after 247456 examples: {'rewards_train/chosen': '0.2038', 'rewards_train/rejected': '0.079124', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12467', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-119.47', 'loss/train': '0.66032', 'examples_per_second': '30.398', 'grad_norm': '19.5', 'counters/examples': 247456, 'counters/updates': 7733}
train stats after 247488 examples: {'rewards_train/chosen': '0.11415', 'rewards_train/rejected': '0.054914', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059238', 'logps_train/rejected': '-107.18', 'logps_train/chosen': '-128.92', 'loss/train': '0.68434', 'examples_per_second': '31.929', 'grad_norm': '20.5', 'counters/examples': 247488, 'counters/updates': 7734}
skipping logging after 247520 examples to avoid logging too frequently
train stats after 247552 examples: {'rewards_train/chosen': '0.22601', 'rewards_train/rejected': '0.043979', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18203', 'logps_train/rejected': '-96.924', 'logps_train/chosen': '-141.23', 'loss/train': '0.61927', 'examples_per_second': '32.474', 'grad_norm': '18.25', 'counters/examples': 247552, 'counters/updates': 7736}
train stats after 247584 examples: {'rewards_train/chosen': '0.26727', 'rewards_train/rejected': '0.0030017', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26426', 'logps_train/rejected': '-130.77', 'logps_train/chosen': '-123.35', 'loss/train': '0.59044', 'examples_per_second': '32.818', 'grad_norm': '19.25', 'counters/examples': 247584, 'counters/updates': 7737}
train stats after 247616 examples: {'rewards_train/chosen': '0.20345', 'rewards_train/rejected': '0.039976', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16347', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-126.89', 'loss/train': '0.62949', 'examples_per_second': '30.824', 'grad_norm': '19.125', 'counters/examples': 247616, 'counters/updates': 7738}
train stats after 247648 examples: {'rewards_train/chosen': '0.17978', 'rewards_train/rejected': '0.056193', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12359', 'logps_train/rejected': '-108.79', 'logps_train/chosen': '-134.44', 'loss/train': '0.64708', 'examples_per_second': '32.935', 'grad_norm': '20', 'counters/examples': 247648, 'counters/updates': 7739}
train stats after 247680 examples: {'rewards_train/chosen': '0.10536', 'rewards_train/rejected': '-0.043722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14908', 'logps_train/rejected': '-95.938', 'logps_train/chosen': '-118.41', 'loss/train': '0.63844', 'examples_per_second': '32.365', 'grad_norm': '17.75', 'counters/examples': 247680, 'counters/updates': 7740}
train stats after 247712 examples: {'rewards_train/chosen': '0.022936', 'rewards_train/rejected': '0.019546', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0033905', 'logps_train/rejected': '-101.62', 'logps_train/chosen': '-159.1', 'loss/train': '0.71036', 'examples_per_second': '31.464', 'grad_norm': '24.375', 'counters/examples': 247712, 'counters/updates': 7741}
train stats after 247744 examples: {'rewards_train/chosen': '0.1173', 'rewards_train/rejected': '0.04945', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067852', 'logps_train/rejected': '-114.01', 'logps_train/chosen': '-112.55', 'loss/train': '0.66778', 'examples_per_second': '32.696', 'grad_norm': '19.25', 'counters/examples': 247744, 'counters/updates': 7742}
train stats after 247776 examples: {'rewards_train/chosen': '0.13451', 'rewards_train/rejected': '0.09696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037555', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-111.05', 'loss/train': '0.69478', 'examples_per_second': '33.06', 'grad_norm': '20.375', 'counters/examples': 247776, 'counters/updates': 7743}
train stats after 247808 examples: {'rewards_train/chosen': '0.14725', 'rewards_train/rejected': '-0.068932', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21618', 'logps_train/rejected': '-82.323', 'logps_train/chosen': '-104.78', 'loss/train': '0.60741', 'examples_per_second': '31.48', 'grad_norm': '17.625', 'counters/examples': 247808, 'counters/updates': 7744}
train stats after 247840 examples: {'rewards_train/chosen': '0.12025', 'rewards_train/rejected': '0.064783', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055462', 'logps_train/rejected': '-91.04', 'logps_train/chosen': '-111.75', 'loss/train': '0.67884', 'examples_per_second': '30.423', 'grad_norm': '17.625', 'counters/examples': 247840, 'counters/updates': 7745}
train stats after 247872 examples: {'rewards_train/chosen': '0.20796', 'rewards_train/rejected': '0.11298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.094981', 'logps_train/rejected': '-104.59', 'logps_train/chosen': '-109.68', 'loss/train': '0.66252', 'examples_per_second': '30.496', 'grad_norm': '19', 'counters/examples': 247872, 'counters/updates': 7746}
train stats after 247904 examples: {'rewards_train/chosen': '0.14546', 'rewards_train/rejected': '-0.032414', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17788', 'logps_train/rejected': '-117.2', 'logps_train/chosen': '-129.66', 'loss/train': '0.61965', 'examples_per_second': '31.269', 'grad_norm': '17.875', 'counters/examples': 247904, 'counters/updates': 7747}
train stats after 247936 examples: {'rewards_train/chosen': '0.1738', 'rewards_train/rejected': '0.10566', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068143', 'logps_train/rejected': '-135.58', 'logps_train/chosen': '-150.13', 'loss/train': '0.68794', 'examples_per_second': '33.094', 'grad_norm': '22.5', 'counters/examples': 247936, 'counters/updates': 7748}
train stats after 247968 examples: {'rewards_train/chosen': '0.24617', 'rewards_train/rejected': '0.013542', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23263', 'logps_train/rejected': '-116.08', 'logps_train/chosen': '-132.69', 'loss/train': '0.59578', 'examples_per_second': '30.506', 'grad_norm': '20.125', 'counters/examples': 247968, 'counters/updates': 7749}
train stats after 248000 examples: {'rewards_train/chosen': '0.15021', 'rewards_train/rejected': '0.071606', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078605', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-112.3', 'loss/train': '0.67272', 'examples_per_second': '31.195', 'grad_norm': '19', 'counters/examples': 248000, 'counters/updates': 7750}
train stats after 248032 examples: {'rewards_train/chosen': '0.28717', 'rewards_train/rejected': '0.025999', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26117', 'logps_train/rejected': '-119.43', 'logps_train/chosen': '-172.22', 'loss/train': '0.60592', 'examples_per_second': '31.143', 'grad_norm': '20.375', 'counters/examples': 248032, 'counters/updates': 7751}
train stats after 248064 examples: {'rewards_train/chosen': '0.18067', 'rewards_train/rejected': '0.025245', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15543', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-137.66', 'loss/train': '0.63328', 'examples_per_second': '31.356', 'grad_norm': '18.75', 'counters/examples': 248064, 'counters/updates': 7752}
train stats after 248096 examples: {'rewards_train/chosen': '0.16748', 'rewards_train/rejected': '0.044292', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12319', 'logps_train/rejected': '-113.91', 'logps_train/chosen': '-120.6', 'loss/train': '0.64528', 'examples_per_second': '30.338', 'grad_norm': '19.125', 'counters/examples': 248096, 'counters/updates': 7753}
train stats after 248128 examples: {'rewards_train/chosen': '0.2019', 'rewards_train/rejected': '-0.016476', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21838', 'logps_train/rejected': '-132.66', 'logps_train/chosen': '-171.94', 'loss/train': '0.6087', 'examples_per_second': '31.603', 'grad_norm': '20.875', 'counters/examples': 248128, 'counters/updates': 7754}
train stats after 248160 examples: {'rewards_train/chosen': '0.13453', 'rewards_train/rejected': '0.011132', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1234', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-122.47', 'loss/train': '0.64938', 'examples_per_second': '30.813', 'grad_norm': '18.125', 'counters/examples': 248160, 'counters/updates': 7755}
train stats after 248192 examples: {'rewards_train/chosen': '0.11079', 'rewards_train/rejected': '0.054389', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056401', 'logps_train/rejected': '-99.788', 'logps_train/chosen': '-122.25', 'loss/train': '0.68341', 'examples_per_second': '31.599', 'grad_norm': '18.875', 'counters/examples': 248192, 'counters/updates': 7756}
train stats after 248224 examples: {'rewards_train/chosen': '0.23984', 'rewards_train/rejected': '0.0073501', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23249', 'logps_train/rejected': '-94.813', 'logps_train/chosen': '-157.22', 'loss/train': '0.60987', 'examples_per_second': '32.849', 'grad_norm': '19.875', 'counters/examples': 248224, 'counters/updates': 7757}
skipping logging after 248256 examples to avoid logging too frequently
train stats after 248288 examples: {'rewards_train/chosen': '0.21442', 'rewards_train/rejected': '0.066791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14763', 'logps_train/rejected': '-146.88', 'logps_train/chosen': '-129.88', 'loss/train': '0.64709', 'examples_per_second': '31.409', 'grad_norm': '20.875', 'counters/examples': 248288, 'counters/updates': 7759}
train stats after 248320 examples: {'rewards_train/chosen': '0.21703', 'rewards_train/rejected': '0.036305', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18072', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-116.76', 'loss/train': '0.62875', 'examples_per_second': '30.513', 'grad_norm': '19.5', 'counters/examples': 248320, 'counters/updates': 7760}
train stats after 248352 examples: {'rewards_train/chosen': '0.14338', 'rewards_train/rejected': '-0.0081223', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1515', 'logps_train/rejected': '-115.1', 'logps_train/chosen': '-155.38', 'loss/train': '0.63655', 'examples_per_second': '33.056', 'grad_norm': '20.125', 'counters/examples': 248352, 'counters/updates': 7761}
skipping logging after 248384 examples to avoid logging too frequently
train stats after 248416 examples: {'rewards_train/chosen': '0.22154', 'rewards_train/rejected': '0.038884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18266', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-125.81', 'loss/train': '0.62272', 'examples_per_second': '30.18', 'grad_norm': '19.5', 'counters/examples': 248416, 'counters/updates': 7763}
skipping logging after 248448 examples to avoid logging too frequently
train stats after 248480 examples: {'rewards_train/chosen': '0.25482', 'rewards_train/rejected': '-0.0053369', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26015', 'logps_train/rejected': '-93.395', 'logps_train/chosen': '-129.85', 'loss/train': '0.58861', 'examples_per_second': '31.442', 'grad_norm': '18.25', 'counters/examples': 248480, 'counters/updates': 7765}
train stats after 248512 examples: {'rewards_train/chosen': '0.24175', 'rewards_train/rejected': '0.086482', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15527', 'logps_train/rejected': '-136.83', 'logps_train/chosen': '-150.31', 'loss/train': '0.63305', 'examples_per_second': '31.05', 'grad_norm': '21.5', 'counters/examples': 248512, 'counters/updates': 7766}
train stats after 248544 examples: {'rewards_train/chosen': '0.28183', 'rewards_train/rejected': '0.1397', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14213', 'logps_train/rejected': '-126', 'logps_train/chosen': '-149.87', 'loss/train': '0.64329', 'examples_per_second': '31.451', 'grad_norm': '22.125', 'counters/examples': 248544, 'counters/updates': 7767}
train stats after 248576 examples: {'rewards_train/chosen': '0.16265', 'rewards_train/rejected': '0.1313', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03135', 'logps_train/rejected': '-151.16', 'logps_train/chosen': '-149.6', 'loss/train': '0.69135', 'examples_per_second': '29.877', 'grad_norm': '23.25', 'counters/examples': 248576, 'counters/updates': 7768}
train stats after 248608 examples: {'rewards_train/chosen': '0.24916', 'rewards_train/rejected': '0.083785', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16537', 'logps_train/rejected': '-157.7', 'logps_train/chosen': '-131.97', 'loss/train': '0.63211', 'examples_per_second': '31.463', 'grad_norm': '21', 'counters/examples': 248608, 'counters/updates': 7769}
train stats after 248640 examples: {'rewards_train/chosen': '0.16427', 'rewards_train/rejected': '-0.010148', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17442', 'logps_train/rejected': '-117.84', 'logps_train/chosen': '-123.62', 'loss/train': '0.62392', 'examples_per_second': '31.46', 'grad_norm': '18.625', 'counters/examples': 248640, 'counters/updates': 7770}
skipping logging after 248672 examples to avoid logging too frequently
train stats after 248704 examples: {'rewards_train/chosen': '0.1913', 'rewards_train/rejected': '0.048776', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14253', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-118.66', 'loss/train': '0.64061', 'examples_per_second': '32.828', 'grad_norm': '19', 'counters/examples': 248704, 'counters/updates': 7772}
train stats after 248736 examples: {'rewards_train/chosen': '0.20883', 'rewards_train/rejected': '0.16211', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046714', 'logps_train/rejected': '-187.65', 'logps_train/chosen': '-170.71', 'loss/train': '0.68473', 'examples_per_second': '31.441', 'grad_norm': '24.875', 'counters/examples': 248736, 'counters/updates': 7773}
train stats after 248768 examples: {'rewards_train/chosen': '0.25139', 'rewards_train/rejected': '0.1248', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12659', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-129.21', 'loss/train': '0.65343', 'examples_per_second': '31.095', 'grad_norm': '19', 'counters/examples': 248768, 'counters/updates': 7774}
skipping logging after 248800 examples to avoid logging too frequently
train stats after 248832 examples: {'rewards_train/chosen': '0.24271', 'rewards_train/rejected': '0.061421', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18129', 'logps_train/rejected': '-126.47', 'logps_train/chosen': '-148.38', 'loss/train': '0.61758', 'examples_per_second': '29.989', 'grad_norm': '20.125', 'counters/examples': 248832, 'counters/updates': 7776}
train stats after 248864 examples: {'rewards_train/chosen': '0.13491', 'rewards_train/rejected': '0.085413', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.049498', 'logps_train/rejected': '-131.12', 'logps_train/chosen': '-136.95', 'loss/train': '0.68846', 'examples_per_second': '29.97', 'grad_norm': '22.25', 'counters/examples': 248864, 'counters/updates': 7777}
train stats after 248896 examples: {'rewards_train/chosen': '0.17156', 'rewards_train/rejected': '0.098918', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072637', 'logps_train/rejected': '-111.21', 'logps_train/chosen': '-156.27', 'loss/train': '0.6797', 'examples_per_second': '32.965', 'grad_norm': '21.25', 'counters/examples': 248896, 'counters/updates': 7778}
skipping logging after 248928 examples to avoid logging too frequently
train stats after 248960 examples: {'rewards_train/chosen': '0.15506', 'rewards_train/rejected': '0.075216', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079842', 'logps_train/rejected': '-116.17', 'logps_train/chosen': '-132.73', 'loss/train': '0.67358', 'examples_per_second': '31.255', 'grad_norm': '19.75', 'counters/examples': 248960, 'counters/updates': 7780}
train stats after 248992 examples: {'rewards_train/chosen': '0.22342', 'rewards_train/rejected': '-0.084125', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.30755', 'logps_train/rejected': '-108.85', 'logps_train/chosen': '-200.04', 'loss/train': '0.56855', 'examples_per_second': '32.723', 'grad_norm': '19.75', 'counters/examples': 248992, 'counters/updates': 7781}
skipping logging after 249024 examples to avoid logging too frequently
train stats after 249056 examples: {'rewards_train/chosen': '0.1778', 'rewards_train/rejected': '0.051078', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12672', 'logps_train/rejected': '-90.683', 'logps_train/chosen': '-109.76', 'loss/train': '0.6579', 'examples_per_second': '30.15', 'grad_norm': '18.125', 'counters/examples': 249056, 'counters/updates': 7783}
train stats after 249088 examples: {'rewards_train/chosen': '0.30525', 'rewards_train/rejected': '0.071258', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.234', 'logps_train/rejected': '-120.81', 'logps_train/chosen': '-152.17', 'loss/train': '0.61262', 'examples_per_second': '31.237', 'grad_norm': '19.875', 'counters/examples': 249088, 'counters/updates': 7784}
train stats after 249120 examples: {'rewards_train/chosen': '0.15019', 'rewards_train/rejected': '0.099533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050657', 'logps_train/rejected': '-137.17', 'logps_train/chosen': '-151.92', 'loss/train': '0.6823', 'examples_per_second': '31.434', 'grad_norm': '21.375', 'counters/examples': 249120, 'counters/updates': 7785}
skipping logging after 249152 examples to avoid logging too frequently
train stats after 249184 examples: {'rewards_train/chosen': '0.11402', 'rewards_train/rejected': '0.027718', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086298', 'logps_train/rejected': '-128.24', 'logps_train/chosen': '-110.43', 'loss/train': '0.66457', 'examples_per_second': '30.593', 'grad_norm': '18.875', 'counters/examples': 249184, 'counters/updates': 7787}
train stats after 249216 examples: {'rewards_train/chosen': '0.23654', 'rewards_train/rejected': '0.010519', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22602', 'logps_train/rejected': '-102.52', 'logps_train/chosen': '-166.03', 'loss/train': '0.60799', 'examples_per_second': '30.419', 'grad_norm': '18.875', 'counters/examples': 249216, 'counters/updates': 7788}
skipping logging after 249248 examples to avoid logging too frequently
train stats after 249280 examples: {'rewards_train/chosen': '0.19786', 'rewards_train/rejected': '0.037832', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16003', 'logps_train/rejected': '-136.33', 'logps_train/chosen': '-149.69', 'loss/train': '0.63721', 'examples_per_second': '29.761', 'grad_norm': '20.75', 'counters/examples': 249280, 'counters/updates': 7790}
train stats after 249312 examples: {'rewards_train/chosen': '0.074456', 'rewards_train/rejected': '0.012612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061845', 'logps_train/rejected': '-115.38', 'logps_train/chosen': '-88.078', 'loss/train': '0.67962', 'examples_per_second': '30.717', 'grad_norm': '18', 'counters/examples': 249312, 'counters/updates': 7791}
train stats after 249344 examples: {'rewards_train/chosen': '0.26565', 'rewards_train/rejected': '0.017656', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24799', 'logps_train/rejected': '-98.327', 'logps_train/chosen': '-141.12', 'loss/train': '0.60476', 'examples_per_second': '30.338', 'grad_norm': '18.5', 'counters/examples': 249344, 'counters/updates': 7792}
train stats after 249376 examples: {'rewards_train/chosen': '0.16718', 'rewards_train/rejected': '0.049637', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11755', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-138.61', 'loss/train': '0.673', 'examples_per_second': '31.072', 'grad_norm': '24.5', 'counters/examples': 249376, 'counters/updates': 7793}
train stats after 249408 examples: {'rewards_train/chosen': '0.19807', 'rewards_train/rejected': '0.088873', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1092', 'logps_train/rejected': '-99.657', 'logps_train/chosen': '-157.67', 'loss/train': '0.65325', 'examples_per_second': '32.008', 'grad_norm': '21.125', 'counters/examples': 249408, 'counters/updates': 7794}
train stats after 249440 examples: {'rewards_train/chosen': '0.19165', 'rewards_train/rejected': '-0.017496', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20915', 'logps_train/rejected': '-153.71', 'logps_train/chosen': '-170.14', 'loss/train': '0.62544', 'examples_per_second': '30.948', 'grad_norm': '21.875', 'counters/examples': 249440, 'counters/updates': 7795}
skipping logging after 249472 examples to avoid logging too frequently
train stats after 249504 examples: {'rewards_train/chosen': '0.11441', 'rewards_train/rejected': '0.013403', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.101', 'logps_train/rejected': '-85.783', 'logps_train/chosen': '-109.4', 'loss/train': '0.6658', 'examples_per_second': '40.912', 'grad_norm': '17.375', 'counters/examples': 249504, 'counters/updates': 7797}
train stats after 249536 examples: {'rewards_train/chosen': '0.13523', 'rewards_train/rejected': '0.09405', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041177', 'logps_train/rejected': '-96.611', 'logps_train/chosen': '-103.35', 'loss/train': '0.68361', 'examples_per_second': '30.893', 'grad_norm': '18', 'counters/examples': 249536, 'counters/updates': 7798}
train stats after 249568 examples: {'rewards_train/chosen': '0.18652', 'rewards_train/rejected': '-0.049433', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23595', 'logps_train/rejected': '-113.56', 'logps_train/chosen': '-123.94', 'loss/train': '0.60171', 'examples_per_second': '31.556', 'grad_norm': '17.75', 'counters/examples': 249568, 'counters/updates': 7799}
train stats after 249600 examples: {'rewards_train/chosen': '0.20639', 'rewards_train/rejected': '-0.022049', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22844', 'logps_train/rejected': '-134.62', 'logps_train/chosen': '-142.35', 'loss/train': '0.61059', 'examples_per_second': '33.064', 'grad_norm': '19.625', 'counters/examples': 249600, 'counters/updates': 7800}
train stats after 249632 examples: {'rewards_train/chosen': '0.2321', 'rewards_train/rejected': '0.11103', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12107', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-187.37', 'loss/train': '0.66083', 'examples_per_second': '31.413', 'grad_norm': '24.875', 'counters/examples': 249632, 'counters/updates': 7801}
train stats after 249664 examples: {'rewards_train/chosen': '0.24847', 'rewards_train/rejected': '0.11028', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13819', 'logps_train/rejected': '-168', 'logps_train/chosen': '-130.5', 'loss/train': '0.6373', 'examples_per_second': '31.479', 'grad_norm': '21.125', 'counters/examples': 249664, 'counters/updates': 7802}
train stats after 249696 examples: {'rewards_train/chosen': '0.24634', 'rewards_train/rejected': '0.042533', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20381', 'logps_train/rejected': '-108.79', 'logps_train/chosen': '-139.82', 'loss/train': '0.61974', 'examples_per_second': '32.504', 'grad_norm': '19.75', 'counters/examples': 249696, 'counters/updates': 7803}
train stats after 249728 examples: {'rewards_train/chosen': '0.17455', 'rewards_train/rejected': '0.012362', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16219', 'logps_train/rejected': '-89.958', 'logps_train/chosen': '-124.67', 'loss/train': '0.63198', 'examples_per_second': '30.509', 'grad_norm': '18.625', 'counters/examples': 249728, 'counters/updates': 7804}
skipping logging after 249760 examples to avoid logging too frequently
train stats after 249792 examples: {'rewards_train/chosen': '0.20292', 'rewards_train/rejected': '0.050398', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15253', 'logps_train/rejected': '-100.99', 'logps_train/chosen': '-105.38', 'loss/train': '0.64381', 'examples_per_second': '30.276', 'grad_norm': '18.375', 'counters/examples': 249792, 'counters/updates': 7806}
train stats after 249824 examples: {'rewards_train/chosen': '0.20874', 'rewards_train/rejected': '0.043306', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16543', 'logps_train/rejected': '-145.28', 'logps_train/chosen': '-147.61', 'loss/train': '0.62779', 'examples_per_second': '31.475', 'grad_norm': '21', 'counters/examples': 249824, 'counters/updates': 7807}
train stats after 249856 examples: {'rewards_train/chosen': '0.19614', 'rewards_train/rejected': '0.088514', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10762', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-120.23', 'loss/train': '0.65924', 'examples_per_second': '32.731', 'grad_norm': '19.125', 'counters/examples': 249856, 'counters/updates': 7808}
train stats after 249888 examples: {'rewards_train/chosen': '0.1469', 'rewards_train/rejected': '0.02261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12429', 'logps_train/rejected': '-87.171', 'logps_train/chosen': '-104.07', 'loss/train': '0.65093', 'examples_per_second': '31.472', 'grad_norm': '17.25', 'counters/examples': 249888, 'counters/updates': 7809}
train stats after 249920 examples: {'rewards_train/chosen': '0.13216', 'rewards_train/rejected': '0.068194', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063963', 'logps_train/rejected': '-112.08', 'logps_train/chosen': '-139.19', 'loss/train': '0.67539', 'examples_per_second': '30.368', 'grad_norm': '21.375', 'counters/examples': 249920, 'counters/updates': 7810}
train stats after 249952 examples: {'rewards_train/chosen': '0.25098', 'rewards_train/rejected': '0.0011405', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24984', 'logps_train/rejected': '-95.214', 'logps_train/chosen': '-118.33', 'loss/train': '0.59918', 'examples_per_second': '31.489', 'grad_norm': '17', 'counters/examples': 249952, 'counters/updates': 7811}
train stats after 249984 examples: {'rewards_train/chosen': '0.14265', 'rewards_train/rejected': '0.080631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06202', 'logps_train/rejected': '-140.9', 'logps_train/chosen': '-146.72', 'loss/train': '0.68621', 'examples_per_second': '29.893', 'grad_norm': '22.125', 'counters/examples': 249984, 'counters/updates': 7812}
skipping logging after 250016 examples to avoid logging too frequently
train stats after 250048 examples: {'rewards_train/chosen': '0.15147', 'rewards_train/rejected': '0.043429', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10804', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-140.35', 'loss/train': '0.64638', 'examples_per_second': '31.48', 'grad_norm': '19.875', 'counters/examples': 250048, 'counters/updates': 7814}
train stats after 250080 examples: {'rewards_train/chosen': '0.054397', 'rewards_train/rejected': '0.013109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041288', 'logps_train/rejected': '-109.52', 'logps_train/chosen': '-113.18', 'loss/train': '0.68951', 'examples_per_second': '32.561', 'grad_norm': '19.75', 'counters/examples': 250080, 'counters/updates': 7815}
skipping logging after 250112 examples to avoid logging too frequently
train stats after 250144 examples: {'rewards_train/chosen': '0.25445', 'rewards_train/rejected': '0.11913', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13532', 'logps_train/rejected': '-126.66', 'logps_train/chosen': '-162.5', 'loss/train': '0.65152', 'examples_per_second': '32.763', 'grad_norm': '21.5', 'counters/examples': 250144, 'counters/updates': 7817}
train stats after 250176 examples: {'rewards_train/chosen': '0.20012', 'rewards_train/rejected': '0.11179', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088333', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-131.5', 'loss/train': '0.66623', 'examples_per_second': '30.088', 'grad_norm': '20.625', 'counters/examples': 250176, 'counters/updates': 7818}
skipping logging after 250208 examples to avoid logging too frequently
train stats after 250240 examples: {'rewards_train/chosen': '0.091286', 'rewards_train/rejected': '0.018944', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072342', 'logps_train/rejected': '-94.638', 'logps_train/chosen': '-130.67', 'loss/train': '0.67962', 'examples_per_second': '33.411', 'grad_norm': '19.125', 'counters/examples': 250240, 'counters/updates': 7820}
train stats after 250272 examples: {'rewards_train/chosen': '0.1815', 'rewards_train/rejected': '-0.092582', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27408', 'logps_train/rejected': '-94.573', 'logps_train/chosen': '-147.32', 'loss/train': '0.59005', 'examples_per_second': '32.439', 'grad_norm': '18.125', 'counters/examples': 250272, 'counters/updates': 7821}
train stats after 250304 examples: {'rewards_train/chosen': '0.08711', 'rewards_train/rejected': '-0.038023', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12513', 'logps_train/rejected': '-105.67', 'logps_train/chosen': '-110.32', 'loss/train': '0.65169', 'examples_per_second': '32.549', 'grad_norm': '17.875', 'counters/examples': 250304, 'counters/updates': 7822}
train stats after 250336 examples: {'rewards_train/chosen': '0.11598', 'rewards_train/rejected': '0.0033559', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11263', 'logps_train/rejected': '-100.92', 'logps_train/chosen': '-132.22', 'loss/train': '0.66652', 'examples_per_second': '31.23', 'grad_norm': '19.875', 'counters/examples': 250336, 'counters/updates': 7823}
train stats after 250368 examples: {'rewards_train/chosen': '0.12199', 'rewards_train/rejected': '0.015036', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10695', 'logps_train/rejected': '-114.43', 'logps_train/chosen': '-118.21', 'loss/train': '0.66448', 'examples_per_second': '31.476', 'grad_norm': '20.5', 'counters/examples': 250368, 'counters/updates': 7824}
skipping logging after 250400 examples to avoid logging too frequently
train stats after 250432 examples: {'rewards_train/chosen': '0.18005', 'rewards_train/rejected': '0.1691', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010953', 'logps_train/rejected': '-178.82', 'logps_train/chosen': '-142.63', 'loss/train': '0.70389', 'examples_per_second': '31.583', 'grad_norm': '23.875', 'counters/examples': 250432, 'counters/updates': 7826}
train stats after 250464 examples: {'rewards_train/chosen': '0.28229', 'rewards_train/rejected': '0.092955', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18933', 'logps_train/rejected': '-166.01', 'logps_train/chosen': '-144.46', 'loss/train': '0.6141', 'examples_per_second': '30.027', 'grad_norm': '21.625', 'counters/examples': 250464, 'counters/updates': 7827}
skipping logging after 250496 examples to avoid logging too frequently
train stats after 250528 examples: {'rewards_train/chosen': '0.16734', 'rewards_train/rejected': '0.036112', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13122', 'logps_train/rejected': '-97.728', 'logps_train/chosen': '-119.17', 'loss/train': '0.64108', 'examples_per_second': '31.494', 'grad_norm': '17.25', 'counters/examples': 250528, 'counters/updates': 7829}
train stats after 250560 examples: {'rewards_train/chosen': '0.21794', 'rewards_train/rejected': '-0.092134', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.31007', 'logps_train/rejected': '-102.12', 'logps_train/chosen': '-107.01', 'loss/train': '0.56977', 'examples_per_second': '32.579', 'grad_norm': '16.125', 'counters/examples': 250560, 'counters/updates': 7830}
train stats after 250592 examples: {'rewards_train/chosen': '0.10551', 'rewards_train/rejected': '0.023527', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081986', 'logps_train/rejected': '-126.55', 'logps_train/chosen': '-124.3', 'loss/train': '0.66738', 'examples_per_second': '30.856', 'grad_norm': '20.125', 'counters/examples': 250592, 'counters/updates': 7831}
train stats after 250624 examples: {'rewards_train/chosen': '0.17623', 'rewards_train/rejected': '0.092255', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083973', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-135.48', 'loss/train': '0.66273', 'examples_per_second': '31.468', 'grad_norm': '21.25', 'counters/examples': 250624, 'counters/updates': 7832}
train stats after 250656 examples: {'rewards_train/chosen': '0.18796', 'rewards_train/rejected': '0.024825', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16313', 'logps_train/rejected': '-100.56', 'logps_train/chosen': '-131.1', 'loss/train': '0.62922', 'examples_per_second': '32.704', 'grad_norm': '17.875', 'counters/examples': 250656, 'counters/updates': 7833}
train stats after 250688 examples: {'rewards_train/chosen': '0.14904', 'rewards_train/rejected': '0.042257', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10678', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-166.36', 'loss/train': '0.66098', 'examples_per_second': '31.541', 'grad_norm': '21.875', 'counters/examples': 250688, 'counters/updates': 7834}
train stats after 250720 examples: {'rewards_train/chosen': '0.2926', 'rewards_train/rejected': '0.087509', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20509', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-157.12', 'loss/train': '0.62307', 'examples_per_second': '31.287', 'grad_norm': '19.5', 'counters/examples': 250720, 'counters/updates': 7835}
train stats after 250752 examples: {'rewards_train/chosen': '0.15213', 'rewards_train/rejected': '0.0094671', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14266', 'logps_train/rejected': '-102.29', 'logps_train/chosen': '-106.96', 'loss/train': '0.63644', 'examples_per_second': '31.981', 'grad_norm': '17.5', 'counters/examples': 250752, 'counters/updates': 7836}
train stats after 250784 examples: {'rewards_train/chosen': '0.26698', 'rewards_train/rejected': '0.083074', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18391', 'logps_train/rejected': '-108.3', 'logps_train/chosen': '-123.86', 'loss/train': '0.6171', 'examples_per_second': '31.498', 'grad_norm': '18.125', 'counters/examples': 250784, 'counters/updates': 7837}
train stats after 250816 examples: {'rewards_train/chosen': '0.25691', 'rewards_train/rejected': '0.09837', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15854', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-136.36', 'loss/train': '0.63389', 'examples_per_second': '30.763', 'grad_norm': '20.375', 'counters/examples': 250816, 'counters/updates': 7838}
train stats after 250848 examples: {'rewards_train/chosen': '0.20323', 'rewards_train/rejected': '0.0095991', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19363', 'logps_train/rejected': '-129.34', 'logps_train/chosen': '-126.38', 'loss/train': '0.61639', 'examples_per_second': '30.585', 'grad_norm': '19', 'counters/examples': 250848, 'counters/updates': 7839}
skipping logging after 250880 examples to avoid logging too frequently
train stats after 250912 examples: {'rewards_train/chosen': '0.15701', 'rewards_train/rejected': '0.17379', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016782', 'logps_train/rejected': '-148.83', 'logps_train/chosen': '-153.96', 'loss/train': '0.72994', 'examples_per_second': '29.908', 'grad_norm': '23.125', 'counters/examples': 250912, 'counters/updates': 7841}
train stats after 250944 examples: {'rewards_train/chosen': '0.085749', 'rewards_train/rejected': '-0.015059', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10081', 'logps_train/rejected': '-91.02', 'logps_train/chosen': '-105.31', 'loss/train': '0.65483', 'examples_per_second': '30.391', 'grad_norm': '17.75', 'counters/examples': 250944, 'counters/updates': 7842}
train stats after 250976 examples: {'rewards_train/chosen': '0.25097', 'rewards_train/rejected': '0.022357', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22862', 'logps_train/rejected': '-101.12', 'logps_train/chosen': '-136.9', 'loss/train': '0.60847', 'examples_per_second': '31.856', 'grad_norm': '17.75', 'counters/examples': 250976, 'counters/updates': 7843}
train stats after 251008 examples: {'rewards_train/chosen': '0.14777', 'rewards_train/rejected': '0.05237', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095402', 'logps_train/rejected': '-146', 'logps_train/chosen': '-144.77', 'loss/train': '0.65857', 'examples_per_second': '31.374', 'grad_norm': '21.875', 'counters/examples': 251008, 'counters/updates': 7844}
skipping logging after 251040 examples to avoid logging too frequently
train stats after 251072 examples: {'rewards_train/chosen': '0.073206', 'rewards_train/rejected': '-0.05034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12355', 'logps_train/rejected': '-91.323', 'logps_train/chosen': '-104.92', 'loss/train': '0.64872', 'examples_per_second': '34.803', 'grad_norm': '16.625', 'counters/examples': 251072, 'counters/updates': 7846}
skipping logging after 251104 examples to avoid logging too frequently
train stats after 251136 examples: {'rewards_train/chosen': '0.19788', 'rewards_train/rejected': '0.16884', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029044', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-150.21', 'loss/train': '0.68907', 'examples_per_second': '31.456', 'grad_norm': '21.25', 'counters/examples': 251136, 'counters/updates': 7848}
train stats after 251168 examples: {'rewards_train/chosen': '0.194', 'rewards_train/rejected': '0.078288', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11571', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-110.71', 'loss/train': '0.65411', 'examples_per_second': '31.821', 'grad_norm': '17.875', 'counters/examples': 251168, 'counters/updates': 7849}
skipping logging after 251200 examples to avoid logging too frequently
train stats after 251232 examples: {'rewards_train/chosen': '0.10832', 'rewards_train/rejected': '0.05779', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050528', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-154.11', 'loss/train': '0.68314', 'examples_per_second': '33.124', 'grad_norm': '21.375', 'counters/examples': 251232, 'counters/updates': 7851}
train stats after 251264 examples: {'rewards_train/chosen': '0.19891', 'rewards_train/rejected': '0.034287', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16463', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-143.91', 'loss/train': '0.64138', 'examples_per_second': '31.52', 'grad_norm': '19.125', 'counters/examples': 251264, 'counters/updates': 7852}
train stats after 251296 examples: {'rewards_train/chosen': '0.067499', 'rewards_train/rejected': '-0.03022', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097719', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-121.47', 'loss/train': '0.67312', 'examples_per_second': '31.367', 'grad_norm': '21', 'counters/examples': 251296, 'counters/updates': 7853}
train stats after 251328 examples: {'rewards_train/chosen': '0.30087', 'rewards_train/rejected': '0.074347', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22653', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-148.3', 'loss/train': '0.60327', 'examples_per_second': '31.427', 'grad_norm': '20.375', 'counters/examples': 251328, 'counters/updates': 7854}
train stats after 251360 examples: {'rewards_train/chosen': '0.14543', 'rewards_train/rejected': '0.040854', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10457', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-120.37', 'loss/train': '0.66897', 'examples_per_second': '31.725', 'grad_norm': '19.125', 'counters/examples': 251360, 'counters/updates': 7855}
train stats after 251392 examples: {'rewards_train/chosen': '0.14571', 'rewards_train/rejected': '0.047168', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098541', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-119.87', 'loss/train': '0.65556', 'examples_per_second': '31.347', 'grad_norm': '20.5', 'counters/examples': 251392, 'counters/updates': 7856}
skipping logging after 251424 examples to avoid logging too frequently
train stats after 251456 examples: {'rewards_train/chosen': '0.20283', 'rewards_train/rejected': '0.086489', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11634', 'logps_train/rejected': '-151.55', 'logps_train/chosen': '-138.8', 'loss/train': '0.65871', 'examples_per_second': '30.87', 'grad_norm': '22.375', 'counters/examples': 251456, 'counters/updates': 7858}
train stats after 251488 examples: {'rewards_train/chosen': '0.17905', 'rewards_train/rejected': '0.040722', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13833', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-122.53', 'loss/train': '0.64351', 'examples_per_second': '32.058', 'grad_norm': '18.875', 'counters/examples': 251488, 'counters/updates': 7859}
train stats after 251520 examples: {'rewards_train/chosen': '0.19956', 'rewards_train/rejected': '0.0069536', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19261', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-128.64', 'loss/train': '0.61849', 'examples_per_second': '30.76', 'grad_norm': '20.375', 'counters/examples': 251520, 'counters/updates': 7860}
train stats after 251552 examples: {'rewards_train/chosen': '0.20338', 'rewards_train/rejected': '0.11388', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089499', 'logps_train/rejected': '-144.87', 'logps_train/chosen': '-133.08', 'loss/train': '0.67366', 'examples_per_second': '31.45', 'grad_norm': '21.25', 'counters/examples': 251552, 'counters/updates': 7861}
train stats after 251584 examples: {'rewards_train/chosen': '0.19889', 'rewards_train/rejected': '0.057104', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14179', 'logps_train/rejected': '-140.56', 'logps_train/chosen': '-185.53', 'loss/train': '0.65792', 'examples_per_second': '24.326', 'grad_norm': '22.375', 'counters/examples': 251584, 'counters/updates': 7862}
train stats after 251616 examples: {'rewards_train/chosen': '0.19926', 'rewards_train/rejected': '0.065369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13389', 'logps_train/rejected': '-119.69', 'logps_train/chosen': '-161.83', 'loss/train': '0.64326', 'examples_per_second': '31.521', 'grad_norm': '21.25', 'counters/examples': 251616, 'counters/updates': 7863}
train stats after 251648 examples: {'rewards_train/chosen': '0.087128', 'rewards_train/rejected': '0.039112', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048016', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-136.42', 'loss/train': '0.68684', 'examples_per_second': '31.463', 'grad_norm': '21', 'counters/examples': 251648, 'counters/updates': 7864}
train stats after 251680 examples: {'rewards_train/chosen': '0.13938', 'rewards_train/rejected': '-0.028635', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16801', 'logps_train/rejected': '-94.738', 'logps_train/chosen': '-130.05', 'loss/train': '0.63699', 'examples_per_second': '24.487', 'grad_norm': '19.125', 'counters/examples': 251680, 'counters/updates': 7865}
train stats after 251712 examples: {'rewards_train/chosen': '0.13965', 'rewards_train/rejected': '0.073909', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065745', 'logps_train/rejected': '-115.79', 'logps_train/chosen': '-152.75', 'loss/train': '0.6744', 'examples_per_second': '30.08', 'grad_norm': '21.875', 'counters/examples': 251712, 'counters/updates': 7866}
train stats after 251744 examples: {'rewards_train/chosen': '0.15181', 'rewards_train/rejected': '0.13607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015737', 'logps_train/rejected': '-163.86', 'logps_train/chosen': '-156.2', 'loss/train': '0.70771', 'examples_per_second': '31.274', 'grad_norm': '24.125', 'counters/examples': 251744, 'counters/updates': 7867}
train stats after 251776 examples: {'rewards_train/chosen': '0.33383', 'rewards_train/rejected': '0.14561', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18822', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-164.92', 'loss/train': '0.62279', 'examples_per_second': '31.384', 'grad_norm': '20.25', 'counters/examples': 251776, 'counters/updates': 7868}
skipping logging after 251808 examples to avoid logging too frequently
train stats after 251840 examples: {'rewards_train/chosen': '0.066477', 'rewards_train/rejected': '-0.046422', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1129', 'logps_train/rejected': '-98.006', 'logps_train/chosen': '-136.86', 'loss/train': '0.64922', 'examples_per_second': '31.532', 'grad_norm': '19', 'counters/examples': 251840, 'counters/updates': 7870}
skipping logging after 251872 examples to avoid logging too frequently
train stats after 251904 examples: {'rewards_train/chosen': '0.28752', 'rewards_train/rejected': '0.053118', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2344', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-129.53', 'loss/train': '0.59754', 'examples_per_second': '31.467', 'grad_norm': '19', 'counters/examples': 251904, 'counters/updates': 7872}
train stats after 251936 examples: {'rewards_train/chosen': '0.15878', 'rewards_train/rejected': '0.047839', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11094', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-148.17', 'loss/train': '0.6756', 'examples_per_second': '31.612', 'grad_norm': '22.5', 'counters/examples': 251936, 'counters/updates': 7873}
skipping logging after 251968 examples to avoid logging too frequently
train stats after 252000 examples: {'rewards_train/chosen': '0.20621', 'rewards_train/rejected': '0.10324', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10297', 'logps_train/rejected': '-152.52', 'logps_train/chosen': '-140.7', 'loss/train': '0.66022', 'examples_per_second': '30.436', 'grad_norm': '21.125', 'counters/examples': 252000, 'counters/updates': 7875}
Running evaluation after 252000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.18it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.81it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.90it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.90it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.81it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.05it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.96it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.92it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.97it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.88it/s]
eval after 252000: {'rewards_eval/chosen': '0.19095', 'rewards_eval/rejected': '0.035057', 'rewards_eval/accuracies': '0.60156', 'rewards_eval/margins': '0.15589', 'logps_eval/rejected': '-115.04', 'logps_eval/chosen': '-133.64', 'loss/eval': '0.6405'}
skipping save for non epoch
train stats after 252032 examples: {'rewards_train/chosen': '0.14542', 'rewards_train/rejected': '0.074347', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071069', 'logps_train/rejected': '-142.22', 'logps_train/chosen': '-136.55', 'loss/train': '0.66845', 'examples_per_second': '32.542', 'grad_norm': '21.25', 'counters/examples': 252032, 'counters/updates': 7876}
train stats after 252064 examples: {'rewards_train/chosen': '0.28759', 'rewards_train/rejected': '0.11078', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17681', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-124.93', 'loss/train': '0.62124', 'examples_per_second': '30.973', 'grad_norm': '19.125', 'counters/examples': 252064, 'counters/updates': 7877}
train stats after 252096 examples: {'rewards_train/chosen': '0.19716', 'rewards_train/rejected': '0.021945', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17522', 'logps_train/rejected': '-124.99', 'logps_train/chosen': '-145.54', 'loss/train': '0.62557', 'examples_per_second': '30.45', 'grad_norm': '20', 'counters/examples': 252096, 'counters/updates': 7878}
train stats after 252128 examples: {'rewards_train/chosen': '0.27246', 'rewards_train/rejected': '0.14085', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13161', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-146.43', 'loss/train': '0.6557', 'examples_per_second': '30.698', 'grad_norm': '20.5', 'counters/examples': 252128, 'counters/updates': 7879}
train stats after 252160 examples: {'rewards_train/chosen': '0.15169', 'rewards_train/rejected': '0.023375', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12831', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-125.08', 'loss/train': '0.65139', 'examples_per_second': '31.891', 'grad_norm': '18.5', 'counters/examples': 252160, 'counters/updates': 7880}
skipping logging after 252192 examples to avoid logging too frequently
train stats after 252224 examples: {'rewards_train/chosen': '0.18834', 'rewards_train/rejected': '-0.018826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20717', 'logps_train/rejected': '-128.2', 'logps_train/chosen': '-127.63', 'loss/train': '0.61517', 'examples_per_second': '30.787', 'grad_norm': '19', 'counters/examples': 252224, 'counters/updates': 7882}
skipping logging after 252256 examples to avoid logging too frequently
train stats after 252288 examples: {'rewards_train/chosen': '0.1107', 'rewards_train/rejected': '-0.036343', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14704', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-116.81', 'loss/train': '0.6478', 'examples_per_second': '32.031', 'grad_norm': '18.625', 'counters/examples': 252288, 'counters/updates': 7884}
train stats after 252320 examples: {'rewards_train/chosen': '0.18703', 'rewards_train/rejected': '0.013304', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17372', 'logps_train/rejected': '-101.89', 'logps_train/chosen': '-111.29', 'loss/train': '0.62327', 'examples_per_second': '30.992', 'grad_norm': '17.875', 'counters/examples': 252320, 'counters/updates': 7885}
train stats after 252352 examples: {'rewards_train/chosen': '0.17224', 'rewards_train/rejected': '0.012288', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15995', 'logps_train/rejected': '-133.05', 'logps_train/chosen': '-149.42', 'loss/train': '0.63416', 'examples_per_second': '29.884', 'grad_norm': '21', 'counters/examples': 252352, 'counters/updates': 7886}
train stats after 252384 examples: {'rewards_train/chosen': '0.20589', 'rewards_train/rejected': '0.12014', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085753', 'logps_train/rejected': '-119.35', 'logps_train/chosen': '-150.3', 'loss/train': '0.67138', 'examples_per_second': '25.465', 'grad_norm': '22.625', 'counters/examples': 252384, 'counters/updates': 7887}
train stats after 252416 examples: {'rewards_train/chosen': '0.26656', 'rewards_train/rejected': '-0.00097665', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26754', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-157.42', 'loss/train': '0.58949', 'examples_per_second': '31.811', 'grad_norm': '17.875', 'counters/examples': 252416, 'counters/updates': 7888}
train stats after 252448 examples: {'rewards_train/chosen': '0.29858', 'rewards_train/rejected': '-0.056785', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.35537', 'logps_train/rejected': '-95.684', 'logps_train/chosen': '-161.25', 'loss/train': '0.56419', 'examples_per_second': '30.413', 'grad_norm': '20', 'counters/examples': 252448, 'counters/updates': 7889}
skipping logging after 252480 examples to avoid logging too frequently
train stats after 252512 examples: {'rewards_train/chosen': '0.14078', 'rewards_train/rejected': '-0.01282', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1536', 'logps_train/rejected': '-94.803', 'logps_train/chosen': '-124.87', 'loss/train': '0.64086', 'examples_per_second': '31.111', 'grad_norm': '19', 'counters/examples': 252512, 'counters/updates': 7891}
train stats after 252544 examples: {'rewards_train/chosen': '0.14909', 'rewards_train/rejected': '-0.013368', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16246', 'logps_train/rejected': '-106.65', 'logps_train/chosen': '-127.17', 'loss/train': '0.63166', 'examples_per_second': '30.549', 'grad_norm': '18.25', 'counters/examples': 252544, 'counters/updates': 7892}
train stats after 252576 examples: {'rewards_train/chosen': '0.26719', 'rewards_train/rejected': '0.00068384', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2665', 'logps_train/rejected': '-96.443', 'logps_train/chosen': '-109.26', 'loss/train': '0.58396', 'examples_per_second': '30.124', 'grad_norm': '17.625', 'counters/examples': 252576, 'counters/updates': 7893}
train stats after 252608 examples: {'rewards_train/chosen': '0.21891', 'rewards_train/rejected': '0.036255', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18266', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-165.73', 'loss/train': '0.6222', 'examples_per_second': '31.634', 'grad_norm': '21.5', 'counters/examples': 252608, 'counters/updates': 7894}
train stats after 252640 examples: {'rewards_train/chosen': '0.18863', 'rewards_train/rejected': '0.0055098', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18312', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-155.75', 'loss/train': '0.62397', 'examples_per_second': '31.477', 'grad_norm': '19.75', 'counters/examples': 252640, 'counters/updates': 7895}
train stats after 252672 examples: {'rewards_train/chosen': '0.17366', 'rewards_train/rejected': '0.011442', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16222', 'logps_train/rejected': '-78.848', 'logps_train/chosen': '-122.15', 'loss/train': '0.62613', 'examples_per_second': '31.569', 'grad_norm': '19.125', 'counters/examples': 252672, 'counters/updates': 7896}
skipping logging after 252704 examples to avoid logging too frequently
train stats after 252736 examples: {'rewards_train/chosen': '0.26108', 'rewards_train/rejected': '0.089961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17112', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-175.13', 'loss/train': '0.63336', 'examples_per_second': '33.662', 'grad_norm': '21.125', 'counters/examples': 252736, 'counters/updates': 7898}
train stats after 252768 examples: {'rewards_train/chosen': '0.19637', 'rewards_train/rejected': '0.11707', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079302', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-124.52', 'loss/train': '0.69301', 'examples_per_second': '30.601', 'grad_norm': '20.625', 'counters/examples': 252768, 'counters/updates': 7899}
train stats after 252800 examples: {'rewards_train/chosen': '0.26783', 'rewards_train/rejected': '0.089861', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17797', 'logps_train/rejected': '-137.48', 'logps_train/chosen': '-138.71', 'loss/train': '0.63214', 'examples_per_second': '30.258', 'grad_norm': '20.5', 'counters/examples': 252800, 'counters/updates': 7900}
train stats after 252832 examples: {'rewards_train/chosen': '0.18925', 'rewards_train/rejected': '0.059488', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12977', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-155.86', 'loss/train': '0.64311', 'examples_per_second': '31.487', 'grad_norm': '21.875', 'counters/examples': 252832, 'counters/updates': 7901}
train stats after 252864 examples: {'rewards_train/chosen': '0.22128', 'rewards_train/rejected': '-0.045851', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26713', 'logps_train/rejected': '-95.398', 'logps_train/chosen': '-154.02', 'loss/train': '0.58292', 'examples_per_second': '32.593', 'grad_norm': '17.875', 'counters/examples': 252864, 'counters/updates': 7902}
train stats after 252896 examples: {'rewards_train/chosen': '0.12312', 'rewards_train/rejected': '0.056561', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066563', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-123.07', 'loss/train': '0.67687', 'examples_per_second': '30.545', 'grad_norm': '20.625', 'counters/examples': 252896, 'counters/updates': 7903}
train stats after 252928 examples: {'rewards_train/chosen': '0.1663', 'rewards_train/rejected': '0.075701', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090603', 'logps_train/rejected': '-108.19', 'logps_train/chosen': '-110.69', 'loss/train': '0.66145', 'examples_per_second': '33.153', 'grad_norm': '18.625', 'counters/examples': 252928, 'counters/updates': 7904}
skipping logging after 252960 examples to avoid logging too frequently
train stats after 252992 examples: {'rewards_train/chosen': '0.2423', 'rewards_train/rejected': '-0.036444', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27874', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-157.02', 'loss/train': '0.58737', 'examples_per_second': '31.401', 'grad_norm': '19.625', 'counters/examples': 252992, 'counters/updates': 7906}
train stats after 253024 examples: {'rewards_train/chosen': '0.18923', 'rewards_train/rejected': '0.0087844', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18045', 'logps_train/rejected': '-94.076', 'logps_train/chosen': '-142.53', 'loss/train': '0.61948', 'examples_per_second': '32.108', 'grad_norm': '19.125', 'counters/examples': 253024, 'counters/updates': 7907}
train stats after 253056 examples: {'rewards_train/chosen': '0.18728', 'rewards_train/rejected': '-0.013636', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20092', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-131.5', 'loss/train': '0.62235', 'examples_per_second': '30.539', 'grad_norm': '20.125', 'counters/examples': 253056, 'counters/updates': 7908}
skipping logging after 253088 examples to avoid logging too frequently
train stats after 253120 examples: {'rewards_train/chosen': '0.31141', 'rewards_train/rejected': '0.059161', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25225', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-129.02', 'loss/train': '0.59272', 'examples_per_second': '34.741', 'grad_norm': '19.125', 'counters/examples': 253120, 'counters/updates': 7910}
train stats after 253152 examples: {'rewards_train/chosen': '0.12869', 'rewards_train/rejected': '0.14953', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020833', 'logps_train/rejected': '-117.38', 'logps_train/chosen': '-127.44', 'loss/train': '0.73304', 'examples_per_second': '31.195', 'grad_norm': '21.25', 'counters/examples': 253152, 'counters/updates': 7911}
skipping logging after 253184 examples to avoid logging too frequently
train stats after 253216 examples: {'rewards_train/chosen': '0.24276', 'rewards_train/rejected': '0.077656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1651', 'logps_train/rejected': '-94.731', 'logps_train/chosen': '-135.48', 'loss/train': '0.62927', 'examples_per_second': '31.063', 'grad_norm': '17.625', 'counters/examples': 253216, 'counters/updates': 7913}
train stats after 253248 examples: {'rewards_train/chosen': '0.11598', 'rewards_train/rejected': '0.032943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083035', 'logps_train/rejected': '-97.314', 'logps_train/chosen': '-102.07', 'loss/train': '0.67188', 'examples_per_second': '32.396', 'grad_norm': '18.5', 'counters/examples': 253248, 'counters/updates': 7914}
train stats after 253280 examples: {'rewards_train/chosen': '0.15175', 'rewards_train/rejected': '0.055716', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096033', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-170.95', 'loss/train': '0.67484', 'examples_per_second': '31.484', 'grad_norm': '23.625', 'counters/examples': 253280, 'counters/updates': 7915}
train stats after 253312 examples: {'rewards_train/chosen': '0.1875', 'rewards_train/rejected': '0.012001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1755', 'logps_train/rejected': '-105.33', 'logps_train/chosen': '-140.99', 'loss/train': '0.6248', 'examples_per_second': '30.934', 'grad_norm': '19.375', 'counters/examples': 253312, 'counters/updates': 7916}
skipping logging after 253344 examples to avoid logging too frequently
train stats after 253376 examples: {'rewards_train/chosen': '0.15218', 'rewards_train/rejected': '0.018805', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-123.24', 'logps_train/chosen': '-110.01', 'loss/train': '0.64327', 'examples_per_second': '31.321', 'grad_norm': '18.875', 'counters/examples': 253376, 'counters/updates': 7918}
skipping logging after 253408 examples to avoid logging too frequently
train stats after 253440 examples: {'rewards_train/chosen': '0.19597', 'rewards_train/rejected': '0.10024', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095731', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-123.38', 'loss/train': '0.66571', 'examples_per_second': '35.946', 'grad_norm': '20.125', 'counters/examples': 253440, 'counters/updates': 7920}
train stats after 253472 examples: {'rewards_train/chosen': '0.22597', 'rewards_train/rejected': '0.097044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12892', 'logps_train/rejected': '-120.81', 'logps_train/chosen': '-158.43', 'loss/train': '0.65304', 'examples_per_second': '31.511', 'grad_norm': '21.375', 'counters/examples': 253472, 'counters/updates': 7921}
train stats after 253504 examples: {'rewards_train/chosen': '0.18785', 'rewards_train/rejected': '-0.018555', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20641', 'logps_train/rejected': '-81.006', 'logps_train/chosen': '-143.2', 'loss/train': '0.60888', 'examples_per_second': '32.074', 'grad_norm': '16.125', 'counters/examples': 253504, 'counters/updates': 7922}
train stats after 253536 examples: {'rewards_train/chosen': '0.18158', 'rewards_train/rejected': '0.042796', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13878', 'logps_train/rejected': '-134.86', 'logps_train/chosen': '-150.03', 'loss/train': '0.64867', 'examples_per_second': '31.62', 'grad_norm': '21.25', 'counters/examples': 253536, 'counters/updates': 7923}
train stats after 253568 examples: {'rewards_train/chosen': '0.21429', 'rewards_train/rejected': '0.057904', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15639', 'logps_train/rejected': '-124.17', 'logps_train/chosen': '-135.82', 'loss/train': '0.63718', 'examples_per_second': '31.194', 'grad_norm': '20.25', 'counters/examples': 253568, 'counters/updates': 7924}
train stats after 253600 examples: {'rewards_train/chosen': '0.23496', 'rewards_train/rejected': '0.053134', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18183', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-109.8', 'loss/train': '0.62705', 'examples_per_second': '31.148', 'grad_norm': '18.125', 'counters/examples': 253600, 'counters/updates': 7925}
train stats after 253632 examples: {'rewards_train/chosen': '0.24089', 'rewards_train/rejected': '0.060978', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17992', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-120.36', 'loss/train': '0.62498', 'examples_per_second': '32.856', 'grad_norm': '17.5', 'counters/examples': 253632, 'counters/updates': 7926}
train stats after 253664 examples: {'rewards_train/chosen': '0.16712', 'rewards_train/rejected': '0.029403', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13772', 'logps_train/rejected': '-98.419', 'logps_train/chosen': '-139.31', 'loss/train': '0.63843', 'examples_per_second': '30.787', 'grad_norm': '20.5', 'counters/examples': 253664, 'counters/updates': 7927}
train stats after 253696 examples: {'rewards_train/chosen': '0.15742', 'rewards_train/rejected': '0.064393', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093024', 'logps_train/rejected': '-109.17', 'logps_train/chosen': '-112.42', 'loss/train': '0.65666', 'examples_per_second': '30.405', 'grad_norm': '18', 'counters/examples': 253696, 'counters/updates': 7928}
train stats after 253728 examples: {'rewards_train/chosen': '0.18744', 'rewards_train/rejected': '0.068644', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11879', 'logps_train/rejected': '-152.27', 'logps_train/chosen': '-160.47', 'loss/train': '0.65384', 'examples_per_second': '31.526', 'grad_norm': '22.375', 'counters/examples': 253728, 'counters/updates': 7929}
train stats after 253760 examples: {'rewards_train/chosen': '0.13586', 'rewards_train/rejected': '-0.060063', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19593', 'logps_train/rejected': '-96.136', 'logps_train/chosen': '-117.12', 'loss/train': '0.61207', 'examples_per_second': '30.023', 'grad_norm': '16.75', 'counters/examples': 253760, 'counters/updates': 7930}
train stats after 253792 examples: {'rewards_train/chosen': '0.23117', 'rewards_train/rejected': '0.0551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17607', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-152.37', 'loss/train': '0.62272', 'examples_per_second': '31.469', 'grad_norm': '20.75', 'counters/examples': 253792, 'counters/updates': 7931}
skipping logging after 253824 examples to avoid logging too frequently
train stats after 253856 examples: {'rewards_train/chosen': '0.22136', 'rewards_train/rejected': '0.1111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11025', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-143.38', 'loss/train': '0.66244', 'examples_per_second': '31.411', 'grad_norm': '20.875', 'counters/examples': 253856, 'counters/updates': 7933}
train stats after 253888 examples: {'rewards_train/chosen': '0.15462', 'rewards_train/rejected': '0.05916', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095461', 'logps_train/rejected': '-156.6', 'logps_train/chosen': '-149.57', 'loss/train': '0.66424', 'examples_per_second': '33.135', 'grad_norm': '22.875', 'counters/examples': 253888, 'counters/updates': 7934}
train stats after 253920 examples: {'rewards_train/chosen': '0.1995', 'rewards_train/rejected': '0.047843', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15165', 'logps_train/rejected': '-143.28', 'logps_train/chosen': '-175.82', 'loss/train': '0.63537', 'examples_per_second': '31.32', 'grad_norm': '22.25', 'counters/examples': 253920, 'counters/updates': 7935}
train stats after 253952 examples: {'rewards_train/chosen': '0.17774', 'rewards_train/rejected': '0.10534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072396', 'logps_train/rejected': '-107.61', 'logps_train/chosen': '-166.85', 'loss/train': '0.67148', 'examples_per_second': '31.529', 'grad_norm': '22.375', 'counters/examples': 253952, 'counters/updates': 7936}
train stats after 253984 examples: {'rewards_train/chosen': '0.23122', 'rewards_train/rejected': '0.0089303', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22229', 'logps_train/rejected': '-110.59', 'logps_train/chosen': '-122.98', 'loss/train': '0.60833', 'examples_per_second': '31.188', 'grad_norm': '17.5', 'counters/examples': 253984, 'counters/updates': 7937}
skipping logging after 254016 examples to avoid logging too frequently
train stats after 254048 examples: {'rewards_train/chosen': '0.1941', 'rewards_train/rejected': '-0.059115', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25322', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-113.04', 'loss/train': '0.59112', 'examples_per_second': '36.007', 'grad_norm': '17.375', 'counters/examples': 254048, 'counters/updates': 7939}
train stats after 254080 examples: {'rewards_train/chosen': '0.12221', 'rewards_train/rejected': '0.078278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04393', 'logps_train/rejected': '-100.64', 'logps_train/chosen': '-121.88', 'loss/train': '0.68649', 'examples_per_second': '31.528', 'grad_norm': '21', 'counters/examples': 254080, 'counters/updates': 7940}
train stats after 254112 examples: {'rewards_train/chosen': '0.19869', 'rewards_train/rejected': '0.077438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12125', 'logps_train/rejected': '-92.591', 'logps_train/chosen': '-125.54', 'loss/train': '0.66196', 'examples_per_second': '31.759', 'grad_norm': '19.375', 'counters/examples': 254112, 'counters/updates': 7941}
skipping logging after 254144 examples to avoid logging too frequently
train stats after 254176 examples: {'rewards_train/chosen': '0.22328', 'rewards_train/rejected': '0.052032', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17125', 'logps_train/rejected': '-122.16', 'logps_train/chosen': '-126.56', 'loss/train': '0.63656', 'examples_per_second': '30.509', 'grad_norm': '20.25', 'counters/examples': 254176, 'counters/updates': 7943}
train stats after 254208 examples: {'rewards_train/chosen': '0.26063', 'rewards_train/rejected': '0.064774', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19586', 'logps_train/rejected': '-116.46', 'logps_train/chosen': '-146.3', 'loss/train': '0.61594', 'examples_per_second': '31.007', 'grad_norm': '19.25', 'counters/examples': 254208, 'counters/updates': 7944}
train stats after 254240 examples: {'rewards_train/chosen': '0.028573', 'rewards_train/rejected': '0.029827', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0012542', 'logps_train/rejected': '-101.99', 'logps_train/chosen': '-120.62', 'loss/train': '0.71441', 'examples_per_second': '31.464', 'grad_norm': '20', 'counters/examples': 254240, 'counters/updates': 7945}
skipping logging after 254272 examples to avoid logging too frequently
train stats after 254304 examples: {'rewards_train/chosen': '0.15735', 'rewards_train/rejected': '0.087845', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069509', 'logps_train/rejected': '-131.4', 'logps_train/chosen': '-131.16', 'loss/train': '0.66944', 'examples_per_second': '30.572', 'grad_norm': '19.625', 'counters/examples': 254304, 'counters/updates': 7947}
train stats after 254336 examples: {'rewards_train/chosen': '0.16321', 'rewards_train/rejected': '0.080116', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083093', 'logps_train/rejected': '-160.2', 'logps_train/chosen': '-119.27', 'loss/train': '0.67066', 'examples_per_second': '32.305', 'grad_norm': '20.375', 'counters/examples': 254336, 'counters/updates': 7948}
train stats after 254368 examples: {'rewards_train/chosen': '0.15785', 'rewards_train/rejected': '-0.0075249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16538', 'logps_train/rejected': '-104.09', 'logps_train/chosen': '-137.6', 'loss/train': '0.63232', 'examples_per_second': '29.765', 'grad_norm': '18', 'counters/examples': 254368, 'counters/updates': 7949}
train stats after 254400 examples: {'rewards_train/chosen': '0.17243', 'rewards_train/rejected': '0.068265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10416', 'logps_train/rejected': '-91.734', 'logps_train/chosen': '-154.16', 'loss/train': '0.65648', 'examples_per_second': '32.741', 'grad_norm': '20.5', 'counters/examples': 254400, 'counters/updates': 7950}
train stats after 254432 examples: {'rewards_train/chosen': '0.19762', 'rewards_train/rejected': '0.088892', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10872', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-129.38', 'loss/train': '0.65402', 'examples_per_second': '31.383', 'grad_norm': '20.25', 'counters/examples': 254432, 'counters/updates': 7951}
train stats after 254464 examples: {'rewards_train/chosen': '0.17126', 'rewards_train/rejected': '0.028218', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14304', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-142.18', 'loss/train': '0.63788', 'examples_per_second': '30.03', 'grad_norm': '19.25', 'counters/examples': 254464, 'counters/updates': 7952}
train stats after 254496 examples: {'rewards_train/chosen': '0.11529', 'rewards_train/rejected': '-0.0014275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11672', 'logps_train/rejected': '-78.73', 'logps_train/chosen': '-148.06', 'loss/train': '0.65165', 'examples_per_second': '33.088', 'grad_norm': '19', 'counters/examples': 254496, 'counters/updates': 7953}
train stats after 254528 examples: {'rewards_train/chosen': '0.21128', 'rewards_train/rejected': '-0.0048758', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21615', 'logps_train/rejected': '-141.87', 'logps_train/chosen': '-141.7', 'loss/train': '0.60661', 'examples_per_second': '31.489', 'grad_norm': '20.25', 'counters/examples': 254528, 'counters/updates': 7954}
train stats after 254560 examples: {'rewards_train/chosen': '0.23635', 'rewards_train/rejected': '0.043784', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19257', 'logps_train/rejected': '-97.6', 'logps_train/chosen': '-135.47', 'loss/train': '0.61855', 'examples_per_second': '30.509', 'grad_norm': '19', 'counters/examples': 254560, 'counters/updates': 7955}
train stats after 254592 examples: {'rewards_train/chosen': '0.27132', 'rewards_train/rejected': '0.089434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18188', 'logps_train/rejected': '-127.02', 'logps_train/chosen': '-118.37', 'loss/train': '0.62722', 'examples_per_second': '31.346', 'grad_norm': '18.75', 'counters/examples': 254592, 'counters/updates': 7956}
train stats after 254624 examples: {'rewards_train/chosen': '0.16828', 'rewards_train/rejected': '0.11367', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.054604', 'logps_train/rejected': '-99.917', 'logps_train/chosen': '-116.76', 'loss/train': '0.68218', 'examples_per_second': '31.705', 'grad_norm': '19.75', 'counters/examples': 254624, 'counters/updates': 7957}
skipping logging after 254656 examples to avoid logging too frequently
train stats after 254688 examples: {'rewards_train/chosen': '0.19398', 'rewards_train/rejected': '0.11339', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080585', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-157.11', 'loss/train': '0.67347', 'examples_per_second': '31.348', 'grad_norm': '22', 'counters/examples': 254688, 'counters/updates': 7959}
skipping logging after 254720 examples to avoid logging too frequently
train stats after 254752 examples: {'rewards_train/chosen': '0.20938', 'rewards_train/rejected': '0.039335', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17004', 'logps_train/rejected': '-118.64', 'logps_train/chosen': '-134.54', 'loss/train': '0.63265', 'examples_per_second': '30.425', 'grad_norm': '19.5', 'counters/examples': 254752, 'counters/updates': 7961}
train stats after 254784 examples: {'rewards_train/chosen': '0.11449', 'rewards_train/rejected': '0.05789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056599', 'logps_train/rejected': '-146.13', 'logps_train/chosen': '-131.35', 'loss/train': '0.67959', 'examples_per_second': '31.456', 'grad_norm': '21.5', 'counters/examples': 254784, 'counters/updates': 7962}
train stats after 254816 examples: {'rewards_train/chosen': '0.18244', 'rewards_train/rejected': '0.067148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11529', 'logps_train/rejected': '-155.52', 'logps_train/chosen': '-137.31', 'loss/train': '0.65694', 'examples_per_second': '29.345', 'grad_norm': '21', 'counters/examples': 254816, 'counters/updates': 7963}
skipping logging after 254848 examples to avoid logging too frequently
train stats after 254880 examples: {'rewards_train/chosen': '0.17272', 'rewards_train/rejected': '0.15573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01699', 'logps_train/rejected': '-169.37', 'logps_train/chosen': '-154.73', 'loss/train': '0.69929', 'examples_per_second': '31.418', 'grad_norm': '24.125', 'counters/examples': 254880, 'counters/updates': 7965}
skipping logging after 254912 examples to avoid logging too frequently
train stats after 254944 examples: {'rewards_train/chosen': '0.13895', 'rewards_train/rejected': '0.040346', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098607', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-133.91', 'loss/train': '0.66768', 'examples_per_second': '34.427', 'grad_norm': '21', 'counters/examples': 254944, 'counters/updates': 7967}
train stats after 254976 examples: {'rewards_train/chosen': '0.11804', 'rewards_train/rejected': '0.034176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083867', 'logps_train/rejected': '-116.42', 'logps_train/chosen': '-106.85', 'loss/train': '0.66564', 'examples_per_second': '31.213', 'grad_norm': '19.875', 'counters/examples': 254976, 'counters/updates': 7968}
train stats after 255008 examples: {'rewards_train/chosen': '0.25443', 'rewards_train/rejected': '-0.014442', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26887', 'logps_train/rejected': '-105.64', 'logps_train/chosen': '-137.97', 'loss/train': '0.60006', 'examples_per_second': '33.11', 'grad_norm': '18.875', 'counters/examples': 255008, 'counters/updates': 7969}
train stats after 255040 examples: {'rewards_train/chosen': '0.29661', 'rewards_train/rejected': '0.094328', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20228', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-159.02', 'loss/train': '0.61759', 'examples_per_second': '31.445', 'grad_norm': '21.25', 'counters/examples': 255040, 'counters/updates': 7970}
train stats after 255072 examples: {'rewards_train/chosen': '0.28536', 'rewards_train/rejected': '0.11925', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16611', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-157.63', 'loss/train': '0.6352', 'examples_per_second': '31.448', 'grad_norm': '22.625', 'counters/examples': 255072, 'counters/updates': 7971}
train stats after 255104 examples: {'rewards_train/chosen': '0.1472', 'rewards_train/rejected': '0.0076735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13953', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-150.32', 'loss/train': '0.65429', 'examples_per_second': '30.369', 'grad_norm': '20.375', 'counters/examples': 255104, 'counters/updates': 7972}
train stats after 255136 examples: {'rewards_train/chosen': '0.20286', 'rewards_train/rejected': '0.02194', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18092', 'logps_train/rejected': '-130.65', 'logps_train/chosen': '-133.79', 'loss/train': '0.62565', 'examples_per_second': '32.117', 'grad_norm': '19.5', 'counters/examples': 255136, 'counters/updates': 7973}
train stats after 255168 examples: {'rewards_train/chosen': '0.17636', 'rewards_train/rejected': '-0.020258', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19662', 'logps_train/rejected': '-112.5', 'logps_train/chosen': '-162.38', 'loss/train': '0.63455', 'examples_per_second': '30.859', 'grad_norm': '20.625', 'counters/examples': 255168, 'counters/updates': 7974}
train stats after 255200 examples: {'rewards_train/chosen': '0.2077', 'rewards_train/rejected': '0.033887', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.17382', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-131.99', 'loss/train': '0.6183', 'examples_per_second': '30.561', 'grad_norm': '19.125', 'counters/examples': 255200, 'counters/updates': 7975}
skipping logging after 255232 examples to avoid logging too frequently
train stats after 255264 examples: {'rewards_train/chosen': '0.21952', 'rewards_train/rejected': '-0.0027109', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22223', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-152.15', 'loss/train': '0.6071', 'examples_per_second': '33.329', 'grad_norm': '20.125', 'counters/examples': 255264, 'counters/updates': 7977}
train stats after 255296 examples: {'rewards_train/chosen': '0.23659', 'rewards_train/rejected': '-0.0077533', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24434', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-116.83', 'loss/train': '0.60111', 'examples_per_second': '30.809', 'grad_norm': '19.875', 'counters/examples': 255296, 'counters/updates': 7978}
train stats after 255328 examples: {'rewards_train/chosen': '0.071786', 'rewards_train/rejected': '-0.025751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097537', 'logps_train/rejected': '-101.91', 'logps_train/chosen': '-104.41', 'loss/train': '0.66224', 'examples_per_second': '31.611', 'grad_norm': '18.125', 'counters/examples': 255328, 'counters/updates': 7979}
train stats after 255360 examples: {'rewards_train/chosen': '0.23176', 'rewards_train/rejected': '0.0035003', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22826', 'logps_train/rejected': '-106.6', 'logps_train/chosen': '-146.35', 'loss/train': '0.60611', 'examples_per_second': '31.633', 'grad_norm': '18.25', 'counters/examples': 255360, 'counters/updates': 7980}
train stats after 255392 examples: {'rewards_train/chosen': '0.10788', 'rewards_train/rejected': '0.049359', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058526', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-137.36', 'loss/train': '0.68242', 'examples_per_second': '33.159', 'grad_norm': '22', 'counters/examples': 255392, 'counters/updates': 7981}
skipping logging after 255424 examples to avoid logging too frequently
train stats after 255456 examples: {'rewards_train/chosen': '0.11712', 'rewards_train/rejected': '0.092605', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.024518', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-127.38', 'loss/train': '0.69297', 'examples_per_second': '29.992', 'grad_norm': '20.25', 'counters/examples': 255456, 'counters/updates': 7983}
train stats after 255488 examples: {'rewards_train/chosen': '0.15059', 'rewards_train/rejected': '-0.011321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16191', 'logps_train/rejected': '-143', 'logps_train/chosen': '-162.85', 'loss/train': '0.65048', 'examples_per_second': '31.262', 'grad_norm': '21.25', 'counters/examples': 255488, 'counters/updates': 7984}
train stats after 255520 examples: {'rewards_train/chosen': '0.16692', 'rewards_train/rejected': '0.051803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11511', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-96.677', 'loss/train': '0.64738', 'examples_per_second': '30.356', 'grad_norm': '17.125', 'counters/examples': 255520, 'counters/updates': 7985}
train stats after 255552 examples: {'rewards_train/chosen': '0.13926', 'rewards_train/rejected': '0.054106', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085158', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-140.53', 'loss/train': '0.66338', 'examples_per_second': '32.444', 'grad_norm': '20.875', 'counters/examples': 255552, 'counters/updates': 7986}
train stats after 255584 examples: {'rewards_train/chosen': '0.19915', 'rewards_train/rejected': '0.046938', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15221', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-145.66', 'loss/train': '0.63221', 'examples_per_second': '30.651', 'grad_norm': '21.125', 'counters/examples': 255584, 'counters/updates': 7987}
train stats after 255616 examples: {'rewards_train/chosen': '0.2004', 'rewards_train/rejected': '0.074118', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12628', 'logps_train/rejected': '-121.03', 'logps_train/chosen': '-166.16', 'loss/train': '0.65147', 'examples_per_second': '31.498', 'grad_norm': '22', 'counters/examples': 255616, 'counters/updates': 7988}
train stats after 255648 examples: {'rewards_train/chosen': '0.096183', 'rewards_train/rejected': '0.096474', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00029088', 'logps_train/rejected': '-94.184', 'logps_train/chosen': '-91.909', 'loss/train': '0.70301', 'examples_per_second': '32.452', 'grad_norm': '17.875', 'counters/examples': 255648, 'counters/updates': 7989}
train stats after 255680 examples: {'rewards_train/chosen': '0.20346', 'rewards_train/rejected': '0.12767', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075793', 'logps_train/rejected': '-181.47', 'logps_train/chosen': '-174.34', 'loss/train': '0.67776', 'examples_per_second': '31.49', 'grad_norm': '25', 'counters/examples': 255680, 'counters/updates': 7990}
train stats after 255712 examples: {'rewards_train/chosen': '0.25972', 'rewards_train/rejected': '0.16617', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093551', 'logps_train/rejected': '-105.11', 'logps_train/chosen': '-128.17', 'loss/train': '0.67084', 'examples_per_second': '31', 'grad_norm': '20.375', 'counters/examples': 255712, 'counters/updates': 7991}
train stats after 255744 examples: {'rewards_train/chosen': '0.14584', 'rewards_train/rejected': '-0.0081221', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15396', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-137.54', 'loss/train': '0.64109', 'examples_per_second': '31.549', 'grad_norm': '21.375', 'counters/examples': 255744, 'counters/updates': 7992}
skipping logging after 255776 examples to avoid logging too frequently
train stats after 255808 examples: {'rewards_train/chosen': '0.12627', 'rewards_train/rejected': '0.037816', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088456', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-111.75', 'loss/train': '0.67357', 'examples_per_second': '33.127', 'grad_norm': '20.875', 'counters/examples': 255808, 'counters/updates': 7994}
train stats after 255840 examples: {'rewards_train/chosen': '0.23025', 'rewards_train/rejected': '0.037489', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19276', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-116.34', 'loss/train': '0.61913', 'examples_per_second': '29.913', 'grad_norm': '18.25', 'counters/examples': 255840, 'counters/updates': 7995}
train stats after 255872 examples: {'rewards_train/chosen': '0.1531', 'rewards_train/rejected': '-0.022442', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17554', 'logps_train/rejected': '-97.263', 'logps_train/chosen': '-127.58', 'loss/train': '0.62578', 'examples_per_second': '30.056', 'grad_norm': '17.625', 'counters/examples': 255872, 'counters/updates': 7996}
train stats after 255904 examples: {'rewards_train/chosen': '0.23153', 'rewards_train/rejected': '0.11552', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11602', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-144.67', 'loss/train': '0.65262', 'examples_per_second': '31.442', 'grad_norm': '22.5', 'counters/examples': 255904, 'counters/updates': 7997}
train stats after 255936 examples: {'rewards_train/chosen': '0.15932', 'rewards_train/rejected': '-0.026437', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18575', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-153.53', 'loss/train': '0.62275', 'examples_per_second': '32.729', 'grad_norm': '20.75', 'counters/examples': 255936, 'counters/updates': 7998}
train stats after 255968 examples: {'rewards_train/chosen': '0.20657', 'rewards_train/rejected': '-0.0029616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20953', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-153.7', 'loss/train': '0.61654', 'examples_per_second': '31.284', 'grad_norm': '19.625', 'counters/examples': 255968, 'counters/updates': 7999}
train stats after 256000 examples: {'rewards_train/chosen': '0.24666', 'rewards_train/rejected': '0.013097', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23356', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-119.78', 'loss/train': '0.61758', 'examples_per_second': '31.284', 'grad_norm': '19.875', 'counters/examples': 256000, 'counters/updates': 8000}
skipping logging after 256032 examples to avoid logging too frequently
train stats after 256064 examples: {'rewards_train/chosen': '0.21987', 'rewards_train/rejected': '-0.0073748', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22725', 'logps_train/rejected': '-113.68', 'logps_train/chosen': '-150.22', 'loss/train': '0.601', 'examples_per_second': '31.538', 'grad_norm': '19.125', 'counters/examples': 256064, 'counters/updates': 8002}
skipping logging after 256096 examples to avoid logging too frequently
train stats after 256128 examples: {'rewards_train/chosen': '0.15277', 'rewards_train/rejected': '0.072314', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080453', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-118.08', 'loss/train': '0.66366', 'examples_per_second': '31.512', 'grad_norm': '19.5', 'counters/examples': 256128, 'counters/updates': 8004}
train stats after 256160 examples: {'rewards_train/chosen': '0.1679', 'rewards_train/rejected': '0.067747', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10015', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-134.91', 'loss/train': '0.65683', 'examples_per_second': '30.952', 'grad_norm': '21.875', 'counters/examples': 256160, 'counters/updates': 8005}
train stats after 256192 examples: {'rewards_train/chosen': '0.10876', 'rewards_train/rejected': '0.059199', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049562', 'logps_train/rejected': '-96.935', 'logps_train/chosen': '-115.92', 'loss/train': '0.68222', 'examples_per_second': '31.208', 'grad_norm': '19.625', 'counters/examples': 256192, 'counters/updates': 8006}
skipping logging after 256224 examples to avoid logging too frequently
train stats after 256256 examples: {'rewards_train/chosen': '0.2175', 'rewards_train/rejected': '0.02732', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19018', 'logps_train/rejected': '-112.64', 'logps_train/chosen': '-148.34', 'loss/train': '0.62573', 'examples_per_second': '30.982', 'grad_norm': '19.375', 'counters/examples': 256256, 'counters/updates': 8008}
train stats after 256288 examples: {'rewards_train/chosen': '0.25252', 'rewards_train/rejected': '0.11727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13525', 'logps_train/rejected': '-107.73', 'logps_train/chosen': '-144.66', 'loss/train': '0.64083', 'examples_per_second': '30.287', 'grad_norm': '19', 'counters/examples': 256288, 'counters/updates': 8009}
train stats after 256320 examples: {'rewards_train/chosen': '0.24456', 'rewards_train/rejected': '0.02652', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21804', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-154.97', 'loss/train': '0.60306', 'examples_per_second': '31.514', 'grad_norm': '19.5', 'counters/examples': 256320, 'counters/updates': 8010}
train stats after 256352 examples: {'rewards_train/chosen': '0.13376', 'rewards_train/rejected': '0.062215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071547', 'logps_train/rejected': '-104.84', 'logps_train/chosen': '-116.6', 'loss/train': '0.67541', 'examples_per_second': '30.393', 'grad_norm': '18.625', 'counters/examples': 256352, 'counters/updates': 8011}
skipping logging after 256384 examples to avoid logging too frequently
train stats after 256416 examples: {'rewards_train/chosen': '0.15676', 'rewards_train/rejected': '0.056176', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10059', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-113.08', 'loss/train': '0.65922', 'examples_per_second': '32.426', 'grad_norm': '18.25', 'counters/examples': 256416, 'counters/updates': 8013}
train stats after 256448 examples: {'rewards_train/chosen': '0.24381', 'rewards_train/rejected': '0.006413', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2374', 'logps_train/rejected': '-163.32', 'logps_train/chosen': '-172.47', 'loss/train': '0.60556', 'examples_per_second': '29.973', 'grad_norm': '22.625', 'counters/examples': 256448, 'counters/updates': 8014}
skipping logging after 256480 examples to avoid logging too frequently
train stats after 256512 examples: {'rewards_train/chosen': '0.15547', 'rewards_train/rejected': '-0.017317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17279', 'logps_train/rejected': '-93.773', 'logps_train/chosen': '-141.29', 'loss/train': '0.62635', 'examples_per_second': '30.454', 'grad_norm': '19.125', 'counters/examples': 256512, 'counters/updates': 8016}
skipping logging after 256544 examples to avoid logging too frequently
train stats after 256576 examples: {'rewards_train/chosen': '0.20179', 'rewards_train/rejected': '0.16788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033904', 'logps_train/rejected': '-143', 'logps_train/chosen': '-118.66', 'loss/train': '0.68521', 'examples_per_second': '30.689', 'grad_norm': '20.75', 'counters/examples': 256576, 'counters/updates': 8018}
train stats after 256608 examples: {'rewards_train/chosen': '0.24777', 'rewards_train/rejected': '0.080201', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16757', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-191.54', 'loss/train': '0.62555', 'examples_per_second': '31.497', 'grad_norm': '23.25', 'counters/examples': 256608, 'counters/updates': 8019}
train stats after 256640 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.089937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050226', 'logps_train/rejected': '-95.152', 'logps_train/chosen': '-104.24', 'loss/train': '0.67846', 'examples_per_second': '30.212', 'grad_norm': '16.625', 'counters/examples': 256640, 'counters/updates': 8020}
train stats after 256672 examples: {'rewards_train/chosen': '0.21191', 'rewards_train/rejected': '0.063165', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14874', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-153.61', 'loss/train': '0.64606', 'examples_per_second': '31.98', 'grad_norm': '19.25', 'counters/examples': 256672, 'counters/updates': 8021}
train stats after 256704 examples: {'rewards_train/chosen': '0.18144', 'rewards_train/rejected': '0.090723', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090718', 'logps_train/rejected': '-128.88', 'logps_train/chosen': '-129.42', 'loss/train': '0.67277', 'examples_per_second': '31.899', 'grad_norm': '19.875', 'counters/examples': 256704, 'counters/updates': 8022}
train stats after 256736 examples: {'rewards_train/chosen': '0.13733', 'rewards_train/rejected': '0.068341', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068989', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-136.73', 'loss/train': '0.68187', 'examples_per_second': '31.143', 'grad_norm': '21', 'counters/examples': 256736, 'counters/updates': 8023}
train stats after 256768 examples: {'rewards_train/chosen': '0.2055', 'rewards_train/rejected': '0.082219', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12328', 'logps_train/rejected': '-131.83', 'logps_train/chosen': '-162.58', 'loss/train': '0.65312', 'examples_per_second': '31.353', 'grad_norm': '23.75', 'counters/examples': 256768, 'counters/updates': 8024}
train stats after 256800 examples: {'rewards_train/chosen': '0.23207', 'rewards_train/rejected': '-0.0017836', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23385', 'logps_train/rejected': '-96.09', 'logps_train/chosen': '-115.58', 'loss/train': '0.59951', 'examples_per_second': '32.26', 'grad_norm': '17.25', 'counters/examples': 256800, 'counters/updates': 8025}
train stats after 256832 examples: {'rewards_train/chosen': '0.17762', 'rewards_train/rejected': '0.078123', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099496', 'logps_train/rejected': '-106.83', 'logps_train/chosen': '-159.29', 'loss/train': '0.66626', 'examples_per_second': '31.509', 'grad_norm': '21', 'counters/examples': 256832, 'counters/updates': 8026}
skipping logging after 256864 examples to avoid logging too frequently
train stats after 256896 examples: {'rewards_train/chosen': '0.14546', 'rewards_train/rejected': '-0.0154', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16086', 'logps_train/rejected': '-109.36', 'logps_train/chosen': '-97.148', 'loss/train': '0.63698', 'examples_per_second': '31.5', 'grad_norm': '18.125', 'counters/examples': 256896, 'counters/updates': 8028}
skipping logging after 256928 examples to avoid logging too frequently
train stats after 256960 examples: {'rewards_train/chosen': '0.15717', 'rewards_train/rejected': '0.033059', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12411', 'logps_train/rejected': '-145.43', 'logps_train/chosen': '-169.48', 'loss/train': '0.64676', 'examples_per_second': '29.994', 'grad_norm': '23.125', 'counters/examples': 256960, 'counters/updates': 8030}
skipping logging after 256992 examples to avoid logging too frequently
train stats after 257024 examples: {'rewards_train/chosen': '0.2364', 'rewards_train/rejected': '0.067871', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16853', 'logps_train/rejected': '-132.29', 'logps_train/chosen': '-139.69', 'loss/train': '0.63236', 'examples_per_second': '30.143', 'grad_norm': '21.125', 'counters/examples': 257024, 'counters/updates': 8032}
train stats after 257056 examples: {'rewards_train/chosen': '0.12358', 'rewards_train/rejected': '-0.062851', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18643', 'logps_train/rejected': '-111.05', 'logps_train/chosen': '-143.44', 'loss/train': '0.61364', 'examples_per_second': '24.264', 'grad_norm': '19.25', 'counters/examples': 257056, 'counters/updates': 8033}
train stats after 257088 examples: {'rewards_train/chosen': '0.1392', 'rewards_train/rejected': '0.026766', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11243', 'logps_train/rejected': '-101.94', 'logps_train/chosen': '-137.71', 'loss/train': '0.65545', 'examples_per_second': '29.984', 'grad_norm': '20.625', 'counters/examples': 257088, 'counters/updates': 8034}
train stats after 257120 examples: {'rewards_train/chosen': '0.19647', 'rewards_train/rejected': '0.094523', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10194', 'logps_train/rejected': '-159.97', 'logps_train/chosen': '-133.78', 'loss/train': '0.65923', 'examples_per_second': '31.452', 'grad_norm': '22.375', 'counters/examples': 257120, 'counters/updates': 8035}
train stats after 257152 examples: {'rewards_train/chosen': '0.27272', 'rewards_train/rejected': '0.068954', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20377', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-145.12', 'loss/train': '0.61664', 'examples_per_second': '24.474', 'grad_norm': '19.75', 'counters/examples': 257152, 'counters/updates': 8036}
train stats after 257184 examples: {'rewards_train/chosen': '0.15672', 'rewards_train/rejected': '0.10407', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052649', 'logps_train/rejected': '-157.94', 'logps_train/chosen': '-161.45', 'loss/train': '0.67835', 'examples_per_second': '29.982', 'grad_norm': '23.125', 'counters/examples': 257184, 'counters/updates': 8037}
train stats after 257216 examples: {'rewards_train/chosen': '0.10235', 'rewards_train/rejected': '0.016814', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085536', 'logps_train/rejected': '-103.65', 'logps_train/chosen': '-113.87', 'loss/train': '0.66024', 'examples_per_second': '30.193', 'grad_norm': '18.75', 'counters/examples': 257216, 'counters/updates': 8038}
train stats after 257248 examples: {'rewards_train/chosen': '0.2132', 'rewards_train/rejected': '0.035252', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17795', 'logps_train/rejected': '-122.27', 'logps_train/chosen': '-130.39', 'loss/train': '0.62783', 'examples_per_second': '30.128', 'grad_norm': '20.875', 'counters/examples': 257248, 'counters/updates': 8039}
train stats after 257280 examples: {'rewards_train/chosen': '0.25038', 'rewards_train/rejected': '0.046525', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20386', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-163.47', 'loss/train': '0.61182', 'examples_per_second': '30.151', 'grad_norm': '22.375', 'counters/examples': 257280, 'counters/updates': 8040}
skipping logging after 257312 examples to avoid logging too frequently
train stats after 257344 examples: {'rewards_train/chosen': '0.28786', 'rewards_train/rejected': '-0.0091694', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.29703', 'logps_train/rejected': '-104.64', 'logps_train/chosen': '-121.71', 'loss/train': '0.58027', 'examples_per_second': '31.067', 'grad_norm': '17.625', 'counters/examples': 257344, 'counters/updates': 8042}
train stats after 257376 examples: {'rewards_train/chosen': '0.17918', 'rewards_train/rejected': '-0.051743', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23093', 'logps_train/rejected': '-138.59', 'logps_train/chosen': '-130.3', 'loss/train': '0.60134', 'examples_per_second': '32.52', 'grad_norm': '19.375', 'counters/examples': 257376, 'counters/updates': 8043}
train stats after 257408 examples: {'rewards_train/chosen': '0.078639', 'rewards_train/rejected': '0.026659', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05198', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-113.8', 'loss/train': '0.68132', 'examples_per_second': '31.06', 'grad_norm': '20.125', 'counters/examples': 257408, 'counters/updates': 8044}
skipping logging after 257440 examples to avoid logging too frequently
train stats after 257472 examples: {'rewards_train/chosen': '0.19039', 'rewards_train/rejected': '0.062063', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12833', 'logps_train/rejected': '-108.16', 'logps_train/chosen': '-152.69', 'loss/train': '0.6647', 'examples_per_second': '31.442', 'grad_norm': '23.125', 'counters/examples': 257472, 'counters/updates': 8046}
train stats after 257504 examples: {'rewards_train/chosen': '0.10654', 'rewards_train/rejected': '0.074998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03154', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-115.92', 'loss/train': '0.69295', 'examples_per_second': '32.872', 'grad_norm': '20.375', 'counters/examples': 257504, 'counters/updates': 8047}
train stats after 257536 examples: {'rewards_train/chosen': '0.1833', 'rewards_train/rejected': '0.0056385', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17766', 'logps_train/rejected': '-97.629', 'logps_train/chosen': '-138.07', 'loss/train': '0.62665', 'examples_per_second': '31.01', 'grad_norm': '19.5', 'counters/examples': 257536, 'counters/updates': 8048}
train stats after 257568 examples: {'rewards_train/chosen': '0.11121', 'rewards_train/rejected': '0.0082919', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10292', 'logps_train/rejected': '-128.71', 'logps_train/chosen': '-133.94', 'loss/train': '0.66367', 'examples_per_second': '32.131', 'grad_norm': '19', 'counters/examples': 257568, 'counters/updates': 8049}
train stats after 257600 examples: {'rewards_train/chosen': '0.12836', 'rewards_train/rejected': '0.0057016', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12265', 'logps_train/rejected': '-102.34', 'logps_train/chosen': '-126.98', 'loss/train': '0.64979', 'examples_per_second': '32.061', 'grad_norm': '18.625', 'counters/examples': 257600, 'counters/updates': 8050}
train stats after 257632 examples: {'rewards_train/chosen': '0.21047', 'rewards_train/rejected': '0.11892', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091546', 'logps_train/rejected': '-110.67', 'logps_train/chosen': '-135.79', 'loss/train': '0.66885', 'examples_per_second': '31.369', 'grad_norm': '20.375', 'counters/examples': 257632, 'counters/updates': 8051}
train stats after 257664 examples: {'rewards_train/chosen': '0.20893', 'rewards_train/rejected': '-0.017161', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22609', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-160.74', 'loss/train': '0.61533', 'examples_per_second': '30.531', 'grad_norm': '21.5', 'counters/examples': 257664, 'counters/updates': 8052}
train stats after 257696 examples: {'rewards_train/chosen': '0.12154', 'rewards_train/rejected': '0.016648', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10489', 'logps_train/rejected': '-175.81', 'logps_train/chosen': '-134.95', 'loss/train': '0.66184', 'examples_per_second': '31.237', 'grad_norm': '22.5', 'counters/examples': 257696, 'counters/updates': 8053}
train stats after 257728 examples: {'rewards_train/chosen': '0.18105', 'rewards_train/rejected': '0.053973', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12707', 'logps_train/rejected': '-129.87', 'logps_train/chosen': '-130.48', 'loss/train': '0.63974', 'examples_per_second': '31.565', 'grad_norm': '19.625', 'counters/examples': 257728, 'counters/updates': 8054}
train stats after 257760 examples: {'rewards_train/chosen': '0.089158', 'rewards_train/rejected': '0.016361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072797', 'logps_train/rejected': '-131.02', 'logps_train/chosen': '-156.17', 'loss/train': '0.67359', 'examples_per_second': '32.191', 'grad_norm': '22.375', 'counters/examples': 257760, 'counters/updates': 8055}
train stats after 257792 examples: {'rewards_train/chosen': '0.24013', 'rewards_train/rejected': '0.10032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13982', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-129.25', 'loss/train': '0.65732', 'examples_per_second': '32.413', 'grad_norm': '19.75', 'counters/examples': 257792, 'counters/updates': 8056}
train stats after 257824 examples: {'rewards_train/chosen': '0.17469', 'rewards_train/rejected': '0.088411', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086283', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-117.03', 'loss/train': '0.66769', 'examples_per_second': '31.857', 'grad_norm': '19.25', 'counters/examples': 257824, 'counters/updates': 8057}
train stats after 257856 examples: {'rewards_train/chosen': '0.16282', 'rewards_train/rejected': '0.0267', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13612', 'logps_train/rejected': '-141.62', 'logps_train/chosen': '-133.52', 'loss/train': '0.66188', 'examples_per_second': '30.403', 'grad_norm': '23.25', 'counters/examples': 257856, 'counters/updates': 8058}
train stats after 257888 examples: {'rewards_train/chosen': '0.21583', 'rewards_train/rejected': '0.14658', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069247', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-117.03', 'loss/train': '0.68688', 'examples_per_second': '32.054', 'grad_norm': '20.25', 'counters/examples': 257888, 'counters/updates': 8059}
skipping logging after 257920 examples to avoid logging too frequently
train stats after 257952 examples: {'rewards_train/chosen': '0.15048', 'rewards_train/rejected': '0.012733', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13774', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-122.42', 'loss/train': '0.64096', 'examples_per_second': '24.473', 'grad_norm': '20.125', 'counters/examples': 257952, 'counters/updates': 8061}
train stats after 257984 examples: {'rewards_train/chosen': '0.040521', 'rewards_train/rejected': '0.022764', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017757', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-136.5', 'loss/train': '0.69994', 'examples_per_second': '32.308', 'grad_norm': '21.5', 'counters/examples': 257984, 'counters/updates': 8062}
train stats after 258016 examples: {'rewards_train/chosen': '0.18271', 'rewards_train/rejected': '0.036656', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14605', 'logps_train/rejected': '-112.8', 'logps_train/chosen': '-128.03', 'loss/train': '0.64196', 'examples_per_second': '32.168', 'grad_norm': '19.375', 'counters/examples': 258016, 'counters/updates': 8063}
train stats after 258048 examples: {'rewards_train/chosen': '0.12027', 'rewards_train/rejected': '0.093065', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027206', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-156.59', 'loss/train': '0.69782', 'examples_per_second': '31.562', 'grad_norm': '21.75', 'counters/examples': 258048, 'counters/updates': 8064}
train stats after 258080 examples: {'rewards_train/chosen': '0.24153', 'rewards_train/rejected': '0.09065', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15088', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-156.64', 'loss/train': '0.64298', 'examples_per_second': '29.967', 'grad_norm': '20.875', 'counters/examples': 258080, 'counters/updates': 8065}
train stats after 258112 examples: {'rewards_train/chosen': '0.1847', 'rewards_train/rejected': '0.098709', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085993', 'logps_train/rejected': '-168.13', 'logps_train/chosen': '-151.62', 'loss/train': '0.66531', 'examples_per_second': '29.845', 'grad_norm': '23.25', 'counters/examples': 258112, 'counters/updates': 8066}
skipping logging after 258144 examples to avoid logging too frequently
train stats after 258176 examples: {'rewards_train/chosen': '0.19978', 'rewards_train/rejected': '0.0081545', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19162', 'logps_train/rejected': '-96.789', 'logps_train/chosen': '-161.79', 'loss/train': '0.62275', 'examples_per_second': '30.221', 'grad_norm': '20.875', 'counters/examples': 258176, 'counters/updates': 8068}
skipping logging after 258208 examples to avoid logging too frequently
train stats after 258240 examples: {'rewards_train/chosen': '0.18927', 'rewards_train/rejected': '0.018871', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1704', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-157.21', 'loss/train': '0.64256', 'examples_per_second': '31.401', 'grad_norm': '21.875', 'counters/examples': 258240, 'counters/updates': 8070}
train stats after 258272 examples: {'rewards_train/chosen': '0.098501', 'rewards_train/rejected': '0.072769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025732', 'logps_train/rejected': '-109.64', 'logps_train/chosen': '-115.49', 'loss/train': '0.68987', 'examples_per_second': '30.785', 'grad_norm': '19.125', 'counters/examples': 258272, 'counters/updates': 8071}
train stats after 258304 examples: {'rewards_train/chosen': '0.096419', 'rewards_train/rejected': '0.095876', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00054383', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-130.05', 'loss/train': '0.70452', 'examples_per_second': '32.529', 'grad_norm': '21.125', 'counters/examples': 258304, 'counters/updates': 8072}
train stats after 258336 examples: {'rewards_train/chosen': '0.2178', 'rewards_train/rejected': '0.077105', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1407', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-145.07', 'loss/train': '0.63911', 'examples_per_second': '31.555', 'grad_norm': '21', 'counters/examples': 258336, 'counters/updates': 8073}
train stats after 258368 examples: {'rewards_train/chosen': '0.15671', 'rewards_train/rejected': '0.075528', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081178', 'logps_train/rejected': '-111.68', 'logps_train/chosen': '-115.18', 'loss/train': '0.66362', 'examples_per_second': '31.565', 'grad_norm': '18.5', 'counters/examples': 258368, 'counters/updates': 8074}
train stats after 258400 examples: {'rewards_train/chosen': '0.1875', 'rewards_train/rejected': '0.10303', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084466', 'logps_train/rejected': '-147.12', 'logps_train/chosen': '-148.66', 'loss/train': '0.67045', 'examples_per_second': '31.486', 'grad_norm': '23.25', 'counters/examples': 258400, 'counters/updates': 8075}
train stats after 258432 examples: {'rewards_train/chosen': '0.15798', 'rewards_train/rejected': '0.02516', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13282', 'logps_train/rejected': '-103.54', 'logps_train/chosen': '-134.62', 'loss/train': '0.64441', 'examples_per_second': '31.285', 'grad_norm': '19.75', 'counters/examples': 258432, 'counters/updates': 8076}
train stats after 258464 examples: {'rewards_train/chosen': '0.24238', 'rewards_train/rejected': '0.048289', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1941', 'logps_train/rejected': '-117.8', 'logps_train/chosen': '-152.38', 'loss/train': '0.61658', 'examples_per_second': '29.727', 'grad_norm': '19.25', 'counters/examples': 258464, 'counters/updates': 8077}
train stats after 258496 examples: {'rewards_train/chosen': '0.3076', 'rewards_train/rejected': '0.1063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2013', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-151.65', 'loss/train': '0.61618', 'examples_per_second': '32.056', 'grad_norm': '20.375', 'counters/examples': 258496, 'counters/updates': 8078}
train stats after 258528 examples: {'rewards_train/chosen': '0.24521', 'rewards_train/rejected': '0.10714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13807', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-156.97', 'loss/train': '0.63985', 'examples_per_second': '29.961', 'grad_norm': '21', 'counters/examples': 258528, 'counters/updates': 8079}
skipping logging after 258560 examples to avoid logging too frequently
train stats after 258592 examples: {'rewards_train/chosen': '0.16815', 'rewards_train/rejected': '0.063289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10486', 'logps_train/rejected': '-118.27', 'logps_train/chosen': '-140.11', 'loss/train': '0.67308', 'examples_per_second': '31.932', 'grad_norm': '20.125', 'counters/examples': 258592, 'counters/updates': 8081}
train stats after 258624 examples: {'rewards_train/chosen': '0.14025', 'rewards_train/rejected': '0.090534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049721', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-161.89', 'loss/train': '0.68505', 'examples_per_second': '31.445', 'grad_norm': '23.5', 'counters/examples': 258624, 'counters/updates': 8082}
train stats after 258656 examples: {'rewards_train/chosen': '0.2539', 'rewards_train/rejected': '0.059122', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19478', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-168', 'loss/train': '0.62674', 'examples_per_second': '31.486', 'grad_norm': '20.875', 'counters/examples': 258656, 'counters/updates': 8083}
skipping logging after 258688 examples to avoid logging too frequently
train stats after 258720 examples: {'rewards_train/chosen': '0.22189', 'rewards_train/rejected': '0.029692', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1922', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-157.21', 'loss/train': '0.62409', 'examples_per_second': '31.199', 'grad_norm': '20', 'counters/examples': 258720, 'counters/updates': 8085}
train stats after 258752 examples: {'rewards_train/chosen': '0.2024', 'rewards_train/rejected': '0.1109', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091504', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-145.77', 'loss/train': '0.66464', 'examples_per_second': '32.74', 'grad_norm': '23', 'counters/examples': 258752, 'counters/updates': 8086}
skipping logging after 258784 examples to avoid logging too frequently
train stats after 258816 examples: {'rewards_train/chosen': '0.22503', 'rewards_train/rejected': '0.020084', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20494', 'logps_train/rejected': '-129.48', 'logps_train/chosen': '-158.85', 'loss/train': '0.61165', 'examples_per_second': '29.909', 'grad_norm': '21', 'counters/examples': 258816, 'counters/updates': 8088}
train stats after 258848 examples: {'rewards_train/chosen': '0.27612', 'rewards_train/rejected': '0.036894', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23923', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-149.84', 'loss/train': '0.59217', 'examples_per_second': '31.205', 'grad_norm': '18.875', 'counters/examples': 258848, 'counters/updates': 8089}
train stats after 258880 examples: {'rewards_train/chosen': '0.28916', 'rewards_train/rejected': '0.086267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20289', 'logps_train/rejected': '-156.2', 'logps_train/chosen': '-147.82', 'loss/train': '0.61936', 'examples_per_second': '30.507', 'grad_norm': '20.75', 'counters/examples': 258880, 'counters/updates': 8090}
train stats after 258912 examples: {'rewards_train/chosen': '0.24723', 'rewards_train/rejected': '0.095203', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15202', 'logps_train/rejected': '-159.92', 'logps_train/chosen': '-135.09', 'loss/train': '0.63452', 'examples_per_second': '30.221', 'grad_norm': '22.125', 'counters/examples': 258912, 'counters/updates': 8091}
train stats after 258944 examples: {'rewards_train/chosen': '0.20125', 'rewards_train/rejected': '0.034521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16673', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-138.5', 'loss/train': '0.63196', 'examples_per_second': '30.803', 'grad_norm': '20.375', 'counters/examples': 258944, 'counters/updates': 8092}
train stats after 258976 examples: {'rewards_train/chosen': '0.1679', 'rewards_train/rejected': '0.021636', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14627', 'logps_train/rejected': '-95.419', 'logps_train/chosen': '-152.21', 'loss/train': '0.63826', 'examples_per_second': '31.953', 'grad_norm': '20.125', 'counters/examples': 258976, 'counters/updates': 8093}
train stats after 259008 examples: {'rewards_train/chosen': '0.20213', 'rewards_train/rejected': '0.079156', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12298', 'logps_train/rejected': '-116.15', 'logps_train/chosen': '-152.51', 'loss/train': '0.65412', 'examples_per_second': '31.211', 'grad_norm': '21.375', 'counters/examples': 259008, 'counters/updates': 8094}
train stats after 259040 examples: {'rewards_train/chosen': '0.29037', 'rewards_train/rejected': '0.057646', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23272', 'logps_train/rejected': '-116.58', 'logps_train/chosen': '-178.14', 'loss/train': '0.60875', 'examples_per_second': '31.105', 'grad_norm': '21.625', 'counters/examples': 259040, 'counters/updates': 8095}
train stats after 259072 examples: {'rewards_train/chosen': '0.19977', 'rewards_train/rejected': '-0.032635', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23241', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-148.61', 'loss/train': '0.60186', 'examples_per_second': '30.651', 'grad_norm': '21.375', 'counters/examples': 259072, 'counters/updates': 8096}
train stats after 259104 examples: {'rewards_train/chosen': '0.15375', 'rewards_train/rejected': '0.045369', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10838', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-155.68', 'loss/train': '0.66452', 'examples_per_second': '31.216', 'grad_norm': '20.375', 'counters/examples': 259104, 'counters/updates': 8097}
train stats after 259136 examples: {'rewards_train/chosen': '0.2465', 'rewards_train/rejected': '0.081744', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16475', 'logps_train/rejected': '-90.256', 'logps_train/chosen': '-125.76', 'loss/train': '0.63275', 'examples_per_second': '31.504', 'grad_norm': '18.125', 'counters/examples': 259136, 'counters/updates': 8098}
train stats after 259168 examples: {'rewards_train/chosen': '0.088322', 'rewards_train/rejected': '-0.025538', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11386', 'logps_train/rejected': '-91.11', 'logps_train/chosen': '-116.36', 'loss/train': '0.64969', 'examples_per_second': '30.204', 'grad_norm': '18.125', 'counters/examples': 259168, 'counters/updates': 8099}
train stats after 259200 examples: {'rewards_train/chosen': '0.1193', 'rewards_train/rejected': '-0.13254', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25184', 'logps_train/rejected': '-151.27', 'logps_train/chosen': '-149.39', 'loss/train': '0.59201', 'examples_per_second': '31.235', 'grad_norm': '19.625', 'counters/examples': 259200, 'counters/updates': 8100}
train stats after 259232 examples: {'rewards_train/chosen': '0.2382', 'rewards_train/rejected': '0.069825', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16837', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-140.27', 'loss/train': '0.62653', 'examples_per_second': '30.237', 'grad_norm': '20.375', 'counters/examples': 259232, 'counters/updates': 8101}
train stats after 259264 examples: {'rewards_train/chosen': '0.13888', 'rewards_train/rejected': '-0.0099257', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14881', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-132.74', 'loss/train': '0.63858', 'examples_per_second': '30.099', 'grad_norm': '20.375', 'counters/examples': 259264, 'counters/updates': 8102}
train stats after 259296 examples: {'rewards_train/chosen': '0.19628', 'rewards_train/rejected': '0.046454', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14983', 'logps_train/rejected': '-97.169', 'logps_train/chosen': '-109.56', 'loss/train': '0.64832', 'examples_per_second': '31.023', 'grad_norm': '18.375', 'counters/examples': 259296, 'counters/updates': 8103}
train stats after 259328 examples: {'rewards_train/chosen': '0.20772', 'rewards_train/rejected': '0.064041', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14368', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-130.91', 'loss/train': '0.6376', 'examples_per_second': '32.498', 'grad_norm': '19.125', 'counters/examples': 259328, 'counters/updates': 8104}
skipping logging after 259360 examples to avoid logging too frequently
train stats after 259392 examples: {'rewards_train/chosen': '0.16917', 'rewards_train/rejected': '0.060177', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.109', 'logps_train/rejected': '-96.522', 'logps_train/chosen': '-122.97', 'loss/train': '0.65366', 'examples_per_second': '31.691', 'grad_norm': '18', 'counters/examples': 259392, 'counters/updates': 8106}
train stats after 259424 examples: {'rewards_train/chosen': '0.043787', 'rewards_train/rejected': '-0.028273', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07206', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-138.78', 'loss/train': '0.66926', 'examples_per_second': '31.219', 'grad_norm': '21.375', 'counters/examples': 259424, 'counters/updates': 8107}
train stats after 259456 examples: {'rewards_train/chosen': '0.21158', 'rewards_train/rejected': '0.018081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1935', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-154.94', 'loss/train': '0.62399', 'examples_per_second': '31.257', 'grad_norm': '20.625', 'counters/examples': 259456, 'counters/updates': 8108}
train stats after 259488 examples: {'rewards_train/chosen': '0.18703', 'rewards_train/rejected': '0.047616', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13942', 'logps_train/rejected': '-103.83', 'logps_train/chosen': '-128.66', 'loss/train': '0.64042', 'examples_per_second': '30.403', 'grad_norm': '18.625', 'counters/examples': 259488, 'counters/updates': 8109}
train stats after 259520 examples: {'rewards_train/chosen': '0.21019', 'rewards_train/rejected': '-0.019518', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2297', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-154.21', 'loss/train': '0.61084', 'examples_per_second': '30.192', 'grad_norm': '19.875', 'counters/examples': 259520, 'counters/updates': 8110}
train stats after 259552 examples: {'rewards_train/chosen': '0.20714', 'rewards_train/rejected': '0.014305', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19284', 'logps_train/rejected': '-97.298', 'logps_train/chosen': '-112.3', 'loss/train': '0.6267', 'examples_per_second': '30.364', 'grad_norm': '16.625', 'counters/examples': 259552, 'counters/updates': 8111}
train stats after 259584 examples: {'rewards_train/chosen': '0.27842', 'rewards_train/rejected': '0.011508', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26691', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-136.17', 'loss/train': '0.59408', 'examples_per_second': '31.222', 'grad_norm': '18.25', 'counters/examples': 259584, 'counters/updates': 8112}
train stats after 259616 examples: {'rewards_train/chosen': '0.16375', 'rewards_train/rejected': '0.064609', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099145', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-139.38', 'loss/train': '0.66168', 'examples_per_second': '30.264', 'grad_norm': '22.875', 'counters/examples': 259616, 'counters/updates': 8113}
skipping logging after 259648 examples to avoid logging too frequently
train stats after 259680 examples: {'rewards_train/chosen': '0.20567', 'rewards_train/rejected': '-0.0064965', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21216', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-190.95', 'loss/train': '0.61086', 'examples_per_second': '30.986', 'grad_norm': '22', 'counters/examples': 259680, 'counters/updates': 8115}
train stats after 259712 examples: {'rewards_train/chosen': '0.32193', 'rewards_train/rejected': '0.11241', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20953', 'logps_train/rejected': '-138.2', 'logps_train/chosen': '-166.8', 'loss/train': '0.61321', 'examples_per_second': '31.18', 'grad_norm': '20.5', 'counters/examples': 259712, 'counters/updates': 8116}
train stats after 259744 examples: {'rewards_train/chosen': '0.24181', 'rewards_train/rejected': '0.0077351', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23407', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-145.77', 'loss/train': '0.60155', 'examples_per_second': '32.293', 'grad_norm': '19.375', 'counters/examples': 259744, 'counters/updates': 8117}
train stats after 259776 examples: {'rewards_train/chosen': '0.14685', 'rewards_train/rejected': '0.036989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10987', 'logps_train/rejected': '-86.82', 'logps_train/chosen': '-158.65', 'loss/train': '0.66106', 'examples_per_second': '31.245', 'grad_norm': '20.25', 'counters/examples': 259776, 'counters/updates': 8118}
train stats after 259808 examples: {'rewards_train/chosen': '0.22533', 'rewards_train/rejected': '0.068724', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1566', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-132.67', 'loss/train': '0.63379', 'examples_per_second': '31.338', 'grad_norm': '19.625', 'counters/examples': 259808, 'counters/updates': 8119}
skipping logging after 259840 examples to avoid logging too frequently
train stats after 259872 examples: {'rewards_train/chosen': '0.19962', 'rewards_train/rejected': '-9.3703e-05', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19971', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-120.08', 'loss/train': '0.61691', 'examples_per_second': '33.6', 'grad_norm': '18.875', 'counters/examples': 259872, 'counters/updates': 8121}
train stats after 259904 examples: {'rewards_train/chosen': '0.15044', 'rewards_train/rejected': '0.043769', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10667', 'logps_train/rejected': '-97.78', 'logps_train/chosen': '-133.3', 'loss/train': '0.66956', 'examples_per_second': '31.594', 'grad_norm': '19.25', 'counters/examples': 259904, 'counters/updates': 8122}
train stats after 259936 examples: {'rewards_train/chosen': '0.29742', 'rewards_train/rejected': '0.037438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25999', 'logps_train/rejected': '-111.14', 'logps_train/chosen': '-188.69', 'loss/train': '0.60333', 'examples_per_second': '32.779', 'grad_norm': '20.625', 'counters/examples': 259936, 'counters/updates': 8123}
train stats after 259968 examples: {'rewards_train/chosen': '0.2076', 'rewards_train/rejected': '-0.040668', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24826', 'logps_train/rejected': '-96.242', 'logps_train/chosen': '-105.04', 'loss/train': '0.60266', 'examples_per_second': '31.407', 'grad_norm': '16.375', 'counters/examples': 259968, 'counters/updates': 8124}
train stats after 260000 examples: {'rewards_train/chosen': '0.2065', 'rewards_train/rejected': '0.034133', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17237', 'logps_train/rejected': '-145.21', 'logps_train/chosen': '-123.39', 'loss/train': '0.63412', 'examples_per_second': '30.836', 'grad_norm': '22.75', 'counters/examples': 260000, 'counters/updates': 8125}
train stats after 260032 examples: {'rewards_train/chosen': '0.11704', 'rewards_train/rejected': '0.052585', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064456', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-112.39', 'loss/train': '0.67577', 'examples_per_second': '31.191', 'grad_norm': '20.5', 'counters/examples': 260032, 'counters/updates': 8126}
train stats after 260064 examples: {'rewards_train/chosen': '0.13255', 'rewards_train/rejected': '-0.018036', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15058', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-114.01', 'loss/train': '0.64755', 'examples_per_second': '32.857', 'grad_norm': '19', 'counters/examples': 260064, 'counters/updates': 8127}
train stats after 260096 examples: {'rewards_train/chosen': '0.13187', 'rewards_train/rejected': '0.089925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041949', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-137.02', 'loss/train': '0.69203', 'examples_per_second': '31.248', 'grad_norm': '21.375', 'counters/examples': 260096, 'counters/updates': 8128}
train stats after 260128 examples: {'rewards_train/chosen': '0.17764', 'rewards_train/rejected': '0.10515', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072497', 'logps_train/rejected': '-115.15', 'logps_train/chosen': '-145.96', 'loss/train': '0.66889', 'examples_per_second': '30.265', 'grad_norm': '22', 'counters/examples': 260128, 'counters/updates': 8129}
train stats after 260160 examples: {'rewards_train/chosen': '0.21631', 'rewards_train/rejected': '0.039507', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17681', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-129.61', 'loss/train': '0.62766', 'examples_per_second': '31.208', 'grad_norm': '19.5', 'counters/examples': 260160, 'counters/updates': 8130}
train stats after 260192 examples: {'rewards_train/chosen': '0.1021', 'rewards_train/rejected': '0.038466', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063638', 'logps_train/rejected': '-141.94', 'logps_train/chosen': '-123.63', 'loss/train': '0.69061', 'examples_per_second': '30.095', 'grad_norm': '22', 'counters/examples': 260192, 'counters/updates': 8131}
train stats after 260224 examples: {'rewards_train/chosen': '0.12057', 'rewards_train/rejected': '0.12243', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0018535', 'logps_train/rejected': '-105.62', 'logps_train/chosen': '-144.88', 'loss/train': '0.70473', 'examples_per_second': '31.303', 'grad_norm': '21', 'counters/examples': 260224, 'counters/updates': 8132}
train stats after 260256 examples: {'rewards_train/chosen': '0.20108', 'rewards_train/rejected': '0.1168', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084283', 'logps_train/rejected': '-121.19', 'logps_train/chosen': '-123.46', 'loss/train': '0.67967', 'examples_per_second': '30.946', 'grad_norm': '20.75', 'counters/examples': 260256, 'counters/updates': 8133}
train stats after 260288 examples: {'rewards_train/chosen': '0.17754', 'rewards_train/rejected': '-0.036129', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21367', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-144.24', 'loss/train': '0.60981', 'examples_per_second': '32.458', 'grad_norm': '20.25', 'counters/examples': 260288, 'counters/updates': 8134}
skipping logging after 260320 examples to avoid logging too frequently
train stats after 260352 examples: {'rewards_train/chosen': '0.18417', 'rewards_train/rejected': '0.014194', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16997', 'logps_train/rejected': '-131.76', 'logps_train/chosen': '-145.31', 'loss/train': '0.62875', 'examples_per_second': '32.789', 'grad_norm': '20.25', 'counters/examples': 260352, 'counters/updates': 8136}
train stats after 260384 examples: {'rewards_train/chosen': '0.17825', 'rewards_train/rejected': '0.1878', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0095476', 'logps_train/rejected': '-150.9', 'logps_train/chosen': '-152.04', 'loss/train': '0.71769', 'examples_per_second': '32.073', 'grad_norm': '24', 'counters/examples': 260384, 'counters/updates': 8137}
train stats after 260416 examples: {'rewards_train/chosen': '0.19346', 'rewards_train/rejected': '0.064378', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12908', 'logps_train/rejected': '-107.28', 'logps_train/chosen': '-126.45', 'loss/train': '0.64675', 'examples_per_second': '31.756', 'grad_norm': '19.625', 'counters/examples': 260416, 'counters/updates': 8138}
train stats after 260448 examples: {'rewards_train/chosen': '0.20757', 'rewards_train/rejected': '0.016256', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19132', 'logps_train/rejected': '-119.13', 'logps_train/chosen': '-133.27', 'loss/train': '0.61723', 'examples_per_second': '31.323', 'grad_norm': '19.625', 'counters/examples': 260448, 'counters/updates': 8139}
train stats after 260480 examples: {'rewards_train/chosen': '0.10943', 'rewards_train/rejected': '0.041457', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067971', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-156.22', 'loss/train': '0.68225', 'examples_per_second': '29.823', 'grad_norm': '20.875', 'counters/examples': 260480, 'counters/updates': 8140}
train stats after 260512 examples: {'rewards_train/chosen': '0.046794', 'rewards_train/rejected': '-0.011694', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058487', 'logps_train/rejected': '-99.029', 'logps_train/chosen': '-111.04', 'loss/train': '0.67822', 'examples_per_second': '32.39', 'grad_norm': '18.75', 'counters/examples': 260512, 'counters/updates': 8141}
train stats after 260544 examples: {'rewards_train/chosen': '0.12789', 'rewards_train/rejected': '0.060617', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067271', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-110.64', 'loss/train': '0.67631', 'examples_per_second': '30.871', 'grad_norm': '19.5', 'counters/examples': 260544, 'counters/updates': 8142}
train stats after 260576 examples: {'rewards_train/chosen': '0.084256', 'rewards_train/rejected': '0.064768', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019488', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-132.98', 'loss/train': '0.697', 'examples_per_second': '30.944', 'grad_norm': '22.625', 'counters/examples': 260576, 'counters/updates': 8143}
train stats after 260608 examples: {'rewards_train/chosen': '0.19527', 'rewards_train/rejected': '0.14309', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052181', 'logps_train/rejected': '-120.18', 'logps_train/chosen': '-168.83', 'loss/train': '0.68345', 'examples_per_second': '30.825', 'grad_norm': '22.25', 'counters/examples': 260608, 'counters/updates': 8144}
train stats after 260640 examples: {'rewards_train/chosen': '0.23489', 'rewards_train/rejected': '0.048581', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18631', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-177.43', 'loss/train': '0.63333', 'examples_per_second': '30.572', 'grad_norm': '23', 'counters/examples': 260640, 'counters/updates': 8145}
train stats after 260672 examples: {'rewards_train/chosen': '0.025835', 'rewards_train/rejected': '0.014588', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011247', 'logps_train/rejected': '-156.35', 'logps_train/chosen': '-141.29', 'loss/train': '0.70205', 'examples_per_second': '30.983', 'grad_norm': '21.875', 'counters/examples': 260672, 'counters/updates': 8146}
skipping logging after 260704 examples to avoid logging too frequently
train stats after 260736 examples: {'rewards_train/chosen': '0.15804', 'rewards_train/rejected': '0.11625', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041795', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-142.05', 'loss/train': '0.68811', 'examples_per_second': '34.07', 'grad_norm': '20.75', 'counters/examples': 260736, 'counters/updates': 8148}
train stats after 260768 examples: {'rewards_train/chosen': '0.17731', 'rewards_train/rejected': '-0.062641', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23995', 'logps_train/rejected': '-93.391', 'logps_train/chosen': '-139.56', 'loss/train': '0.60236', 'examples_per_second': '30.637', 'grad_norm': '17.5', 'counters/examples': 260768, 'counters/updates': 8149}
skipping logging after 260800 examples to avoid logging too frequently
train stats after 260832 examples: {'rewards_train/chosen': '0.18448', 'rewards_train/rejected': '0.031523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15296', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-139.66', 'loss/train': '0.64574', 'examples_per_second': '30.818', 'grad_norm': '20.25', 'counters/examples': 260832, 'counters/updates': 8151}
skipping logging after 260864 examples to avoid logging too frequently
train stats after 260896 examples: {'rewards_train/chosen': '0.21299', 'rewards_train/rejected': '0.0095653', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20342', 'logps_train/rejected': '-111.9', 'logps_train/chosen': '-112.41', 'loss/train': '0.61391', 'examples_per_second': '32.756', 'grad_norm': '18', 'counters/examples': 260896, 'counters/updates': 8153}
train stats after 260928 examples: {'rewards_train/chosen': '0.11401', 'rewards_train/rejected': '0.030242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083772', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-143.87', 'loss/train': '0.66599', 'examples_per_second': '31.195', 'grad_norm': '20.375', 'counters/examples': 260928, 'counters/updates': 8154}
train stats after 260960 examples: {'rewards_train/chosen': '0.20854', 'rewards_train/rejected': '0.11095', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097582', 'logps_train/rejected': '-93.206', 'logps_train/chosen': '-105.61', 'loss/train': '0.66494', 'examples_per_second': '31.197', 'grad_norm': '18', 'counters/examples': 260960, 'counters/updates': 8155}
skipping logging after 260992 examples to avoid logging too frequently
train stats after 261024 examples: {'rewards_train/chosen': '0.19819', 'rewards_train/rejected': '0.06827', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12992', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-112.11', 'loss/train': '0.65122', 'examples_per_second': '30.294', 'grad_norm': '19.25', 'counters/examples': 261024, 'counters/updates': 8157}
train stats after 261056 examples: {'rewards_train/chosen': '0.13906', 'rewards_train/rejected': '0.063399', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075661', 'logps_train/rejected': '-133.39', 'logps_train/chosen': '-140.37', 'loss/train': '0.67194', 'examples_per_second': '30.492', 'grad_norm': '20.5', 'counters/examples': 261056, 'counters/updates': 8158}
train stats after 261088 examples: {'rewards_train/chosen': '0.18321', 'rewards_train/rejected': '0.017307', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1659', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-148.1', 'loss/train': '0.63316', 'examples_per_second': '32.042', 'grad_norm': '20.25', 'counters/examples': 261088, 'counters/updates': 8159}
train stats after 261120 examples: {'rewards_train/chosen': '0.17509', 'rewards_train/rejected': '0.033727', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14136', 'logps_train/rejected': '-93.858', 'logps_train/chosen': '-116.77', 'loss/train': '0.63484', 'examples_per_second': '30.358', 'grad_norm': '17.875', 'counters/examples': 261120, 'counters/updates': 8160}
skipping logging after 261152 examples to avoid logging too frequently
train stats after 261184 examples: {'rewards_train/chosen': '0.11256', 'rewards_train/rejected': '0.076982', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035574', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-134.58', 'loss/train': '0.68841', 'examples_per_second': '31.452', 'grad_norm': '20.875', 'counters/examples': 261184, 'counters/updates': 8162}
train stats after 261216 examples: {'rewards_train/chosen': '0.24709', 'rewards_train/rejected': '0.049621', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19747', 'logps_train/rejected': '-120.32', 'logps_train/chosen': '-155.98', 'loss/train': '0.63323', 'examples_per_second': '30.267', 'grad_norm': '21.25', 'counters/examples': 261216, 'counters/updates': 8163}
train stats after 261248 examples: {'rewards_train/chosen': '0.1978', 'rewards_train/rejected': '-0.0041888', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20199', 'logps_train/rejected': '-101.2', 'logps_train/chosen': '-126.85', 'loss/train': '0.61108', 'examples_per_second': '30.741', 'grad_norm': '18', 'counters/examples': 261248, 'counters/updates': 8164}
train stats after 261280 examples: {'rewards_train/chosen': '0.13113', 'rewards_train/rejected': '0.057065', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.074064', 'logps_train/rejected': '-81.183', 'logps_train/chosen': '-129.96', 'loss/train': '0.67551', 'examples_per_second': '30.508', 'grad_norm': '19.375', 'counters/examples': 261280, 'counters/updates': 8165}
train stats after 261312 examples: {'rewards_train/chosen': '0.1455', 'rewards_train/rejected': '0.042836', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10266', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-128.86', 'loss/train': '0.65782', 'examples_per_second': '31.491', 'grad_norm': '20.75', 'counters/examples': 261312, 'counters/updates': 8166}
train stats after 261344 examples: {'rewards_train/chosen': '0.13385', 'rewards_train/rejected': '0.083845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050009', 'logps_train/rejected': '-163.64', 'logps_train/chosen': '-167.57', 'loss/train': '0.68933', 'examples_per_second': '32.679', 'grad_norm': '24.25', 'counters/examples': 261344, 'counters/updates': 8167}
train stats after 261376 examples: {'rewards_train/chosen': '0.1248', 'rewards_train/rejected': '0.0058957', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11891', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-106.25', 'loss/train': '0.6482', 'examples_per_second': '30.149', 'grad_norm': '18.875', 'counters/examples': 261376, 'counters/updates': 8168}
skipping logging after 261408 examples to avoid logging too frequently
train stats after 261440 examples: {'rewards_train/chosen': '0.27093', 'rewards_train/rejected': '0.051014', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21992', 'logps_train/rejected': '-131.79', 'logps_train/chosen': '-161.79', 'loss/train': '0.60704', 'examples_per_second': '30.845', 'grad_norm': '21.75', 'counters/examples': 261440, 'counters/updates': 8170}
train stats after 261472 examples: {'rewards_train/chosen': '0.13443', 'rewards_train/rejected': '0.013009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12142', 'logps_train/rejected': '-97.78', 'logps_train/chosen': '-114.15', 'loss/train': '0.65516', 'examples_per_second': '31.549', 'grad_norm': '19.375', 'counters/examples': 261472, 'counters/updates': 8171}
skipping logging after 261504 examples to avoid logging too frequently
train stats after 261536 examples: {'rewards_train/chosen': '0.2664', 'rewards_train/rejected': '0.068377', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19802', 'logps_train/rejected': '-133.4', 'logps_train/chosen': '-138.1', 'loss/train': '0.61671', 'examples_per_second': '31.445', 'grad_norm': '20.75', 'counters/examples': 261536, 'counters/updates': 8173}
train stats after 261568 examples: {'rewards_train/chosen': '0.055297', 'rewards_train/rejected': '-0.016958', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072255', 'logps_train/rejected': '-100.23', 'logps_train/chosen': '-133.29', 'loss/train': '0.67007', 'examples_per_second': '31.613', 'grad_norm': '20', 'counters/examples': 261568, 'counters/updates': 8174}
train stats after 261600 examples: {'rewards_train/chosen': '0.09607', 'rewards_train/rejected': '-0.087645', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18372', 'logps_train/rejected': '-99.048', 'logps_train/chosen': '-104.57', 'loss/train': '0.63595', 'examples_per_second': '31.244', 'grad_norm': '17.25', 'counters/examples': 261600, 'counters/updates': 8175}
train stats after 261632 examples: {'rewards_train/chosen': '0.13598', 'rewards_train/rejected': '-0.0052133', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1412', 'logps_train/rejected': '-134.01', 'logps_train/chosen': '-145.55', 'loss/train': '0.65662', 'examples_per_second': '30.112', 'grad_norm': '21.25', 'counters/examples': 261632, 'counters/updates': 8176}
skipping logging after 261664 examples to avoid logging too frequently
train stats after 261696 examples: {'rewards_train/chosen': '0.12164', 'rewards_train/rejected': '-0.043652', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16529', 'logps_train/rejected': '-95.863', 'logps_train/chosen': '-103.66', 'loss/train': '0.6365', 'examples_per_second': '31.707', 'grad_norm': '17.25', 'counters/examples': 261696, 'counters/updates': 8178}
skipping logging after 261728 examples to avoid logging too frequently
train stats after 261760 examples: {'rewards_train/chosen': '0.16859', 'rewards_train/rejected': '-0.07104', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23963', 'logps_train/rejected': '-90.315', 'logps_train/chosen': '-138.37', 'loss/train': '0.59711', 'examples_per_second': '33.72', 'grad_norm': '18.125', 'counters/examples': 261760, 'counters/updates': 8180}
train stats after 261792 examples: {'rewards_train/chosen': '0.23668', 'rewards_train/rejected': '0.094833', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14185', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-127.04', 'loss/train': '0.63875', 'examples_per_second': '31.146', 'grad_norm': '18.25', 'counters/examples': 261792, 'counters/updates': 8181}
train stats after 261824 examples: {'rewards_train/chosen': '0.17145', 'rewards_train/rejected': '0.0090259', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16243', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-134.85', 'loss/train': '0.63935', 'examples_per_second': '32.387', 'grad_norm': '21', 'counters/examples': 261824, 'counters/updates': 8182}
train stats after 261856 examples: {'rewards_train/chosen': '0.27168', 'rewards_train/rejected': '0.10076', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17092', 'logps_train/rejected': '-116.6', 'logps_train/chosen': '-138.95', 'loss/train': '0.63337', 'examples_per_second': '31.818', 'grad_norm': '19.5', 'counters/examples': 261856, 'counters/updates': 8183}
train stats after 261888 examples: {'rewards_train/chosen': '0.29676', 'rewards_train/rejected': '0.012745', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.28401', 'logps_train/rejected': '-107.65', 'logps_train/chosen': '-156.99', 'loss/train': '0.57388', 'examples_per_second': '31.516', 'grad_norm': '19.375', 'counters/examples': 261888, 'counters/updates': 8184}
train stats after 261920 examples: {'rewards_train/chosen': '0.090647', 'rewards_train/rejected': '0.052469', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038178', 'logps_train/rejected': '-146.44', 'logps_train/chosen': '-145.79', 'loss/train': '0.68536', 'examples_per_second': '30.902', 'grad_norm': '22.125', 'counters/examples': 261920, 'counters/updates': 8185}
train stats after 261952 examples: {'rewards_train/chosen': '0.12852', 'rewards_train/rejected': '0.088324', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040193', 'logps_train/rejected': '-104.76', 'logps_train/chosen': '-122.98', 'loss/train': '0.68486', 'examples_per_second': '32.904', 'grad_norm': '19.625', 'counters/examples': 261952, 'counters/updates': 8186}
skipping logging after 261984 examples to avoid logging too frequently
train stats after 262016 examples: {'rewards_train/chosen': '0.16338', 'rewards_train/rejected': '-0.065345', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22873', 'logps_train/rejected': '-93.155', 'logps_train/chosen': '-136.64', 'loss/train': '0.61452', 'examples_per_second': '32.245', 'grad_norm': '19', 'counters/examples': 262016, 'counters/updates': 8188}
train stats after 262048 examples: {'rewards_train/chosen': '0.15765', 'rewards_train/rejected': '0.053945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10371', 'logps_train/rejected': '-154.08', 'logps_train/chosen': '-161.36', 'loss/train': '0.66005', 'examples_per_second': '30.07', 'grad_norm': '23.375', 'counters/examples': 262048, 'counters/updates': 8189}
train stats after 262080 examples: {'rewards_train/chosen': '0.16717', 'rewards_train/rejected': '0.089495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07768', 'logps_train/rejected': '-98.813', 'logps_train/chosen': '-127.27', 'loss/train': '0.67701', 'examples_per_second': '31.845', 'grad_norm': '19.625', 'counters/examples': 262080, 'counters/updates': 8190}
train stats after 262112 examples: {'rewards_train/chosen': '0.16355', 'rewards_train/rejected': '0.18308', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019529', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-153.92', 'loss/train': '0.72099', 'examples_per_second': '30.697', 'grad_norm': '24.375', 'counters/examples': 262112, 'counters/updates': 8191}
train stats after 262144 examples: {'rewards_train/chosen': '0.25272', 'rewards_train/rejected': '0.072391', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18033', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-149.16', 'loss/train': '0.62582', 'examples_per_second': '31.85', 'grad_norm': '19.625', 'counters/examples': 262144, 'counters/updates': 8192}
train stats after 262176 examples: {'rewards_train/chosen': '0.11216', 'rewards_train/rejected': '-0.056378', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16854', 'logps_train/rejected': '-106.92', 'logps_train/chosen': '-124.99', 'loss/train': '0.62939', 'examples_per_second': '30.541', 'grad_norm': '19', 'counters/examples': 262176, 'counters/updates': 8193}
train stats after 262208 examples: {'rewards_train/chosen': '0.22891', 'rewards_train/rejected': '0.064283', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16463', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-127.27', 'loss/train': '0.6255', 'examples_per_second': '32.287', 'grad_norm': '20.25', 'counters/examples': 262208, 'counters/updates': 8194}
train stats after 262240 examples: {'rewards_train/chosen': '0.10458', 'rewards_train/rejected': '0.12108', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016495', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-149.77', 'loss/train': '0.71414', 'examples_per_second': '31.501', 'grad_norm': '24.5', 'counters/examples': 262240, 'counters/updates': 8195}
train stats after 262272 examples: {'rewards_train/chosen': '0.16539', 'rewards_train/rejected': '0.10347', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061916', 'logps_train/rejected': '-101.04', 'logps_train/chosen': '-123.58', 'loss/train': '0.68247', 'examples_per_second': '31.617', 'grad_norm': '21.125', 'counters/examples': 262272, 'counters/updates': 8196}
train stats after 262304 examples: {'rewards_train/chosen': '0.19069', 'rewards_train/rejected': '0.041143', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14955', 'logps_train/rejected': '-96.631', 'logps_train/chosen': '-137.04', 'loss/train': '0.6487', 'examples_per_second': '31.985', 'grad_norm': '18.5', 'counters/examples': 262304, 'counters/updates': 8197}
train stats after 262336 examples: {'rewards_train/chosen': '0.19873', 'rewards_train/rejected': '0.097276', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10145', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-139.72', 'loss/train': '0.65116', 'examples_per_second': '31.227', 'grad_norm': '20.5', 'counters/examples': 262336, 'counters/updates': 8198}
train stats after 262368 examples: {'rewards_train/chosen': '0.1569', 'rewards_train/rejected': '0.045302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1116', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-137.65', 'loss/train': '0.652', 'examples_per_second': '31.556', 'grad_norm': '21.625', 'counters/examples': 262368, 'counters/updates': 8199}
train stats after 262400 examples: {'rewards_train/chosen': '0.12251', 'rewards_train/rejected': '0.065521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056986', 'logps_train/rejected': '-134.94', 'logps_train/chosen': '-97.503', 'loss/train': '0.67893', 'examples_per_second': '31.522', 'grad_norm': '19.125', 'counters/examples': 262400, 'counters/updates': 8200}
train stats after 262432 examples: {'rewards_train/chosen': '0.16563', 'rewards_train/rejected': '0.16081', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0048229', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-107.1', 'loss/train': '0.71594', 'examples_per_second': '30.167', 'grad_norm': '19.625', 'counters/examples': 262432, 'counters/updates': 8201}
skipping logging after 262464 examples to avoid logging too frequently
train stats after 262496 examples: {'rewards_train/chosen': '0.3414', 'rewards_train/rejected': '0.011461', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.32994', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-154.33', 'loss/train': '0.55987', 'examples_per_second': '31.713', 'grad_norm': '19', 'counters/examples': 262496, 'counters/updates': 8203}
train stats after 262528 examples: {'rewards_train/chosen': '0.19138', 'rewards_train/rejected': '0.015149', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17624', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-139.97', 'loss/train': '0.62848', 'examples_per_second': '24.38', 'grad_norm': '20.25', 'counters/examples': 262528, 'counters/updates': 8204}
train stats after 262560 examples: {'rewards_train/chosen': '0.193', 'rewards_train/rejected': '0.018496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1745', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-134.08', 'loss/train': '0.63018', 'examples_per_second': '31.393', 'grad_norm': '19.25', 'counters/examples': 262560, 'counters/updates': 8205}
train stats after 262592 examples: {'rewards_train/chosen': '0.17433', 'rewards_train/rejected': '0.010431', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1639', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-137.37', 'loss/train': '0.63992', 'examples_per_second': '31.553', 'grad_norm': '19.5', 'counters/examples': 262592, 'counters/updates': 8206}
train stats after 262624 examples: {'rewards_train/chosen': '0.21716', 'rewards_train/rejected': '0.037366', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17979', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-147.81', 'loss/train': '0.62624', 'examples_per_second': '27.071', 'grad_norm': '21.125', 'counters/examples': 262624, 'counters/updates': 8207}
train stats after 262656 examples: {'rewards_train/chosen': '0.21811', 'rewards_train/rejected': '0.11465', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10346', 'logps_train/rejected': '-135.01', 'logps_train/chosen': '-130.22', 'loss/train': '0.66472', 'examples_per_second': '30.393', 'grad_norm': '20.75', 'counters/examples': 262656, 'counters/updates': 8208}
train stats after 262688 examples: {'rewards_train/chosen': '0.32903', 'rewards_train/rejected': '0.14994', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17909', 'logps_train/rejected': '-141.36', 'logps_train/chosen': '-172.28', 'loss/train': '0.62892', 'examples_per_second': '33.12', 'grad_norm': '22.25', 'counters/examples': 262688, 'counters/updates': 8209}
skipping logging after 262720 examples to avoid logging too frequently
train stats after 262752 examples: {'rewards_train/chosen': '0.068783', 'rewards_train/rejected': '0.036259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032523', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-123.66', 'loss/train': '0.6984', 'examples_per_second': '31.683', 'grad_norm': '22.875', 'counters/examples': 262752, 'counters/updates': 8211}
train stats after 262784 examples: {'rewards_train/chosen': '0.23638', 'rewards_train/rejected': '0.022778', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2136', 'logps_train/rejected': '-143.89', 'logps_train/chosen': '-138.8', 'loss/train': '0.61258', 'examples_per_second': '31.926', 'grad_norm': '20.25', 'counters/examples': 262784, 'counters/updates': 8212}
train stats after 262816 examples: {'rewards_train/chosen': '0.26989', 'rewards_train/rejected': '-0.0047115', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2746', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-142.58', 'loss/train': '0.59784', 'examples_per_second': '31.264', 'grad_norm': '17.875', 'counters/examples': 262816, 'counters/updates': 8213}
train stats after 262848 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '0.084048', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070937', 'logps_train/rejected': '-150.9', 'logps_train/chosen': '-175.56', 'loss/train': '0.67532', 'examples_per_second': '30.018', 'grad_norm': '23.125', 'counters/examples': 262848, 'counters/updates': 8214}
skipping logging after 262880 examples to avoid logging too frequently
train stats after 262912 examples: {'rewards_train/chosen': '0.24459', 'rewards_train/rejected': '0.086955', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15763', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-153.67', 'loss/train': '0.64753', 'examples_per_second': '34.2', 'grad_norm': '22', 'counters/examples': 262912, 'counters/updates': 8216}
skipping logging after 262944 examples to avoid logging too frequently
train stats after 262976 examples: {'rewards_train/chosen': '0.21223', 'rewards_train/rejected': '-0.01285', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22508', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-147.53', 'loss/train': '0.61503', 'examples_per_second': '31.443', 'grad_norm': '21', 'counters/examples': 262976, 'counters/updates': 8218}
train stats after 263008 examples: {'rewards_train/chosen': '0.3166', 'rewards_train/rejected': '0.11102', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20558', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-122.41', 'loss/train': '0.61374', 'examples_per_second': '30.62', 'grad_norm': '19.625', 'counters/examples': 263008, 'counters/updates': 8219}
skipping logging after 263040 examples to avoid logging too frequently
train stats after 263072 examples: {'rewards_train/chosen': '0.26531', 'rewards_train/rejected': '0.023324', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24199', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-141.29', 'loss/train': '0.59765', 'examples_per_second': '30.936', 'grad_norm': '19.625', 'counters/examples': 263072, 'counters/updates': 8221}
skipping logging after 263104 examples to avoid logging too frequently
train stats after 263136 examples: {'rewards_train/chosen': '0.21872', 'rewards_train/rejected': '0.075035', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14369', 'logps_train/rejected': '-145.16', 'logps_train/chosen': '-154.69', 'loss/train': '0.6357', 'examples_per_second': '31.336', 'grad_norm': '23.625', 'counters/examples': 263136, 'counters/updates': 8223}
train stats after 263168 examples: {'rewards_train/chosen': '0.20919', 'rewards_train/rejected': '0.038739', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17045', 'logps_train/rejected': '-133.39', 'logps_train/chosen': '-155.7', 'loss/train': '0.63671', 'examples_per_second': '32.534', 'grad_norm': '22.625', 'counters/examples': 263168, 'counters/updates': 8224}
train stats after 263200 examples: {'rewards_train/chosen': '0.19322', 'rewards_train/rejected': '0.013836', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.17939', 'logps_train/rejected': '-104.11', 'logps_train/chosen': '-135.2', 'loss/train': '0.62812', 'examples_per_second': '31.033', 'grad_norm': '19.5', 'counters/examples': 263200, 'counters/updates': 8225}
train stats after 263232 examples: {'rewards_train/chosen': '0.077985', 'rewards_train/rejected': '-0.011949', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089933', 'logps_train/rejected': '-109.46', 'logps_train/chosen': '-162.75', 'loss/train': '0.66646', 'examples_per_second': '32.805', 'grad_norm': '21.5', 'counters/examples': 263232, 'counters/updates': 8226}
train stats after 263264 examples: {'rewards_train/chosen': '0.23233', 'rewards_train/rejected': '0.080942', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15139', 'logps_train/rejected': '-163.96', 'logps_train/chosen': '-180.83', 'loss/train': '0.63657', 'examples_per_second': '31.503', 'grad_norm': '23.625', 'counters/examples': 263264, 'counters/updates': 8227}
train stats after 263296 examples: {'rewards_train/chosen': '0.24902', 'rewards_train/rejected': '0.05553', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19349', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-174.68', 'loss/train': '0.62578', 'examples_per_second': '30.967', 'grad_norm': '22.5', 'counters/examples': 263296, 'counters/updates': 8228}
train stats after 263328 examples: {'rewards_train/chosen': '0.28757', 'rewards_train/rejected': '-0.0092848', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.29685', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-148.42', 'loss/train': '0.56776', 'examples_per_second': '32.759', 'grad_norm': '18.125', 'counters/examples': 263328, 'counters/updates': 8229}
train stats after 263360 examples: {'rewards_train/chosen': '0.15268', 'rewards_train/rejected': '0.034038', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11864', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-162.1', 'loss/train': '0.65346', 'examples_per_second': '31.595', 'grad_norm': '21.5', 'counters/examples': 263360, 'counters/updates': 8230}
train stats after 263392 examples: {'rewards_train/chosen': '0.20091', 'rewards_train/rejected': '0.022017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17889', 'logps_train/rejected': '-97.36', 'logps_train/chosen': '-140.1', 'loss/train': '0.63176', 'examples_per_second': '31.212', 'grad_norm': '19.125', 'counters/examples': 263392, 'counters/updates': 8231}
train stats after 263424 examples: {'rewards_train/chosen': '0.16274', 'rewards_train/rejected': '-0.032966', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19571', 'logps_train/rejected': '-93.358', 'logps_train/chosen': '-146.68', 'loss/train': '0.60845', 'examples_per_second': '31.546', 'grad_norm': '19.5', 'counters/examples': 263424, 'counters/updates': 8232}
train stats after 263456 examples: {'rewards_train/chosen': '0.17118', 'rewards_train/rejected': '-0.027953', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19913', 'logps_train/rejected': '-93.558', 'logps_train/chosen': '-117.85', 'loss/train': '0.61939', 'examples_per_second': '31.57', 'grad_norm': '17.375', 'counters/examples': 263456, 'counters/updates': 8233}
train stats after 263488 examples: {'rewards_train/chosen': '0.24746', 'rewards_train/rejected': '0.028065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21939', 'logps_train/rejected': '-152.23', 'logps_train/chosen': '-132.82', 'loss/train': '0.61386', 'examples_per_second': '31.397', 'grad_norm': '18.875', 'counters/examples': 263488, 'counters/updates': 8234}
train stats after 263520 examples: {'rewards_train/chosen': '0.27937', 'rewards_train/rejected': '0.063027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21634', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-164.38', 'loss/train': '0.60672', 'examples_per_second': '23.806', 'grad_norm': '20.875', 'counters/examples': 263520, 'counters/updates': 8235}
train stats after 263552 examples: {'rewards_train/chosen': '0.15618', 'rewards_train/rejected': '0.050909', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10527', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-140.11', 'loss/train': '0.66638', 'examples_per_second': '30.026', 'grad_norm': '21.125', 'counters/examples': 263552, 'counters/updates': 8236}
train stats after 263584 examples: {'rewards_train/chosen': '0.16335', 'rewards_train/rejected': '0.06364', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099705', 'logps_train/rejected': '-98.188', 'logps_train/chosen': '-116.68', 'loss/train': '0.65649', 'examples_per_second': '31.119', 'grad_norm': '17.875', 'counters/examples': 263584, 'counters/updates': 8237}
skipping logging after 263616 examples to avoid logging too frequently
train stats after 263648 examples: {'rewards_train/chosen': '0.19086', 'rewards_train/rejected': '-0.031939', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2228', 'logps_train/rejected': '-128.51', 'logps_train/chosen': '-118.29', 'loss/train': '0.60178', 'examples_per_second': '36.102', 'grad_norm': '18', 'counters/examples': 263648, 'counters/updates': 8239}
train stats after 263680 examples: {'rewards_train/chosen': '0.1879', 'rewards_train/rejected': '0.080766', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10713', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-111.57', 'loss/train': '0.66423', 'examples_per_second': '32.084', 'grad_norm': '19.25', 'counters/examples': 263680, 'counters/updates': 8240}
train stats after 263712 examples: {'rewards_train/chosen': '0.16981', 'rewards_train/rejected': '-0.0058379', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17565', 'logps_train/rejected': '-144.88', 'logps_train/chosen': '-145.15', 'loss/train': '0.62402', 'examples_per_second': '31.971', 'grad_norm': '22.375', 'counters/examples': 263712, 'counters/updates': 8241}
train stats after 263744 examples: {'rewards_train/chosen': '0.15102', 'rewards_train/rejected': '0.06225', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088765', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-111.72', 'loss/train': '0.66429', 'examples_per_second': '30.449', 'grad_norm': '19.875', 'counters/examples': 263744, 'counters/updates': 8242}
train stats after 263776 examples: {'rewards_train/chosen': '0.19056', 'rewards_train/rejected': '0.068533', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12203', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-101.5', 'loss/train': '0.64414', 'examples_per_second': '31.478', 'grad_norm': '18.625', 'counters/examples': 263776, 'counters/updates': 8243}
train stats after 263808 examples: {'rewards_train/chosen': '0.35583', 'rewards_train/rejected': '0.11108', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24475', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-186.4', 'loss/train': '0.60104', 'examples_per_second': '31.596', 'grad_norm': '22.5', 'counters/examples': 263808, 'counters/updates': 8244}
train stats after 263840 examples: {'rewards_train/chosen': '0.2469', 'rewards_train/rejected': '0.079527', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16737', 'logps_train/rejected': '-150.73', 'logps_train/chosen': '-159.1', 'loss/train': '0.64078', 'examples_per_second': '29.927', 'grad_norm': '21.625', 'counters/examples': 263840, 'counters/updates': 8245}
skipping logging after 263872 examples to avoid logging too frequently
train stats after 263904 examples: {'rewards_train/chosen': '0.18542', 'rewards_train/rejected': '0.03618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14924', 'logps_train/rejected': '-88.046', 'logps_train/chosen': '-132.43', 'loss/train': '0.63091', 'examples_per_second': '31.5', 'grad_norm': '18.375', 'counters/examples': 263904, 'counters/updates': 8247}
train stats after 263936 examples: {'rewards_train/chosen': '0.14843', 'rewards_train/rejected': '0.063334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085094', 'logps_train/rejected': '-105.57', 'logps_train/chosen': '-161.83', 'loss/train': '0.66518', 'examples_per_second': '30.135', 'grad_norm': '22', 'counters/examples': 263936, 'counters/updates': 8248}
train stats after 263968 examples: {'rewards_train/chosen': '0.12593', 'rewards_train/rejected': '0.041708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084218', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-138.45', 'loss/train': '0.66885', 'examples_per_second': '31.044', 'grad_norm': '20.375', 'counters/examples': 263968, 'counters/updates': 8249}
train stats after 264000 examples: {'rewards_train/chosen': '0.16676', 'rewards_train/rejected': '0.040098', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12667', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-165.49', 'loss/train': '0.64375', 'examples_per_second': '30.962', 'grad_norm': '21.625', 'counters/examples': 264000, 'counters/updates': 8250}
Running evaluation after 264000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.21it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.89it/s]
eval after 264000: {'rewards_eval/chosen': '0.1946', 'rewards_eval/rejected': '0.038139', 'rewards_eval/accuracies': '0.60938', 'rewards_eval/margins': '0.15646', 'logps_eval/rejected': '-115.01', 'logps_eval/chosen': '-133.6', 'loss/eval': '0.64134'}
skipping save for non epoch
train stats after 264032 examples: {'rewards_train/chosen': '0.1301', 'rewards_train/rejected': '0.061413', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068688', 'logps_train/rejected': '-127.28', 'logps_train/chosen': '-139.73', 'loss/train': '0.68487', 'examples_per_second': '36.427', 'grad_norm': '21.375', 'counters/examples': 264032, 'counters/updates': 8251}
train stats after 264064 examples: {'rewards_train/chosen': '0.14006', 'rewards_train/rejected': '0.03593', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10413', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-123.39', 'loss/train': '0.65177', 'examples_per_second': '32.408', 'grad_norm': '18.625', 'counters/examples': 264064, 'counters/updates': 8252}
train stats after 264096 examples: {'rewards_train/chosen': '0.15278', 'rewards_train/rejected': '0.0070063', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14578', 'logps_train/rejected': '-95.649', 'logps_train/chosen': '-140.88', 'loss/train': '0.6393', 'examples_per_second': '32.902', 'grad_norm': '19.375', 'counters/examples': 264096, 'counters/updates': 8253}
train stats after 264128 examples: {'rewards_train/chosen': '0.16702', 'rewards_train/rejected': '0.017271', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14975', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-159.14', 'loss/train': '0.63993', 'examples_per_second': '31.558', 'grad_norm': '21.625', 'counters/examples': 264128, 'counters/updates': 8254}
train stats after 264160 examples: {'rewards_train/chosen': '0.23729', 'rewards_train/rejected': '0.0078867', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22941', 'logps_train/rejected': '-87.734', 'logps_train/chosen': '-137.22', 'loss/train': '0.60602', 'examples_per_second': '31.71', 'grad_norm': '18.375', 'counters/examples': 264160, 'counters/updates': 8255}
train stats after 264192 examples: {'rewards_train/chosen': '0.10688', 'rewards_train/rejected': '0.047069', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059814', 'logps_train/rejected': '-114.29', 'logps_train/chosen': '-141.9', 'loss/train': '0.6844', 'examples_per_second': '32.03', 'grad_norm': '21.875', 'counters/examples': 264192, 'counters/updates': 8256}
train stats after 264224 examples: {'rewards_train/chosen': '0.2563', 'rewards_train/rejected': '0.046274', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21002', 'logps_train/rejected': '-89.85', 'logps_train/chosen': '-112.66', 'loss/train': '0.615', 'examples_per_second': '31.11', 'grad_norm': '17.375', 'counters/examples': 264224, 'counters/updates': 8257}
train stats after 264256 examples: {'rewards_train/chosen': '0.14188', 'rewards_train/rejected': '-0.0016783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14356', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-110.88', 'loss/train': '0.64177', 'examples_per_second': '30.751', 'grad_norm': '19.625', 'counters/examples': 264256, 'counters/updates': 8258}
skipping logging after 264288 examples to avoid logging too frequently
train stats after 264320 examples: {'rewards_train/chosen': '0.20392', 'rewards_train/rejected': '0.018274', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18565', 'logps_train/rejected': '-104.57', 'logps_train/chosen': '-152.62', 'loss/train': '0.63224', 'examples_per_second': '31.57', 'grad_norm': '18.875', 'counters/examples': 264320, 'counters/updates': 8260}
train stats after 264352 examples: {'rewards_train/chosen': '0.078072', 'rewards_train/rejected': '0.098235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.020163', 'logps_train/rejected': '-106.38', 'logps_train/chosen': '-108.72', 'loss/train': '0.70962', 'examples_per_second': '31.249', 'grad_norm': '19.625', 'counters/examples': 264352, 'counters/updates': 8261}
train stats after 264384 examples: {'rewards_train/chosen': '0.11105', 'rewards_train/rejected': '0.10099', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010061', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-141.46', 'loss/train': '0.71038', 'examples_per_second': '31.523', 'grad_norm': '22.875', 'counters/examples': 264384, 'counters/updates': 8262}
train stats after 264416 examples: {'rewards_train/chosen': '0.13545', 'rewards_train/rejected': '0.029551', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1059', 'logps_train/rejected': '-144.84', 'logps_train/chosen': '-107.15', 'loss/train': '0.65442', 'examples_per_second': '31.588', 'grad_norm': '18.625', 'counters/examples': 264416, 'counters/updates': 8263}
train stats after 264448 examples: {'rewards_train/chosen': '0.17679', 'rewards_train/rejected': '0.12547', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051314', 'logps_train/rejected': '-159.28', 'logps_train/chosen': '-159.57', 'loss/train': '0.67874', 'examples_per_second': '29.988', 'grad_norm': '22.5', 'counters/examples': 264448, 'counters/updates': 8264}
train stats after 264480 examples: {'rewards_train/chosen': '0.27056', 'rewards_train/rejected': '0.010304', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.26026', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-123.46', 'loss/train': '0.59035', 'examples_per_second': '32.565', 'grad_norm': '19.625', 'counters/examples': 264480, 'counters/updates': 8265}
train stats after 264512 examples: {'rewards_train/chosen': '0.16786', 'rewards_train/rejected': '0.0076857', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16018', 'logps_train/rejected': '-113.4', 'logps_train/chosen': '-136.25', 'loss/train': '0.62785', 'examples_per_second': '31.555', 'grad_norm': '18.875', 'counters/examples': 264512, 'counters/updates': 8266}
train stats after 264544 examples: {'rewards_train/chosen': '0.10158', 'rewards_train/rejected': '0.016044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085535', 'logps_train/rejected': '-108.16', 'logps_train/chosen': '-131.22', 'loss/train': '0.66295', 'examples_per_second': '30.753', 'grad_norm': '21.625', 'counters/examples': 264544, 'counters/updates': 8267}
train stats after 264576 examples: {'rewards_train/chosen': '0.13485', 'rewards_train/rejected': '0.077141', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05771', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-129.56', 'loss/train': '0.67943', 'examples_per_second': '30.196', 'grad_norm': '20.5', 'counters/examples': 264576, 'counters/updates': 8268}
train stats after 264608 examples: {'rewards_train/chosen': '0.33436', 'rewards_train/rejected': '0.10219', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23217', 'logps_train/rejected': '-101.47', 'logps_train/chosen': '-161.94', 'loss/train': '0.61202', 'examples_per_second': '30.213', 'grad_norm': '19.125', 'counters/examples': 264608, 'counters/updates': 8269}
skipping logging after 264640 examples to avoid logging too frequently
train stats after 264672 examples: {'rewards_train/chosen': '0.1039', 'rewards_train/rejected': '-0.012401', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1163', 'logps_train/rejected': '-105.47', 'logps_train/chosen': '-104.19', 'loss/train': '0.65732', 'examples_per_second': '32.869', 'grad_norm': '18', 'counters/examples': 264672, 'counters/updates': 8271}
train stats after 264704 examples: {'rewards_train/chosen': '0.28042', 'rewards_train/rejected': '0.042071', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23835', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-141.35', 'loss/train': '0.59734', 'examples_per_second': '32.127', 'grad_norm': '17.875', 'counters/examples': 264704, 'counters/updates': 8272}
skipping logging after 264736 examples to avoid logging too frequently
train stats after 264768 examples: {'rewards_train/chosen': '0.20886', 'rewards_train/rejected': '0.044604', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16426', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-164.82', 'loss/train': '0.64178', 'examples_per_second': '34.88', 'grad_norm': '21', 'counters/examples': 264768, 'counters/updates': 8274}
skipping logging after 264800 examples to avoid logging too frequently
train stats after 264832 examples: {'rewards_train/chosen': '0.14545', 'rewards_train/rejected': '0.037195', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10825', 'logps_train/rejected': '-115.86', 'logps_train/chosen': '-118.5', 'loss/train': '0.65644', 'examples_per_second': '31.669', 'grad_norm': '18.625', 'counters/examples': 264832, 'counters/updates': 8276}
train stats after 264864 examples: {'rewards_train/chosen': '0.18068', 'rewards_train/rejected': '0.073371', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10731', 'logps_train/rejected': '-124.16', 'logps_train/chosen': '-128.55', 'loss/train': '0.66086', 'examples_per_second': '30.946', 'grad_norm': '22.625', 'counters/examples': 264864, 'counters/updates': 8277}
train stats after 264896 examples: {'rewards_train/chosen': '0.25482', 'rewards_train/rejected': '0.13012', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1247', 'logps_train/rejected': '-112.83', 'logps_train/chosen': '-138.72', 'loss/train': '0.64315', 'examples_per_second': '32.328', 'grad_norm': '18.75', 'counters/examples': 264896, 'counters/updates': 8278}
train stats after 264928 examples: {'rewards_train/chosen': '0.17091', 'rewards_train/rejected': '0.077766', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093141', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-133', 'loss/train': '0.65894', 'examples_per_second': '30.597', 'grad_norm': '21', 'counters/examples': 264928, 'counters/updates': 8279}
train stats after 264960 examples: {'rewards_train/chosen': '0.22869', 'rewards_train/rejected': '0.056076', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17261', 'logps_train/rejected': '-118.73', 'logps_train/chosen': '-143.69', 'loss/train': '0.62707', 'examples_per_second': '30.521', 'grad_norm': '21', 'counters/examples': 264960, 'counters/updates': 8280}
train stats after 264992 examples: {'rewards_train/chosen': '0.24274', 'rewards_train/rejected': '0.037684', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20505', 'logps_train/rejected': '-131.5', 'logps_train/chosen': '-154.69', 'loss/train': '0.62322', 'examples_per_second': '32.501', 'grad_norm': '22.875', 'counters/examples': 264992, 'counters/updates': 8281}
train stats after 265024 examples: {'rewards_train/chosen': '0.13908', 'rewards_train/rejected': '0.003863', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13522', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-127.61', 'loss/train': '0.64543', 'examples_per_second': '31.648', 'grad_norm': '20.875', 'counters/examples': 265024, 'counters/updates': 8282}
train stats after 265056 examples: {'rewards_train/chosen': '0.29307', 'rewards_train/rejected': '0.13354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15952', 'logps_train/rejected': '-143.73', 'logps_train/chosen': '-158.25', 'loss/train': '0.64303', 'examples_per_second': '30.139', 'grad_norm': '22.5', 'counters/examples': 265056, 'counters/updates': 8283}
train stats after 265088 examples: {'rewards_train/chosen': '0.15676', 'rewards_train/rejected': '0.059426', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097334', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-122.39', 'loss/train': '0.66417', 'examples_per_second': '32.755', 'grad_norm': '21.125', 'counters/examples': 265088, 'counters/updates': 8284}
train stats after 265120 examples: {'rewards_train/chosen': '0.17797', 'rewards_train/rejected': '0.041266', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13671', 'logps_train/rejected': '-132.9', 'logps_train/chosen': '-141.27', 'loss/train': '0.64546', 'examples_per_second': '31.503', 'grad_norm': '20.75', 'counters/examples': 265120, 'counters/updates': 8285}
train stats after 265152 examples: {'rewards_train/chosen': '0.11184', 'rewards_train/rejected': '0.027695', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084141', 'logps_train/rejected': '-111.15', 'logps_train/chosen': '-121.6', 'loss/train': '0.67156', 'examples_per_second': '30.917', 'grad_norm': '21.125', 'counters/examples': 265152, 'counters/updates': 8286}
train stats after 265184 examples: {'rewards_train/chosen': '0.10478', 'rewards_train/rejected': '-0.02079', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12557', 'logps_train/rejected': '-132.07', 'logps_train/chosen': '-113.21', 'loss/train': '0.64746', 'examples_per_second': '32.329', 'grad_norm': '19.5', 'counters/examples': 265184, 'counters/updates': 8287}
skipping logging after 265216 examples to avoid logging too frequently
train stats after 265248 examples: {'rewards_train/chosen': '0.17154', 'rewards_train/rejected': '-0.00020748', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17175', 'logps_train/rejected': '-90.904', 'logps_train/chosen': '-153.98', 'loss/train': '0.62977', 'examples_per_second': '30.123', 'grad_norm': '18.5', 'counters/examples': 265248, 'counters/updates': 8289}
train stats after 265280 examples: {'rewards_train/chosen': '0.10275', 'rewards_train/rejected': '0.054337', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.04841', 'logps_train/rejected': '-117.9', 'logps_train/chosen': '-152.73', 'loss/train': '0.68259', 'examples_per_second': '32.466', 'grad_norm': '22.25', 'counters/examples': 265280, 'counters/updates': 8290}
train stats after 265312 examples: {'rewards_train/chosen': '0.19956', 'rewards_train/rejected': '0.017277', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18228', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-161.84', 'loss/train': '0.63007', 'examples_per_second': '31.598', 'grad_norm': '21.25', 'counters/examples': 265312, 'counters/updates': 8291}
train stats after 265344 examples: {'rewards_train/chosen': '0.30317', 'rewards_train/rejected': '-0.001194', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.30436', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-123.28', 'loss/train': '0.58723', 'examples_per_second': '32.842', 'grad_norm': '19.25', 'counters/examples': 265344, 'counters/updates': 8292}
train stats after 265376 examples: {'rewards_train/chosen': '0.14085', 'rewards_train/rejected': '0.0090216', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13183', 'logps_train/rejected': '-100.29', 'logps_train/chosen': '-126.84', 'loss/train': '0.63768', 'examples_per_second': '31.466', 'grad_norm': '19.125', 'counters/examples': 265376, 'counters/updates': 8293}
train stats after 265408 examples: {'rewards_train/chosen': '0.16915', 'rewards_train/rejected': '0.14248', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026675', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-130.68', 'loss/train': '0.69293', 'examples_per_second': '31.134', 'grad_norm': '22.625', 'counters/examples': 265408, 'counters/updates': 8294}
skipping logging after 265440 examples to avoid logging too frequently
train stats after 265472 examples: {'rewards_train/chosen': '0.13841', 'rewards_train/rejected': '-0.02238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16079', 'logps_train/rejected': '-95.335', 'logps_train/chosen': '-100.78', 'loss/train': '0.63783', 'examples_per_second': '34.258', 'grad_norm': '16.75', 'counters/examples': 265472, 'counters/updates': 8296}
train stats after 265504 examples: {'rewards_train/chosen': '0.20286', 'rewards_train/rejected': '0.084784', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11807', 'logps_train/rejected': '-118.3', 'logps_train/chosen': '-162.72', 'loss/train': '0.65914', 'examples_per_second': '29.923', 'grad_norm': '21.875', 'counters/examples': 265504, 'counters/updates': 8297}
train stats after 265536 examples: {'rewards_train/chosen': '0.093003', 'rewards_train/rejected': '-0.026785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11979', 'logps_train/rejected': '-98.434', 'logps_train/chosen': '-136.73', 'loss/train': '0.65362', 'examples_per_second': '31.891', 'grad_norm': '19.75', 'counters/examples': 265536, 'counters/updates': 8298}
skipping logging after 265568 examples to avoid logging too frequently
train stats after 265600 examples: {'rewards_train/chosen': '0.27533', 'rewards_train/rejected': '0.064946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21038', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-151.76', 'loss/train': '0.61398', 'examples_per_second': '33.284', 'grad_norm': '19', 'counters/examples': 265600, 'counters/updates': 8300}
train stats after 265632 examples: {'rewards_train/chosen': '0.14938', 'rewards_train/rejected': '-0.047898', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19728', 'logps_train/rejected': '-91.913', 'logps_train/chosen': '-117.51', 'loss/train': '0.61502', 'examples_per_second': '32.667', 'grad_norm': '17', 'counters/examples': 265632, 'counters/updates': 8301}
train stats after 265664 examples: {'rewards_train/chosen': '0.12743', 'rewards_train/rejected': '-0.06978', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19721', 'logps_train/rejected': '-141.44', 'logps_train/chosen': '-135.39', 'loss/train': '0.61325', 'examples_per_second': '30.036', 'grad_norm': '20.875', 'counters/examples': 265664, 'counters/updates': 8302}
train stats after 265696 examples: {'rewards_train/chosen': '0.26923', 'rewards_train/rejected': '0.0012437', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26798', 'logps_train/rejected': '-145.19', 'logps_train/chosen': '-171.86', 'loss/train': '0.59932', 'examples_per_second': '30.035', 'grad_norm': '19.75', 'counters/examples': 265696, 'counters/updates': 8303}
train stats after 265728 examples: {'rewards_train/chosen': '0.1749', 'rewards_train/rejected': '-0.006475', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18138', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-126.75', 'loss/train': '0.63009', 'examples_per_second': '32.38', 'grad_norm': '19.875', 'counters/examples': 265728, 'counters/updates': 8304}
train stats after 265760 examples: {'rewards_train/chosen': '0.092095', 'rewards_train/rejected': '0.056145', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03595', 'logps_train/rejected': '-105.73', 'logps_train/chosen': '-126.57', 'loss/train': '0.6887', 'examples_per_second': '31.459', 'grad_norm': '19.75', 'counters/examples': 265760, 'counters/updates': 8305}
train stats after 265792 examples: {'rewards_train/chosen': '0.24956', 'rewards_train/rejected': '0.056494', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19307', 'logps_train/rejected': '-121.84', 'logps_train/chosen': '-176.5', 'loss/train': '0.61893', 'examples_per_second': '31.579', 'grad_norm': '20.75', 'counters/examples': 265792, 'counters/updates': 8306}
train stats after 265824 examples: {'rewards_train/chosen': '0.16453', 'rewards_train/rejected': '-0.035457', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19999', 'logps_train/rejected': '-98.377', 'logps_train/chosen': '-162.35', 'loss/train': '0.61254', 'examples_per_second': '32.833', 'grad_norm': '20.5', 'counters/examples': 265824, 'counters/updates': 8307}
train stats after 265856 examples: {'rewards_train/chosen': '0.15183', 'rewards_train/rejected': '0.16384', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01201', 'logps_train/rejected': '-105.38', 'logps_train/chosen': '-131.09', 'loss/train': '0.71663', 'examples_per_second': '31.495', 'grad_norm': '22.875', 'counters/examples': 265856, 'counters/updates': 8308}
train stats after 265888 examples: {'rewards_train/chosen': '0.21918', 'rewards_train/rejected': '0.10565', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11353', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-125.58', 'loss/train': '0.65741', 'examples_per_second': '30.668', 'grad_norm': '19.875', 'counters/examples': 265888, 'counters/updates': 8309}
train stats after 265920 examples: {'rewards_train/chosen': '0.22829', 'rewards_train/rejected': '0.094668', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13362', 'logps_train/rejected': '-141.4', 'logps_train/chosen': '-154.54', 'loss/train': '0.64159', 'examples_per_second': '31.908', 'grad_norm': '20.875', 'counters/examples': 265920, 'counters/updates': 8310}
train stats after 265952 examples: {'rewards_train/chosen': '0.23261', 'rewards_train/rejected': '-0.01291', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24552', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-160.87', 'loss/train': '0.58978', 'examples_per_second': '32.087', 'grad_norm': '20.75', 'counters/examples': 265952, 'counters/updates': 8311}
train stats after 265984 examples: {'rewards_train/chosen': '0.24043', 'rewards_train/rejected': '-0.039055', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27948', 'logps_train/rejected': '-106.83', 'logps_train/chosen': '-136.83', 'loss/train': '0.58709', 'examples_per_second': '31.434', 'grad_norm': '17.75', 'counters/examples': 265984, 'counters/updates': 8312}
train stats after 266016 examples: {'rewards_train/chosen': '0.18008', 'rewards_train/rejected': '0.032883', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1472', 'logps_train/rejected': '-107.27', 'logps_train/chosen': '-138.18', 'loss/train': '0.64191', 'examples_per_second': '30.546', 'grad_norm': '20.125', 'counters/examples': 266016, 'counters/updates': 8313}
train stats after 266048 examples: {'rewards_train/chosen': '0.17441', 'rewards_train/rejected': '-0.01299', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1874', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-186.7', 'loss/train': '0.62749', 'examples_per_second': '30.621', 'grad_norm': '21.125', 'counters/examples': 266048, 'counters/updates': 8314}
train stats after 266080 examples: {'rewards_train/chosen': '0.1551', 'rewards_train/rejected': '0.084565', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070539', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-152.9', 'loss/train': '0.67265', 'examples_per_second': '31.485', 'grad_norm': '20.875', 'counters/examples': 266080, 'counters/updates': 8315}
train stats after 266112 examples: {'rewards_train/chosen': '0.15488', 'rewards_train/rejected': '0.018738', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13614', 'logps_train/rejected': '-124', 'logps_train/chosen': '-120.41', 'loss/train': '0.64124', 'examples_per_second': '31.921', 'grad_norm': '20', 'counters/examples': 266112, 'counters/updates': 8316}
train stats after 266144 examples: {'rewards_train/chosen': '0.19104', 'rewards_train/rejected': '0.11321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077829', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-150.21', 'loss/train': '0.66548', 'examples_per_second': '30.261', 'grad_norm': '20.875', 'counters/examples': 266144, 'counters/updates': 8317}
train stats after 266176 examples: {'rewards_train/chosen': '0.12729', 'rewards_train/rejected': '-0.07487', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20216', 'logps_train/rejected': '-97.47', 'logps_train/chosen': '-117.47', 'loss/train': '0.62062', 'examples_per_second': '30.671', 'grad_norm': '17.125', 'counters/examples': 266176, 'counters/updates': 8318}
train stats after 266208 examples: {'rewards_train/chosen': '0.13931', 'rewards_train/rejected': '0.075471', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063837', 'logps_train/rejected': '-119.35', 'logps_train/chosen': '-124.55', 'loss/train': '0.67088', 'examples_per_second': '31.57', 'grad_norm': '20.375', 'counters/examples': 266208, 'counters/updates': 8319}
train stats after 266240 examples: {'rewards_train/chosen': '0.19701', 'rewards_train/rejected': '0.087931', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10908', 'logps_train/rejected': '-98.279', 'logps_train/chosen': '-136.49', 'loss/train': '0.65839', 'examples_per_second': '31.322', 'grad_norm': '19.5', 'counters/examples': 266240, 'counters/updates': 8320}
train stats after 266272 examples: {'rewards_train/chosen': '0.28166', 'rewards_train/rejected': '0.012258', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2694', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-164.87', 'loss/train': '0.59024', 'examples_per_second': '31.568', 'grad_norm': '20.5', 'counters/examples': 266272, 'counters/updates': 8321}
skipping logging after 266304 examples to avoid logging too frequently
train stats after 266336 examples: {'rewards_train/chosen': '0.17407', 'rewards_train/rejected': '0.06033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11374', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-148.56', 'loss/train': '0.65433', 'examples_per_second': '36.081', 'grad_norm': '20.375', 'counters/examples': 266336, 'counters/updates': 8323}
train stats after 266368 examples: {'rewards_train/chosen': '0.16136', 'rewards_train/rejected': '-0.031304', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19266', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-163.34', 'loss/train': '0.61271', 'examples_per_second': '31.328', 'grad_norm': '20.375', 'counters/examples': 266368, 'counters/updates': 8324}
skipping logging after 266400 examples to avoid logging too frequently
train stats after 266432 examples: {'rewards_train/chosen': '0.058813', 'rewards_train/rejected': '0.0033509', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.055463', 'logps_train/rejected': '-103.86', 'logps_train/chosen': '-97.732', 'loss/train': '0.68715', 'examples_per_second': '33.713', 'grad_norm': '18.25', 'counters/examples': 266432, 'counters/updates': 8326}
skipping logging after 266464 examples to avoid logging too frequently
train stats after 266496 examples: {'rewards_train/chosen': '0.19562', 'rewards_train/rejected': '0.059184', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13644', 'logps_train/rejected': '-112.63', 'logps_train/chosen': '-145.99', 'loss/train': '0.64313', 'examples_per_second': '31.521', 'grad_norm': '20.375', 'counters/examples': 266496, 'counters/updates': 8328}
train stats after 266528 examples: {'rewards_train/chosen': '0.17116', 'rewards_train/rejected': '-0.030523', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20169', 'logps_train/rejected': '-96.243', 'logps_train/chosen': '-132.02', 'loss/train': '0.61048', 'examples_per_second': '30.065', 'grad_norm': '17.375', 'counters/examples': 266528, 'counters/updates': 8329}
skipping logging after 266560 examples to avoid logging too frequently
train stats after 266592 examples: {'rewards_train/chosen': '0.18572', 'rewards_train/rejected': '0.088534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097189', 'logps_train/rejected': '-155.53', 'logps_train/chosen': '-183.27', 'loss/train': '0.67895', 'examples_per_second': '31.471', 'grad_norm': '25.875', 'counters/examples': 266592, 'counters/updates': 8331}
train stats after 266624 examples: {'rewards_train/chosen': '0.12438', 'rewards_train/rejected': '0.084362', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040015', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-141.91', 'loss/train': '0.69719', 'examples_per_second': '31.459', 'grad_norm': '21.625', 'counters/examples': 266624, 'counters/updates': 8332}
train stats after 266656 examples: {'rewards_train/chosen': '0.17267', 'rewards_train/rejected': '0.083143', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089529', 'logps_train/rejected': '-105.51', 'logps_train/chosen': '-129.4', 'loss/train': '0.66837', 'examples_per_second': '30.66', 'grad_norm': '19.75', 'counters/examples': 266656, 'counters/updates': 8333}
train stats after 266688 examples: {'rewards_train/chosen': '0.25624', 'rewards_train/rejected': '0.11802', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13822', 'logps_train/rejected': '-122.36', 'logps_train/chosen': '-120.19', 'loss/train': '0.64078', 'examples_per_second': '30.119', 'grad_norm': '19.25', 'counters/examples': 266688, 'counters/updates': 8334}
train stats after 266720 examples: {'rewards_train/chosen': '0.14685', 'rewards_train/rejected': '0.057358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089493', 'logps_train/rejected': '-144.79', 'logps_train/chosen': '-124.64', 'loss/train': '0.66292', 'examples_per_second': '30.554', 'grad_norm': '20.75', 'counters/examples': 266720, 'counters/updates': 8335}
train stats after 266752 examples: {'rewards_train/chosen': '0.1909', 'rewards_train/rejected': '0.046746', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14415', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-164.17', 'loss/train': '0.65681', 'examples_per_second': '31.779', 'grad_norm': '23.25', 'counters/examples': 266752, 'counters/updates': 8336}
train stats after 266784 examples: {'rewards_train/chosen': '0.10752', 'rewards_train/rejected': '0.1453', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.03778', 'logps_train/rejected': '-109.79', 'logps_train/chosen': '-154.06', 'loss/train': '0.72459', 'examples_per_second': '32.406', 'grad_norm': '23.125', 'counters/examples': 266784, 'counters/updates': 8337}
train stats after 266816 examples: {'rewards_train/chosen': '0.28163', 'rewards_train/rejected': '-0.011048', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.29268', 'logps_train/rejected': '-97.124', 'logps_train/chosen': '-132.63', 'loss/train': '0.56758', 'examples_per_second': '30.036', 'grad_norm': '17.875', 'counters/examples': 266816, 'counters/updates': 8338}
train stats after 266848 examples: {'rewards_train/chosen': '0.14918', 'rewards_train/rejected': '0.083602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065578', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-135.77', 'loss/train': '0.67712', 'examples_per_second': '31.427', 'grad_norm': '20.125', 'counters/examples': 266848, 'counters/updates': 8339}
train stats after 266880 examples: {'rewards_train/chosen': '0.22798', 'rewards_train/rejected': '0.037358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19063', 'logps_train/rejected': '-134.92', 'logps_train/chosen': '-157.56', 'loss/train': '0.62053', 'examples_per_second': '30.03', 'grad_norm': '20.375', 'counters/examples': 266880, 'counters/updates': 8340}
train stats after 266912 examples: {'rewards_train/chosen': '0.28189', 'rewards_train/rejected': '0.050225', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23167', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-170.28', 'loss/train': '0.59835', 'examples_per_second': '30.057', 'grad_norm': '20.375', 'counters/examples': 266912, 'counters/updates': 8341}
train stats after 266944 examples: {'rewards_train/chosen': '0.21511', 'rewards_train/rejected': '0.098964', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11615', 'logps_train/rejected': '-172.85', 'logps_train/chosen': '-147.61', 'loss/train': '0.6581', 'examples_per_second': '31.011', 'grad_norm': '21.875', 'counters/examples': 266944, 'counters/updates': 8342}
train stats after 266976 examples: {'rewards_train/chosen': '0.20166', 'rewards_train/rejected': '0.074477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12719', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-114.05', 'loss/train': '0.65426', 'examples_per_second': '31.015', 'grad_norm': '19.25', 'counters/examples': 266976, 'counters/updates': 8343}
train stats after 267008 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '0.11021', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04477', 'logps_train/rejected': '-96.745', 'logps_train/chosen': '-133.93', 'loss/train': '0.69026', 'examples_per_second': '30.248', 'grad_norm': '20.5', 'counters/examples': 267008, 'counters/updates': 8344}
train stats after 267040 examples: {'rewards_train/chosen': '0.15632', 'rewards_train/rejected': '-0.038175', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19449', 'logps_train/rejected': '-83.542', 'logps_train/chosen': '-103.33', 'loss/train': '0.61796', 'examples_per_second': '31.493', 'grad_norm': '16.25', 'counters/examples': 267040, 'counters/updates': 8345}
train stats after 267072 examples: {'rewards_train/chosen': '0.29215', 'rewards_train/rejected': '-0.045484', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.33763', 'logps_train/rejected': '-132.03', 'logps_train/chosen': '-146.88', 'loss/train': '0.56305', 'examples_per_second': '31.443', 'grad_norm': '20.125', 'counters/examples': 267072, 'counters/updates': 8346}
train stats after 267104 examples: {'rewards_train/chosen': '0.3117', 'rewards_train/rejected': '0.195', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11671', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-129.99', 'loss/train': '0.65647', 'examples_per_second': '30.277', 'grad_norm': '21.25', 'counters/examples': 267104, 'counters/updates': 8347}
train stats after 267136 examples: {'rewards_train/chosen': '0.21251', 'rewards_train/rejected': '0.038727', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17378', 'logps_train/rejected': '-120.29', 'logps_train/chosen': '-175.88', 'loss/train': '0.63079', 'examples_per_second': '30.001', 'grad_norm': '21.5', 'counters/examples': 267136, 'counters/updates': 8348}
train stats after 267168 examples: {'rewards_train/chosen': '0.16933', 'rewards_train/rejected': '-0.013818', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18315', 'logps_train/rejected': '-90.513', 'logps_train/chosen': '-121.09', 'loss/train': '0.62394', 'examples_per_second': '31.956', 'grad_norm': '17.375', 'counters/examples': 267168, 'counters/updates': 8349}
train stats after 267200 examples: {'rewards_train/chosen': '0.2257', 'rewards_train/rejected': '0.093168', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13253', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-146.23', 'loss/train': '0.64484', 'examples_per_second': '31.553', 'grad_norm': '21.5', 'counters/examples': 267200, 'counters/updates': 8350}
skipping logging after 267232 examples to avoid logging too frequently
train stats after 267264 examples: {'rewards_train/chosen': '0.096435', 'rewards_train/rejected': '0.045469', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050965', 'logps_train/rejected': '-126.33', 'logps_train/chosen': '-130.44', 'loss/train': '0.69018', 'examples_per_second': '31.059', 'grad_norm': '21.25', 'counters/examples': 267264, 'counters/updates': 8352}
skipping logging after 267296 examples to avoid logging too frequently
train stats after 267328 examples: {'rewards_train/chosen': '0.1152', 'rewards_train/rejected': '0.07039', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044814', 'logps_train/rejected': '-106.75', 'logps_train/chosen': '-118.69', 'loss/train': '0.69558', 'examples_per_second': '31.482', 'grad_norm': '22.5', 'counters/examples': 267328, 'counters/updates': 8354}
train stats after 267360 examples: {'rewards_train/chosen': '0.28996', 'rewards_train/rejected': '0.00060907', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.28935', 'logps_train/rejected': '-90.962', 'logps_train/chosen': '-147.25', 'loss/train': '0.57835', 'examples_per_second': '30.562', 'grad_norm': '18.375', 'counters/examples': 267360, 'counters/updates': 8355}
train stats after 267392 examples: {'rewards_train/chosen': '0.15753', 'rewards_train/rejected': '0.13004', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027495', 'logps_train/rejected': '-103.41', 'logps_train/chosen': '-140.97', 'loss/train': '0.71589', 'examples_per_second': '31.46', 'grad_norm': '21.875', 'counters/examples': 267392, 'counters/updates': 8356}
train stats after 267424 examples: {'rewards_train/chosen': '0.082506', 'rewards_train/rejected': '-0.10697', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18947', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-122.32', 'loss/train': '0.61121', 'examples_per_second': '32.564', 'grad_norm': '18.875', 'counters/examples': 267424, 'counters/updates': 8357}
train stats after 267456 examples: {'rewards_train/chosen': '0.20736', 'rewards_train/rejected': '0.059641', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14772', 'logps_train/rejected': '-133.73', 'logps_train/chosen': '-181.93', 'loss/train': '0.64102', 'examples_per_second': '31.439', 'grad_norm': '23.25', 'counters/examples': 267456, 'counters/updates': 8358}
train stats after 267488 examples: {'rewards_train/chosen': '0.14382', 'rewards_train/rejected': '0.073213', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070608', 'logps_train/rejected': '-109.03', 'logps_train/chosen': '-132.76', 'loss/train': '0.68901', 'examples_per_second': '30.914', 'grad_norm': '20.5', 'counters/examples': 267488, 'counters/updates': 8359}
train stats after 267520 examples: {'rewards_train/chosen': '0.30711', 'rewards_train/rejected': '0.065196', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24191', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-133.79', 'loss/train': '0.60742', 'examples_per_second': '30.788', 'grad_norm': '20.5', 'counters/examples': 267520, 'counters/updates': 8360}
train stats after 267552 examples: {'rewards_train/chosen': '0.19443', 'rewards_train/rejected': '0.0057791', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18865', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-143.98', 'loss/train': '0.62163', 'examples_per_second': '31.272', 'grad_norm': '19.375', 'counters/examples': 267552, 'counters/updates': 8361}
train stats after 267584 examples: {'rewards_train/chosen': '0.15267', 'rewards_train/rejected': '0.062032', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090638', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-143.68', 'loss/train': '0.67907', 'examples_per_second': '33.322', 'grad_norm': '21.625', 'counters/examples': 267584, 'counters/updates': 8362}
skipping logging after 267616 examples to avoid logging too frequently
train stats after 267648 examples: {'rewards_train/chosen': '0.13561', 'rewards_train/rejected': '0.038573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097036', 'logps_train/rejected': '-101.06', 'logps_train/chosen': '-119.37', 'loss/train': '0.6619', 'examples_per_second': '30.692', 'grad_norm': '18.625', 'counters/examples': 267648, 'counters/updates': 8364}
train stats after 267680 examples: {'rewards_train/chosen': '0.38575', 'rewards_train/rejected': '0.11492', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27083', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-184.6', 'loss/train': '0.59918', 'examples_per_second': '30.012', 'grad_norm': '20.5', 'counters/examples': 267680, 'counters/updates': 8365}
train stats after 267712 examples: {'rewards_train/chosen': '0.096679', 'rewards_train/rejected': '0.069119', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02756', 'logps_train/rejected': '-100.2', 'logps_train/chosen': '-91.13', 'loss/train': '0.69208', 'examples_per_second': '31.495', 'grad_norm': '17.25', 'counters/examples': 267712, 'counters/updates': 8366}
skipping logging after 267744 examples to avoid logging too frequently
train stats after 267776 examples: {'rewards_train/chosen': '0.21346', 'rewards_train/rejected': '-0.011246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22471', 'logps_train/rejected': '-102.09', 'logps_train/chosen': '-136.11', 'loss/train': '0.60244', 'examples_per_second': '31.303', 'grad_norm': '18.75', 'counters/examples': 267776, 'counters/updates': 8368}
train stats after 267808 examples: {'rewards_train/chosen': '0.29536', 'rewards_train/rejected': '0.074266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22109', 'logps_train/rejected': '-90.285', 'logps_train/chosen': '-134.66', 'loss/train': '0.60778', 'examples_per_second': '30.468', 'grad_norm': '17.625', 'counters/examples': 267808, 'counters/updates': 8369}
train stats after 267840 examples: {'rewards_train/chosen': '0.20796', 'rewards_train/rejected': '-0.052449', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26041', 'logps_train/rejected': '-89.592', 'logps_train/chosen': '-122.54', 'loss/train': '0.58918', 'examples_per_second': '31.026', 'grad_norm': '16.25', 'counters/examples': 267840, 'counters/updates': 8370}
skipping logging after 267872 examples to avoid logging too frequently
train stats after 267904 examples: {'rewards_train/chosen': '0.1915', 'rewards_train/rejected': '0.014569', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17693', 'logps_train/rejected': '-98.066', 'logps_train/chosen': '-131.33', 'loss/train': '0.62914', 'examples_per_second': '32.517', 'grad_norm': '18.375', 'counters/examples': 267904, 'counters/updates': 8372}
train stats after 267936 examples: {'rewards_train/chosen': '0.19862', 'rewards_train/rejected': '0.047786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15084', 'logps_train/rejected': '-119.19', 'logps_train/chosen': '-128.18', 'loss/train': '0.63758', 'examples_per_second': '30.513', 'grad_norm': '19.5', 'counters/examples': 267936, 'counters/updates': 8373}
train stats after 267968 examples: {'rewards_train/chosen': '0.23172', 'rewards_train/rejected': '0.09384', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13788', 'logps_train/rejected': '-130.21', 'logps_train/chosen': '-152.96', 'loss/train': '0.64763', 'examples_per_second': '30.072', 'grad_norm': '20.25', 'counters/examples': 267968, 'counters/updates': 8374}
train stats after 268000 examples: {'rewards_train/chosen': '0.15205', 'rewards_train/rejected': '0.11068', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.041373', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-154.22', 'loss/train': '0.70401', 'examples_per_second': '22.669', 'grad_norm': '21.875', 'counters/examples': 268000, 'counters/updates': 8375}
train stats after 268032 examples: {'rewards_train/chosen': '0.18012', 'rewards_train/rejected': '-0.11264', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.29276', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-132.08', 'loss/train': '0.57768', 'examples_per_second': '30.816', 'grad_norm': '18.125', 'counters/examples': 268032, 'counters/updates': 8376}
train stats after 268064 examples: {'rewards_train/chosen': '0.19785', 'rewards_train/rejected': '-0.10416', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.30201', 'logps_train/rejected': '-99.401', 'logps_train/chosen': '-120.39', 'loss/train': '0.57514', 'examples_per_second': '30.491', 'grad_norm': '18.25', 'counters/examples': 268064, 'counters/updates': 8377}
train stats after 268096 examples: {'rewards_train/chosen': '0.17244', 'rewards_train/rejected': '0.17592', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0034788', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-174.46', 'loss/train': '0.71594', 'examples_per_second': '24.358', 'grad_norm': '23.125', 'counters/examples': 268096, 'counters/updates': 8378}
train stats after 268128 examples: {'rewards_train/chosen': '0.19692', 'rewards_train/rejected': '-0.014431', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21135', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-127.92', 'loss/train': '0.61597', 'examples_per_second': '31.528', 'grad_norm': '19.125', 'counters/examples': 268128, 'counters/updates': 8379}
skipping logging after 268160 examples to avoid logging too frequently
train stats after 268192 examples: {'rewards_train/chosen': '0.13311', 'rewards_train/rejected': '0.016541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11657', 'logps_train/rejected': '-103.84', 'logps_train/chosen': '-146.79', 'loss/train': '0.6556', 'examples_per_second': '32.47', 'grad_norm': '20.625', 'counters/examples': 268192, 'counters/updates': 8381}
train stats after 268224 examples: {'rewards_train/chosen': '0.072685', 'rewards_train/rejected': '0.044722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027963', 'logps_train/rejected': '-94.73', 'logps_train/chosen': '-93.029', 'loss/train': '0.68711', 'examples_per_second': '31.464', 'grad_norm': '17.875', 'counters/examples': 268224, 'counters/updates': 8382}
train stats after 268256 examples: {'rewards_train/chosen': '0.19615', 'rewards_train/rejected': '0.11105', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085105', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-136.83', 'loss/train': '0.66886', 'examples_per_second': '30.417', 'grad_norm': '20.625', 'counters/examples': 268256, 'counters/updates': 8383}
train stats after 268288 examples: {'rewards_train/chosen': '0.11326', 'rewards_train/rejected': '0.020014', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093246', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-158.52', 'loss/train': '0.66063', 'examples_per_second': '30.854', 'grad_norm': '20.875', 'counters/examples': 268288, 'counters/updates': 8384}
train stats after 268320 examples: {'rewards_train/chosen': '0.30914', 'rewards_train/rejected': '0.095394', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21374', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-177.62', 'loss/train': '0.61609', 'examples_per_second': '30.507', 'grad_norm': '22.25', 'counters/examples': 268320, 'counters/updates': 8385}
train stats after 268352 examples: {'rewards_train/chosen': '0.18979', 'rewards_train/rejected': '-0.073913', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2637', 'logps_train/rejected': '-182.12', 'logps_train/chosen': '-116.95', 'loss/train': '0.59273', 'examples_per_second': '31.532', 'grad_norm': '19.875', 'counters/examples': 268352, 'counters/updates': 8386}
train stats after 268384 examples: {'rewards_train/chosen': '0.16696', 'rewards_train/rejected': '0.094837', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072127', 'logps_train/rejected': '-155.27', 'logps_train/chosen': '-149', 'loss/train': '0.67046', 'examples_per_second': '31.515', 'grad_norm': '24.125', 'counters/examples': 268384, 'counters/updates': 8387}
train stats after 268416 examples: {'rewards_train/chosen': '0.24007', 'rewards_train/rejected': '0.10803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13204', 'logps_train/rejected': '-156.01', 'logps_train/chosen': '-184.83', 'loss/train': '0.66276', 'examples_per_second': '31.486', 'grad_norm': '24.25', 'counters/examples': 268416, 'counters/updates': 8388}
skipping logging after 268448 examples to avoid logging too frequently
train stats after 268480 examples: {'rewards_train/chosen': '0.11061', 'rewards_train/rejected': '0.021618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088989', 'logps_train/rejected': '-115.49', 'logps_train/chosen': '-129.13', 'loss/train': '0.65953', 'examples_per_second': '31.505', 'grad_norm': '19.875', 'counters/examples': 268480, 'counters/updates': 8390}
skipping logging after 268512 examples to avoid logging too frequently
train stats after 268544 examples: {'rewards_train/chosen': '0.078011', 'rewards_train/rejected': '0.15711', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.079098', 'logps_train/rejected': '-109.98', 'logps_train/chosen': '-116.98', 'loss/train': '0.76397', 'examples_per_second': '33.859', 'grad_norm': '22.125', 'counters/examples': 268544, 'counters/updates': 8392}
train stats after 268576 examples: {'rewards_train/chosen': '0.093753', 'rewards_train/rejected': '0.02329', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070463', 'logps_train/rejected': '-125.41', 'logps_train/chosen': '-124.51', 'loss/train': '0.68371', 'examples_per_second': '31.624', 'grad_norm': '19.625', 'counters/examples': 268576, 'counters/updates': 8393}
skipping logging after 268608 examples to avoid logging too frequently
train stats after 268640 examples: {'rewards_train/chosen': '0.15493', 'rewards_train/rejected': '0.054761', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10017', 'logps_train/rejected': '-133.11', 'logps_train/chosen': '-167.38', 'loss/train': '0.67151', 'examples_per_second': '29.972', 'grad_norm': '21.375', 'counters/examples': 268640, 'counters/updates': 8395}
train stats after 268672 examples: {'rewards_train/chosen': '0.32361', 'rewards_train/rejected': '0.014666', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.30894', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-159.98', 'loss/train': '0.56668', 'examples_per_second': '31.294', 'grad_norm': '19.875', 'counters/examples': 268672, 'counters/updates': 8396}
train stats after 268704 examples: {'rewards_train/chosen': '0.17362', 'rewards_train/rejected': '-0.058752', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23237', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-120.17', 'loss/train': '0.60216', 'examples_per_second': '31.197', 'grad_norm': '17.625', 'counters/examples': 268704, 'counters/updates': 8397}
train stats after 268736 examples: {'rewards_train/chosen': '0.31783', 'rewards_train/rejected': '0.020159', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.29767', 'logps_train/rejected': '-127.94', 'logps_train/chosen': '-134.06', 'loss/train': '0.5865', 'examples_per_second': '31.55', 'grad_norm': '19.875', 'counters/examples': 268736, 'counters/updates': 8398}
train stats after 268768 examples: {'rewards_train/chosen': '0.1525', 'rewards_train/rejected': '-0.035656', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18816', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-143.2', 'loss/train': '0.61741', 'examples_per_second': '31.526', 'grad_norm': '20.375', 'counters/examples': 268768, 'counters/updates': 8399}
skipping logging after 268800 examples to avoid logging too frequently
train stats after 268832 examples: {'rewards_train/chosen': '0.1588', 'rewards_train/rejected': '0.04689', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11191', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-135.75', 'loss/train': '0.65641', 'examples_per_second': '31.572', 'grad_norm': '21.875', 'counters/examples': 268832, 'counters/updates': 8401}
train stats after 268864 examples: {'rewards_train/chosen': '0.28703', 'rewards_train/rejected': '0.03025', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25678', 'logps_train/rejected': '-146.86', 'logps_train/chosen': '-167.41', 'loss/train': '0.60094', 'examples_per_second': '30.952', 'grad_norm': '20.5', 'counters/examples': 268864, 'counters/updates': 8402}
skipping logging after 268896 examples to avoid logging too frequently
train stats after 268928 examples: {'rewards_train/chosen': '0.25546', 'rewards_train/rejected': '0.1068', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14866', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-104.89', 'loss/train': '0.65077', 'examples_per_second': '31.388', 'grad_norm': '21.75', 'counters/examples': 268928, 'counters/updates': 8404}
skipping logging after 268960 examples to avoid logging too frequently
train stats after 268992 examples: {'rewards_train/chosen': '0.2347', 'rewards_train/rejected': '0.098775', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13593', 'logps_train/rejected': '-97.198', 'logps_train/chosen': '-103.94', 'loss/train': '0.64487', 'examples_per_second': '34.352', 'grad_norm': '18.375', 'counters/examples': 268992, 'counters/updates': 8406}
train stats after 269024 examples: {'rewards_train/chosen': '0.15452', 'rewards_train/rejected': '0.10938', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045142', 'logps_train/rejected': '-145.61', 'logps_train/chosen': '-121.48', 'loss/train': '0.68385', 'examples_per_second': '30.538', 'grad_norm': '21.625', 'counters/examples': 269024, 'counters/updates': 8407}
skipping logging after 269056 examples to avoid logging too frequently
train stats after 269088 examples: {'rewards_train/chosen': '0.14491', 'rewards_train/rejected': '-0.033839', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17875', 'logps_train/rejected': '-113.95', 'logps_train/chosen': '-162.98', 'loss/train': '0.62393', 'examples_per_second': '23.702', 'grad_norm': '20.75', 'counters/examples': 269088, 'counters/updates': 8409}
train stats after 269120 examples: {'rewards_train/chosen': '0.19623', 'rewards_train/rejected': '0.075915', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12031', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-115.8', 'loss/train': '0.64184', 'examples_per_second': '31.538', 'grad_norm': '18.75', 'counters/examples': 269120, 'counters/updates': 8410}
train stats after 269152 examples: {'rewards_train/chosen': '0.14552', 'rewards_train/rejected': '0.079618', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065906', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-104.54', 'loss/train': '0.68591', 'examples_per_second': '32.601', 'grad_norm': '19', 'counters/examples': 269152, 'counters/updates': 8411}
skipping logging after 269184 examples to avoid logging too frequently
train stats after 269216 examples: {'rewards_train/chosen': '0.15052', 'rewards_train/rejected': '-0.087347', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23787', 'logps_train/rejected': '-120.5', 'logps_train/chosen': '-154.93', 'loss/train': '0.60616', 'examples_per_second': '31.97', 'grad_norm': '21', 'counters/examples': 269216, 'counters/updates': 8413}
train stats after 269248 examples: {'rewards_train/chosen': '0.14867', 'rewards_train/rejected': '-0.027616', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17629', 'logps_train/rejected': '-113', 'logps_train/chosen': '-147.33', 'loss/train': '0.63523', 'examples_per_second': '31.635', 'grad_norm': '21.375', 'counters/examples': 269248, 'counters/updates': 8414}
train stats after 269280 examples: {'rewards_train/chosen': '0.20125', 'rewards_train/rejected': '0.076158', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12509', 'logps_train/rejected': '-156.44', 'logps_train/chosen': '-159.42', 'loss/train': '0.65292', 'examples_per_second': '31.434', 'grad_norm': '22.375', 'counters/examples': 269280, 'counters/updates': 8415}
train stats after 269312 examples: {'rewards_train/chosen': '0.15696', 'rewards_train/rejected': '-0.045176', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20214', 'logps_train/rejected': '-92.042', 'logps_train/chosen': '-148.52', 'loss/train': '0.61422', 'examples_per_second': '32.27', 'grad_norm': '18.375', 'counters/examples': 269312, 'counters/updates': 8416}
train stats after 269344 examples: {'rewards_train/chosen': '0.18742', 'rewards_train/rejected': '0.057133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13029', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-133.66', 'loss/train': '0.65053', 'examples_per_second': '31.52', 'grad_norm': '21.375', 'counters/examples': 269344, 'counters/updates': 8417}
train stats after 269376 examples: {'rewards_train/chosen': '0.19078', 'rewards_train/rejected': '-0.015877', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20666', 'logps_train/rejected': '-100.3', 'logps_train/chosen': '-104.41', 'loss/train': '0.61436', 'examples_per_second': '32.422', 'grad_norm': '17', 'counters/examples': 269376, 'counters/updates': 8418}
skipping logging after 269408 examples to avoid logging too frequently
train stats after 269440 examples: {'rewards_train/chosen': '0.13395', 'rewards_train/rejected': '0.10318', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030773', 'logps_train/rejected': '-135.26', 'logps_train/chosen': '-124.63', 'loss/train': '0.70114', 'examples_per_second': '30.11', 'grad_norm': '21.75', 'counters/examples': 269440, 'counters/updates': 8420}
train stats after 269472 examples: {'rewards_train/chosen': '0.19205', 'rewards_train/rejected': '-0.011831', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20388', 'logps_train/rejected': '-155.15', 'logps_train/chosen': '-122.99', 'loss/train': '0.60857', 'examples_per_second': '30.637', 'grad_norm': '19.625', 'counters/examples': 269472, 'counters/updates': 8421}
train stats after 269504 examples: {'rewards_train/chosen': '0.14399', 'rewards_train/rejected': '0.12254', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021454', 'logps_train/rejected': '-147.33', 'logps_train/chosen': '-137.31', 'loss/train': '0.6905', 'examples_per_second': '30.05', 'grad_norm': '21.125', 'counters/examples': 269504, 'counters/updates': 8422}
train stats after 269536 examples: {'rewards_train/chosen': '0.098831', 'rewards_train/rejected': '0.0052106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09362', 'logps_train/rejected': '-134.73', 'logps_train/chosen': '-183.48', 'loss/train': '0.66852', 'examples_per_second': '32.172', 'grad_norm': '22.75', 'counters/examples': 269536, 'counters/updates': 8423}
train stats after 269568 examples: {'rewards_train/chosen': '0.20324', 'rewards_train/rejected': '0.051543', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1517', 'logps_train/rejected': '-103.96', 'logps_train/chosen': '-147.57', 'loss/train': '0.64017', 'examples_per_second': '30.021', 'grad_norm': '19.75', 'counters/examples': 269568, 'counters/updates': 8424}
train stats after 269600 examples: {'rewards_train/chosen': '0.10937', 'rewards_train/rejected': '0.12345', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014077', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-136.11', 'loss/train': '0.71053', 'examples_per_second': '31.218', 'grad_norm': '21.5', 'counters/examples': 269600, 'counters/updates': 8425}
skipping logging after 269632 examples to avoid logging too frequently
train stats after 269664 examples: {'rewards_train/chosen': '0.16734', 'rewards_train/rejected': '-0.028938', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19628', 'logps_train/rejected': '-129.16', 'logps_train/chosen': '-140.57', 'loss/train': '0.61281', 'examples_per_second': '31.532', 'grad_norm': '20', 'counters/examples': 269664, 'counters/updates': 8427}
train stats after 269696 examples: {'rewards_train/chosen': '0.16002', 'rewards_train/rejected': '0.10398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056048', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-131.28', 'loss/train': '0.68424', 'examples_per_second': '31.495', 'grad_norm': '23.875', 'counters/examples': 269696, 'counters/updates': 8428}
train stats after 269728 examples: {'rewards_train/chosen': '0.10733', 'rewards_train/rejected': '-0.017523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12486', 'logps_train/rejected': '-91.032', 'logps_train/chosen': '-104.37', 'loss/train': '0.65747', 'examples_per_second': '31.952', 'grad_norm': '18.125', 'counters/examples': 269728, 'counters/updates': 8429}
train stats after 269760 examples: {'rewards_train/chosen': '0.27748', 'rewards_train/rejected': '-0.010357', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.28784', 'logps_train/rejected': '-114.33', 'logps_train/chosen': '-140.6', 'loss/train': '0.58273', 'examples_per_second': '30.224', 'grad_norm': '18.25', 'counters/examples': 269760, 'counters/updates': 8430}
train stats after 269792 examples: {'rewards_train/chosen': '0.118', 'rewards_train/rejected': '0.00020213', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1178', 'logps_train/rejected': '-120.97', 'logps_train/chosen': '-123.97', 'loss/train': '0.6549', 'examples_per_second': '32.738', 'grad_norm': '19.875', 'counters/examples': 269792, 'counters/updates': 8431}
train stats after 269824 examples: {'rewards_train/chosen': '0.16249', 'rewards_train/rejected': '-0.042731', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20522', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-117.18', 'loss/train': '0.60594', 'examples_per_second': '32.78', 'grad_norm': '17.875', 'counters/examples': 269824, 'counters/updates': 8432}
train stats after 269856 examples: {'rewards_train/chosen': '0.23078', 'rewards_train/rejected': '-0.023295', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25408', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-140.25', 'loss/train': '0.59387', 'examples_per_second': '31.564', 'grad_norm': '20.125', 'counters/examples': 269856, 'counters/updates': 8433}
train stats after 269888 examples: {'rewards_train/chosen': '0.1899', 'rewards_train/rejected': '0.051827', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13807', 'logps_train/rejected': '-89.557', 'logps_train/chosen': '-135.91', 'loss/train': '0.65089', 'examples_per_second': '30.753', 'grad_norm': '19.5', 'counters/examples': 269888, 'counters/updates': 8434}
train stats after 269920 examples: {'rewards_train/chosen': '0.2226', 'rewards_train/rejected': '0.068066', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15454', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-105.71', 'loss/train': '0.63241', 'examples_per_second': '30.816', 'grad_norm': '19.375', 'counters/examples': 269920, 'counters/updates': 8435}
train stats after 269952 examples: {'rewards_train/chosen': '0.17718', 'rewards_train/rejected': '0.0047635', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17242', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-128.53', 'loss/train': '0.62431', 'examples_per_second': '30.544', 'grad_norm': '21.125', 'counters/examples': 269952, 'counters/updates': 8436}
train stats after 269984 examples: {'rewards_train/chosen': '0.20287', 'rewards_train/rejected': '-0.060748', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26362', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-164.45', 'loss/train': '0.59104', 'examples_per_second': '32.552', 'grad_norm': '19.125', 'counters/examples': 269984, 'counters/updates': 8437}
train stats after 270016 examples: {'rewards_train/chosen': '0.30462', 'rewards_train/rejected': '0.090046', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21457', 'logps_train/rejected': '-123.2', 'logps_train/chosen': '-164.27', 'loss/train': '0.60749', 'examples_per_second': '32.751', 'grad_norm': '19.625', 'counters/examples': 270016, 'counters/updates': 8438}
skipping logging after 270048 examples to avoid logging too frequently
train stats after 270080 examples: {'rewards_train/chosen': '0.31599', 'rewards_train/rejected': '-0.019918', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.3359', 'logps_train/rejected': '-97.454', 'logps_train/chosen': '-159.4', 'loss/train': '0.56742', 'examples_per_second': '30.965', 'grad_norm': '17.375', 'counters/examples': 270080, 'counters/updates': 8440}
train stats after 270112 examples: {'rewards_train/chosen': '0.29116', 'rewards_train/rejected': '-0.038841', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.33', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-150.11', 'loss/train': '0.56422', 'examples_per_second': '30.739', 'grad_norm': '18.625', 'counters/examples': 270112, 'counters/updates': 8441}
train stats after 270144 examples: {'rewards_train/chosen': '0.13631', 'rewards_train/rejected': '0.020041', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11627', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-111.43', 'loss/train': '0.67467', 'examples_per_second': '30.821', 'grad_norm': '20.5', 'counters/examples': 270144, 'counters/updates': 8442}
train stats after 270176 examples: {'rewards_train/chosen': '0.29255', 'rewards_train/rejected': '-0.0092844', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.30183', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-164.51', 'loss/train': '0.57103', 'examples_per_second': '31.345', 'grad_norm': '18.875', 'counters/examples': 270176, 'counters/updates': 8443}
train stats after 270208 examples: {'rewards_train/chosen': '0.18085', 'rewards_train/rejected': '-0.032195', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21305', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-136.41', 'loss/train': '0.62673', 'examples_per_second': '31.866', 'grad_norm': '19.375', 'counters/examples': 270208, 'counters/updates': 8444}
train stats after 270240 examples: {'rewards_train/chosen': '0.25968', 'rewards_train/rejected': '0.022501', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23718', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-158.71', 'loss/train': '0.59922', 'examples_per_second': '31.56', 'grad_norm': '19.75', 'counters/examples': 270240, 'counters/updates': 8445}
train stats after 270272 examples: {'rewards_train/chosen': '0.14745', 'rewards_train/rejected': '0.052265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095188', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-143.99', 'loss/train': '0.65476', 'examples_per_second': '32.429', 'grad_norm': '19.875', 'counters/examples': 270272, 'counters/updates': 8446}
train stats after 270304 examples: {'rewards_train/chosen': '0.22529', 'rewards_train/rejected': '-0.0056599', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23094', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-166.44', 'loss/train': '0.59988', 'examples_per_second': '31.664', 'grad_norm': '20.25', 'counters/examples': 270304, 'counters/updates': 8447}
train stats after 270336 examples: {'rewards_train/chosen': '0.16614', 'rewards_train/rejected': '-0.013605', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17975', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-132.19', 'loss/train': '0.64073', 'examples_per_second': '32.29', 'grad_norm': '19.625', 'counters/examples': 270336, 'counters/updates': 8448}
train stats after 270368 examples: {'rewards_train/chosen': '0.17631', 'rewards_train/rejected': '-0.063668', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.23998', 'logps_train/rejected': '-90.849', 'logps_train/chosen': '-126.86', 'loss/train': '0.61016', 'examples_per_second': '32.277', 'grad_norm': '18.25', 'counters/examples': 270368, 'counters/updates': 8449}
skipping logging after 270400 examples to avoid logging too frequently
train stats after 270432 examples: {'rewards_train/chosen': '0.26305', 'rewards_train/rejected': '-0.010587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27363', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-131.11', 'loss/train': '0.58561', 'examples_per_second': '32.338', 'grad_norm': '17', 'counters/examples': 270432, 'counters/updates': 8451}
train stats after 270464 examples: {'rewards_train/chosen': '0.19641', 'rewards_train/rejected': '0.010026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18638', 'logps_train/rejected': '-143.03', 'logps_train/chosen': '-151.85', 'loss/train': '0.61557', 'examples_per_second': '30.536', 'grad_norm': '20.875', 'counters/examples': 270464, 'counters/updates': 8452}
skipping logging after 270496 examples to avoid logging too frequently
train stats after 270528 examples: {'rewards_train/chosen': '0.097719', 'rewards_train/rejected': '0.024337', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073382', 'logps_train/rejected': '-151.52', 'logps_train/chosen': '-139.43', 'loss/train': '0.66849', 'examples_per_second': '30.076', 'grad_norm': '22.25', 'counters/examples': 270528, 'counters/updates': 8454}
skipping logging after 270560 examples to avoid logging too frequently
train stats after 270592 examples: {'rewards_train/chosen': '0.29237', 'rewards_train/rejected': '0.06212', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23025', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-151.28', 'loss/train': '0.61694', 'examples_per_second': '31.505', 'grad_norm': '21.875', 'counters/examples': 270592, 'counters/updates': 8456}
train stats after 270624 examples: {'rewards_train/chosen': '0.24054', 'rewards_train/rejected': '0.075857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16469', 'logps_train/rejected': '-148.71', 'logps_train/chosen': '-125.24', 'loss/train': '0.64457', 'examples_per_second': '32.085', 'grad_norm': '20.5', 'counters/examples': 270624, 'counters/updates': 8457}
train stats after 270656 examples: {'rewards_train/chosen': '0.15382', 'rewards_train/rejected': '0.012262', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14156', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-153.07', 'loss/train': '0.64445', 'examples_per_second': '30.683', 'grad_norm': '21.25', 'counters/examples': 270656, 'counters/updates': 8458}
train stats after 270688 examples: {'rewards_train/chosen': '0.2805', 'rewards_train/rejected': '-0.018963', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.29947', 'logps_train/rejected': '-137.93', 'logps_train/chosen': '-140.22', 'loss/train': '0.58824', 'examples_per_second': '31.496', 'grad_norm': '19.125', 'counters/examples': 270688, 'counters/updates': 8459}
train stats after 270720 examples: {'rewards_train/chosen': '0.1295', 'rewards_train/rejected': '-0.014654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14415', 'logps_train/rejected': '-95.037', 'logps_train/chosen': '-109.23', 'loss/train': '0.63919', 'examples_per_second': '31.362', 'grad_norm': '19', 'counters/examples': 270720, 'counters/updates': 8460}
train stats after 270752 examples: {'rewards_train/chosen': '0.17883', 'rewards_train/rejected': '0.048113', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13072', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-111.75', 'loss/train': '0.64229', 'examples_per_second': '30.401', 'grad_norm': '19.375', 'counters/examples': 270752, 'counters/updates': 8461}
train stats after 270784 examples: {'rewards_train/chosen': '0.15989', 'rewards_train/rejected': '0.070382', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08951', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-112.5', 'loss/train': '0.66393', 'examples_per_second': '30.977', 'grad_norm': '19.75', 'counters/examples': 270784, 'counters/updates': 8462}
skipping logging after 270816 examples to avoid logging too frequently
train stats after 270848 examples: {'rewards_train/chosen': '0.23771', 'rewards_train/rejected': '0.086926', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15078', 'logps_train/rejected': '-110.64', 'logps_train/chosen': '-108.92', 'loss/train': '0.63395', 'examples_per_second': '36.086', 'grad_norm': '17.75', 'counters/examples': 270848, 'counters/updates': 8464}
train stats after 270880 examples: {'rewards_train/chosen': '0.13644', 'rewards_train/rejected': '-0.0069366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14337', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-127.09', 'loss/train': '0.63994', 'examples_per_second': '32.599', 'grad_norm': '20.125', 'counters/examples': 270880, 'counters/updates': 8465}
skipping logging after 270912 examples to avoid logging too frequently
train stats after 270944 examples: {'rewards_train/chosen': '0.20752', 'rewards_train/rejected': '0.082239', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12528', 'logps_train/rejected': '-109.31', 'logps_train/chosen': '-114.58', 'loss/train': '0.64912', 'examples_per_second': '31.482', 'grad_norm': '19.125', 'counters/examples': 270944, 'counters/updates': 8467}
skipping logging after 270976 examples to avoid logging too frequently
train stats after 271008 examples: {'rewards_train/chosen': '0.17444', 'rewards_train/rejected': '0.051221', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12322', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-105.01', 'loss/train': '0.64951', 'examples_per_second': '30.374', 'grad_norm': '18.25', 'counters/examples': 271008, 'counters/updates': 8469}
skipping logging after 271040 examples to avoid logging too frequently
train stats after 271072 examples: {'rewards_train/chosen': '0.24603', 'rewards_train/rejected': '0.053414', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19262', 'logps_train/rejected': '-122.21', 'logps_train/chosen': '-140.19', 'loss/train': '0.62164', 'examples_per_second': '36.094', 'grad_norm': '20.125', 'counters/examples': 271072, 'counters/updates': 8471}
train stats after 271104 examples: {'rewards_train/chosen': '0.13684', 'rewards_train/rejected': '0.022386', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11445', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-136.81', 'loss/train': '0.65046', 'examples_per_second': '31.552', 'grad_norm': '20', 'counters/examples': 271104, 'counters/updates': 8472}
train stats after 271136 examples: {'rewards_train/chosen': '0.13597', 'rewards_train/rejected': '-0.080532', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2165', 'logps_train/rejected': '-95.536', 'logps_train/chosen': '-100.25', 'loss/train': '0.60707', 'examples_per_second': '30.246', 'grad_norm': '16.375', 'counters/examples': 271136, 'counters/updates': 8473}
train stats after 271168 examples: {'rewards_train/chosen': '0.23294', 'rewards_train/rejected': '-0.043989', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27693', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-114.51', 'loss/train': '0.57911', 'examples_per_second': '31.512', 'grad_norm': '17.5', 'counters/examples': 271168, 'counters/updates': 8474}
train stats after 271200 examples: {'rewards_train/chosen': '0.24748', 'rewards_train/rejected': '0.052594', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19488', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-124.14', 'loss/train': '0.62099', 'examples_per_second': '30.583', 'grad_norm': '18.5', 'counters/examples': 271200, 'counters/updates': 8475}
train stats after 271232 examples: {'rewards_train/chosen': '0.20166', 'rewards_train/rejected': '-0.00021976', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20188', 'logps_train/rejected': '-142.74', 'logps_train/chosen': '-132.22', 'loss/train': '0.62371', 'examples_per_second': '31.399', 'grad_norm': '20.75', 'counters/examples': 271232, 'counters/updates': 8476}
train stats after 271264 examples: {'rewards_train/chosen': '0.32685', 'rewards_train/rejected': '-0.011897', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.33875', 'logps_train/rejected': '-118.8', 'logps_train/chosen': '-150.46', 'loss/train': '0.55908', 'examples_per_second': '30.678', 'grad_norm': '18.75', 'counters/examples': 271264, 'counters/updates': 8477}
train stats after 271296 examples: {'rewards_train/chosen': '0.11169', 'rewards_train/rejected': '0.028839', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08285', 'logps_train/rejected': '-127.2', 'logps_train/chosen': '-137.23', 'loss/train': '0.66219', 'examples_per_second': '31.566', 'grad_norm': '20.625', 'counters/examples': 271296, 'counters/updates': 8478}
train stats after 271328 examples: {'rewards_train/chosen': '0.22426', 'rewards_train/rejected': '0.012441', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21181', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-143.79', 'loss/train': '0.62752', 'examples_per_second': '31.536', 'grad_norm': '21.375', 'counters/examples': 271328, 'counters/updates': 8479}
train stats after 271360 examples: {'rewards_train/chosen': '0.26359', 'rewards_train/rejected': '0.055725', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20787', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-147.57', 'loss/train': '0.60781', 'examples_per_second': '31.494', 'grad_norm': '20', 'counters/examples': 271360, 'counters/updates': 8480}
train stats after 271392 examples: {'rewards_train/chosen': '0.25332', 'rewards_train/rejected': '0.05279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20053', 'logps_train/rejected': '-103.09', 'logps_train/chosen': '-136.03', 'loss/train': '0.62475', 'examples_per_second': '30.43', 'grad_norm': '20.75', 'counters/examples': 271392, 'counters/updates': 8481}
train stats after 271424 examples: {'rewards_train/chosen': '0.14551', 'rewards_train/rejected': '0.01858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12693', 'logps_train/rejected': '-119.9', 'logps_train/chosen': '-103.95', 'loss/train': '0.66116', 'examples_per_second': '30.961', 'grad_norm': '18.125', 'counters/examples': 271424, 'counters/updates': 8482}
train stats after 271456 examples: {'rewards_train/chosen': '0.17522', 'rewards_train/rejected': '0.095347', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07987', 'logps_train/rejected': '-121.2', 'logps_train/chosen': '-176.9', 'loss/train': '0.67454', 'examples_per_second': '31.501', 'grad_norm': '23', 'counters/examples': 271456, 'counters/updates': 8483}
skipping logging after 271488 examples to avoid logging too frequently
train stats after 271520 examples: {'rewards_train/chosen': '0.061294', 'rewards_train/rejected': '0.0062648', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055029', 'logps_train/rejected': '-149.34', 'logps_train/chosen': '-112.01', 'loss/train': '0.68419', 'examples_per_second': '30.6', 'grad_norm': '21.125', 'counters/examples': 271520, 'counters/updates': 8485}
skipping logging after 271552 examples to avoid logging too frequently
train stats after 271584 examples: {'rewards_train/chosen': '0.20736', 'rewards_train/rejected': '-0.04575', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25311', 'logps_train/rejected': '-124.74', 'logps_train/chosen': '-162.44', 'loss/train': '0.5903', 'examples_per_second': '30.539', 'grad_norm': '20.125', 'counters/examples': 271584, 'counters/updates': 8487}
skipping logging after 271616 examples to avoid logging too frequently
train stats after 271648 examples: {'rewards_train/chosen': '0.27119', 'rewards_train/rejected': '0.1196', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15159', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-144.86', 'loss/train': '0.63504', 'examples_per_second': '31.529', 'grad_norm': '21.25', 'counters/examples': 271648, 'counters/updates': 8489}
train stats after 271680 examples: {'rewards_train/chosen': '0.17008', 'rewards_train/rejected': '0.11179', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058291', 'logps_train/rejected': '-118.18', 'logps_train/chosen': '-117.67', 'loss/train': '0.67893', 'examples_per_second': '30.719', 'grad_norm': '19.625', 'counters/examples': 271680, 'counters/updates': 8490}
train stats after 271712 examples: {'rewards_train/chosen': '0.16436', 'rewards_train/rejected': '-0.026059', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19042', 'logps_train/rejected': '-88.353', 'logps_train/chosen': '-116.79', 'loss/train': '0.61808', 'examples_per_second': '32.432', 'grad_norm': '17.25', 'counters/examples': 271712, 'counters/updates': 8491}
train stats after 271744 examples: {'rewards_train/chosen': '0.24879', 'rewards_train/rejected': '0.067545', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18124', 'logps_train/rejected': '-137.92', 'logps_train/chosen': '-145.67', 'loss/train': '0.62742', 'examples_per_second': '31.513', 'grad_norm': '20.375', 'counters/examples': 271744, 'counters/updates': 8492}
train stats after 271776 examples: {'rewards_train/chosen': '0.17832', 'rewards_train/rejected': '0.12029', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05803', 'logps_train/rejected': '-175.27', 'logps_train/chosen': '-187.72', 'loss/train': '0.69008', 'examples_per_second': '30.562', 'grad_norm': '25.375', 'counters/examples': 271776, 'counters/updates': 8493}
train stats after 271808 examples: {'rewards_train/chosen': '0.22266', 'rewards_train/rejected': '0.16935', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053305', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-131.07', 'loss/train': '0.68152', 'examples_per_second': '31.436', 'grad_norm': '20.875', 'counters/examples': 271808, 'counters/updates': 8494}
train stats after 271840 examples: {'rewards_train/chosen': '0.24305', 'rewards_train/rejected': '0.033606', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20944', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-129.3', 'loss/train': '0.60938', 'examples_per_second': '31.424', 'grad_norm': '17.75', 'counters/examples': 271840, 'counters/updates': 8495}
train stats after 271872 examples: {'rewards_train/chosen': '0.30069', 'rewards_train/rejected': '0.098306', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20238', 'logps_train/rejected': '-112.82', 'logps_train/chosen': '-156.23', 'loss/train': '0.61751', 'examples_per_second': '31.508', 'grad_norm': '20.75', 'counters/examples': 271872, 'counters/updates': 8496}
train stats after 271904 examples: {'rewards_train/chosen': '0.075568', 'rewards_train/rejected': '0.03475', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040818', 'logps_train/rejected': '-125.63', 'logps_train/chosen': '-106.86', 'loss/train': '0.68165', 'examples_per_second': '31.691', 'grad_norm': '19.75', 'counters/examples': 271904, 'counters/updates': 8497}
skipping logging after 271936 examples to avoid logging too frequently
train stats after 271968 examples: {'rewards_train/chosen': '0.1131', 'rewards_train/rejected': '0.024871', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.088226', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-132.79', 'loss/train': '0.6701', 'examples_per_second': '32.981', 'grad_norm': '20.625', 'counters/examples': 271968, 'counters/updates': 8499}
train stats after 272000 examples: {'rewards_train/chosen': '0.040883', 'rewards_train/rejected': '-0.013886', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054769', 'logps_train/rejected': '-107.93', 'logps_train/chosen': '-103.53', 'loss/train': '0.67491', 'examples_per_second': '31.527', 'grad_norm': '19.75', 'counters/examples': 272000, 'counters/updates': 8500}
train stats after 272032 examples: {'rewards_train/chosen': '0.13286', 'rewards_train/rejected': '-0.051257', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18412', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-138.49', 'loss/train': '0.62611', 'examples_per_second': '32.461', 'grad_norm': '20', 'counters/examples': 272032, 'counters/updates': 8501}
train stats after 272064 examples: {'rewards_train/chosen': '0.091447', 'rewards_train/rejected': '0.008549', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082898', 'logps_train/rejected': '-98.151', 'logps_train/chosen': '-109.8', 'loss/train': '0.67853', 'examples_per_second': '32.528', 'grad_norm': '19.25', 'counters/examples': 272064, 'counters/updates': 8502}
train stats after 272096 examples: {'rewards_train/chosen': '0.15013', 'rewards_train/rejected': '-0.041607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19173', 'logps_train/rejected': '-144.27', 'logps_train/chosen': '-180.68', 'loss/train': '0.61811', 'examples_per_second': '30.16', 'grad_norm': '21.875', 'counters/examples': 272096, 'counters/updates': 8503}
skipping logging after 272128 examples to avoid logging too frequently
train stats after 272160 examples: {'rewards_train/chosen': '0.17413', 'rewards_train/rejected': '-0.027713', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20185', 'logps_train/rejected': '-146.63', 'logps_train/chosen': '-152.32', 'loss/train': '0.62352', 'examples_per_second': '30.54', 'grad_norm': '20.75', 'counters/examples': 272160, 'counters/updates': 8505}
train stats after 272192 examples: {'rewards_train/chosen': '0.20477', 'rewards_train/rejected': '-0.026607', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23137', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-129.89', 'loss/train': '0.59835', 'examples_per_second': '31.472', 'grad_norm': '19', 'counters/examples': 272192, 'counters/updates': 8506}
skipping logging after 272224 examples to avoid logging too frequently
train stats after 272256 examples: {'rewards_train/chosen': '0.10524', 'rewards_train/rejected': '0.075383', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029856', 'logps_train/rejected': '-130.8', 'logps_train/chosen': '-154.68', 'loss/train': '0.70478', 'examples_per_second': '31.583', 'grad_norm': '21.5', 'counters/examples': 272256, 'counters/updates': 8508}
train stats after 272288 examples: {'rewards_train/chosen': '0.18725', 'rewards_train/rejected': '0.0071846', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18007', 'logps_train/rejected': '-78.665', 'logps_train/chosen': '-100.87', 'loss/train': '0.62449', 'examples_per_second': '30.604', 'grad_norm': '16', 'counters/examples': 272288, 'counters/updates': 8509}
train stats after 272320 examples: {'rewards_train/chosen': '0.12137', 'rewards_train/rejected': '0.017366', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.104', 'logps_train/rejected': '-111.84', 'logps_train/chosen': '-127.04', 'loss/train': '0.67027', 'examples_per_second': '31.512', 'grad_norm': '20.375', 'counters/examples': 272320, 'counters/updates': 8510}
train stats after 272352 examples: {'rewards_train/chosen': '0.19355', 'rewards_train/rejected': '0.028215', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16533', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-142.22', 'loss/train': '0.65476', 'examples_per_second': '31.759', 'grad_norm': '19.75', 'counters/examples': 272352, 'counters/updates': 8511}
train stats after 272384 examples: {'rewards_train/chosen': '0.1506', 'rewards_train/rejected': '0.0089413', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14166', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-159.09', 'loss/train': '0.63583', 'examples_per_second': '31.366', 'grad_norm': '21.5', 'counters/examples': 272384, 'counters/updates': 8512}
train stats after 272416 examples: {'rewards_train/chosen': '0.21452', 'rewards_train/rejected': '0.063459', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15106', 'logps_train/rejected': '-112.67', 'logps_train/chosen': '-143.55', 'loss/train': '0.64107', 'examples_per_second': '31.288', 'grad_norm': '20.875', 'counters/examples': 272416, 'counters/updates': 8513}
train stats after 272448 examples: {'rewards_train/chosen': '0.19988', 'rewards_train/rejected': '-0.007173', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20706', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-188.63', 'loss/train': '0.62', 'examples_per_second': '30.039', 'grad_norm': '22.375', 'counters/examples': 272448, 'counters/updates': 8514}
train stats after 272480 examples: {'rewards_train/chosen': '0.16638', 'rewards_train/rejected': '-0.012151', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17853', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-138.75', 'loss/train': '0.63143', 'examples_per_second': '32.423', 'grad_norm': '20.5', 'counters/examples': 272480, 'counters/updates': 8515}
train stats after 272512 examples: {'rewards_train/chosen': '0.1127', 'rewards_train/rejected': '0.017261', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095443', 'logps_train/rejected': '-105.47', 'logps_train/chosen': '-107.26', 'loss/train': '0.65889', 'examples_per_second': '30.129', 'grad_norm': '17.875', 'counters/examples': 272512, 'counters/updates': 8516}
train stats after 272544 examples: {'rewards_train/chosen': '0.19991', 'rewards_train/rejected': '-0.010223', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21013', 'logps_train/rejected': '-135.59', 'logps_train/chosen': '-153.23', 'loss/train': '0.61775', 'examples_per_second': '31.32', 'grad_norm': '18.875', 'counters/examples': 272544, 'counters/updates': 8517}
skipping logging after 272576 examples to avoid logging too frequently
train stats after 272608 examples: {'rewards_train/chosen': '0.10897', 'rewards_train/rejected': '-0.0078024', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11677', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-107.98', 'loss/train': '0.65244', 'examples_per_second': '32.999', 'grad_norm': '19.625', 'counters/examples': 272608, 'counters/updates': 8519}
skipping logging after 272640 examples to avoid logging too frequently
train stats after 272672 examples: {'rewards_train/chosen': '0.2872', 'rewards_train/rejected': '0.035785', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25142', 'logps_train/rejected': '-108.19', 'logps_train/chosen': '-147.6', 'loss/train': '0.59795', 'examples_per_second': '36.968', 'grad_norm': '18.125', 'counters/examples': 272672, 'counters/updates': 8521}
train stats after 272704 examples: {'rewards_train/chosen': '0.1534', 'rewards_train/rejected': '0.035469', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11793', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-126.64', 'loss/train': '0.66091', 'examples_per_second': '32.166', 'grad_norm': '20.75', 'counters/examples': 272704, 'counters/updates': 8522}
skipping logging after 272736 examples to avoid logging too frequently
train stats after 272768 examples: {'rewards_train/chosen': '0.14554', 'rewards_train/rejected': '-0.016154', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16169', 'logps_train/rejected': '-78.529', 'logps_train/chosen': '-137.04', 'loss/train': '0.62864', 'examples_per_second': '31.134', 'grad_norm': '19', 'counters/examples': 272768, 'counters/updates': 8524}
train stats after 272800 examples: {'rewards_train/chosen': '0.21334', 'rewards_train/rejected': '-0.027212', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.24055', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-127.99', 'loss/train': '0.59514', 'examples_per_second': '31.636', 'grad_norm': '18.375', 'counters/examples': 272800, 'counters/updates': 8525}
train stats after 272832 examples: {'rewards_train/chosen': '0.24634', 'rewards_train/rejected': '0.12632', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12002', 'logps_train/rejected': '-130.1', 'logps_train/chosen': '-120.23', 'loss/train': '0.65093', 'examples_per_second': '31.559', 'grad_norm': '21.375', 'counters/examples': 272832, 'counters/updates': 8526}
train stats after 272864 examples: {'rewards_train/chosen': '0.14515', 'rewards_train/rejected': '0.029988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11516', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-137.34', 'loss/train': '0.65621', 'examples_per_second': '30.088', 'grad_norm': '20', 'counters/examples': 272864, 'counters/updates': 8527}
train stats after 272896 examples: {'rewards_train/chosen': '0.16953', 'rewards_train/rejected': '0.04269', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12684', 'logps_train/rejected': '-91.774', 'logps_train/chosen': '-103.63', 'loss/train': '0.65006', 'examples_per_second': '32.843', 'grad_norm': '17.375', 'counters/examples': 272896, 'counters/updates': 8528}
train stats after 272928 examples: {'rewards_train/chosen': '0.19643', 'rewards_train/rejected': '-0.049759', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24619', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-137.51', 'loss/train': '0.6027', 'examples_per_second': '31.903', 'grad_norm': '17.5', 'counters/examples': 272928, 'counters/updates': 8529}
skipping logging after 272960 examples to avoid logging too frequently
train stats after 272992 examples: {'rewards_train/chosen': '0.14566', 'rewards_train/rejected': '0.088124', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.057533', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-97.448', 'loss/train': '0.68266', 'examples_per_second': '31.691', 'grad_norm': '18.625', 'counters/examples': 272992, 'counters/updates': 8531}
skipping logging after 273024 examples to avoid logging too frequently
train stats after 273056 examples: {'rewards_train/chosen': '0.18404', 'rewards_train/rejected': '0.0036571', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18038', 'logps_train/rejected': '-102.1', 'logps_train/chosen': '-133.49', 'loss/train': '0.62306', 'examples_per_second': '30.374', 'grad_norm': '18.125', 'counters/examples': 273056, 'counters/updates': 8533}
train stats after 273088 examples: {'rewards_train/chosen': '0.2344', 'rewards_train/rejected': '-0.12388', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.35828', 'logps_train/rejected': '-108.89', 'logps_train/chosen': '-169.43', 'loss/train': '0.5429', 'examples_per_second': '32.692', 'grad_norm': '18.875', 'counters/examples': 273088, 'counters/updates': 8534}
train stats after 273120 examples: {'rewards_train/chosen': '0.17973', 'rewards_train/rejected': '0.023717', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15601', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-131.01', 'loss/train': '0.62734', 'examples_per_second': '32.584', 'grad_norm': '20', 'counters/examples': 273120, 'counters/updates': 8535}
train stats after 273152 examples: {'rewards_train/chosen': '0.21382', 'rewards_train/rejected': '0.02532', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1885', 'logps_train/rejected': '-102.65', 'logps_train/chosen': '-142.14', 'loss/train': '0.63139', 'examples_per_second': '31.92', 'grad_norm': '20.75', 'counters/examples': 273152, 'counters/updates': 8536}
train stats after 273184 examples: {'rewards_train/chosen': '0.12817', 'rewards_train/rejected': '-0.015185', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14335', 'logps_train/rejected': '-122.07', 'logps_train/chosen': '-127.63', 'loss/train': '0.64051', 'examples_per_second': '33.237', 'grad_norm': '18.875', 'counters/examples': 273184, 'counters/updates': 8537}
train stats after 273216 examples: {'rewards_train/chosen': '0.16988', 'rewards_train/rejected': '0.03529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13459', 'logps_train/rejected': '-129.15', 'logps_train/chosen': '-139.16', 'loss/train': '0.63708', 'examples_per_second': '32.246', 'grad_norm': '20', 'counters/examples': 273216, 'counters/updates': 8538}
train stats after 273248 examples: {'rewards_train/chosen': '0.092855', 'rewards_train/rejected': '-0.020522', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.11338', 'logps_train/rejected': '-113.91', 'logps_train/chosen': '-95.015', 'loss/train': '0.65993', 'examples_per_second': '30.908', 'grad_norm': '18.25', 'counters/examples': 273248, 'counters/updates': 8539}
train stats after 273280 examples: {'rewards_train/chosen': '0.12303', 'rewards_train/rejected': '0.055226', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067802', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-138.41', 'loss/train': '0.67763', 'examples_per_second': '31.893', 'grad_norm': '21.75', 'counters/examples': 273280, 'counters/updates': 8540}
train stats after 273312 examples: {'rewards_train/chosen': '0.16852', 'rewards_train/rejected': '-0.032983', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2015', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-147.64', 'loss/train': '0.60803', 'examples_per_second': '31', 'grad_norm': '19.625', 'counters/examples': 273312, 'counters/updates': 8541}
train stats after 273344 examples: {'rewards_train/chosen': '0.36557', 'rewards_train/rejected': '0.2349', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13067', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-145.19', 'loss/train': '0.64774', 'examples_per_second': '31.368', 'grad_norm': '22', 'counters/examples': 273344, 'counters/updates': 8542}
train stats after 273376 examples: {'rewards_train/chosen': '0.13897', 'rewards_train/rejected': '-0.013844', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15281', 'logps_train/rejected': '-85.535', 'logps_train/chosen': '-89.106', 'loss/train': '0.64995', 'examples_per_second': '31.587', 'grad_norm': '16.125', 'counters/examples': 273376, 'counters/updates': 8543}
train stats after 273408 examples: {'rewards_train/chosen': '0.24533', 'rewards_train/rejected': '0.20059', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.044736', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-130.09', 'loss/train': '0.68613', 'examples_per_second': '30.558', 'grad_norm': '20.125', 'counters/examples': 273408, 'counters/updates': 8544}
skipping logging after 273440 examples to avoid logging too frequently
train stats after 273472 examples: {'rewards_train/chosen': '0.14247', 'rewards_train/rejected': '-0.01227', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15474', 'logps_train/rejected': '-119.35', 'logps_train/chosen': '-154.99', 'loss/train': '0.63867', 'examples_per_second': '27.127', 'grad_norm': '21.25', 'counters/examples': 273472, 'counters/updates': 8546}
train stats after 273504 examples: {'rewards_train/chosen': '0.15007', 'rewards_train/rejected': '0.040755', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10931', 'logps_train/rejected': '-95.494', 'logps_train/chosen': '-123.29', 'loss/train': '0.65161', 'examples_per_second': '30.806', 'grad_norm': '17.625', 'counters/examples': 273504, 'counters/updates': 8547}
train stats after 273536 examples: {'rewards_train/chosen': '0.14831', 'rewards_train/rejected': '0.030272', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11804', 'logps_train/rejected': '-144.96', 'logps_train/chosen': '-134.08', 'loss/train': '0.65221', 'examples_per_second': '32.205', 'grad_norm': '22.625', 'counters/examples': 273536, 'counters/updates': 8548}
train stats after 273568 examples: {'rewards_train/chosen': '0.22585', 'rewards_train/rejected': '-0.0056758', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23153', 'logps_train/rejected': '-110.9', 'logps_train/chosen': '-173.01', 'loss/train': '0.59383', 'examples_per_second': '25.821', 'grad_norm': '20.625', 'counters/examples': 273568, 'counters/updates': 8549}
train stats after 273600 examples: {'rewards_train/chosen': '0.21908', 'rewards_train/rejected': '0.014974', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20411', 'logps_train/rejected': '-105.88', 'logps_train/chosen': '-124.41', 'loss/train': '0.6195', 'examples_per_second': '31.488', 'grad_norm': '17.125', 'counters/examples': 273600, 'counters/updates': 8550}
train stats after 273632 examples: {'rewards_train/chosen': '0.16449', 'rewards_train/rejected': '0.032102', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13239', 'logps_train/rejected': '-147.81', 'logps_train/chosen': '-114.43', 'loss/train': '0.6581', 'examples_per_second': '31.302', 'grad_norm': '20.875', 'counters/examples': 273632, 'counters/updates': 8551}
train stats after 273664 examples: {'rewards_train/chosen': '0.17711', 'rewards_train/rejected': '-0.033834', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21094', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-120.66', 'loss/train': '0.60969', 'examples_per_second': '31.676', 'grad_norm': '18.375', 'counters/examples': 273664, 'counters/updates': 8552}
train stats after 273696 examples: {'rewards_train/chosen': '0.15177', 'rewards_train/rejected': '0.10439', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047378', 'logps_train/rejected': '-111.62', 'logps_train/chosen': '-125.09', 'loss/train': '0.68982', 'examples_per_second': '31.106', 'grad_norm': '20.375', 'counters/examples': 273696, 'counters/updates': 8553}
skipping logging after 273728 examples to avoid logging too frequently
train stats after 273760 examples: {'rewards_train/chosen': '0.16856', 'rewards_train/rejected': '-0.003026', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17158', 'logps_train/rejected': '-111.19', 'logps_train/chosen': '-120.68', 'loss/train': '0.62749', 'examples_per_second': '32.331', 'grad_norm': '19.125', 'counters/examples': 273760, 'counters/updates': 8555}
train stats after 273792 examples: {'rewards_train/chosen': '0.1453', 'rewards_train/rejected': '0.013569', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13173', 'logps_train/rejected': '-144.56', 'logps_train/chosen': '-136.96', 'loss/train': '0.6529', 'examples_per_second': '30.718', 'grad_norm': '21.75', 'counters/examples': 273792, 'counters/updates': 8556}
skipping logging after 273824 examples to avoid logging too frequently
train stats after 273856 examples: {'rewards_train/chosen': '0.19125', 'rewards_train/rejected': '0.058111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13314', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-165.6', 'loss/train': '0.65181', 'examples_per_second': '39.228', 'grad_norm': '21.875', 'counters/examples': 273856, 'counters/updates': 8558}
train stats after 273888 examples: {'rewards_train/chosen': '0.14612', 'rewards_train/rejected': '0.082043', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064073', 'logps_train/rejected': '-122.63', 'logps_train/chosen': '-122.89', 'loss/train': '0.67236', 'examples_per_second': '30.151', 'grad_norm': '19.375', 'counters/examples': 273888, 'counters/updates': 8559}
train stats after 273920 examples: {'rewards_train/chosen': '0.12707', 'rewards_train/rejected': '0.089937', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.037132', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-138.35', 'loss/train': '0.69536', 'examples_per_second': '31.359', 'grad_norm': '21.25', 'counters/examples': 273920, 'counters/updates': 8560}
train stats after 273952 examples: {'rewards_train/chosen': '0.20875', 'rewards_train/rejected': '0.07337', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13538', 'logps_train/rejected': '-114.56', 'logps_train/chosen': '-129.09', 'loss/train': '0.63731', 'examples_per_second': '31.447', 'grad_norm': '19', 'counters/examples': 273952, 'counters/updates': 8561}
train stats after 273984 examples: {'rewards_train/chosen': '0.10398', 'rewards_train/rejected': '0.015878', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088102', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-109.11', 'loss/train': '0.66296', 'examples_per_second': '30.92', 'grad_norm': '20', 'counters/examples': 273984, 'counters/updates': 8562}
train stats after 274016 examples: {'rewards_train/chosen': '0.17247', 'rewards_train/rejected': '-0.0076339', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1801', 'logps_train/rejected': '-108.09', 'logps_train/chosen': '-120.03', 'loss/train': '0.62869', 'examples_per_second': '30.19', 'grad_norm': '19', 'counters/examples': 274016, 'counters/updates': 8563}
skipping logging after 274048 examples to avoid logging too frequently
train stats after 274080 examples: {'rewards_train/chosen': '0.25355', 'rewards_train/rejected': '0.034023', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21952', 'logps_train/rejected': '-106.64', 'logps_train/chosen': '-138.72', 'loss/train': '0.60766', 'examples_per_second': '33.251', 'grad_norm': '18.25', 'counters/examples': 274080, 'counters/updates': 8565}
train stats after 274112 examples: {'rewards_train/chosen': '0.13732', 'rewards_train/rejected': '0.040633', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09669', 'logps_train/rejected': '-113.59', 'logps_train/chosen': '-151.26', 'loss/train': '0.67187', 'examples_per_second': '30.115', 'grad_norm': '20.625', 'counters/examples': 274112, 'counters/updates': 8566}
train stats after 274144 examples: {'rewards_train/chosen': '0.22981', 'rewards_train/rejected': '0.13852', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091288', 'logps_train/rejected': '-160.37', 'logps_train/chosen': '-153.96', 'loss/train': '0.66979', 'examples_per_second': '31.379', 'grad_norm': '22.625', 'counters/examples': 274144, 'counters/updates': 8567}
train stats after 274176 examples: {'rewards_train/chosen': '0.14268', 'rewards_train/rejected': '-0.0038963', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14658', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-139.37', 'loss/train': '0.64406', 'examples_per_second': '32.829', 'grad_norm': '22.125', 'counters/examples': 274176, 'counters/updates': 8568}
skipping logging after 274208 examples to avoid logging too frequently
train stats after 274240 examples: {'rewards_train/chosen': '0.19363', 'rewards_train/rejected': '0.025373', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16825', 'logps_train/rejected': '-125.99', 'logps_train/chosen': '-119.32', 'loss/train': '0.63341', 'examples_per_second': '31.495', 'grad_norm': '19.875', 'counters/examples': 274240, 'counters/updates': 8570}
train stats after 274272 examples: {'rewards_train/chosen': '0.10868', 'rewards_train/rejected': '-0.099198', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20787', 'logps_train/rejected': '-128.37', 'logps_train/chosen': '-145.79', 'loss/train': '0.61888', 'examples_per_second': '31.537', 'grad_norm': '20.625', 'counters/examples': 274272, 'counters/updates': 8571}
train stats after 274304 examples: {'rewards_train/chosen': '0.17435', 'rewards_train/rejected': '0.0057645', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16858', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-107.91', 'loss/train': '0.62212', 'examples_per_second': '33.103', 'grad_norm': '18.5', 'counters/examples': 274304, 'counters/updates': 8572}
train stats after 274336 examples: {'rewards_train/chosen': '0.15182', 'rewards_train/rejected': '-0.049352', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20117', 'logps_train/rejected': '-105.87', 'logps_train/chosen': '-132.53', 'loss/train': '0.61428', 'examples_per_second': '31.896', 'grad_norm': '18.625', 'counters/examples': 274336, 'counters/updates': 8573}
train stats after 274368 examples: {'rewards_train/chosen': '0.20717', 'rewards_train/rejected': '0.0096361', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19753', 'logps_train/rejected': '-119.82', 'logps_train/chosen': '-127.5', 'loss/train': '0.62702', 'examples_per_second': '31.581', 'grad_norm': '17.75', 'counters/examples': 274368, 'counters/updates': 8574}
skipping logging after 274400 examples to avoid logging too frequently
train stats after 274432 examples: {'rewards_train/chosen': '0.043859', 'rewards_train/rejected': '0.099846', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.055987', 'logps_train/rejected': '-95.709', 'logps_train/chosen': '-117.49', 'loss/train': '0.74094', 'examples_per_second': '31.468', 'grad_norm': '21.125', 'counters/examples': 274432, 'counters/updates': 8576}
train stats after 274464 examples: {'rewards_train/chosen': '0.17584', 'rewards_train/rejected': '0.042036', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1338', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-136.3', 'loss/train': '0.65803', 'examples_per_second': '31.189', 'grad_norm': '21', 'counters/examples': 274464, 'counters/updates': 8577}
train stats after 274496 examples: {'rewards_train/chosen': '0.14442', 'rewards_train/rejected': '0.073662', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070755', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-117.32', 'loss/train': '0.67671', 'examples_per_second': '31.631', 'grad_norm': '18.875', 'counters/examples': 274496, 'counters/updates': 8578}
train stats after 274528 examples: {'rewards_train/chosen': '0.18597', 'rewards_train/rejected': '-0.0016486', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18762', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-137.89', 'loss/train': '0.63406', 'examples_per_second': '30.641', 'grad_norm': '20.125', 'counters/examples': 274528, 'counters/updates': 8579}
train stats after 274560 examples: {'rewards_train/chosen': '0.19688', 'rewards_train/rejected': '0.13038', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066502', 'logps_train/rejected': '-154.83', 'logps_train/chosen': '-153.64', 'loss/train': '0.67404', 'examples_per_second': '31.205', 'grad_norm': '23', 'counters/examples': 274560, 'counters/updates': 8580}
skipping logging after 274592 examples to avoid logging too frequently
train stats after 274624 examples: {'rewards_train/chosen': '0.10529', 'rewards_train/rejected': '-0.17094', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27622', 'logps_train/rejected': '-78.198', 'logps_train/chosen': '-100.92', 'loss/train': '0.58386', 'examples_per_second': '34.053', 'grad_norm': '14.688', 'counters/examples': 274624, 'counters/updates': 8582}
train stats after 274656 examples: {'rewards_train/chosen': '0.081771', 'rewards_train/rejected': '0.052768', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029002', 'logps_train/rejected': '-152.62', 'logps_train/chosen': '-166.84', 'loss/train': '0.69924', 'examples_per_second': '25.799', 'grad_norm': '23', 'counters/examples': 274656, 'counters/updates': 8583}
train stats after 274688 examples: {'rewards_train/chosen': '0.19497', 'rewards_train/rejected': '0.092771', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1022', 'logps_train/rejected': '-128.82', 'logps_train/chosen': '-132.96', 'loss/train': '0.65972', 'examples_per_second': '31.349', 'grad_norm': '20.375', 'counters/examples': 274688, 'counters/updates': 8584}
train stats after 274720 examples: {'rewards_train/chosen': '0.13551', 'rewards_train/rejected': '0.011885', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12362', 'logps_train/rejected': '-159.43', 'logps_train/chosen': '-130.47', 'loss/train': '0.64509', 'examples_per_second': '30.853', 'grad_norm': '21.75', 'counters/examples': 274720, 'counters/updates': 8585}
train stats after 274752 examples: {'rewards_train/chosen': '0.063805', 'rewards_train/rejected': '-0.06277', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12657', 'logps_train/rejected': '-94.365', 'logps_train/chosen': '-153.33', 'loss/train': '0.64343', 'examples_per_second': '31.342', 'grad_norm': '18.75', 'counters/examples': 274752, 'counters/updates': 8586}
train stats after 274784 examples: {'rewards_train/chosen': '0.17513', 'rewards_train/rejected': '0.094142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080992', 'logps_train/rejected': '-113.91', 'logps_train/chosen': '-142.49', 'loss/train': '0.66245', 'examples_per_second': '30.836', 'grad_norm': '19', 'counters/examples': 274784, 'counters/updates': 8587}
train stats after 274816 examples: {'rewards_train/chosen': '0.15113', 'rewards_train/rejected': '0.020158', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13097', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-133.6', 'loss/train': '0.64835', 'examples_per_second': '31.544', 'grad_norm': '22.25', 'counters/examples': 274816, 'counters/updates': 8588}
skipping logging after 274848 examples to avoid logging too frequently
train stats after 274880 examples: {'rewards_train/chosen': '0.24409', 'rewards_train/rejected': '0.10742', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13667', 'logps_train/rejected': '-137.15', 'logps_train/chosen': '-138.02', 'loss/train': '0.63667', 'examples_per_second': '31.052', 'grad_norm': '21', 'counters/examples': 274880, 'counters/updates': 8590}
train stats after 274912 examples: {'rewards_train/chosen': '0.15107', 'rewards_train/rejected': '0.062228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088846', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-150.54', 'loss/train': '0.66672', 'examples_per_second': '31.477', 'grad_norm': '21.75', 'counters/examples': 274912, 'counters/updates': 8591}
train stats after 274944 examples: {'rewards_train/chosen': '0.17548', 'rewards_train/rejected': '-0.011759', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18724', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-104.87', 'loss/train': '0.62222', 'examples_per_second': '29.996', 'grad_norm': '19.125', 'counters/examples': 274944, 'counters/updates': 8592}
train stats after 274976 examples: {'rewards_train/chosen': '0.13071', 'rewards_train/rejected': '0.023662', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10705', 'logps_train/rejected': '-100.14', 'logps_train/chosen': '-120.2', 'loss/train': '0.66401', 'examples_per_second': '33.197', 'grad_norm': '18.875', 'counters/examples': 274976, 'counters/updates': 8593}
train stats after 275008 examples: {'rewards_train/chosen': '0.22931', 'rewards_train/rejected': '0.023784', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20552', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-128.52', 'loss/train': '0.61902', 'examples_per_second': '30.514', 'grad_norm': '18.125', 'counters/examples': 275008, 'counters/updates': 8594}
skipping logging after 275040 examples to avoid logging too frequently
train stats after 275072 examples: {'rewards_train/chosen': '0.32651', 'rewards_train/rejected': '0.1391', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18742', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-131', 'loss/train': '0.63712', 'examples_per_second': '30.566', 'grad_norm': '21', 'counters/examples': 275072, 'counters/updates': 8596}
train stats after 275104 examples: {'rewards_train/chosen': '0.17911', 'rewards_train/rejected': '0.12972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049395', 'logps_train/rejected': '-144.32', 'logps_train/chosen': '-175.2', 'loss/train': '0.68676', 'examples_per_second': '31.537', 'grad_norm': '24.25', 'counters/examples': 275104, 'counters/updates': 8597}
train stats after 275136 examples: {'rewards_train/chosen': '0.19928', 'rewards_train/rejected': '0.05228', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.147', 'logps_train/rejected': '-98.985', 'logps_train/chosen': '-129.32', 'loss/train': '0.64866', 'examples_per_second': '31.368', 'grad_norm': '18.375', 'counters/examples': 275136, 'counters/updates': 8598}
train stats after 275168 examples: {'rewards_train/chosen': '0.1302', 'rewards_train/rejected': '0.019107', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11109', 'logps_train/rejected': '-110.07', 'logps_train/chosen': '-107.54', 'loss/train': '0.65306', 'examples_per_second': '32.524', 'grad_norm': '19.5', 'counters/examples': 275168, 'counters/updates': 8599}
skipping logging after 275200 examples to avoid logging too frequently
train stats after 275232 examples: {'rewards_train/chosen': '0.21285', 'rewards_train/rejected': '0.11042', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10243', 'logps_train/rejected': '-110.4', 'logps_train/chosen': '-133.26', 'loss/train': '0.67897', 'examples_per_second': '31.41', 'grad_norm': '20', 'counters/examples': 275232, 'counters/updates': 8601}
train stats after 275264 examples: {'rewards_train/chosen': '0.12503', 'rewards_train/rejected': '0.067985', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057049', 'logps_train/rejected': '-100.91', 'logps_train/chosen': '-121.24', 'loss/train': '0.68761', 'examples_per_second': '31.067', 'grad_norm': '18.75', 'counters/examples': 275264, 'counters/updates': 8602}
train stats after 275296 examples: {'rewards_train/chosen': '0.23094', 'rewards_train/rejected': '0.028714', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20222', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-133.45', 'loss/train': '0.61479', 'examples_per_second': '31.363', 'grad_norm': '20', 'counters/examples': 275296, 'counters/updates': 8603}
train stats after 275328 examples: {'rewards_train/chosen': '0.12408', 'rewards_train/rejected': '0.040845', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083235', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-95.38', 'loss/train': '0.67929', 'examples_per_second': '30.899', 'grad_norm': '20.875', 'counters/examples': 275328, 'counters/updates': 8604}
train stats after 275360 examples: {'rewards_train/chosen': '0.19548', 'rewards_train/rejected': '0.039663', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15582', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-162.71', 'loss/train': '0.63221', 'examples_per_second': '31.567', 'grad_norm': '20.75', 'counters/examples': 275360, 'counters/updates': 8605}
train stats after 275392 examples: {'rewards_train/chosen': '0.23611', 'rewards_train/rejected': '-0.010572', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24668', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-131.41', 'loss/train': '0.6089', 'examples_per_second': '32.335', 'grad_norm': '18.25', 'counters/examples': 275392, 'counters/updates': 8606}
train stats after 275424 examples: {'rewards_train/chosen': '0.19517', 'rewards_train/rejected': '0.060849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13432', 'logps_train/rejected': '-107.68', 'logps_train/chosen': '-132.48', 'loss/train': '0.64729', 'examples_per_second': '32.1', 'grad_norm': '20', 'counters/examples': 275424, 'counters/updates': 8607}
train stats after 275456 examples: {'rewards_train/chosen': '0.32056', 'rewards_train/rejected': '0.16999', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15057', 'logps_train/rejected': '-137.39', 'logps_train/chosen': '-134.63', 'loss/train': '0.64182', 'examples_per_second': '31.628', 'grad_norm': '22', 'counters/examples': 275456, 'counters/updates': 8608}
train stats after 275488 examples: {'rewards_train/chosen': '0.17879', 'rewards_train/rejected': '0.01095', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16784', 'logps_train/rejected': '-105.96', 'logps_train/chosen': '-124.7', 'loss/train': '0.63027', 'examples_per_second': '30.216', 'grad_norm': '19.875', 'counters/examples': 275488, 'counters/updates': 8609}
train stats after 275520 examples: {'rewards_train/chosen': '0.23404', 'rewards_train/rejected': '0.050248', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18379', 'logps_train/rejected': '-109.34', 'logps_train/chosen': '-155.35', 'loss/train': '0.61999', 'examples_per_second': '31.145', 'grad_norm': '19.5', 'counters/examples': 275520, 'counters/updates': 8610}
skipping logging after 275552 examples to avoid logging too frequently
train stats after 275584 examples: {'rewards_train/chosen': '0.2853', 'rewards_train/rejected': '0.16832', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11697', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-153.48', 'loss/train': '0.65154', 'examples_per_second': '32.938', 'grad_norm': '21.875', 'counters/examples': 275584, 'counters/updates': 8612}
train stats after 275616 examples: {'rewards_train/chosen': '0.20116', 'rewards_train/rejected': '0.00461', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19655', 'logps_train/rejected': '-102.03', 'logps_train/chosen': '-146.35', 'loss/train': '0.62701', 'examples_per_second': '31.368', 'grad_norm': '18.625', 'counters/examples': 275616, 'counters/updates': 8613}
train stats after 275648 examples: {'rewards_train/chosen': '0.061329', 'rewards_train/rejected': '0.013537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047792', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-95.036', 'loss/train': '0.67985', 'examples_per_second': '31.911', 'grad_norm': '18.75', 'counters/examples': 275648, 'counters/updates': 8614}
skipping logging after 275680 examples to avoid logging too frequently
train stats after 275712 examples: {'rewards_train/chosen': '0.16622', 'rewards_train/rejected': '0.073263', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092952', 'logps_train/rejected': '-122.16', 'logps_train/chosen': '-116.02', 'loss/train': '0.66307', 'examples_per_second': '31.44', 'grad_norm': '19.375', 'counters/examples': 275712, 'counters/updates': 8616}
train stats after 275744 examples: {'rewards_train/chosen': '0.23987', 'rewards_train/rejected': '0.062034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17783', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-174.23', 'loss/train': '0.63811', 'examples_per_second': '33.065', 'grad_norm': '22.875', 'counters/examples': 275744, 'counters/updates': 8617}
train stats after 275776 examples: {'rewards_train/chosen': '0.26641', 'rewards_train/rejected': '0.020494', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24591', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-145.71', 'loss/train': '0.59983', 'examples_per_second': '30.689', 'grad_norm': '18.625', 'counters/examples': 275776, 'counters/updates': 8618}
train stats after 275808 examples: {'rewards_train/chosen': '0.26597', 'rewards_train/rejected': '0.040841', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22513', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-140.3', 'loss/train': '0.60488', 'examples_per_second': '31.661', 'grad_norm': '19.375', 'counters/examples': 275808, 'counters/updates': 8619}
train stats after 275840 examples: {'rewards_train/chosen': '0.13372', 'rewards_train/rejected': '0.10181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031912', 'logps_train/rejected': '-131.4', 'logps_train/chosen': '-121.18', 'loss/train': '0.69201', 'examples_per_second': '31.085', 'grad_norm': '21.375', 'counters/examples': 275840, 'counters/updates': 8620}
train stats after 275872 examples: {'rewards_train/chosen': '0.22365', 'rewards_train/rejected': '0.12229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10136', 'logps_train/rejected': '-135.02', 'logps_train/chosen': '-159.69', 'loss/train': '0.65978', 'examples_per_second': '31.172', 'grad_norm': '22.375', 'counters/examples': 275872, 'counters/updates': 8621}
train stats after 275904 examples: {'rewards_train/chosen': '0.063531', 'rewards_train/rejected': '0.007816', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055714', 'logps_train/rejected': '-93.413', 'logps_train/chosen': '-81.435', 'loss/train': '0.68957', 'examples_per_second': '31.982', 'grad_norm': '18.125', 'counters/examples': 275904, 'counters/updates': 8622}
skipping logging after 275936 examples to avoid logging too frequently
train stats after 275968 examples: {'rewards_train/chosen': '0.2858', 'rewards_train/rejected': '0.040804', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24499', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-143.76', 'loss/train': '0.60535', 'examples_per_second': '31.623', 'grad_norm': '19.75', 'counters/examples': 275968, 'counters/updates': 8624}
train stats after 276000 examples: {'rewards_train/chosen': '0.23742', 'rewards_train/rejected': '0.074247', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16317', 'logps_train/rejected': '-123.34', 'logps_train/chosen': '-108.74', 'loss/train': '0.63493', 'examples_per_second': '32.007', 'grad_norm': '18.5', 'counters/examples': 276000, 'counters/updates': 8625}
Running evaluation after 276000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.91it/s]
eval after 276000: {'rewards_eval/chosen': '0.18918', 'rewards_eval/rejected': '0.039223', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.14996', 'logps_eval/rejected': '-115', 'logps_eval/chosen': '-133.66', 'loss/eval': '0.64527'}
skipping save for non epoch
train stats after 276032 examples: {'rewards_train/chosen': '0.081646', 'rewards_train/rejected': '-0.070658', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1523', 'logps_train/rejected': '-114.62', 'logps_train/chosen': '-133.39', 'loss/train': '0.63821', 'examples_per_second': '33.182', 'grad_norm': '20.5', 'counters/examples': 276032, 'counters/updates': 8626}
skipping logging after 276064 examples to avoid logging too frequently
train stats after 276096 examples: {'rewards_train/chosen': '0.1922', 'rewards_train/rejected': '0.22591', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033703', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-142.14', 'loss/train': '0.73056', 'examples_per_second': '34.687', 'grad_norm': '23.875', 'counters/examples': 276096, 'counters/updates': 8628}
train stats after 276128 examples: {'rewards_train/chosen': '0.12935', 'rewards_train/rejected': '0.071846', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.057503', 'logps_train/rejected': '-104.93', 'logps_train/chosen': '-125.04', 'loss/train': '0.68787', 'examples_per_second': '30.543', 'grad_norm': '20.25', 'counters/examples': 276128, 'counters/updates': 8629}
train stats after 276160 examples: {'rewards_train/chosen': '0.19585', 'rewards_train/rejected': '0.029191', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16665', 'logps_train/rejected': '-101.38', 'logps_train/chosen': '-126.25', 'loss/train': '0.63281', 'examples_per_second': '31.538', 'grad_norm': '18.5', 'counters/examples': 276160, 'counters/updates': 8630}
skipping logging after 276192 examples to avoid logging too frequently
train stats after 276224 examples: {'rewards_train/chosen': '0.11055', 'rewards_train/rejected': '0.12626', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015714', 'logps_train/rejected': '-135.08', 'logps_train/chosen': '-121.05', 'loss/train': '0.71524', 'examples_per_second': '31.261', 'grad_norm': '21.75', 'counters/examples': 276224, 'counters/updates': 8632}
train stats after 276256 examples: {'rewards_train/chosen': '0.13176', 'rewards_train/rejected': '-0.043', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17476', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-154.31', 'loss/train': '0.62109', 'examples_per_second': '31.289', 'grad_norm': '21.625', 'counters/examples': 276256, 'counters/updates': 8633}
train stats after 276288 examples: {'rewards_train/chosen': '0.20429', 'rewards_train/rejected': '0.072126', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13217', 'logps_train/rejected': '-124.61', 'logps_train/chosen': '-142.11', 'loss/train': '0.65625', 'examples_per_second': '32.127', 'grad_norm': '20.75', 'counters/examples': 276288, 'counters/updates': 8634}
train stats after 276320 examples: {'rewards_train/chosen': '0.24707', 'rewards_train/rejected': '-0.02455', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27162', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-152.48', 'loss/train': '0.58901', 'examples_per_second': '30.915', 'grad_norm': '19.125', 'counters/examples': 276320, 'counters/updates': 8635}
train stats after 276352 examples: {'rewards_train/chosen': '0.20346', 'rewards_train/rejected': '0.051756', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15171', 'logps_train/rejected': '-149.33', 'logps_train/chosen': '-148.57', 'loss/train': '0.64494', 'examples_per_second': '31.309', 'grad_norm': '20.75', 'counters/examples': 276352, 'counters/updates': 8636}
train stats after 276384 examples: {'rewards_train/chosen': '0.15573', 'rewards_train/rejected': '0.030794', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12494', 'logps_train/rejected': '-92.885', 'logps_train/chosen': '-112.58', 'loss/train': '0.64769', 'examples_per_second': '30.819', 'grad_norm': '18.875', 'counters/examples': 276384, 'counters/updates': 8637}
skipping logging after 276416 examples to avoid logging too frequently
train stats after 276448 examples: {'rewards_train/chosen': '0.20957', 'rewards_train/rejected': '0.065127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14445', 'logps_train/rejected': '-148.29', 'logps_train/chosen': '-159.76', 'loss/train': '0.65074', 'examples_per_second': '31.686', 'grad_norm': '21.875', 'counters/examples': 276448, 'counters/updates': 8639}
skipping logging after 276480 examples to avoid logging too frequently
train stats after 276512 examples: {'rewards_train/chosen': '0.24286', 'rewards_train/rejected': '-0.009648', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2525', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-125.7', 'loss/train': '0.60045', 'examples_per_second': '33.858', 'grad_norm': '17.125', 'counters/examples': 276512, 'counters/updates': 8641}
train stats after 276544 examples: {'rewards_train/chosen': '0.035532', 'rewards_train/rejected': '0.0014548', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034077', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-113.05', 'loss/train': '0.69847', 'examples_per_second': '32.193', 'grad_norm': '18.75', 'counters/examples': 276544, 'counters/updates': 8642}
train stats after 276576 examples: {'rewards_train/chosen': '0.093467', 'rewards_train/rejected': '-0.031935', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1254', 'logps_train/rejected': '-103.8', 'logps_train/chosen': '-155.7', 'loss/train': '0.64269', 'examples_per_second': '31.716', 'grad_norm': '21', 'counters/examples': 276576, 'counters/updates': 8643}
train stats after 276608 examples: {'rewards_train/chosen': '0.13433', 'rewards_train/rejected': '-0.020477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1548', 'logps_train/rejected': '-101.72', 'logps_train/chosen': '-142.89', 'loss/train': '0.63321', 'examples_per_second': '31.711', 'grad_norm': '19.625', 'counters/examples': 276608, 'counters/updates': 8644}
train stats after 276640 examples: {'rewards_train/chosen': '0.23884', 'rewards_train/rejected': '0.038903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19993', 'logps_train/rejected': '-101.32', 'logps_train/chosen': '-135.19', 'loss/train': '0.6122', 'examples_per_second': '30.314', 'grad_norm': '17.75', 'counters/examples': 276640, 'counters/updates': 8645}
train stats after 276672 examples: {'rewards_train/chosen': '0.14522', 'rewards_train/rejected': '0.058606', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086617', 'logps_train/rejected': '-154.72', 'logps_train/chosen': '-128.45', 'loss/train': '0.67478', 'examples_per_second': '31.613', 'grad_norm': '20.75', 'counters/examples': 276672, 'counters/updates': 8646}
train stats after 276704 examples: {'rewards_train/chosen': '0.1186', 'rewards_train/rejected': '0.012903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1057', 'logps_train/rejected': '-106.21', 'logps_train/chosen': '-137.1', 'loss/train': '0.67136', 'examples_per_second': '30.258', 'grad_norm': '20.25', 'counters/examples': 276704, 'counters/updates': 8647}
skipping logging after 276736 examples to avoid logging too frequently
train stats after 276768 examples: {'rewards_train/chosen': '0.25839', 'rewards_train/rejected': '0.042853', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21554', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-122.41', 'loss/train': '0.61391', 'examples_per_second': '33.197', 'grad_norm': '18.375', 'counters/examples': 276768, 'counters/updates': 8649}
train stats after 276800 examples: {'rewards_train/chosen': '0.1264', 'rewards_train/rejected': '0.019911', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10649', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-158.34', 'loss/train': '0.65638', 'examples_per_second': '30.177', 'grad_norm': '21.25', 'counters/examples': 276800, 'counters/updates': 8650}
train stats after 276832 examples: {'rewards_train/chosen': '0.084577', 'rewards_train/rejected': '0.01133', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073247', 'logps_train/rejected': '-105.83', 'logps_train/chosen': '-89.438', 'loss/train': '0.67814', 'examples_per_second': '32.002', 'grad_norm': '17.125', 'counters/examples': 276832, 'counters/updates': 8651}
train stats after 276864 examples: {'rewards_train/chosen': '0.099133', 'rewards_train/rejected': '-0.085547', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18468', 'logps_train/rejected': '-90.668', 'logps_train/chosen': '-124.78', 'loss/train': '0.61691', 'examples_per_second': '31.484', 'grad_norm': '17.5', 'counters/examples': 276864, 'counters/updates': 8652}
train stats after 276896 examples: {'rewards_train/chosen': '0.20593', 'rewards_train/rejected': '0.092103', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11382', 'logps_train/rejected': '-152.63', 'logps_train/chosen': '-116.62', 'loss/train': '0.65769', 'examples_per_second': '31.421', 'grad_norm': '19.875', 'counters/examples': 276896, 'counters/updates': 8653}
train stats after 276928 examples: {'rewards_train/chosen': '0.12908', 'rewards_train/rejected': '0.0055726', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12351', 'logps_train/rejected': '-130.4', 'logps_train/chosen': '-149.83', 'loss/train': '0.65612', 'examples_per_second': '32.678', 'grad_norm': '22', 'counters/examples': 276928, 'counters/updates': 8654}
train stats after 276960 examples: {'rewards_train/chosen': '0.15357', 'rewards_train/rejected': '-0.0060506', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15962', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-105.65', 'loss/train': '0.63274', 'examples_per_second': '32.157', 'grad_norm': '17.5', 'counters/examples': 276960, 'counters/updates': 8655}
train stats after 276992 examples: {'rewards_train/chosen': '0.22229', 'rewards_train/rejected': '0.049928', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17236', 'logps_train/rejected': '-93.699', 'logps_train/chosen': '-133', 'loss/train': '0.6304', 'examples_per_second': '31.632', 'grad_norm': '18.5', 'counters/examples': 276992, 'counters/updates': 8656}
skipping logging after 277024 examples to avoid logging too frequently
train stats after 277056 examples: {'rewards_train/chosen': '0.17007', 'rewards_train/rejected': '0.046598', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12347', 'logps_train/rejected': '-119.19', 'logps_train/chosen': '-140.5', 'loss/train': '0.65176', 'examples_per_second': '32.569', 'grad_norm': '21.125', 'counters/examples': 277056, 'counters/updates': 8658}
train stats after 277088 examples: {'rewards_train/chosen': '0.17266', 'rewards_train/rejected': '-0.06378', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23644', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-119.49', 'loss/train': '0.60668', 'examples_per_second': '32.348', 'grad_norm': '18.375', 'counters/examples': 277088, 'counters/updates': 8659}
train stats after 277120 examples: {'rewards_train/chosen': '0.26818', 'rewards_train/rejected': '0.12137', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14681', 'logps_train/rejected': '-172.83', 'logps_train/chosen': '-138.27', 'loss/train': '0.64832', 'examples_per_second': '31.584', 'grad_norm': '23', 'counters/examples': 277120, 'counters/updates': 8660}
skipping logging after 277152 examples to avoid logging too frequently
train stats after 277184 examples: {'rewards_train/chosen': '0.20304', 'rewards_train/rejected': '-0.067711', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27075', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-145.62', 'loss/train': '0.59284', 'examples_per_second': '35.6', 'grad_norm': '17.25', 'counters/examples': 277184, 'counters/updates': 8662}
skipping logging after 277216 examples to avoid logging too frequently
train stats after 277248 examples: {'rewards_train/chosen': '0.12827', 'rewards_train/rejected': '0.065993', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.062274', 'logps_train/rejected': '-146.79', 'logps_train/chosen': '-125.73', 'loss/train': '0.67836', 'examples_per_second': '33.286', 'grad_norm': '22.125', 'counters/examples': 277248, 'counters/updates': 8664}
train stats after 277280 examples: {'rewards_train/chosen': '0.10538', 'rewards_train/rejected': '0.062611', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042767', 'logps_train/rejected': '-139.84', 'logps_train/chosen': '-163.47', 'loss/train': '0.68231', 'examples_per_second': '31.666', 'grad_norm': '22.375', 'counters/examples': 277280, 'counters/updates': 8665}
train stats after 277312 examples: {'rewards_train/chosen': '0.17032', 'rewards_train/rejected': '0.035494', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13482', 'logps_train/rejected': '-137.49', 'logps_train/chosen': '-121.9', 'loss/train': '0.64535', 'examples_per_second': '30.892', 'grad_norm': '20', 'counters/examples': 277312, 'counters/updates': 8666}
train stats after 277344 examples: {'rewards_train/chosen': '0.26413', 'rewards_train/rejected': '0.14843', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11569', 'logps_train/rejected': '-130.95', 'logps_train/chosen': '-134.77', 'loss/train': '0.66856', 'examples_per_second': '31.243', 'grad_norm': '22.25', 'counters/examples': 277344, 'counters/updates': 8667}
train stats after 277376 examples: {'rewards_train/chosen': '0.16783', 'rewards_train/rejected': '0.07032', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097515', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-128.93', 'loss/train': '0.66364', 'examples_per_second': '31.884', 'grad_norm': '20', 'counters/examples': 277376, 'counters/updates': 8668}
train stats after 277408 examples: {'rewards_train/chosen': '0.18549', 'rewards_train/rejected': '-0.0064981', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19198', 'logps_train/rejected': '-94.568', 'logps_train/chosen': '-160.37', 'loss/train': '0.62236', 'examples_per_second': '32.033', 'grad_norm': '19.25', 'counters/examples': 277408, 'counters/updates': 8669}
train stats after 277440 examples: {'rewards_train/chosen': '0.18625', 'rewards_train/rejected': '0.18427', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0019775', 'logps_train/rejected': '-138.9', 'logps_train/chosen': '-136.9', 'loss/train': '0.71225', 'examples_per_second': '32.433', 'grad_norm': '23.125', 'counters/examples': 277440, 'counters/updates': 8670}
train stats after 277472 examples: {'rewards_train/chosen': '0.24926', 'rewards_train/rejected': '0.1307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11856', 'logps_train/rejected': '-114.4', 'logps_train/chosen': '-121.49', 'loss/train': '0.6501', 'examples_per_second': '30.854', 'grad_norm': '17.75', 'counters/examples': 277472, 'counters/updates': 8671}
train stats after 277504 examples: {'rewards_train/chosen': '0.19223', 'rewards_train/rejected': '0.051506', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14073', 'logps_train/rejected': '-148.57', 'logps_train/chosen': '-139.68', 'loss/train': '0.64523', 'examples_per_second': '30.565', 'grad_norm': '23.125', 'counters/examples': 277504, 'counters/updates': 8672}
train stats after 277536 examples: {'rewards_train/chosen': '0.16408', 'rewards_train/rejected': '0.071623', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092458', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-96.965', 'loss/train': '0.66246', 'examples_per_second': '32.399', 'grad_norm': '18.375', 'counters/examples': 277536, 'counters/updates': 8673}
train stats after 277568 examples: {'rewards_train/chosen': '0.21758', 'rewards_train/rejected': '-0.010066', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22765', 'logps_train/rejected': '-140.27', 'logps_train/chosen': '-125.14', 'loss/train': '0.60175', 'examples_per_second': '30.132', 'grad_norm': '18.75', 'counters/examples': 277568, 'counters/updates': 8674}
train stats after 277600 examples: {'rewards_train/chosen': '0.21497', 'rewards_train/rejected': '0.077302', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13767', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-124.58', 'loss/train': '0.64958', 'examples_per_second': '31.534', 'grad_norm': '19.375', 'counters/examples': 277600, 'counters/updates': 8675}
skipping logging after 277632 examples to avoid logging too frequently
train stats after 277664 examples: {'rewards_train/chosen': '0.070108', 'rewards_train/rejected': '-0.0062535', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076361', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-102.95', 'loss/train': '0.66899', 'examples_per_second': '30.821', 'grad_norm': '19.375', 'counters/examples': 277664, 'counters/updates': 8677}
train stats after 277696 examples: {'rewards_train/chosen': '0.18483', 'rewards_train/rejected': '0.037013', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14781', 'logps_train/rejected': '-111.35', 'logps_train/chosen': '-124.65', 'loss/train': '0.63821', 'examples_per_second': '32.963', 'grad_norm': '19.375', 'counters/examples': 277696, 'counters/updates': 8678}
skipping logging after 277728 examples to avoid logging too frequently
train stats after 277760 examples: {'rewards_train/chosen': '0.07603', 'rewards_train/rejected': '-0.035012', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11104', 'logps_train/rejected': '-119.89', 'logps_train/chosen': '-163.15', 'loss/train': '0.64996', 'examples_per_second': '33.86', 'grad_norm': '21.375', 'counters/examples': 277760, 'counters/updates': 8680}
train stats after 277792 examples: {'rewards_train/chosen': '0.18069', 'rewards_train/rejected': '0.0080389', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17265', 'logps_train/rejected': '-157.49', 'logps_train/chosen': '-158.84', 'loss/train': '0.63068', 'examples_per_second': '31.477', 'grad_norm': '22.625', 'counters/examples': 277792, 'counters/updates': 8681}
train stats after 277824 examples: {'rewards_train/chosen': '0.19219', 'rewards_train/rejected': '-0.0075374', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19973', 'logps_train/rejected': '-111.67', 'logps_train/chosen': '-131.84', 'loss/train': '0.61579', 'examples_per_second': '31.324', 'grad_norm': '19', 'counters/examples': 277824, 'counters/updates': 8682}
train stats after 277856 examples: {'rewards_train/chosen': '0.1718', 'rewards_train/rejected': '0.11969', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05211', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-150.57', 'loss/train': '0.68588', 'examples_per_second': '31.648', 'grad_norm': '22.5', 'counters/examples': 277856, 'counters/updates': 8683}
train stats after 277888 examples: {'rewards_train/chosen': '0.21935', 'rewards_train/rejected': '0.051087', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16826', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-134.33', 'loss/train': '0.62807', 'examples_per_second': '31.071', 'grad_norm': '20.75', 'counters/examples': 277888, 'counters/updates': 8684}
train stats after 277920 examples: {'rewards_train/chosen': '0.29298', 'rewards_train/rejected': '0.10144', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19153', 'logps_train/rejected': '-104.79', 'logps_train/chosen': '-152', 'loss/train': '0.61967', 'examples_per_second': '31.559', 'grad_norm': '20.625', 'counters/examples': 277920, 'counters/updates': 8685}
train stats after 277952 examples: {'rewards_train/chosen': '0.12828', 'rewards_train/rejected': '0.014313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11396', 'logps_train/rejected': '-81.994', 'logps_train/chosen': '-104.15', 'loss/train': '0.64719', 'examples_per_second': '31.658', 'grad_norm': '17.625', 'counters/examples': 277952, 'counters/updates': 8686}
train stats after 277984 examples: {'rewards_train/chosen': '0.16239', 'rewards_train/rejected': '0.01148', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15091', 'logps_train/rejected': '-91.445', 'logps_train/chosen': '-128.42', 'loss/train': '0.64093', 'examples_per_second': '30.604', 'grad_norm': '18', 'counters/examples': 277984, 'counters/updates': 8687}
train stats after 278016 examples: {'rewards_train/chosen': '0.31663', 'rewards_train/rejected': '-0.0036596', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.32029', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-147.65', 'loss/train': '0.58345', 'examples_per_second': '31.571', 'grad_norm': '19.625', 'counters/examples': 278016, 'counters/updates': 8688}
train stats after 278048 examples: {'rewards_train/chosen': '0.12848', 'rewards_train/rejected': '0.029432', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099049', 'logps_train/rejected': '-101', 'logps_train/chosen': '-124.81', 'loss/train': '0.66676', 'examples_per_second': '31.679', 'grad_norm': '19.875', 'counters/examples': 278048, 'counters/updates': 8689}
skipping logging after 278080 examples to avoid logging too frequently
train stats after 278112 examples: {'rewards_train/chosen': '0.31102', 'rewards_train/rejected': '-0.015053', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.32607', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-151.4', 'loss/train': '0.5833', 'examples_per_second': '31.926', 'grad_norm': '20.25', 'counters/examples': 278112, 'counters/updates': 8691}
train stats after 278144 examples: {'rewards_train/chosen': '0.13801', 'rewards_train/rejected': '0.037995', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10001', 'logps_train/rejected': '-115.03', 'logps_train/chosen': '-129.68', 'loss/train': '0.67323', 'examples_per_second': '31.464', 'grad_norm': '20.125', 'counters/examples': 278144, 'counters/updates': 8692}
train stats after 278176 examples: {'rewards_train/chosen': '0.2517', 'rewards_train/rejected': '0.1574', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094291', 'logps_train/rejected': '-121.39', 'logps_train/chosen': '-122.75', 'loss/train': '0.6623', 'examples_per_second': '32.783', 'grad_norm': '20.625', 'counters/examples': 278176, 'counters/updates': 8693}
train stats after 278208 examples: {'rewards_train/chosen': '0.25748', 'rewards_train/rejected': '-0.021451', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27893', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-162.38', 'loss/train': '0.58679', 'examples_per_second': '30.302', 'grad_norm': '20.875', 'counters/examples': 278208, 'counters/updates': 8694}
train stats after 278240 examples: {'rewards_train/chosen': '0.11184', 'rewards_train/rejected': '0.15211', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.040268', 'logps_train/rejected': '-128', 'logps_train/chosen': '-135.26', 'loss/train': '0.73178', 'examples_per_second': '31.112', 'grad_norm': '22.25', 'counters/examples': 278240, 'counters/updates': 8695}
train stats after 278272 examples: {'rewards_train/chosen': '0.16613', 'rewards_train/rejected': '0.10766', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058479', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-125.29', 'loss/train': '0.67777', 'examples_per_second': '31.647', 'grad_norm': '21.75', 'counters/examples': 278272, 'counters/updates': 8696}
train stats after 278304 examples: {'rewards_train/chosen': '0.23828', 'rewards_train/rejected': '0.078295', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15999', 'logps_train/rejected': '-92.73', 'logps_train/chosen': '-151.52', 'loss/train': '0.64551', 'examples_per_second': '31.067', 'grad_norm': '20.625', 'counters/examples': 278304, 'counters/updates': 8697}
train stats after 278336 examples: {'rewards_train/chosen': '0.075289', 'rewards_train/rejected': '0.021705', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053584', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-140.43', 'loss/train': '0.68309', 'examples_per_second': '32.233', 'grad_norm': '19.375', 'counters/examples': 278336, 'counters/updates': 8698}
train stats after 278368 examples: {'rewards_train/chosen': '0.16392', 'rewards_train/rejected': '0.071162', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092755', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-138.01', 'loss/train': '0.67204', 'examples_per_second': '31.484', 'grad_norm': '22.75', 'counters/examples': 278368, 'counters/updates': 8699}
train stats after 278400 examples: {'rewards_train/chosen': '0.20235', 'rewards_train/rejected': '0.0061433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19621', 'logps_train/rejected': '-140.32', 'logps_train/chosen': '-145.31', 'loss/train': '0.62876', 'examples_per_second': '31.673', 'grad_norm': '21.125', 'counters/examples': 278400, 'counters/updates': 8700}
train stats after 278432 examples: {'rewards_train/chosen': '0.13479', 'rewards_train/rejected': '-0.055346', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19014', 'logps_train/rejected': '-104.23', 'logps_train/chosen': '-137.8', 'loss/train': '0.62835', 'examples_per_second': '31.977', 'grad_norm': '18.875', 'counters/examples': 278432, 'counters/updates': 8701}
train stats after 278464 examples: {'rewards_train/chosen': '0.071194', 'rewards_train/rejected': '-0.049093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12029', 'logps_train/rejected': '-122.21', 'logps_train/chosen': '-155.25', 'loss/train': '0.64506', 'examples_per_second': '30.666', 'grad_norm': '22.375', 'counters/examples': 278464, 'counters/updates': 8702}
train stats after 278496 examples: {'rewards_train/chosen': '0.10367', 'rewards_train/rejected': '-0.041064', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14473', 'logps_train/rejected': '-115.76', 'logps_train/chosen': '-143.25', 'loss/train': '0.64896', 'examples_per_second': '32.349', 'grad_norm': '19.875', 'counters/examples': 278496, 'counters/updates': 8703}
train stats after 278528 examples: {'rewards_train/chosen': '0.12784', 'rewards_train/rejected': '0.15817', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030331', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-141.45', 'loss/train': '0.71967', 'examples_per_second': '30.631', 'grad_norm': '23.75', 'counters/examples': 278528, 'counters/updates': 8704}
train stats after 278560 examples: {'rewards_train/chosen': '0.17393', 'rewards_train/rejected': '0.034482', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.13945', 'logps_train/rejected': '-103.33', 'logps_train/chosen': '-116.91', 'loss/train': '0.6357', 'examples_per_second': '30.972', 'grad_norm': '17.5', 'counters/examples': 278560, 'counters/updates': 8705}
train stats after 278592 examples: {'rewards_train/chosen': '0.16179', 'rewards_train/rejected': '-0.019675', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18146', 'logps_train/rejected': '-116.41', 'logps_train/chosen': '-122.46', 'loss/train': '0.62081', 'examples_per_second': '31.016', 'grad_norm': '18.5', 'counters/examples': 278592, 'counters/updates': 8706}
train stats after 278624 examples: {'rewards_train/chosen': '0.13895', 'rewards_train/rejected': '0.029653', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1093', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-121.13', 'loss/train': '0.66938', 'examples_per_second': '32.977', 'grad_norm': '20', 'counters/examples': 278624, 'counters/updates': 8707}
train stats after 278656 examples: {'rewards_train/chosen': '0.18889', 'rewards_train/rejected': '0.05986', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12903', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-196.54', 'loss/train': '0.64815', 'examples_per_second': '31.8', 'grad_norm': '22.75', 'counters/examples': 278656, 'counters/updates': 8708}
train stats after 278688 examples: {'rewards_train/chosen': '0.20315', 'rewards_train/rejected': '0.086778', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11637', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-174.21', 'loss/train': '0.65393', 'examples_per_second': '30.131', 'grad_norm': '23.5', 'counters/examples': 278688, 'counters/updates': 8709}
skipping logging after 278720 examples to avoid logging too frequently
train stats after 278752 examples: {'rewards_train/chosen': '0.089634', 'rewards_train/rejected': '-0.020735', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11037', 'logps_train/rejected': '-122.65', 'logps_train/chosen': '-128.09', 'loss/train': '0.65202', 'examples_per_second': '31.572', 'grad_norm': '19.625', 'counters/examples': 278752, 'counters/updates': 8711}
train stats after 278784 examples: {'rewards_train/chosen': '0.18792', 'rewards_train/rejected': '0.038338', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14959', 'logps_train/rejected': '-124.71', 'logps_train/chosen': '-131.14', 'loss/train': '0.63828', 'examples_per_second': '30.641', 'grad_norm': '19.875', 'counters/examples': 278784, 'counters/updates': 8712}
train stats after 278816 examples: {'rewards_train/chosen': '0.1179', 'rewards_train/rejected': '-0.11808', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23598', 'logps_train/rejected': '-98.676', 'logps_train/chosen': '-112.6', 'loss/train': '0.60375', 'examples_per_second': '31.894', 'grad_norm': '17.25', 'counters/examples': 278816, 'counters/updates': 8713}
train stats after 278848 examples: {'rewards_train/chosen': '0.214', 'rewards_train/rejected': '0.069805', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1442', 'logps_train/rejected': '-138.21', 'logps_train/chosen': '-138.09', 'loss/train': '0.63872', 'examples_per_second': '31.219', 'grad_norm': '20.5', 'counters/examples': 278848, 'counters/updates': 8714}
skipping logging after 278880 examples to avoid logging too frequently
train stats after 278912 examples: {'rewards_train/chosen': '0.14493', 'rewards_train/rejected': '-0.01165', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15658', 'logps_train/rejected': '-107.86', 'logps_train/chosen': '-128.44', 'loss/train': '0.63807', 'examples_per_second': '32.385', 'grad_norm': '22.25', 'counters/examples': 278912, 'counters/updates': 8716}
train stats after 278944 examples: {'rewards_train/chosen': '0.1534', 'rewards_train/rejected': '0.030879', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12252', 'logps_train/rejected': '-149.4', 'logps_train/chosen': '-151.52', 'loss/train': '0.64603', 'examples_per_second': '23.824', 'grad_norm': '19.875', 'counters/examples': 278944, 'counters/updates': 8717}
skipping logging after 278976 examples to avoid logging too frequently
train stats after 279008 examples: {'rewards_train/chosen': '0.2867', 'rewards_train/rejected': '0.088703', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19799', 'logps_train/rejected': '-124.05', 'logps_train/chosen': '-126.51', 'loss/train': '0.626', 'examples_per_second': '31.629', 'grad_norm': '19.625', 'counters/examples': 279008, 'counters/updates': 8719}
train stats after 279040 examples: {'rewards_train/chosen': '0.2611', 'rewards_train/rejected': '0.0035986', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2575', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-132.41', 'loss/train': '0.59968', 'examples_per_second': '24.39', 'grad_norm': '18.25', 'counters/examples': 279040, 'counters/updates': 8720}
skipping logging after 279072 examples to avoid logging too frequently
train stats after 279104 examples: {'rewards_train/chosen': '0.18739', 'rewards_train/rejected': '0.044693', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1427', 'logps_train/rejected': '-152.09', 'logps_train/chosen': '-149.17', 'loss/train': '0.65708', 'examples_per_second': '32.39', 'grad_norm': '21.875', 'counters/examples': 279104, 'counters/updates': 8722}
train stats after 279136 examples: {'rewards_train/chosen': '0.20525', 'rewards_train/rejected': '0.018056', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1872', 'logps_train/rejected': '-160.52', 'logps_train/chosen': '-160.5', 'loss/train': '0.62826', 'examples_per_second': '31.542', 'grad_norm': '21.375', 'counters/examples': 279136, 'counters/updates': 8723}
train stats after 279168 examples: {'rewards_train/chosen': '0.16332', 'rewards_train/rejected': '0.072537', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.090785', 'logps_train/rejected': '-120.92', 'logps_train/chosen': '-151.67', 'loss/train': '0.66691', 'examples_per_second': '32.419', 'grad_norm': '20.75', 'counters/examples': 279168, 'counters/updates': 8724}
train stats after 279200 examples: {'rewards_train/chosen': '0.18341', 'rewards_train/rejected': '-0.019958', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20337', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-142.74', 'loss/train': '0.61151', 'examples_per_second': '30.011', 'grad_norm': '19.625', 'counters/examples': 279200, 'counters/updates': 8725}
train stats after 279232 examples: {'rewards_train/chosen': '0.26848', 'rewards_train/rejected': '0.059166', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20931', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-153.98', 'loss/train': '0.61608', 'examples_per_second': '31.706', 'grad_norm': '19.625', 'counters/examples': 279232, 'counters/updates': 8726}
train stats after 279264 examples: {'rewards_train/chosen': '0.21197', 'rewards_train/rejected': '-0.0046169', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21658', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-114.18', 'loss/train': '0.62135', 'examples_per_second': '31.195', 'grad_norm': '19.625', 'counters/examples': 279264, 'counters/updates': 8727}
train stats after 279296 examples: {'rewards_train/chosen': '0.061202', 'rewards_train/rejected': '0.041456', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.019746', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-129.72', 'loss/train': '0.70441', 'examples_per_second': '30.326', 'grad_norm': '20.125', 'counters/examples': 279296, 'counters/updates': 8728}
train stats after 279328 examples: {'rewards_train/chosen': '0.18886', 'rewards_train/rejected': '-0.030715', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21958', 'logps_train/rejected': '-89.693', 'logps_train/chosen': '-108.35', 'loss/train': '0.62654', 'examples_per_second': '32.814', 'grad_norm': '17.375', 'counters/examples': 279328, 'counters/updates': 8729}
train stats after 279360 examples: {'rewards_train/chosen': '0.18013', 'rewards_train/rejected': '0.18211', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0019826', 'logps_train/rejected': '-165.63', 'logps_train/chosen': '-160.89', 'loss/train': '0.71556', 'examples_per_second': '31.575', 'grad_norm': '25.375', 'counters/examples': 279360, 'counters/updates': 8730}
train stats after 279392 examples: {'rewards_train/chosen': '0.20426', 'rewards_train/rejected': '-0.023994', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22825', 'logps_train/rejected': '-136.15', 'logps_train/chosen': '-151.41', 'loss/train': '0.6088', 'examples_per_second': '31.59', 'grad_norm': '18.75', 'counters/examples': 279392, 'counters/updates': 8731}
skipping logging after 279424 examples to avoid logging too frequently
train stats after 279456 examples: {'rewards_train/chosen': '0.19244', 'rewards_train/rejected': '0.0050222', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18741', 'logps_train/rejected': '-105.22', 'logps_train/chosen': '-126.72', 'loss/train': '0.61817', 'examples_per_second': '30.323', 'grad_norm': '17.625', 'counters/examples': 279456, 'counters/updates': 8733}
train stats after 279488 examples: {'rewards_train/chosen': '0.13749', 'rewards_train/rejected': '0.054334', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083157', 'logps_train/rejected': '-134.6', 'logps_train/chosen': '-107.29', 'loss/train': '0.6766', 'examples_per_second': '31.759', 'grad_norm': '21.625', 'counters/examples': 279488, 'counters/updates': 8734}
train stats after 279520 examples: {'rewards_train/chosen': '0.20836', 'rewards_train/rejected': '0.027767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18059', 'logps_train/rejected': '-106.85', 'logps_train/chosen': '-112.51', 'loss/train': '0.63039', 'examples_per_second': '32.008', 'grad_norm': '18.125', 'counters/examples': 279520, 'counters/updates': 8735}
train stats after 279552 examples: {'rewards_train/chosen': '0.20198', 'rewards_train/rejected': '-0.023249', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22523', 'logps_train/rejected': '-117.11', 'logps_train/chosen': '-134.93', 'loss/train': '0.60014', 'examples_per_second': '32.132', 'grad_norm': '18.875', 'counters/examples': 279552, 'counters/updates': 8736}
train stats after 279584 examples: {'rewards_train/chosen': '0.13015', 'rewards_train/rejected': '0.10231', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027842', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-158.21', 'loss/train': '0.69244', 'examples_per_second': '31.71', 'grad_norm': '23.75', 'counters/examples': 279584, 'counters/updates': 8737}
train stats after 279616 examples: {'rewards_train/chosen': '0.17323', 'rewards_train/rejected': '0.094611', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078618', 'logps_train/rejected': '-140.05', 'logps_train/chosen': '-181.11', 'loss/train': '0.68454', 'examples_per_second': '31.838', 'grad_norm': '24.875', 'counters/examples': 279616, 'counters/updates': 8738}
train stats after 279648 examples: {'rewards_train/chosen': '0.15572', 'rewards_train/rejected': '0.14834', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0073764', 'logps_train/rejected': '-144.03', 'logps_train/chosen': '-153.55', 'loss/train': '0.70883', 'examples_per_second': '31.089', 'grad_norm': '23.25', 'counters/examples': 279648, 'counters/updates': 8739}
train stats after 279680 examples: {'rewards_train/chosen': '0.12562', 'rewards_train/rejected': '0.067353', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058264', 'logps_train/rejected': '-141.06', 'logps_train/chosen': '-126.71', 'loss/train': '0.68213', 'examples_per_second': '31.32', 'grad_norm': '20.75', 'counters/examples': 279680, 'counters/updates': 8740}
train stats after 279712 examples: {'rewards_train/chosen': '0.26216', 'rewards_train/rejected': '0.045949', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.21621', 'logps_train/rejected': '-99.843', 'logps_train/chosen': '-125.86', 'loss/train': '0.60042', 'examples_per_second': '31.52', 'grad_norm': '18', 'counters/examples': 279712, 'counters/updates': 8741}
train stats after 279744 examples: {'rewards_train/chosen': '0.2246', 'rewards_train/rejected': '0.11255', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11205', 'logps_train/rejected': '-125.47', 'logps_train/chosen': '-186.82', 'loss/train': '0.65377', 'examples_per_second': '31.699', 'grad_norm': '22.125', 'counters/examples': 279744, 'counters/updates': 8742}
skipping logging after 279776 examples to avoid logging too frequently
train stats after 279808 examples: {'rewards_train/chosen': '0.18541', 'rewards_train/rejected': '-0.018511', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20393', 'logps_train/rejected': '-119.82', 'logps_train/chosen': '-116.9', 'loss/train': '0.61426', 'examples_per_second': '30.273', 'grad_norm': '18.125', 'counters/examples': 279808, 'counters/updates': 8744}
skipping logging after 279840 examples to avoid logging too frequently
train stats after 279872 examples: {'rewards_train/chosen': '0.095526', 'rewards_train/rejected': '-0.098042', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19357', 'logps_train/rejected': '-85.99', 'logps_train/chosen': '-98.891', 'loss/train': '0.61698', 'examples_per_second': '31.379', 'grad_norm': '16.625', 'counters/examples': 279872, 'counters/updates': 8746}
skipping logging after 279904 examples to avoid logging too frequently
train stats after 279936 examples: {'rewards_train/chosen': '0.20985', 'rewards_train/rejected': '0.10725', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1026', 'logps_train/rejected': '-95.385', 'logps_train/chosen': '-125.44', 'loss/train': '0.65664', 'examples_per_second': '31.655', 'grad_norm': '17.5', 'counters/examples': 279936, 'counters/updates': 8748}
train stats after 279968 examples: {'rewards_train/chosen': '0.23134', 'rewards_train/rejected': '0.11875', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11259', 'logps_train/rejected': '-136.33', 'logps_train/chosen': '-162', 'loss/train': '0.65573', 'examples_per_second': '31.67', 'grad_norm': '20.75', 'counters/examples': 279968, 'counters/updates': 8749}
train stats after 280000 examples: {'rewards_train/chosen': '0.25794', 'rewards_train/rejected': '0.14413', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11381', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-129.26', 'loss/train': '0.64974', 'examples_per_second': '30.81', 'grad_norm': '19.125', 'counters/examples': 280000, 'counters/updates': 8750}
train stats after 280032 examples: {'rewards_train/chosen': '0.1822', 'rewards_train/rejected': '0.085008', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097193', 'logps_train/rejected': '-125.09', 'logps_train/chosen': '-105.2', 'loss/train': '0.6621', 'examples_per_second': '31.17', 'grad_norm': '19.375', 'counters/examples': 280032, 'counters/updates': 8751}
train stats after 280064 examples: {'rewards_train/chosen': '0.068222', 'rewards_train/rejected': '0.044202', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02402', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-133.44', 'loss/train': '0.69866', 'examples_per_second': '32.21', 'grad_norm': '20.875', 'counters/examples': 280064, 'counters/updates': 8752}
train stats after 280096 examples: {'rewards_train/chosen': '0.15185', 'rewards_train/rejected': '-0.048044', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19989', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-115.33', 'loss/train': '0.62454', 'examples_per_second': '31.697', 'grad_norm': '19.125', 'counters/examples': 280096, 'counters/updates': 8753}
train stats after 280128 examples: {'rewards_train/chosen': '0.21486', 'rewards_train/rejected': '0.15201', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062851', 'logps_train/rejected': '-145.72', 'logps_train/chosen': '-105.36', 'loss/train': '0.67207', 'examples_per_second': '31.695', 'grad_norm': '20', 'counters/examples': 280128, 'counters/updates': 8754}
train stats after 280160 examples: {'rewards_train/chosen': '0.15329', 'rewards_train/rejected': '-0.021525', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17482', 'logps_train/rejected': '-88.154', 'logps_train/chosen': '-120.34', 'loss/train': '0.62329', 'examples_per_second': '32.631', 'grad_norm': '16.375', 'counters/examples': 280160, 'counters/updates': 8755}
train stats after 280192 examples: {'rewards_train/chosen': '0.17353', 'rewards_train/rejected': '-0.0064187', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17995', 'logps_train/rejected': '-140.09', 'logps_train/chosen': '-154.73', 'loss/train': '0.62037', 'examples_per_second': '30.233', 'grad_norm': '21.125', 'counters/examples': 280192, 'counters/updates': 8756}
train stats after 280224 examples: {'rewards_train/chosen': '0.11801', 'rewards_train/rejected': '0.051499', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066508', 'logps_train/rejected': '-112.22', 'logps_train/chosen': '-120.02', 'loss/train': '0.67839', 'examples_per_second': '24.772', 'grad_norm': '19', 'counters/examples': 280224, 'counters/updates': 8757}
train stats after 280256 examples: {'rewards_train/chosen': '0.20415', 'rewards_train/rejected': '0.029047', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17511', 'logps_train/rejected': '-125.16', 'logps_train/chosen': '-155.38', 'loss/train': '0.62936', 'examples_per_second': '32.22', 'grad_norm': '19.375', 'counters/examples': 280256, 'counters/updates': 8758}
train stats after 280288 examples: {'rewards_train/chosen': '0.24658', 'rewards_train/rejected': '0.12236', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12423', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-184.55', 'loss/train': '0.65008', 'examples_per_second': '30.769', 'grad_norm': '22.125', 'counters/examples': 280288, 'counters/updates': 8759}
train stats after 280320 examples: {'rewards_train/chosen': '0.26639', 'rewards_train/rejected': '0.085065', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.18133', 'logps_train/rejected': '-160.33', 'logps_train/chosen': '-169.56', 'loss/train': '0.62861', 'examples_per_second': '31.005', 'grad_norm': '22.25', 'counters/examples': 280320, 'counters/updates': 8760}
train stats after 280352 examples: {'rewards_train/chosen': '0.20077', 'rewards_train/rejected': '0.17193', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028838', 'logps_train/rejected': '-141.82', 'logps_train/chosen': '-142.74', 'loss/train': '0.69752', 'examples_per_second': '30.848', 'grad_norm': '23.25', 'counters/examples': 280352, 'counters/updates': 8761}
train stats after 280384 examples: {'rewards_train/chosen': '0.21904', 'rewards_train/rejected': '-0.066706', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.28575', 'logps_train/rejected': '-98.51', 'logps_train/chosen': '-123.62', 'loss/train': '0.58175', 'examples_per_second': '30.645', 'grad_norm': '17.25', 'counters/examples': 280384, 'counters/updates': 8762}
train stats after 280416 examples: {'rewards_train/chosen': '0.16716', 'rewards_train/rejected': '0.15749', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0096675', 'logps_train/rejected': '-155.65', 'logps_train/chosen': '-150', 'loss/train': '0.70761', 'examples_per_second': '32.519', 'grad_norm': '23.375', 'counters/examples': 280416, 'counters/updates': 8763}
skipping logging after 280448 examples to avoid logging too frequently
train stats after 280480 examples: {'rewards_train/chosen': '0.20126', 'rewards_train/rejected': '0.093766', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10749', 'logps_train/rejected': '-101.42', 'logps_train/chosen': '-109.01', 'loss/train': '0.65673', 'examples_per_second': '36.037', 'grad_norm': '18.5', 'counters/examples': 280480, 'counters/updates': 8765}
skipping logging after 280512 examples to avoid logging too frequently
train stats after 280544 examples: {'rewards_train/chosen': '0.23537', 'rewards_train/rejected': '-0.01278', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24815', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-160.61', 'loss/train': '0.59674', 'examples_per_second': '32.648', 'grad_norm': '21', 'counters/examples': 280544, 'counters/updates': 8767}
train stats after 280576 examples: {'rewards_train/chosen': '0.25092', 'rewards_train/rejected': '0.071784', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17914', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-146.11', 'loss/train': '0.62529', 'examples_per_second': '30.106', 'grad_norm': '21.625', 'counters/examples': 280576, 'counters/updates': 8768}
train stats after 280608 examples: {'rewards_train/chosen': '0.15247', 'rewards_train/rejected': '0.059442', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093026', 'logps_train/rejected': '-137.73', 'logps_train/chosen': '-145.86', 'loss/train': '0.66554', 'examples_per_second': '31.647', 'grad_norm': '21.5', 'counters/examples': 280608, 'counters/updates': 8769}
train stats after 280640 examples: {'rewards_train/chosen': '0.22402', 'rewards_train/rejected': '0.047016', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.177', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-175.08', 'loss/train': '0.62764', 'examples_per_second': '30.773', 'grad_norm': '23.375', 'counters/examples': 280640, 'counters/updates': 8770}
train stats after 280672 examples: {'rewards_train/chosen': '0.10252', 'rewards_train/rejected': '0.059966', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042557', 'logps_train/rejected': '-150.09', 'logps_train/chosen': '-164.8', 'loss/train': '0.6957', 'examples_per_second': '30.032', 'grad_norm': '24.25', 'counters/examples': 280672, 'counters/updates': 8771}
train stats after 280704 examples: {'rewards_train/chosen': '0.26922', 'rewards_train/rejected': '0.034151', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23507', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-139.15', 'loss/train': '0.61588', 'examples_per_second': '32.911', 'grad_norm': '21.5', 'counters/examples': 280704, 'counters/updates': 8772}
train stats after 280736 examples: {'rewards_train/chosen': '0.331', 'rewards_train/rejected': '0.040051', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29095', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-145.44', 'loss/train': '0.58354', 'examples_per_second': '32.518', 'grad_norm': '20.125', 'counters/examples': 280736, 'counters/updates': 8773}
train stats after 280768 examples: {'rewards_train/chosen': '0.1371', 'rewards_train/rejected': '-0.014116', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15122', 'logps_train/rejected': '-96.687', 'logps_train/chosen': '-118.09', 'loss/train': '0.64266', 'examples_per_second': '32.914', 'grad_norm': '18.125', 'counters/examples': 280768, 'counters/updates': 8774}
skipping logging after 280800 examples to avoid logging too frequently
train stats after 280832 examples: {'rewards_train/chosen': '0.06418', 'rewards_train/rejected': '-0.0035501', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06773', 'logps_train/rejected': '-92.17', 'logps_train/chosen': '-114.36', 'loss/train': '0.67383', 'examples_per_second': '31.08', 'grad_norm': '18.75', 'counters/examples': 280832, 'counters/updates': 8776}
train stats after 280864 examples: {'rewards_train/chosen': '0.17947', 'rewards_train/rejected': '0.1075', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071971', 'logps_train/rejected': '-128.85', 'logps_train/chosen': '-94.02', 'loss/train': '0.67845', 'examples_per_second': '32.153', 'grad_norm': '18.375', 'counters/examples': 280864, 'counters/updates': 8777}
train stats after 280896 examples: {'rewards_train/chosen': '0.14988', 'rewards_train/rejected': '-0.0040187', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1539', 'logps_train/rejected': '-106.65', 'logps_train/chosen': '-157.96', 'loss/train': '0.63162', 'examples_per_second': '31.605', 'grad_norm': '20.25', 'counters/examples': 280896, 'counters/updates': 8778}
train stats after 280928 examples: {'rewards_train/chosen': '0.084175', 'rewards_train/rejected': '0.035062', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049113', 'logps_train/rejected': '-97.941', 'logps_train/chosen': '-104.1', 'loss/train': '0.68396', 'examples_per_second': '30.774', 'grad_norm': '17.75', 'counters/examples': 280928, 'counters/updates': 8779}
skipping logging after 280960 examples to avoid logging too frequently
train stats after 280992 examples: {'rewards_train/chosen': '0.1545', 'rewards_train/rejected': '-0.047978', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20248', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-132.72', 'loss/train': '0.62047', 'examples_per_second': '30.294', 'grad_norm': '18.375', 'counters/examples': 280992, 'counters/updates': 8781}
train stats after 281024 examples: {'rewards_train/chosen': '0.20672', 'rewards_train/rejected': '0.028401', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17831', 'logps_train/rejected': '-117.31', 'logps_train/chosen': '-120.36', 'loss/train': '0.62988', 'examples_per_second': '31.018', 'grad_norm': '18.625', 'counters/examples': 281024, 'counters/updates': 8782}
train stats after 281056 examples: {'rewards_train/chosen': '0.23377', 'rewards_train/rejected': '0.068044', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16573', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-146.86', 'loss/train': '0.62403', 'examples_per_second': '32.868', 'grad_norm': '20.375', 'counters/examples': 281056, 'counters/updates': 8783}
train stats after 281088 examples: {'rewards_train/chosen': '0.24902', 'rewards_train/rejected': '0.043033', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20598', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-179.06', 'loss/train': '0.61243', 'examples_per_second': '31.654', 'grad_norm': '20.75', 'counters/examples': 281088, 'counters/updates': 8784}
train stats after 281120 examples: {'rewards_train/chosen': '0.18986', 'rewards_train/rejected': '0.014588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17527', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-144.27', 'loss/train': '0.62137', 'examples_per_second': '31.073', 'grad_norm': '19', 'counters/examples': 281120, 'counters/updates': 8785}
train stats after 281152 examples: {'rewards_train/chosen': '0.25438', 'rewards_train/rejected': '0.10661', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14777', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-121.05', 'loss/train': '0.63773', 'examples_per_second': '30.707', 'grad_norm': '19', 'counters/examples': 281152, 'counters/updates': 8786}
train stats after 281184 examples: {'rewards_train/chosen': '0.11453', 'rewards_train/rejected': '0.010315', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10422', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-155.43', 'loss/train': '0.65993', 'examples_per_second': '30.766', 'grad_norm': '21.875', 'counters/examples': 281184, 'counters/updates': 8787}
train stats after 281216 examples: {'rewards_train/chosen': '0.26752', 'rewards_train/rejected': '0.01935', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24817', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-134.07', 'loss/train': '0.60463', 'examples_per_second': '29.921', 'grad_norm': '19.25', 'counters/examples': 281216, 'counters/updates': 8788}
skipping logging after 281248 examples to avoid logging too frequently
train stats after 281280 examples: {'rewards_train/chosen': '0.18112', 'rewards_train/rejected': '0.04311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13801', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-143.63', 'loss/train': '0.65096', 'examples_per_second': '33.882', 'grad_norm': '21.25', 'counters/examples': 281280, 'counters/updates': 8790}
train stats after 281312 examples: {'rewards_train/chosen': '0.21565', 'rewards_train/rejected': '0.053734', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16191', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-158.9', 'loss/train': '0.63277', 'examples_per_second': '32.885', 'grad_norm': '21.625', 'counters/examples': 281312, 'counters/updates': 8791}
train stats after 281344 examples: {'rewards_train/chosen': '0.18106', 'rewards_train/rejected': '0.056074', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12499', 'logps_train/rejected': '-125.6', 'logps_train/chosen': '-148.71', 'loss/train': '0.65106', 'examples_per_second': '30.142', 'grad_norm': '20', 'counters/examples': 281344, 'counters/updates': 8792}
train stats after 281376 examples: {'rewards_train/chosen': '0.2289', 'rewards_train/rejected': '0.08958', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13932', 'logps_train/rejected': '-130.18', 'logps_train/chosen': '-172.87', 'loss/train': '0.64024', 'examples_per_second': '30.065', 'grad_norm': '23.5', 'counters/examples': 281376, 'counters/updates': 8793}
skipping logging after 281408 examples to avoid logging too frequently
train stats after 281440 examples: {'rewards_train/chosen': '0.20108', 'rewards_train/rejected': '-0.029557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.23064', 'logps_train/rejected': '-117.92', 'logps_train/chosen': '-132.56', 'loss/train': '0.62175', 'examples_per_second': '35.705', 'grad_norm': '18.875', 'counters/examples': 281440, 'counters/updates': 8795}
train stats after 281472 examples: {'rewards_train/chosen': '0.24852', 'rewards_train/rejected': '0.0047765', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24375', 'logps_train/rejected': '-166.41', 'logps_train/chosen': '-150.26', 'loss/train': '0.61892', 'examples_per_second': '30.65', 'grad_norm': '22.125', 'counters/examples': 281472, 'counters/updates': 8796}
train stats after 281504 examples: {'rewards_train/chosen': '0.17099', 'rewards_train/rejected': '-0.041003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.212', 'logps_train/rejected': '-115.07', 'logps_train/chosen': '-137.9', 'loss/train': '0.61245', 'examples_per_second': '31.481', 'grad_norm': '19.5', 'counters/examples': 281504, 'counters/updates': 8797}
skipping logging after 281536 examples to avoid logging too frequently
train stats after 281568 examples: {'rewards_train/chosen': '0.16648', 'rewards_train/rejected': '0.095885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070591', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-156.17', 'loss/train': '0.67598', 'examples_per_second': '32.715', 'grad_norm': '20.875', 'counters/examples': 281568, 'counters/updates': 8799}
train stats after 281600 examples: {'rewards_train/chosen': '0.16185', 'rewards_train/rejected': '0.006483', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15537', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-135.97', 'loss/train': '0.63418', 'examples_per_second': '32.235', 'grad_norm': '20.5', 'counters/examples': 281600, 'counters/updates': 8800}
train stats after 281632 examples: {'rewards_train/chosen': '0.19958', 'rewards_train/rejected': '0.023202', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17638', 'logps_train/rejected': '-159.42', 'logps_train/chosen': '-198.24', 'loss/train': '0.62933', 'examples_per_second': '32.003', 'grad_norm': '24.25', 'counters/examples': 281632, 'counters/updates': 8801}
skipping logging after 281664 examples to avoid logging too frequently
train stats after 281696 examples: {'rewards_train/chosen': '0.20946', 'rewards_train/rejected': '0.063553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14591', 'logps_train/rejected': '-111.3', 'logps_train/chosen': '-144.2', 'loss/train': '0.64498', 'examples_per_second': '31.597', 'grad_norm': '18.75', 'counters/examples': 281696, 'counters/updates': 8803}
train stats after 281728 examples: {'rewards_train/chosen': '0.27537', 'rewards_train/rejected': '0.12602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14935', 'logps_train/rejected': '-115.26', 'logps_train/chosen': '-140', 'loss/train': '0.63979', 'examples_per_second': '30.134', 'grad_norm': '21', 'counters/examples': 281728, 'counters/updates': 8804}
train stats after 281760 examples: {'rewards_train/chosen': '0.14107', 'rewards_train/rejected': '-0.0085316', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1496', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-108.19', 'loss/train': '0.63527', 'examples_per_second': '30.641', 'grad_norm': '18.375', 'counters/examples': 281760, 'counters/updates': 8805}
train stats after 281792 examples: {'rewards_train/chosen': '-0.0042059', 'rewards_train/rejected': '-0.079812', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075606', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-145.82', 'loss/train': '0.68621', 'examples_per_second': '31.629', 'grad_norm': '20.875', 'counters/examples': 281792, 'counters/updates': 8806}
train stats after 281824 examples: {'rewards_train/chosen': '0.18509', 'rewards_train/rejected': '0.14169', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043398', 'logps_train/rejected': '-111.95', 'logps_train/chosen': '-145.97', 'loss/train': '0.69598', 'examples_per_second': '31.971', 'grad_norm': '21.875', 'counters/examples': 281824, 'counters/updates': 8807}
train stats after 281856 examples: {'rewards_train/chosen': '0.20671', 'rewards_train/rejected': '0.059504', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14721', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-156.09', 'loss/train': '0.63391', 'examples_per_second': '31.574', 'grad_norm': '20.875', 'counters/examples': 281856, 'counters/updates': 8808}
train stats after 281888 examples: {'rewards_train/chosen': '0.12648', 'rewards_train/rejected': '-0.038638', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16512', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-136.13', 'loss/train': '0.63525', 'examples_per_second': '32.161', 'grad_norm': '19.5', 'counters/examples': 281888, 'counters/updates': 8809}
train stats after 281920 examples: {'rewards_train/chosen': '0.27988', 'rewards_train/rejected': '0.10076', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17912', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-153.22', 'loss/train': '0.62798', 'examples_per_second': '31.644', 'grad_norm': '20.5', 'counters/examples': 281920, 'counters/updates': 8810}
train stats after 281952 examples: {'rewards_train/chosen': '0.24544', 'rewards_train/rejected': '-0.00015107', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24559', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-108.72', 'loss/train': '0.60035', 'examples_per_second': '32.567', 'grad_norm': '16.75', 'counters/examples': 281952, 'counters/updates': 8811}
train stats after 281984 examples: {'rewards_train/chosen': '0.14974', 'rewards_train/rejected': '0.076969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072766', 'logps_train/rejected': '-104.19', 'logps_train/chosen': '-127.17', 'loss/train': '0.6766', 'examples_per_second': '30.713', 'grad_norm': '19.5', 'counters/examples': 281984, 'counters/updates': 8812}
skipping logging after 282016 examples to avoid logging too frequently
train stats after 282048 examples: {'rewards_train/chosen': '0.30797', 'rewards_train/rejected': '0.033371', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2746', 'logps_train/rejected': '-100.35', 'logps_train/chosen': '-137.62', 'loss/train': '0.58217', 'examples_per_second': '30.359', 'grad_norm': '17.75', 'counters/examples': 282048, 'counters/updates': 8814}
train stats after 282080 examples: {'rewards_train/chosen': '0.28288', 'rewards_train/rejected': '0.040719', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24216', 'logps_train/rejected': '-114.55', 'logps_train/chosen': '-124.91', 'loss/train': '0.59818', 'examples_per_second': '31.648', 'grad_norm': '17.75', 'counters/examples': 282080, 'counters/updates': 8815}
train stats after 282112 examples: {'rewards_train/chosen': '0.22254', 'rewards_train/rejected': '-0.050081', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.27262', 'logps_train/rejected': '-101.79', 'logps_train/chosen': '-144.56', 'loss/train': '0.58831', 'examples_per_second': '30.709', 'grad_norm': '18.5', 'counters/examples': 282112, 'counters/updates': 8816}
train stats after 282144 examples: {'rewards_train/chosen': '0.30186', 'rewards_train/rejected': '-0.023857', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.32571', 'logps_train/rejected': '-110.02', 'logps_train/chosen': '-158.29', 'loss/train': '0.57567', 'examples_per_second': '31.632', 'grad_norm': '18.75', 'counters/examples': 282144, 'counters/updates': 8817}
train stats after 282176 examples: {'rewards_train/chosen': '0.21798', 'rewards_train/rejected': '0.076426', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14155', 'logps_train/rejected': '-125.56', 'logps_train/chosen': '-139.81', 'loss/train': '0.63654', 'examples_per_second': '31.576', 'grad_norm': '20', 'counters/examples': 282176, 'counters/updates': 8818}
train stats after 282208 examples: {'rewards_train/chosen': '0.17292', 'rewards_train/rejected': '0.020098', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15283', 'logps_train/rejected': '-113.44', 'logps_train/chosen': '-97.93', 'loss/train': '0.63875', 'examples_per_second': '31.463', 'grad_norm': '17.875', 'counters/examples': 282208, 'counters/updates': 8819}
skipping logging after 282240 examples to avoid logging too frequently
train stats after 282272 examples: {'rewards_train/chosen': '0.15039', 'rewards_train/rejected': '-0.0027528', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15315', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-134.16', 'loss/train': '0.63573', 'examples_per_second': '31.679', 'grad_norm': '19.875', 'counters/examples': 282272, 'counters/updates': 8821}
skipping logging after 282304 examples to avoid logging too frequently
train stats after 282336 examples: {'rewards_train/chosen': '0.2974', 'rewards_train/rejected': '0.04244', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25496', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-143.19', 'loss/train': '0.59742', 'examples_per_second': '31.707', 'grad_norm': '18.75', 'counters/examples': 282336, 'counters/updates': 8823}
skipping logging after 282368 examples to avoid logging too frequently
train stats after 282400 examples: {'rewards_train/chosen': '0.17104', 'rewards_train/rejected': '-0.015835', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18688', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-114.27', 'loss/train': '0.62102', 'examples_per_second': '30.379', 'grad_norm': '18', 'counters/examples': 282400, 'counters/updates': 8825}
train stats after 282432 examples: {'rewards_train/chosen': '0.18879', 'rewards_train/rejected': '0.011766', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17702', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-147.44', 'loss/train': '0.62832', 'examples_per_second': '32.616', 'grad_norm': '20.375', 'counters/examples': 282432, 'counters/updates': 8826}
train stats after 282464 examples: {'rewards_train/chosen': '0.1242', 'rewards_train/rejected': '-0.014939', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13914', 'logps_train/rejected': '-109.4', 'logps_train/chosen': '-139.71', 'loss/train': '0.64609', 'examples_per_second': '31.021', 'grad_norm': '20.25', 'counters/examples': 282464, 'counters/updates': 8827}
train stats after 282496 examples: {'rewards_train/chosen': '0.18764', 'rewards_train/rejected': '0.083962', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10367', 'logps_train/rejected': '-130.17', 'logps_train/chosen': '-163.35', 'loss/train': '0.65769', 'examples_per_second': '30.658', 'grad_norm': '21.625', 'counters/examples': 282496, 'counters/updates': 8828}
skipping logging after 282528 examples to avoid logging too frequently
train stats after 282560 examples: {'rewards_train/chosen': '0.19369', 'rewards_train/rejected': '-0.062959', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25665', 'logps_train/rejected': '-133.74', 'logps_train/chosen': '-167.92', 'loss/train': '0.59383', 'examples_per_second': '31.204', 'grad_norm': '20.375', 'counters/examples': 282560, 'counters/updates': 8830}
train stats after 282592 examples: {'rewards_train/chosen': '0.14459', 'rewards_train/rejected': '0.012645', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13195', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-167.9', 'loss/train': '0.65552', 'examples_per_second': '30.5', 'grad_norm': '22', 'counters/examples': 282592, 'counters/updates': 8831}
train stats after 282624 examples: {'rewards_train/chosen': '0.22134', 'rewards_train/rejected': '0.20244', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018897', 'logps_train/rejected': '-163.63', 'logps_train/chosen': '-140.37', 'loss/train': '0.70132', 'examples_per_second': '30.581', 'grad_norm': '24', 'counters/examples': 282624, 'counters/updates': 8832}
train stats after 282656 examples: {'rewards_train/chosen': '0.19782', 'rewards_train/rejected': '-0.090276', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2881', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-157.12', 'loss/train': '0.58413', 'examples_per_second': '31.648', 'grad_norm': '20.875', 'counters/examples': 282656, 'counters/updates': 8833}
train stats after 282688 examples: {'rewards_train/chosen': '0.19965', 'rewards_train/rejected': '-0.047812', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24746', 'logps_train/rejected': '-103.89', 'logps_train/chosen': '-123.95', 'loss/train': '0.60344', 'examples_per_second': '30.264', 'grad_norm': '19.125', 'counters/examples': 282688, 'counters/updates': 8834}
skipping logging after 282720 examples to avoid logging too frequently
train stats after 282752 examples: {'rewards_train/chosen': '0.21914', 'rewards_train/rejected': '0.02169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19745', 'logps_train/rejected': '-159.69', 'logps_train/chosen': '-155.6', 'loss/train': '0.61602', 'examples_per_second': '30.651', 'grad_norm': '22.5', 'counters/examples': 282752, 'counters/updates': 8836}
skipping logging after 282784 examples to avoid logging too frequently
train stats after 282816 examples: {'rewards_train/chosen': '0.10968', 'rewards_train/rejected': '-0.0068377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11652', 'logps_train/rejected': '-111.02', 'logps_train/chosen': '-155.38', 'loss/train': '0.65475', 'examples_per_second': '31.895', 'grad_norm': '21', 'counters/examples': 282816, 'counters/updates': 8838}
train stats after 282848 examples: {'rewards_train/chosen': '0.22268', 'rewards_train/rejected': '0.085074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1376', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-146.6', 'loss/train': '0.63973', 'examples_per_second': '32.536', 'grad_norm': '19.25', 'counters/examples': 282848, 'counters/updates': 8839}
train stats after 282880 examples: {'rewards_train/chosen': '0.075548', 'rewards_train/rejected': '0.011016', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064532', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-180.43', 'loss/train': '0.67778', 'examples_per_second': '31.676', 'grad_norm': '23.5', 'counters/examples': 282880, 'counters/updates': 8840}
train stats after 282912 examples: {'rewards_train/chosen': '0.15032', 'rewards_train/rejected': '0.040924', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1094', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-157.69', 'loss/train': '0.67066', 'examples_per_second': '31.076', 'grad_norm': '21.5', 'counters/examples': 282912, 'counters/updates': 8841}
train stats after 282944 examples: {'rewards_train/chosen': '0.18561', 'rewards_train/rejected': '0.064099', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12151', 'logps_train/rejected': '-107.93', 'logps_train/chosen': '-121.23', 'loss/train': '0.64422', 'examples_per_second': '30.18', 'grad_norm': '18.875', 'counters/examples': 282944, 'counters/updates': 8842}
train stats after 282976 examples: {'rewards_train/chosen': '0.27811', 'rewards_train/rejected': '0.059213', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2189', 'logps_train/rejected': '-121.25', 'logps_train/chosen': '-144.3', 'loss/train': '0.60951', 'examples_per_second': '32.565', 'grad_norm': '18.75', 'counters/examples': 282976, 'counters/updates': 8843}
train stats after 283008 examples: {'rewards_train/chosen': '0.16015', 'rewards_train/rejected': '0.014727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14542', 'logps_train/rejected': '-123.57', 'logps_train/chosen': '-131.69', 'loss/train': '0.64869', 'examples_per_second': '33.007', 'grad_norm': '20.625', 'counters/examples': 283008, 'counters/updates': 8844}
train stats after 283040 examples: {'rewards_train/chosen': '0.20622', 'rewards_train/rejected': '-0.0070548', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21327', 'logps_train/rejected': '-110.05', 'logps_train/chosen': '-126.46', 'loss/train': '0.61302', 'examples_per_second': '32.582', 'grad_norm': '17.875', 'counters/examples': 283040, 'counters/updates': 8845}
train stats after 283072 examples: {'rewards_train/chosen': '0.11136', 'rewards_train/rejected': '-0.033356', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14471', 'logps_train/rejected': '-78.375', 'logps_train/chosen': '-97.358', 'loss/train': '0.63941', 'examples_per_second': '31.161', 'grad_norm': '16.5', 'counters/examples': 283072, 'counters/updates': 8846}
train stats after 283104 examples: {'rewards_train/chosen': '0.22514', 'rewards_train/rejected': '0.10908', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11606', 'logps_train/rejected': '-151.09', 'logps_train/chosen': '-131.97', 'loss/train': '0.6586', 'examples_per_second': '31.54', 'grad_norm': '19.5', 'counters/examples': 283104, 'counters/updates': 8847}
train stats after 283136 examples: {'rewards_train/chosen': '0.12525', 'rewards_train/rejected': '-0.0079751', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13322', 'logps_train/rejected': '-106.79', 'logps_train/chosen': '-146.85', 'loss/train': '0.64649', 'examples_per_second': '30.693', 'grad_norm': '20.25', 'counters/examples': 283136, 'counters/updates': 8848}
train stats after 283168 examples: {'rewards_train/chosen': '0.22824', 'rewards_train/rejected': '0.08762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14062', 'logps_train/rejected': '-134.5', 'logps_train/chosen': '-135.68', 'loss/train': '0.64074', 'examples_per_second': '31.983', 'grad_norm': '21.375', 'counters/examples': 283168, 'counters/updates': 8849}
train stats after 283200 examples: {'rewards_train/chosen': '0.13041', 'rewards_train/rejected': '-0.050217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18063', 'logps_train/rejected': '-96.234', 'logps_train/chosen': '-150.09', 'loss/train': '0.62414', 'examples_per_second': '31.704', 'grad_norm': '18.25', 'counters/examples': 283200, 'counters/updates': 8850}
train stats after 283232 examples: {'rewards_train/chosen': '0.18027', 'rewards_train/rejected': '0.0047277', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17555', 'logps_train/rejected': '-126.79', 'logps_train/chosen': '-148.81', 'loss/train': '0.62554', 'examples_per_second': '32.35', 'grad_norm': '19.125', 'counters/examples': 283232, 'counters/updates': 8851}
train stats after 283264 examples: {'rewards_train/chosen': '0.27517', 'rewards_train/rejected': '0.088804', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18637', 'logps_train/rejected': '-146.13', 'logps_train/chosen': '-146.64', 'loss/train': '0.64235', 'examples_per_second': '31.292', 'grad_norm': '22.875', 'counters/examples': 283264, 'counters/updates': 8852}
train stats after 283296 examples: {'rewards_train/chosen': '0.20436', 'rewards_train/rejected': '-0.011043', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2154', 'logps_train/rejected': '-110.66', 'logps_train/chosen': '-139.52', 'loss/train': '0.61146', 'examples_per_second': '30.025', 'grad_norm': '18.75', 'counters/examples': 283296, 'counters/updates': 8853}
train stats after 283328 examples: {'rewards_train/chosen': '0.30383', 'rewards_train/rejected': '0.071107', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23272', 'logps_train/rejected': '-98.428', 'logps_train/chosen': '-120.74', 'loss/train': '0.60195', 'examples_per_second': '31.128', 'grad_norm': '18.375', 'counters/examples': 283328, 'counters/updates': 8854}
train stats after 283360 examples: {'rewards_train/chosen': '0.13774', 'rewards_train/rejected': '0.057034', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080702', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-132.81', 'loss/train': '0.68332', 'examples_per_second': '30.737', 'grad_norm': '21.25', 'counters/examples': 283360, 'counters/updates': 8855}
skipping logging after 283392 examples to avoid logging too frequently
train stats after 283424 examples: {'rewards_train/chosen': '0.30669', 'rewards_train/rejected': '0.11088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19581', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-136.54', 'loss/train': '0.62372', 'examples_per_second': '30.975', 'grad_norm': '19.125', 'counters/examples': 283424, 'counters/updates': 8857}
skipping logging after 283456 examples to avoid logging too frequently
train stats after 283488 examples: {'rewards_train/chosen': '0.16384', 'rewards_train/rejected': '-0.010326', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17417', 'logps_train/rejected': '-83.191', 'logps_train/chosen': '-120.51', 'loss/train': '0.63039', 'examples_per_second': '30.102', 'grad_norm': '16.75', 'counters/examples': 283488, 'counters/updates': 8859}
train stats after 283520 examples: {'rewards_train/chosen': '0.18391', 'rewards_train/rejected': '0.055544', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12837', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-167.77', 'loss/train': '0.64575', 'examples_per_second': '31.511', 'grad_norm': '24.375', 'counters/examples': 283520, 'counters/updates': 8860}
train stats after 283552 examples: {'rewards_train/chosen': '0.30062', 'rewards_train/rejected': '0.072407', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.22821', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-133.16', 'loss/train': '0.61232', 'examples_per_second': '31.464', 'grad_norm': '19', 'counters/examples': 283552, 'counters/updates': 8861}
train stats after 283584 examples: {'rewards_train/chosen': '0.16686', 'rewards_train/rejected': '0.042416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12444', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-143.85', 'loss/train': '0.64421', 'examples_per_second': '32.517', 'grad_norm': '20.5', 'counters/examples': 283584, 'counters/updates': 8862}
train stats after 283616 examples: {'rewards_train/chosen': '0.23955', 'rewards_train/rejected': '-0.02249', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26204', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-138.8', 'loss/train': '0.59228', 'examples_per_second': '32.381', 'grad_norm': '18.75', 'counters/examples': 283616, 'counters/updates': 8863}
train stats after 283648 examples: {'rewards_train/chosen': '0.19838', 'rewards_train/rejected': '0.050182', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1482', 'logps_train/rejected': '-89.276', 'logps_train/chosen': '-131.02', 'loss/train': '0.63703', 'examples_per_second': '32.777', 'grad_norm': '17.125', 'counters/examples': 283648, 'counters/updates': 8864}
train stats after 283680 examples: {'rewards_train/chosen': '0.14083', 'rewards_train/rejected': '0.11656', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024276', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-121.61', 'loss/train': '0.69903', 'examples_per_second': '30.492', 'grad_norm': '20.125', 'counters/examples': 283680, 'counters/updates': 8865}
train stats after 283712 examples: {'rewards_train/chosen': '0.24959', 'rewards_train/rejected': '0.053194', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19639', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-132.86', 'loss/train': '0.61755', 'examples_per_second': '31.619', 'grad_norm': '17.875', 'counters/examples': 283712, 'counters/updates': 8866}
skipping logging after 283744 examples to avoid logging too frequently
train stats after 283776 examples: {'rewards_train/chosen': '0.073243', 'rewards_train/rejected': '0.04402', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029223', 'logps_train/rejected': '-101.17', 'logps_train/chosen': '-107.97', 'loss/train': '0.69843', 'examples_per_second': '31.091', 'grad_norm': '20.75', 'counters/examples': 283776, 'counters/updates': 8868}
train stats after 283808 examples: {'rewards_train/chosen': '0.24182', 'rewards_train/rejected': '0.1092', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13262', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-135.99', 'loss/train': '0.64988', 'examples_per_second': '31.077', 'grad_norm': '19.875', 'counters/examples': 283808, 'counters/updates': 8869}
train stats after 283840 examples: {'rewards_train/chosen': '0.33022', 'rewards_train/rejected': '-0.016884', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.3471', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-147.26', 'loss/train': '0.5672', 'examples_per_second': '30.369', 'grad_norm': '20.125', 'counters/examples': 283840, 'counters/updates': 8870}
train stats after 283872 examples: {'rewards_train/chosen': '0.15047', 'rewards_train/rejected': '-0.024413', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17488', 'logps_train/rejected': '-78.25', 'logps_train/chosen': '-114.32', 'loss/train': '0.62992', 'examples_per_second': '31.658', 'grad_norm': '17.375', 'counters/examples': 283872, 'counters/updates': 8871}
train stats after 283904 examples: {'rewards_train/chosen': '0.21059', 'rewards_train/rejected': '0.0062163', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20437', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-153.34', 'loss/train': '0.62561', 'examples_per_second': '31.767', 'grad_norm': '19.375', 'counters/examples': 283904, 'counters/updates': 8872}
skipping logging after 283936 examples to avoid logging too frequently
train stats after 283968 examples: {'rewards_train/chosen': '0.14541', 'rewards_train/rejected': '-0.0081341', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15355', 'logps_train/rejected': '-100.07', 'logps_train/chosen': '-149.89', 'loss/train': '0.66523', 'examples_per_second': '30.715', 'grad_norm': '19.125', 'counters/examples': 283968, 'counters/updates': 8874}
train stats after 284000 examples: {'rewards_train/chosen': '0.2613', 'rewards_train/rejected': '-0.0018125', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26311', 'logps_train/rejected': '-101.99', 'logps_train/chosen': '-132.28', 'loss/train': '0.59511', 'examples_per_second': '31.654', 'grad_norm': '18.5', 'counters/examples': 284000, 'counters/updates': 8875}
skipping logging after 284032 examples to avoid logging too frequently
train stats after 284064 examples: {'rewards_train/chosen': '0.087293', 'rewards_train/rejected': '0.0036329', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08366', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-130.68', 'loss/train': '0.68013', 'examples_per_second': '31.523', 'grad_norm': '20.5', 'counters/examples': 284064, 'counters/updates': 8877}
train stats after 284096 examples: {'rewards_train/chosen': '0.24864', 'rewards_train/rejected': '0.081242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1674', 'logps_train/rejected': '-153.43', 'logps_train/chosen': '-139.93', 'loss/train': '0.63238', 'examples_per_second': '32.335', 'grad_norm': '20.75', 'counters/examples': 284096, 'counters/updates': 8878}
train stats after 284128 examples: {'rewards_train/chosen': '0.12862', 'rewards_train/rejected': '-0.011737', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14036', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-121.07', 'loss/train': '0.6457', 'examples_per_second': '31.804', 'grad_norm': '20.5', 'counters/examples': 284128, 'counters/updates': 8879}
train stats after 284160 examples: {'rewards_train/chosen': '0.18099', 'rewards_train/rejected': '0.0087254', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17226', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-147.01', 'loss/train': '0.62837', 'examples_per_second': '31.846', 'grad_norm': '20.75', 'counters/examples': 284160, 'counters/updates': 8880}
train stats after 284192 examples: {'rewards_train/chosen': '0.13979', 'rewards_train/rejected': '-0.032862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17265', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-124.08', 'loss/train': '0.62872', 'examples_per_second': '31.096', 'grad_norm': '18.5', 'counters/examples': 284192, 'counters/updates': 8881}
train stats after 284224 examples: {'rewards_train/chosen': '0.092593', 'rewards_train/rejected': '0.095445', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0028519', 'logps_train/rejected': '-158.4', 'logps_train/chosen': '-115.81', 'loss/train': '0.70866', 'examples_per_second': '31.639', 'grad_norm': '23', 'counters/examples': 284224, 'counters/updates': 8882}
train stats after 284256 examples: {'rewards_train/chosen': '0.16637', 'rewards_train/rejected': '-0.0079437', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17431', 'logps_train/rejected': '-80.85', 'logps_train/chosen': '-103.33', 'loss/train': '0.64262', 'examples_per_second': '30.117', 'grad_norm': '17.625', 'counters/examples': 284256, 'counters/updates': 8883}
train stats after 284288 examples: {'rewards_train/chosen': '0.070565', 'rewards_train/rejected': '-0.00028556', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070851', 'logps_train/rejected': '-106.6', 'logps_train/chosen': '-164.11', 'loss/train': '0.68172', 'examples_per_second': '30.754', 'grad_norm': '22.25', 'counters/examples': 284288, 'counters/updates': 8884}
train stats after 284320 examples: {'rewards_train/chosen': '0.34743', 'rewards_train/rejected': '0.098787', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24864', 'logps_train/rejected': '-102.56', 'logps_train/chosen': '-117.94', 'loss/train': '0.59911', 'examples_per_second': '31.676', 'grad_norm': '17.125', 'counters/examples': 284320, 'counters/updates': 8885}
train stats after 284352 examples: {'rewards_train/chosen': '0.20584', 'rewards_train/rejected': '0.18453', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021307', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-123.93', 'loss/train': '0.6925', 'examples_per_second': '30.828', 'grad_norm': '21.375', 'counters/examples': 284352, 'counters/updates': 8886}
train stats after 284384 examples: {'rewards_train/chosen': '0.12038', 'rewards_train/rejected': '0.024208', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096167', 'logps_train/rejected': '-103.32', 'logps_train/chosen': '-161.26', 'loss/train': '0.6583', 'examples_per_second': '30.871', 'grad_norm': '20.25', 'counters/examples': 284384, 'counters/updates': 8887}
train stats after 284416 examples: {'rewards_train/chosen': '0.13112', 'rewards_train/rejected': '0.044938', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.086184', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-113.09', 'loss/train': '0.66829', 'examples_per_second': '24.476', 'grad_norm': '21', 'counters/examples': 284416, 'counters/updates': 8888}
train stats after 284448 examples: {'rewards_train/chosen': '0.2675', 'rewards_train/rejected': '0.003635', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26387', 'logps_train/rejected': '-159.09', 'logps_train/chosen': '-148.49', 'loss/train': '0.6075', 'examples_per_second': '31.591', 'grad_norm': '20.5', 'counters/examples': 284448, 'counters/updates': 8889}
skipping logging after 284480 examples to avoid logging too frequently
train stats after 284512 examples: {'rewards_train/chosen': '0.23131', 'rewards_train/rejected': '0.060083', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17123', 'logps_train/rejected': '-93.905', 'logps_train/chosen': '-98.586', 'loss/train': '0.62252', 'examples_per_second': '24.004', 'grad_norm': '17.125', 'counters/examples': 284512, 'counters/updates': 8891}
train stats after 284544 examples: {'rewards_train/chosen': '0.18394', 'rewards_train/rejected': '0.1255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.058445', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-128.21', 'loss/train': '0.6899', 'examples_per_second': '30.96', 'grad_norm': '20.75', 'counters/examples': 284544, 'counters/updates': 8892}
train stats after 284576 examples: {'rewards_train/chosen': '0.16126', 'rewards_train/rejected': '0.042738', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11852', 'logps_train/rejected': '-89.81', 'logps_train/chosen': '-133.38', 'loss/train': '0.64647', 'examples_per_second': '31.332', 'grad_norm': '18.5', 'counters/examples': 284576, 'counters/updates': 8893}
train stats after 284608 examples: {'rewards_train/chosen': '0.1709', 'rewards_train/rejected': '0.004116', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16678', 'logps_train/rejected': '-144.09', 'logps_train/chosen': '-160.25', 'loss/train': '0.63886', 'examples_per_second': '31.556', 'grad_norm': '21.25', 'counters/examples': 284608, 'counters/updates': 8894}
train stats after 284640 examples: {'rewards_train/chosen': '0.12291', 'rewards_train/rejected': '0.065324', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057584', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-119.87', 'loss/train': '0.6741', 'examples_per_second': '31.536', 'grad_norm': '20.5', 'counters/examples': 284640, 'counters/updates': 8895}
skipping logging after 284672 examples to avoid logging too frequently
train stats after 284704 examples: {'rewards_train/chosen': '0.28547', 'rewards_train/rejected': '0.10377', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1817', 'logps_train/rejected': '-121.07', 'logps_train/chosen': '-146.73', 'loss/train': '0.62343', 'examples_per_second': '31.631', 'grad_norm': '20', 'counters/examples': 284704, 'counters/updates': 8897}
skipping logging after 284736 examples to avoid logging too frequently
train stats after 284768 examples: {'rewards_train/chosen': '0.15512', 'rewards_train/rejected': '0.013013', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14211', 'logps_train/rejected': '-105.8', 'logps_train/chosen': '-102.13', 'loss/train': '0.63942', 'examples_per_second': '31.616', 'grad_norm': '17.875', 'counters/examples': 284768, 'counters/updates': 8899}
train stats after 284800 examples: {'rewards_train/chosen': '0.26748', 'rewards_train/rejected': '0.099747', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16773', 'logps_train/rejected': '-150.23', 'logps_train/chosen': '-125.26', 'loss/train': '0.6388', 'examples_per_second': '31.126', 'grad_norm': '19.875', 'counters/examples': 284800, 'counters/updates': 8900}
train stats after 284832 examples: {'rewards_train/chosen': '0.15719', 'rewards_train/rejected': '0.080179', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.077006', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-141.71', 'loss/train': '0.68004', 'examples_per_second': '32.034', 'grad_norm': '21.125', 'counters/examples': 284832, 'counters/updates': 8901}
train stats after 284864 examples: {'rewards_train/chosen': '0.097376', 'rewards_train/rejected': '-0.016424', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1138', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-138.33', 'loss/train': '0.65686', 'examples_per_second': '31.382', 'grad_norm': '19', 'counters/examples': 284864, 'counters/updates': 8902}
train stats after 284896 examples: {'rewards_train/chosen': '0.12652', 'rewards_train/rejected': '0.018954', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10756', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-120.69', 'loss/train': '0.66549', 'examples_per_second': '32.79', 'grad_norm': '19.75', 'counters/examples': 284896, 'counters/updates': 8903}
train stats after 284928 examples: {'rewards_train/chosen': '0.1448', 'rewards_train/rejected': '0.070227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074577', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-132', 'loss/train': '0.68279', 'examples_per_second': '31.652', 'grad_norm': '22.75', 'counters/examples': 284928, 'counters/updates': 8904}
train stats after 284960 examples: {'rewards_train/chosen': '0.13992', 'rewards_train/rejected': '0.0069177', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.133', 'logps_train/rejected': '-94.676', 'logps_train/chosen': '-110.52', 'loss/train': '0.65212', 'examples_per_second': '31.629', 'grad_norm': '18.25', 'counters/examples': 284960, 'counters/updates': 8905}
train stats after 284992 examples: {'rewards_train/chosen': '0.23403', 'rewards_train/rejected': '-0.023831', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25786', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-135.68', 'loss/train': '0.59848', 'examples_per_second': '32.149', 'grad_norm': '17.375', 'counters/examples': 284992, 'counters/updates': 8906}
train stats after 285024 examples: {'rewards_train/chosen': '0.10436', 'rewards_train/rejected': '0.049831', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054525', 'logps_train/rejected': '-96.912', 'logps_train/chosen': '-121.96', 'loss/train': '0.67979', 'examples_per_second': '30.957', 'grad_norm': '18.5', 'counters/examples': 285024, 'counters/updates': 8907}
train stats after 285056 examples: {'rewards_train/chosen': '0.22564', 'rewards_train/rejected': '0.034693', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19095', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-161.81', 'loss/train': '0.61907', 'examples_per_second': '31.381', 'grad_norm': '20.5', 'counters/examples': 285056, 'counters/updates': 8908}
train stats after 285088 examples: {'rewards_train/chosen': '0.18955', 'rewards_train/rejected': '-0.036458', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22601', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-158.18', 'loss/train': '0.60287', 'examples_per_second': '31.665', 'grad_norm': '20.5', 'counters/examples': 285088, 'counters/updates': 8909}
train stats after 285120 examples: {'rewards_train/chosen': '0.21656', 'rewards_train/rejected': '-0.0095189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22608', 'logps_train/rejected': '-94.925', 'logps_train/chosen': '-118.43', 'loss/train': '0.62501', 'examples_per_second': '30.134', 'grad_norm': '19.25', 'counters/examples': 285120, 'counters/updates': 8910}
train stats after 285152 examples: {'rewards_train/chosen': '0.1055', 'rewards_train/rejected': '-0.050778', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15627', 'logps_train/rejected': '-104.92', 'logps_train/chosen': '-136.99', 'loss/train': '0.63237', 'examples_per_second': '30.521', 'grad_norm': '18.375', 'counters/examples': 285152, 'counters/updates': 8911}
train stats after 285184 examples: {'rewards_train/chosen': '0.16856', 'rewards_train/rejected': '-0.016275', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18484', 'logps_train/rejected': '-110.56', 'logps_train/chosen': '-131.33', 'loss/train': '0.6231', 'examples_per_second': '31.264', 'grad_norm': '20.375', 'counters/examples': 285184, 'counters/updates': 8912}
train stats after 285216 examples: {'rewards_train/chosen': '0.24976', 'rewards_train/rejected': '0.047733', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20203', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-129.15', 'loss/train': '0.6134', 'examples_per_second': '31.714', 'grad_norm': '19.875', 'counters/examples': 285216, 'counters/updates': 8913}
train stats after 285248 examples: {'rewards_train/chosen': '0.15365', 'rewards_train/rejected': '0.10125', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052399', 'logps_train/rejected': '-99.583', 'logps_train/chosen': '-114.85', 'loss/train': '0.67615', 'examples_per_second': '30.471', 'grad_norm': '19.375', 'counters/examples': 285248, 'counters/updates': 8914}
skipping logging after 285280 examples to avoid logging too frequently
train stats after 285312 examples: {'rewards_train/chosen': '0.17706', 'rewards_train/rejected': '0.087429', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089626', 'logps_train/rejected': '-137.48', 'logps_train/chosen': '-151.48', 'loss/train': '0.66653', 'examples_per_second': '31.222', 'grad_norm': '22.125', 'counters/examples': 285312, 'counters/updates': 8916}
train stats after 285344 examples: {'rewards_train/chosen': '0.087326', 'rewards_train/rejected': '-0.00042399', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08775', 'logps_train/rejected': '-107.93', 'logps_train/chosen': '-143.85', 'loss/train': '0.6686', 'examples_per_second': '33.144', 'grad_norm': '21.125', 'counters/examples': 285344, 'counters/updates': 8917}
train stats after 285376 examples: {'rewards_train/chosen': '0.20432', 'rewards_train/rejected': '0.036627', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16769', 'logps_train/rejected': '-140.6', 'logps_train/chosen': '-151.8', 'loss/train': '0.62844', 'examples_per_second': '31.521', 'grad_norm': '20.875', 'counters/examples': 285376, 'counters/updates': 8918}
train stats after 285408 examples: {'rewards_train/chosen': '0.20874', 'rewards_train/rejected': '-0.0087449', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21748', 'logps_train/rejected': '-97.836', 'logps_train/chosen': '-127.67', 'loss/train': '0.60511', 'examples_per_second': '32.45', 'grad_norm': '17.75', 'counters/examples': 285408, 'counters/updates': 8919}
train stats after 285440 examples: {'rewards_train/chosen': '0.27499', 'rewards_train/rejected': '0.0090099', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26598', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-156.38', 'loss/train': '0.58869', 'examples_per_second': '31.567', 'grad_norm': '20.125', 'counters/examples': 285440, 'counters/updates': 8920}
skipping logging after 285472 examples to avoid logging too frequently
train stats after 285504 examples: {'rewards_train/chosen': '0.090477', 'rewards_train/rejected': '0.091105', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00062789', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-104.27', 'loss/train': '0.70413', 'examples_per_second': '33.38', 'grad_norm': '20.5', 'counters/examples': 285504, 'counters/updates': 8922}
train stats after 285536 examples: {'rewards_train/chosen': '0.053897', 'rewards_train/rejected': '-0.029646', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083544', 'logps_train/rejected': '-151.81', 'logps_train/chosen': '-157.96', 'loss/train': '0.66434', 'examples_per_second': '30.409', 'grad_norm': '23.625', 'counters/examples': 285536, 'counters/updates': 8923}
train stats after 285568 examples: {'rewards_train/chosen': '0.11596', 'rewards_train/rejected': '0.13175', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015782', 'logps_train/rejected': '-159.03', 'logps_train/chosen': '-149.11', 'loss/train': '0.72024', 'examples_per_second': '32.267', 'grad_norm': '23.5', 'counters/examples': 285568, 'counters/updates': 8924}
train stats after 285600 examples: {'rewards_train/chosen': '0.12627', 'rewards_train/rejected': '-0.0030875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12936', 'logps_train/rejected': '-92.961', 'logps_train/chosen': '-126', 'loss/train': '0.6373', 'examples_per_second': '32.115', 'grad_norm': '17.375', 'counters/examples': 285600, 'counters/updates': 8925}
train stats after 285632 examples: {'rewards_train/chosen': '0.14569', 'rewards_train/rejected': '-0.030282', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17597', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-156.71', 'loss/train': '0.63171', 'examples_per_second': '31.682', 'grad_norm': '21', 'counters/examples': 285632, 'counters/updates': 8926}
skipping logging after 285664 examples to avoid logging too frequently
train stats after 285696 examples: {'rewards_train/chosen': '0.19305', 'rewards_train/rejected': '0.095526', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097526', 'logps_train/rejected': '-136.66', 'logps_train/chosen': '-164.6', 'loss/train': '0.67301', 'examples_per_second': '33.2', 'grad_norm': '22.875', 'counters/examples': 285696, 'counters/updates': 8928}
train stats after 285728 examples: {'rewards_train/chosen': '0.13338', 'rewards_train/rejected': '0.014158', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11923', 'logps_train/rejected': '-97.295', 'logps_train/chosen': '-141.07', 'loss/train': '0.64566', 'examples_per_second': '31.515', 'grad_norm': '20.125', 'counters/examples': 285728, 'counters/updates': 8929}
train stats after 285760 examples: {'rewards_train/chosen': '0.14071', 'rewards_train/rejected': '0.15697', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016263', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-147.92', 'loss/train': '0.71614', 'examples_per_second': '31.25', 'grad_norm': '21.25', 'counters/examples': 285760, 'counters/updates': 8930}
train stats after 285792 examples: {'rewards_train/chosen': '0.18978', 'rewards_train/rejected': '0.093178', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096606', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-117.28', 'loss/train': '0.66453', 'examples_per_second': '24.759', 'grad_norm': '20.875', 'counters/examples': 285792, 'counters/updates': 8931}
train stats after 285824 examples: {'rewards_train/chosen': '0.27259', 'rewards_train/rejected': '-0.072082', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.34468', 'logps_train/rejected': '-92.292', 'logps_train/chosen': '-152.85', 'loss/train': '0.55231', 'examples_per_second': '31.68', 'grad_norm': '17.375', 'counters/examples': 285824, 'counters/updates': 8932}
train stats after 285856 examples: {'rewards_train/chosen': '0.31453', 'rewards_train/rejected': '0.1103', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20423', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-142.22', 'loss/train': '0.61833', 'examples_per_second': '31.608', 'grad_norm': '17.875', 'counters/examples': 285856, 'counters/updates': 8933}
train stats after 285888 examples: {'rewards_train/chosen': '0.26472', 'rewards_train/rejected': '0.025233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23949', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-124.73', 'loss/train': '0.60741', 'examples_per_second': '30.673', 'grad_norm': '19.625', 'counters/examples': 285888, 'counters/updates': 8934}
train stats after 285920 examples: {'rewards_train/chosen': '0.22808', 'rewards_train/rejected': '0.0070525', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22103', 'logps_train/rejected': '-92.15', 'logps_train/chosen': '-143.68', 'loss/train': '0.61503', 'examples_per_second': '31.625', 'grad_norm': '18', 'counters/examples': 285920, 'counters/updates': 8935}
train stats after 285952 examples: {'rewards_train/chosen': '0.21074', 'rewards_train/rejected': '0.024531', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18621', 'logps_train/rejected': '-157.86', 'logps_train/chosen': '-158.04', 'loss/train': '0.61713', 'examples_per_second': '32.585', 'grad_norm': '22', 'counters/examples': 285952, 'counters/updates': 8936}
skipping logging after 285984 examples to avoid logging too frequently
train stats after 286016 examples: {'rewards_train/chosen': '0.11928', 'rewards_train/rejected': '0.035091', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084189', 'logps_train/rejected': '-96.013', 'logps_train/chosen': '-89.883', 'loss/train': '0.66806', 'examples_per_second': '30.697', 'grad_norm': '17.125', 'counters/examples': 286016, 'counters/updates': 8938}
train stats after 286048 examples: {'rewards_train/chosen': '0.11478', 'rewards_train/rejected': '0.0038461', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11094', 'logps_train/rejected': '-101.84', 'logps_train/chosen': '-137.14', 'loss/train': '0.65436', 'examples_per_second': '32.443', 'grad_norm': '18.5', 'counters/examples': 286048, 'counters/updates': 8939}
train stats after 286080 examples: {'rewards_train/chosen': '0.19055', 'rewards_train/rejected': '0.066234', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12432', 'logps_train/rejected': '-128.6', 'logps_train/chosen': '-117.49', 'loss/train': '0.65345', 'examples_per_second': '31.407', 'grad_norm': '21', 'counters/examples': 286080, 'counters/updates': 8940}
train stats after 286112 examples: {'rewards_train/chosen': '0.14558', 'rewards_train/rejected': '0.019478', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1261', 'logps_train/rejected': '-147.47', 'logps_train/chosen': '-152.16', 'loss/train': '0.66174', 'examples_per_second': '31.651', 'grad_norm': '21.375', 'counters/examples': 286112, 'counters/updates': 8941}
train stats after 286144 examples: {'rewards_train/chosen': '0.19914', 'rewards_train/rejected': '0.0071461', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19199', 'logps_train/rejected': '-101.45', 'logps_train/chosen': '-118.51', 'loss/train': '0.61816', 'examples_per_second': '30.573', 'grad_norm': '19.125', 'counters/examples': 286144, 'counters/updates': 8942}
train stats after 286176 examples: {'rewards_train/chosen': '0.14646', 'rewards_train/rejected': '0.097499', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048961', 'logps_train/rejected': '-140.33', 'logps_train/chosen': '-111.97', 'loss/train': '0.6929', 'examples_per_second': '31.627', 'grad_norm': '20.75', 'counters/examples': 286176, 'counters/updates': 8943}
skipping logging after 286208 examples to avoid logging too frequently
train stats after 286240 examples: {'rewards_train/chosen': '0.23907', 'rewards_train/rejected': '0.030903', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20817', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-121.34', 'loss/train': '0.61032', 'examples_per_second': '30.992', 'grad_norm': '18.625', 'counters/examples': 286240, 'counters/updates': 8945}
train stats after 286272 examples: {'rewards_train/chosen': '0.16671', 'rewards_train/rejected': '0.10418', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062527', 'logps_train/rejected': '-106.4', 'logps_train/chosen': '-131.16', 'loss/train': '0.67953', 'examples_per_second': '31.413', 'grad_norm': '20.5', 'counters/examples': 286272, 'counters/updates': 8946}
train stats after 286304 examples: {'rewards_train/chosen': '0.1571', 'rewards_train/rejected': '0.070561', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086535', 'logps_train/rejected': '-149.06', 'logps_train/chosen': '-139', 'loss/train': '0.66428', 'examples_per_second': '30.921', 'grad_norm': '23', 'counters/examples': 286304, 'counters/updates': 8947}
train stats after 286336 examples: {'rewards_train/chosen': '0.35013', 'rewards_train/rejected': '-0.0067073', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.35683', 'logps_train/rejected': '-107.32', 'logps_train/chosen': '-123.98', 'loss/train': '0.55716', 'examples_per_second': '32.247', 'grad_norm': '17', 'counters/examples': 286336, 'counters/updates': 8948}
train stats after 286368 examples: {'rewards_train/chosen': '0.17555', 'rewards_train/rejected': '-0.016808', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19236', 'logps_train/rejected': '-97.892', 'logps_train/chosen': '-108.87', 'loss/train': '0.62472', 'examples_per_second': '31.803', 'grad_norm': '17.5', 'counters/examples': 286368, 'counters/updates': 8949}
train stats after 286400 examples: {'rewards_train/chosen': '0.18102', 'rewards_train/rejected': '-0.019369', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20039', 'logps_train/rejected': '-100.3', 'logps_train/chosen': '-130.67', 'loss/train': '0.62079', 'examples_per_second': '31.641', 'grad_norm': '17.75', 'counters/examples': 286400, 'counters/updates': 8950}
train stats after 286432 examples: {'rewards_train/chosen': '0.19803', 'rewards_train/rejected': '-0.026427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22446', 'logps_train/rejected': '-153.42', 'logps_train/chosen': '-152.92', 'loss/train': '0.6064', 'examples_per_second': '31.274', 'grad_norm': '21.375', 'counters/examples': 286432, 'counters/updates': 8951}
skipping logging after 286464 examples to avoid logging too frequently
train stats after 286496 examples: {'rewards_train/chosen': '0.2523', 'rewards_train/rejected': '0.08675', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16556', 'logps_train/rejected': '-84.94', 'logps_train/chosen': '-119.4', 'loss/train': '0.62801', 'examples_per_second': '31.114', 'grad_norm': '17.875', 'counters/examples': 286496, 'counters/updates': 8953}
train stats after 286528 examples: {'rewards_train/chosen': '0.15539', 'rewards_train/rejected': '0.0060777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14931', 'logps_train/rejected': '-94.564', 'logps_train/chosen': '-121.35', 'loss/train': '0.65212', 'examples_per_second': '32.133', 'grad_norm': '17.5', 'counters/examples': 286528, 'counters/updates': 8954}
train stats after 286560 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '0.0679', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046916', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-167.51', 'loss/train': '0.68637', 'examples_per_second': '32.316', 'grad_norm': '23.375', 'counters/examples': 286560, 'counters/updates': 8955}
train stats after 286592 examples: {'rewards_train/chosen': '0.12971', 'rewards_train/rejected': '0.13063', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00092257', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-133.14', 'loss/train': '0.7162', 'examples_per_second': '31.654', 'grad_norm': '21.875', 'counters/examples': 286592, 'counters/updates': 8956}
train stats after 286624 examples: {'rewards_train/chosen': '0.18509', 'rewards_train/rejected': '0.017206', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16789', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-110.85', 'loss/train': '0.6271', 'examples_per_second': '31.648', 'grad_norm': '18.875', 'counters/examples': 286624, 'counters/updates': 8957}
train stats after 286656 examples: {'rewards_train/chosen': '0.18691', 'rewards_train/rejected': '0.080771', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10614', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-143.55', 'loss/train': '0.66555', 'examples_per_second': '31.622', 'grad_norm': '21.25', 'counters/examples': 286656, 'counters/updates': 8958}
train stats after 286688 examples: {'rewards_train/chosen': '0.20292', 'rewards_train/rejected': '0.079729', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12319', 'logps_train/rejected': '-112.24', 'logps_train/chosen': '-133.64', 'loss/train': '0.6477', 'examples_per_second': '31.693', 'grad_norm': '19.75', 'counters/examples': 286688, 'counters/updates': 8959}
skipping logging after 286720 examples to avoid logging too frequently
train stats after 286752 examples: {'rewards_train/chosen': '0.20723', 'rewards_train/rejected': '0.011638', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19559', 'logps_train/rejected': '-139.13', 'logps_train/chosen': '-159.38', 'loss/train': '0.62041', 'examples_per_second': '32.826', 'grad_norm': '21.125', 'counters/examples': 286752, 'counters/updates': 8961}
train stats after 286784 examples: {'rewards_train/chosen': '0.32187', 'rewards_train/rejected': '0.049205', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27267', 'logps_train/rejected': '-165.39', 'logps_train/chosen': '-170.99', 'loss/train': '0.59269', 'examples_per_second': '33.238', 'grad_norm': '22.375', 'counters/examples': 286784, 'counters/updates': 8962}
train stats after 286816 examples: {'rewards_train/chosen': '0.1674', 'rewards_train/rejected': '0.031181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13622', 'logps_train/rejected': '-104.17', 'logps_train/chosen': '-112.45', 'loss/train': '0.64474', 'examples_per_second': '32.347', 'grad_norm': '19.25', 'counters/examples': 286816, 'counters/updates': 8963}
train stats after 286848 examples: {'rewards_train/chosen': '0.16582', 'rewards_train/rejected': '-0.036006', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20183', 'logps_train/rejected': '-94.424', 'logps_train/chosen': '-114.66', 'loss/train': '0.61155', 'examples_per_second': '31.009', 'grad_norm': '17.125', 'counters/examples': 286848, 'counters/updates': 8964}
train stats after 286880 examples: {'rewards_train/chosen': '0.12563', 'rewards_train/rejected': '0.094238', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.031393', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-147.99', 'loss/train': '0.69575', 'examples_per_second': '31.41', 'grad_norm': '22.25', 'counters/examples': 286880, 'counters/updates': 8965}
train stats after 286912 examples: {'rewards_train/chosen': '0.12004', 'rewards_train/rejected': '-0.050772', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17082', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-170.87', 'loss/train': '0.63072', 'examples_per_second': '31.65', 'grad_norm': '20.25', 'counters/examples': 286912, 'counters/updates': 8966}
train stats after 286944 examples: {'rewards_train/chosen': '0.1588', 'rewards_train/rejected': '-0.030713', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18951', 'logps_train/rejected': '-81.158', 'logps_train/chosen': '-131.55', 'loss/train': '0.62136', 'examples_per_second': '31.242', 'grad_norm': '17.375', 'counters/examples': 286944, 'counters/updates': 8967}
train stats after 286976 examples: {'rewards_train/chosen': '0.24859', 'rewards_train/rejected': '0.092082', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15651', 'logps_train/rejected': '-136.92', 'logps_train/chosen': '-138.05', 'loss/train': '0.63769', 'examples_per_second': '30.391', 'grad_norm': '20.75', 'counters/examples': 286976, 'counters/updates': 8968}
train stats after 287008 examples: {'rewards_train/chosen': '0.20103', 'rewards_train/rejected': '0.14238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058657', 'logps_train/rejected': '-116.72', 'logps_train/chosen': '-124.94', 'loss/train': '0.68367', 'examples_per_second': '32.583', 'grad_norm': '19.875', 'counters/examples': 287008, 'counters/updates': 8969}
skipping logging after 287040 examples to avoid logging too frequently
train stats after 287072 examples: {'rewards_train/chosen': '0.21814', 'rewards_train/rejected': '0.067588', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15055', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-166.14', 'loss/train': '0.64044', 'examples_per_second': '31.594', 'grad_norm': '21.25', 'counters/examples': 287072, 'counters/updates': 8971}
train stats after 287104 examples: {'rewards_train/chosen': '0.074127', 'rewards_train/rejected': '0.0034487', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070679', 'logps_train/rejected': '-112.48', 'logps_train/chosen': '-116.75', 'loss/train': '0.6653', 'examples_per_second': '31.673', 'grad_norm': '19.25', 'counters/examples': 287104, 'counters/updates': 8972}
skipping logging after 287136 examples to avoid logging too frequently
train stats after 287168 examples: {'rewards_train/chosen': '0.16456', 'rewards_train/rejected': '0.090136', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074422', 'logps_train/rejected': '-104.81', 'logps_train/chosen': '-96.717', 'loss/train': '0.66954', 'examples_per_second': '31.771', 'grad_norm': '19.75', 'counters/examples': 287168, 'counters/updates': 8974}
train stats after 287200 examples: {'rewards_train/chosen': '0.35129', 'rewards_train/rejected': '0.088489', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2628', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-163.84', 'loss/train': '0.5892', 'examples_per_second': '31.82', 'grad_norm': '20', 'counters/examples': 287200, 'counters/updates': 8975}
skipping logging after 287232 examples to avoid logging too frequently
train stats after 287264 examples: {'rewards_train/chosen': '0.067322', 'rewards_train/rejected': '-0.048336', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11566', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-118.32', 'loss/train': '0.65176', 'examples_per_second': '31.97', 'grad_norm': '21.25', 'counters/examples': 287264, 'counters/updates': 8977}
train stats after 287296 examples: {'rewards_train/chosen': '0.18439', 'rewards_train/rejected': '0.057324', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12706', 'logps_train/rejected': '-122.31', 'logps_train/chosen': '-171.62', 'loss/train': '0.65367', 'examples_per_second': '30.286', 'grad_norm': '22', 'counters/examples': 287296, 'counters/updates': 8978}
train stats after 287328 examples: {'rewards_train/chosen': '0.25705', 'rewards_train/rejected': '-0.020688', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27774', 'logps_train/rejected': '-167.98', 'logps_train/chosen': '-152.97', 'loss/train': '0.57955', 'examples_per_second': '31.634', 'grad_norm': '20.5', 'counters/examples': 287328, 'counters/updates': 8979}
train stats after 287360 examples: {'rewards_train/chosen': '0.16397', 'rewards_train/rejected': '0.074935', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08904', 'logps_train/rejected': '-139.1', 'logps_train/chosen': '-141.43', 'loss/train': '0.68537', 'examples_per_second': '31.636', 'grad_norm': '22', 'counters/examples': 287360, 'counters/updates': 8980}
train stats after 287392 examples: {'rewards_train/chosen': '0.24377', 'rewards_train/rejected': '-0.00072567', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2445', 'logps_train/rejected': '-101.09', 'logps_train/chosen': '-139.42', 'loss/train': '0.59807', 'examples_per_second': '30.366', 'grad_norm': '19.125', 'counters/examples': 287392, 'counters/updates': 8981}
skipping logging after 287424 examples to avoid logging too frequently
train stats after 287456 examples: {'rewards_train/chosen': '0.16496', 'rewards_train/rejected': '-0.028443', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1934', 'logps_train/rejected': '-118.08', 'logps_train/chosen': '-122.38', 'loss/train': '0.61749', 'examples_per_second': '34.766', 'grad_norm': '18.625', 'counters/examples': 287456, 'counters/updates': 8983}
train stats after 287488 examples: {'rewards_train/chosen': '0.19334', 'rewards_train/rejected': '0.15388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039462', 'logps_train/rejected': '-152.51', 'logps_train/chosen': '-129.76', 'loss/train': '0.68613', 'examples_per_second': '31.653', 'grad_norm': '22', 'counters/examples': 287488, 'counters/updates': 8984}
train stats after 287520 examples: {'rewards_train/chosen': '0.32349', 'rewards_train/rejected': '0.081404', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24209', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-161.23', 'loss/train': '0.59847', 'examples_per_second': '32.568', 'grad_norm': '20.75', 'counters/examples': 287520, 'counters/updates': 8985}
train stats after 287552 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '0.14468', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011253', 'logps_train/rejected': '-133.13', 'logps_train/chosen': '-146.69', 'loss/train': '0.71698', 'examples_per_second': '31.863', 'grad_norm': '23', 'counters/examples': 287552, 'counters/updates': 8986}
train stats after 287584 examples: {'rewards_train/chosen': '0.16156', 'rewards_train/rejected': '0.036349', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12521', 'logps_train/rejected': '-84.594', 'logps_train/chosen': '-119.3', 'loss/train': '0.64536', 'examples_per_second': '32.761', 'grad_norm': '17.75', 'counters/examples': 287584, 'counters/updates': 8987}
train stats after 287616 examples: {'rewards_train/chosen': '0.17918', 'rewards_train/rejected': '0.097167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082017', 'logps_train/rejected': '-116.63', 'logps_train/chosen': '-131.19', 'loss/train': '0.66957', 'examples_per_second': '31.98', 'grad_norm': '19.75', 'counters/examples': 287616, 'counters/updates': 8988}
train stats after 287648 examples: {'rewards_train/chosen': '0.10991', 'rewards_train/rejected': '0.032562', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.077348', 'logps_train/rejected': '-133.22', 'logps_train/chosen': '-136.26', 'loss/train': '0.6712', 'examples_per_second': '31.649', 'grad_norm': '20.5', 'counters/examples': 287648, 'counters/updates': 8989}
train stats after 287680 examples: {'rewards_train/chosen': '0.11773', 'rewards_train/rejected': '-0.0010303', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11876', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-141.71', 'loss/train': '0.6587', 'examples_per_second': '30.159', 'grad_norm': '21.375', 'counters/examples': 287680, 'counters/updates': 8990}
train stats after 287712 examples: {'rewards_train/chosen': '0.043965', 'rewards_train/rejected': '-0.08011', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12408', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-154.53', 'loss/train': '0.64873', 'examples_per_second': '31.555', 'grad_norm': '20.75', 'counters/examples': 287712, 'counters/updates': 8991}
skipping logging after 287744 examples to avoid logging too frequently
train stats after 287776 examples: {'rewards_train/chosen': '0.25443', 'rewards_train/rejected': '0.098666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15576', 'logps_train/rejected': '-122.79', 'logps_train/chosen': '-146.85', 'loss/train': '0.6321', 'examples_per_second': '31.604', 'grad_norm': '20.75', 'counters/examples': 287776, 'counters/updates': 8993}
skipping logging after 287808 examples to avoid logging too frequently
train stats after 287840 examples: {'rewards_train/chosen': '0.13993', 'rewards_train/rejected': '0.066261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07367', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-113.95', 'loss/train': '0.67245', 'examples_per_second': '38.312', 'grad_norm': '18.5', 'counters/examples': 287840, 'counters/updates': 8995}
train stats after 287872 examples: {'rewards_train/chosen': '0.25779', 'rewards_train/rejected': '0.066026', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19177', 'logps_train/rejected': '-112.93', 'logps_train/chosen': '-142.28', 'loss/train': '0.6316', 'examples_per_second': '32.229', 'grad_norm': '20.25', 'counters/examples': 287872, 'counters/updates': 8996}
train stats after 287904 examples: {'rewards_train/chosen': '0.23962', 'rewards_train/rejected': '0.028', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21162', 'logps_train/rejected': '-104.65', 'logps_train/chosen': '-160.09', 'loss/train': '0.60849', 'examples_per_second': '30.506', 'grad_norm': '19.625', 'counters/examples': 287904, 'counters/updates': 8997}
train stats after 287936 examples: {'rewards_train/chosen': '0.11738', 'rewards_train/rejected': '-0.0091613', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12654', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-114.88', 'loss/train': '0.64294', 'examples_per_second': '32.92', 'grad_norm': '18.25', 'counters/examples': 287936, 'counters/updates': 8998}
train stats after 287968 examples: {'rewards_train/chosen': '0.26658', 'rewards_train/rejected': '0.0096082', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25698', 'logps_train/rejected': '-149.69', 'logps_train/chosen': '-162.82', 'loss/train': '0.59011', 'examples_per_second': '31.707', 'grad_norm': '21.75', 'counters/examples': 287968, 'counters/updates': 8999}
train stats after 288000 examples: {'rewards_train/chosen': '0.20437', 'rewards_train/rejected': '0.036212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16815', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-118.81', 'loss/train': '0.63064', 'examples_per_second': '32.054', 'grad_norm': '18.75', 'counters/examples': 288000, 'counters/updates': 9000}
Running evaluation after 288000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.92it/s]
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
eval after 288000: {'rewards_eval/chosen': '0.18446', 'rewards_eval/rejected': '0.041625', 'rewards_eval/accuracies': '0.56641', 'rewards_eval/margins': '0.14283', 'logps_eval/rejected': '-114.98', 'logps_eval/chosen': '-133.7', 'loss/eval': '0.65053'}
skipping save for non epoch
train stats after 288032 examples: {'rewards_train/chosen': '0.14103', 'rewards_train/rejected': '0.070956', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070074', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-153.53', 'loss/train': '0.67246', 'examples_per_second': '31.805', 'grad_norm': '21.75', 'counters/examples': 288032, 'counters/updates': 9001}
train stats after 288064 examples: {'rewards_train/chosen': '0.22644', 'rewards_train/rejected': '0.023971', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20247', 'logps_train/rejected': '-109.7', 'logps_train/chosen': '-126.44', 'loss/train': '0.62109', 'examples_per_second': '30.111', 'grad_norm': '19.25', 'counters/examples': 288064, 'counters/updates': 9002}
train stats after 288096 examples: {'rewards_train/chosen': '0.14746', 'rewards_train/rejected': '-0.071954', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21942', 'logps_train/rejected': '-97.862', 'logps_train/chosen': '-140.04', 'loss/train': '0.60524', 'examples_per_second': '32.41', 'grad_norm': '18.375', 'counters/examples': 288096, 'counters/updates': 9003}
train stats after 288128 examples: {'rewards_train/chosen': '0.25813', 'rewards_train/rejected': '0.063619', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19451', 'logps_train/rejected': '-117.21', 'logps_train/chosen': '-164.97', 'loss/train': '0.61465', 'examples_per_second': '32.42', 'grad_norm': '19.5', 'counters/examples': 288128, 'counters/updates': 9004}
skipping logging after 288160 examples to avoid logging too frequently
train stats after 288192 examples: {'rewards_train/chosen': '0.24707', 'rewards_train/rejected': '0.03525', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21182', 'logps_train/rejected': '-134.07', 'logps_train/chosen': '-165.4', 'loss/train': '0.62583', 'examples_per_second': '32.533', 'grad_norm': '22', 'counters/examples': 288192, 'counters/updates': 9006}
train stats after 288224 examples: {'rewards_train/chosen': '0.22993', 'rewards_train/rejected': '0.023293', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20664', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-164.9', 'loss/train': '0.61925', 'examples_per_second': '32.492', 'grad_norm': '21.375', 'counters/examples': 288224, 'counters/updates': 9007}
skipping logging after 288256 examples to avoid logging too frequently
train stats after 288288 examples: {'rewards_train/chosen': '0.18201', 'rewards_train/rejected': '0.049907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1321', 'logps_train/rejected': '-127.49', 'logps_train/chosen': '-149.14', 'loss/train': '0.64505', 'examples_per_second': '31.022', 'grad_norm': '21.375', 'counters/examples': 288288, 'counters/updates': 9009}
train stats after 288320 examples: {'rewards_train/chosen': '0.1875', 'rewards_train/rejected': '0.019671', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16783', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-150.23', 'loss/train': '0.63567', 'examples_per_second': '31.081', 'grad_norm': '21', 'counters/examples': 288320, 'counters/updates': 9010}
train stats after 288352 examples: {'rewards_train/chosen': '0.19162', 'rewards_train/rejected': '0.030172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16145', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-130.29', 'loss/train': '0.63754', 'examples_per_second': '30.312', 'grad_norm': '19.75', 'counters/examples': 288352, 'counters/updates': 9011}
skipping logging after 288384 examples to avoid logging too frequently
train stats after 288416 examples: {'rewards_train/chosen': '0.26292', 'rewards_train/rejected': '0.023205', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23971', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-176.6', 'loss/train': '0.60457', 'examples_per_second': '29.684', 'grad_norm': '20.875', 'counters/examples': 288416, 'counters/updates': 9013}
train stats after 288448 examples: {'rewards_train/chosen': '0.22229', 'rewards_train/rejected': '0.085518', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13678', 'logps_train/rejected': '-107.22', 'logps_train/chosen': '-134.39', 'loss/train': '0.63962', 'examples_per_second': '32.699', 'grad_norm': '19.625', 'counters/examples': 288448, 'counters/updates': 9014}
train stats after 288480 examples: {'rewards_train/chosen': '0.09504', 'rewards_train/rejected': '-0.033634', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12867', 'logps_train/rejected': '-109.46', 'logps_train/chosen': '-148.1', 'loss/train': '0.6485', 'examples_per_second': '30.247', 'grad_norm': '20.625', 'counters/examples': 288480, 'counters/updates': 9015}
train stats after 288512 examples: {'rewards_train/chosen': '0.19292', 'rewards_train/rejected': '0.046835', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14608', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-124.37', 'loss/train': '0.64459', 'examples_per_second': '31.932', 'grad_norm': '18.75', 'counters/examples': 288512, 'counters/updates': 9016}
train stats after 288544 examples: {'rewards_train/chosen': '0.17982', 'rewards_train/rejected': '0.009804', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17002', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-145.98', 'loss/train': '0.63149', 'examples_per_second': '31.71', 'grad_norm': '20.625', 'counters/examples': 288544, 'counters/updates': 9017}
train stats after 288576 examples: {'rewards_train/chosen': '0.064521', 'rewards_train/rejected': '0.05148', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013042', 'logps_train/rejected': '-142.05', 'logps_train/chosen': '-156.57', 'loss/train': '0.70437', 'examples_per_second': '31.64', 'grad_norm': '24.75', 'counters/examples': 288576, 'counters/updates': 9018}
skipping logging after 288608 examples to avoid logging too frequently
train stats after 288640 examples: {'rewards_train/chosen': '0.18168', 'rewards_train/rejected': '-0.059777', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24146', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-162.27', 'loss/train': '0.60467', 'examples_per_second': '32.386', 'grad_norm': '20.625', 'counters/examples': 288640, 'counters/updates': 9020}
train stats after 288672 examples: {'rewards_train/chosen': '0.21718', 'rewards_train/rejected': '0.050517', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16666', 'logps_train/rejected': '-107.38', 'logps_train/chosen': '-144.38', 'loss/train': '0.63141', 'examples_per_second': '31.723', 'grad_norm': '20.125', 'counters/examples': 288672, 'counters/updates': 9021}
train stats after 288704 examples: {'rewards_train/chosen': '0.20456', 'rewards_train/rejected': '-0.036663', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24122', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-148', 'loss/train': '0.61688', 'examples_per_second': '31.493', 'grad_norm': '20.25', 'counters/examples': 288704, 'counters/updates': 9022}
skipping logging after 288736 examples to avoid logging too frequently
train stats after 288768 examples: {'rewards_train/chosen': '0.16562', 'rewards_train/rejected': '-0.0027479', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16837', 'logps_train/rejected': '-120.06', 'logps_train/chosen': '-151.69', 'loss/train': '0.63575', 'examples_per_second': '31.441', 'grad_norm': '20.5', 'counters/examples': 288768, 'counters/updates': 9024}
Finished generating 3 epochs on train split
writing checkpoint to .cache/laura/pythia2.8b_sfted0_dpo3_seed0_2024-03-19_01-35-35_313933/LATEST/policy.pt...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         counters/updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      examples_per_second ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÖ
wandb:                grad_norm ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ
wandb:        logps_eval/chosen ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      logps_eval/rejected ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:       logps_train/chosen ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñà
wandb:     logps_train/rejected ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà
wandb:                loss/eval ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:               loss/train ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÑ‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñá‚ñá‚ñÇ‚ñÉ
wandb:  rewards_eval/accuracies ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ
wandb:      rewards_eval/chosen ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     rewards_eval/margins ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá
wandb:    rewards_eval/rejected ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: rewards_train/accuracies ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÖ
wandb:     rewards_train/chosen ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÖ‚ñÖ‚ñá‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÜ
wandb:    rewards_train/margins ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÜ
wandb:   rewards_train/rejected ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb:        counters/examples 288768
wandb:         counters/updates 9024
wandb:      examples_per_second 31.44072
wandb:                grad_norm 20.5
wandb:        logps_eval/chosen -133.70338
wandb:      logps_eval/rejected -114.97637
wandb:       logps_train/chosen -151.69279
wandb:     logps_train/rejected -120.06131
wandb:                loss/eval 0.65053
wandb:               loss/train 0.63575
wandb:  rewards_eval/accuracies 0.56641
wandb:      rewards_eval/chosen 0.18446
wandb:     rewards_eval/margins 0.14283
wandb:    rewards_eval/rejected 0.04162
wandb: rewards_train/accuracies 0.71875
wandb:     rewards_train/chosen 0.16562
wandb:    rewards_train/margins 0.16837
wandb:   rewards_train/rejected -0.00275
wandb: 
wandb: üöÄ View run pythia2.8b_sfted0_dpo3_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/sb8hm4my
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240319_013736-sb8hm4my/logs
