no FSDP port specified; using open port for FSDP: 47339
seed: 0
exp_name: pythia2.8b_sfted1_dpo3_seed0
batch_size: 32
eval_batch_size: 16
debug: false
fsdp_port: 47339
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880
lr: 4.0e-07
gradient_accumulation_steps: 2
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 3
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 12000
minimum_log_interval_secs: 1.0
revision: epoch1-6000
model:
  name_or_path: lomahony/pythia-2.8b-helpful-sft-3epochs
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: bfloat16
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-222-192:.cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.04it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  2.04it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.79it/s]
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-2.8b-helpful-sft-3epochs and are newly initialized: ['gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.25.attention.bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.27.attention.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
building reference model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 12.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.45s/it]
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-2.8b-helpful-sft-3epochs and are newly initialized: ['gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.25.attention.bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.27.attention.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240319_022927-s3qgcala
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia2.8b_sfted1_dpo3_seed0
wandb: â­ï¸ View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: ðŸš€ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/s3qgcala
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-2.8b-helpful-sft-3epochs
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2806/5103 [00:00<00:00, 28046.59it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5103/5103 [00:00<00:00, 28283.46it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   1%|          | 487/96256 [00:00<00:41, 2305.05it/s]Processing HH static:   3%|â–Ž         | 3354/96256 [00:00<00:07, 12939.58it/s]Processing HH static:   6%|â–‹         | 6187/96256 [00:00<00:04, 18564.69it/s]Processing HH static:   9%|â–‰         | 9042/96256 [00:00<00:03, 21987.29it/s]Processing HH static:  12%|â–ˆâ–        | 11903/96256 [00:00<00:03, 24164.25it/s]Processing HH static:  15%|â–ˆâ–Œ        | 14755/96256 [00:00<00:03, 25555.46it/s]Processing HH static:  18%|â–ˆâ–Š        | 17648/96256 [00:00<00:02, 26612.55it/s]Processing HH static:  21%|â–ˆâ–ˆâ–       | 20529/96256 [00:00<00:02, 27291.74it/s]Processing HH static:  24%|â–ˆâ–ˆâ–       | 23406/96256 [00:01<00:02, 27742.99it/s]Processing HH static:  27%|â–ˆâ–ˆâ–‹       | 26293/96256 [00:01<00:02, 28084.65it/s]Processing HH static:  30%|â–ˆâ–ˆâ–ˆ       | 29147/96256 [00:01<00:02, 28219.29it/s]Processing HH static:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 32030/96256 [00:01<00:02, 28402.08it/s]Processing HH static:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34888/96256 [00:01<00:03, 19007.71it/s]Processing HH static:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 37713/96256 [00:01<00:02, 21071.07it/s]Processing HH static:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40475/96256 [00:01<00:02, 22652.86it/s]Processing HH static:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43299/96256 [00:01<00:02, 24086.08it/s]Processing HH static:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46033/96256 [00:01<00:02, 24957.28it/s]Processing HH static:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48753/96256 [00:02<00:01, 25577.84it/s]Processing HH static:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51436/96256 [00:02<00:02, 20725.57it/s]Processing HH static:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53741/96256 [00:03<00:05, 7950.01it/s] Processing HH static:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55442/96256 [00:03<00:04, 8513.63it/s]Processing HH static:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58180/96256 [00:03<00:03, 11059.67it/s]Processing HH static:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60910/96256 [00:03<00:02, 13684.44it/s]Processing HH static:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 63631/96256 [00:03<00:02, 16221.06it/s]Processing HH static:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66381/96256 [00:03<00:01, 18603.61it/s]Processing HH static:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 69131/96256 [00:03<00:01, 20664.88it/s]Processing HH static:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71876/96256 [00:03<00:01, 22351.28it/s]Processing HH static:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 74476/96256 [00:04<00:01, 12117.86it/s]Processing HH static:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 76469/96256 [00:04<00:02, 6770.59it/s] Processing HH static:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77949/96256 [00:05<00:03, 4865.89it/s]Processing HH static:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 79055/96256 [00:06<00:04, 4071.68it/s]Processing HH static:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79899/96256 [00:06<00:03, 4154.04it/s]Processing HH static:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80629/96256 [00:06<00:04, 3785.66it/s]Processing HH static:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81219/96256 [00:06<00:04, 3623.97it/s]Processing HH static:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81721/96256 [00:06<00:04, 3517.88it/s]Processing HH static:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 82165/96256 [00:07<00:04, 3405.18it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 82564/96256 [00:07<00:04, 3102.99it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 82910/96256 [00:07<00:04, 2896.31it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83219/96256 [00:07<00:04, 2790.94it/s]Processing HH static:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83508/96256 [00:07<00:04, 2645.82it/s]Processing HH static:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83776/96256 [00:07<00:04, 2592.03it/s]Processing HH static:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 84036/96256 [00:07<00:04, 2531.62it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84289/96256 [00:07<00:04, 2525.44it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84543/96256 [00:08<00:04, 2528.05it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84797/96256 [00:08<00:04, 2515.91it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 85070/96256 [00:08<00:04, 2571.69it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 85328/96256 [00:08<00:04, 2504.84it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85579/96256 [00:08<00:04, 2443.38it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85824/96256 [00:08<00:04, 2364.07it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 86061/96256 [00:08<00:04, 2292.80it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 86315/96256 [00:08<00:04, 2356.12it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 86564/96256 [00:08<00:04, 2390.82it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86816/96256 [00:09<00:03, 2425.81it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 87060/96256 [00:09<00:03, 2409.14it/s]Processing HH static:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 87302/96256 [00:09<00:03, 2409.13it/s]Processing HH static:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 87544/96256 [00:09<00:03, 2348.76it/s]Processing HH static:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 87780/96256 [00:09<00:03, 2265.82it/s]Processing HH static:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88011/96256 [00:09<00:03, 2275.34it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88240/96256 [00:09<00:03, 2223.68it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88478/96256 [00:09<00:03, 2268.34it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88744/96256 [00:09<00:03, 2374.82it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88983/96256 [00:09<00:03, 2334.02it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89217/96256 [00:10<00:03, 2302.39it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89448/96256 [00:10<00:02, 2295.05it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89702/96256 [00:10<00:02, 2365.90it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89961/96256 [00:10<00:02, 2425.95it/s]Processing HH static:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 90204/96256 [00:10<00:02, 2364.74it/s]Processing HH static:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90441/96256 [00:10<00:02, 2301.15it/s]Processing HH static:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90672/96256 [00:10<00:02, 2246.24it/s]Processing HH static:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90900/96256 [00:10<00:02, 2244.17it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91125/96256 [00:10<00:02, 2226.91it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91356/96256 [00:10<00:02, 2249.26it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91600/96256 [00:11<00:02, 2304.21it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91831/96256 [00:11<00:01, 2290.19it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 92062/96256 [00:11<00:01, 2295.70it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 92292/96256 [00:11<00:01, 2254.96it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 92537/96256 [00:11<00:01, 2310.85it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92769/96256 [00:11<00:01, 2285.64it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92998/96256 [00:11<00:01, 2275.97it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 93226/96256 [00:11<00:01, 2196.13it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 93449/96256 [00:11<00:01, 2202.87it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 93672/96256 [00:12<00:01, 2210.53it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93907/96256 [00:12<00:01, 2248.97it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94146/96256 [00:12<00:00, 2284.61it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94395/96256 [00:12<00:00, 2342.13it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94645/96256 [00:12<00:00, 2388.76it/s]Processing HH static:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94897/96256 [00:12<00:00, 2425.46it/s]Processing HH static:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95140/96256 [00:12<00:00, 2341.91it/s]Processing HH static:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95375/96256 [00:12<00:00, 2329.85it/s]Processing HH static:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95609/96256 [00:12<00:00, 2274.50it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95837/96256 [00:12<00:00, 2255.81it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 96069/96256 [00:13<00:00, 2270.50it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96256/96256 [00:13<00:00, 7337.93it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:01<00:29,  1.98s/it]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:02<00:13,  1.01it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:02<00:08,  1.53it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:05,  2.01it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:03<00:04,  2.37it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:03,  2.83it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:02,  3.04it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:03<00:02,  3.24it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:02,  3.46it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:04<00:01,  3.52it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:04<00:01,  3.60it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:04<00:01,  3.67it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:05<00:00,  3.75it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:05<00:00,  3.71it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:05<00:00,  3.79it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  3.75it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.69it/s]
eval after 0: {'rewards_eval/chosen': '-0.036416', 'rewards_eval/rejected': '-0.033665', 'rewards_eval/accuracies': '0.46875', 'rewards_eval/margins': '-0.0027508', 'logps_eval/rejected': '-118.95', 'logps_eval/chosen': '-139.8', 'loss/eval': '0.70206'}
train stats after 32 examples: {'rewards_train/chosen': '-0.073496', 'rewards_train/rejected': '-0.0069391', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.066557', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-126.64', 'loss/train': '0.73136', 'examples_per_second': '22.074', 'grad_norm': '27.5', 'counters/examples': 32, 'counters/updates': 1}
train stats after 64 examples: {'rewards_train/chosen': '-0.051591', 'rewards_train/rejected': '-0.10921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057621', 'logps_train/rejected': '-157.91', 'logps_train/chosen': '-146.35', 'loss/train': '0.67125', 'examples_per_second': '29.296', 'grad_norm': '31.375', 'counters/examples': 64, 'counters/updates': 2}
train stats after 96 examples: {'rewards_train/chosen': '-0.037464', 'rewards_train/rejected': '0.022317', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.059782', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-116.18', 'loss/train': '0.72731', 'examples_per_second': '31.568', 'grad_norm': '28.25', 'counters/examples': 96, 'counters/updates': 3}
train stats after 128 examples: {'rewards_train/chosen': '-0.092469', 'rewards_train/rejected': '-0.046615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.045854', 'logps_train/rejected': '-149.11', 'logps_train/chosen': '-170.68', 'loss/train': '0.73132', 'examples_per_second': '31.582', 'grad_norm': '46', 'counters/examples': 128, 'counters/updates': 4}
train stats after 160 examples: {'rewards_train/chosen': '-0.061359', 'rewards_train/rejected': '-0.052664', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0086955', 'logps_train/rejected': '-114.35', 'logps_train/chosen': '-154.24', 'loss/train': '0.70145', 'examples_per_second': '32.125', 'grad_norm': '29.25', 'counters/examples': 160, 'counters/updates': 5}
train stats after 192 examples: {'rewards_train/chosen': '-0.032549', 'rewards_train/rejected': '-0.055606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023057', 'logps_train/rejected': '-124.74', 'logps_train/chosen': '-120.77', 'loss/train': '0.68629', 'examples_per_second': '31.209', 'grad_norm': '27.875', 'counters/examples': 192, 'counters/updates': 6}
train stats after 224 examples: {'rewards_train/chosen': '-0.037002', 'rewards_train/rejected': '-0.013986', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023016', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-131.32', 'loss/train': '0.71171', 'examples_per_second': '31.325', 'grad_norm': '32.5', 'counters/examples': 224, 'counters/updates': 7}
train stats after 256 examples: {'rewards_train/chosen': '-0.12065', 'rewards_train/rejected': '-0.04218', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.078472', 'logps_train/rejected': '-155.65', 'logps_train/chosen': '-193.78', 'loss/train': '0.7435', 'examples_per_second': '31.554', 'grad_norm': '39.5', 'counters/examples': 256, 'counters/updates': 8}
train stats after 288 examples: {'rewards_train/chosen': '-0.046819', 'rewards_train/rejected': '-0.02691', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019908', 'logps_train/rejected': '-151.86', 'logps_train/chosen': '-128.37', 'loss/train': '0.70608', 'examples_per_second': '32.112', 'grad_norm': '31.5', 'counters/examples': 288, 'counters/updates': 9}
train stats after 320 examples: {'rewards_train/chosen': '-0.0038252', 'rewards_train/rejected': '-0.093452', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089626', 'logps_train/rejected': '-138.32', 'logps_train/chosen': '-158.41', 'loss/train': '0.65615', 'examples_per_second': '29.945', 'grad_norm': '28.125', 'counters/examples': 320, 'counters/updates': 10}
train stats after 352 examples: {'rewards_train/chosen': '-0.012863', 'rewards_train/rejected': '0.0065202', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.019384', 'logps_train/rejected': '-138.83', 'logps_train/chosen': '-127.87', 'loss/train': '0.70765', 'examples_per_second': '31.732', 'grad_norm': '32.5', 'counters/examples': 352, 'counters/updates': 11}
skipping logging after 384 examples to avoid logging too frequently
train stats after 416 examples: {'rewards_train/chosen': '-0.099809', 'rewards_train/rejected': '-0.037083', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.062726', 'logps_train/rejected': '-111.7', 'logps_train/chosen': '-159.08', 'loss/train': '0.73044', 'examples_per_second': '31.14', 'grad_norm': '31.875', 'counters/examples': 416, 'counters/updates': 13}
train stats after 448 examples: {'rewards_train/chosen': '-0.028996', 'rewards_train/rejected': '-0.06382', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034824', 'logps_train/rejected': '-123.69', 'logps_train/chosen': '-152.78', 'loss/train': '0.67891', 'examples_per_second': '31.636', 'grad_norm': '25.625', 'counters/examples': 448, 'counters/updates': 14}
skipping logging after 480 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '-0.026256', 'rewards_train/rejected': '-0.020053', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0062034', 'logps_train/rejected': '-151.48', 'logps_train/chosen': '-150.38', 'loss/train': '0.70559', 'examples_per_second': '34.01', 'grad_norm': '31.375', 'counters/examples': 512, 'counters/updates': 16}
train stats after 544 examples: {'rewards_train/chosen': '-0.068416', 'rewards_train/rejected': '-0.071277', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0028614', 'logps_train/rejected': '-116.57', 'logps_train/chosen': '-153.2', 'loss/train': '0.7042', 'examples_per_second': '31.604', 'grad_norm': '32.5', 'counters/examples': 544, 'counters/updates': 17}
train stats after 576 examples: {'rewards_train/chosen': '-0.024717', 'rewards_train/rejected': '-0.013184', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011533', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-173.05', 'loss/train': '0.7027', 'examples_per_second': '31.541', 'grad_norm': '33', 'counters/examples': 576, 'counters/updates': 18}
train stats after 608 examples: {'rewards_train/chosen': '-0.063864', 'rewards_train/rejected': '-0.036668', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027196', 'logps_train/rejected': '-117.3', 'logps_train/chosen': '-132.42', 'loss/train': '0.71295', 'examples_per_second': '32.293', 'grad_norm': '28.5', 'counters/examples': 608, 'counters/updates': 19}
train stats after 640 examples: {'rewards_train/chosen': '-0.043665', 'rewards_train/rejected': '-0.055099', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011433', 'logps_train/rejected': '-147.36', 'logps_train/chosen': '-155.63', 'loss/train': '0.69097', 'examples_per_second': '30.899', 'grad_norm': '35.75', 'counters/examples': 640, 'counters/updates': 20}
train stats after 672 examples: {'rewards_train/chosen': '-0.05849', 'rewards_train/rejected': '-0.018168', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040322', 'logps_train/rejected': '-132.12', 'logps_train/chosen': '-160.77', 'loss/train': '0.72353', 'examples_per_second': '33.172', 'grad_norm': '33', 'counters/examples': 672, 'counters/updates': 21}
skipping logging after 704 examples to avoid logging too frequently
train stats after 736 examples: {'rewards_train/chosen': '-0.037602', 'rewards_train/rejected': '-0.057391', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01979', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-122.06', 'loss/train': '0.69027', 'examples_per_second': '33.43', 'grad_norm': '28.375', 'counters/examples': 736, 'counters/updates': 23}
skipping logging after 768 examples to avoid logging too frequently
train stats after 800 examples: {'rewards_train/chosen': '-0.11304', 'rewards_train/rejected': '-0.037914', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.075126', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-193.4', 'loss/train': '0.73887', 'examples_per_second': '30.65', 'grad_norm': '36.75', 'counters/examples': 800, 'counters/updates': 25}
train stats after 832 examples: {'rewards_train/chosen': '-0.040473', 'rewards_train/rejected': '-0.037692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027816', 'logps_train/rejected': '-106.17', 'logps_train/chosen': '-127.11', 'loss/train': '0.70123', 'examples_per_second': '31.603', 'grad_norm': '29.375', 'counters/examples': 832, 'counters/updates': 26}
train stats after 864 examples: {'rewards_train/chosen': '-0.063747', 'rewards_train/rejected': '0.0087884', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.072535', 'logps_train/rejected': '-102.07', 'logps_train/chosen': '-117.09', 'loss/train': '0.73452', 'examples_per_second': '31.336', 'grad_norm': '28.125', 'counters/examples': 864, 'counters/updates': 27}
skipping logging after 896 examples to avoid logging too frequently
train stats after 928 examples: {'rewards_train/chosen': '-0.083132', 'rewards_train/rejected': '-0.0076877', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.075444', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-176.16', 'loss/train': '0.73796', 'examples_per_second': '31.496', 'grad_norm': '34.5', 'counters/examples': 928, 'counters/updates': 29}
train stats after 960 examples: {'rewards_train/chosen': '-0.11178', 'rewards_train/rejected': '-0.0018401', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.10994', 'logps_train/rejected': '-117.18', 'logps_train/chosen': '-124.74', 'loss/train': '0.75711', 'examples_per_second': '31.812', 'grad_norm': '36.25', 'counters/examples': 960, 'counters/updates': 30}
train stats after 992 examples: {'rewards_train/chosen': '-0.07402', 'rewards_train/rejected': '-0.062257', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011763', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-157.12', 'loss/train': '0.7104', 'examples_per_second': '30.727', 'grad_norm': '30.375', 'counters/examples': 992, 'counters/updates': 31}
train stats after 1024 examples: {'rewards_train/chosen': '-0.08856', 'rewards_train/rejected': '-0.068859', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019701', 'logps_train/rejected': '-149.83', 'logps_train/chosen': '-126.61', 'loss/train': '0.70821', 'examples_per_second': '32.223', 'grad_norm': '28.375', 'counters/examples': 1024, 'counters/updates': 32}
skipping logging after 1056 examples to avoid logging too frequently
train stats after 1088 examples: {'rewards_train/chosen': '-0.043007', 'rewards_train/rejected': '-0.0083629', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.034645', 'logps_train/rejected': '-101.75', 'logps_train/chosen': '-169.42', 'loss/train': '0.71644', 'examples_per_second': '30.282', 'grad_norm': '31.5', 'counters/examples': 1088, 'counters/updates': 34}
skipping logging after 1120 examples to avoid logging too frequently
train stats after 1152 examples: {'rewards_train/chosen': '-0.089708', 'rewards_train/rejected': '-0.035424', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054285', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-143.58', 'loss/train': '0.72752', 'examples_per_second': '29.996', 'grad_norm': '29.125', 'counters/examples': 1152, 'counters/updates': 36}
train stats after 1184 examples: {'rewards_train/chosen': '-0.034868', 'rewards_train/rejected': '-0.07394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039072', 'logps_train/rejected': '-159.81', 'logps_train/chosen': '-158.81', 'loss/train': '0.67646', 'examples_per_second': '30.694', 'grad_norm': '31', 'counters/examples': 1184, 'counters/updates': 37}
train stats after 1216 examples: {'rewards_train/chosen': '-0.036918', 'rewards_train/rejected': '-0.025258', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01166', 'logps_train/rejected': '-126.95', 'logps_train/chosen': '-156.99', 'loss/train': '0.70686', 'examples_per_second': '30.789', 'grad_norm': '42.75', 'counters/examples': 1216, 'counters/updates': 38}
train stats after 1248 examples: {'rewards_train/chosen': '-0.065102', 'rewards_train/rejected': '-0.031009', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034092', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-156.23', 'loss/train': '0.71488', 'examples_per_second': '31.643', 'grad_norm': '31.125', 'counters/examples': 1248, 'counters/updates': 39}
train stats after 1280 examples: {'rewards_train/chosen': '-0.060065', 'rewards_train/rejected': '-0.083611', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023547', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-124.63', 'loss/train': '0.68524', 'examples_per_second': '30.522', 'grad_norm': '30.75', 'counters/examples': 1280, 'counters/updates': 40}
train stats after 1312 examples: {'rewards_train/chosen': '-0.028619', 'rewards_train/rejected': '-0.064749', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03613', 'logps_train/rejected': '-138', 'logps_train/chosen': '-146.34', 'loss/train': '0.68119', 'examples_per_second': '30.619', 'grad_norm': '30.375', 'counters/examples': 1312, 'counters/updates': 41}
train stats after 1344 examples: {'rewards_train/chosen': '-0.069688', 'rewards_train/rejected': '-0.021711', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047977', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-229.16', 'loss/train': '0.72461', 'examples_per_second': '31.001', 'grad_norm': '35.5', 'counters/examples': 1344, 'counters/updates': 42}
train stats after 1376 examples: {'rewards_train/chosen': '-0.046291', 'rewards_train/rejected': '-0.052631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0063403', 'logps_train/rejected': '-168.47', 'logps_train/chosen': '-176.35', 'loss/train': '0.70083', 'examples_per_second': '31.448', 'grad_norm': '34.75', 'counters/examples': 1376, 'counters/updates': 43}
skipping logging after 1408 examples to avoid logging too frequently
train stats after 1440 examples: {'rewards_train/chosen': '-0.031072', 'rewards_train/rejected': '-0.053539', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022467', 'logps_train/rejected': '-123.2', 'logps_train/chosen': '-145.26', 'loss/train': '0.68946', 'examples_per_second': '31.464', 'grad_norm': '41.25', 'counters/examples': 1440, 'counters/updates': 45}
train stats after 1472 examples: {'rewards_train/chosen': '0.011344', 'rewards_train/rejected': '-0.04888', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060224', 'logps_train/rejected': '-151.13', 'logps_train/chosen': '-143.2', 'loss/train': '0.66923', 'examples_per_second': '30.142', 'grad_norm': '36.75', 'counters/examples': 1472, 'counters/updates': 46}
train stats after 1504 examples: {'rewards_train/chosen': '0.00055433', 'rewards_train/rejected': '-0.032584', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033138', 'logps_train/rejected': '-116.39', 'logps_train/chosen': '-127.56', 'loss/train': '0.68045', 'examples_per_second': '32.552', 'grad_norm': '28.25', 'counters/examples': 1504, 'counters/updates': 47}
train stats after 1536 examples: {'rewards_train/chosen': '-0.022587', 'rewards_train/rejected': '-0.043036', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020449', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-149.9', 'loss/train': '0.68666', 'examples_per_second': '31.003', 'grad_norm': '27.375', 'counters/examples': 1536, 'counters/updates': 48}
train stats after 1568 examples: {'rewards_train/chosen': '-0.020872', 'rewards_train/rejected': '-0.036458', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015586', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-118.22', 'loss/train': '0.69051', 'examples_per_second': '31.244', 'grad_norm': '28.5', 'counters/examples': 1568, 'counters/updates': 49}
train stats after 1600 examples: {'rewards_train/chosen': '-0.14334', 'rewards_train/rejected': '-0.089033', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.054304', 'logps_train/rejected': '-167.77', 'logps_train/chosen': '-189.43', 'loss/train': '0.73491', 'examples_per_second': '30.178', 'grad_norm': '54.25', 'counters/examples': 1600, 'counters/updates': 50}
skipping logging after 1632 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '-0.033218', 'rewards_train/rejected': '-0.0025214', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030697', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-127.01', 'loss/train': '0.71433', 'examples_per_second': '30.109', 'grad_norm': '28.75', 'counters/examples': 1664, 'counters/updates': 52}
skipping logging after 1696 examples to avoid logging too frequently
train stats after 1728 examples: {'rewards_train/chosen': '0.025666', 'rewards_train/rejected': '-0.048743', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.074409', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-156.53', 'loss/train': '0.66484', 'examples_per_second': '32.726', 'grad_norm': '29.375', 'counters/examples': 1728, 'counters/updates': 54}
train stats after 1760 examples: {'rewards_train/chosen': '-0.081379', 'rewards_train/rejected': '-0.034198', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.047181', 'logps_train/rejected': '-100.21', 'logps_train/chosen': '-182.78', 'loss/train': '0.73065', 'examples_per_second': '31.13', 'grad_norm': '44', 'counters/examples': 1760, 'counters/updates': 55}
skipping logging after 1792 examples to avoid logging too frequently
train stats after 1824 examples: {'rewards_train/chosen': '-0.051753', 'rewards_train/rejected': '-0.069807', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018053', 'logps_train/rejected': '-149.05', 'logps_train/chosen': '-159.94', 'loss/train': '0.69021', 'examples_per_second': '31.67', 'grad_norm': '33', 'counters/examples': 1824, 'counters/updates': 57}
train stats after 1856 examples: {'rewards_train/chosen': '-0.029402', 'rewards_train/rejected': '-0.034216', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0048136', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-123.47', 'loss/train': '0.69495', 'examples_per_second': '30.664', 'grad_norm': '30.375', 'counters/examples': 1856, 'counters/updates': 58}
train stats after 1888 examples: {'rewards_train/chosen': '-0.044076', 'rewards_train/rejected': '-0.054389', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010312', 'logps_train/rejected': '-99.667', 'logps_train/chosen': '-111.47', 'loss/train': '0.69179', 'examples_per_second': '30.426', 'grad_norm': '25.125', 'counters/examples': 1888, 'counters/updates': 59}
train stats after 1920 examples: {'rewards_train/chosen': '0.0054093', 'rewards_train/rejected': '-0.04895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054359', 'logps_train/rejected': '-166.42', 'logps_train/chosen': '-124.44', 'loss/train': '0.66974', 'examples_per_second': '32.635', 'grad_norm': '32.25', 'counters/examples': 1920, 'counters/updates': 60}
skipping logging after 1952 examples to avoid logging too frequently
train stats after 1984 examples: {'rewards_train/chosen': '-0.11877', 'rewards_train/rejected': '-0.077087', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041686', 'logps_train/rejected': '-114.07', 'logps_train/chosen': '-156.99', 'loss/train': '0.72031', 'examples_per_second': '30.72', 'grad_norm': '29.25', 'counters/examples': 1984, 'counters/updates': 62}
train stats after 2016 examples: {'rewards_train/chosen': '-0.049665', 'rewards_train/rejected': '-0.066661', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016996', 'logps_train/rejected': '-162.58', 'logps_train/chosen': '-115.24', 'loss/train': '0.69109', 'examples_per_second': '31.107', 'grad_norm': '28.5', 'counters/examples': 2016, 'counters/updates': 63}
train stats after 2048 examples: {'rewards_train/chosen': '-0.14019', 'rewards_train/rejected': '-0.025713', 'rewards_train/accuracies': '0.1875', 'rewards_train/margins': '-0.11448', 'logps_train/rejected': '-147.88', 'logps_train/chosen': '-183.58', 'loss/train': '0.75834', 'examples_per_second': '31.837', 'grad_norm': '36.5', 'counters/examples': 2048, 'counters/updates': 64}
train stats after 2080 examples: {'rewards_train/chosen': '-0.037061', 'rewards_train/rejected': '-0.064055', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026994', 'logps_train/rejected': '-137.16', 'logps_train/chosen': '-127.17', 'loss/train': '0.68412', 'examples_per_second': '31.208', 'grad_norm': '28.875', 'counters/examples': 2080, 'counters/updates': 65}
train stats after 2112 examples: {'rewards_train/chosen': '-0.054448', 'rewards_train/rejected': '-0.045303', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0091453', 'logps_train/rejected': '-109.85', 'logps_train/chosen': '-105.69', 'loss/train': '0.70618', 'examples_per_second': '30.893', 'grad_norm': '25.375', 'counters/examples': 2112, 'counters/updates': 66}
train stats after 2144 examples: {'rewards_train/chosen': '-0.017956', 'rewards_train/rejected': '-0.018598', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00064274', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-115.41', 'loss/train': '0.6978', 'examples_per_second': '31.984', 'grad_norm': '31.125', 'counters/examples': 2144, 'counters/updates': 67}
skipping logging after 2176 examples to avoid logging too frequently
train stats after 2208 examples: {'rewards_train/chosen': '-0.16138', 'rewards_train/rejected': '-0.011041', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.15034', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-143.16', 'loss/train': '0.78957', 'examples_per_second': '31.104', 'grad_norm': '38', 'counters/examples': 2208, 'counters/updates': 69}
train stats after 2240 examples: {'rewards_train/chosen': '-0.06669', 'rewards_train/rejected': '-0.037989', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028701', 'logps_train/rejected': '-125.88', 'logps_train/chosen': '-155.17', 'loss/train': '0.71448', 'examples_per_second': '31.212', 'grad_norm': '29.125', 'counters/examples': 2240, 'counters/updates': 70}
train stats after 2272 examples: {'rewards_train/chosen': '-0.075158', 'rewards_train/rejected': '-0.041813', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033345', 'logps_train/rejected': '-108.33', 'logps_train/chosen': '-133.28', 'loss/train': '0.71482', 'examples_per_second': '31.971', 'grad_norm': '27.75', 'counters/examples': 2272, 'counters/updates': 71}
train stats after 2304 examples: {'rewards_train/chosen': '-0.064649', 'rewards_train/rejected': '-0.073083', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0084341', 'logps_train/rejected': '-98.155', 'logps_train/chosen': '-128.88', 'loss/train': '0.69681', 'examples_per_second': '30.905', 'grad_norm': '27', 'counters/examples': 2304, 'counters/updates': 72}
train stats after 2336 examples: {'rewards_train/chosen': '0.0057945', 'rewards_train/rejected': '0.0059199', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0001254', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-139.52', 'loss/train': '0.70508', 'examples_per_second': '31.629', 'grad_norm': '35', 'counters/examples': 2336, 'counters/updates': 73}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2400 examples: {'rewards_train/chosen': '-0.043302', 'rewards_train/rejected': '-0.017933', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.025368', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-158.63', 'loss/train': '0.70915', 'examples_per_second': '30.292', 'grad_norm': '29.625', 'counters/examples': 2400, 'counters/updates': 75}
train stats after 2432 examples: {'rewards_train/chosen': '-0.071531', 'rewards_train/rejected': '-0.023614', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047917', 'logps_train/rejected': '-113.55', 'logps_train/chosen': '-112.89', 'loss/train': '0.722', 'examples_per_second': '30.592', 'grad_norm': '30.125', 'counters/examples': 2432, 'counters/updates': 76}
train stats after 2464 examples: {'rewards_train/chosen': '-0.020789', 'rewards_train/rejected': '-0.021083', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0002941', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-128.75', 'loss/train': '0.69753', 'examples_per_second': '32.93', 'grad_norm': '25.75', 'counters/examples': 2464, 'counters/updates': 77}
train stats after 2496 examples: {'rewards_train/chosen': '-0.0038639', 'rewards_train/rejected': '-0.0043568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00049292', 'logps_train/rejected': '-143.62', 'logps_train/chosen': '-142.31', 'loss/train': '0.69948', 'examples_per_second': '31.696', 'grad_norm': '34.75', 'counters/examples': 2496, 'counters/updates': 78}
train stats after 2528 examples: {'rewards_train/chosen': '-0.023041', 'rewards_train/rejected': '-0.055021', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03198', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-160.86', 'loss/train': '0.68559', 'examples_per_second': '30.559', 'grad_norm': '29.25', 'counters/examples': 2528, 'counters/updates': 79}
train stats after 2560 examples: {'rewards_train/chosen': '-0.015345', 'rewards_train/rejected': '-0.039988', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024643', 'logps_train/rejected': '-117.84', 'logps_train/chosen': '-123.06', 'loss/train': '0.68617', 'examples_per_second': '31.802', 'grad_norm': '31', 'counters/examples': 2560, 'counters/updates': 80}
skipping logging after 2592 examples to avoid logging too frequently
train stats after 2624 examples: {'rewards_train/chosen': '-0.0061224', 'rewards_train/rejected': '0.0016417', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0077641', 'logps_train/rejected': '-142.58', 'logps_train/chosen': '-168.94', 'loss/train': '0.69962', 'examples_per_second': '31.595', 'grad_norm': '30.5', 'counters/examples': 2624, 'counters/updates': 82}
train stats after 2656 examples: {'rewards_train/chosen': '-0.054597', 'rewards_train/rejected': '-0.065131', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010534', 'logps_train/rejected': '-145.82', 'logps_train/chosen': '-142.33', 'loss/train': '0.69815', 'examples_per_second': '30.812', 'grad_norm': '35.25', 'counters/examples': 2656, 'counters/updates': 83}
train stats after 2688 examples: {'rewards_train/chosen': '-0.0017959', 'rewards_train/rejected': '-0.055661', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.053865', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-148.78', 'loss/train': '0.67531', 'examples_per_second': '33.184', 'grad_norm': '27.5', 'counters/examples': 2688, 'counters/updates': 84}
train stats after 2720 examples: {'rewards_train/chosen': '-0.011032', 'rewards_train/rejected': '-0.032803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021772', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-160.12', 'loss/train': '0.68843', 'examples_per_second': '32.378', 'grad_norm': '30.375', 'counters/examples': 2720, 'counters/updates': 85}
train stats after 2752 examples: {'rewards_train/chosen': '-0.062719', 'rewards_train/rejected': '-0.036907', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025812', 'logps_train/rejected': '-121.28', 'logps_train/chosen': '-132.08', 'loss/train': '0.71141', 'examples_per_second': '31.91', 'grad_norm': '28.25', 'counters/examples': 2752, 'counters/updates': 86}
train stats after 2784 examples: {'rewards_train/chosen': '-0.043817', 'rewards_train/rejected': '-0.042111', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.001706', 'logps_train/rejected': '-128.71', 'logps_train/chosen': '-135.46', 'loss/train': '0.6984', 'examples_per_second': '30.875', 'grad_norm': '28.5', 'counters/examples': 2784, 'counters/updates': 87}
train stats after 2816 examples: {'rewards_train/chosen': '-0.051617', 'rewards_train/rejected': '-0.035944', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015674', 'logps_train/rejected': '-142.97', 'logps_train/chosen': '-140.2', 'loss/train': '0.7074', 'examples_per_second': '31.301', 'grad_norm': '32', 'counters/examples': 2816, 'counters/updates': 88}
train stats after 2848 examples: {'rewards_train/chosen': '-0.015314', 'rewards_train/rejected': '-0.016883', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0015691', 'logps_train/rejected': '-96.584', 'logps_train/chosen': '-156.71', 'loss/train': '0.69857', 'examples_per_second': '31.545', 'grad_norm': '30.375', 'counters/examples': 2848, 'counters/updates': 89}
train stats after 2880 examples: {'rewards_train/chosen': '-0.081821', 'rewards_train/rejected': '-0.083834', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.002013', 'logps_train/rejected': '-157.72', 'logps_train/chosen': '-160.38', 'loss/train': '0.70191', 'examples_per_second': '31.023', 'grad_norm': '30.625', 'counters/examples': 2880, 'counters/updates': 90}
skipping logging after 2912 examples to avoid logging too frequently
train stats after 2944 examples: {'rewards_train/chosen': '-0.07262', 'rewards_train/rejected': '-0.11008', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037457', 'logps_train/rejected': '-147.82', 'logps_train/chosen': '-161.46', 'loss/train': '0.67959', 'examples_per_second': '32.231', 'grad_norm': '33.25', 'counters/examples': 2944, 'counters/updates': 92}
train stats after 2976 examples: {'rewards_train/chosen': '-0.012379', 'rewards_train/rejected': '-0.013776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0013967', 'logps_train/rejected': '-129.5', 'logps_train/chosen': '-138.77', 'loss/train': '0.69725', 'examples_per_second': '31.939', 'grad_norm': '29.875', 'counters/examples': 2976, 'counters/updates': 93}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3040 examples: {'rewards_train/chosen': '-0.066721', 'rewards_train/rejected': '-0.052041', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.01468', 'logps_train/rejected': '-104.67', 'logps_train/chosen': '-120.65', 'loss/train': '0.70914', 'examples_per_second': '32.052', 'grad_norm': '25.75', 'counters/examples': 3040, 'counters/updates': 95}
train stats after 3072 examples: {'rewards_train/chosen': '-0.060326', 'rewards_train/rejected': '-0.052929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.007397', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-156.19', 'loss/train': '0.70657', 'examples_per_second': '32.473', 'grad_norm': '35', 'counters/examples': 3072, 'counters/updates': 96}
train stats after 3104 examples: {'rewards_train/chosen': '-0.055879', 'rewards_train/rejected': '-0.069073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013194', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-132.14', 'loss/train': '0.69177', 'examples_per_second': '32.868', 'grad_norm': '24.75', 'counters/examples': 3104, 'counters/updates': 97}
skipping logging after 3136 examples to avoid logging too frequently
train stats after 3168 examples: {'rewards_train/chosen': '-0.027382', 'rewards_train/rejected': '-0.06881', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041428', 'logps_train/rejected': '-118.01', 'logps_train/chosen': '-113.29', 'loss/train': '0.67506', 'examples_per_second': '31.692', 'grad_norm': '26.125', 'counters/examples': 3168, 'counters/updates': 99}
skipping logging after 3200 examples to avoid logging too frequently
train stats after 3232 examples: {'rewards_train/chosen': '-0.080507', 'rewards_train/rejected': '-0.1165', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035997', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-164.35', 'loss/train': '0.68281', 'examples_per_second': '30.297', 'grad_norm': '29.5', 'counters/examples': 3232, 'counters/updates': 101}
train stats after 3264 examples: {'rewards_train/chosen': '-0.0018859', 'rewards_train/rejected': '-0.018601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016715', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-146.35', 'loss/train': '0.68796', 'examples_per_second': '31.829', 'grad_norm': '28.875', 'counters/examples': 3264, 'counters/updates': 102}
train stats after 3296 examples: {'rewards_train/chosen': '-0.043674', 'rewards_train/rejected': '-0.014097', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029578', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-148.24', 'loss/train': '0.71283', 'examples_per_second': '31.084', 'grad_norm': '29.125', 'counters/examples': 3296, 'counters/updates': 103}
train stats after 3328 examples: {'rewards_train/chosen': '-0.03688', 'rewards_train/rejected': '-0.0021182', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034762', 'logps_train/rejected': '-131.98', 'logps_train/chosen': '-147.61', 'loss/train': '0.72102', 'examples_per_second': '30.838', 'grad_norm': '31.375', 'counters/examples': 3328, 'counters/updates': 104}
train stats after 3360 examples: {'rewards_train/chosen': '-0.055249', 'rewards_train/rejected': '-0.045203', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010046', 'logps_train/rejected': '-117.76', 'logps_train/chosen': '-128.47', 'loss/train': '0.7035', 'examples_per_second': '32.63', 'grad_norm': '24.875', 'counters/examples': 3360, 'counters/updates': 105}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3424 examples: {'rewards_train/chosen': '-0.073864', 'rewards_train/rejected': '-0.031906', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041958', 'logps_train/rejected': '-117.5', 'logps_train/chosen': '-164.15', 'loss/train': '0.72163', 'examples_per_second': '34.047', 'grad_norm': '34', 'counters/examples': 3424, 'counters/updates': 107}
train stats after 3456 examples: {'rewards_train/chosen': '-0.058197', 'rewards_train/rejected': '-0.090339', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032142', 'logps_train/rejected': '-159.62', 'logps_train/chosen': '-163.83', 'loss/train': '0.68241', 'examples_per_second': '30.667', 'grad_norm': '29.875', 'counters/examples': 3456, 'counters/updates': 108}
train stats after 3488 examples: {'rewards_train/chosen': '-0.064765', 'rewards_train/rejected': '-0.0035688', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.061196', 'logps_train/rejected': '-158.52', 'logps_train/chosen': '-158.28', 'loss/train': '0.72957', 'examples_per_second': '30.213', 'grad_norm': '33.5', 'counters/examples': 3488, 'counters/updates': 109}
train stats after 3520 examples: {'rewards_train/chosen': '-0.07684', 'rewards_train/rejected': '-0.072641', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0041989', 'logps_train/rejected': '-119.96', 'logps_train/chosen': '-143.19', 'loss/train': '0.70358', 'examples_per_second': '29.527', 'grad_norm': '32.25', 'counters/examples': 3520, 'counters/updates': 110}
train stats after 3552 examples: {'rewards_train/chosen': '-0.046475', 'rewards_train/rejected': '-0.057328', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010853', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-148.33', 'loss/train': '0.69612', 'examples_per_second': '31.633', 'grad_norm': '28.375', 'counters/examples': 3552, 'counters/updates': 111}
train stats after 3584 examples: {'rewards_train/chosen': '-0.027105', 'rewards_train/rejected': '-0.0096188', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017487', 'logps_train/rejected': '-116.99', 'logps_train/chosen': '-191.36', 'loss/train': '0.70917', 'examples_per_second': '32.959', 'grad_norm': '31.375', 'counters/examples': 3584, 'counters/updates': 112}
train stats after 3616 examples: {'rewards_train/chosen': '-0.04874', 'rewards_train/rejected': '-0.019802', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028938', 'logps_train/rejected': '-125.95', 'logps_train/chosen': '-149.16', 'loss/train': '0.71268', 'examples_per_second': '32.679', 'grad_norm': '27.125', 'counters/examples': 3616, 'counters/updates': 113}
train stats after 3648 examples: {'rewards_train/chosen': '-0.031035', 'rewards_train/rejected': '-0.068296', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037261', 'logps_train/rejected': '-123.89', 'logps_train/chosen': '-128.7', 'loss/train': '0.6815', 'examples_per_second': '30.948', 'grad_norm': '28.125', 'counters/examples': 3648, 'counters/updates': 114}
train stats after 3680 examples: {'rewards_train/chosen': '-0.067425', 'rewards_train/rejected': '-0.035222', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.032203', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-164.58', 'loss/train': '0.7187', 'examples_per_second': '31.608', 'grad_norm': '34.5', 'counters/examples': 3680, 'counters/updates': 115}
train stats after 3712 examples: {'rewards_train/chosen': '0.010407', 'rewards_train/rejected': '-0.012724', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023132', 'logps_train/rejected': '-131.1', 'logps_train/chosen': '-123.17', 'loss/train': '0.68572', 'examples_per_second': '31.863', 'grad_norm': '30', 'counters/examples': 3712, 'counters/updates': 116}
train stats after 3744 examples: {'rewards_train/chosen': '-0.078229', 'rewards_train/rejected': '-0.01923', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.058999', 'logps_train/rejected': '-145.95', 'logps_train/chosen': '-133', 'loss/train': '0.73227', 'examples_per_second': '31.619', 'grad_norm': '33.5', 'counters/examples': 3744, 'counters/updates': 117}
train stats after 3776 examples: {'rewards_train/chosen': '-0.07585', 'rewards_train/rejected': '-0.025205', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.050644', 'logps_train/rejected': '-122.43', 'logps_train/chosen': '-174.14', 'loss/train': '0.72283', 'examples_per_second': '32.741', 'grad_norm': '31.125', 'counters/examples': 3776, 'counters/updates': 118}
train stats after 3808 examples: {'rewards_train/chosen': '-0.027418', 'rewards_train/rejected': '-0.041862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014443', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-126.58', 'loss/train': '0.68899', 'examples_per_second': '31.692', 'grad_norm': '27.5', 'counters/examples': 3808, 'counters/updates': 119}
train stats after 3840 examples: {'rewards_train/chosen': '-0.046745', 'rewards_train/rejected': '-0.031503', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015242', 'logps_train/rejected': '-104.42', 'logps_train/chosen': '-135.07', 'loss/train': '0.70465', 'examples_per_second': '30.801', 'grad_norm': '27.25', 'counters/examples': 3840, 'counters/updates': 120}
train stats after 3872 examples: {'rewards_train/chosen': '-0.018423', 'rewards_train/rejected': '0.033687', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.05211', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-134.38', 'loss/train': '0.72864', 'examples_per_second': '30.171', 'grad_norm': '29.125', 'counters/examples': 3872, 'counters/updates': 121}
train stats after 3904 examples: {'rewards_train/chosen': '-0.058877', 'rewards_train/rejected': '-0.057435', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0014414', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-156.04', 'loss/train': '0.70636', 'examples_per_second': '31.589', 'grad_norm': '29.5', 'counters/examples': 3904, 'counters/updates': 122}
train stats after 3936 examples: {'rewards_train/chosen': '-0.031549', 'rewards_train/rejected': '-0.05611', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024561', 'logps_train/rejected': '-106.3', 'logps_train/chosen': '-153.54', 'loss/train': '0.68583', 'examples_per_second': '30.183', 'grad_norm': '27.25', 'counters/examples': 3936, 'counters/updates': 123}
skipping logging after 3968 examples to avoid logging too frequently
train stats after 4000 examples: {'rewards_train/chosen': '-0.021514', 'rewards_train/rejected': '-0.0063758', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015138', 'logps_train/rejected': '-126.76', 'logps_train/chosen': '-132.98', 'loss/train': '0.70453', 'examples_per_second': '34.395', 'grad_norm': '32.5', 'counters/examples': 4000, 'counters/updates': 125}
train stats after 4032 examples: {'rewards_train/chosen': '-0.035764', 'rewards_train/rejected': '0.027319', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.063083', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-127.2', 'loss/train': '0.72983', 'examples_per_second': '31.696', 'grad_norm': '38.5', 'counters/examples': 4032, 'counters/updates': 126}
skipping logging after 4064 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '-0.0040804', 'rewards_train/rejected': '-0.020147', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016067', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-148.51', 'loss/train': '0.6908', 'examples_per_second': '32.348', 'grad_norm': '26.375', 'counters/examples': 4096, 'counters/updates': 128}
train stats after 4128 examples: {'rewards_train/chosen': '-0.037832', 'rewards_train/rejected': '0.015566', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.053398', 'logps_train/rejected': '-119.47', 'logps_train/chosen': '-174.21', 'loss/train': '0.72517', 'examples_per_second': '31.627', 'grad_norm': '33.75', 'counters/examples': 4128, 'counters/updates': 129}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4192 examples: {'rewards_train/chosen': '-0.016459', 'rewards_train/rejected': '-0.036547', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.020087', 'logps_train/rejected': '-147.62', 'logps_train/chosen': '-112.63', 'loss/train': '0.68765', 'examples_per_second': '31.668', 'grad_norm': '30.875', 'counters/examples': 4192, 'counters/updates': 131}
train stats after 4224 examples: {'rewards_train/chosen': '-0.058115', 'rewards_train/rejected': '-0.020147', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037968', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-121.69', 'loss/train': '0.71768', 'examples_per_second': '31.339', 'grad_norm': '27.5', 'counters/examples': 4224, 'counters/updates': 132}
train stats after 4256 examples: {'rewards_train/chosen': '-0.012979', 'rewards_train/rejected': '-0.0053411', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0076383', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-131.95', 'loss/train': '0.70082', 'examples_per_second': '30.156', 'grad_norm': '27.75', 'counters/examples': 4256, 'counters/updates': 133}
train stats after 4288 examples: {'rewards_train/chosen': '-0.034436', 'rewards_train/rejected': '-0.058148', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023712', 'logps_train/rejected': '-128.41', 'logps_train/chosen': '-124.62', 'loss/train': '0.69104', 'examples_per_second': '30.632', 'grad_norm': '32.75', 'counters/examples': 4288, 'counters/updates': 134}
skipping logging after 4320 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '-0.066761', 'rewards_train/rejected': '-0.057132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0096292', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-136.96', 'loss/train': '0.70939', 'examples_per_second': '33.069', 'grad_norm': '32.25', 'counters/examples': 4352, 'counters/updates': 136}
skipping logging after 4384 examples to avoid logging too frequently
train stats after 4416 examples: {'rewards_train/chosen': '-0.10391', 'rewards_train/rejected': '-0.077607', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0263', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-137.34', 'loss/train': '0.7162', 'examples_per_second': '30.19', 'grad_norm': '33.25', 'counters/examples': 4416, 'counters/updates': 138}
train stats after 4448 examples: {'rewards_train/chosen': '-0.042326', 'rewards_train/rejected': '-0.043812', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0014853', 'logps_train/rejected': '-128.69', 'logps_train/chosen': '-151.01', 'loss/train': '0.70085', 'examples_per_second': '32.873', 'grad_norm': '31', 'counters/examples': 4448, 'counters/updates': 139}
skipping logging after 4480 examples to avoid logging too frequently
train stats after 4512 examples: {'rewards_train/chosen': '-0.040438', 'rewards_train/rejected': '-0.039988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00044993', 'logps_train/rejected': '-102.51', 'logps_train/chosen': '-158.13', 'loss/train': '0.70018', 'examples_per_second': '32.551', 'grad_norm': '32.5', 'counters/examples': 4512, 'counters/updates': 141}
train stats after 4544 examples: {'rewards_train/chosen': '-0.034522', 'rewards_train/rejected': '-0.077769', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043247', 'logps_train/rejected': '-143.03', 'logps_train/chosen': '-115.03', 'loss/train': '0.67463', 'examples_per_second': '30.772', 'grad_norm': '31.375', 'counters/examples': 4544, 'counters/updates': 142}
train stats after 4576 examples: {'rewards_train/chosen': '-0.0847', 'rewards_train/rejected': '-0.053346', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.031353', 'logps_train/rejected': '-108.27', 'logps_train/chosen': '-147.97', 'loss/train': '0.71857', 'examples_per_second': '31.901', 'grad_norm': '35.75', 'counters/examples': 4576, 'counters/updates': 143}
train stats after 4608 examples: {'rewards_train/chosen': '0.0023184', 'rewards_train/rejected': '0.012429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010111', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-138.52', 'loss/train': '0.70446', 'examples_per_second': '31.53', 'grad_norm': '31.125', 'counters/examples': 4608, 'counters/updates': 144}
train stats after 4640 examples: {'rewards_train/chosen': '-0.037071', 'rewards_train/rejected': '-0.016299', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020772', 'logps_train/rejected': '-155.39', 'logps_train/chosen': '-171.01', 'loss/train': '0.7139', 'examples_per_second': '30.578', 'grad_norm': '31.125', 'counters/examples': 4640, 'counters/updates': 145}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4704 examples: {'rewards_train/chosen': '-0.046663', 'rewards_train/rejected': '-0.061194', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014531', 'logps_train/rejected': '-134.11', 'logps_train/chosen': '-163.07', 'loss/train': '0.69034', 'examples_per_second': '31.649', 'grad_norm': '33.25', 'counters/examples': 4704, 'counters/updates': 147}
train stats after 4736 examples: {'rewards_train/chosen': '-0.022571', 'rewards_train/rejected': '-0.060043', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037471', 'logps_train/rejected': '-139.78', 'logps_train/chosen': '-158.72', 'loss/train': '0.67883', 'examples_per_second': '30.745', 'grad_norm': '28.875', 'counters/examples': 4736, 'counters/updates': 148}
skipping logging after 4768 examples to avoid logging too frequently
train stats after 4800 examples: {'rewards_train/chosen': '-0.065458', 'rewards_train/rejected': '-0.027085', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.038373', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-108.89', 'loss/train': '0.71747', 'examples_per_second': '31.669', 'grad_norm': '28.625', 'counters/examples': 4800, 'counters/updates': 150}
train stats after 4832 examples: {'rewards_train/chosen': '-0.10204', 'rewards_train/rejected': '-0.042674', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.059361', 'logps_train/rejected': '-120.06', 'logps_train/chosen': '-165.57', 'loss/train': '0.72771', 'examples_per_second': '32.291', 'grad_norm': '28.75', 'counters/examples': 4832, 'counters/updates': 151}
train stats after 4864 examples: {'rewards_train/chosen': '-0.052187', 'rewards_train/rejected': '-0.016308', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.035878', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-149.4', 'loss/train': '0.71664', 'examples_per_second': '30.171', 'grad_norm': '38.75', 'counters/examples': 4864, 'counters/updates': 152}
train stats after 4896 examples: {'rewards_train/chosen': '-0.049163', 'rewards_train/rejected': '-0.04307', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0060934', 'logps_train/rejected': '-128.01', 'logps_train/chosen': '-140.36', 'loss/train': '0.70383', 'examples_per_second': '31.663', 'grad_norm': '31.75', 'counters/examples': 4896, 'counters/updates': 153}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4960 examples: {'rewards_train/chosen': '-0.046255', 'rewards_train/rejected': '-0.070683', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024427', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-161.78', 'loss/train': '0.69371', 'examples_per_second': '30.928', 'grad_norm': '31.375', 'counters/examples': 4960, 'counters/updates': 155}
train stats after 4992 examples: {'rewards_train/chosen': '-0.094432', 'rewards_train/rejected': '-0.06809', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026342', 'logps_train/rejected': '-119.91', 'logps_train/chosen': '-155.83', 'loss/train': '0.71525', 'examples_per_second': '30.797', 'grad_norm': '31.5', 'counters/examples': 4992, 'counters/updates': 156}
train stats after 5024 examples: {'rewards_train/chosen': '-0.037877', 'rewards_train/rejected': '-0.035442', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0024347', 'logps_train/rejected': '-147.15', 'logps_train/chosen': '-122.3', 'loss/train': '0.70041', 'examples_per_second': '31.632', 'grad_norm': '36.75', 'counters/examples': 5024, 'counters/updates': 157}
train stats after 5056 examples: {'rewards_train/chosen': '-0.019347', 'rewards_train/rejected': '-0.05656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037212', 'logps_train/rejected': '-132.4', 'logps_train/chosen': '-127.89', 'loss/train': '0.67762', 'examples_per_second': '31.622', 'grad_norm': '28.875', 'counters/examples': 5056, 'counters/updates': 158}
skipping logging after 5088 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '-0.0097752', 'rewards_train/rejected': '-0.072678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062902', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-118.49', 'loss/train': '0.66788', 'examples_per_second': '31.621', 'grad_norm': '26.5', 'counters/examples': 5120, 'counters/updates': 160}
train stats after 5152 examples: {'rewards_train/chosen': '-0.0064998', 'rewards_train/rejected': '-0.089644', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083144', 'logps_train/rejected': '-122.35', 'logps_train/chosen': '-184.55', 'loss/train': '0.66109', 'examples_per_second': '30.564', 'grad_norm': '33', 'counters/examples': 5152, 'counters/updates': 161}
train stats after 5184 examples: {'rewards_train/chosen': '-0.066567', 'rewards_train/rejected': '-0.052119', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014448', 'logps_train/rejected': '-119.47', 'logps_train/chosen': '-131', 'loss/train': '0.70537', 'examples_per_second': '31.41', 'grad_norm': '33.75', 'counters/examples': 5184, 'counters/updates': 162}
train stats after 5216 examples: {'rewards_train/chosen': '-0.050276', 'rewards_train/rejected': '-0.0010173', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.049259', 'logps_train/rejected': '-110.41', 'logps_train/chosen': '-149.05', 'loss/train': '0.72163', 'examples_per_second': '31.32', 'grad_norm': '31', 'counters/examples': 5216, 'counters/updates': 163}
train stats after 5248 examples: {'rewards_train/chosen': '-0.042836', 'rewards_train/rejected': '-0.065313', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022476', 'logps_train/rejected': '-118.28', 'logps_train/chosen': '-110.34', 'loss/train': '0.68644', 'examples_per_second': '31.644', 'grad_norm': '34', 'counters/examples': 5248, 'counters/updates': 164}
train stats after 5280 examples: {'rewards_train/chosen': '-0.036596', 'rewards_train/rejected': '-0.074829', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038232', 'logps_train/rejected': '-122.15', 'logps_train/chosen': '-149.51', 'loss/train': '0.67921', 'examples_per_second': '31.697', 'grad_norm': '27.375', 'counters/examples': 5280, 'counters/updates': 165}
train stats after 5312 examples: {'rewards_train/chosen': '-0.050748', 'rewards_train/rejected': '-0.05909', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0083421', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-129.33', 'loss/train': '0.69602', 'examples_per_second': '30.4', 'grad_norm': '28.125', 'counters/examples': 5312, 'counters/updates': 166}
train stats after 5344 examples: {'rewards_train/chosen': '-0.012551', 'rewards_train/rejected': '-0.049723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037172', 'logps_train/rejected': '-129.79', 'logps_train/chosen': '-190.71', 'loss/train': '0.68545', 'examples_per_second': '31.614', 'grad_norm': '30.375', 'counters/examples': 5344, 'counters/updates': 167}
train stats after 5376 examples: {'rewards_train/chosen': '-0.005707', 'rewards_train/rejected': '0.0020858', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0077928', 'logps_train/rejected': '-113.02', 'logps_train/chosen': '-137.81', 'loss/train': '0.70221', 'examples_per_second': '31.692', 'grad_norm': '41.25', 'counters/examples': 5376, 'counters/updates': 168}
train stats after 5408 examples: {'rewards_train/chosen': '-0.05281', 'rewards_train/rejected': '-0.046958', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0058514', 'logps_train/rejected': '-147.08', 'logps_train/chosen': '-125.12', 'loss/train': '0.70165', 'examples_per_second': '31.88', 'grad_norm': '32.25', 'counters/examples': 5408, 'counters/updates': 169}
train stats after 5440 examples: {'rewards_train/chosen': '-0.022341', 'rewards_train/rejected': '-0.054014', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031673', 'logps_train/rejected': '-96.958', 'logps_train/chosen': '-120.66', 'loss/train': '0.68217', 'examples_per_second': '32.025', 'grad_norm': '27', 'counters/examples': 5440, 'counters/updates': 170}
train stats after 5472 examples: {'rewards_train/chosen': '0.00010016', 'rewards_train/rejected': '-0.077295', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077395', 'logps_train/rejected': '-124.29', 'logps_train/chosen': '-164.05', 'loss/train': '0.66295', 'examples_per_second': '32.785', 'grad_norm': '28.375', 'counters/examples': 5472, 'counters/updates': 171}
skipping logging after 5504 examples to avoid logging too frequently
train stats after 5536 examples: {'rewards_train/chosen': '-0.029583', 'rewards_train/rejected': '-0.011957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.017627', 'logps_train/rejected': '-97.001', 'logps_train/chosen': '-93.3', 'loss/train': '0.7058', 'examples_per_second': '31.653', 'grad_norm': '25.125', 'counters/examples': 5536, 'counters/updates': 173}
train stats after 5568 examples: {'rewards_train/chosen': '-0.021048', 'rewards_train/rejected': '-0.028325', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0072767', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-142.87', 'loss/train': '0.69328', 'examples_per_second': '30.498', 'grad_norm': '27.875', 'counters/examples': 5568, 'counters/updates': 174}
skipping logging after 5600 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '-0.010261', 'rewards_train/rejected': '-0.0055251', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0047355', 'logps_train/rejected': '-133.74', 'logps_train/chosen': '-106.94', 'loss/train': '0.70031', 'examples_per_second': '35.985', 'grad_norm': '28.875', 'counters/examples': 5632, 'counters/updates': 176}
train stats after 5664 examples: {'rewards_train/chosen': '0.011925', 'rewards_train/rejected': '-0.053949', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065874', 'logps_train/rejected': '-169.32', 'logps_train/chosen': '-129.87', 'loss/train': '0.67001', 'examples_per_second': '30.793', 'grad_norm': '31.625', 'counters/examples': 5664, 'counters/updates': 177}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5728 examples: {'rewards_train/chosen': '-0.086837', 'rewards_train/rejected': '-0.043254', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043583', 'logps_train/rejected': '-93.976', 'logps_train/chosen': '-157.17', 'loss/train': '0.71955', 'examples_per_second': '32.767', 'grad_norm': '33.5', 'counters/examples': 5728, 'counters/updates': 179}
skipping logging after 5760 examples to avoid logging too frequently
train stats after 5792 examples: {'rewards_train/chosen': '-0.02673', 'rewards_train/rejected': '-0.041682', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014953', 'logps_train/rejected': '-139.53', 'logps_train/chosen': '-177.43', 'loss/train': '0.69019', 'examples_per_second': '31.726', 'grad_norm': '35.5', 'counters/examples': 5792, 'counters/updates': 181}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5856 examples: {'rewards_train/chosen': '-0.037501', 'rewards_train/rejected': '-0.029788', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0077128', 'logps_train/rejected': '-111.55', 'logps_train/chosen': '-132.27', 'loss/train': '0.70146', 'examples_per_second': '31.465', 'grad_norm': '29.625', 'counters/examples': 5856, 'counters/updates': 183}
train stats after 5888 examples: {'rewards_train/chosen': '-0.069803', 'rewards_train/rejected': '0.00028088', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.070083', 'logps_train/rejected': '-98.069', 'logps_train/chosen': '-136.13', 'loss/train': '0.73356', 'examples_per_second': '31.597', 'grad_norm': '27', 'counters/examples': 5888, 'counters/updates': 184}
skipping logging after 5920 examples to avoid logging too frequently
train stats after 5952 examples: {'rewards_train/chosen': '-0.07284', 'rewards_train/rejected': '-0.040046', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032794', 'logps_train/rejected': '-105.73', 'logps_train/chosen': '-114.89', 'loss/train': '0.71561', 'examples_per_second': '32.346', 'grad_norm': '28.875', 'counters/examples': 5952, 'counters/updates': 186}
train stats after 5984 examples: {'rewards_train/chosen': '-0.0021859', 'rewards_train/rejected': '-0.030075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027889', 'logps_train/rejected': '-115.99', 'logps_train/chosen': '-102.16', 'loss/train': '0.68421', 'examples_per_second': '30.894', 'grad_norm': '27.375', 'counters/examples': 5984, 'counters/updates': 187}
train stats after 6016 examples: {'rewards_train/chosen': '-0.0084009', 'rewards_train/rejected': '-0.021941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01354', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-163.27', 'loss/train': '0.69226', 'examples_per_second': '31.348', 'grad_norm': '31.75', 'counters/examples': 6016, 'counters/updates': 188}
train stats after 6048 examples: {'rewards_train/chosen': '-0.12873', 'rewards_train/rejected': '-0.090544', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.038185', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-162.91', 'loss/train': '0.72015', 'examples_per_second': '31.861', 'grad_norm': '31.625', 'counters/examples': 6048, 'counters/updates': 189}
train stats after 6080 examples: {'rewards_train/chosen': '-0.071901', 'rewards_train/rejected': '0.0016007', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.073502', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-143.4', 'loss/train': '0.74098', 'examples_per_second': '31.616', 'grad_norm': '32.25', 'counters/examples': 6080, 'counters/updates': 190}
skipping logging after 6112 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '-0.047076', 'rewards_train/rejected': '-0.048139', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0010631', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-121.77', 'loss/train': '0.69712', 'examples_per_second': '31.593', 'grad_norm': '27.875', 'counters/examples': 6144, 'counters/updates': 192}
skipping logging after 6176 examples to avoid logging too frequently
train stats after 6208 examples: {'rewards_train/chosen': '-0.071902', 'rewards_train/rejected': '-0.026266', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045635', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-161.09', 'loss/train': '0.72532', 'examples_per_second': '31.86', 'grad_norm': '35', 'counters/examples': 6208, 'counters/updates': 194}
skipping logging after 6240 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '-0.018409', 'rewards_train/rejected': '-0.047227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028818', 'logps_train/rejected': '-84.393', 'logps_train/chosen': '-146.07', 'loss/train': '0.68359', 'examples_per_second': '31.344', 'grad_norm': '28.375', 'counters/examples': 6272, 'counters/updates': 196}
train stats after 6304 examples: {'rewards_train/chosen': '0.0085237', 'rewards_train/rejected': '-0.017578', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026102', 'logps_train/rejected': '-135.14', 'logps_train/chosen': '-159.58', 'loss/train': '0.68757', 'examples_per_second': '31.669', 'grad_norm': '35.25', 'counters/examples': 6304, 'counters/updates': 197}
train stats after 6336 examples: {'rewards_train/chosen': '-0.068678', 'rewards_train/rejected': '-0.020378', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0483', 'logps_train/rejected': '-112.4', 'logps_train/chosen': '-140.64', 'loss/train': '0.72282', 'examples_per_second': '31.632', 'grad_norm': '28.75', 'counters/examples': 6336, 'counters/updates': 198}
train stats after 6368 examples: {'rewards_train/chosen': '-0.027373', 'rewards_train/rejected': '-0.026236', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0011373', 'logps_train/rejected': '-118.4', 'logps_train/chosen': '-166.6', 'loss/train': '0.70128', 'examples_per_second': '31.2', 'grad_norm': '38.5', 'counters/examples': 6368, 'counters/updates': 199}
train stats after 6400 examples: {'rewards_train/chosen': '-0.13223', 'rewards_train/rejected': '-0.023352', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.10888', 'logps_train/rejected': '-133.35', 'logps_train/chosen': '-155.57', 'loss/train': '0.76107', 'examples_per_second': '32.656', 'grad_norm': '38.25', 'counters/examples': 6400, 'counters/updates': 200}
train stats after 6432 examples: {'rewards_train/chosen': '-0.012873', 'rewards_train/rejected': '0.022782', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035655', 'logps_train/rejected': '-121.1', 'logps_train/chosen': '-154.21', 'loss/train': '0.71751', 'examples_per_second': '31.237', 'grad_norm': '31.5', 'counters/examples': 6432, 'counters/updates': 201}
skipping logging after 6464 examples to avoid logging too frequently
train stats after 6496 examples: {'rewards_train/chosen': '-0.033362', 'rewards_train/rejected': '-0.022594', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.010767', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-135.17', 'loss/train': '0.7071', 'examples_per_second': '30.219', 'grad_norm': '31.5', 'counters/examples': 6496, 'counters/updates': 203}
train stats after 6528 examples: {'rewards_train/chosen': '-0.023137', 'rewards_train/rejected': '-0.032921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0097842', 'logps_train/rejected': '-105.15', 'logps_train/chosen': '-104.31', 'loss/train': '0.69307', 'examples_per_second': '31.703', 'grad_norm': '27.125', 'counters/examples': 6528, 'counters/updates': 204}
train stats after 6560 examples: {'rewards_train/chosen': '-0.054164', 'rewards_train/rejected': '-0.07256', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018396', 'logps_train/rejected': '-115.46', 'logps_train/chosen': '-116.74', 'loss/train': '0.68869', 'examples_per_second': '24.994', 'grad_norm': '31.375', 'counters/examples': 6560, 'counters/updates': 205}
train stats after 6592 examples: {'rewards_train/chosen': '-0.10976', 'rewards_train/rejected': '-0.0073142', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.10245', 'logps_train/rejected': '-184.36', 'logps_train/chosen': '-158.43', 'loss/train': '0.75226', 'examples_per_second': '31.731', 'grad_norm': '38', 'counters/examples': 6592, 'counters/updates': 206}
skipping logging after 6624 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '0.02124', 'rewards_train/rejected': '-0.0083177', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029558', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-133.24', 'loss/train': '0.68478', 'examples_per_second': '25.394', 'grad_norm': '30.25', 'counters/examples': 6656, 'counters/updates': 208}
train stats after 6688 examples: {'rewards_train/chosen': '-0.047251', 'rewards_train/rejected': '0.031528', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.078779', 'logps_train/rejected': '-136.74', 'logps_train/chosen': '-159.57', 'loss/train': '0.74152', 'examples_per_second': '31.367', 'grad_norm': '31.375', 'counters/examples': 6688, 'counters/updates': 209}
train stats after 6720 examples: {'rewards_train/chosen': '0.009211', 'rewards_train/rejected': '-0.04569', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.054901', 'logps_train/rejected': '-92.507', 'logps_train/chosen': '-104.58', 'loss/train': '0.66796', 'examples_per_second': '32.912', 'grad_norm': '23.5', 'counters/examples': 6720, 'counters/updates': 210}
train stats after 6752 examples: {'rewards_train/chosen': '-0.14797', 'rewards_train/rejected': '-0.10389', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044072', 'logps_train/rejected': '-125.18', 'logps_train/chosen': '-146.41', 'loss/train': '0.72918', 'examples_per_second': '30.863', 'grad_norm': '45.5', 'counters/examples': 6752, 'counters/updates': 211}
train stats after 6784 examples: {'rewards_train/chosen': '0.0021096', 'rewards_train/rejected': '-0.0023215', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0044311', 'logps_train/rejected': '-110.57', 'logps_train/chosen': '-150.25', 'loss/train': '0.69837', 'examples_per_second': '30.961', 'grad_norm': '30', 'counters/examples': 6784, 'counters/updates': 212}
train stats after 6816 examples: {'rewards_train/chosen': '-0.02367', 'rewards_train/rejected': '-0.062585', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038915', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-144.54', 'loss/train': '0.68277', 'examples_per_second': '31.599', 'grad_norm': '38', 'counters/examples': 6816, 'counters/updates': 213}
train stats after 6848 examples: {'rewards_train/chosen': '-0.013608', 'rewards_train/rejected': '-0.030132', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016524', 'logps_train/rejected': '-156.19', 'logps_train/chosen': '-152.06', 'loss/train': '0.6895', 'examples_per_second': '31.632', 'grad_norm': '32.75', 'counters/examples': 6848, 'counters/updates': 214}
train stats after 6880 examples: {'rewards_train/chosen': '-0.04312', 'rewards_train/rejected': '-0.024652', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.018468', 'logps_train/rejected': '-149.18', 'logps_train/chosen': '-168.35', 'loss/train': '0.71191', 'examples_per_second': '30.067', 'grad_norm': '39.5', 'counters/examples': 6880, 'counters/updates': 215}
train stats after 6912 examples: {'rewards_train/chosen': '-0.036234', 'rewards_train/rejected': '-0.01557', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020664', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-135.48', 'loss/train': '0.71369', 'examples_per_second': '31.229', 'grad_norm': '31.625', 'counters/examples': 6912, 'counters/updates': 216}
skipping logging after 6944 examples to avoid logging too frequently
train stats after 6976 examples: {'rewards_train/chosen': '-0.026758', 'rewards_train/rejected': '-0.075044', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.048285', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-158.22', 'loss/train': '0.67956', 'examples_per_second': '30.514', 'grad_norm': '32.25', 'counters/examples': 6976, 'counters/updates': 218}
train stats after 7008 examples: {'rewards_train/chosen': '-0.078653', 'rewards_train/rejected': '-0.021713', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05694', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-156.22', 'loss/train': '0.72958', 'examples_per_second': '31.579', 'grad_norm': '38.75', 'counters/examples': 7008, 'counters/updates': 219}
train stats after 7040 examples: {'rewards_train/chosen': '-0.02027', 'rewards_train/rejected': '-0.037132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016862', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-146.98', 'loss/train': '0.69031', 'examples_per_second': '31.683', 'grad_norm': '32.25', 'counters/examples': 7040, 'counters/updates': 220}
train stats after 7072 examples: {'rewards_train/chosen': '-0.090045', 'rewards_train/rejected': '-0.039307', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.050738', 'logps_train/rejected': '-103.2', 'logps_train/chosen': '-143.33', 'loss/train': '0.72691', 'examples_per_second': '30.498', 'grad_norm': '33', 'counters/examples': 7072, 'counters/updates': 221}
train stats after 7104 examples: {'rewards_train/chosen': '0.0095107', 'rewards_train/rejected': '-0.064507', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074018', 'logps_train/rejected': '-110.74', 'logps_train/chosen': '-118.33', 'loss/train': '0.66285', 'examples_per_second': '31.05', 'grad_norm': '25.875', 'counters/examples': 7104, 'counters/updates': 222}
train stats after 7136 examples: {'rewards_train/chosen': '-0.069749', 'rewards_train/rejected': '-0.060343', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.009406', 'logps_train/rejected': '-136.33', 'logps_train/chosen': '-173.08', 'loss/train': '0.70172', 'examples_per_second': '24.536', 'grad_norm': '33', 'counters/examples': 7136, 'counters/updates': 223}
train stats after 7168 examples: {'rewards_train/chosen': '-0.031013', 'rewards_train/rejected': '-0.021358', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.009655', 'logps_train/rejected': '-105.69', 'logps_train/chosen': '-140.54', 'loss/train': '0.70173', 'examples_per_second': '30.309', 'grad_norm': '27.875', 'counters/examples': 7168, 'counters/updates': 224}
train stats after 7200 examples: {'rewards_train/chosen': '-0.0803', 'rewards_train/rejected': '-0.03376', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046539', 'logps_train/rejected': '-89.632', 'logps_train/chosen': '-143.66', 'loss/train': '0.72321', 'examples_per_second': '32.785', 'grad_norm': '35.5', 'counters/examples': 7200, 'counters/updates': 225}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7264 examples: {'rewards_train/chosen': '-0.017986', 'rewards_train/rejected': '-0.020018', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0020317', 'logps_train/rejected': '-113.8', 'logps_train/chosen': '-154.03', 'loss/train': '0.69486', 'examples_per_second': '31.724', 'grad_norm': '27.625', 'counters/examples': 7264, 'counters/updates': 227}
train stats after 7296 examples: {'rewards_train/chosen': '-0.0042074', 'rewards_train/rejected': '0.065294', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.069502', 'logps_train/rejected': '-137.49', 'logps_train/chosen': '-156.46', 'loss/train': '0.73348', 'examples_per_second': '32.26', 'grad_norm': '32.5', 'counters/examples': 7296, 'counters/updates': 228}
skipping logging after 7328 examples to avoid logging too frequently
train stats after 7360 examples: {'rewards_train/chosen': '-0.024947', 'rewards_train/rejected': '-0.00096406', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023983', 'logps_train/rejected': '-117.27', 'logps_train/chosen': '-110.32', 'loss/train': '0.70871', 'examples_per_second': '31.836', 'grad_norm': '29.125', 'counters/examples': 7360, 'counters/updates': 230}
skipping logging after 7392 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '0.01484', 'rewards_train/rejected': '0.026848', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.012008', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-134.57', 'loss/train': '0.70469', 'examples_per_second': '31.651', 'grad_norm': '34.25', 'counters/examples': 7424, 'counters/updates': 232}
train stats after 7456 examples: {'rewards_train/chosen': '0.027058', 'rewards_train/rejected': '-0.025598', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052656', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-151.94', 'loss/train': '0.67098', 'examples_per_second': '33.122', 'grad_norm': '30.875', 'counters/examples': 7456, 'counters/updates': 233}
train stats after 7488 examples: {'rewards_train/chosen': '-0.10711', 'rewards_train/rejected': '-0.0053691', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.10174', 'logps_train/rejected': '-119.91', 'logps_train/chosen': '-171.96', 'loss/train': '0.75226', 'examples_per_second': '30.187', 'grad_norm': '40.75', 'counters/examples': 7488, 'counters/updates': 234}
train stats after 7520 examples: {'rewards_train/chosen': '-0.00046872', 'rewards_train/rejected': '-0.054027', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053558', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-105.01', 'loss/train': '0.67321', 'examples_per_second': '30.705', 'grad_norm': '26.625', 'counters/examples': 7520, 'counters/updates': 235}
train stats after 7552 examples: {'rewards_train/chosen': '-0.022274', 'rewards_train/rejected': '-0.064353', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042079', 'logps_train/rejected': '-126.15', 'logps_train/chosen': '-147.52', 'loss/train': '0.67951', 'examples_per_second': '32.63', 'grad_norm': '40', 'counters/examples': 7552, 'counters/updates': 236}
train stats after 7584 examples: {'rewards_train/chosen': '-0.071343', 'rewards_train/rejected': '-0.054971', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016372', 'logps_train/rejected': '-147.71', 'logps_train/chosen': '-131.79', 'loss/train': '0.70628', 'examples_per_second': '31.697', 'grad_norm': '27.5', 'counters/examples': 7584, 'counters/updates': 237}
train stats after 7616 examples: {'rewards_train/chosen': '-0.043663', 'rewards_train/rejected': '-0.035206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.008456', 'logps_train/rejected': '-93.558', 'logps_train/chosen': '-125.42', 'loss/train': '0.70216', 'examples_per_second': '32.74', 'grad_norm': '33', 'counters/examples': 7616, 'counters/updates': 238}
train stats after 7648 examples: {'rewards_train/chosen': '-0.045708', 'rewards_train/rejected': '-0.06251', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016802', 'logps_train/rejected': '-136.53', 'logps_train/chosen': '-144.95', 'loss/train': '0.69573', 'examples_per_second': '32.765', 'grad_norm': '30.5', 'counters/examples': 7648, 'counters/updates': 239}
skipping logging after 7680 examples to avoid logging too frequently
train stats after 7712 examples: {'rewards_train/chosen': '-0.01579', 'rewards_train/rejected': '-0.016693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.000903', 'logps_train/rejected': '-88.064', 'logps_train/chosen': '-137.16', 'loss/train': '0.69611', 'examples_per_second': '34.253', 'grad_norm': '27', 'counters/examples': 7712, 'counters/updates': 241}
train stats after 7744 examples: {'rewards_train/chosen': '-0.092938', 'rewards_train/rejected': '0.0045274', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.097465', 'logps_train/rejected': '-119.34', 'logps_train/chosen': '-173.59', 'loss/train': '0.75069', 'examples_per_second': '30.701', 'grad_norm': '33.75', 'counters/examples': 7744, 'counters/updates': 242}
skipping logging after 7776 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '-0.0050292', 'rewards_train/rejected': '-0.026045', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021016', 'logps_train/rejected': '-103.69', 'logps_train/chosen': '-167.9', 'loss/train': '0.68886', 'examples_per_second': '31.966', 'grad_norm': '28', 'counters/examples': 7808, 'counters/updates': 244}
train stats after 7840 examples: {'rewards_train/chosen': '0.010535', 'rewards_train/rejected': '-0.05369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064225', 'logps_train/rejected': '-155.23', 'logps_train/chosen': '-158.09', 'loss/train': '0.66853', 'examples_per_second': '31.686', 'grad_norm': '29.5', 'counters/examples': 7840, 'counters/updates': 245}
train stats after 7872 examples: {'rewards_train/chosen': '-0.0092307', 'rewards_train/rejected': '-0.056054', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046823', 'logps_train/rejected': '-112.25', 'logps_train/chosen': '-122.6', 'loss/train': '0.67397', 'examples_per_second': '29.82', 'grad_norm': '28.5', 'counters/examples': 7872, 'counters/updates': 246}
train stats after 7904 examples: {'rewards_train/chosen': '0.0047817', 'rewards_train/rejected': '0.037215', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.032433', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-100.49', 'loss/train': '0.71256', 'examples_per_second': '29.631', 'grad_norm': '26.625', 'counters/examples': 7904, 'counters/updates': 247}
skipping logging after 7936 examples to avoid logging too frequently
train stats after 7968 examples: {'rewards_train/chosen': '-0.045074', 'rewards_train/rejected': '0.023627', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.068701', 'logps_train/rejected': '-114.2', 'logps_train/chosen': '-141.76', 'loss/train': '0.7325', 'examples_per_second': '33.338', 'grad_norm': '31.125', 'counters/examples': 7968, 'counters/updates': 249}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8032 examples: {'rewards_train/chosen': '0.069598', 'rewards_train/rejected': '0.0045752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065022', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-146.86', 'loss/train': '0.66702', 'examples_per_second': '31.248', 'grad_norm': '27.625', 'counters/examples': 8032, 'counters/updates': 251}
train stats after 8064 examples: {'rewards_train/chosen': '-0.02619', 'rewards_train/rejected': '-0.066007', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039817', 'logps_train/rejected': '-134.7', 'logps_train/chosen': '-162.18', 'loss/train': '0.69236', 'examples_per_second': '30.71', 'grad_norm': '31.125', 'counters/examples': 8064, 'counters/updates': 252}
skipping logging after 8096 examples to avoid logging too frequently
train stats after 8128 examples: {'rewards_train/chosen': '-0.090439', 'rewards_train/rejected': '-0.072301', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018138', 'logps_train/rejected': '-145.11', 'logps_train/chosen': '-150.98', 'loss/train': '0.70648', 'examples_per_second': '31.573', 'grad_norm': '36.5', 'counters/examples': 8128, 'counters/updates': 254}
train stats after 8160 examples: {'rewards_train/chosen': '-0.020457', 'rewards_train/rejected': '-0.021454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.00099747', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-152.89', 'loss/train': '0.69849', 'examples_per_second': '30.256', 'grad_norm': '27.5', 'counters/examples': 8160, 'counters/updates': 255}
train stats after 8192 examples: {'rewards_train/chosen': '0.0094217', 'rewards_train/rejected': '-0.043454', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052876', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-189.47', 'loss/train': '0.67295', 'examples_per_second': '30.372', 'grad_norm': '32.5', 'counters/examples': 8192, 'counters/updates': 256}
train stats after 8224 examples: {'rewards_train/chosen': '-0.054949', 'rewards_train/rejected': '-0.035257', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019692', 'logps_train/rejected': '-115.18', 'logps_train/chosen': '-138.78', 'loss/train': '0.70807', 'examples_per_second': '32.46', 'grad_norm': '26.875', 'counters/examples': 8224, 'counters/updates': 257}
train stats after 8256 examples: {'rewards_train/chosen': '-0.066443', 'rewards_train/rejected': '-0.063637', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0028055', 'logps_train/rejected': '-156.75', 'logps_train/chosen': '-152.27', 'loss/train': '0.70035', 'examples_per_second': '30.8', 'grad_norm': '32.5', 'counters/examples': 8256, 'counters/updates': 258}
train stats after 8288 examples: {'rewards_train/chosen': '0.0060493', 'rewards_train/rejected': '0.011467', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.005418', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-136.05', 'loss/train': '0.71095', 'examples_per_second': '32.221', 'grad_norm': '28.5', 'counters/examples': 8288, 'counters/updates': 259}
train stats after 8320 examples: {'rewards_train/chosen': '-0.072114', 'rewards_train/rejected': '-0.055006', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.017108', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-132.03', 'loss/train': '0.70492', 'examples_per_second': '32.191', 'grad_norm': '25.625', 'counters/examples': 8320, 'counters/updates': 260}
skipping logging after 8352 examples to avoid logging too frequently
train stats after 8384 examples: {'rewards_train/chosen': '-0.041911', 'rewards_train/rejected': '-0.019937', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.021975', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-140.95', 'loss/train': '0.71297', 'examples_per_second': '31.024', 'grad_norm': '34.5', 'counters/examples': 8384, 'counters/updates': 262}
train stats after 8416 examples: {'rewards_train/chosen': '0.062149', 'rewards_train/rejected': '-0.078461', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14061', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-119.48', 'loss/train': '0.63218', 'examples_per_second': '31.676', 'grad_norm': '33.75', 'counters/examples': 8416, 'counters/updates': 263}
train stats after 8448 examples: {'rewards_train/chosen': '-0.0518', 'rewards_train/rejected': '-0.042605', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0091942', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-178.59', 'loss/train': '0.70774', 'examples_per_second': '30.1', 'grad_norm': '30.75', 'counters/examples': 8448, 'counters/updates': 264}
train stats after 8480 examples: {'rewards_train/chosen': '-0.012256', 'rewards_train/rejected': '-0.032803', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020547', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-150', 'loss/train': '0.6908', 'examples_per_second': '31.616', 'grad_norm': '34.75', 'counters/examples': 8480, 'counters/updates': 265}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8544 examples: {'rewards_train/chosen': '0.017881', 'rewards_train/rejected': '0.0071757', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010705', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-160.08', 'loss/train': '0.69416', 'examples_per_second': '30.537', 'grad_norm': '29.5', 'counters/examples': 8544, 'counters/updates': 267}
train stats after 8576 examples: {'rewards_train/chosen': '-0.033148', 'rewards_train/rejected': '0.015017', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048165', 'logps_train/rejected': '-115.51', 'logps_train/chosen': '-147.43', 'loss/train': '0.72449', 'examples_per_second': '31.693', 'grad_norm': '30.75', 'counters/examples': 8576, 'counters/updates': 268}
train stats after 8608 examples: {'rewards_train/chosen': '-0.008774', 'rewards_train/rejected': '-0.0089251', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.00015107', 'logps_train/rejected': '-110.74', 'logps_train/chosen': '-129.21', 'loss/train': '0.69634', 'examples_per_second': '32.087', 'grad_norm': '34.5', 'counters/examples': 8608, 'counters/updates': 269}
train stats after 8640 examples: {'rewards_train/chosen': '0.018243', 'rewards_train/rejected': '0.027371', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0091283', 'logps_train/rejected': '-91.169', 'logps_train/chosen': '-129.97', 'loss/train': '0.7031', 'examples_per_second': '31.354', 'grad_norm': '35.5', 'counters/examples': 8640, 'counters/updates': 270}
train stats after 8672 examples: {'rewards_train/chosen': '-0.0017784', 'rewards_train/rejected': '-0.12682', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12504', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-142.95', 'loss/train': '0.64112', 'examples_per_second': '31.611', 'grad_norm': '27.75', 'counters/examples': 8672, 'counters/updates': 271}
train stats after 8704 examples: {'rewards_train/chosen': '-0.002999', 'rewards_train/rejected': '0.0076566', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010656', 'logps_train/rejected': '-101.64', 'logps_train/chosen': '-109.43', 'loss/train': '0.70093', 'examples_per_second': '31.994', 'grad_norm': '25.125', 'counters/examples': 8704, 'counters/updates': 272}
skipping logging after 8736 examples to avoid logging too frequently
train stats after 8768 examples: {'rewards_train/chosen': '-0.043265', 'rewards_train/rejected': '-0.059702', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.016436', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-132.4', 'loss/train': '0.69129', 'examples_per_second': '30.633', 'grad_norm': '31.625', 'counters/examples': 8768, 'counters/updates': 274}
train stats after 8800 examples: {'rewards_train/chosen': '0.0017427', 'rewards_train/rejected': '-0.015582', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017324', 'logps_train/rejected': '-140.89', 'logps_train/chosen': '-138.74', 'loss/train': '0.68891', 'examples_per_second': '31.592', 'grad_norm': '32', 'counters/examples': 8800, 'counters/updates': 275}
train stats after 8832 examples: {'rewards_train/chosen': '-0.075292', 'rewards_train/rejected': '-0.01485', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.060441', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-159.9', 'loss/train': '0.72958', 'examples_per_second': '32.494', 'grad_norm': '31.875', 'counters/examples': 8832, 'counters/updates': 276}
skipping logging after 8864 examples to avoid logging too frequently
train stats after 8896 examples: {'rewards_train/chosen': '-0.040243', 'rewards_train/rejected': '-0.049863', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0096194', 'logps_train/rejected': '-102.12', 'logps_train/chosen': '-152.58', 'loss/train': '0.69617', 'examples_per_second': '31.627', 'grad_norm': '33.25', 'counters/examples': 8896, 'counters/updates': 278}
train stats after 8928 examples: {'rewards_train/chosen': '-0.094043', 'rewards_train/rejected': '-0.0066942', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.087349', 'logps_train/rejected': '-112.1', 'logps_train/chosen': '-138.69', 'loss/train': '0.74801', 'examples_per_second': '32.786', 'grad_norm': '33', 'counters/examples': 8928, 'counters/updates': 279}
train stats after 8960 examples: {'rewards_train/chosen': '-0.039764', 'rewards_train/rejected': '-0.014884', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.02488', 'logps_train/rejected': '-99.342', 'logps_train/chosen': '-121.9', 'loss/train': '0.71257', 'examples_per_second': '30.593', 'grad_norm': '29.125', 'counters/examples': 8960, 'counters/updates': 280}
train stats after 8992 examples: {'rewards_train/chosen': '-0.046539', 'rewards_train/rejected': '0.0058167', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.052356', 'logps_train/rejected': '-120.85', 'logps_train/chosen': '-144.84', 'loss/train': '0.72722', 'examples_per_second': '30.29', 'grad_norm': '42', 'counters/examples': 8992, 'counters/updates': 281}
train stats after 9024 examples: {'rewards_train/chosen': '-0.053805', 'rewards_train/rejected': '-0.019104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034701', 'logps_train/rejected': '-118.29', 'logps_train/chosen': '-155.02', 'loss/train': '0.71886', 'examples_per_second': '30.855', 'grad_norm': '31', 'counters/examples': 9024, 'counters/updates': 282}
skipping logging after 9056 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '0.003821', 'rewards_train/rejected': '0.0053421', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.001521', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-122.95', 'loss/train': '0.69816', 'examples_per_second': '31.327', 'grad_norm': '31', 'counters/examples': 9088, 'counters/updates': 284}
train stats after 9120 examples: {'rewards_train/chosen': '-0.014905', 'rewards_train/rejected': '-0.0092162', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0056884', 'logps_train/rejected': '-101.96', 'logps_train/chosen': '-145.59', 'loss/train': '0.70151', 'examples_per_second': '31.188', 'grad_norm': '27.75', 'counters/examples': 9120, 'counters/updates': 285}
train stats after 9152 examples: {'rewards_train/chosen': '-0.040328', 'rewards_train/rejected': '0.032406', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.072734', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-128.8', 'loss/train': '0.73723', 'examples_per_second': '30.9', 'grad_norm': '31.625', 'counters/examples': 9152, 'counters/updates': 286}
train stats after 9184 examples: {'rewards_train/chosen': '-0.084975', 'rewards_train/rejected': '-0.11018', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025207', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-147.7', 'loss/train': '0.68399', 'examples_per_second': '31.777', 'grad_norm': '34.25', 'counters/examples': 9184, 'counters/updates': 287}
train stats after 9216 examples: {'rewards_train/chosen': '-0.079121', 'rewards_train/rejected': '-0.053998', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025123', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-157', 'loss/train': '0.71592', 'examples_per_second': '31.627', 'grad_norm': '32.25', 'counters/examples': 9216, 'counters/updates': 288}
train stats after 9248 examples: {'rewards_train/chosen': '-0.052512', 'rewards_train/rejected': '-0.0094812', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.043031', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-182.7', 'loss/train': '0.72146', 'examples_per_second': '31.321', 'grad_norm': '32.25', 'counters/examples': 9248, 'counters/updates': 289}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9312 examples: {'rewards_train/chosen': '-0.019819', 'rewards_train/rejected': '-0.017316', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0025023', 'logps_train/rejected': '-125.05', 'logps_train/chosen': '-134.37', 'loss/train': '0.70321', 'examples_per_second': '31.166', 'grad_norm': '34.25', 'counters/examples': 9312, 'counters/updates': 291}
train stats after 9344 examples: {'rewards_train/chosen': '-0.0086367', 'rewards_train/rejected': '-0.027883', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.019246', 'logps_train/rejected': '-128.39', 'logps_train/chosen': '-152.3', 'loss/train': '0.69208', 'examples_per_second': '31.526', 'grad_norm': '28.625', 'counters/examples': 9344, 'counters/updates': 292}
skipping logging after 9376 examples to avoid logging too frequently
train stats after 9408 examples: {'rewards_train/chosen': '-0.051808', 'rewards_train/rejected': '-0.027035', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.024773', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-124.47', 'loss/train': '0.70967', 'examples_per_second': '31.56', 'grad_norm': '29.125', 'counters/examples': 9408, 'counters/updates': 294}
train stats after 9440 examples: {'rewards_train/chosen': '-0.056633', 'rewards_train/rejected': '-0.082916', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026283', 'logps_train/rejected': '-104.22', 'logps_train/chosen': '-124.39', 'loss/train': '0.68433', 'examples_per_second': '30.692', 'grad_norm': '28.125', 'counters/examples': 9440, 'counters/updates': 295}
train stats after 9472 examples: {'rewards_train/chosen': '-0.044457', 'rewards_train/rejected': '0.0022647', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046721', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-103.69', 'loss/train': '0.71926', 'examples_per_second': '31.596', 'grad_norm': '28.75', 'counters/examples': 9472, 'counters/updates': 296}
skipping logging after 9504 examples to avoid logging too frequently
train stats after 9536 examples: {'rewards_train/chosen': '-0.064051', 'rewards_train/rejected': '-0.049273', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014778', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-131.98', 'loss/train': '0.70728', 'examples_per_second': '31.454', 'grad_norm': '29.125', 'counters/examples': 9536, 'counters/updates': 298}
train stats after 9568 examples: {'rewards_train/chosen': '-0.034247', 'rewards_train/rejected': '-0.051874', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017627', 'logps_train/rejected': '-133.12', 'logps_train/chosen': '-110.59', 'loss/train': '0.69037', 'examples_per_second': '31.593', 'grad_norm': '28.75', 'counters/examples': 9568, 'counters/updates': 299}
train stats after 9600 examples: {'rewards_train/chosen': '-0.07396', 'rewards_train/rejected': '-0.034425', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039535', 'logps_train/rejected': '-108.86', 'logps_train/chosen': '-171.83', 'loss/train': '0.7178', 'examples_per_second': '31.336', 'grad_norm': '33', 'counters/examples': 9600, 'counters/updates': 300}
train stats after 9632 examples: {'rewards_train/chosen': '0.00083625', 'rewards_train/rejected': '-0.043543', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044379', 'logps_train/rejected': '-127.76', 'logps_train/chosen': '-187.33', 'loss/train': '0.6802', 'examples_per_second': '31.555', 'grad_norm': '30', 'counters/examples': 9632, 'counters/updates': 301}
train stats after 9664 examples: {'rewards_train/chosen': '0.0021591', 'rewards_train/rejected': '-0.0027721', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0049312', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-175.18', 'loss/train': '0.69646', 'examples_per_second': '30.998', 'grad_norm': '34.25', 'counters/examples': 9664, 'counters/updates': 302}
train stats after 9696 examples: {'rewards_train/chosen': '-0.057414', 'rewards_train/rejected': '-0.0042689', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.053146', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-140.03', 'loss/train': '0.72232', 'examples_per_second': '30.034', 'grad_norm': '31.875', 'counters/examples': 9696, 'counters/updates': 303}
train stats after 9728 examples: {'rewards_train/chosen': '-0.060071', 'rewards_train/rejected': '-0.015907', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.044164', 'logps_train/rejected': '-122.25', 'logps_train/chosen': '-146.4', 'loss/train': '0.71861', 'examples_per_second': '31.797', 'grad_norm': '28.25', 'counters/examples': 9728, 'counters/updates': 304}
train stats after 9760 examples: {'rewards_train/chosen': '-0.057427', 'rewards_train/rejected': '-0.042016', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015411', 'logps_train/rejected': '-203.19', 'logps_train/chosen': '-170.01', 'loss/train': '0.70971', 'examples_per_second': '31.725', 'grad_norm': '35', 'counters/examples': 9760, 'counters/updates': 305}
train stats after 9792 examples: {'rewards_train/chosen': '-0.031869', 'rewards_train/rejected': '0.01505', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.04692', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-162.3', 'loss/train': '0.72395', 'examples_per_second': '30.935', 'grad_norm': '34.5', 'counters/examples': 9792, 'counters/updates': 306}
train stats after 9824 examples: {'rewards_train/chosen': '-0.051139', 'rewards_train/rejected': '-0.063365', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012226', 'logps_train/rejected': '-89.387', 'logps_train/chosen': '-116.78', 'loss/train': '0.69237', 'examples_per_second': '32.534', 'grad_norm': '23.25', 'counters/examples': 9824, 'counters/updates': 307}
train stats after 9856 examples: {'rewards_train/chosen': '-0.010546', 'rewards_train/rejected': '0.030564', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041109', 'logps_train/rejected': '-92.36', 'logps_train/chosen': '-106.77', 'loss/train': '0.71995', 'examples_per_second': '32.204', 'grad_norm': '38', 'counters/examples': 9856, 'counters/updates': 308}
train stats after 9888 examples: {'rewards_train/chosen': '-0.050131', 'rewards_train/rejected': '0.010101', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.060232', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-126.53', 'loss/train': '0.72788', 'examples_per_second': '31.042', 'grad_norm': '32.5', 'counters/examples': 9888, 'counters/updates': 309}
train stats after 9920 examples: {'rewards_train/chosen': '-0.064321', 'rewards_train/rejected': '-0.012699', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.051622', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-101.19', 'loss/train': '0.72295', 'examples_per_second': '32.195', 'grad_norm': '25.625', 'counters/examples': 9920, 'counters/updates': 310}
train stats after 9952 examples: {'rewards_train/chosen': '-0.041856', 'rewards_train/rejected': '-0.036888', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0049678', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-158.52', 'loss/train': '0.7022', 'examples_per_second': '31.619', 'grad_norm': '30.375', 'counters/examples': 9952, 'counters/updates': 311}
train stats after 9984 examples: {'rewards_train/chosen': '-0.01398', 'rewards_train/rejected': '-0.028965', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014985', 'logps_train/rejected': '-119.96', 'logps_train/chosen': '-114.45', 'loss/train': '0.69193', 'examples_per_second': '30.938', 'grad_norm': '27.375', 'counters/examples': 9984, 'counters/updates': 312}
train stats after 10016 examples: {'rewards_train/chosen': '-0.010701', 'rewards_train/rejected': '-0.038308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027607', 'logps_train/rejected': '-101.39', 'logps_train/chosen': '-126.55', 'loss/train': '0.68208', 'examples_per_second': '32.066', 'grad_norm': '26.25', 'counters/examples': 10016, 'counters/updates': 313}
train stats after 10048 examples: {'rewards_train/chosen': '-0.11743', 'rewards_train/rejected': '-0.016904', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.10053', 'logps_train/rejected': '-122.79', 'logps_train/chosen': '-138.66', 'loss/train': '0.752', 'examples_per_second': '30.185', 'grad_norm': '37.75', 'counters/examples': 10048, 'counters/updates': 314}
train stats after 10080 examples: {'rewards_train/chosen': '-0.052827', 'rewards_train/rejected': '-0.0062062', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.046621', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-169.8', 'loss/train': '0.72764', 'examples_per_second': '31.569', 'grad_norm': '32.25', 'counters/examples': 10080, 'counters/updates': 315}
train stats after 10112 examples: {'rewards_train/chosen': '-0.069418', 'rewards_train/rejected': '-0.042283', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.027135', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-173.1', 'loss/train': '0.71081', 'examples_per_second': '31.602', 'grad_norm': '32', 'counters/examples': 10112, 'counters/updates': 316}
train stats after 10144 examples: {'rewards_train/chosen': '-0.041821', 'rewards_train/rejected': '-0.051549', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0097276', 'logps_train/rejected': '-110.25', 'logps_train/chosen': '-160.33', 'loss/train': '0.69209', 'examples_per_second': '30.909', 'grad_norm': '26', 'counters/examples': 10144, 'counters/updates': 317}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10208 examples: {'rewards_train/chosen': '-0.045683', 'rewards_train/rejected': '-0.042371', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0033124', 'logps_train/rejected': '-141.04', 'logps_train/chosen': '-173.23', 'loss/train': '0.69937', 'examples_per_second': '30.616', 'grad_norm': '33.75', 'counters/examples': 10208, 'counters/updates': 319}
train stats after 10240 examples: {'rewards_train/chosen': '-0.046147', 'rewards_train/rejected': '-0.013114', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033033', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-154.41', 'loss/train': '0.7155', 'examples_per_second': '31.538', 'grad_norm': '30.875', 'counters/examples': 10240, 'counters/updates': 320}
train stats after 10272 examples: {'rewards_train/chosen': '0.034162', 'rewards_train/rejected': '-0.077111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11127', 'logps_train/rejected': '-143.26', 'logps_train/chosen': '-177.43', 'loss/train': '0.64985', 'examples_per_second': '30.134', 'grad_norm': '26.625', 'counters/examples': 10272, 'counters/updates': 321}
train stats after 10304 examples: {'rewards_train/chosen': '0.006057', 'rewards_train/rejected': '0.01128', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0052235', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-121.17', 'loss/train': '0.69951', 'examples_per_second': '31.619', 'grad_norm': '28.625', 'counters/examples': 10304, 'counters/updates': 322}
train stats after 10336 examples: {'rewards_train/chosen': '0.013716', 'rewards_train/rejected': '0.020616', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0069001', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-168.11', 'loss/train': '0.70141', 'examples_per_second': '31.643', 'grad_norm': '32.5', 'counters/examples': 10336, 'counters/updates': 323}
train stats after 10368 examples: {'rewards_train/chosen': '-0.022371', 'rewards_train/rejected': '-0.050627', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028255', 'logps_train/rejected': '-94.64', 'logps_train/chosen': '-125.18', 'loss/train': '0.68373', 'examples_per_second': '32.553', 'grad_norm': '31.875', 'counters/examples': 10368, 'counters/updates': 324}
skipping logging after 10400 examples to avoid logging too frequently
train stats after 10432 examples: {'rewards_train/chosen': '-0.045284', 'rewards_train/rejected': '-0.081951', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.036666', 'logps_train/rejected': '-154.94', 'logps_train/chosen': '-158.87', 'loss/train': '0.68225', 'examples_per_second': '31.31', 'grad_norm': '32.5', 'counters/examples': 10432, 'counters/updates': 326}
train stats after 10464 examples: {'rewards_train/chosen': '-0.0056623', 'rewards_train/rejected': '-0.02487', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019207', 'logps_train/rejected': '-134.73', 'logps_train/chosen': '-112.96', 'loss/train': '0.68821', 'examples_per_second': '31.495', 'grad_norm': '27.5', 'counters/examples': 10464, 'counters/updates': 327}
skipping logging after 10496 examples to avoid logging too frequently
train stats after 10528 examples: {'rewards_train/chosen': '0.0059775', 'rewards_train/rejected': '-0.036385', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042363', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-151.76', 'loss/train': '0.68054', 'examples_per_second': '31.402', 'grad_norm': '28.875', 'counters/examples': 10528, 'counters/updates': 329}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10592 examples: {'rewards_train/chosen': '0.0083305', 'rewards_train/rejected': '0.028556', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020226', 'logps_train/rejected': '-115.18', 'logps_train/chosen': '-138.89', 'loss/train': '0.7095', 'examples_per_second': '30.778', 'grad_norm': '34.5', 'counters/examples': 10592, 'counters/updates': 331}
train stats after 10624 examples: {'rewards_train/chosen': '-0.078521', 'rewards_train/rejected': '0.012032', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.090553', 'logps_train/rejected': '-102.4', 'logps_train/chosen': '-110.3', 'loss/train': '0.74609', 'examples_per_second': '31.362', 'grad_norm': '78', 'counters/examples': 10624, 'counters/updates': 332}
train stats after 10656 examples: {'rewards_train/chosen': '-0.0090415', 'rewards_train/rejected': '0.064272', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.073313', 'logps_train/rejected': '-91.014', 'logps_train/chosen': '-119.29', 'loss/train': '0.7333', 'examples_per_second': '30.15', 'grad_norm': '30.875', 'counters/examples': 10656, 'counters/updates': 333}
train stats after 10688 examples: {'rewards_train/chosen': '-0.025485', 'rewards_train/rejected': '-0.042576', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017091', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-142.25', 'loss/train': '0.6908', 'examples_per_second': '31.525', 'grad_norm': '26.5', 'counters/examples': 10688, 'counters/updates': 334}
train stats after 10720 examples: {'rewards_train/chosen': '-0.044364', 'rewards_train/rejected': '-0.061948', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017585', 'logps_train/rejected': '-134.5', 'logps_train/chosen': '-145.12', 'loss/train': '0.68943', 'examples_per_second': '30.165', 'grad_norm': '29.125', 'counters/examples': 10720, 'counters/updates': 335}
skipping logging after 10752 examples to avoid logging too frequently
train stats after 10784 examples: {'rewards_train/chosen': '-0.05489', 'rewards_train/rejected': '-0.080816', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.025926', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-172.1', 'loss/train': '0.69566', 'examples_per_second': '31.624', 'grad_norm': '34', 'counters/examples': 10784, 'counters/updates': 337}
train stats after 10816 examples: {'rewards_train/chosen': '-0.1099', 'rewards_train/rejected': '-0.052881', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057023', 'logps_train/rejected': '-193.96', 'logps_train/chosen': '-153.22', 'loss/train': '0.74109', 'examples_per_second': '31.155', 'grad_norm': '50.5', 'counters/examples': 10816, 'counters/updates': 338}
train stats after 10848 examples: {'rewards_train/chosen': '-0.03978', 'rewards_train/rejected': '-0.065368', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025588', 'logps_train/rejected': '-170.13', 'logps_train/chosen': '-135.68', 'loss/train': '0.6906', 'examples_per_second': '31.604', 'grad_norm': '30.25', 'counters/examples': 10848, 'counters/updates': 339}
train stats after 10880 examples: {'rewards_train/chosen': '-0.059956', 'rewards_train/rejected': '0.050168', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.11012', 'logps_train/rejected': '-172.29', 'logps_train/chosen': '-188.06', 'loss/train': '0.76764', 'examples_per_second': '30.67', 'grad_norm': '46.25', 'counters/examples': 10880, 'counters/updates': 340}
train stats after 10912 examples: {'rewards_train/chosen': '-0.0393', 'rewards_train/rejected': '-0.07124', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031939', 'logps_train/rejected': '-121.37', 'logps_train/chosen': '-153.81', 'loss/train': '0.68578', 'examples_per_second': '31.625', 'grad_norm': '30.125', 'counters/examples': 10912, 'counters/updates': 341}
train stats after 10944 examples: {'rewards_train/chosen': '0.0039044', 'rewards_train/rejected': '-0.019562', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023467', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-133.25', 'loss/train': '0.68442', 'examples_per_second': '31.637', 'grad_norm': '29.125', 'counters/examples': 10944, 'counters/updates': 342}
train stats after 10976 examples: {'rewards_train/chosen': '-0.031458', 'rewards_train/rejected': '-0.014099', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017359', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-165.9', 'loss/train': '0.70914', 'examples_per_second': '30.813', 'grad_norm': '30.625', 'counters/examples': 10976, 'counters/updates': 343}
skipping logging after 11008 examples to avoid logging too frequently
train stats after 11040 examples: {'rewards_train/chosen': '-0.040606', 'rewards_train/rejected': '-0.0059172', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034688', 'logps_train/rejected': '-99.737', 'logps_train/chosen': '-140.96', 'loss/train': '0.7161', 'examples_per_second': '31.208', 'grad_norm': '30', 'counters/examples': 11040, 'counters/updates': 345}
train stats after 11072 examples: {'rewards_train/chosen': '0.018692', 'rewards_train/rejected': '-0.011785', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030477', 'logps_train/rejected': '-157.43', 'logps_train/chosen': '-136.05', 'loss/train': '0.68727', 'examples_per_second': '31.369', 'grad_norm': '36', 'counters/examples': 11072, 'counters/updates': 346}
train stats after 11104 examples: {'rewards_train/chosen': '0.0059429', 'rewards_train/rejected': '-0.099644', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10559', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-145.34', 'loss/train': '0.65335', 'examples_per_second': '31.613', 'grad_norm': '26.125', 'counters/examples': 11104, 'counters/updates': 347}
train stats after 11136 examples: {'rewards_train/chosen': '-0.044871', 'rewards_train/rejected': '-0.029114', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015758', 'logps_train/rejected': '-157.84', 'logps_train/chosen': '-181.01', 'loss/train': '0.71008', 'examples_per_second': '30.904', 'grad_norm': '37', 'counters/examples': 11136, 'counters/updates': 348}
train stats after 11168 examples: {'rewards_train/chosen': '-0.018148', 'rewards_train/rejected': '0.024935', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043083', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-129.86', 'loss/train': '0.71824', 'examples_per_second': '31.582', 'grad_norm': '28.125', 'counters/examples': 11168, 'counters/updates': 349}
train stats after 11200 examples: {'rewards_train/chosen': '0.01342', 'rewards_train/rejected': '-0.024908', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038328', 'logps_train/rejected': '-149.21', 'logps_train/chosen': '-156.66', 'loss/train': '0.68013', 'examples_per_second': '32.531', 'grad_norm': '34.25', 'counters/examples': 11200, 'counters/updates': 350}
train stats after 11232 examples: {'rewards_train/chosen': '-0.046562', 'rewards_train/rejected': '-0.060793', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014231', 'logps_train/rejected': '-161.74', 'logps_train/chosen': '-139.93', 'loss/train': '0.69461', 'examples_per_second': '31.642', 'grad_norm': '30.125', 'counters/examples': 11232, 'counters/updates': 351}
train stats after 11264 examples: {'rewards_train/chosen': '-0.026466', 'rewards_train/rejected': '-0.03527', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0088034', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-140.38', 'loss/train': '0.69184', 'examples_per_second': '30.132', 'grad_norm': '34', 'counters/examples': 11264, 'counters/updates': 352}
skipping logging after 11296 examples to avoid logging too frequently
train stats after 11328 examples: {'rewards_train/chosen': '-0.013458', 'rewards_train/rejected': '-0.031343', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017885', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-148.99', 'loss/train': '0.69329', 'examples_per_second': '30.818', 'grad_norm': '32.75', 'counters/examples': 11328, 'counters/updates': 354}
train stats after 11360 examples: {'rewards_train/chosen': '-0.003734', 'rewards_train/rejected': '-0.060987', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057253', 'logps_train/rejected': '-139.48', 'logps_train/chosen': '-118.45', 'loss/train': '0.66813', 'examples_per_second': '30.696', 'grad_norm': '27.125', 'counters/examples': 11360, 'counters/updates': 355}
train stats after 11392 examples: {'rewards_train/chosen': '-0.022042', 'rewards_train/rejected': '-0.039821', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017779', 'logps_train/rejected': '-115.82', 'logps_train/chosen': '-161.97', 'loss/train': '0.68926', 'examples_per_second': '32.342', 'grad_norm': '28', 'counters/examples': 11392, 'counters/updates': 356}
train stats after 11424 examples: {'rewards_train/chosen': '-0.0099263', 'rewards_train/rejected': '-0.016442', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0065155', 'logps_train/rejected': '-127.29', 'logps_train/chosen': '-153.91', 'loss/train': '0.69622', 'examples_per_second': '30.293', 'grad_norm': '35.5', 'counters/examples': 11424, 'counters/updates': 357}
train stats after 11456 examples: {'rewards_train/chosen': '-0.053181', 'rewards_train/rejected': '-0.055102', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0019215', 'logps_train/rejected': '-142.08', 'logps_train/chosen': '-126.42', 'loss/train': '0.70208', 'examples_per_second': '31.582', 'grad_norm': '31.375', 'counters/examples': 11456, 'counters/updates': 358}
train stats after 11488 examples: {'rewards_train/chosen': '-0.034452', 'rewards_train/rejected': '-0.081538', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047086', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-143.67', 'loss/train': '0.67827', 'examples_per_second': '32.929', 'grad_norm': '36.25', 'counters/examples': 11488, 'counters/updates': 359}
train stats after 11520 examples: {'rewards_train/chosen': '0.0036407', 'rewards_train/rejected': '-0.0042992', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0079399', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-178.18', 'loss/train': '0.69927', 'examples_per_second': '31.596', 'grad_norm': '35.75', 'counters/examples': 11520, 'counters/updates': 360}
train stats after 11552 examples: {'rewards_train/chosen': '-0.053222', 'rewards_train/rejected': '-0.023771', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.029451', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-133.05', 'loss/train': '0.71222', 'examples_per_second': '31.158', 'grad_norm': '28.625', 'counters/examples': 11552, 'counters/updates': 361}
train stats after 11584 examples: {'rewards_train/chosen': '-0.040056', 'rewards_train/rejected': '-0.044516', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0044602', 'logps_train/rejected': '-102.15', 'logps_train/chosen': '-134.99', 'loss/train': '0.69491', 'examples_per_second': '31.578', 'grad_norm': '28.5', 'counters/examples': 11584, 'counters/updates': 362}
skipping logging after 11616 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '0.055147', 'rewards_train/rejected': '-0.074012', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12916', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-120.93', 'loss/train': '0.63604', 'examples_per_second': '30.584', 'grad_norm': '25.5', 'counters/examples': 11648, 'counters/updates': 364}
skipping logging after 11680 examples to avoid logging too frequently
train stats after 11712 examples: {'rewards_train/chosen': '0.043186', 'rewards_train/rejected': '-0.025696', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068882', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-125.79', 'loss/train': '0.66601', 'examples_per_second': '31.622', 'grad_norm': '29.125', 'counters/examples': 11712, 'counters/updates': 366}
train stats after 11744 examples: {'rewards_train/chosen': '-0.076221', 'rewards_train/rejected': '-0.034718', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041503', 'logps_train/rejected': '-137.35', 'logps_train/chosen': '-188.2', 'loss/train': '0.72053', 'examples_per_second': '30.818', 'grad_norm': '36', 'counters/examples': 11744, 'counters/updates': 367}
train stats after 11776 examples: {'rewards_train/chosen': '-0.026933', 'rewards_train/rejected': '0.013765', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.040698', 'logps_train/rejected': '-101.68', 'logps_train/chosen': '-154.32', 'loss/train': '0.71713', 'examples_per_second': '31.659', 'grad_norm': '26.375', 'counters/examples': 11776, 'counters/updates': 368}
train stats after 11808 examples: {'rewards_train/chosen': '-0.062044', 'rewards_train/rejected': '-0.037199', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024845', 'logps_train/rejected': '-123.57', 'logps_train/chosen': '-121.95', 'loss/train': '0.711', 'examples_per_second': '31.435', 'grad_norm': '32', 'counters/examples': 11808, 'counters/updates': 369}
train stats after 11840 examples: {'rewards_train/chosen': '-0.0017403', 'rewards_train/rejected': '0.00087128', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0026116', 'logps_train/rejected': '-104.93', 'logps_train/chosen': '-146.52', 'loss/train': '0.69825', 'examples_per_second': '31.204', 'grad_norm': '26.875', 'counters/examples': 11840, 'counters/updates': 370}
train stats after 11872 examples: {'rewards_train/chosen': '-0.035836', 'rewards_train/rejected': '0.00090662', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.036742', 'logps_train/rejected': '-170.69', 'logps_train/chosen': '-152.99', 'loss/train': '0.72471', 'examples_per_second': '31.586', 'grad_norm': '96.5', 'counters/examples': 11872, 'counters/updates': 371}
train stats after 11904 examples: {'rewards_train/chosen': '-0.049502', 'rewards_train/rejected': '-0.020188', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.029314', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-148.39', 'loss/train': '0.71689', 'examples_per_second': '30.212', 'grad_norm': '29.875', 'counters/examples': 11904, 'counters/updates': 372}
train stats after 11936 examples: {'rewards_train/chosen': '0.0038979', 'rewards_train/rejected': '-0.10889', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11279', 'logps_train/rejected': '-125.05', 'logps_train/chosen': '-139.6', 'loss/train': '0.65279', 'examples_per_second': '31.873', 'grad_norm': '25.25', 'counters/examples': 11936, 'counters/updates': 373}
train stats after 11968 examples: {'rewards_train/chosen': '-0.015972', 'rewards_train/rejected': '-0.016087', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00011479', 'logps_train/rejected': '-145.36', 'logps_train/chosen': '-153.93', 'loss/train': '0.70141', 'examples_per_second': '31.594', 'grad_norm': '35.5', 'counters/examples': 11968, 'counters/updates': 374}
train stats after 12000 examples: {'rewards_train/chosen': '0.010461', 'rewards_train/rejected': '-0.024191', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034651', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-116.75', 'loss/train': '0.67936', 'examples_per_second': '31.58', 'grad_norm': '26.375', 'counters/examples': 12000, 'counters/updates': 375}
Running evaluation after 12000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 12000: {'rewards_eval/chosen': '-0.022282', 'rewards_eval/rejected': '-0.016015', 'rewards_eval/accuracies': '0.47266', 'rewards_eval/margins': '-0.0062662', 'logps_eval/rejected': '-118.77', 'logps_eval/chosen': '-139.66', 'loss/eval': '0.70324'}
skipping save for non epoch
train stats after 12032 examples: {'rewards_train/chosen': '-0.12361', 'rewards_train/rejected': '-0.0028935', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.12072', 'logps_train/rejected': '-110.16', 'logps_train/chosen': '-170.27', 'loss/train': '0.7656', 'examples_per_second': '31.888', 'grad_norm': '37', 'counters/examples': 12032, 'counters/updates': 376}
skipping logging after 12064 examples to avoid logging too frequently
train stats after 12096 examples: {'rewards_train/chosen': '0.0093024', 'rewards_train/rejected': '-0.02639', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035693', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-120.4', 'loss/train': '0.68769', 'examples_per_second': '30.049', 'grad_norm': '33', 'counters/examples': 12096, 'counters/updates': 378}
skipping logging after 12128 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.0068236', 'rewards_train/rejected': '-0.021202', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014378', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-162.27', 'loss/train': '0.69583', 'examples_per_second': '32.473', 'grad_norm': '34', 'counters/examples': 12160, 'counters/updates': 380}
train stats after 12192 examples: {'rewards_train/chosen': '0.005347', 'rewards_train/rejected': '-0.040614', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.045961', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-152.09', 'loss/train': '0.677', 'examples_per_second': '30.451', 'grad_norm': '28.25', 'counters/examples': 12192, 'counters/updates': 381}
train stats after 12224 examples: {'rewards_train/chosen': '-0.038308', 'rewards_train/rejected': '-0.056771', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018463', 'logps_train/rejected': '-97.23', 'logps_train/chosen': '-110.53', 'loss/train': '0.69258', 'examples_per_second': '33.101', 'grad_norm': '24', 'counters/examples': 12224, 'counters/updates': 382}
train stats after 12256 examples: {'rewards_train/chosen': '-0.030753', 'rewards_train/rejected': '-0.09207', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061317', 'logps_train/rejected': '-114.85', 'logps_train/chosen': '-133.42', 'loss/train': '0.67132', 'examples_per_second': '31.484', 'grad_norm': '28.625', 'counters/examples': 12256, 'counters/updates': 383}
train stats after 12288 examples: {'rewards_train/chosen': '-0.045166', 'rewards_train/rejected': '-0.011794', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033372', 'logps_train/rejected': '-142.5', 'logps_train/chosen': '-152.79', 'loss/train': '0.71702', 'examples_per_second': '31.084', 'grad_norm': '55.25', 'counters/examples': 12288, 'counters/updates': 384}
skipping logging after 12320 examples to avoid logging too frequently
train stats after 12352 examples: {'rewards_train/chosen': '0.039583', 'rewards_train/rejected': '-0.032522', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072105', 'logps_train/rejected': '-107.39', 'logps_train/chosen': '-134.19', 'loss/train': '0.66177', 'examples_per_second': '34.968', 'grad_norm': '28.875', 'counters/examples': 12352, 'counters/updates': 386}
train stats after 12384 examples: {'rewards_train/chosen': '-0.028706', 'rewards_train/rejected': '-0.067638', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038932', 'logps_train/rejected': '-97.01', 'logps_train/chosen': '-118.99', 'loss/train': '0.67791', 'examples_per_second': '31.707', 'grad_norm': '23.875', 'counters/examples': 12384, 'counters/updates': 387}
train stats after 12416 examples: {'rewards_train/chosen': '-0.026056', 'rewards_train/rejected': '-0.06283', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036774', 'logps_train/rejected': '-131.9', 'logps_train/chosen': '-107.07', 'loss/train': '0.67957', 'examples_per_second': '31.422', 'grad_norm': '27.5', 'counters/examples': 12416, 'counters/updates': 388}
train stats after 12448 examples: {'rewards_train/chosen': '-0.014723', 'rewards_train/rejected': '-0.032463', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01774', 'logps_train/rejected': '-162.81', 'logps_train/chosen': '-181.93', 'loss/train': '0.68895', 'examples_per_second': '30.123', 'grad_norm': '34.5', 'counters/examples': 12448, 'counters/updates': 389}
train stats after 12480 examples: {'rewards_train/chosen': '-0.10325', 'rewards_train/rejected': '0.019455', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.1227', 'logps_train/rejected': '-127.88', 'logps_train/chosen': '-176.42', 'loss/train': '0.77217', 'examples_per_second': '30.752', 'grad_norm': '37.75', 'counters/examples': 12480, 'counters/updates': 390}
train stats after 12512 examples: {'rewards_train/chosen': '0.0021157', 'rewards_train/rejected': '-0.0050397', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0071554', 'logps_train/rejected': '-133.33', 'logps_train/chosen': '-146.67', 'loss/train': '0.69176', 'examples_per_second': '30.926', 'grad_norm': '28.375', 'counters/examples': 12512, 'counters/updates': 391}
train stats after 12544 examples: {'rewards_train/chosen': '0.0093049', 'rewards_train/rejected': '-0.010375', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01968', 'logps_train/rejected': '-89.479', 'logps_train/chosen': '-160.05', 'loss/train': '0.68668', 'examples_per_second': '31.618', 'grad_norm': '26', 'counters/examples': 12544, 'counters/updates': 392}
train stats after 12576 examples: {'rewards_train/chosen': '-0.063803', 'rewards_train/rejected': '-0.059433', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0043696', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-125.08', 'loss/train': '0.70242', 'examples_per_second': '32.036', 'grad_norm': '30.375', 'counters/examples': 12576, 'counters/updates': 393}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12640 examples: {'rewards_train/chosen': '0.012181', 'rewards_train/rejected': '0.004344', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0078367', 'logps_train/rejected': '-131.73', 'logps_train/chosen': '-143.47', 'loss/train': '0.69777', 'examples_per_second': '30.301', 'grad_norm': '28.375', 'counters/examples': 12640, 'counters/updates': 395}
train stats after 12672 examples: {'rewards_train/chosen': '-0.015876', 'rewards_train/rejected': '-0.044437', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028562', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-151.33', 'loss/train': '0.68371', 'examples_per_second': '30.585', 'grad_norm': '30.5', 'counters/examples': 12672, 'counters/updates': 396}
skipping logging after 12704 examples to avoid logging too frequently
train stats after 12736 examples: {'rewards_train/chosen': '-0.024037', 'rewards_train/rejected': '-0.011995', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012042', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-141.67', 'loss/train': '0.70241', 'examples_per_second': '25.222', 'grad_norm': '33.25', 'counters/examples': 12736, 'counters/updates': 398}
train stats after 12768 examples: {'rewards_train/chosen': '0.023508', 'rewards_train/rejected': '-0.030326', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053834', 'logps_train/rejected': '-113.82', 'logps_train/chosen': '-140.15', 'loss/train': '0.67301', 'examples_per_second': '30.981', 'grad_norm': '27.5', 'counters/examples': 12768, 'counters/updates': 399}
train stats after 12800 examples: {'rewards_train/chosen': '-0.11858', 'rewards_train/rejected': '-0.06688', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051699', 'logps_train/rejected': '-114.88', 'logps_train/chosen': '-135.33', 'loss/train': '0.72878', 'examples_per_second': '31.381', 'grad_norm': '36.5', 'counters/examples': 12800, 'counters/updates': 400}
skipping logging after 12832 examples to avoid logging too frequently
train stats after 12864 examples: {'rewards_train/chosen': '-0.020261', 'rewards_train/rejected': '0.0075189', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02778', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-150.42', 'loss/train': '0.71006', 'examples_per_second': '31.668', 'grad_norm': '35.25', 'counters/examples': 12864, 'counters/updates': 402}
train stats after 12896 examples: {'rewards_train/chosen': '-0.062093', 'rewards_train/rejected': '-0.058936', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0031572', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-127.57', 'loss/train': '0.69903', 'examples_per_second': '31.569', 'grad_norm': '27.375', 'counters/examples': 12896, 'counters/updates': 403}
skipping logging after 12928 examples to avoid logging too frequently
train stats after 12960 examples: {'rewards_train/chosen': '-0.016754', 'rewards_train/rejected': '-0.023964', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0072103', 'logps_train/rejected': '-129.64', 'logps_train/chosen': '-145.29', 'loss/train': '0.69303', 'examples_per_second': '33.986', 'grad_norm': '27.25', 'counters/examples': 12960, 'counters/updates': 405}
train stats after 12992 examples: {'rewards_train/chosen': '-0.033453', 'rewards_train/rejected': '-0.043825', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010373', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-129.72', 'loss/train': '0.69547', 'examples_per_second': '31.108', 'grad_norm': '33.75', 'counters/examples': 12992, 'counters/updates': 406}
train stats after 13024 examples: {'rewards_train/chosen': '-0.0095453', 'rewards_train/rejected': '-0.0011494', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0083959', 'logps_train/rejected': '-111.85', 'logps_train/chosen': '-121.18', 'loss/train': '0.70049', 'examples_per_second': '30.703', 'grad_norm': '25.875', 'counters/examples': 13024, 'counters/updates': 407}
train stats after 13056 examples: {'rewards_train/chosen': '0.010515', 'rewards_train/rejected': '-0.018737', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029252', 'logps_train/rejected': '-104.17', 'logps_train/chosen': '-111.31', 'loss/train': '0.68329', 'examples_per_second': '32.456', 'grad_norm': '22.75', 'counters/examples': 13056, 'counters/updates': 408}
train stats after 13088 examples: {'rewards_train/chosen': '-0.0024274', 'rewards_train/rejected': '0.0072345', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.009662', 'logps_train/rejected': '-122.24', 'logps_train/chosen': '-135.57', 'loss/train': '0.70318', 'examples_per_second': '32.881', 'grad_norm': '29.875', 'counters/examples': 13088, 'counters/updates': 409}
train stats after 13120 examples: {'rewards_train/chosen': '-0.093373', 'rewards_train/rejected': '0.014364', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10774', 'logps_train/rejected': '-131.71', 'logps_train/chosen': '-176.05', 'loss/train': '0.75962', 'examples_per_second': '31.491', 'grad_norm': '60.5', 'counters/examples': 13120, 'counters/updates': 410}
train stats after 13152 examples: {'rewards_train/chosen': '-0.038469', 'rewards_train/rejected': '0.0026927', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041162', 'logps_train/rejected': '-110.38', 'logps_train/chosen': '-144.28', 'loss/train': '0.71999', 'examples_per_second': '31.324', 'grad_norm': '28.125', 'counters/examples': 13152, 'counters/updates': 411}
train stats after 13184 examples: {'rewards_train/chosen': '-0.0050581', 'rewards_train/rejected': '-0.040356', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035298', 'logps_train/rejected': '-130.5', 'logps_train/chosen': '-138.47', 'loss/train': '0.6797', 'examples_per_second': '30.815', 'grad_norm': '32.5', 'counters/examples': 13184, 'counters/updates': 412}
skipping logging after 13216 examples to avoid logging too frequently
train stats after 13248 examples: {'rewards_train/chosen': '-0.036093', 'rewards_train/rejected': '0.020832', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.056925', 'logps_train/rejected': '-113.51', 'logps_train/chosen': '-155.01', 'loss/train': '0.72813', 'examples_per_second': '34.516', 'grad_norm': '29.875', 'counters/examples': 13248, 'counters/updates': 414}
train stats after 13280 examples: {'rewards_train/chosen': '-0.012961', 'rewards_train/rejected': '-0.015038', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0020775', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-149.11', 'loss/train': '0.69645', 'examples_per_second': '31.68', 'grad_norm': '29.75', 'counters/examples': 13280, 'counters/updates': 415}
skipping logging after 13312 examples to avoid logging too frequently
train stats after 13344 examples: {'rewards_train/chosen': '-0.0079199', 'rewards_train/rejected': '-0.032446', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024526', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-146.57', 'loss/train': '0.68765', 'examples_per_second': '31.569', 'grad_norm': '30.5', 'counters/examples': 13344, 'counters/updates': 417}
train stats after 13376 examples: {'rewards_train/chosen': '-0.048281', 'rewards_train/rejected': '-0.038651', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0096299', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-139.55', 'loss/train': '0.70996', 'examples_per_second': '30.217', 'grad_norm': '35.25', 'counters/examples': 13376, 'counters/updates': 418}
train stats after 13408 examples: {'rewards_train/chosen': '0.070787', 'rewards_train/rejected': '-0.063114', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1339', 'logps_train/rejected': '-129.31', 'logps_train/chosen': '-149.82', 'loss/train': '0.63436', 'examples_per_second': '31.727', 'grad_norm': '26.625', 'counters/examples': 13408, 'counters/updates': 419}
train stats after 13440 examples: {'rewards_train/chosen': '-0.018053', 'rewards_train/rejected': '-0.040698', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022645', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-133.61', 'loss/train': '0.68722', 'examples_per_second': '31.697', 'grad_norm': '32', 'counters/examples': 13440, 'counters/updates': 420}
train stats after 13472 examples: {'rewards_train/chosen': '-0.0023231', 'rewards_train/rejected': '-0.036973', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034649', 'logps_train/rejected': '-90.569', 'logps_train/chosen': '-134.38', 'loss/train': '0.67902', 'examples_per_second': '25.353', 'grad_norm': '24.75', 'counters/examples': 13472, 'counters/updates': 421}
train stats after 13504 examples: {'rewards_train/chosen': '0.019102', 'rewards_train/rejected': '-0.029678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04878', 'logps_train/rejected': '-147.81', 'logps_train/chosen': '-162.89', 'loss/train': '0.67393', 'examples_per_second': '31.655', 'grad_norm': '29.5', 'counters/examples': 13504, 'counters/updates': 422}
skipping logging after 13536 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '0.022185', 'rewards_train/rejected': '-0.042091', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064276', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-141.67', 'loss/train': '0.66721', 'examples_per_second': '24.18', 'grad_norm': '29.5', 'counters/examples': 13568, 'counters/updates': 424}
train stats after 13600 examples: {'rewards_train/chosen': '0.025357', 'rewards_train/rejected': '-0.0022815', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027638', 'logps_train/rejected': '-139.27', 'logps_train/chosen': '-152.37', 'loss/train': '0.68618', 'examples_per_second': '31.052', 'grad_norm': '38.25', 'counters/examples': 13600, 'counters/updates': 425}
train stats after 13632 examples: {'rewards_train/chosen': '-0.090605', 'rewards_train/rejected': '0.0014858', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.09209', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-120.83', 'loss/train': '0.75121', 'examples_per_second': '31.082', 'grad_norm': '34', 'counters/examples': 13632, 'counters/updates': 426}
skipping logging after 13664 examples to avoid logging too frequently
train stats after 13696 examples: {'rewards_train/chosen': '-0.023474', 'rewards_train/rejected': '-0.025957', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0024832', 'logps_train/rejected': '-147.58', 'logps_train/chosen': '-152.63', 'loss/train': '0.69716', 'examples_per_second': '32.265', 'grad_norm': '33.75', 'counters/examples': 13696, 'counters/updates': 428}
train stats after 13728 examples: {'rewards_train/chosen': '-0.01436', 'rewards_train/rejected': '-0.0084892', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0058713', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-166.35', 'loss/train': '0.70101', 'examples_per_second': '30.745', 'grad_norm': '29.875', 'counters/examples': 13728, 'counters/updates': 429}
train stats after 13760 examples: {'rewards_train/chosen': '-0.078505', 'rewards_train/rejected': '-0.059024', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019481', 'logps_train/rejected': '-116.72', 'logps_train/chosen': '-166.14', 'loss/train': '0.7108', 'examples_per_second': '30.479', 'grad_norm': '33.75', 'counters/examples': 13760, 'counters/updates': 430}
train stats after 13792 examples: {'rewards_train/chosen': '-0.033565', 'rewards_train/rejected': '-0.022969', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010596', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-159.09', 'loss/train': '0.70221', 'examples_per_second': '31.05', 'grad_norm': '33.75', 'counters/examples': 13792, 'counters/updates': 431}
train stats after 13824 examples: {'rewards_train/chosen': '-0.028553', 'rewards_train/rejected': '0.0031527', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.031706', 'logps_train/rejected': '-119.61', 'logps_train/chosen': '-126.11', 'loss/train': '0.71239', 'examples_per_second': '31.693', 'grad_norm': '27.125', 'counters/examples': 13824, 'counters/updates': 432}
train stats after 13856 examples: {'rewards_train/chosen': '0.014365', 'rewards_train/rejected': '0.019472', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0051068', 'logps_train/rejected': '-155.77', 'logps_train/chosen': '-148.8', 'loss/train': '0.70447', 'examples_per_second': '32.87', 'grad_norm': '34', 'counters/examples': 13856, 'counters/updates': 433}
train stats after 13888 examples: {'rewards_train/chosen': '-0.011415', 'rewards_train/rejected': '-0.012887', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0014721', 'logps_train/rejected': '-154.77', 'logps_train/chosen': '-129.32', 'loss/train': '0.69937', 'examples_per_second': '31.725', 'grad_norm': '31.125', 'counters/examples': 13888, 'counters/updates': 434}
skipping logging after 13920 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '-0.001084', 'rewards_train/rejected': '-0.037752', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036668', 'logps_train/rejected': '-100.34', 'logps_train/chosen': '-125.42', 'loss/train': '0.67864', 'examples_per_second': '33.388', 'grad_norm': '25.25', 'counters/examples': 13952, 'counters/updates': 436}
train stats after 13984 examples: {'rewards_train/chosen': '-0.027272', 'rewards_train/rejected': '-0.018539', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0087332', 'logps_train/rejected': '-96.671', 'logps_train/chosen': '-123.15', 'loss/train': '0.7016', 'examples_per_second': '32.715', 'grad_norm': '24.25', 'counters/examples': 13984, 'counters/updates': 437}
train stats after 14016 examples: {'rewards_train/chosen': '-0.02523', 'rewards_train/rejected': '-0.068351', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043121', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-155.4', 'loss/train': '0.67982', 'examples_per_second': '32.636', 'grad_norm': '28.5', 'counters/examples': 14016, 'counters/updates': 438}
train stats after 14048 examples: {'rewards_train/chosen': '-0.00068138', 'rewards_train/rejected': '-0.0094074', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.008726', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-115.42', 'loss/train': '0.69204', 'examples_per_second': '32.441', 'grad_norm': '26.875', 'counters/examples': 14048, 'counters/updates': 439}
train stats after 14080 examples: {'rewards_train/chosen': '0.033834', 'rewards_train/rejected': '-0.00084162', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034676', 'logps_train/rejected': '-135.04', 'logps_train/chosen': '-126.66', 'loss/train': '0.6801', 'examples_per_second': '31.604', 'grad_norm': '28.625', 'counters/examples': 14080, 'counters/updates': 440}
train stats after 14112 examples: {'rewards_train/chosen': '0.0026543', 'rewards_train/rejected': '-0.053699', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056353', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-151', 'loss/train': '0.67131', 'examples_per_second': '32.984', 'grad_norm': '27.25', 'counters/examples': 14112, 'counters/updates': 441}
train stats after 14144 examples: {'rewards_train/chosen': '-0.037276', 'rewards_train/rejected': '0.03833', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.075606', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-126.43', 'loss/train': '0.73645', 'examples_per_second': '31.72', 'grad_norm': '29.875', 'counters/examples': 14144, 'counters/updates': 442}
train stats after 14176 examples: {'rewards_train/chosen': '0.013039', 'rewards_train/rejected': '-0.012203', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025242', 'logps_train/rejected': '-136.04', 'logps_train/chosen': '-158.83', 'loss/train': '0.68495', 'examples_per_second': '33.219', 'grad_norm': '33.25', 'counters/examples': 14176, 'counters/updates': 443}
skipping logging after 14208 examples to avoid logging too frequently
train stats after 14240 examples: {'rewards_train/chosen': '-0.017078', 'rewards_train/rejected': '-0.030877', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013799', 'logps_train/rejected': '-105.64', 'logps_train/chosen': '-138.85', 'loss/train': '0.68912', 'examples_per_second': '31.723', 'grad_norm': '29.875', 'counters/examples': 14240, 'counters/updates': 445}
train stats after 14272 examples: {'rewards_train/chosen': '0.090278', 'rewards_train/rejected': '0.052394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037885', 'logps_train/rejected': '-100.2', 'logps_train/chosen': '-112.73', 'loss/train': '0.67773', 'examples_per_second': '32.383', 'grad_norm': '33.25', 'counters/examples': 14272, 'counters/updates': 446}
skipping logging after 14304 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.0098252', 'rewards_train/rejected': '0.036142', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026317', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-164.87', 'loss/train': '0.71865', 'examples_per_second': '30.841', 'grad_norm': '31.625', 'counters/examples': 14336, 'counters/updates': 448}
train stats after 14368 examples: {'rewards_train/chosen': '-0.03356', 'rewards_train/rejected': '0.012638', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.046198', 'logps_train/rejected': '-98.085', 'logps_train/chosen': '-140.1', 'loss/train': '0.71982', 'examples_per_second': '30.328', 'grad_norm': '27', 'counters/examples': 14368, 'counters/updates': 449}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14432 examples: {'rewards_train/chosen': '-0.055242', 'rewards_train/rejected': '-0.064483', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0092406', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-172.13', 'loss/train': '0.69331', 'examples_per_second': '31.773', 'grad_norm': '36', 'counters/examples': 14432, 'counters/updates': 451}
train stats after 14464 examples: {'rewards_train/chosen': '-0.01237', 'rewards_train/rejected': '-0.0096566', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0027134', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-132.33', 'loss/train': '0.70246', 'examples_per_second': '31.842', 'grad_norm': '29.125', 'counters/examples': 14464, 'counters/updates': 452}
train stats after 14496 examples: {'rewards_train/chosen': '-0.037027', 'rewards_train/rejected': '-0.013659', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023368', 'logps_train/rejected': '-149.42', 'logps_train/chosen': '-157.22', 'loss/train': '0.71162', 'examples_per_second': '30.656', 'grad_norm': '36.5', 'counters/examples': 14496, 'counters/updates': 453}
train stats after 14528 examples: {'rewards_train/chosen': '0.00075586', 'rewards_train/rejected': '-0.037704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03846', 'logps_train/rejected': '-135.57', 'logps_train/chosen': '-114.87', 'loss/train': '0.67801', 'examples_per_second': '32.419', 'grad_norm': '27.75', 'counters/examples': 14528, 'counters/updates': 454}
skipping logging after 14560 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '-0.01425', 'rewards_train/rejected': '-0.0021204', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.012129', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-133.23', 'loss/train': '0.70604', 'examples_per_second': '31.619', 'grad_norm': '28.125', 'counters/examples': 14592, 'counters/updates': 456}
train stats after 14624 examples: {'rewards_train/chosen': '-0.020657', 'rewards_train/rejected': '-0.0012529', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019404', 'logps_train/rejected': '-84.223', 'logps_train/chosen': '-110.09', 'loss/train': '0.70685', 'examples_per_second': '31.609', 'grad_norm': '25.5', 'counters/examples': 14624, 'counters/updates': 457}
train stats after 14656 examples: {'rewards_train/chosen': '0.012544', 'rewards_train/rejected': '0.0029478', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0095957', 'logps_train/rejected': '-111.02', 'logps_train/chosen': '-127.26', 'loss/train': '0.69221', 'examples_per_second': '31.653', 'grad_norm': '27.625', 'counters/examples': 14656, 'counters/updates': 458}
skipping logging after 14688 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '-0.042135', 'rewards_train/rejected': '-0.010472', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.031664', 'logps_train/rejected': '-153.41', 'logps_train/chosen': '-177.66', 'loss/train': '0.71529', 'examples_per_second': '31.774', 'grad_norm': '50.75', 'counters/examples': 14720, 'counters/updates': 460}
train stats after 14752 examples: {'rewards_train/chosen': '-0.04742', 'rewards_train/rejected': '-0.069155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021735', 'logps_train/rejected': '-160.02', 'logps_train/chosen': '-161.71', 'loss/train': '0.68588', 'examples_per_second': '31.637', 'grad_norm': '31.25', 'counters/examples': 14752, 'counters/updates': 461}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14816 examples: {'rewards_train/chosen': '-0.022175', 'rewards_train/rejected': '-0.00052465', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.02165', 'logps_train/rejected': '-146.54', 'logps_train/chosen': '-125.97', 'loss/train': '0.70692', 'examples_per_second': '31.638', 'grad_norm': '31', 'counters/examples': 14816, 'counters/updates': 463}
skipping logging after 14848 examples to avoid logging too frequently
train stats after 14880 examples: {'rewards_train/chosen': '-0.014083', 'rewards_train/rejected': '-0.011637', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0024465', 'logps_train/rejected': '-132.65', 'logps_train/chosen': '-138.76', 'loss/train': '0.69869', 'examples_per_second': '32.717', 'grad_norm': '27.25', 'counters/examples': 14880, 'counters/updates': 465}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14944 examples: {'rewards_train/chosen': '0.038823', 'rewards_train/rejected': '-0.024472', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063295', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-128.57', 'loss/train': '0.66725', 'examples_per_second': '31.676', 'grad_norm': '33.5', 'counters/examples': 14944, 'counters/updates': 467}
train stats after 14976 examples: {'rewards_train/chosen': '-0.037505', 'rewards_train/rejected': '-0.042941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0054355', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-118.88', 'loss/train': '0.69501', 'examples_per_second': '32.576', 'grad_norm': '29.375', 'counters/examples': 14976, 'counters/updates': 468}
train stats after 15008 examples: {'rewards_train/chosen': '0.0054174', 'rewards_train/rejected': '-0.011066', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016483', 'logps_train/rejected': '-96.428', 'logps_train/chosen': '-97.56', 'loss/train': '0.68791', 'examples_per_second': '31.109', 'grad_norm': '26.375', 'counters/examples': 15008, 'counters/updates': 469}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15072 examples: {'rewards_train/chosen': '0.07163', 'rewards_train/rejected': '-0.079879', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15151', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-147.86', 'loss/train': '0.62563', 'examples_per_second': '30.431', 'grad_norm': '29', 'counters/examples': 15072, 'counters/updates': 471}
train stats after 15104 examples: {'rewards_train/chosen': '0.015901', 'rewards_train/rejected': '0.04497', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.029069', 'logps_train/rejected': '-130.4', 'logps_train/chosen': '-135.97', 'loss/train': '0.71817', 'examples_per_second': '30.997', 'grad_norm': '29.625', 'counters/examples': 15104, 'counters/updates': 472}
train stats after 15136 examples: {'rewards_train/chosen': '-0.031845', 'rewards_train/rejected': '0.012771', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.044616', 'logps_train/rejected': '-140.5', 'logps_train/chosen': '-144.23', 'loss/train': '0.72607', 'examples_per_second': '32.791', 'grad_norm': '39', 'counters/examples': 15136, 'counters/updates': 473}
train stats after 15168 examples: {'rewards_train/chosen': '0.0006537', 'rewards_train/rejected': '-0.012059', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012713', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-120.2', 'loss/train': '0.6905', 'examples_per_second': '31.633', 'grad_norm': '28.5', 'counters/examples': 15168, 'counters/updates': 474}
train stats after 15200 examples: {'rewards_train/chosen': '-0.020319', 'rewards_train/rejected': '-0.0062309', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014088', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-167.66', 'loss/train': '0.71005', 'examples_per_second': '33.134', 'grad_norm': '32.25', 'counters/examples': 15200, 'counters/updates': 475}
train stats after 15232 examples: {'rewards_train/chosen': '0.016651', 'rewards_train/rejected': '-0.019464', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036115', 'logps_train/rejected': '-115.36', 'logps_train/chosen': '-113.2', 'loss/train': '0.6782', 'examples_per_second': '31.325', 'grad_norm': '23.75', 'counters/examples': 15232, 'counters/updates': 476}
train stats after 15264 examples: {'rewards_train/chosen': '-0.015604', 'rewards_train/rejected': '0.011107', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02671', 'logps_train/rejected': '-121.31', 'logps_train/chosen': '-155.02', 'loss/train': '0.71236', 'examples_per_second': '30.841', 'grad_norm': '29.375', 'counters/examples': 15264, 'counters/updates': 477}
train stats after 15296 examples: {'rewards_train/chosen': '-0.0049513', 'rewards_train/rejected': '-0.011044', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0060926', 'logps_train/rejected': '-99.129', 'logps_train/chosen': '-157.01', 'loss/train': '0.69374', 'examples_per_second': '30.215', 'grad_norm': '29.375', 'counters/examples': 15296, 'counters/updates': 478}
skipping logging after 15328 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '-0.0033895', 'rewards_train/rejected': '0.034506', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037895', 'logps_train/rejected': '-109.32', 'logps_train/chosen': '-153.44', 'loss/train': '0.7177', 'examples_per_second': '30.292', 'grad_norm': '35.5', 'counters/examples': 15360, 'counters/updates': 480}
train stats after 15392 examples: {'rewards_train/chosen': '0.023023', 'rewards_train/rejected': '-0.061838', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.084861', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-137.34', 'loss/train': '0.65717', 'examples_per_second': '31.974', 'grad_norm': '24.25', 'counters/examples': 15392, 'counters/updates': 481}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15456 examples: {'rewards_train/chosen': '-0.032525', 'rewards_train/rejected': '-0.023974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0085515', 'logps_train/rejected': '-98.104', 'logps_train/chosen': '-129.87', 'loss/train': '0.70111', 'examples_per_second': '31.642', 'grad_norm': '33', 'counters/examples': 15456, 'counters/updates': 483}
train stats after 15488 examples: {'rewards_train/chosen': '-0.035382', 'rewards_train/rejected': '-0.061553', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02617', 'logps_train/rejected': '-96.144', 'logps_train/chosen': '-118.57', 'loss/train': '0.68516', 'examples_per_second': '31.764', 'grad_norm': '25', 'counters/examples': 15488, 'counters/updates': 484}
train stats after 15520 examples: {'rewards_train/chosen': '0.003436', 'rewards_train/rejected': '-0.013113', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016549', 'logps_train/rejected': '-174.57', 'logps_train/chosen': '-179.22', 'loss/train': '0.6982', 'examples_per_second': '32.735', 'grad_norm': '33.75', 'counters/examples': 15520, 'counters/updates': 485}
skipping logging after 15552 examples to avoid logging too frequently
train stats after 15584 examples: {'rewards_train/chosen': '0.028761', 'rewards_train/rejected': '0.024295', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.004466', 'logps_train/rejected': '-123.56', 'logps_train/chosen': '-150.87', 'loss/train': '0.69524', 'examples_per_second': '30.371', 'grad_norm': '27.375', 'counters/examples': 15584, 'counters/updates': 487}
train stats after 15616 examples: {'rewards_train/chosen': '0.008292', 'rewards_train/rejected': '0.02441', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016118', 'logps_train/rejected': '-117.76', 'logps_train/chosen': '-144.18', 'loss/train': '0.7087', 'examples_per_second': '31.54', 'grad_norm': '29.5', 'counters/examples': 15616, 'counters/updates': 488}
skipping logging after 15648 examples to avoid logging too frequently
train stats after 15680 examples: {'rewards_train/chosen': '0.0033078', 'rewards_train/rejected': '-0.016676', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019984', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-114.39', 'loss/train': '0.68621', 'examples_per_second': '32.619', 'grad_norm': '25.75', 'counters/examples': 15680, 'counters/updates': 490}
train stats after 15712 examples: {'rewards_train/chosen': '-0.016593', 'rewards_train/rejected': '0.0077516', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024345', 'logps_train/rejected': '-141.45', 'logps_train/chosen': '-137.52', 'loss/train': '0.71079', 'examples_per_second': '32.769', 'grad_norm': '39', 'counters/examples': 15712, 'counters/updates': 491}
train stats after 15744 examples: {'rewards_train/chosen': '0.030481', 'rewards_train/rejected': '-0.025529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05601', 'logps_train/rejected': '-113.59', 'logps_train/chosen': '-115.17', 'loss/train': '0.6682', 'examples_per_second': '31.33', 'grad_norm': '23.625', 'counters/examples': 15744, 'counters/updates': 492}
train stats after 15776 examples: {'rewards_train/chosen': '0.00050057', 'rewards_train/rejected': '-0.038446', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038947', 'logps_train/rejected': '-118.1', 'logps_train/chosen': '-138.91', 'loss/train': '0.67719', 'examples_per_second': '31.834', 'grad_norm': '25', 'counters/examples': 15776, 'counters/updates': 493}
train stats after 15808 examples: {'rewards_train/chosen': '-0.018294', 'rewards_train/rejected': '0.0018788', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020173', 'logps_train/rejected': '-146.47', 'logps_train/chosen': '-119.43', 'loss/train': '0.71138', 'examples_per_second': '31.708', 'grad_norm': '33.5', 'counters/examples': 15808, 'counters/updates': 494}
train stats after 15840 examples: {'rewards_train/chosen': '0.021194', 'rewards_train/rejected': '0.0073859', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013808', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-116.67', 'loss/train': '0.69216', 'examples_per_second': '31.869', 'grad_norm': '29.375', 'counters/examples': 15840, 'counters/updates': 495}
train stats after 15872 examples: {'rewards_train/chosen': '0.018321', 'rewards_train/rejected': '0.0046981', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013622', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-139.22', 'loss/train': '0.69004', 'examples_per_second': '31.72', 'grad_norm': '31', 'counters/examples': 15872, 'counters/updates': 496}
skipping logging after 15904 examples to avoid logging too frequently
train stats after 15936 examples: {'rewards_train/chosen': '0.023641', 'rewards_train/rejected': '-0.055917', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079558', 'logps_train/rejected': '-167.2', 'logps_train/chosen': '-116.19', 'loss/train': '0.65891', 'examples_per_second': '35.702', 'grad_norm': '27.875', 'counters/examples': 15936, 'counters/updates': 498}
train stats after 15968 examples: {'rewards_train/chosen': '0.0021172', 'rewards_train/rejected': '-0.027234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029351', 'logps_train/rejected': '-151.38', 'logps_train/chosen': '-147.32', 'loss/train': '0.68331', 'examples_per_second': '30.892', 'grad_norm': '29.75', 'counters/examples': 15968, 'counters/updates': 499}
train stats after 16000 examples: {'rewards_train/chosen': '-0.032468', 'rewards_train/rejected': '-0.040533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0080645', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-155.24', 'loss/train': '0.69483', 'examples_per_second': '31.573', 'grad_norm': '28.25', 'counters/examples': 16000, 'counters/updates': 500}
train stats after 16032 examples: {'rewards_train/chosen': '0.020339', 'rewards_train/rejected': '-0.021669', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042008', 'logps_train/rejected': '-103.36', 'logps_train/chosen': '-163.84', 'loss/train': '0.67741', 'examples_per_second': '31.573', 'grad_norm': '27.375', 'counters/examples': 16032, 'counters/updates': 501}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16096 examples: {'rewards_train/chosen': '-0.012734', 'rewards_train/rejected': '-0.022873', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010138', 'logps_train/rejected': '-169.33', 'logps_train/chosen': '-160.77', 'loss/train': '0.69272', 'examples_per_second': '31.42', 'grad_norm': '37', 'counters/examples': 16096, 'counters/updates': 503}
train stats after 16128 examples: {'rewards_train/chosen': '0.0015809', 'rewards_train/rejected': '-0.0053831', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.006964', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-152.51', 'loss/train': '0.69254', 'examples_per_second': '31.586', 'grad_norm': '28.875', 'counters/examples': 16128, 'counters/updates': 504}
train stats after 16160 examples: {'rewards_train/chosen': '-0.022795', 'rewards_train/rejected': '0.0065732', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.029368', 'logps_train/rejected': '-116.37', 'logps_train/chosen': '-123.27', 'loss/train': '0.7143', 'examples_per_second': '31.665', 'grad_norm': '28.75', 'counters/examples': 16160, 'counters/updates': 505}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16224 examples: {'rewards_train/chosen': '-0.048644', 'rewards_train/rejected': '-0.01992', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028725', 'logps_train/rejected': '-120.97', 'logps_train/chosen': '-134.76', 'loss/train': '0.71039', 'examples_per_second': '31.854', 'grad_norm': '28.375', 'counters/examples': 16224, 'counters/updates': 507}
skipping logging after 16256 examples to avoid logging too frequently
train stats after 16288 examples: {'rewards_train/chosen': '-0.020725', 'rewards_train/rejected': '-0.039192', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018467', 'logps_train/rejected': '-91.154', 'logps_train/chosen': '-119.85', 'loss/train': '0.68871', 'examples_per_second': '30.782', 'grad_norm': '24.125', 'counters/examples': 16288, 'counters/updates': 509}
skipping logging after 16320 examples to avoid logging too frequently
train stats after 16352 examples: {'rewards_train/chosen': '-0.036976', 'rewards_train/rejected': '0.0043749', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041351', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-132.27', 'loss/train': '0.71779', 'examples_per_second': '30.448', 'grad_norm': '30', 'counters/examples': 16352, 'counters/updates': 511}
train stats after 16384 examples: {'rewards_train/chosen': '-0.03225', 'rewards_train/rejected': '-0.0267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.0055498', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-159.26', 'loss/train': '0.70106', 'examples_per_second': '31.675', 'grad_norm': '32.75', 'counters/examples': 16384, 'counters/updates': 512}
skipping logging after 16416 examples to avoid logging too frequently
train stats after 16448 examples: {'rewards_train/chosen': '0.012282', 'rewards_train/rejected': '-0.02243', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034713', 'logps_train/rejected': '-138.74', 'logps_train/chosen': '-155.67', 'loss/train': '0.6813', 'examples_per_second': '31.597', 'grad_norm': '32', 'counters/examples': 16448, 'counters/updates': 514}
train stats after 16480 examples: {'rewards_train/chosen': '-0.080674', 'rewards_train/rejected': '-0.015437', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.065237', 'logps_train/rejected': '-94.609', 'logps_train/chosen': '-121.01', 'loss/train': '0.73297', 'examples_per_second': '31.627', 'grad_norm': '28.5', 'counters/examples': 16480, 'counters/updates': 515}
skipping logging after 16512 examples to avoid logging too frequently
train stats after 16544 examples: {'rewards_train/chosen': '-0.051775', 'rewards_train/rejected': '0.021405', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.07318', 'logps_train/rejected': '-106.64', 'logps_train/chosen': '-172.93', 'loss/train': '0.74007', 'examples_per_second': '31.842', 'grad_norm': '31.875', 'counters/examples': 16544, 'counters/updates': 517}
train stats after 16576 examples: {'rewards_train/chosen': '-0.0013007', 'rewards_train/rejected': '0.0014284', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027291', 'logps_train/rejected': '-116.41', 'logps_train/chosen': '-106.26', 'loss/train': '0.69959', 'examples_per_second': '30.836', 'grad_norm': '24', 'counters/examples': 16576, 'counters/updates': 518}
train stats after 16608 examples: {'rewards_train/chosen': '-0.044894', 'rewards_train/rejected': '-0.044706', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00018815', 'logps_train/rejected': '-103.97', 'logps_train/chosen': '-115.51', 'loss/train': '0.69672', 'examples_per_second': '32.705', 'grad_norm': '26', 'counters/examples': 16608, 'counters/updates': 519}
train stats after 16640 examples: {'rewards_train/chosen': '-0.061026', 'rewards_train/rejected': '-0.006332', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.054694', 'logps_train/rejected': '-147.87', 'logps_train/chosen': '-155.73', 'loss/train': '0.72466', 'examples_per_second': '30.198', 'grad_norm': '38.5', 'counters/examples': 16640, 'counters/updates': 520}
train stats after 16672 examples: {'rewards_train/chosen': '0.0064227', 'rewards_train/rejected': '-0.056829', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063251', 'logps_train/rejected': '-128.39', 'logps_train/chosen': '-146.09', 'loss/train': '0.67484', 'examples_per_second': '30.748', 'grad_norm': '30.875', 'counters/examples': 16672, 'counters/updates': 521}
train stats after 16704 examples: {'rewards_train/chosen': '0.0085693', 'rewards_train/rejected': '-0.014556', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023125', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-124.38', 'loss/train': '0.68752', 'examples_per_second': '30.892', 'grad_norm': '27.875', 'counters/examples': 16704, 'counters/updates': 522}
train stats after 16736 examples: {'rewards_train/chosen': '-0.0082565', 'rewards_train/rejected': '0.071206', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.079463', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-117.15', 'loss/train': '0.74031', 'examples_per_second': '31.693', 'grad_norm': '30.375', 'counters/examples': 16736, 'counters/updates': 523}
train stats after 16768 examples: {'rewards_train/chosen': '-0.00040325', 'rewards_train/rejected': '-0.0018709', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0014676', 'logps_train/rejected': '-113.67', 'logps_train/chosen': '-144.28', 'loss/train': '0.69989', 'examples_per_second': '31.208', 'grad_norm': '33', 'counters/examples': 16768, 'counters/updates': 524}
train stats after 16800 examples: {'rewards_train/chosen': '-0.0026996', 'rewards_train/rejected': '0.030599', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033299', 'logps_train/rejected': '-176.2', 'logps_train/chosen': '-160.44', 'loss/train': '0.71628', 'examples_per_second': '31.714', 'grad_norm': '34', 'counters/examples': 16800, 'counters/updates': 525}
train stats after 16832 examples: {'rewards_train/chosen': '0.034593', 'rewards_train/rejected': '-0.0062673', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.040861', 'logps_train/rejected': '-155.52', 'logps_train/chosen': '-121.7', 'loss/train': '0.67778', 'examples_per_second': '30.554', 'grad_norm': '26.875', 'counters/examples': 16832, 'counters/updates': 526}
train stats after 16864 examples: {'rewards_train/chosen': '0.014002', 'rewards_train/rejected': '-0.00071727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014719', 'logps_train/rejected': '-185.03', 'logps_train/chosen': '-132.87', 'loss/train': '0.69334', 'examples_per_second': '31.726', 'grad_norm': '51.75', 'counters/examples': 16864, 'counters/updates': 527}
train stats after 16896 examples: {'rewards_train/chosen': '-0.0070132', 'rewards_train/rejected': '0.0049728', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011986', 'logps_train/rejected': '-140.76', 'logps_train/chosen': '-170.83', 'loss/train': '0.70722', 'examples_per_second': '31.664', 'grad_norm': '32.25', 'counters/examples': 16896, 'counters/updates': 528}
train stats after 16928 examples: {'rewards_train/chosen': '-0.055114', 'rewards_train/rejected': '-0.038984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01613', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-155.51', 'loss/train': '0.70651', 'examples_per_second': '30.157', 'grad_norm': '30.125', 'counters/examples': 16928, 'counters/updates': 529}
train stats after 16960 examples: {'rewards_train/chosen': '0.032432', 'rewards_train/rejected': '0.033885', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0014534', 'logps_train/rejected': '-76.246', 'logps_train/chosen': '-114.47', 'loss/train': '0.69799', 'examples_per_second': '30.895', 'grad_norm': '22.125', 'counters/examples': 16960, 'counters/updates': 530}
train stats after 16992 examples: {'rewards_train/chosen': '-0.052152', 'rewards_train/rejected': '-0.01455', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.037602', 'logps_train/rejected': '-193.44', 'logps_train/chosen': '-157.11', 'loss/train': '0.72622', 'examples_per_second': '30.086', 'grad_norm': '37', 'counters/examples': 16992, 'counters/updates': 531}
train stats after 17024 examples: {'rewards_train/chosen': '-0.036159', 'rewards_train/rejected': '-0.030828', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.005331', 'logps_train/rejected': '-134.54', 'logps_train/chosen': '-119.88', 'loss/train': '0.70079', 'examples_per_second': '31.661', 'grad_norm': '26.875', 'counters/examples': 17024, 'counters/updates': 532}
train stats after 17056 examples: {'rewards_train/chosen': '-0.01775', 'rewards_train/rejected': '-0.018918', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0011676', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-188.38', 'loss/train': '0.70058', 'examples_per_second': '31.707', 'grad_norm': '33.75', 'counters/examples': 17056, 'counters/updates': 533}
train stats after 17088 examples: {'rewards_train/chosen': '-0.033257', 'rewards_train/rejected': '0.04087', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.074126', 'logps_train/rejected': '-108.96', 'logps_train/chosen': '-164.66', 'loss/train': '0.73661', 'examples_per_second': '30.248', 'grad_norm': '35.5', 'counters/examples': 17088, 'counters/updates': 534}
train stats after 17120 examples: {'rewards_train/chosen': '-0.0020716', 'rewards_train/rejected': '-0.011314', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0092425', 'logps_train/rejected': '-113.48', 'logps_train/chosen': '-136.23', 'loss/train': '0.69627', 'examples_per_second': '31.136', 'grad_norm': '28.875', 'counters/examples': 17120, 'counters/updates': 535}
train stats after 17152 examples: {'rewards_train/chosen': '0.037886', 'rewards_train/rejected': '-0.050548', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088433', 'logps_train/rejected': '-129.69', 'logps_train/chosen': '-149.33', 'loss/train': '0.65532', 'examples_per_second': '30.198', 'grad_norm': '30.375', 'counters/examples': 17152, 'counters/updates': 536}
skipping logging after 17184 examples to avoid logging too frequently
train stats after 17216 examples: {'rewards_train/chosen': '0.031205', 'rewards_train/rejected': '0.013661', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017544', 'logps_train/rejected': '-110.17', 'logps_train/chosen': '-128.43', 'loss/train': '0.6896', 'examples_per_second': '30.719', 'grad_norm': '31.875', 'counters/examples': 17216, 'counters/updates': 538}
train stats after 17248 examples: {'rewards_train/chosen': '-0.052956', 'rewards_train/rejected': '-0.008652', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.044304', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-161.76', 'loss/train': '0.72048', 'examples_per_second': '33.212', 'grad_norm': '31.125', 'counters/examples': 17248, 'counters/updates': 539}
train stats after 17280 examples: {'rewards_train/chosen': '-0.035744', 'rewards_train/rejected': '0.0070381', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042782', 'logps_train/rejected': '-132.63', 'logps_train/chosen': '-117.26', 'loss/train': '0.72057', 'examples_per_second': '31.726', 'grad_norm': '28.875', 'counters/examples': 17280, 'counters/updates': 540}
train stats after 17312 examples: {'rewards_train/chosen': '-0.080495', 'rewards_train/rejected': '-0.013053', 'rewards_train/accuracies': '0.21875', 'rewards_train/margins': '-0.067442', 'logps_train/rejected': '-73.519', 'logps_train/chosen': '-147.82', 'loss/train': '0.73279', 'examples_per_second': '32.971', 'grad_norm': '27.625', 'counters/examples': 17312, 'counters/updates': 541}
train stats after 17344 examples: {'rewards_train/chosen': '-0.0567', 'rewards_train/rejected': '-0.01587', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04083', 'logps_train/rejected': '-105.08', 'logps_train/chosen': '-127.48', 'loss/train': '0.71878', 'examples_per_second': '32.975', 'grad_norm': '35', 'counters/examples': 17344, 'counters/updates': 542}
skipping logging after 17376 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '0.054448', 'rewards_train/rejected': '-0.035649', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090097', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-145.01', 'loss/train': '0.65804', 'examples_per_second': '30.565', 'grad_norm': '24.75', 'counters/examples': 17408, 'counters/updates': 544}
train stats after 17440 examples: {'rewards_train/chosen': '0.011444', 'rewards_train/rejected': '0.0094316', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0020126', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-130.47', 'loss/train': '0.69847', 'examples_per_second': '32.267', 'grad_norm': '30.875', 'counters/examples': 17440, 'counters/updates': 545}
train stats after 17472 examples: {'rewards_train/chosen': '-0.023453', 'rewards_train/rejected': '-0.0038167', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.019636', 'logps_train/rejected': '-109.45', 'logps_train/chosen': '-122.81', 'loss/train': '0.71304', 'examples_per_second': '29.532', 'grad_norm': '32.25', 'counters/examples': 17472, 'counters/updates': 546}
train stats after 17504 examples: {'rewards_train/chosen': '-0.012487', 'rewards_train/rejected': '0.023023', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.035509', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-117.43', 'loss/train': '0.71351', 'examples_per_second': '34.2', 'grad_norm': '29', 'counters/examples': 17504, 'counters/updates': 547}
train stats after 17536 examples: {'rewards_train/chosen': '-0.010117', 'rewards_train/rejected': '-0.046058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035941', 'logps_train/rejected': '-107.38', 'logps_train/chosen': '-138', 'loss/train': '0.68066', 'examples_per_second': '32.874', 'grad_norm': '29.125', 'counters/examples': 17536, 'counters/updates': 548}
train stats after 17568 examples: {'rewards_train/chosen': '-0.0083175', 'rewards_train/rejected': '-0.047185', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038868', 'logps_train/rejected': '-150.44', 'logps_train/chosen': '-147.53', 'loss/train': '0.68041', 'examples_per_second': '30.704', 'grad_norm': '29', 'counters/examples': 17568, 'counters/updates': 549}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17632 examples: {'rewards_train/chosen': '-0.021208', 'rewards_train/rejected': '-0.060813', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039605', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-109.46', 'loss/train': '0.67684', 'examples_per_second': '34.03', 'grad_norm': '29.125', 'counters/examples': 17632, 'counters/updates': 551}
train stats after 17664 examples: {'rewards_train/chosen': '-0.019175', 'rewards_train/rejected': '-0.023892', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0047176', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-134.44', 'loss/train': '0.69695', 'examples_per_second': '30.76', 'grad_norm': '27.25', 'counters/examples': 17664, 'counters/updates': 552}
skipping logging after 17696 examples to avoid logging too frequently
train stats after 17728 examples: {'rewards_train/chosen': '-0.020765', 'rewards_train/rejected': '-0.042807', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022042', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-142.47', 'loss/train': '0.68801', 'examples_per_second': '32.627', 'grad_norm': '29.875', 'counters/examples': 17728, 'counters/updates': 554}
train stats after 17760 examples: {'rewards_train/chosen': '0.0062468', 'rewards_train/rejected': '-0.0060522', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012299', 'logps_train/rejected': '-115.18', 'logps_train/chosen': '-106.94', 'loss/train': '0.69028', 'examples_per_second': '32.392', 'grad_norm': '31', 'counters/examples': 17760, 'counters/updates': 555}
train stats after 17792 examples: {'rewards_train/chosen': '0.016738', 'rewards_train/rejected': '0.0065446', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010193', 'logps_train/rejected': '-91.516', 'logps_train/chosen': '-102.58', 'loss/train': '0.69282', 'examples_per_second': '32.812', 'grad_norm': '22.25', 'counters/examples': 17792, 'counters/updates': 556}
train stats after 17824 examples: {'rewards_train/chosen': '-0.011986', 'rewards_train/rejected': '0.037011', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048997', 'logps_train/rejected': '-94.318', 'logps_train/chosen': '-138.38', 'loss/train': '0.72456', 'examples_per_second': '31.662', 'grad_norm': '38.5', 'counters/examples': 17824, 'counters/updates': 557}
train stats after 17856 examples: {'rewards_train/chosen': '0.12607', 'rewards_train/rejected': '-0.014423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14049', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-192.16', 'loss/train': '0.63458', 'examples_per_second': '31.701', 'grad_norm': '29.25', 'counters/examples': 17856, 'counters/updates': 558}
skipping logging after 17888 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '0.006868', 'rewards_train/rejected': '-0.036721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043589', 'logps_train/rejected': '-129.17', 'logps_train/chosen': '-115.48', 'loss/train': '0.67371', 'examples_per_second': '34.501', 'grad_norm': '23.375', 'counters/examples': 17920, 'counters/updates': 560}
skipping logging after 17952 examples to avoid logging too frequently
train stats after 17984 examples: {'rewards_train/chosen': '-0.033525', 'rewards_train/rejected': '0.01101', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044536', 'logps_train/rejected': '-115.28', 'logps_train/chosen': '-139.63', 'loss/train': '0.72258', 'examples_per_second': '33.375', 'grad_norm': '29.875', 'counters/examples': 17984, 'counters/updates': 562}
train stats after 18016 examples: {'rewards_train/chosen': '0.027563', 'rewards_train/rejected': '-0.013139', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040702', 'logps_train/rejected': '-102.65', 'logps_train/chosen': '-139.05', 'loss/train': '0.67603', 'examples_per_second': '32.858', 'grad_norm': '25.25', 'counters/examples': 18016, 'counters/updates': 563}
train stats after 18048 examples: {'rewards_train/chosen': '-0.013072', 'rewards_train/rejected': '-0.046318', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033246', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-143.88', 'loss/train': '0.68352', 'examples_per_second': '31.659', 'grad_norm': '30.125', 'counters/examples': 18048, 'counters/updates': 564}
train stats after 18080 examples: {'rewards_train/chosen': '-0.0030242', 'rewards_train/rejected': '-0.013503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010479', 'logps_train/rejected': '-157.12', 'logps_train/chosen': '-138.72', 'loss/train': '0.6956', 'examples_per_second': '30.901', 'grad_norm': '32', 'counters/examples': 18080, 'counters/updates': 565}
train stats after 18112 examples: {'rewards_train/chosen': '0.014509', 'rewards_train/rejected': '-0.033496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048005', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-109.76', 'loss/train': '0.67498', 'examples_per_second': '32.799', 'grad_norm': '24.375', 'counters/examples': 18112, 'counters/updates': 566}
train stats after 18144 examples: {'rewards_train/chosen': '0.0021487', 'rewards_train/rejected': '-0.0032291', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0053778', 'logps_train/rejected': '-100.05', 'logps_train/chosen': '-111.06', 'loss/train': '0.69281', 'examples_per_second': '30.16', 'grad_norm': '25.75', 'counters/examples': 18144, 'counters/updates': 567}
skipping logging after 18176 examples to avoid logging too frequently
train stats after 18208 examples: {'rewards_train/chosen': '-0.059168', 'rewards_train/rejected': '-0.022304', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036864', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-132.51', 'loss/train': '0.72106', 'examples_per_second': '37.384', 'grad_norm': '30.5', 'counters/examples': 18208, 'counters/updates': 569}
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18272 examples: {'rewards_train/chosen': '-0.010823', 'rewards_train/rejected': '-0.068744', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057921', 'logps_train/rejected': '-133.93', 'logps_train/chosen': '-165.08', 'loss/train': '0.66924', 'examples_per_second': '30.892', 'grad_norm': '30.625', 'counters/examples': 18272, 'counters/updates': 571}
train stats after 18304 examples: {'rewards_train/chosen': '0.026601', 'rewards_train/rejected': '-0.035111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061712', 'logps_train/rejected': '-139.23', 'logps_train/chosen': '-171.57', 'loss/train': '0.6692', 'examples_per_second': '17.841', 'grad_norm': '28.125', 'counters/examples': 18304, 'counters/updates': 572}
train stats after 18336 examples: {'rewards_train/chosen': '-0.03246', 'rewards_train/rejected': '-0.042826', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010366', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-140.94', 'loss/train': '0.69234', 'examples_per_second': '31.636', 'grad_norm': '27.875', 'counters/examples': 18336, 'counters/updates': 573}
train stats after 18368 examples: {'rewards_train/chosen': '0.024929', 'rewards_train/rejected': '0.0015324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023397', 'logps_train/rejected': '-111.64', 'logps_train/chosen': '-132.8', 'loss/train': '0.6852', 'examples_per_second': '31.544', 'grad_norm': '28.375', 'counters/examples': 18368, 'counters/updates': 574}
skipping logging after 18400 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '0.010012', 'rewards_train/rejected': '-0.030157', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040169', 'logps_train/rejected': '-113.83', 'logps_train/chosen': '-144.02', 'loss/train': '0.67884', 'examples_per_second': '30.748', 'grad_norm': '32.25', 'counters/examples': 18432, 'counters/updates': 576}
skipping logging after 18464 examples to avoid logging too frequently
train stats after 18496 examples: {'rewards_train/chosen': '0.0042075', 'rewards_train/rejected': '-0.030378', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034586', 'logps_train/rejected': '-141.42', 'logps_train/chosen': '-124.57', 'loss/train': '0.68275', 'examples_per_second': '31.846', 'grad_norm': '35.25', 'counters/examples': 18496, 'counters/updates': 578}
train stats after 18528 examples: {'rewards_train/chosen': '-0.020692', 'rewards_train/rejected': '-0.018735', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0019573', 'logps_train/rejected': '-127', 'logps_train/chosen': '-207.42', 'loss/train': '0.7003', 'examples_per_second': '31.594', 'grad_norm': '36.25', 'counters/examples': 18528, 'counters/updates': 579}
train stats after 18560 examples: {'rewards_train/chosen': '-0.015111', 'rewards_train/rejected': '-0.012937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0021739', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-121.84', 'loss/train': '0.69823', 'examples_per_second': '30.837', 'grad_norm': '28.25', 'counters/examples': 18560, 'counters/updates': 580}
train stats after 18592 examples: {'rewards_train/chosen': '-0.013067', 'rewards_train/rejected': '-0.037523', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024456', 'logps_train/rejected': '-78.173', 'logps_train/chosen': '-119.17', 'loss/train': '0.68353', 'examples_per_second': '32.738', 'grad_norm': '23.875', 'counters/examples': 18592, 'counters/updates': 581}
train stats after 18624 examples: {'rewards_train/chosen': '-0.017599', 'rewards_train/rejected': '-0.033264', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015664', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-171.16', 'loss/train': '0.69461', 'examples_per_second': '31.704', 'grad_norm': '34.5', 'counters/examples': 18624, 'counters/updates': 582}
skipping logging after 18656 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '-0.0096145', 'rewards_train/rejected': '-0.048429', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038815', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-145.89', 'loss/train': '0.68336', 'examples_per_second': '31.895', 'grad_norm': '28.5', 'counters/examples': 18688, 'counters/updates': 584}
train stats after 18720 examples: {'rewards_train/chosen': '0.0020157', 'rewards_train/rejected': '-0.040159', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042175', 'logps_train/rejected': '-103.59', 'logps_train/chosen': '-119.08', 'loss/train': '0.67834', 'examples_per_second': '29.946', 'grad_norm': '25', 'counters/examples': 18720, 'counters/updates': 585}
train stats after 18752 examples: {'rewards_train/chosen': '-0.022289', 'rewards_train/rejected': '0.010704', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.032993', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-154.46', 'loss/train': '0.71305', 'examples_per_second': '30.576', 'grad_norm': '33', 'counters/examples': 18752, 'counters/updates': 586}
train stats after 18784 examples: {'rewards_train/chosen': '0.0061619', 'rewards_train/rejected': '0.016597', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010435', 'logps_train/rejected': '-140.4', 'logps_train/chosen': '-141.62', 'loss/train': '0.7054', 'examples_per_second': '31.701', 'grad_norm': '35.75', 'counters/examples': 18784, 'counters/updates': 587}
train stats after 18816 examples: {'rewards_train/chosen': '0.037898', 'rewards_train/rejected': '-0.0049251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042823', 'logps_train/rejected': '-127.28', 'logps_train/chosen': '-147.93', 'loss/train': '0.6772', 'examples_per_second': '30.2', 'grad_norm': '30.5', 'counters/examples': 18816, 'counters/updates': 588}
skipping logging after 18848 examples to avoid logging too frequently
train stats after 18880 examples: {'rewards_train/chosen': '-0.037239', 'rewards_train/rejected': '-0.043064', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0058242', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-176.91', 'loss/train': '0.69364', 'examples_per_second': '32.919', 'grad_norm': '28.75', 'counters/examples': 18880, 'counters/updates': 590}
train stats after 18912 examples: {'rewards_train/chosen': '-0.0001339', 'rewards_train/rejected': '-0.0064163', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0062824', 'logps_train/rejected': '-95.439', 'logps_train/chosen': '-131.12', 'loss/train': '0.69456', 'examples_per_second': '30.174', 'grad_norm': '26', 'counters/examples': 18912, 'counters/updates': 591}
train stats after 18944 examples: {'rewards_train/chosen': '0.0051753', 'rewards_train/rejected': '-0.037642', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042817', 'logps_train/rejected': '-144.91', 'logps_train/chosen': '-138.87', 'loss/train': '0.67977', 'examples_per_second': '31.7', 'grad_norm': '30.625', 'counters/examples': 18944, 'counters/updates': 592}
train stats after 18976 examples: {'rewards_train/chosen': '0.056925', 'rewards_train/rejected': '-0.023808', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080733', 'logps_train/rejected': '-114.96', 'logps_train/chosen': '-141.26', 'loss/train': '0.66177', 'examples_per_second': '32.315', 'grad_norm': '27.75', 'counters/examples': 18976, 'counters/updates': 593}
train stats after 19008 examples: {'rewards_train/chosen': '-0.036863', 'rewards_train/rejected': '0.021874', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.058737', 'logps_train/rejected': '-135.3', 'logps_train/chosen': '-161.27', 'loss/train': '0.7291', 'examples_per_second': '31.682', 'grad_norm': '40', 'counters/examples': 19008, 'counters/updates': 594}
skipping logging after 19040 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '-0.021676', 'rewards_train/rejected': '-0.0096291', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012047', 'logps_train/rejected': '-137.35', 'logps_train/chosen': '-167.32', 'loss/train': '0.70559', 'examples_per_second': '23.075', 'grad_norm': '45', 'counters/examples': 19072, 'counters/updates': 596}
skipping logging after 19104 examples to avoid logging too frequently
train stats after 19136 examples: {'rewards_train/chosen': '0.024121', 'rewards_train/rejected': '-0.039612', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063733', 'logps_train/rejected': '-112.46', 'logps_train/chosen': '-132.24', 'loss/train': '0.66559', 'examples_per_second': '32.243', 'grad_norm': '24.375', 'counters/examples': 19136, 'counters/updates': 598}
train stats after 19168 examples: {'rewards_train/chosen': '-0.018546', 'rewards_train/rejected': '-0.02711', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0085642', 'logps_train/rejected': '-146.96', 'logps_train/chosen': '-150.57', 'loss/train': '0.69507', 'examples_per_second': '23.673', 'grad_norm': '30.5', 'counters/examples': 19168, 'counters/updates': 599}
train stats after 19200 examples: {'rewards_train/chosen': '-0.00012019', 'rewards_train/rejected': '-0.043632', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043511', 'logps_train/rejected': '-122.56', 'logps_train/chosen': '-169.98', 'loss/train': '0.67543', 'examples_per_second': '30.193', 'grad_norm': '31.75', 'counters/examples': 19200, 'counters/updates': 600}
skipping logging after 19232 examples to avoid logging too frequently
train stats after 19264 examples: {'rewards_train/chosen': '0.045482', 'rewards_train/rejected': '0.0011714', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044311', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-144.55', 'loss/train': '0.67892', 'examples_per_second': '33.218', 'grad_norm': '30.625', 'counters/examples': 19264, 'counters/updates': 602}
train stats after 19296 examples: {'rewards_train/chosen': '-0.012981', 'rewards_train/rejected': '-0.036084', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023103', 'logps_train/rejected': '-137.57', 'logps_train/chosen': '-164.39', 'loss/train': '0.69065', 'examples_per_second': '31.206', 'grad_norm': '31.875', 'counters/examples': 19296, 'counters/updates': 603}
train stats after 19328 examples: {'rewards_train/chosen': '0.0058132', 'rewards_train/rejected': '-0.022881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028695', 'logps_train/rejected': '-110.89', 'logps_train/chosen': '-145.31', 'loss/train': '0.68351', 'examples_per_second': '32.458', 'grad_norm': '28.5', 'counters/examples': 19328, 'counters/updates': 604}
train stats after 19360 examples: {'rewards_train/chosen': '0.060885', 'rewards_train/rejected': '-0.022366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083252', 'logps_train/rejected': '-115.12', 'logps_train/chosen': '-131.45', 'loss/train': '0.65983', 'examples_per_second': '30.242', 'grad_norm': '29.125', 'counters/examples': 19360, 'counters/updates': 605}
train stats after 19392 examples: {'rewards_train/chosen': '0.0037296', 'rewards_train/rejected': '0.041344', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.037614', 'logps_train/rejected': '-152.53', 'logps_train/chosen': '-136.25', 'loss/train': '0.71835', 'examples_per_second': '31.684', 'grad_norm': '30.75', 'counters/examples': 19392, 'counters/updates': 606}
train stats after 19424 examples: {'rewards_train/chosen': '-0.015938', 'rewards_train/rejected': '-0.0090482', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00689', 'logps_train/rejected': '-136.26', 'logps_train/chosen': '-135.99', 'loss/train': '0.70286', 'examples_per_second': '30.858', 'grad_norm': '36', 'counters/examples': 19424, 'counters/updates': 607}
skipping logging after 19456 examples to avoid logging too frequently
train stats after 19488 examples: {'rewards_train/chosen': '-0.0029969', 'rewards_train/rejected': '-0.019527', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01653', 'logps_train/rejected': '-161.98', 'logps_train/chosen': '-153.22', 'loss/train': '0.69214', 'examples_per_second': '33.423', 'grad_norm': '28.625', 'counters/examples': 19488, 'counters/updates': 609}
train stats after 19520 examples: {'rewards_train/chosen': '-0.019221', 'rewards_train/rejected': '-0.042639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023417', 'logps_train/rejected': '-111.33', 'logps_train/chosen': '-133.3', 'loss/train': '0.68657', 'examples_per_second': '31.039', 'grad_norm': '28.375', 'counters/examples': 19520, 'counters/updates': 610}
train stats after 19552 examples: {'rewards_train/chosen': '0.016967', 'rewards_train/rejected': '-0.025806', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042773', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-157.53', 'loss/train': '0.67964', 'examples_per_second': '32.616', 'grad_norm': '29.25', 'counters/examples': 19552, 'counters/updates': 611}
train stats after 19584 examples: {'rewards_train/chosen': '0.039139', 'rewards_train/rejected': '-0.055891', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09503', 'logps_train/rejected': '-162.56', 'logps_train/chosen': '-155.6', 'loss/train': '0.66189', 'examples_per_second': '31.878', 'grad_norm': '27.625', 'counters/examples': 19584, 'counters/updates': 612}
train stats after 19616 examples: {'rewards_train/chosen': '-0.0039421', 'rewards_train/rejected': '-0.021811', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017869', 'logps_train/rejected': '-77.182', 'logps_train/chosen': '-109.3', 'loss/train': '0.68579', 'examples_per_second': '31.891', 'grad_norm': '24.5', 'counters/examples': 19616, 'counters/updates': 613}
train stats after 19648 examples: {'rewards_train/chosen': '-0.091386', 'rewards_train/rejected': '-0.050639', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.040747', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-149.61', 'loss/train': '0.72193', 'examples_per_second': '30.681', 'grad_norm': '30', 'counters/examples': 19648, 'counters/updates': 614}
train stats after 19680 examples: {'rewards_train/chosen': '0.0024149', 'rewards_train/rejected': '-0.0037776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0061925', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-159.77', 'loss/train': '0.69367', 'examples_per_second': '31.417', 'grad_norm': '35.5', 'counters/examples': 19680, 'counters/updates': 615}
train stats after 19712 examples: {'rewards_train/chosen': '-0.032409', 'rewards_train/rejected': '-0.013174', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019235', 'logps_train/rejected': '-116.31', 'logps_train/chosen': '-116.93', 'loss/train': '0.70901', 'examples_per_second': '31.688', 'grad_norm': '33.75', 'counters/examples': 19712, 'counters/updates': 616}
train stats after 19744 examples: {'rewards_train/chosen': '-0.014476', 'rewards_train/rejected': '-0.054738', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040262', 'logps_train/rejected': '-122.07', 'logps_train/chosen': '-124.26', 'loss/train': '0.67845', 'examples_per_second': '31.97', 'grad_norm': '27.375', 'counters/examples': 19744, 'counters/updates': 617}
train stats after 19776 examples: {'rewards_train/chosen': '0.041595', 'rewards_train/rejected': '-0.025035', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06663', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-137.25', 'loss/train': '0.6686', 'examples_per_second': '30.898', 'grad_norm': '31.375', 'counters/examples': 19776, 'counters/updates': 618}
train stats after 19808 examples: {'rewards_train/chosen': '0.013999', 'rewards_train/rejected': '0.010849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0031503', 'logps_train/rejected': '-129.22', 'logps_train/chosen': '-135.22', 'loss/train': '0.69618', 'examples_per_second': '31.735', 'grad_norm': '27.375', 'counters/examples': 19808, 'counters/updates': 619}
train stats after 19840 examples: {'rewards_train/chosen': '-0.053397', 'rewards_train/rejected': '-0.026765', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.026632', 'logps_train/rejected': '-135.81', 'logps_train/chosen': '-164.56', 'loss/train': '0.71647', 'examples_per_second': '32.844', 'grad_norm': '30.375', 'counters/examples': 19840, 'counters/updates': 620}
skipping logging after 19872 examples to avoid logging too frequently
train stats after 19904 examples: {'rewards_train/chosen': '-0.033452', 'rewards_train/rejected': '-0.038126', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0046742', 'logps_train/rejected': '-159.31', 'logps_train/chosen': '-163.51', 'loss/train': '0.70356', 'examples_per_second': '30.184', 'grad_norm': '41.75', 'counters/examples': 19904, 'counters/updates': 622}
train stats after 19936 examples: {'rewards_train/chosen': '-0.048111', 'rewards_train/rejected': '0.014299', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.06241', 'logps_train/rejected': '-96.186', 'logps_train/chosen': '-168.39', 'loss/train': '0.72996', 'examples_per_second': '32.276', 'grad_norm': '28.5', 'counters/examples': 19936, 'counters/updates': 623}
skipping logging after 19968 examples to avoid logging too frequently
train stats after 20000 examples: {'rewards_train/chosen': '-0.0061967', 'rewards_train/rejected': '-0.017207', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01101', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-186.36', 'loss/train': '0.69249', 'examples_per_second': '30.607', 'grad_norm': '33.25', 'counters/examples': 20000, 'counters/updates': 625}
train stats after 20032 examples: {'rewards_train/chosen': '-0.060809', 'rewards_train/rejected': '-0.010734', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.050075', 'logps_train/rejected': '-111.18', 'logps_train/chosen': '-146.11', 'loss/train': '0.72401', 'examples_per_second': '30.349', 'grad_norm': '35.25', 'counters/examples': 20032, 'counters/updates': 626}
skipping logging after 20064 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '0.044283', 'rewards_train/rejected': '-0.056253', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10054', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-165.67', 'loss/train': '0.6484', 'examples_per_second': '34.823', 'grad_norm': '27.125', 'counters/examples': 20096, 'counters/updates': 628}
train stats after 20128 examples: {'rewards_train/chosen': '-0.031363', 'rewards_train/rejected': '-0.037984', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0066215', 'logps_train/rejected': '-155.97', 'logps_train/chosen': '-134.32', 'loss/train': '0.69475', 'examples_per_second': '31.643', 'grad_norm': '34.75', 'counters/examples': 20128, 'counters/updates': 629}
train stats after 20160 examples: {'rewards_train/chosen': '0.023845', 'rewards_train/rejected': '-0.031696', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055541', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-155.1', 'loss/train': '0.67528', 'examples_per_second': '31.759', 'grad_norm': '29.375', 'counters/examples': 20160, 'counters/updates': 630}
train stats after 20192 examples: {'rewards_train/chosen': '-0.057486', 'rewards_train/rejected': '-0.042103', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015384', 'logps_train/rejected': '-130.1', 'logps_train/chosen': '-128.06', 'loss/train': '0.70628', 'examples_per_second': '31.029', 'grad_norm': '26.875', 'counters/examples': 20192, 'counters/updates': 631}
train stats after 20224 examples: {'rewards_train/chosen': '-0.0088655', 'rewards_train/rejected': '-0.041486', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03262', 'logps_train/rejected': '-137.35', 'logps_train/chosen': '-199.21', 'loss/train': '0.68563', 'examples_per_second': '31.653', 'grad_norm': '33', 'counters/examples': 20224, 'counters/updates': 632}
train stats after 20256 examples: {'rewards_train/chosen': '-0.012219', 'rewards_train/rejected': '-0.024139', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01192', 'logps_train/rejected': '-164', 'logps_train/chosen': '-190.64', 'loss/train': '0.69771', 'examples_per_second': '31.247', 'grad_norm': '34.5', 'counters/examples': 20256, 'counters/updates': 633}
train stats after 20288 examples: {'rewards_train/chosen': '0.030219', 'rewards_train/rejected': '0.014118', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0161', 'logps_train/rejected': '-104.44', 'logps_train/chosen': '-130.99', 'loss/train': '0.68936', 'examples_per_second': '30.194', 'grad_norm': '25.375', 'counters/examples': 20288, 'counters/updates': 634}
train stats after 20320 examples: {'rewards_train/chosen': '0.029551', 'rewards_train/rejected': '0.010857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018694', 'logps_train/rejected': '-89.955', 'logps_train/chosen': '-119.4', 'loss/train': '0.69021', 'examples_per_second': '30.937', 'grad_norm': '27.75', 'counters/examples': 20320, 'counters/updates': 635}
train stats after 20352 examples: {'rewards_train/chosen': '-0.046633', 'rewards_train/rejected': '-0.035392', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011241', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-156.04', 'loss/train': '0.70305', 'examples_per_second': '31.385', 'grad_norm': '32.5', 'counters/examples': 20352, 'counters/updates': 636}
train stats after 20384 examples: {'rewards_train/chosen': '0.048466', 'rewards_train/rejected': '-0.012622', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061088', 'logps_train/rejected': '-160.73', 'logps_train/chosen': '-153.15', 'loss/train': '0.67188', 'examples_per_second': '30.883', 'grad_norm': '30.625', 'counters/examples': 20384, 'counters/updates': 637}
train stats after 20416 examples: {'rewards_train/chosen': '-0.0076388', 'rewards_train/rejected': '0.0026923', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010331', 'logps_train/rejected': '-121.48', 'logps_train/chosen': '-150.3', 'loss/train': '0.70198', 'examples_per_second': '31.076', 'grad_norm': '28.5', 'counters/examples': 20416, 'counters/updates': 638}
train stats after 20448 examples: {'rewards_train/chosen': '-0.042133', 'rewards_train/rejected': '-0.022667', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019465', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-138.64', 'loss/train': '0.70675', 'examples_per_second': '31.573', 'grad_norm': '27.625', 'counters/examples': 20448, 'counters/updates': 639}
train stats after 20480 examples: {'rewards_train/chosen': '-0.00075537', 'rewards_train/rejected': '-0.043397', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042641', 'logps_train/rejected': '-181.89', 'logps_train/chosen': '-154.84', 'loss/train': '0.67828', 'examples_per_second': '31.677', 'grad_norm': '33.5', 'counters/examples': 20480, 'counters/updates': 640}
train stats after 20512 examples: {'rewards_train/chosen': '-0.020211', 'rewards_train/rejected': '-0.004991', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01522', 'logps_train/rejected': '-99.74', 'logps_train/chosen': '-147.21', 'loss/train': '0.70567', 'examples_per_second': '31.697', 'grad_norm': '33.75', 'counters/examples': 20512, 'counters/updates': 641}
train stats after 20544 examples: {'rewards_train/chosen': '-0.065843', 'rewards_train/rejected': '-0.034361', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031482', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-141.07', 'loss/train': '0.71609', 'examples_per_second': '30.962', 'grad_norm': '30.25', 'counters/examples': 20544, 'counters/updates': 642}
train stats after 20576 examples: {'rewards_train/chosen': '0.0076058', 'rewards_train/rejected': '0.0070937', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00051203', 'logps_train/rejected': '-150.1', 'logps_train/chosen': '-182.27', 'loss/train': '0.69849', 'examples_per_second': '31.694', 'grad_norm': '39.5', 'counters/examples': 20576, 'counters/updates': 643}
skipping logging after 20608 examples to avoid logging too frequently
train stats after 20640 examples: {'rewards_train/chosen': '0.0014462', 'rewards_train/rejected': '0.020638', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019192', 'logps_train/rejected': '-92.452', 'logps_train/chosen': '-148.9', 'loss/train': '0.70672', 'examples_per_second': '33.343', 'grad_norm': '31.5', 'counters/examples': 20640, 'counters/updates': 645}
train stats after 20672 examples: {'rewards_train/chosen': '-0.00090348', 'rewards_train/rejected': '-0.0038421', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0029386', 'logps_train/rejected': '-98.05', 'logps_train/chosen': '-134.09', 'loss/train': '0.69736', 'examples_per_second': '30.39', 'grad_norm': '32', 'counters/examples': 20672, 'counters/updates': 646}
train stats after 20704 examples: {'rewards_train/chosen': '-0.032663', 'rewards_train/rejected': '0.026414', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.059077', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-152.11', 'loss/train': '0.72672', 'examples_per_second': '31.644', 'grad_norm': '29.5', 'counters/examples': 20704, 'counters/updates': 647}
train stats after 20736 examples: {'rewards_train/chosen': '-0.034556', 'rewards_train/rejected': '-0.053029', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.018473', 'logps_train/rejected': '-138.79', 'logps_train/chosen': '-196.47', 'loss/train': '0.68733', 'examples_per_second': '30.273', 'grad_norm': '37', 'counters/examples': 20736, 'counters/updates': 648}
train stats after 20768 examples: {'rewards_train/chosen': '-0.0084255', 'rewards_train/rejected': '0.010861', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019286', 'logps_train/rejected': '-117.83', 'logps_train/chosen': '-124.61', 'loss/train': '0.70641', 'examples_per_second': '31.117', 'grad_norm': '31.125', 'counters/examples': 20768, 'counters/updates': 649}
train stats after 20800 examples: {'rewards_train/chosen': '-0.015559', 'rewards_train/rejected': '-0.014347', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0012123', 'logps_train/rejected': '-121.21', 'logps_train/chosen': '-113.06', 'loss/train': '0.69715', 'examples_per_second': '31.091', 'grad_norm': '24.875', 'counters/examples': 20800, 'counters/updates': 650}
train stats after 20832 examples: {'rewards_train/chosen': '-0.037539', 'rewards_train/rejected': '-0.012811', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.024728', 'logps_train/rejected': '-104.54', 'logps_train/chosen': '-128.86', 'loss/train': '0.70838', 'examples_per_second': '30.037', 'grad_norm': '29.25', 'counters/examples': 20832, 'counters/updates': 651}
train stats after 20864 examples: {'rewards_train/chosen': '0.037805', 'rewards_train/rejected': '0.0034859', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034319', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-158.54', 'loss/train': '0.68674', 'examples_per_second': '33.1', 'grad_norm': '29.75', 'counters/examples': 20864, 'counters/updates': 652}
train stats after 20896 examples: {'rewards_train/chosen': '-0.0053575', 'rewards_train/rejected': '-0.036742', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.031384', 'logps_train/rejected': '-102.34', 'logps_train/chosen': '-159.14', 'loss/train': '0.68087', 'examples_per_second': '32.404', 'grad_norm': '27.375', 'counters/examples': 20896, 'counters/updates': 653}
train stats after 20928 examples: {'rewards_train/chosen': '-0.014516', 'rewards_train/rejected': '-0.014172', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00034379', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-136.79', 'loss/train': '0.69895', 'examples_per_second': '31.346', 'grad_norm': '30', 'counters/examples': 20928, 'counters/updates': 654}
train stats after 20960 examples: {'rewards_train/chosen': '-0.023282', 'rewards_train/rejected': '0.014184', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037467', 'logps_train/rejected': '-122.68', 'logps_train/chosen': '-142.74', 'loss/train': '0.71712', 'examples_per_second': '30.43', 'grad_norm': '30.375', 'counters/examples': 20960, 'counters/updates': 655}
train stats after 20992 examples: {'rewards_train/chosen': '-0.0066523', 'rewards_train/rejected': '0.0076625', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014315', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-154.5', 'loss/train': '0.70765', 'examples_per_second': '31.665', 'grad_norm': '37.5', 'counters/examples': 20992, 'counters/updates': 656}
train stats after 21024 examples: {'rewards_train/chosen': '0.028918', 'rewards_train/rejected': '0.010518', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0184', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-118.9', 'loss/train': '0.6877', 'examples_per_second': '31.158', 'grad_norm': '25.75', 'counters/examples': 21024, 'counters/updates': 657}
train stats after 21056 examples: {'rewards_train/chosen': '0.0070197', 'rewards_train/rejected': '-0.044013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051033', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-181.25', 'loss/train': '0.67753', 'examples_per_second': '31.505', 'grad_norm': '47', 'counters/examples': 21056, 'counters/updates': 658}
train stats after 21088 examples: {'rewards_train/chosen': '-0.062764', 'rewards_train/rejected': '0.01286', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.075624', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-119.85', 'loss/train': '0.73773', 'examples_per_second': '31.549', 'grad_norm': '31.25', 'counters/examples': 21088, 'counters/updates': 659}
train stats after 21120 examples: {'rewards_train/chosen': '-0.0021981', 'rewards_train/rejected': '0.034083', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036281', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-142.57', 'loss/train': '0.71516', 'examples_per_second': '31.492', 'grad_norm': '30.875', 'counters/examples': 21120, 'counters/updates': 660}
train stats after 21152 examples: {'rewards_train/chosen': '-0.0082286', 'rewards_train/rejected': '-0.037499', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02927', 'logps_train/rejected': '-113.51', 'logps_train/chosen': '-134.51', 'loss/train': '0.69516', 'examples_per_second': '31.868', 'grad_norm': '43.5', 'counters/examples': 21152, 'counters/updates': 661}
train stats after 21184 examples: {'rewards_train/chosen': '-0.0056945', 'rewards_train/rejected': '-0.0029866', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0027078', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-132.61', 'loss/train': '0.70047', 'examples_per_second': '30.804', 'grad_norm': '33.75', 'counters/examples': 21184, 'counters/updates': 662}
skipping logging after 21216 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '0.0018522', 'rewards_train/rejected': '-0.049412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051264', 'logps_train/rejected': '-111.48', 'logps_train/chosen': '-114.02', 'loss/train': '0.66977', 'examples_per_second': '32.268', 'grad_norm': '24.5', 'counters/examples': 21248, 'counters/updates': 664}
train stats after 21280 examples: {'rewards_train/chosen': '0.039122', 'rewards_train/rejected': '0.011595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027527', 'logps_train/rejected': '-133.12', 'logps_train/chosen': '-146.46', 'loss/train': '0.68408', 'examples_per_second': '30.855', 'grad_norm': '31.75', 'counters/examples': 21280, 'counters/updates': 665}
train stats after 21312 examples: {'rewards_train/chosen': '-0.031671', 'rewards_train/rejected': '-0.029189', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0024821', 'logps_train/rejected': '-144.06', 'logps_train/chosen': '-140.17', 'loss/train': '0.70236', 'examples_per_second': '32.99', 'grad_norm': '31.875', 'counters/examples': 21312, 'counters/updates': 666}
train stats after 21344 examples: {'rewards_train/chosen': '-0.0073316', 'rewards_train/rejected': '-0.0018752', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0054564', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-140.4', 'loss/train': '0.69987', 'examples_per_second': '30.677', 'grad_norm': '31', 'counters/examples': 21344, 'counters/updates': 667}
train stats after 21376 examples: {'rewards_train/chosen': '0.010905', 'rewards_train/rejected': '-0.00082547', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011731', 'logps_train/rejected': '-164.11', 'logps_train/chosen': '-132.76', 'loss/train': '0.69526', 'examples_per_second': '31.748', 'grad_norm': '33.25', 'counters/examples': 21376, 'counters/updates': 668}
train stats after 21408 examples: {'rewards_train/chosen': '-0.0088291', 'rewards_train/rejected': '-0.037372', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028543', 'logps_train/rejected': '-95.115', 'logps_train/chosen': '-145.64', 'loss/train': '0.68503', 'examples_per_second': '31.278', 'grad_norm': '27.25', 'counters/examples': 21408, 'counters/updates': 669}
train stats after 21440 examples: {'rewards_train/chosen': '0.036535', 'rewards_train/rejected': '-0.0030708', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039606', 'logps_train/rejected': '-132.23', 'logps_train/chosen': '-167.59', 'loss/train': '0.67892', 'examples_per_second': '31.608', 'grad_norm': '34.25', 'counters/examples': 21440, 'counters/updates': 670}
train stats after 21472 examples: {'rewards_train/chosen': '0.10546', 'rewards_train/rejected': '0.0045719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10089', 'logps_train/rejected': '-114.47', 'logps_train/chosen': '-146.63', 'loss/train': '0.65099', 'examples_per_second': '31.667', 'grad_norm': '25.75', 'counters/examples': 21472, 'counters/updates': 671}
train stats after 21504 examples: {'rewards_train/chosen': '0.0035669', 'rewards_train/rejected': '-0.036022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039589', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-165.83', 'loss/train': '0.68004', 'examples_per_second': '31.633', 'grad_norm': '47.75', 'counters/examples': 21504, 'counters/updates': 672}
train stats after 21536 examples: {'rewards_train/chosen': '0.031034', 'rewards_train/rejected': '-0.0027772', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.033811', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-158.77', 'loss/train': '0.67982', 'examples_per_second': '31.519', 'grad_norm': '27.875', 'counters/examples': 21536, 'counters/updates': 673}
train stats after 21568 examples: {'rewards_train/chosen': '-0.04077', 'rewards_train/rejected': '0.019079', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.059849', 'logps_train/rejected': '-126.88', 'logps_train/chosen': '-143.58', 'loss/train': '0.73326', 'examples_per_second': '30.248', 'grad_norm': '28.75', 'counters/examples': 21568, 'counters/updates': 674}
train stats after 21600 examples: {'rewards_train/chosen': '-0.026451', 'rewards_train/rejected': '-0.0060933', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020357', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-154.48', 'loss/train': '0.70756', 'examples_per_second': '31.63', 'grad_norm': '29.25', 'counters/examples': 21600, 'counters/updates': 675}
train stats after 21632 examples: {'rewards_train/chosen': '-0.019989', 'rewards_train/rejected': '0.01633', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.03632', 'logps_train/rejected': '-95.104', 'logps_train/chosen': '-113.69', 'loss/train': '0.71758', 'examples_per_second': '31.292', 'grad_norm': '23.875', 'counters/examples': 21632, 'counters/updates': 676}
train stats after 21664 examples: {'rewards_train/chosen': '-0.080657', 'rewards_train/rejected': '-0.030378', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.050279', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-185.07', 'loss/train': '0.7251', 'examples_per_second': '31.665', 'grad_norm': '30.75', 'counters/examples': 21664, 'counters/updates': 677}
train stats after 21696 examples: {'rewards_train/chosen': '0.024504', 'rewards_train/rejected': '0.034249', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.009745', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-140.54', 'loss/train': '0.70403', 'examples_per_second': '30.695', 'grad_norm': '35.75', 'counters/examples': 21696, 'counters/updates': 678}
train stats after 21728 examples: {'rewards_train/chosen': '0.061701', 'rewards_train/rejected': '0.0082684', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053433', 'logps_train/rejected': '-156.78', 'logps_train/chosen': '-162.42', 'loss/train': '0.67325', 'examples_per_second': '32.03', 'grad_norm': '29.5', 'counters/examples': 21728, 'counters/updates': 679}
skipping logging after 21760 examples to avoid logging too frequently
train stats after 21792 examples: {'rewards_train/chosen': '-0.01639', 'rewards_train/rejected': '-0.0077738', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0086162', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-165.04', 'loss/train': '0.70221', 'examples_per_second': '30.6', 'grad_norm': '30.875', 'counters/examples': 21792, 'counters/updates': 681}
train stats after 21824 examples: {'rewards_train/chosen': '0.0020436', 'rewards_train/rejected': '-0.055216', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057259', 'logps_train/rejected': '-117.23', 'logps_train/chosen': '-144.52', 'loss/train': '0.66877', 'examples_per_second': '31.02', 'grad_norm': '29.25', 'counters/examples': 21824, 'counters/updates': 682}
train stats after 21856 examples: {'rewards_train/chosen': '0.026494', 'rewards_train/rejected': '-0.040849', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067342', 'logps_train/rejected': '-136.65', 'logps_train/chosen': '-123.53', 'loss/train': '0.66447', 'examples_per_second': '31.689', 'grad_norm': '29.375', 'counters/examples': 21856, 'counters/updates': 683}
skipping logging after 21888 examples to avoid logging too frequently
train stats after 21920 examples: {'rewards_train/chosen': '-0.016061', 'rewards_train/rejected': '-0.0046415', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01142', 'logps_train/rejected': '-113.58', 'logps_train/chosen': '-151.61', 'loss/train': '0.7045', 'examples_per_second': '31.039', 'grad_norm': '35.25', 'counters/examples': 21920, 'counters/updates': 685}
train stats after 21952 examples: {'rewards_train/chosen': '0.0051779', 'rewards_train/rejected': '-0.0052368', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010415', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-149.18', 'loss/train': '0.69492', 'examples_per_second': '31.047', 'grad_norm': '54', 'counters/examples': 21952, 'counters/updates': 686}
train stats after 21984 examples: {'rewards_train/chosen': '-0.024581', 'rewards_train/rejected': '-0.018837', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0057447', 'logps_train/rejected': '-151.51', 'logps_train/chosen': '-161.65', 'loss/train': '0.70073', 'examples_per_second': '31.428', 'grad_norm': '38.25', 'counters/examples': 21984, 'counters/updates': 687}
train stats after 22016 examples: {'rewards_train/chosen': '-0.010581', 'rewards_train/rejected': '-0.039369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028788', 'logps_train/rejected': '-144.46', 'logps_train/chosen': '-106.73', 'loss/train': '0.68052', 'examples_per_second': '31.113', 'grad_norm': '26.625', 'counters/examples': 22016, 'counters/updates': 688}
train stats after 22048 examples: {'rewards_train/chosen': '-0.00090923', 'rewards_train/rejected': '-0.0038335', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0029243', 'logps_train/rejected': '-96.969', 'logps_train/chosen': '-126.5', 'loss/train': '0.6939', 'examples_per_second': '30.88', 'grad_norm': '26.875', 'counters/examples': 22048, 'counters/updates': 689}
train stats after 22080 examples: {'rewards_train/chosen': '-0.036742', 'rewards_train/rejected': '-0.088087', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051345', 'logps_train/rejected': '-175.64', 'logps_train/chosen': '-172.34', 'loss/train': '0.678', 'examples_per_second': '31.738', 'grad_norm': '29.25', 'counters/examples': 22080, 'counters/updates': 690}
skipping logging after 22112 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '-0.023322', 'rewards_train/rejected': '-0.013366', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0099553', 'logps_train/rejected': '-94.671', 'logps_train/chosen': '-114.54', 'loss/train': '0.70355', 'examples_per_second': '30.658', 'grad_norm': '24.5', 'counters/examples': 22144, 'counters/updates': 692}
train stats after 22176 examples: {'rewards_train/chosen': '-0.048623', 'rewards_train/rejected': '-0.013278', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.035344', 'logps_train/rejected': '-118.08', 'logps_train/chosen': '-102.24', 'loss/train': '0.71485', 'examples_per_second': '31.758', 'grad_norm': '30.625', 'counters/examples': 22176, 'counters/updates': 693}
train stats after 22208 examples: {'rewards_train/chosen': '0.007186', 'rewards_train/rejected': '-0.023968', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031154', 'logps_train/rejected': '-113.59', 'logps_train/chosen': '-155.26', 'loss/train': '0.68045', 'examples_per_second': '32.044', 'grad_norm': '33.5', 'counters/examples': 22208, 'counters/updates': 694}
train stats after 22240 examples: {'rewards_train/chosen': '-0.0022925', 'rewards_train/rejected': '-0.0054136', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0031211', 'logps_train/rejected': '-107.6', 'logps_train/chosen': '-150.66', 'loss/train': '0.6944', 'examples_per_second': '31.96', 'grad_norm': '26.75', 'counters/examples': 22240, 'counters/updates': 695}
train stats after 22272 examples: {'rewards_train/chosen': '-0.030226', 'rewards_train/rejected': '-0.049001', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018776', 'logps_train/rejected': '-109.8', 'logps_train/chosen': '-106.3', 'loss/train': '0.68839', 'examples_per_second': '30.225', 'grad_norm': '27.625', 'counters/examples': 22272, 'counters/updates': 696}
train stats after 22304 examples: {'rewards_train/chosen': '0.049197', 'rewards_train/rejected': '-0.003716', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052913', 'logps_train/rejected': '-148.55', 'logps_train/chosen': '-185.7', 'loss/train': '0.67628', 'examples_per_second': '31.672', 'grad_norm': '31.25', 'counters/examples': 22304, 'counters/updates': 697}
train stats after 22336 examples: {'rewards_train/chosen': '-0.029396', 'rewards_train/rejected': '-0.02553', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0038655', 'logps_train/rejected': '-123.28', 'logps_train/chosen': '-167.16', 'loss/train': '0.69993', 'examples_per_second': '31.691', 'grad_norm': '27.5', 'counters/examples': 22336, 'counters/updates': 698}
train stats after 22368 examples: {'rewards_train/chosen': '0.013652', 'rewards_train/rejected': '0.051962', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.03831', 'logps_train/rejected': '-123.98', 'logps_train/chosen': '-168.99', 'loss/train': '0.71726', 'examples_per_second': '32.838', 'grad_norm': '31.625', 'counters/examples': 22368, 'counters/updates': 699}
train stats after 22400 examples: {'rewards_train/chosen': '0.032002', 'rewards_train/rejected': '0.00012662', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031876', 'logps_train/rejected': '-102.5', 'logps_train/chosen': '-137.98', 'loss/train': '0.68141', 'examples_per_second': '31.648', 'grad_norm': '34.5', 'counters/examples': 22400, 'counters/updates': 700}
train stats after 22432 examples: {'rewards_train/chosen': '0.028632', 'rewards_train/rejected': '0.0089622', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.01967', 'logps_train/rejected': '-151.74', 'logps_train/chosen': '-151.75', 'loss/train': '0.68746', 'examples_per_second': '31.647', 'grad_norm': '32.5', 'counters/examples': 22432, 'counters/updates': 701}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22496 examples: {'rewards_train/chosen': '-0.043602', 'rewards_train/rejected': '-0.0082769', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.035325', 'logps_train/rejected': '-137.46', 'logps_train/chosen': '-177.77', 'loss/train': '0.71708', 'examples_per_second': '31.586', 'grad_norm': '33', 'counters/examples': 22496, 'counters/updates': 703}
train stats after 22528 examples: {'rewards_train/chosen': '-0.019097', 'rewards_train/rejected': '-0.032722', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013624', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-134.03', 'loss/train': '0.69195', 'examples_per_second': '31.429', 'grad_norm': '27.75', 'counters/examples': 22528, 'counters/updates': 704}
skipping logging after 22560 examples to avoid logging too frequently
train stats after 22592 examples: {'rewards_train/chosen': '-0.0021316', 'rewards_train/rejected': '0.0046009', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0067325', 'logps_train/rejected': '-148.82', 'logps_train/chosen': '-145.48', 'loss/train': '0.70181', 'examples_per_second': '30.597', 'grad_norm': '31.375', 'counters/examples': 22592, 'counters/updates': 706}
train stats after 22624 examples: {'rewards_train/chosen': '-0.01931', 'rewards_train/rejected': '0.00087164', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020182', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-164.79', 'loss/train': '0.71125', 'examples_per_second': '31.547', 'grad_norm': '34.25', 'counters/examples': 22624, 'counters/updates': 707}
train stats after 22656 examples: {'rewards_train/chosen': '0.013184', 'rewards_train/rejected': '0.01687', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0036858', 'logps_train/rejected': '-107.04', 'logps_train/chosen': '-132.31', 'loss/train': '0.70264', 'examples_per_second': '31.662', 'grad_norm': '26.875', 'counters/examples': 22656, 'counters/updates': 708}
train stats after 22688 examples: {'rewards_train/chosen': '-0.035182', 'rewards_train/rejected': '-0.054508', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019326', 'logps_train/rejected': '-136.57', 'logps_train/chosen': '-153.75', 'loss/train': '0.68999', 'examples_per_second': '31.91', 'grad_norm': '31.75', 'counters/examples': 22688, 'counters/updates': 709}
train stats after 22720 examples: {'rewards_train/chosen': '0.052464', 'rewards_train/rejected': '0.053876', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.001412', 'logps_train/rejected': '-139.48', 'logps_train/chosen': '-121.34', 'loss/train': '0.70104', 'examples_per_second': '31.556', 'grad_norm': '31.125', 'counters/examples': 22720, 'counters/updates': 710}
skipping logging after 22752 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '0.0026849', 'rewards_train/rejected': '0.022719', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020034', 'logps_train/rejected': '-137.16', 'logps_train/chosen': '-154.39', 'loss/train': '0.70834', 'examples_per_second': '32.448', 'grad_norm': '30.625', 'counters/examples': 22784, 'counters/updates': 712}
train stats after 22816 examples: {'rewards_train/chosen': '0.018462', 'rewards_train/rejected': '-0.008494', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026956', 'logps_train/rejected': '-125.39', 'logps_train/chosen': '-97.565', 'loss/train': '0.68508', 'examples_per_second': '30.823', 'grad_norm': '26.125', 'counters/examples': 22816, 'counters/updates': 713}
train stats after 22848 examples: {'rewards_train/chosen': '-0.033866', 'rewards_train/rejected': '0.0011331', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.035', 'logps_train/rejected': '-129.37', 'logps_train/chosen': '-129.07', 'loss/train': '0.71708', 'examples_per_second': '30.111', 'grad_norm': '29.375', 'counters/examples': 22848, 'counters/updates': 714}
skipping logging after 22880 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '-0.0028486', 'rewards_train/rejected': '-0.043572', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040724', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-152.29', 'loss/train': '0.67591', 'examples_per_second': '34.096', 'grad_norm': '29.75', 'counters/examples': 22912, 'counters/updates': 716}
skipping logging after 22944 examples to avoid logging too frequently
train stats after 22976 examples: {'rewards_train/chosen': '0.030344', 'rewards_train/rejected': '0.047248', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016904', 'logps_train/rejected': '-112.91', 'logps_train/chosen': '-125.21', 'loss/train': '0.70713', 'examples_per_second': '33.421', 'grad_norm': '34.5', 'counters/examples': 22976, 'counters/updates': 718}
train stats after 23008 examples: {'rewards_train/chosen': '0.010234', 'rewards_train/rejected': '-0.016623', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026857', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-115.11', 'loss/train': '0.68458', 'examples_per_second': '31.552', 'grad_norm': '26', 'counters/examples': 23008, 'counters/updates': 719}
train stats after 23040 examples: {'rewards_train/chosen': '-0.044127', 'rewards_train/rejected': '-0.015457', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028671', 'logps_train/rejected': '-108.95', 'logps_train/chosen': '-132.68', 'loss/train': '0.71432', 'examples_per_second': '30.661', 'grad_norm': '28.25', 'counters/examples': 23040, 'counters/updates': 720}
skipping logging after 23072 examples to avoid logging too frequently
train stats after 23104 examples: {'rewards_train/chosen': '0.0091599', 'rewards_train/rejected': '-0.017237', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.026397', 'logps_train/rejected': '-138.44', 'logps_train/chosen': '-141.58', 'loss/train': '0.68981', 'examples_per_second': '33.841', 'grad_norm': '31.125', 'counters/examples': 23104, 'counters/updates': 722}
skipping logging after 23136 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '0.022406', 'rewards_train/rejected': '0.022281', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00012458', 'logps_train/rejected': '-142.53', 'logps_train/chosen': '-181.81', 'loss/train': '0.69638', 'examples_per_second': '31.573', 'grad_norm': '29.25', 'counters/examples': 23168, 'counters/updates': 724}
train stats after 23200 examples: {'rewards_train/chosen': '-0.059681', 'rewards_train/rejected': '-0.0084808', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.0512', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-158.23', 'loss/train': '0.72558', 'examples_per_second': '31.612', 'grad_norm': '39', 'counters/examples': 23200, 'counters/updates': 725}
train stats after 23232 examples: {'rewards_train/chosen': '-0.062511', 'rewards_train/rejected': '-0.02082', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041691', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-122.99', 'loss/train': '0.72744', 'examples_per_second': '31.411', 'grad_norm': '31.625', 'counters/examples': 23232, 'counters/updates': 726}
train stats after 23264 examples: {'rewards_train/chosen': '0.062334', 'rewards_train/rejected': '-0.041737', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10407', 'logps_train/rejected': '-157.76', 'logps_train/chosen': '-145.96', 'loss/train': '0.649', 'examples_per_second': '31.721', 'grad_norm': '27.125', 'counters/examples': 23264, 'counters/updates': 727}
train stats after 23296 examples: {'rewards_train/chosen': '0.019347', 'rewards_train/rejected': '-0.052576', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071922', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-166.23', 'loss/train': '0.66103', 'examples_per_second': '31.026', 'grad_norm': '30.875', 'counters/examples': 23296, 'counters/updates': 728}
train stats after 23328 examples: {'rewards_train/chosen': '-0.00013127', 'rewards_train/rejected': '0.058487', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.058618', 'logps_train/rejected': '-131.19', 'logps_train/chosen': '-169.53', 'loss/train': '0.72719', 'examples_per_second': '31.694', 'grad_norm': '36.5', 'counters/examples': 23328, 'counters/updates': 729}
train stats after 23360 examples: {'rewards_train/chosen': '-0.013944', 'rewards_train/rejected': '0.03258', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.046524', 'logps_train/rejected': '-145.06', 'logps_train/chosen': '-134.8', 'loss/train': '0.71991', 'examples_per_second': '31.931', 'grad_norm': '30.125', 'counters/examples': 23360, 'counters/updates': 730}
train stats after 23392 examples: {'rewards_train/chosen': '0.016523', 'rewards_train/rejected': '0.027021', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010498', 'logps_train/rejected': '-126.19', 'logps_train/chosen': '-131.15', 'loss/train': '0.70469', 'examples_per_second': '31.64', 'grad_norm': '33', 'counters/examples': 23392, 'counters/updates': 731}
train stats after 23424 examples: {'rewards_train/chosen': '0.0051652', 'rewards_train/rejected': '-0.014399', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019564', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-132.63', 'loss/train': '0.68717', 'examples_per_second': '32.21', 'grad_norm': '25.75', 'counters/examples': 23424, 'counters/updates': 732}
train stats after 23456 examples: {'rewards_train/chosen': '0.058839', 'rewards_train/rejected': '-0.022372', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081211', 'logps_train/rejected': '-153.66', 'logps_train/chosen': '-149.95', 'loss/train': '0.65782', 'examples_per_second': '30.564', 'grad_norm': '34.75', 'counters/examples': 23456, 'counters/updates': 733}
train stats after 23488 examples: {'rewards_train/chosen': '-0.01266', 'rewards_train/rejected': '-0.0028334', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0098263', 'logps_train/rejected': '-161.55', 'logps_train/chosen': '-140.71', 'loss/train': '0.70409', 'examples_per_second': '31.698', 'grad_norm': '35.25', 'counters/examples': 23488, 'counters/updates': 734}
train stats after 23520 examples: {'rewards_train/chosen': '-0.052027', 'rewards_train/rejected': '0.010126', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.062153', 'logps_train/rejected': '-164.63', 'logps_train/chosen': '-129.05', 'loss/train': '0.73876', 'examples_per_second': '31.663', 'grad_norm': '37', 'counters/examples': 23520, 'counters/updates': 735}
skipping logging after 23552 examples to avoid logging too frequently
train stats after 23584 examples: {'rewards_train/chosen': '-0.038445', 'rewards_train/rejected': '0.022775', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06122', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-173.54', 'loss/train': '0.7287', 'examples_per_second': '30.133', 'grad_norm': '31.875', 'counters/examples': 23584, 'counters/updates': 737}
train stats after 23616 examples: {'rewards_train/chosen': '-0.056783', 'rewards_train/rejected': '0.022589', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.079373', 'logps_train/rejected': '-168.76', 'logps_train/chosen': '-158.05', 'loss/train': '0.73938', 'examples_per_second': '30.143', 'grad_norm': '33.5', 'counters/examples': 23616, 'counters/updates': 738}
train stats after 23648 examples: {'rewards_train/chosen': '-0.0076756', 'rewards_train/rejected': '0.033206', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040882', 'logps_train/rejected': '-119.86', 'logps_train/chosen': '-130.1', 'loss/train': '0.71927', 'examples_per_second': '32.43', 'grad_norm': '28.75', 'counters/examples': 23648, 'counters/updates': 739}
train stats after 23680 examples: {'rewards_train/chosen': '0.032369', 'rewards_train/rejected': '0.028514', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0038555', 'logps_train/rejected': '-96.921', 'logps_train/chosen': '-132.99', 'loss/train': '0.69545', 'examples_per_second': '30.159', 'grad_norm': '26.75', 'counters/examples': 23680, 'counters/updates': 740}
train stats after 23712 examples: {'rewards_train/chosen': '0.038862', 'rewards_train/rejected': '-0.0029654', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041827', 'logps_train/rejected': '-153.02', 'logps_train/chosen': '-149.56', 'loss/train': '0.67536', 'examples_per_second': '31.604', 'grad_norm': '28.25', 'counters/examples': 23712, 'counters/updates': 741}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23776 examples: {'rewards_train/chosen': '-0.027227', 'rewards_train/rejected': '0.043046', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.070273', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-131.69', 'loss/train': '0.73343', 'examples_per_second': '31.138', 'grad_norm': '30.875', 'counters/examples': 23776, 'counters/updates': 743}
train stats after 23808 examples: {'rewards_train/chosen': '-0.010498', 'rewards_train/rejected': '0.015515', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026014', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-150.87', 'loss/train': '0.71212', 'examples_per_second': '32.62', 'grad_norm': '28.5', 'counters/examples': 23808, 'counters/updates': 744}
skipping logging after 23840 examples to avoid logging too frequently
train stats after 23872 examples: {'rewards_train/chosen': '0.019597', 'rewards_train/rejected': '-0.025896', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045493', 'logps_train/rejected': '-95.327', 'logps_train/chosen': '-122.04', 'loss/train': '0.67554', 'examples_per_second': '24.047', 'grad_norm': '23.375', 'counters/examples': 23872, 'counters/updates': 746}
train stats after 23904 examples: {'rewards_train/chosen': '-0.0050121', 'rewards_train/rejected': '-0.024103', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019091', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-146.85', 'loss/train': '0.68672', 'examples_per_second': '31.401', 'grad_norm': '29', 'counters/examples': 23904, 'counters/updates': 747}
skipping logging after 23936 examples to avoid logging too frequently
train stats after 23968 examples: {'rewards_train/chosen': '-0.022512', 'rewards_train/rejected': '-0.029509', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0069974', 'logps_train/rejected': '-109.49', 'logps_train/chosen': '-125.1', 'loss/train': '0.69645', 'examples_per_second': '34.47', 'grad_norm': '26.875', 'counters/examples': 23968, 'counters/updates': 749}
train stats after 24000 examples: {'rewards_train/chosen': '-0.036562', 'rewards_train/rejected': '-0.025531', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011031', 'logps_train/rejected': '-112.35', 'logps_train/chosen': '-147.75', 'loss/train': '0.70366', 'examples_per_second': '31.676', 'grad_norm': '27.625', 'counters/examples': 24000, 'counters/updates': 750}
Running evaluation after 24000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 24000: {'rewards_eval/chosen': '0.018794', 'rewards_eval/rejected': '0.0077346', 'rewards_eval/accuracies': '0.52734', 'rewards_eval/margins': '0.01106', 'logps_eval/rejected': '-118.53', 'logps_eval/chosen': '-139.25', 'loss/eval': '0.69399'}
skipping save for non epoch
train stats after 24032 examples: {'rewards_train/chosen': '0.03877', 'rewards_train/rejected': '0.037702', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0010683', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-126.29', 'loss/train': '0.69727', 'examples_per_second': '32.318', 'grad_norm': '30.375', 'counters/examples': 24032, 'counters/updates': 751}
train stats after 24064 examples: {'rewards_train/chosen': '0.019301', 'rewards_train/rejected': '-0.011426', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030726', 'logps_train/rejected': '-99.876', 'logps_train/chosen': '-129.61', 'loss/train': '0.68029', 'examples_per_second': '30.134', 'grad_norm': '29', 'counters/examples': 24064, 'counters/updates': 752}
train stats after 24096 examples: {'rewards_train/chosen': '0.016373', 'rewards_train/rejected': '-0.05047', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066843', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-159.8', 'loss/train': '0.66437', 'examples_per_second': '31.871', 'grad_norm': '27.125', 'counters/examples': 24096, 'counters/updates': 753}
train stats after 24128 examples: {'rewards_train/chosen': '0.072464', 'rewards_train/rejected': '-0.051913', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12438', 'logps_train/rejected': '-114.2', 'logps_train/chosen': '-128.14', 'loss/train': '0.63671', 'examples_per_second': '31.637', 'grad_norm': '38.25', 'counters/examples': 24128, 'counters/updates': 754}
train stats after 24160 examples: {'rewards_train/chosen': '0.016992', 'rewards_train/rejected': '-0.026339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04333', 'logps_train/rejected': '-106.13', 'logps_train/chosen': '-129.44', 'loss/train': '0.67481', 'examples_per_second': '32.403', 'grad_norm': '26.125', 'counters/examples': 24160, 'counters/updates': 755}
train stats after 24192 examples: {'rewards_train/chosen': '0.0036598', 'rewards_train/rejected': '0.072842', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.069182', 'logps_train/rejected': '-108.67', 'logps_train/chosen': '-132.47', 'loss/train': '0.73151', 'examples_per_second': '30.84', 'grad_norm': '31', 'counters/examples': 24192, 'counters/updates': 756}
train stats after 24224 examples: {'rewards_train/chosen': '0.027353', 'rewards_train/rejected': '-0.0014086', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028762', 'logps_train/rejected': '-102.23', 'logps_train/chosen': '-181.22', 'loss/train': '0.68611', 'examples_per_second': '30.857', 'grad_norm': '27.75', 'counters/examples': 24224, 'counters/updates': 757}
train stats after 24256 examples: {'rewards_train/chosen': '0.03862', 'rewards_train/rejected': '-0.010313', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048933', 'logps_train/rejected': '-130.1', 'logps_train/chosen': '-165.31', 'loss/train': '0.67718', 'examples_per_second': '31.708', 'grad_norm': '32', 'counters/examples': 24256, 'counters/updates': 758}
train stats after 24288 examples: {'rewards_train/chosen': '0.0031143', 'rewards_train/rejected': '0.0097669', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0066526', 'logps_train/rejected': '-109.68', 'logps_train/chosen': '-142.31', 'loss/train': '0.7021', 'examples_per_second': '32.778', 'grad_norm': '30.625', 'counters/examples': 24288, 'counters/updates': 759}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24352 examples: {'rewards_train/chosen': '-0.041937', 'rewards_train/rejected': '-0.016856', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.025081', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-139.5', 'loss/train': '0.71244', 'examples_per_second': '31.08', 'grad_norm': '33.25', 'counters/examples': 24352, 'counters/updates': 761}
train stats after 24384 examples: {'rewards_train/chosen': '0.028091', 'rewards_train/rejected': '-0.044839', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072929', 'logps_train/rejected': '-119.98', 'logps_train/chosen': '-148.28', 'loss/train': '0.66215', 'examples_per_second': '31.084', 'grad_norm': '34.75', 'counters/examples': 24384, 'counters/updates': 762}
train stats after 24416 examples: {'rewards_train/chosen': '0.021952', 'rewards_train/rejected': '0.041922', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01997', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-128.34', 'loss/train': '0.70873', 'examples_per_second': '30.155', 'grad_norm': '30.75', 'counters/examples': 24416, 'counters/updates': 763}
train stats after 24448 examples: {'rewards_train/chosen': '-0.031786', 'rewards_train/rejected': '-0.04068', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0088938', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-142.01', 'loss/train': '0.69409', 'examples_per_second': '31.732', 'grad_norm': '33.5', 'counters/examples': 24448, 'counters/updates': 764}
train stats after 24480 examples: {'rewards_train/chosen': '0.039479', 'rewards_train/rejected': '-0.00096867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040447', 'logps_train/rejected': '-105.04', 'logps_train/chosen': '-146.71', 'loss/train': '0.67707', 'examples_per_second': '30.215', 'grad_norm': '27.375', 'counters/examples': 24480, 'counters/updates': 765}
train stats after 24512 examples: {'rewards_train/chosen': '0.032279', 'rewards_train/rejected': '0.034154', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0018746', 'logps_train/rejected': '-109.56', 'logps_train/chosen': '-137.35', 'loss/train': '0.69899', 'examples_per_second': '31.988', 'grad_norm': '28.125', 'counters/examples': 24512, 'counters/updates': 766}
train stats after 24544 examples: {'rewards_train/chosen': '-0.011472', 'rewards_train/rejected': '0.017845', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029317', 'logps_train/rejected': '-146.67', 'logps_train/chosen': '-157.18', 'loss/train': '0.71555', 'examples_per_second': '24.191', 'grad_norm': '36.5', 'counters/examples': 24544, 'counters/updates': 767}
train stats after 24576 examples: {'rewards_train/chosen': '0.0052126', 'rewards_train/rejected': '0.043734', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.038522', 'logps_train/rejected': '-118.96', 'logps_train/chosen': '-136.38', 'loss/train': '0.7166', 'examples_per_second': '31.681', 'grad_norm': '31', 'counters/examples': 24576, 'counters/updates': 768}
train stats after 24608 examples: {'rewards_train/chosen': '-0.052924', 'rewards_train/rejected': '-0.028248', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024676', 'logps_train/rejected': '-137.74', 'logps_train/chosen': '-133.42', 'loss/train': '0.70922', 'examples_per_second': '32.052', 'grad_norm': '30', 'counters/examples': 24608, 'counters/updates': 769}
train stats after 24640 examples: {'rewards_train/chosen': '-0.016033', 'rewards_train/rejected': '-0.029463', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013429', 'logps_train/rejected': '-80.343', 'logps_train/chosen': '-159.55', 'loss/train': '0.69133', 'examples_per_second': '24.886', 'grad_norm': '29', 'counters/examples': 24640, 'counters/updates': 770}
skipping logging after 24672 examples to avoid logging too frequently
train stats after 24704 examples: {'rewards_train/chosen': '0.018073', 'rewards_train/rejected': '0.039099', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.021026', 'logps_train/rejected': '-116.6', 'logps_train/chosen': '-128.15', 'loss/train': '0.70644', 'examples_per_second': '32.36', 'grad_norm': '26.75', 'counters/examples': 24704, 'counters/updates': 772}
train stats after 24736 examples: {'rewards_train/chosen': '0.016143', 'rewards_train/rejected': '0.023293', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0071503', 'logps_train/rejected': '-154.97', 'logps_train/chosen': '-134.14', 'loss/train': '0.70629', 'examples_per_second': '31.775', 'grad_norm': '33.25', 'counters/examples': 24736, 'counters/updates': 773}
train stats after 24768 examples: {'rewards_train/chosen': '-0.0036508', 'rewards_train/rejected': '-0.0069189', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0032681', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-137.77', 'loss/train': '0.69691', 'examples_per_second': '32.9', 'grad_norm': '28.375', 'counters/examples': 24768, 'counters/updates': 774}
train stats after 24800 examples: {'rewards_train/chosen': '-0.032088', 'rewards_train/rejected': '-0.024964', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0071243', 'logps_train/rejected': '-113.12', 'logps_train/chosen': '-149.24', 'loss/train': '0.70913', 'examples_per_second': '31.403', 'grad_norm': '30.375', 'counters/examples': 24800, 'counters/updates': 775}
train stats after 24832 examples: {'rewards_train/chosen': '-0.021354', 'rewards_train/rejected': '0.007283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028637', 'logps_train/rejected': '-96.506', 'logps_train/chosen': '-148.03', 'loss/train': '0.71208', 'examples_per_second': '32.127', 'grad_norm': '30.5', 'counters/examples': 24832, 'counters/updates': 776}
train stats after 24864 examples: {'rewards_train/chosen': '-0.0025613', 'rewards_train/rejected': '0.0030559', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0056172', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-158.18', 'loss/train': '0.70072', 'examples_per_second': '30.694', 'grad_norm': '34.5', 'counters/examples': 24864, 'counters/updates': 777}
train stats after 24896 examples: {'rewards_train/chosen': '0.033435', 'rewards_train/rejected': '-0.030083', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063517', 'logps_train/rejected': '-182.65', 'logps_train/chosen': '-158.98', 'loss/train': '0.66949', 'examples_per_second': '31.126', 'grad_norm': '34.25', 'counters/examples': 24896, 'counters/updates': 778}
skipping logging after 24928 examples to avoid logging too frequently
train stats after 24960 examples: {'rewards_train/chosen': '0.043892', 'rewards_train/rejected': '-0.031534', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.075425', 'logps_train/rejected': '-98.031', 'logps_train/chosen': '-117.67', 'loss/train': '0.65942', 'examples_per_second': '30.606', 'grad_norm': '26.25', 'counters/examples': 24960, 'counters/updates': 780}
train stats after 24992 examples: {'rewards_train/chosen': '0.060853', 'rewards_train/rejected': '0.014615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046238', 'logps_train/rejected': '-153.5', 'logps_train/chosen': '-164.29', 'loss/train': '0.67639', 'examples_per_second': '30.972', 'grad_norm': '34.5', 'counters/examples': 24992, 'counters/updates': 781}
train stats after 25024 examples: {'rewards_train/chosen': '-0.016482', 'rewards_train/rejected': '0.0092698', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025752', 'logps_train/rejected': '-110.34', 'logps_train/chosen': '-134.65', 'loss/train': '0.71107', 'examples_per_second': '32.591', 'grad_norm': '29.75', 'counters/examples': 25024, 'counters/updates': 782}
train stats after 25056 examples: {'rewards_train/chosen': '-0.0069482', 'rewards_train/rejected': '0.037066', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.044014', 'logps_train/rejected': '-153.48', 'logps_train/chosen': '-145.95', 'loss/train': '0.71965', 'examples_per_second': '31.582', 'grad_norm': '32', 'counters/examples': 25056, 'counters/updates': 783}
train stats after 25088 examples: {'rewards_train/chosen': '0.031654', 'rewards_train/rejected': '-0.016694', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048348', 'logps_train/rejected': '-119.33', 'logps_train/chosen': '-150.86', 'loss/train': '0.68061', 'examples_per_second': '31.582', 'grad_norm': '32.25', 'counters/examples': 25088, 'counters/updates': 784}
train stats after 25120 examples: {'rewards_train/chosen': '0.0063167', 'rewards_train/rejected': '-0.0021455', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0084621', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-135.69', 'loss/train': '0.69341', 'examples_per_second': '30.815', 'grad_norm': '29.625', 'counters/examples': 25120, 'counters/updates': 785}
train stats after 25152 examples: {'rewards_train/chosen': '-0.020596', 'rewards_train/rejected': '-0.038971', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018375', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-124.41', 'loss/train': '0.6873', 'examples_per_second': '31.537', 'grad_norm': '31.375', 'counters/examples': 25152, 'counters/updates': 786}
skipping logging after 25184 examples to avoid logging too frequently
train stats after 25216 examples: {'rewards_train/chosen': '0.0077814', 'rewards_train/rejected': '-0.030276', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038057', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-136.91', 'loss/train': '0.67835', 'examples_per_second': '32.985', 'grad_norm': '28.625', 'counters/examples': 25216, 'counters/updates': 788}
train stats after 25248 examples: {'rewards_train/chosen': '-0.0050871', 'rewards_train/rejected': '0.060335', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.065422', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-123.21', 'loss/train': '0.73457', 'examples_per_second': '32.173', 'grad_norm': '28.25', 'counters/examples': 25248, 'counters/updates': 789}
train stats after 25280 examples: {'rewards_train/chosen': '-0.031407', 'rewards_train/rejected': '0.0067228', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.03813', 'logps_train/rejected': '-106.38', 'logps_train/chosen': '-124.01', 'loss/train': '0.71762', 'examples_per_second': '31.453', 'grad_norm': '28.5', 'counters/examples': 25280, 'counters/updates': 790}
skipping logging after 25312 examples to avoid logging too frequently
train stats after 25344 examples: {'rewards_train/chosen': '-0.020131', 'rewards_train/rejected': '-0.018956', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0011749', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-147.64', 'loss/train': '0.69756', 'examples_per_second': '30.715', 'grad_norm': '31.625', 'counters/examples': 25344, 'counters/updates': 792}
skipping logging after 25376 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '0.021659', 'rewards_train/rejected': '-0.0034246', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025084', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-145.71', 'loss/train': '0.68836', 'examples_per_second': '31.685', 'grad_norm': '33.25', 'counters/examples': 25408, 'counters/updates': 794}
train stats after 25440 examples: {'rewards_train/chosen': '0.018907', 'rewards_train/rejected': '-0.022007', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040914', 'logps_train/rejected': '-104.91', 'logps_train/chosen': '-141.74', 'loss/train': '0.67703', 'examples_per_second': '31.696', 'grad_norm': '27', 'counters/examples': 25440, 'counters/updates': 795}
train stats after 25472 examples: {'rewards_train/chosen': '-0.012056', 'rewards_train/rejected': '-0.024857', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012801', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-102.56', 'loss/train': '0.6904', 'examples_per_second': '31.722', 'grad_norm': '24.75', 'counters/examples': 25472, 'counters/updates': 796}
train stats after 25504 examples: {'rewards_train/chosen': '-0.01866', 'rewards_train/rejected': '-0.0082596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.010401', 'logps_train/rejected': '-113.74', 'logps_train/chosen': '-145.64', 'loss/train': '0.70482', 'examples_per_second': '31.599', 'grad_norm': '32.25', 'counters/examples': 25504, 'counters/updates': 797}
train stats after 25536 examples: {'rewards_train/chosen': '-0.020671', 'rewards_train/rejected': '-0.077372', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0567', 'logps_train/rejected': '-97.61', 'logps_train/chosen': '-123.5', 'loss/train': '0.67212', 'examples_per_second': '31.794', 'grad_norm': '24', 'counters/examples': 25536, 'counters/updates': 798}
train stats after 25568 examples: {'rewards_train/chosen': '0.036449', 'rewards_train/rejected': '-0.031631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068081', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-139.87', 'loss/train': '0.66759', 'examples_per_second': '30.911', 'grad_norm': '28.125', 'counters/examples': 25568, 'counters/updates': 799}
train stats after 25600 examples: {'rewards_train/chosen': '-0.050451', 'rewards_train/rejected': '0.022599', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.07305', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-148.2', 'loss/train': '0.7349', 'examples_per_second': '31.672', 'grad_norm': '31.625', 'counters/examples': 25600, 'counters/updates': 800}
train stats after 25632 examples: {'rewards_train/chosen': '-0.047055', 'rewards_train/rejected': '0.0080754', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05513', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-130.3', 'loss/train': '0.72886', 'examples_per_second': '31.205', 'grad_norm': '31.875', 'counters/examples': 25632, 'counters/updates': 801}
train stats after 25664 examples: {'rewards_train/chosen': '0.030728', 'rewards_train/rejected': '-0.0044681', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035196', 'logps_train/rejected': '-115.81', 'logps_train/chosen': '-135.97', 'loss/train': '0.68127', 'examples_per_second': '30.658', 'grad_norm': '25.375', 'counters/examples': 25664, 'counters/updates': 802}
train stats after 25696 examples: {'rewards_train/chosen': '-0.019117', 'rewards_train/rejected': '-0.0091954', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0099215', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-193.49', 'loss/train': '0.70392', 'examples_per_second': '30.582', 'grad_norm': '33.75', 'counters/examples': 25696, 'counters/updates': 803}
train stats after 25728 examples: {'rewards_train/chosen': '0.037701', 'rewards_train/rejected': '-0.016706', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054407', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-162', 'loss/train': '0.67229', 'examples_per_second': '32.461', 'grad_norm': '28.25', 'counters/examples': 25728, 'counters/updates': 804}
train stats after 25760 examples: {'rewards_train/chosen': '0.019399', 'rewards_train/rejected': '0.0030143', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016384', 'logps_train/rejected': '-135.66', 'logps_train/chosen': '-134.86', 'loss/train': '0.68895', 'examples_per_second': '29.912', 'grad_norm': '24.875', 'counters/examples': 25760, 'counters/updates': 805}
skipping logging after 25792 examples to avoid logging too frequently
train stats after 25824 examples: {'rewards_train/chosen': '0.028168', 'rewards_train/rejected': '-0.046064', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074232', 'logps_train/rejected': '-147.03', 'logps_train/chosen': '-136.47', 'loss/train': '0.66319', 'examples_per_second': '30.116', 'grad_norm': '33.25', 'counters/examples': 25824, 'counters/updates': 807}
train stats after 25856 examples: {'rewards_train/chosen': '0.0027052', 'rewards_train/rejected': '-0.026535', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02924', 'logps_train/rejected': '-103.06', 'logps_train/chosen': '-158.83', 'loss/train': '0.68677', 'examples_per_second': '31.775', 'grad_norm': '28.125', 'counters/examples': 25856, 'counters/updates': 808}
train stats after 25888 examples: {'rewards_train/chosen': '0.023906', 'rewards_train/rejected': '-0.020337', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044242', 'logps_train/rejected': '-122.71', 'logps_train/chosen': '-152.71', 'loss/train': '0.67891', 'examples_per_second': '31.979', 'grad_norm': '27.25', 'counters/examples': 25888, 'counters/updates': 809}
train stats after 25920 examples: {'rewards_train/chosen': '-0.003759', 'rewards_train/rejected': '-0.0071834', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0034244', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-166.03', 'loss/train': '0.69523', 'examples_per_second': '31.634', 'grad_norm': '29.375', 'counters/examples': 25920, 'counters/updates': 810}
train stats after 25952 examples: {'rewards_train/chosen': '-0.028034', 'rewards_train/rejected': '-0.012993', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015042', 'logps_train/rejected': '-106.55', 'logps_train/chosen': '-129.56', 'loss/train': '0.70431', 'examples_per_second': '31.657', 'grad_norm': '24.5', 'counters/examples': 25952, 'counters/updates': 811}
train stats after 25984 examples: {'rewards_train/chosen': '0.016956', 'rewards_train/rejected': '-0.041994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05895', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-125.69', 'loss/train': '0.67096', 'examples_per_second': '32.396', 'grad_norm': '24.75', 'counters/examples': 25984, 'counters/updates': 812}
train stats after 26016 examples: {'rewards_train/chosen': '-0.01831', 'rewards_train/rejected': '-0.01066', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0076497', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-145.56', 'loss/train': '0.70319', 'examples_per_second': '32.362', 'grad_norm': '28.625', 'counters/examples': 26016, 'counters/updates': 813}
train stats after 26048 examples: {'rewards_train/chosen': '0.015791', 'rewards_train/rejected': '0.028577', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012786', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-182.47', 'loss/train': '0.70342', 'examples_per_second': '30.679', 'grad_norm': '31.25', 'counters/examples': 26048, 'counters/updates': 814}
skipping logging after 26080 examples to avoid logging too frequently
train stats after 26112 examples: {'rewards_train/chosen': '-0.0022302', 'rewards_train/rejected': '-0.017009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014779', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-132.67', 'loss/train': '0.69354', 'examples_per_second': '36.476', 'grad_norm': '30.375', 'counters/examples': 26112, 'counters/updates': 816}
train stats after 26144 examples: {'rewards_train/chosen': '0.02631', 'rewards_train/rejected': '0.0064235', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019886', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-139.3', 'loss/train': '0.68644', 'examples_per_second': '32.213', 'grad_norm': '25.5', 'counters/examples': 26144, 'counters/updates': 817}
train stats after 26176 examples: {'rewards_train/chosen': '-0.022097', 'rewards_train/rejected': '-0.057189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035091', 'logps_train/rejected': '-102.31', 'logps_train/chosen': '-109.72', 'loss/train': '0.67978', 'examples_per_second': '30.388', 'grad_norm': '23.75', 'counters/examples': 26176, 'counters/updates': 818}
train stats after 26208 examples: {'rewards_train/chosen': '-0.0043716', 'rewards_train/rejected': '0.0096482', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01402', 'logps_train/rejected': '-141.52', 'logps_train/chosen': '-121.86', 'loss/train': '0.70386', 'examples_per_second': '30.792', 'grad_norm': '28', 'counters/examples': 26208, 'counters/updates': 819}
train stats after 26240 examples: {'rewards_train/chosen': '0.021829', 'rewards_train/rejected': '0.039375', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017547', 'logps_train/rejected': '-89.445', 'logps_train/chosen': '-124.01', 'loss/train': '0.70651', 'examples_per_second': '30.796', 'grad_norm': '26.5', 'counters/examples': 26240, 'counters/updates': 820}
train stats after 26272 examples: {'rewards_train/chosen': '0.022966', 'rewards_train/rejected': '0.016499', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0064669', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-146.57', 'loss/train': '0.6942', 'examples_per_second': '32.281', 'grad_norm': '27.25', 'counters/examples': 26272, 'counters/updates': 821}
train stats after 26304 examples: {'rewards_train/chosen': '0.026076', 'rewards_train/rejected': '-0.044163', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070239', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-156.44', 'loss/train': '0.66451', 'examples_per_second': '32.343', 'grad_norm': '27.125', 'counters/examples': 26304, 'counters/updates': 822}
train stats after 26336 examples: {'rewards_train/chosen': '-0.05886', 'rewards_train/rejected': '-0.0056903', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.053169', 'logps_train/rejected': '-124.49', 'logps_train/chosen': '-163.42', 'loss/train': '0.72695', 'examples_per_second': '32.817', 'grad_norm': '33.5', 'counters/examples': 26336, 'counters/updates': 823}
train stats after 26368 examples: {'rewards_train/chosen': '0.01366', 'rewards_train/rejected': '-0.017279', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.030939', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-140.92', 'loss/train': '0.6838', 'examples_per_second': '30.96', 'grad_norm': '32.5', 'counters/examples': 26368, 'counters/updates': 824}
train stats after 26400 examples: {'rewards_train/chosen': '0.012638', 'rewards_train/rejected': '0.03276', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020122', 'logps_train/rejected': '-174.55', 'logps_train/chosen': '-180.62', 'loss/train': '0.70733', 'examples_per_second': '30.357', 'grad_norm': '33', 'counters/examples': 26400, 'counters/updates': 825}
skipping logging after 26432 examples to avoid logging too frequently
train stats after 26464 examples: {'rewards_train/chosen': '-0.0056628', 'rewards_train/rejected': '-0.030674', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025012', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-155.12', 'loss/train': '0.68523', 'examples_per_second': '30.242', 'grad_norm': '27.75', 'counters/examples': 26464, 'counters/updates': 827}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26528 examples: {'rewards_train/chosen': '-0.001236', 'rewards_train/rejected': '-0.013091', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011855', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-127.29', 'loss/train': '0.69307', 'examples_per_second': '32.894', 'grad_norm': '29.5', 'counters/examples': 26528, 'counters/updates': 829}
train stats after 26560 examples: {'rewards_train/chosen': '0.025672', 'rewards_train/rejected': '-0.011748', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03742', 'logps_train/rejected': '-111.58', 'logps_train/chosen': '-113.89', 'loss/train': '0.68126', 'examples_per_second': '31.978', 'grad_norm': '26', 'counters/examples': 26560, 'counters/updates': 830}
skipping logging after 26592 examples to avoid logging too frequently
train stats after 26624 examples: {'rewards_train/chosen': '-0.0089919', 'rewards_train/rejected': '-0.0049503', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0040416', 'logps_train/rejected': '-106.59', 'logps_train/chosen': '-131.35', 'loss/train': '0.70391', 'examples_per_second': '31.2', 'grad_norm': '30.625', 'counters/examples': 26624, 'counters/updates': 832}
skipping logging after 26656 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '0.031738', 'rewards_train/rejected': '0.051936', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020198', 'logps_train/rejected': '-159.08', 'logps_train/chosen': '-176.38', 'loss/train': '0.71087', 'examples_per_second': '31.597', 'grad_norm': '35', 'counters/examples': 26688, 'counters/updates': 834}
train stats after 26720 examples: {'rewards_train/chosen': '0.0075003', 'rewards_train/rejected': '-0.026348', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033849', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-129.42', 'loss/train': '0.67945', 'examples_per_second': '30.504', 'grad_norm': '25.75', 'counters/examples': 26720, 'counters/updates': 835}
train stats after 26752 examples: {'rewards_train/chosen': '-0.015435', 'rewards_train/rejected': '-0.034987', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.019552', 'logps_train/rejected': '-98.629', 'logps_train/chosen': '-120.1', 'loss/train': '0.68822', 'examples_per_second': '30.774', 'grad_norm': '24', 'counters/examples': 26752, 'counters/updates': 836}
skipping logging after 26784 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.034531', 'rewards_train/rejected': '-0.042918', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0083873', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-168.33', 'loss/train': '0.69517', 'examples_per_second': '31.644', 'grad_norm': '34', 'counters/examples': 26816, 'counters/updates': 838}
train stats after 26848 examples: {'rewards_train/chosen': '0.02345', 'rewards_train/rejected': '0.012265', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011185', 'logps_train/rejected': '-105.29', 'logps_train/chosen': '-189.89', 'loss/train': '0.69145', 'examples_per_second': '31.673', 'grad_norm': '32.75', 'counters/examples': 26848, 'counters/updates': 839}
train stats after 26880 examples: {'rewards_train/chosen': '0.024746', 'rewards_train/rejected': '0.03727', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012524', 'logps_train/rejected': '-107.61', 'logps_train/chosen': '-161.16', 'loss/train': '0.7045', 'examples_per_second': '31.65', 'grad_norm': '30.75', 'counters/examples': 26880, 'counters/updates': 840}
train stats after 26912 examples: {'rewards_train/chosen': '-0.0083174', 'rewards_train/rejected': '-0.052451', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044134', 'logps_train/rejected': '-154.69', 'logps_train/chosen': '-154.75', 'loss/train': '0.67649', 'examples_per_second': '32.535', 'grad_norm': '32.25', 'counters/examples': 26912, 'counters/updates': 841}
skipping logging after 26944 examples to avoid logging too frequently
train stats after 26976 examples: {'rewards_train/chosen': '0.011694', 'rewards_train/rejected': '0.016475', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0047816', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-122.43', 'loss/train': '0.7018', 'examples_per_second': '34.39', 'grad_norm': '24.25', 'counters/examples': 26976, 'counters/updates': 843}
skipping logging after 27008 examples to avoid logging too frequently
train stats after 27040 examples: {'rewards_train/chosen': '-0.048832', 'rewards_train/rejected': '-0.025928', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022904', 'logps_train/rejected': '-106.74', 'logps_train/chosen': '-129.14', 'loss/train': '0.71132', 'examples_per_second': '31.161', 'grad_norm': '28.875', 'counters/examples': 27040, 'counters/updates': 845}
train stats after 27072 examples: {'rewards_train/chosen': '-0.0094016', 'rewards_train/rejected': '-0.037793', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028391', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-153.32', 'loss/train': '0.68511', 'examples_per_second': '30.669', 'grad_norm': '27.25', 'counters/examples': 27072, 'counters/updates': 846}
train stats after 27104 examples: {'rewards_train/chosen': '0.028703', 'rewards_train/rejected': '-0.068849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097553', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-157.95', 'loss/train': '0.65233', 'examples_per_second': '31.672', 'grad_norm': '28', 'counters/examples': 27104, 'counters/updates': 847}
train stats after 27136 examples: {'rewards_train/chosen': '-0.050579', 'rewards_train/rejected': '-0.014176', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036403', 'logps_train/rejected': '-100.86', 'logps_train/chosen': '-119.33', 'loss/train': '0.71469', 'examples_per_second': '31.294', 'grad_norm': '24.5', 'counters/examples': 27136, 'counters/updates': 848}
skipping logging after 27168 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '0.012947', 'rewards_train/rejected': '0.02086', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0079136', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-110.3', 'loss/train': '0.69909', 'examples_per_second': '30.123', 'grad_norm': '24.125', 'counters/examples': 27200, 'counters/updates': 850}
train stats after 27232 examples: {'rewards_train/chosen': '0.0061053', 'rewards_train/rejected': '0.030453', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024348', 'logps_train/rejected': '-119.27', 'logps_train/chosen': '-160.32', 'loss/train': '0.70905', 'examples_per_second': '31.678', 'grad_norm': '32', 'counters/examples': 27232, 'counters/updates': 851}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27296 examples: {'rewards_train/chosen': '7.1361e-05', 'rewards_train/rejected': '-0.019206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019278', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-103.81', 'loss/train': '0.68927', 'examples_per_second': '34.567', 'grad_norm': '26', 'counters/examples': 27296, 'counters/updates': 853}
train stats after 27328 examples: {'rewards_train/chosen': '0.0033735', 'rewards_train/rejected': '-0.053662', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057036', 'logps_train/rejected': '-173.39', 'logps_train/chosen': '-153.21', 'loss/train': '0.67101', 'examples_per_second': '31.628', 'grad_norm': '29.375', 'counters/examples': 27328, 'counters/updates': 854}
train stats after 27360 examples: {'rewards_train/chosen': '-0.03039', 'rewards_train/rejected': '-0.017114', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013276', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-134.1', 'loss/train': '0.7041', 'examples_per_second': '32.374', 'grad_norm': '28.75', 'counters/examples': 27360, 'counters/updates': 855}
train stats after 27392 examples: {'rewards_train/chosen': '0.06542', 'rewards_train/rejected': '0.033579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.031841', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-147.67', 'loss/train': '0.68508', 'examples_per_second': '31.606', 'grad_norm': '52', 'counters/examples': 27392, 'counters/updates': 856}
train stats after 27424 examples: {'rewards_train/chosen': '-0.079262', 'rewards_train/rejected': '-0.0096392', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.069622', 'logps_train/rejected': '-157.61', 'logps_train/chosen': '-175.17', 'loss/train': '0.73537', 'examples_per_second': '30.917', 'grad_norm': '33.5', 'counters/examples': 27424, 'counters/updates': 857}
train stats after 27456 examples: {'rewards_train/chosen': '-0.040368', 'rewards_train/rejected': '-0.016872', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.023496', 'logps_train/rejected': '-146.28', 'logps_train/chosen': '-177.45', 'loss/train': '0.71363', 'examples_per_second': '31.628', 'grad_norm': '46', 'counters/examples': 27456, 'counters/updates': 858}
train stats after 27488 examples: {'rewards_train/chosen': '-0.034158', 'rewards_train/rejected': '-0.023298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01086', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-124.73', 'loss/train': '0.70171', 'examples_per_second': '32.268', 'grad_norm': '29.25', 'counters/examples': 27488, 'counters/updates': 859}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27552 examples: {'rewards_train/chosen': '0.042768', 'rewards_train/rejected': '0.0251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017668', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-151.12', 'loss/train': '0.68967', 'examples_per_second': '30.739', 'grad_norm': '36.75', 'counters/examples': 27552, 'counters/updates': 861}
train stats after 27584 examples: {'rewards_train/chosen': '0.047091', 'rewards_train/rejected': '-0.012125', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059216', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-130.07', 'loss/train': '0.67107', 'examples_per_second': '30.453', 'grad_norm': '23.75', 'counters/examples': 27584, 'counters/updates': 862}
train stats after 27616 examples: {'rewards_train/chosen': '0.024876', 'rewards_train/rejected': '-0.051326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076202', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-153.59', 'loss/train': '0.66259', 'examples_per_second': '31.143', 'grad_norm': '27.75', 'counters/examples': 27616, 'counters/updates': 863}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27680 examples: {'rewards_train/chosen': '-0.0011988', 'rewards_train/rejected': '-0.03967', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038472', 'logps_train/rejected': '-92.272', 'logps_train/chosen': '-121.22', 'loss/train': '0.67942', 'examples_per_second': '31.528', 'grad_norm': '25.125', 'counters/examples': 27680, 'counters/updates': 865}
train stats after 27712 examples: {'rewards_train/chosen': '0.018514', 'rewards_train/rejected': '-0.010509', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029023', 'logps_train/rejected': '-152.03', 'logps_train/chosen': '-147.91', 'loss/train': '0.6836', 'examples_per_second': '30.555', 'grad_norm': '30.125', 'counters/examples': 27712, 'counters/updates': 866}
skipping logging after 27744 examples to avoid logging too frequently
train stats after 27776 examples: {'rewards_train/chosen': '-0.016524', 'rewards_train/rejected': '0.0088335', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025358', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-142.7', 'loss/train': '0.71197', 'examples_per_second': '37.024', 'grad_norm': '28.625', 'counters/examples': 27776, 'counters/updates': 868}
train stats after 27808 examples: {'rewards_train/chosen': '-0.0089711', 'rewards_train/rejected': '0.039136', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.048107', 'logps_train/rejected': '-113.74', 'logps_train/chosen': '-130.65', 'loss/train': '0.72236', 'examples_per_second': '31.625', 'grad_norm': '30.25', 'counters/examples': 27808, 'counters/updates': 869}
train stats after 27840 examples: {'rewards_train/chosen': '0.0086088', 'rewards_train/rejected': '-0.06862', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077229', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-139.74', 'loss/train': '0.65922', 'examples_per_second': '31.823', 'grad_norm': '26.125', 'counters/examples': 27840, 'counters/updates': 870}
train stats after 27872 examples: {'rewards_train/chosen': '0.025229', 'rewards_train/rejected': '-0.017401', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.04263', 'logps_train/rejected': '-79.257', 'logps_train/chosen': '-134.14', 'loss/train': '0.67535', 'examples_per_second': '31.271', 'grad_norm': '24.5', 'counters/examples': 27872, 'counters/updates': 871}
train stats after 27904 examples: {'rewards_train/chosen': '0.0035489', 'rewards_train/rejected': '0.016732', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013183', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-150.6', 'loss/train': '0.70471', 'examples_per_second': '31.483', 'grad_norm': '29.75', 'counters/examples': 27904, 'counters/updates': 872}
train stats after 27936 examples: {'rewards_train/chosen': '0.0055276', 'rewards_train/rejected': '-0.0025506', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0080782', 'logps_train/rejected': '-95.351', 'logps_train/chosen': '-115.12', 'loss/train': '0.69267', 'examples_per_second': '31.511', 'grad_norm': '23.375', 'counters/examples': 27936, 'counters/updates': 873}
train stats after 27968 examples: {'rewards_train/chosen': '0.0077795', 'rewards_train/rejected': '0.027139', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01936', 'logps_train/rejected': '-128.22', 'logps_train/chosen': '-136.58', 'loss/train': '0.70782', 'examples_per_second': '30.401', 'grad_norm': '29.125', 'counters/examples': 27968, 'counters/updates': 874}
train stats after 28000 examples: {'rewards_train/chosen': '0.041567', 'rewards_train/rejected': '0.00048554', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041082', 'logps_train/rejected': '-179.95', 'logps_train/chosen': '-182.07', 'loss/train': '0.67684', 'examples_per_second': '31.669', 'grad_norm': '33.75', 'counters/examples': 28000, 'counters/updates': 875}
train stats after 28032 examples: {'rewards_train/chosen': '-0.0020835', 'rewards_train/rejected': '-0.013086', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011002', 'logps_train/rejected': '-181.65', 'logps_train/chosen': '-141.41', 'loss/train': '0.6948', 'examples_per_second': '30.823', 'grad_norm': '35', 'counters/examples': 28032, 'counters/updates': 876}
train stats after 28064 examples: {'rewards_train/chosen': '-0.0073404', 'rewards_train/rejected': '-0.020387', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.013047', 'logps_train/rejected': '-106.31', 'logps_train/chosen': '-182.61', 'loss/train': '0.69146', 'examples_per_second': '31.634', 'grad_norm': '34.75', 'counters/examples': 28064, 'counters/updates': 877}
train stats after 28096 examples: {'rewards_train/chosen': '-0.00047594', 'rewards_train/rejected': '0.0056098', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0060858', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-146.36', 'loss/train': '0.69932', 'examples_per_second': '31.554', 'grad_norm': '28.75', 'counters/examples': 28096, 'counters/updates': 878}
skipping logging after 28128 examples to avoid logging too frequently
train stats after 28160 examples: {'rewards_train/chosen': '0.0047945', 'rewards_train/rejected': '0.05934', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.054545', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-135.23', 'loss/train': '0.72569', 'examples_per_second': '30.091', 'grad_norm': '29.875', 'counters/examples': 28160, 'counters/updates': 880}
train stats after 28192 examples: {'rewards_train/chosen': '-0.016886', 'rewards_train/rejected': '-0.01224', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0046461', 'logps_train/rejected': '-101.87', 'logps_train/chosen': '-147.48', 'loss/train': '0.70084', 'examples_per_second': '30.103', 'grad_norm': '25.125', 'counters/examples': 28192, 'counters/updates': 881}
skipping logging after 28224 examples to avoid logging too frequently
train stats after 28256 examples: {'rewards_train/chosen': '0.024164', 'rewards_train/rejected': '0.0085805', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015583', 'logps_train/rejected': '-146.27', 'logps_train/chosen': '-135.17', 'loss/train': '0.69116', 'examples_per_second': '31.603', 'grad_norm': '29.125', 'counters/examples': 28256, 'counters/updates': 883}
train stats after 28288 examples: {'rewards_train/chosen': '-0.017992', 'rewards_train/rejected': '-0.0097427', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0082491', 'logps_train/rejected': '-100.39', 'logps_train/chosen': '-125.54', 'loss/train': '0.70313', 'examples_per_second': '31.617', 'grad_norm': '39.5', 'counters/examples': 28288, 'counters/updates': 884}
train stats after 28320 examples: {'rewards_train/chosen': '0.025236', 'rewards_train/rejected': '0.0038444', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021391', 'logps_train/rejected': '-153.22', 'logps_train/chosen': '-140.79', 'loss/train': '0.68825', 'examples_per_second': '32.744', 'grad_norm': '30.25', 'counters/examples': 28320, 'counters/updates': 885}
train stats after 28352 examples: {'rewards_train/chosen': '0.011613', 'rewards_train/rejected': '0.0083741', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0032384', 'logps_train/rejected': '-116.59', 'logps_train/chosen': '-118.45', 'loss/train': '0.69454', 'examples_per_second': '32.808', 'grad_norm': '34.5', 'counters/examples': 28352, 'counters/updates': 886}
train stats after 28384 examples: {'rewards_train/chosen': '-0.017296', 'rewards_train/rejected': '-0.035727', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018431', 'logps_train/rejected': '-137.19', 'logps_train/chosen': '-169.38', 'loss/train': '0.69076', 'examples_per_second': '31.599', 'grad_norm': '31.5', 'counters/examples': 28384, 'counters/updates': 887}
train stats after 28416 examples: {'rewards_train/chosen': '-0.015424', 'rewards_train/rejected': '-0.00092441', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014499', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-147.99', 'loss/train': '0.7069', 'examples_per_second': '31.013', 'grad_norm': '31.25', 'counters/examples': 28416, 'counters/updates': 888}
train stats after 28448 examples: {'rewards_train/chosen': '-0.01123', 'rewards_train/rejected': '0.0047642', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015995', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-165.88', 'loss/train': '0.70502', 'examples_per_second': '31.572', 'grad_norm': '29.375', 'counters/examples': 28448, 'counters/updates': 889}
train stats after 28480 examples: {'rewards_train/chosen': '0.02653', 'rewards_train/rejected': '0.040093', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013563', 'logps_train/rejected': '-105.26', 'logps_train/chosen': '-117.96', 'loss/train': '0.70448', 'examples_per_second': '30.603', 'grad_norm': '24.25', 'counters/examples': 28480, 'counters/updates': 890}
train stats after 28512 examples: {'rewards_train/chosen': '0.0086552', 'rewards_train/rejected': '-0.018111', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026767', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-123.26', 'loss/train': '0.68317', 'examples_per_second': '31.615', 'grad_norm': '28.875', 'counters/examples': 28512, 'counters/updates': 891}
train stats after 28544 examples: {'rewards_train/chosen': '0.033904', 'rewards_train/rejected': '-0.036152', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070056', 'logps_train/rejected': '-129.15', 'logps_train/chosen': '-158.2', 'loss/train': '0.66464', 'examples_per_second': '29.99', 'grad_norm': '26.25', 'counters/examples': 28544, 'counters/updates': 892}
skipping logging after 28576 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '-0.047952', 'rewards_train/rejected': '-0.027506', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020447', 'logps_train/rejected': '-127.69', 'logps_train/chosen': '-167.78', 'loss/train': '0.71036', 'examples_per_second': '32.579', 'grad_norm': '32.25', 'counters/examples': 28608, 'counters/updates': 894}
train stats after 28640 examples: {'rewards_train/chosen': '0.032399', 'rewards_train/rejected': '-0.034896', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067294', 'logps_train/rejected': '-136.36', 'logps_train/chosen': '-143.21', 'loss/train': '0.66815', 'examples_per_second': '32.083', 'grad_norm': '28.25', 'counters/examples': 28640, 'counters/updates': 895}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28704 examples: {'rewards_train/chosen': '-0.013845', 'rewards_train/rejected': '-0.079603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065759', 'logps_train/rejected': '-104.49', 'logps_train/chosen': '-166.42', 'loss/train': '0.66578', 'examples_per_second': '31.557', 'grad_norm': '29.375', 'counters/examples': 28704, 'counters/updates': 897}
train stats after 28736 examples: {'rewards_train/chosen': '0.057933', 'rewards_train/rejected': '0.035688', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022244', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-146.72', 'loss/train': '0.68934', 'examples_per_second': '30.599', 'grad_norm': '30.625', 'counters/examples': 28736, 'counters/updates': 898}
train stats after 28768 examples: {'rewards_train/chosen': '0.033525', 'rewards_train/rejected': '-0.032699', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066223', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-177.71', 'loss/train': '0.66595', 'examples_per_second': '30.23', 'grad_norm': '27.625', 'counters/examples': 28768, 'counters/updates': 899}
train stats after 28800 examples: {'rewards_train/chosen': '0.00046956', 'rewards_train/rejected': '0.038638', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.038168', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-129.38', 'loss/train': '0.71823', 'examples_per_second': '31.594', 'grad_norm': '26.375', 'counters/examples': 28800, 'counters/updates': 900}
train stats after 28832 examples: {'rewards_train/chosen': '0.074143', 'rewards_train/rejected': '0.028945', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045198', 'logps_train/rejected': '-94.013', 'logps_train/chosen': '-121.71', 'loss/train': '0.6751', 'examples_per_second': '31.009', 'grad_norm': '25.125', 'counters/examples': 28832, 'counters/updates': 901}
train stats after 28864 examples: {'rewards_train/chosen': '-0.013769', 'rewards_train/rejected': '-0.032091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018322', 'logps_train/rejected': '-113.14', 'logps_train/chosen': '-120.29', 'loss/train': '0.68714', 'examples_per_second': '32.749', 'grad_norm': '26.5', 'counters/examples': 28864, 'counters/updates': 902}
train stats after 28896 examples: {'rewards_train/chosen': '0.0068049', 'rewards_train/rejected': '-0.046278', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053083', 'logps_train/rejected': '-95.002', 'logps_train/chosen': '-130.76', 'loss/train': '0.67093', 'examples_per_second': '30.774', 'grad_norm': '29.75', 'counters/examples': 28896, 'counters/updates': 903}
train stats after 28928 examples: {'rewards_train/chosen': '-0.021145', 'rewards_train/rejected': '0.0019966', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023142', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-132.45', 'loss/train': '0.71001', 'examples_per_second': '30.628', 'grad_norm': '30.75', 'counters/examples': 28928, 'counters/updates': 904}
skipping logging after 28960 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '-0.036588', 'rewards_train/rejected': '0.014159', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.050746', 'logps_train/rejected': '-101.26', 'logps_train/chosen': '-114.58', 'loss/train': '0.72454', 'examples_per_second': '32.408', 'grad_norm': '28.625', 'counters/examples': 28992, 'counters/updates': 906}
train stats after 29024 examples: {'rewards_train/chosen': '-0.018227', 'rewards_train/rejected': '-0.043041', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024814', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-154.83', 'loss/train': '0.68652', 'examples_per_second': '30.604', 'grad_norm': '27.75', 'counters/examples': 29024, 'counters/updates': 907}
train stats after 29056 examples: {'rewards_train/chosen': '-0.045428', 'rewards_train/rejected': '-0.0089278', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036501', 'logps_train/rejected': '-104.25', 'logps_train/chosen': '-147.54', 'loss/train': '0.71609', 'examples_per_second': '31.357', 'grad_norm': '30.875', 'counters/examples': 29056, 'counters/updates': 908}
train stats after 29088 examples: {'rewards_train/chosen': '-0.00019775', 'rewards_train/rejected': '0.020287', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020484', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-153', 'loss/train': '0.71357', 'examples_per_second': '31.166', 'grad_norm': '28', 'counters/examples': 29088, 'counters/updates': 909}
train stats after 29120 examples: {'rewards_train/chosen': '0.044056', 'rewards_train/rejected': '0.017661', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026395', 'logps_train/rejected': '-117.18', 'logps_train/chosen': '-161.2', 'loss/train': '0.68313', 'examples_per_second': '30.183', 'grad_norm': '35.25', 'counters/examples': 29120, 'counters/updates': 910}
train stats after 29152 examples: {'rewards_train/chosen': '-0.017775', 'rewards_train/rejected': '0.028308', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046084', 'logps_train/rejected': '-133.2', 'logps_train/chosen': '-147.86', 'loss/train': '0.72145', 'examples_per_second': '31.617', 'grad_norm': '30.125', 'counters/examples': 29152, 'counters/updates': 911}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29216 examples: {'rewards_train/chosen': '0.044531', 'rewards_train/rejected': '-0.028368', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072899', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-141.95', 'loss/train': '0.66345', 'examples_per_second': '30.293', 'grad_norm': '26.5', 'counters/examples': 29216, 'counters/updates': 913}
train stats after 29248 examples: {'rewards_train/chosen': '0.038231', 'rewards_train/rejected': '-0.0010348', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039266', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-157.27', 'loss/train': '0.67949', 'examples_per_second': '31.593', 'grad_norm': '27.75', 'counters/examples': 29248, 'counters/updates': 914}
train stats after 29280 examples: {'rewards_train/chosen': '-0.11516', 'rewards_train/rejected': '-0.057594', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.057568', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-144.14', 'loss/train': '0.72834', 'examples_per_second': '31.511', 'grad_norm': '28.875', 'counters/examples': 29280, 'counters/updates': 915}
train stats after 29312 examples: {'rewards_train/chosen': '0.022729', 'rewards_train/rejected': '-0.0094636', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032193', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-145.6', 'loss/train': '0.68611', 'examples_per_second': '30.538', 'grad_norm': '31.625', 'counters/examples': 29312, 'counters/updates': 916}
train stats after 29344 examples: {'rewards_train/chosen': '-0.096599', 'rewards_train/rejected': '-0.024879', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.07172', 'logps_train/rejected': '-118.12', 'logps_train/chosen': '-177.43', 'loss/train': '0.73535', 'examples_per_second': '30.549', 'grad_norm': '30.75', 'counters/examples': 29344, 'counters/updates': 917}
train stats after 29376 examples: {'rewards_train/chosen': '0.025383', 'rewards_train/rejected': '0.034937', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0095544', 'logps_train/rejected': '-112.27', 'logps_train/chosen': '-136.71', 'loss/train': '0.70253', 'examples_per_second': '30.241', 'grad_norm': '29.25', 'counters/examples': 29376, 'counters/updates': 918}
train stats after 29408 examples: {'rewards_train/chosen': '0.03987', 'rewards_train/rejected': '-0.025992', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065863', 'logps_train/rejected': '-125.72', 'logps_train/chosen': '-146.4', 'loss/train': '0.66602', 'examples_per_second': '32.413', 'grad_norm': '28.5', 'counters/examples': 29408, 'counters/updates': 919}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29472 examples: {'rewards_train/chosen': '-0.016622', 'rewards_train/rejected': '-0.025539', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0089167', 'logps_train/rejected': '-95.965', 'logps_train/chosen': '-144.12', 'loss/train': '0.69363', 'examples_per_second': '32.298', 'grad_norm': '25.25', 'counters/examples': 29472, 'counters/updates': 921}
train stats after 29504 examples: {'rewards_train/chosen': '-0.021394', 'rewards_train/rejected': '-0.0017668', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019628', 'logps_train/rejected': '-87.493', 'logps_train/chosen': '-103.92', 'loss/train': '0.7066', 'examples_per_second': '31.326', 'grad_norm': '23.5', 'counters/examples': 29504, 'counters/updates': 922}
train stats after 29536 examples: {'rewards_train/chosen': '-0.00067004', 'rewards_train/rejected': '0.018699', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019369', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-156.38', 'loss/train': '0.70731', 'examples_per_second': '30.651', 'grad_norm': '29.625', 'counters/examples': 29536, 'counters/updates': 923}
train stats after 29568 examples: {'rewards_train/chosen': '0.0017382', 'rewards_train/rejected': '-0.040657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042395', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-126.89', 'loss/train': '0.67346', 'examples_per_second': '31.323', 'grad_norm': '28.25', 'counters/examples': 29568, 'counters/updates': 924}
skipping logging after 29600 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '0.047799', 'rewards_train/rejected': '0.015539', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03226', 'logps_train/rejected': '-111.43', 'logps_train/chosen': '-121.45', 'loss/train': '0.68029', 'examples_per_second': '31.57', 'grad_norm': '25.875', 'counters/examples': 29632, 'counters/updates': 926}
train stats after 29664 examples: {'rewards_train/chosen': '0.04294', 'rewards_train/rejected': '-0.05028', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.09322', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-129.74', 'loss/train': '0.65244', 'examples_per_second': '31.915', 'grad_norm': '25.625', 'counters/examples': 29664, 'counters/updates': 927}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29728 examples: {'rewards_train/chosen': '0.023633', 'rewards_train/rejected': '0.057895', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034263', 'logps_train/rejected': '-150.71', 'logps_train/chosen': '-146.65', 'loss/train': '0.71355', 'examples_per_second': '31.307', 'grad_norm': '31.5', 'counters/examples': 29728, 'counters/updates': 929}
train stats after 29760 examples: {'rewards_train/chosen': '-0.019038', 'rewards_train/rejected': '0.016603', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.035641', 'logps_train/rejected': '-142.07', 'logps_train/chosen': '-130.51', 'loss/train': '0.71641', 'examples_per_second': '30.762', 'grad_norm': '29.875', 'counters/examples': 29760, 'counters/updates': 930}
train stats after 29792 examples: {'rewards_train/chosen': '0.053098', 'rewards_train/rejected': '0.0028153', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050283', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-142.94', 'loss/train': '0.67233', 'examples_per_second': '30.532', 'grad_norm': '29.25', 'counters/examples': 29792, 'counters/updates': 931}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29856 examples: {'rewards_train/chosen': '-0.012606', 'rewards_train/rejected': '-0.0062277', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0063783', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-128.08', 'loss/train': '0.69947', 'examples_per_second': '34.625', 'grad_norm': '26.25', 'counters/examples': 29856, 'counters/updates': 933}
skipping logging after 29888 examples to avoid logging too frequently
train stats after 29920 examples: {'rewards_train/chosen': '0.03778', 'rewards_train/rejected': '-0.091713', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12949', 'logps_train/rejected': '-122.33', 'logps_train/chosen': '-110.66', 'loss/train': '0.64837', 'examples_per_second': '31.763', 'grad_norm': '27.875', 'counters/examples': 29920, 'counters/updates': 935}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 29984 examples: {'rewards_train/chosen': '0.019665', 'rewards_train/rejected': '-0.040223', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059888', 'logps_train/rejected': '-99.205', 'logps_train/chosen': '-111.11', 'loss/train': '0.66778', 'examples_per_second': '35.893', 'grad_norm': '23.25', 'counters/examples': 29984, 'counters/updates': 937}
train stats after 30016 examples: {'rewards_train/chosen': '0.039946', 'rewards_train/rejected': '-0.020837', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060782', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-135.95', 'loss/train': '0.66656', 'examples_per_second': '25.772', 'grad_norm': '25.25', 'counters/examples': 30016, 'counters/updates': 938}
skipping logging after 30048 examples to avoid logging too frequently
train stats after 30080 examples: {'rewards_train/chosen': '-0.036656', 'rewards_train/rejected': '-0.046307', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0096512', 'logps_train/rejected': '-125.78', 'logps_train/chosen': '-147.63', 'loss/train': '0.6918', 'examples_per_second': '30.592', 'grad_norm': '29', 'counters/examples': 30080, 'counters/updates': 940}
train stats after 30112 examples: {'rewards_train/chosen': '-0.030006', 'rewards_train/rejected': '0.010787', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.040792', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-135.51', 'loss/train': '0.71958', 'examples_per_second': '24.052', 'grad_norm': '33.75', 'counters/examples': 30112, 'counters/updates': 941}
train stats after 30144 examples: {'rewards_train/chosen': '-0.027563', 'rewards_train/rejected': '-0.021991', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0055719', 'logps_train/rejected': '-120.5', 'logps_train/chosen': '-131.58', 'loss/train': '0.70386', 'examples_per_second': '32.311', 'grad_norm': '27.125', 'counters/examples': 30144, 'counters/updates': 942}
train stats after 30176 examples: {'rewards_train/chosen': '0.030918', 'rewards_train/rejected': '-0.08151', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.11243', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-112.92', 'loss/train': '0.64133', 'examples_per_second': '32.551', 'grad_norm': '24.375', 'counters/examples': 30176, 'counters/updates': 943}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30240 examples: {'rewards_train/chosen': '0.037589', 'rewards_train/rejected': '0.041619', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0040304', 'logps_train/rejected': '-158.23', 'logps_train/chosen': '-149.53', 'loss/train': '0.70449', 'examples_per_second': '33.081', 'grad_norm': '29.75', 'counters/examples': 30240, 'counters/updates': 945}
train stats after 30272 examples: {'rewards_train/chosen': '0.010605', 'rewards_train/rejected': '-0.0055818', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016186', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-140.71', 'loss/train': '0.68867', 'examples_per_second': '31.543', 'grad_norm': '26.75', 'counters/examples': 30272, 'counters/updates': 946}
train stats after 30304 examples: {'rewards_train/chosen': '0.008504', 'rewards_train/rejected': '-0.0089872', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017491', 'logps_train/rejected': '-113.79', 'logps_train/chosen': '-137.95', 'loss/train': '0.69099', 'examples_per_second': '31.094', 'grad_norm': '36', 'counters/examples': 30304, 'counters/updates': 947}
train stats after 30336 examples: {'rewards_train/chosen': '-0.074428', 'rewards_train/rejected': '-0.021031', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.053396', 'logps_train/rejected': '-148.21', 'logps_train/chosen': '-154.24', 'loss/train': '0.72727', 'examples_per_second': '31.617', 'grad_norm': '38.75', 'counters/examples': 30336, 'counters/updates': 948}
train stats after 30368 examples: {'rewards_train/chosen': '-0.012611', 'rewards_train/rejected': '-0.010489', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0021223', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-140.49', 'loss/train': '0.6989', 'examples_per_second': '32.259', 'grad_norm': '32.75', 'counters/examples': 30368, 'counters/updates': 949}
train stats after 30400 examples: {'rewards_train/chosen': '0.022483', 'rewards_train/rejected': '0.026246', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.003763', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-143.38', 'loss/train': '0.70368', 'examples_per_second': '31.528', 'grad_norm': '32.75', 'counters/examples': 30400, 'counters/updates': 950}
skipping logging after 30432 examples to avoid logging too frequently
train stats after 30464 examples: {'rewards_train/chosen': '-0.030859', 'rewards_train/rejected': '-0.019552', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011306', 'logps_train/rejected': '-119.86', 'logps_train/chosen': '-139.91', 'loss/train': '0.70335', 'examples_per_second': '30.131', 'grad_norm': '32', 'counters/examples': 30464, 'counters/updates': 952}
train stats after 30496 examples: {'rewards_train/chosen': '0.025017', 'rewards_train/rejected': '-0.027927', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052943', 'logps_train/rejected': '-145.08', 'logps_train/chosen': '-132.87', 'loss/train': '0.67042', 'examples_per_second': '31.674', 'grad_norm': '28.875', 'counters/examples': 30496, 'counters/updates': 953}
skipping logging after 30528 examples to avoid logging too frequently
train stats after 30560 examples: {'rewards_train/chosen': '0.0094175', 'rewards_train/rejected': '0.01577', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.006352', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-120.24', 'loss/train': '0.69901', 'examples_per_second': '30.803', 'grad_norm': '26.75', 'counters/examples': 30560, 'counters/updates': 955}
train stats after 30592 examples: {'rewards_train/chosen': '0.043628', 'rewards_train/rejected': '0.011937', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031691', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-149.17', 'loss/train': '0.68185', 'examples_per_second': '30.888', 'grad_norm': '26.375', 'counters/examples': 30592, 'counters/updates': 956}
train stats after 30624 examples: {'rewards_train/chosen': '0.029508', 'rewards_train/rejected': '0.030155', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00064702', 'logps_train/rejected': '-151.96', 'logps_train/chosen': '-141.84', 'loss/train': '0.70114', 'examples_per_second': '32.407', 'grad_norm': '30.5', 'counters/examples': 30624, 'counters/updates': 957}
train stats after 30656 examples: {'rewards_train/chosen': '0.02914', 'rewards_train/rejected': '-0.017456', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046596', 'logps_train/rejected': '-126', 'logps_train/chosen': '-137.16', 'loss/train': '0.67269', 'examples_per_second': '30.246', 'grad_norm': '26.25', 'counters/examples': 30656, 'counters/updates': 958}
skipping logging after 30688 examples to avoid logging too frequently
train stats after 30720 examples: {'rewards_train/chosen': '0.038316', 'rewards_train/rejected': '0.015757', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022559', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-121.99', 'loss/train': '0.68649', 'examples_per_second': '35.861', 'grad_norm': '27.625', 'counters/examples': 30720, 'counters/updates': 960}
train stats after 30752 examples: {'rewards_train/chosen': '0.0012614', 'rewards_train/rejected': '-0.011087', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012349', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-136.16', 'loss/train': '0.69256', 'examples_per_second': '25.663', 'grad_norm': '36', 'counters/examples': 30752, 'counters/updates': 961}
train stats after 30784 examples: {'rewards_train/chosen': '0.04471', 'rewards_train/rejected': '-0.043808', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088517', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-116.33', 'loss/train': '0.65557', 'examples_per_second': '31.647', 'grad_norm': '25.25', 'counters/examples': 30784, 'counters/updates': 962}
train stats after 30816 examples: {'rewards_train/chosen': '0.02154', 'rewards_train/rejected': '0.023908', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0023684', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-166.45', 'loss/train': '0.69859', 'examples_per_second': '31.345', 'grad_norm': '34', 'counters/examples': 30816, 'counters/updates': 963}
train stats after 30848 examples: {'rewards_train/chosen': '0.041468', 'rewards_train/rejected': '0.024852', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016616', 'logps_train/rejected': '-139.26', 'logps_train/chosen': '-163.15', 'loss/train': '0.69125', 'examples_per_second': '31.481', 'grad_norm': '31.125', 'counters/examples': 30848, 'counters/updates': 964}
train stats after 30880 examples: {'rewards_train/chosen': '0.014161', 'rewards_train/rejected': '0.050915', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036755', 'logps_train/rejected': '-94.753', 'logps_train/chosen': '-141.01', 'loss/train': '0.71549', 'examples_per_second': '31.611', 'grad_norm': '29.75', 'counters/examples': 30880, 'counters/updates': 965}
train stats after 30912 examples: {'rewards_train/chosen': '0.052539', 'rewards_train/rejected': '0.0048093', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04773', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-162.08', 'loss/train': '0.67373', 'examples_per_second': '30.22', 'grad_norm': '28.375', 'counters/examples': 30912, 'counters/updates': 966}
train stats after 30944 examples: {'rewards_train/chosen': '0.030715', 'rewards_train/rejected': '0.027161', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0035541', 'logps_train/rejected': '-139.72', 'logps_train/chosen': '-128.76', 'loss/train': '0.69772', 'examples_per_second': '31.498', 'grad_norm': '29.625', 'counters/examples': 30944, 'counters/updates': 967}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31008 examples: {'rewards_train/chosen': '0.030495', 'rewards_train/rejected': '0.031098', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0006032', 'logps_train/rejected': '-116.29', 'logps_train/chosen': '-145.26', 'loss/train': '0.69861', 'examples_per_second': '34.306', 'grad_norm': '27.125', 'counters/examples': 31008, 'counters/updates': 969}
train stats after 31040 examples: {'rewards_train/chosen': '0.032189', 'rewards_train/rejected': '0.0018604', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030329', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-101.89', 'loss/train': '0.6809', 'examples_per_second': '31.449', 'grad_norm': '25.375', 'counters/examples': 31040, 'counters/updates': 970}
train stats after 31072 examples: {'rewards_train/chosen': '-0.019176', 'rewards_train/rejected': '-0.037073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017897', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-139.16', 'loss/train': '0.68857', 'examples_per_second': '33.034', 'grad_norm': '26.375', 'counters/examples': 31072, 'counters/updates': 971}
train stats after 31104 examples: {'rewards_train/chosen': '0.00097339', 'rewards_train/rejected': '-0.023698', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024671', 'logps_train/rejected': '-155.87', 'logps_train/chosen': '-119.43', 'loss/train': '0.68584', 'examples_per_second': '32.751', 'grad_norm': '25.75', 'counters/examples': 31104, 'counters/updates': 972}
train stats after 31136 examples: {'rewards_train/chosen': '-0.027567', 'rewards_train/rejected': '0.028097', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.055664', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-147.8', 'loss/train': '0.7271', 'examples_per_second': '30.688', 'grad_norm': '32.75', 'counters/examples': 31136, 'counters/updates': 973}
train stats after 31168 examples: {'rewards_train/chosen': '0.0085727', 'rewards_train/rejected': '0.0033418', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0052309', 'logps_train/rejected': '-170.01', 'logps_train/chosen': '-140.19', 'loss/train': '0.69424', 'examples_per_second': '31.637', 'grad_norm': '30.625', 'counters/examples': 31168, 'counters/updates': 974}
train stats after 31200 examples: {'rewards_train/chosen': '0.033441', 'rewards_train/rejected': '0.020462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012979', 'logps_train/rejected': '-120.48', 'logps_train/chosen': '-162.84', 'loss/train': '0.69222', 'examples_per_second': '32.769', 'grad_norm': '26.5', 'counters/examples': 31200, 'counters/updates': 975}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31264 examples: {'rewards_train/chosen': '0.0030671', 'rewards_train/rejected': '0.019147', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.01608', 'logps_train/rejected': '-134.74', 'logps_train/chosen': '-165.94', 'loss/train': '0.70615', 'examples_per_second': '31.399', 'grad_norm': '31.875', 'counters/examples': 31264, 'counters/updates': 977}
train stats after 31296 examples: {'rewards_train/chosen': '-0.0014818', 'rewards_train/rejected': '-0.029412', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02793', 'logps_train/rejected': '-104.1', 'logps_train/chosen': '-127.95', 'loss/train': '0.68806', 'examples_per_second': '30.622', 'grad_norm': '26.875', 'counters/examples': 31296, 'counters/updates': 978}
train stats after 31328 examples: {'rewards_train/chosen': '-0.0039952', 'rewards_train/rejected': '0.0047277', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.008723', 'logps_train/rejected': '-92.098', 'logps_train/chosen': '-143.83', 'loss/train': '0.70747', 'examples_per_second': '30.095', 'grad_norm': '34', 'counters/examples': 31328, 'counters/updates': 979}
train stats after 31360 examples: {'rewards_train/chosen': '-0.011401', 'rewards_train/rejected': '0.017182', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028583', 'logps_train/rejected': '-163.81', 'logps_train/chosen': '-161.27', 'loss/train': '0.71233', 'examples_per_second': '31.74', 'grad_norm': '31.375', 'counters/examples': 31360, 'counters/updates': 980}
train stats after 31392 examples: {'rewards_train/chosen': '-0.0041615', 'rewards_train/rejected': '-0.016702', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012541', 'logps_train/rejected': '-115.87', 'logps_train/chosen': '-160.49', 'loss/train': '0.69405', 'examples_per_second': '30.589', 'grad_norm': '26.125', 'counters/examples': 31392, 'counters/updates': 981}
train stats after 31424 examples: {'rewards_train/chosen': '0.032817', 'rewards_train/rejected': '-0.013896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046713', 'logps_train/rejected': '-108.85', 'logps_train/chosen': '-137.59', 'loss/train': '0.67494', 'examples_per_second': '30.639', 'grad_norm': '25.625', 'counters/examples': 31424, 'counters/updates': 982}
train stats after 31456 examples: {'rewards_train/chosen': '0.032214', 'rewards_train/rejected': '0.01659', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015624', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-164.08', 'loss/train': '0.69181', 'examples_per_second': '31.61', 'grad_norm': '34.25', 'counters/examples': 31456, 'counters/updates': 983}
train stats after 31488 examples: {'rewards_train/chosen': '0.0090779', 'rewards_train/rejected': '0.020665', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011587', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-152.65', 'loss/train': '0.7056', 'examples_per_second': '31.592', 'grad_norm': '34', 'counters/examples': 31488, 'counters/updates': 984}
skipping logging after 31520 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '0.062592', 'rewards_train/rejected': '-0.026959', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089552', 'logps_train/rejected': '-131.03', 'logps_train/chosen': '-148.15', 'loss/train': '0.65458', 'examples_per_second': '31.521', 'grad_norm': '26.5', 'counters/examples': 31552, 'counters/updates': 986}
train stats after 31584 examples: {'rewards_train/chosen': '-0.0088159', 'rewards_train/rejected': '0.0043465', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013162', 'logps_train/rejected': '-142.83', 'logps_train/chosen': '-142.81', 'loss/train': '0.70873', 'examples_per_second': '31.598', 'grad_norm': '34', 'counters/examples': 31584, 'counters/updates': 987}
train stats after 31616 examples: {'rewards_train/chosen': '-0.027862', 'rewards_train/rejected': '0.0061528', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.034014', 'logps_train/rejected': '-116.81', 'logps_train/chosen': '-158.43', 'loss/train': '0.71696', 'examples_per_second': '30.793', 'grad_norm': '29.625', 'counters/examples': 31616, 'counters/updates': 988}
train stats after 31648 examples: {'rewards_train/chosen': '0.047383', 'rewards_train/rejected': '0.0017937', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045589', 'logps_train/rejected': '-136.03', 'logps_train/chosen': '-139.53', 'loss/train': '0.6757', 'examples_per_second': '31.054', 'grad_norm': '27.625', 'counters/examples': 31648, 'counters/updates': 989}
train stats after 31680 examples: {'rewards_train/chosen': '0.0068883', 'rewards_train/rejected': '0.0076501', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.00076178', 'logps_train/rejected': '-117.05', 'logps_train/chosen': '-161.37', 'loss/train': '0.69829', 'examples_per_second': '31.491', 'grad_norm': '28.5', 'counters/examples': 31680, 'counters/updates': 990}
train stats after 31712 examples: {'rewards_train/chosen': '-0.020249', 'rewards_train/rejected': '-0.022887', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0026374', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-146.23', 'loss/train': '0.69845', 'examples_per_second': '31.107', 'grad_norm': '29.875', 'counters/examples': 31712, 'counters/updates': 991}
train stats after 31744 examples: {'rewards_train/chosen': '0.01414', 'rewards_train/rejected': '0.011829', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0023104', 'logps_train/rejected': '-152.73', 'logps_train/chosen': '-166.75', 'loss/train': '0.69861', 'examples_per_second': '31.288', 'grad_norm': '38.75', 'counters/examples': 31744, 'counters/updates': 992}
skipping logging after 31776 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '-0.036507', 'rewards_train/rejected': '0.040277', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.076784', 'logps_train/rejected': '-135.17', 'logps_train/chosen': '-177.76', 'loss/train': '0.73655', 'examples_per_second': '31.47', 'grad_norm': '33.5', 'counters/examples': 31808, 'counters/updates': 994}
train stats after 31840 examples: {'rewards_train/chosen': '0.010124', 'rewards_train/rejected': '0.016357', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.0062329', 'logps_train/rejected': '-91.279', 'logps_train/chosen': '-103.01', 'loss/train': '0.69945', 'examples_per_second': '30.933', 'grad_norm': '21.625', 'counters/examples': 31840, 'counters/updates': 995}
train stats after 31872 examples: {'rewards_train/chosen': '0.012974', 'rewards_train/rejected': '-0.0373', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050274', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-139.26', 'loss/train': '0.67302', 'examples_per_second': '32.132', 'grad_norm': '28.125', 'counters/examples': 31872, 'counters/updates': 996}
train stats after 31904 examples: {'rewards_train/chosen': '0.0422', 'rewards_train/rejected': '-0.027583', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069783', 'logps_train/rejected': '-162.17', 'logps_train/chosen': '-124.15', 'loss/train': '0.66817', 'examples_per_second': '30.581', 'grad_norm': '31.5', 'counters/examples': 31904, 'counters/updates': 997}
skipping logging after 31936 examples to avoid logging too frequently
train stats after 31968 examples: {'rewards_train/chosen': '0.027695', 'rewards_train/rejected': '0.025864', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.001831', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-150.99', 'loss/train': '0.70209', 'examples_per_second': '32.112', 'grad_norm': '31.375', 'counters/examples': 31968, 'counters/updates': 999}
train stats after 32000 examples: {'rewards_train/chosen': '-0.0071322', 'rewards_train/rejected': '-0.014877', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0077447', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-147.87', 'loss/train': '0.69335', 'examples_per_second': '31.213', 'grad_norm': '30.25', 'counters/examples': 32000, 'counters/updates': 1000}
train stats after 32032 examples: {'rewards_train/chosen': '-0.0053445', 'rewards_train/rejected': '-0.015423', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010079', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-162.38', 'loss/train': '0.69474', 'examples_per_second': '30.452', 'grad_norm': '28.25', 'counters/examples': 32032, 'counters/updates': 1001}
train stats after 32064 examples: {'rewards_train/chosen': '-0.0056266', 'rewards_train/rejected': '0.0041296', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0097563', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-134.15', 'loss/train': '0.70497', 'examples_per_second': '31.622', 'grad_norm': '29.375', 'counters/examples': 32064, 'counters/updates': 1002}
train stats after 32096 examples: {'rewards_train/chosen': '0.018351', 'rewards_train/rejected': '-0.021273', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039624', 'logps_train/rejected': '-126.45', 'logps_train/chosen': '-100.79', 'loss/train': '0.67816', 'examples_per_second': '31.751', 'grad_norm': '26.375', 'counters/examples': 32096, 'counters/updates': 1003}
train stats after 32128 examples: {'rewards_train/chosen': '0.018668', 'rewards_train/rejected': '-0.036111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054778', 'logps_train/rejected': '-101.56', 'logps_train/chosen': '-120.41', 'loss/train': '0.67144', 'examples_per_second': '31.642', 'grad_norm': '25.875', 'counters/examples': 32128, 'counters/updates': 1004}
skipping logging after 32160 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '-0.03359', 'rewards_train/rejected': '-0.0039809', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.029609', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-155.09', 'loss/train': '0.71673', 'examples_per_second': '30.313', 'grad_norm': '33', 'counters/examples': 32192, 'counters/updates': 1006}
train stats after 32224 examples: {'rewards_train/chosen': '0.011042', 'rewards_train/rejected': '0.0020141', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0090281', 'logps_train/rejected': '-112.81', 'logps_train/chosen': '-118.68', 'loss/train': '0.6919', 'examples_per_second': '30.444', 'grad_norm': '23.875', 'counters/examples': 32224, 'counters/updates': 1007}
train stats after 32256 examples: {'rewards_train/chosen': '0.0076942', 'rewards_train/rejected': '-0.015924', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023618', 'logps_train/rejected': '-100.4', 'logps_train/chosen': '-123.76', 'loss/train': '0.68678', 'examples_per_second': '30.525', 'grad_norm': '27', 'counters/examples': 32256, 'counters/updates': 1008}
skipping logging after 32288 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '-0.011378', 'rewards_train/rejected': '-0.030903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019524', 'logps_train/rejected': '-101.08', 'logps_train/chosen': '-149.18', 'loss/train': '0.68568', 'examples_per_second': '32.91', 'grad_norm': '26.5', 'counters/examples': 32320, 'counters/updates': 1010}
train stats after 32352 examples: {'rewards_train/chosen': '-0.02317', 'rewards_train/rejected': '0.015031', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.038201', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-137', 'loss/train': '0.71548', 'examples_per_second': '31.217', 'grad_norm': '31.375', 'counters/examples': 32352, 'counters/updates': 1011}
train stats after 32384 examples: {'rewards_train/chosen': '0.048316', 'rewards_train/rejected': '0.019194', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029122', 'logps_train/rejected': '-92.898', 'logps_train/chosen': '-152.17', 'loss/train': '0.68624', 'examples_per_second': '33.022', 'grad_norm': '29.625', 'counters/examples': 32384, 'counters/updates': 1012}
train stats after 32416 examples: {'rewards_train/chosen': '-0.01032', 'rewards_train/rejected': '-0.0014559', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0088644', 'logps_train/rejected': '-124.01', 'logps_train/chosen': '-131.45', 'loss/train': '0.70321', 'examples_per_second': '31.617', 'grad_norm': '29', 'counters/examples': 32416, 'counters/updates': 1013}
train stats after 32448 examples: {'rewards_train/chosen': '0.047608', 'rewards_train/rejected': '0.016627', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030981', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-164.8', 'loss/train': '0.68243', 'examples_per_second': '31.642', 'grad_norm': '29', 'counters/examples': 32448, 'counters/updates': 1014}
train stats after 32480 examples: {'rewards_train/chosen': '-0.0053807', 'rewards_train/rejected': '0.021107', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026488', 'logps_train/rejected': '-107.97', 'logps_train/chosen': '-109.35', 'loss/train': '0.71014', 'examples_per_second': '31.568', 'grad_norm': '35', 'counters/examples': 32480, 'counters/updates': 1015}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32544 examples: {'rewards_train/chosen': '0.050293', 'rewards_train/rejected': '-0.03399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084283', 'logps_train/rejected': '-110.68', 'logps_train/chosen': '-138.93', 'loss/train': '0.66132', 'examples_per_second': '33.526', 'grad_norm': '31.125', 'counters/examples': 32544, 'counters/updates': 1017}
train stats after 32576 examples: {'rewards_train/chosen': '0.049868', 'rewards_train/rejected': '-0.0164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066268', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-134.51', 'loss/train': '0.66614', 'examples_per_second': '31.642', 'grad_norm': '27.375', 'counters/examples': 32576, 'counters/updates': 1018}
train stats after 32608 examples: {'rewards_train/chosen': '0.052614', 'rewards_train/rejected': '0.02256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030054', 'logps_train/rejected': '-103.27', 'logps_train/chosen': '-128.1', 'loss/train': '0.6812', 'examples_per_second': '32.721', 'grad_norm': '26.375', 'counters/examples': 32608, 'counters/updates': 1019}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32672 examples: {'rewards_train/chosen': '0.0054971', 'rewards_train/rejected': '-0.01377', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019267', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-158.6', 'loss/train': '0.68842', 'examples_per_second': '31.596', 'grad_norm': '27.375', 'counters/examples': 32672, 'counters/updates': 1021}
skipping logging after 32704 examples to avoid logging too frequently
train stats after 32736 examples: {'rewards_train/chosen': '0.029353', 'rewards_train/rejected': '-0.038389', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067742', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-161.87', 'loss/train': '0.66326', 'examples_per_second': '30.131', 'grad_norm': '30.5', 'counters/examples': 32736, 'counters/updates': 1023}
train stats after 32768 examples: {'rewards_train/chosen': '-0.010342', 'rewards_train/rejected': '-0.021291', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010949', 'logps_train/rejected': '-167.41', 'logps_train/chosen': '-160.31', 'loss/train': '0.69722', 'examples_per_second': '31.566', 'grad_norm': '34', 'counters/examples': 32768, 'counters/updates': 1024}
skipping logging after 32800 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '0.031449', 'rewards_train/rejected': '-0.0056229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037072', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-121.61', 'loss/train': '0.68012', 'examples_per_second': '31.604', 'grad_norm': '26.625', 'counters/examples': 32832, 'counters/updates': 1026}
train stats after 32864 examples: {'rewards_train/chosen': '0.030896', 'rewards_train/rejected': '0.098618', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.067722', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-151.76', 'loss/train': '0.73822', 'examples_per_second': '31.172', 'grad_norm': '34.25', 'counters/examples': 32864, 'counters/updates': 1027}
train stats after 32896 examples: {'rewards_train/chosen': '0.059725', 'rewards_train/rejected': '0.051154', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0085712', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-131.35', 'loss/train': '0.69263', 'examples_per_second': '31.793', 'grad_norm': '30.125', 'counters/examples': 32896, 'counters/updates': 1028}
skipping logging after 32928 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '-0.012437', 'rewards_train/rejected': '-0.016564', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0041262', 'logps_train/rejected': '-156.8', 'logps_train/chosen': '-143.08', 'loss/train': '0.69635', 'examples_per_second': '30.598', 'grad_norm': '32', 'counters/examples': 32960, 'counters/updates': 1030}
train stats after 32992 examples: {'rewards_train/chosen': '-0.0032328', 'rewards_train/rejected': '-0.020828', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017595', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-122.72', 'loss/train': '0.68923', 'examples_per_second': '31.261', 'grad_norm': '35.75', 'counters/examples': 32992, 'counters/updates': 1031}
train stats after 33024 examples: {'rewards_train/chosen': '-0.0066063', 'rewards_train/rejected': '0.049052', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.055659', 'logps_train/rejected': '-103.46', 'logps_train/chosen': '-110.16', 'loss/train': '0.72448', 'examples_per_second': '31.547', 'grad_norm': '31.125', 'counters/examples': 33024, 'counters/updates': 1032}
train stats after 33056 examples: {'rewards_train/chosen': '-0.044123', 'rewards_train/rejected': '0.023454', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.067576', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-151.98', 'loss/train': '0.73165', 'examples_per_second': '31.774', 'grad_norm': '30', 'counters/examples': 33056, 'counters/updates': 1033}
train stats after 33088 examples: {'rewards_train/chosen': '0.0080104', 'rewards_train/rejected': '-0.027333', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035344', 'logps_train/rejected': '-188.71', 'logps_train/chosen': '-168.82', 'loss/train': '0.68609', 'examples_per_second': '31.628', 'grad_norm': '31', 'counters/examples': 33088, 'counters/updates': 1034}
skipping logging after 33120 examples to avoid logging too frequently
train stats after 33152 examples: {'rewards_train/chosen': '0.041159', 'rewards_train/rejected': '0.05726', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016102', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-141', 'loss/train': '0.70807', 'examples_per_second': '30.63', 'grad_norm': '29.875', 'counters/examples': 33152, 'counters/updates': 1036}
train stats after 33184 examples: {'rewards_train/chosen': '0.003338', 'rewards_train/rejected': '0.034037', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030699', 'logps_train/rejected': '-139.56', 'logps_train/chosen': '-136.72', 'loss/train': '0.71522', 'examples_per_second': '31.447', 'grad_norm': '29.875', 'counters/examples': 33184, 'counters/updates': 1037}
train stats after 33216 examples: {'rewards_train/chosen': '0.00070253', 'rewards_train/rejected': '-0.023228', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02393', 'logps_train/rejected': '-123.87', 'logps_train/chosen': '-135.2', 'loss/train': '0.68753', 'examples_per_second': '30.656', 'grad_norm': '30.375', 'counters/examples': 33216, 'counters/updates': 1038}
train stats after 33248 examples: {'rewards_train/chosen': '0.014526', 'rewards_train/rejected': '-0.042636', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057162', 'logps_train/rejected': '-177.04', 'logps_train/chosen': '-177.56', 'loss/train': '0.6751', 'examples_per_second': '30.016', 'grad_norm': '33', 'counters/examples': 33248, 'counters/updates': 1039}
train stats after 33280 examples: {'rewards_train/chosen': '0.013632', 'rewards_train/rejected': '-0.037458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051091', 'logps_train/rejected': '-155.29', 'logps_train/chosen': '-132.68', 'loss/train': '0.67598', 'examples_per_second': '32.516', 'grad_norm': '29.125', 'counters/examples': 33280, 'counters/updates': 1040}
train stats after 33312 examples: {'rewards_train/chosen': '0.0027311', 'rewards_train/rejected': '0.031022', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028291', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-156.67', 'loss/train': '0.71553', 'examples_per_second': '32.114', 'grad_norm': '29.25', 'counters/examples': 33312, 'counters/updates': 1041}
train stats after 33344 examples: {'rewards_train/chosen': '0.041154', 'rewards_train/rejected': '-0.082447', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1236', 'logps_train/rejected': '-156.37', 'logps_train/chosen': '-150.16', 'loss/train': '0.63911', 'examples_per_second': '33.043', 'grad_norm': '27.875', 'counters/examples': 33344, 'counters/updates': 1042}
train stats after 33376 examples: {'rewards_train/chosen': '-0.033154', 'rewards_train/rejected': '-0.00025852', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032896', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-172.6', 'loss/train': '0.71664', 'examples_per_second': '32.573', 'grad_norm': '33.5', 'counters/examples': 33376, 'counters/updates': 1043}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33440 examples: {'rewards_train/chosen': '-0.0086449', 'rewards_train/rejected': '0.01487', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023515', 'logps_train/rejected': '-116.37', 'logps_train/chosen': '-161.21', 'loss/train': '0.70946', 'examples_per_second': '33.309', 'grad_norm': '29.625', 'counters/examples': 33440, 'counters/updates': 1045}
train stats after 33472 examples: {'rewards_train/chosen': '0.030317', 'rewards_train/rejected': '0.064018', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033701', 'logps_train/rejected': '-137.01', 'logps_train/chosen': '-146.29', 'loss/train': '0.71702', 'examples_per_second': '31.618', 'grad_norm': '35', 'counters/examples': 33472, 'counters/updates': 1046}
skipping logging after 33504 examples to avoid logging too frequently
train stats after 33536 examples: {'rewards_train/chosen': '-0.014385', 'rewards_train/rejected': '-0.051676', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037291', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-154.34', 'loss/train': '0.67987', 'examples_per_second': '32.808', 'grad_norm': '29.25', 'counters/examples': 33536, 'counters/updates': 1048}
train stats after 33568 examples: {'rewards_train/chosen': '0.01755', 'rewards_train/rejected': '0.0411', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023549', 'logps_train/rejected': '-106.84', 'logps_train/chosen': '-130.71', 'loss/train': '0.70948', 'examples_per_second': '31.427', 'grad_norm': '26.5', 'counters/examples': 33568, 'counters/updates': 1049}
train stats after 33600 examples: {'rewards_train/chosen': '0.0063236', 'rewards_train/rejected': '-0.022897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02922', 'logps_train/rejected': '-111.33', 'logps_train/chosen': '-182.91', 'loss/train': '0.68481', 'examples_per_second': '31.552', 'grad_norm': '31.5', 'counters/examples': 33600, 'counters/updates': 1050}
train stats after 33632 examples: {'rewards_train/chosen': '0.012699', 'rewards_train/rejected': '0.04686', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034162', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-154.24', 'loss/train': '0.71643', 'examples_per_second': '31.569', 'grad_norm': '33.5', 'counters/examples': 33632, 'counters/updates': 1051}
train stats after 33664 examples: {'rewards_train/chosen': '0.054681', 'rewards_train/rejected': '0.020785', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033896', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-126.68', 'loss/train': '0.68311', 'examples_per_second': '30.145', 'grad_norm': '26.75', 'counters/examples': 33664, 'counters/updates': 1052}
train stats after 33696 examples: {'rewards_train/chosen': '-0.01795', 'rewards_train/rejected': '0.035363', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.053313', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-152.53', 'loss/train': '0.72918', 'examples_per_second': '30.094', 'grad_norm': '29.25', 'counters/examples': 33696, 'counters/updates': 1053}
train stats after 33728 examples: {'rewards_train/chosen': '0.066552', 'rewards_train/rejected': '0.044393', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022159', 'logps_train/rejected': '-142.41', 'logps_train/chosen': '-133.31', 'loss/train': '0.69081', 'examples_per_second': '30.445', 'grad_norm': '28.875', 'counters/examples': 33728, 'counters/updates': 1054}
train stats after 33760 examples: {'rewards_train/chosen': '0.018822', 'rewards_train/rejected': '-0.0094692', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028291', 'logps_train/rejected': '-105.6', 'logps_train/chosen': '-153.9', 'loss/train': '0.68448', 'examples_per_second': '30.585', 'grad_norm': '26.375', 'counters/examples': 33760, 'counters/updates': 1055}
train stats after 33792 examples: {'rewards_train/chosen': '0.052407', 'rewards_train/rejected': '0.018162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034245', 'logps_train/rejected': '-103.81', 'logps_train/chosen': '-149.83', 'loss/train': '0.68174', 'examples_per_second': '31.694', 'grad_norm': '25.625', 'counters/examples': 33792, 'counters/updates': 1056}
train stats after 33824 examples: {'rewards_train/chosen': '-0.024425', 'rewards_train/rejected': '-0.030089', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0056642', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-157.42', 'loss/train': '0.69485', 'examples_per_second': '30.23', 'grad_norm': '27.75', 'counters/examples': 33824, 'counters/updates': 1057}
train stats after 33856 examples: {'rewards_train/chosen': '0.017538', 'rewards_train/rejected': '-0.011289', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028826', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-123.09', 'loss/train': '0.68196', 'examples_per_second': '31.522', 'grad_norm': '25.375', 'counters/examples': 33856, 'counters/updates': 1058}
train stats after 33888 examples: {'rewards_train/chosen': '-0.046464', 'rewards_train/rejected': '0.037265', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.083729', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-153.41', 'loss/train': '0.74615', 'examples_per_second': '31.482', 'grad_norm': '32.75', 'counters/examples': 33888, 'counters/updates': 1059}
train stats after 33920 examples: {'rewards_train/chosen': '0.070771', 'rewards_train/rejected': '-0.038471', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10924', 'logps_train/rejected': '-113.01', 'logps_train/chosen': '-147.74', 'loss/train': '0.64906', 'examples_per_second': '31.664', 'grad_norm': '25.625', 'counters/examples': 33920, 'counters/updates': 1060}
train stats after 33952 examples: {'rewards_train/chosen': '0.062422', 'rewards_train/rejected': '0.026792', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035631', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-130.56', 'loss/train': '0.68576', 'examples_per_second': '32.165', 'grad_norm': '32.5', 'counters/examples': 33952, 'counters/updates': 1061}
train stats after 33984 examples: {'rewards_train/chosen': '0.094128', 'rewards_train/rejected': '-0.028329', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12246', 'logps_train/rejected': '-96.893', 'logps_train/chosen': '-160.62', 'loss/train': '0.63847', 'examples_per_second': '31.512', 'grad_norm': '31.375', 'counters/examples': 33984, 'counters/updates': 1062}
train stats after 34016 examples: {'rewards_train/chosen': '0.010654', 'rewards_train/rejected': '0.032994', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.02234', 'logps_train/rejected': '-109.9', 'logps_train/chosen': '-135.85', 'loss/train': '0.70811', 'examples_per_second': '30.515', 'grad_norm': '31.125', 'counters/examples': 34016, 'counters/updates': 1063}
train stats after 34048 examples: {'rewards_train/chosen': '0.068631', 'rewards_train/rejected': '0.0055681', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063062', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-138.22', 'loss/train': '0.66805', 'examples_per_second': '30.623', 'grad_norm': '26.5', 'counters/examples': 34048, 'counters/updates': 1064}
train stats after 34080 examples: {'rewards_train/chosen': '-0.030976', 'rewards_train/rejected': '-0.015157', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01582', 'logps_train/rejected': '-88.102', 'logps_train/chosen': '-157.64', 'loss/train': '0.70722', 'examples_per_second': '30.14', 'grad_norm': '32.75', 'counters/examples': 34080, 'counters/updates': 1065}
skipping logging after 34112 examples to avoid logging too frequently
train stats after 34144 examples: {'rewards_train/chosen': '0.017395', 'rewards_train/rejected': '0.017101', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00029382', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-125.07', 'loss/train': '0.69841', 'examples_per_second': '34.013', 'grad_norm': '34', 'counters/examples': 34144, 'counters/updates': 1067}
train stats after 34176 examples: {'rewards_train/chosen': '0.031328', 'rewards_train/rejected': '0.0060917', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025236', 'logps_train/rejected': '-134.1', 'logps_train/chosen': '-162.4', 'loss/train': '0.68818', 'examples_per_second': '31.236', 'grad_norm': '65.5', 'counters/examples': 34176, 'counters/updates': 1068}
train stats after 34208 examples: {'rewards_train/chosen': '0.025711', 'rewards_train/rejected': '0.00054651', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025164', 'logps_train/rejected': '-95.639', 'logps_train/chosen': '-127.84', 'loss/train': '0.68497', 'examples_per_second': '32.699', 'grad_norm': '25.625', 'counters/examples': 34208, 'counters/updates': 1069}
train stats after 34240 examples: {'rewards_train/chosen': '-0.011162', 'rewards_train/rejected': '0.02651', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037672', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-149.86', 'loss/train': '0.71663', 'examples_per_second': '31.709', 'grad_norm': '29.125', 'counters/examples': 34240, 'counters/updates': 1070}
train stats after 34272 examples: {'rewards_train/chosen': '0.013491', 'rewards_train/rejected': '-0.014628', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028118', 'logps_train/rejected': '-140.06', 'logps_train/chosen': '-184.73', 'loss/train': '0.68569', 'examples_per_second': '31.558', 'grad_norm': '43.75', 'counters/examples': 34272, 'counters/updates': 1071}
train stats after 34304 examples: {'rewards_train/chosen': '0.0028011', 'rewards_train/rejected': '-0.021136', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023937', 'logps_train/rejected': '-138.18', 'logps_train/chosen': '-149.19', 'loss/train': '0.68683', 'examples_per_second': '31.669', 'grad_norm': '32.75', 'counters/examples': 34304, 'counters/updates': 1072}
train stats after 34336 examples: {'rewards_train/chosen': '0.048528', 'rewards_train/rejected': '-0.00082644', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049355', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-93.693', 'loss/train': '0.67176', 'examples_per_second': '31.518', 'grad_norm': '24.25', 'counters/examples': 34336, 'counters/updates': 1073}
skipping logging after 34368 examples to avoid logging too frequently
train stats after 34400 examples: {'rewards_train/chosen': '0.063788', 'rewards_train/rejected': '0.027618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036171', 'logps_train/rejected': '-115.86', 'logps_train/chosen': '-170.19', 'loss/train': '0.68072', 'examples_per_second': '30.317', 'grad_norm': '31.625', 'counters/examples': 34400, 'counters/updates': 1075}
train stats after 34432 examples: {'rewards_train/chosen': '0.0259', 'rewards_train/rejected': '0.0099725', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015927', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-140.44', 'loss/train': '0.68825', 'examples_per_second': '30.933', 'grad_norm': '32', 'counters/examples': 34432, 'counters/updates': 1076}
train stats after 34464 examples: {'rewards_train/chosen': '-0.01611', 'rewards_train/rejected': '-0.027563', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011452', 'logps_train/rejected': '-151.32', 'logps_train/chosen': '-131.62', 'loss/train': '0.69249', 'examples_per_second': '31.487', 'grad_norm': '29.875', 'counters/examples': 34464, 'counters/updates': 1077}
train stats after 34496 examples: {'rewards_train/chosen': '0.010135', 'rewards_train/rejected': '0.0099398', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00019525', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-138', 'loss/train': '0.702', 'examples_per_second': '31.426', 'grad_norm': '25.75', 'counters/examples': 34496, 'counters/updates': 1078}
train stats after 34528 examples: {'rewards_train/chosen': '0.023811', 'rewards_train/rejected': '0.014196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0096148', 'logps_train/rejected': '-131.1', 'logps_train/chosen': '-113.16', 'loss/train': '0.69118', 'examples_per_second': '32.783', 'grad_norm': '29.125', 'counters/examples': 34528, 'counters/updates': 1079}
train stats after 34560 examples: {'rewards_train/chosen': '0.029605', 'rewards_train/rejected': '0.00045485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029151', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-142.58', 'loss/train': '0.68408', 'examples_per_second': '30.484', 'grad_norm': '26.25', 'counters/examples': 34560, 'counters/updates': 1080}
train stats after 34592 examples: {'rewards_train/chosen': '-0.018667', 'rewards_train/rejected': '0.11862', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.13728', 'logps_train/rejected': '-148.6', 'logps_train/chosen': '-154.64', 'loss/train': '0.79479', 'examples_per_second': '31.065', 'grad_norm': '114', 'counters/examples': 34592, 'counters/updates': 1081}
skipping logging after 34624 examples to avoid logging too frequently
train stats after 34656 examples: {'rewards_train/chosen': '0.054012', 'rewards_train/rejected': '0.023791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030221', 'logps_train/rejected': '-97.516', 'logps_train/chosen': '-151.19', 'loss/train': '0.68712', 'examples_per_second': '31.533', 'grad_norm': '34', 'counters/examples': 34656, 'counters/updates': 1083}
train stats after 34688 examples: {'rewards_train/chosen': '0.023187', 'rewards_train/rejected': '0.044372', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.021185', 'logps_train/rejected': '-97.643', 'logps_train/chosen': '-115.44', 'loss/train': '0.70799', 'examples_per_second': '30.726', 'grad_norm': '25.625', 'counters/examples': 34688, 'counters/updates': 1084}
skipping logging after 34720 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '0.055207', 'rewards_train/rejected': '0.0071544', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.048052', 'logps_train/rejected': '-125.19', 'logps_train/chosen': '-120.02', 'loss/train': '0.67209', 'examples_per_second': '31.638', 'grad_norm': '25.25', 'counters/examples': 34752, 'counters/updates': 1086}
train stats after 34784 examples: {'rewards_train/chosen': '0.013143', 'rewards_train/rejected': '0.012727', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.00041584', 'logps_train/rejected': '-128.31', 'logps_train/chosen': '-123.61', 'loss/train': '0.69918', 'examples_per_second': '30.204', 'grad_norm': '41.25', 'counters/examples': 34784, 'counters/updates': 1087}
train stats after 34816 examples: {'rewards_train/chosen': '-0.010154', 'rewards_train/rejected': '-0.041248', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031094', 'logps_train/rejected': '-158.64', 'logps_train/chosen': '-105.78', 'loss/train': '0.68171', 'examples_per_second': '30.905', 'grad_norm': '26.625', 'counters/examples': 34816, 'counters/updates': 1088}
train stats after 34848 examples: {'rewards_train/chosen': '0.050897', 'rewards_train/rejected': '0.035741', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015155', 'logps_train/rejected': '-109.72', 'logps_train/chosen': '-153.49', 'loss/train': '0.69082', 'examples_per_second': '31.434', 'grad_norm': '27.75', 'counters/examples': 34848, 'counters/updates': 1089}
train stats after 34880 examples: {'rewards_train/chosen': '-0.033012', 'rewards_train/rejected': '-0.044546', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011534', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-122.08', 'loss/train': '0.69115', 'examples_per_second': '33.193', 'grad_norm': '27', 'counters/examples': 34880, 'counters/updates': 1090}
train stats after 34912 examples: {'rewards_train/chosen': '0.022043', 'rewards_train/rejected': '0.016838', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0052049', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-156.15', 'loss/train': '0.70022', 'examples_per_second': '30.583', 'grad_norm': '34', 'counters/examples': 34912, 'counters/updates': 1091}
train stats after 34944 examples: {'rewards_train/chosen': '0.075594', 'rewards_train/rejected': '0.040369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035225', 'logps_train/rejected': '-119.3', 'logps_train/chosen': '-146.26', 'loss/train': '0.68644', 'examples_per_second': '30.577', 'grad_norm': '34.75', 'counters/examples': 34944, 'counters/updates': 1092}
train stats after 34976 examples: {'rewards_train/chosen': '0.010341', 'rewards_train/rejected': '-0.02209', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032432', 'logps_train/rejected': '-102.44', 'logps_train/chosen': '-149.71', 'loss/train': '0.68162', 'examples_per_second': '32.091', 'grad_norm': '27.625', 'counters/examples': 34976, 'counters/updates': 1093}
train stats after 35008 examples: {'rewards_train/chosen': '0.016465', 'rewards_train/rejected': '0.001297', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015168', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-128.37', 'loss/train': '0.69049', 'examples_per_second': '31.706', 'grad_norm': '40.5', 'counters/examples': 35008, 'counters/updates': 1094}
train stats after 35040 examples: {'rewards_train/chosen': '0.024531', 'rewards_train/rejected': '-0.025801', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050331', 'logps_train/rejected': '-143.28', 'logps_train/chosen': '-127.99', 'loss/train': '0.67388', 'examples_per_second': '31.549', 'grad_norm': '30.5', 'counters/examples': 35040, 'counters/updates': 1095}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35104 examples: {'rewards_train/chosen': '0.038512', 'rewards_train/rejected': '-0.006038', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04455', 'logps_train/rejected': '-100.29', 'logps_train/chosen': '-139.85', 'loss/train': '0.6759', 'examples_per_second': '31.562', 'grad_norm': '27.125', 'counters/examples': 35104, 'counters/updates': 1097}
train stats after 35136 examples: {'rewards_train/chosen': '-0.025599', 'rewards_train/rejected': '-0.0146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010999', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-115.7', 'loss/train': '0.70387', 'examples_per_second': '32.524', 'grad_norm': '28.25', 'counters/examples': 35136, 'counters/updates': 1098}
train stats after 35168 examples: {'rewards_train/chosen': '0.0075963', 'rewards_train/rejected': '0.035405', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027808', 'logps_train/rejected': '-158.23', 'logps_train/chosen': '-138.67', 'loss/train': '0.71468', 'examples_per_second': '30.604', 'grad_norm': '32', 'counters/examples': 35168, 'counters/updates': 1099}
train stats after 35200 examples: {'rewards_train/chosen': '0.061622', 'rewards_train/rejected': '-0.012951', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.074573', 'logps_train/rejected': '-129.73', 'logps_train/chosen': '-160.72', 'loss/train': '0.661', 'examples_per_second': '31.561', 'grad_norm': '28.125', 'counters/examples': 35200, 'counters/updates': 1100}
train stats after 35232 examples: {'rewards_train/chosen': '-0.0043446', 'rewards_train/rejected': '0.0017013', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.006046', 'logps_train/rejected': '-119.14', 'logps_train/chosen': '-157.13', 'loss/train': '0.7001', 'examples_per_second': '31.537', 'grad_norm': '31.75', 'counters/examples': 35232, 'counters/updates': 1101}
train stats after 35264 examples: {'rewards_train/chosen': '-0.027621', 'rewards_train/rejected': '-0.0030003', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.024621', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-107.47', 'loss/train': '0.70881', 'examples_per_second': '31.215', 'grad_norm': '25.375', 'counters/examples': 35264, 'counters/updates': 1102}
train stats after 35296 examples: {'rewards_train/chosen': '0.0463', 'rewards_train/rejected': '0.052543', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0062438', 'logps_train/rejected': '-94.996', 'logps_train/chosen': '-118.03', 'loss/train': '0.70587', 'examples_per_second': '31.71', 'grad_norm': '27.875', 'counters/examples': 35296, 'counters/updates': 1103}
train stats after 35328 examples: {'rewards_train/chosen': '0.024693', 'rewards_train/rejected': '0.016624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.008069', 'logps_train/rejected': '-125', 'logps_train/chosen': '-136.45', 'loss/train': '0.69269', 'examples_per_second': '30.284', 'grad_norm': '27', 'counters/examples': 35328, 'counters/updates': 1104}
train stats after 35360 examples: {'rewards_train/chosen': '-0.0053135', 'rewards_train/rejected': '-0.0056556', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0003421', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-154.56', 'loss/train': '0.69963', 'examples_per_second': '31.548', 'grad_norm': '29.625', 'counters/examples': 35360, 'counters/updates': 1105}
train stats after 35392 examples: {'rewards_train/chosen': '0.028788', 'rewards_train/rejected': '-0.01367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042458', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-152.67', 'loss/train': '0.68154', 'examples_per_second': '30.096', 'grad_norm': '34.5', 'counters/examples': 35392, 'counters/updates': 1106}
train stats after 35424 examples: {'rewards_train/chosen': '0.024234', 'rewards_train/rejected': '0.01424', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0099934', 'logps_train/rejected': '-140.55', 'logps_train/chosen': '-115.95', 'loss/train': '0.69166', 'examples_per_second': '31.545', 'grad_norm': '27.75', 'counters/examples': 35424, 'counters/updates': 1107}
train stats after 35456 examples: {'rewards_train/chosen': '0.036218', 'rewards_train/rejected': '0.040475', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0042566', 'logps_train/rejected': '-143.29', 'logps_train/chosen': '-133.14', 'loss/train': '0.7036', 'examples_per_second': '31.026', 'grad_norm': '30.875', 'counters/examples': 35456, 'counters/updates': 1108}
train stats after 35488 examples: {'rewards_train/chosen': '0.0060966', 'rewards_train/rejected': '-0.018866', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024963', 'logps_train/rejected': '-104.21', 'logps_train/chosen': '-127.02', 'loss/train': '0.68321', 'examples_per_second': '24.319', 'grad_norm': '36.5', 'counters/examples': 35488, 'counters/updates': 1109}
train stats after 35520 examples: {'rewards_train/chosen': '0.057593', 'rewards_train/rejected': '-0.067955', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12555', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-174.12', 'loss/train': '0.63596', 'examples_per_second': '31.511', 'grad_norm': '27.25', 'counters/examples': 35520, 'counters/updates': 1110}
train stats after 35552 examples: {'rewards_train/chosen': '0.01042', 'rewards_train/rejected': '0.0034322', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0069878', 'logps_train/rejected': '-144.87', 'logps_train/chosen': '-125.97', 'loss/train': '0.69798', 'examples_per_second': '31.578', 'grad_norm': '31.75', 'counters/examples': 35552, 'counters/updates': 1111}
train stats after 35584 examples: {'rewards_train/chosen': '-0.0034631', 'rewards_train/rejected': '-0.0063365', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0028733', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-123.86', 'loss/train': '0.69702', 'examples_per_second': '24.338', 'grad_norm': '24.625', 'counters/examples': 35584, 'counters/updates': 1112}
skipping logging after 35616 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '-0.0058267', 'rewards_train/rejected': '0.036518', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042345', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-155.06', 'loss/train': '0.72003', 'examples_per_second': '31.595', 'grad_norm': '31.875', 'counters/examples': 35648, 'counters/updates': 1114}
train stats after 35680 examples: {'rewards_train/chosen': '0.026566', 'rewards_train/rejected': '0.028081', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.001515', 'logps_train/rejected': '-133.18', 'logps_train/chosen': '-151.66', 'loss/train': '0.69817', 'examples_per_second': '31.975', 'grad_norm': '29.125', 'counters/examples': 35680, 'counters/updates': 1115}
train stats after 35712 examples: {'rewards_train/chosen': '-0.0070127', 'rewards_train/rejected': '-0.035041', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028028', 'logps_train/rejected': '-150.1', 'logps_train/chosen': '-128.43', 'loss/train': '0.68881', 'examples_per_second': '30.728', 'grad_norm': '32.5', 'counters/examples': 35712, 'counters/updates': 1116}
train stats after 35744 examples: {'rewards_train/chosen': '-0.011934', 'rewards_train/rejected': '-0.020528', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0085943', 'logps_train/rejected': '-168.97', 'logps_train/chosen': '-154.38', 'loss/train': '0.69954', 'examples_per_second': '32.398', 'grad_norm': '32.25', 'counters/examples': 35744, 'counters/updates': 1117}
train stats after 35776 examples: {'rewards_train/chosen': '0.071645', 'rewards_train/rejected': '0.044059', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027585', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-140.86', 'loss/train': '0.68586', 'examples_per_second': '30.579', 'grad_norm': '29.375', 'counters/examples': 35776, 'counters/updates': 1118}
train stats after 35808 examples: {'rewards_train/chosen': '-0.014433', 'rewards_train/rejected': '0.063246', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077679', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-159.81', 'loss/train': '0.73824', 'examples_per_second': '31.639', 'grad_norm': '31.75', 'counters/examples': 35808, 'counters/updates': 1119}
train stats after 35840 examples: {'rewards_train/chosen': '0.0062906', 'rewards_train/rejected': '0.032924', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.026633', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-123.49', 'loss/train': '0.7137', 'examples_per_second': '31.636', 'grad_norm': '34', 'counters/examples': 35840, 'counters/updates': 1120}
train stats after 35872 examples: {'rewards_train/chosen': '0.068975', 'rewards_train/rejected': '0.052581', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016394', 'logps_train/rejected': '-146.2', 'logps_train/chosen': '-141.75', 'loss/train': '0.69081', 'examples_per_second': '33.102', 'grad_norm': '32.25', 'counters/examples': 35872, 'counters/updates': 1121}
train stats after 35904 examples: {'rewards_train/chosen': '0.058635', 'rewards_train/rejected': '-0.042077', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10071', 'logps_train/rejected': '-105.47', 'logps_train/chosen': '-146.73', 'loss/train': '0.64708', 'examples_per_second': '32.09', 'grad_norm': '25.375', 'counters/examples': 35904, 'counters/updates': 1122}
train stats after 35936 examples: {'rewards_train/chosen': '0.05209', 'rewards_train/rejected': '0.00014036', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05195', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-129.57', 'loss/train': '0.67242', 'examples_per_second': '30.277', 'grad_norm': '23.75', 'counters/examples': 35936, 'counters/updates': 1123}
train stats after 35968 examples: {'rewards_train/chosen': '0.006153', 'rewards_train/rejected': '-0.0050746', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011228', 'logps_train/rejected': '-101.08', 'logps_train/chosen': '-116.87', 'loss/train': '0.7021', 'examples_per_second': '30.912', 'grad_norm': '27.125', 'counters/examples': 35968, 'counters/updates': 1124}
train stats after 36000 examples: {'rewards_train/chosen': '0.0061882', 'rewards_train/rejected': '0.026163', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019975', 'logps_train/rejected': '-159.49', 'logps_train/chosen': '-150.9', 'loss/train': '0.70592', 'examples_per_second': '31.469', 'grad_norm': '30.875', 'counters/examples': 36000, 'counters/updates': 1125}
Running evaluation after 36000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 36000: {'rewards_eval/chosen': '0.027202', 'rewards_eval/rejected': '0.0062939', 'rewards_eval/accuracies': '0.52734', 'rewards_eval/margins': '0.020908', 'logps_eval/rejected': '-118.55', 'logps_eval/chosen': '-139.16', 'loss/eval': '0.68936'}
skipping save for non epoch
train stats after 36032 examples: {'rewards_train/chosen': '0.040279', 'rewards_train/rejected': '0.019519', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02076', 'logps_train/rejected': '-98.978', 'logps_train/chosen': '-163.69', 'loss/train': '0.68923', 'examples_per_second': '33.333', 'grad_norm': '29.5', 'counters/examples': 36032, 'counters/updates': 1126}
train stats after 36064 examples: {'rewards_train/chosen': '0.058305', 'rewards_train/rejected': '0.027823', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030482', 'logps_train/rejected': '-163.1', 'logps_train/chosen': '-136.19', 'loss/train': '0.68156', 'examples_per_second': '31.431', 'grad_norm': '32.5', 'counters/examples': 36064, 'counters/updates': 1127}
train stats after 36096 examples: {'rewards_train/chosen': '-0.0056701', 'rewards_train/rejected': '-0.017187', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011517', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-119.62', 'loss/train': '0.6927', 'examples_per_second': '31.573', 'grad_norm': '34.5', 'counters/examples': 36096, 'counters/updates': 1128}
train stats after 36128 examples: {'rewards_train/chosen': '-0.0019542', 'rewards_train/rejected': '-0.021055', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019101', 'logps_train/rejected': '-113.02', 'logps_train/chosen': '-142.84', 'loss/train': '0.69015', 'examples_per_second': '30.672', 'grad_norm': '26.625', 'counters/examples': 36128, 'counters/updates': 1129}
train stats after 36160 examples: {'rewards_train/chosen': '-0.060931', 'rewards_train/rejected': '0.0020622', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.062993', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-135.38', 'loss/train': '0.73185', 'examples_per_second': '30.821', 'grad_norm': '26.25', 'counters/examples': 36160, 'counters/updates': 1130}
train stats after 36192 examples: {'rewards_train/chosen': '0.061993', 'rewards_train/rejected': '0.007556', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054437', 'logps_train/rejected': '-140.78', 'logps_train/chosen': '-145.83', 'loss/train': '0.67229', 'examples_per_second': '32.009', 'grad_norm': '30.5', 'counters/examples': 36192, 'counters/updates': 1131}
train stats after 36224 examples: {'rewards_train/chosen': '0.040417', 'rewards_train/rejected': '0.0066817', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033736', 'logps_train/rejected': '-142.04', 'logps_train/chosen': '-146.52', 'loss/train': '0.68191', 'examples_per_second': '31.225', 'grad_norm': '31.5', 'counters/examples': 36224, 'counters/updates': 1132}
train stats after 36256 examples: {'rewards_train/chosen': '0.070234', 'rewards_train/rejected': '0.024015', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046219', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-163.19', 'loss/train': '0.67456', 'examples_per_second': '31.163', 'grad_norm': '27.875', 'counters/examples': 36256, 'counters/updates': 1133}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36320 examples: {'rewards_train/chosen': '0.024119', 'rewards_train/rejected': '-0.058155', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082274', 'logps_train/rejected': '-154.24', 'logps_train/chosen': '-170.95', 'loss/train': '0.65687', 'examples_per_second': '32.413', 'grad_norm': '33.75', 'counters/examples': 36320, 'counters/updates': 1135}
train stats after 36352 examples: {'rewards_train/chosen': '-0.022555', 'rewards_train/rejected': '-0.0013165', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021239', 'logps_train/rejected': '-92.9', 'logps_train/chosen': '-130.55', 'loss/train': '0.70956', 'examples_per_second': '25.264', 'grad_norm': '26.875', 'counters/examples': 36352, 'counters/updates': 1136}
skipping logging after 36384 examples to avoid logging too frequently
train stats after 36416 examples: {'rewards_train/chosen': '-0.00096357', 'rewards_train/rejected': '-0.025845', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024882', 'logps_train/rejected': '-101.6', 'logps_train/chosen': '-140.02', 'loss/train': '0.68758', 'examples_per_second': '30.039', 'grad_norm': '29.375', 'counters/examples': 36416, 'counters/updates': 1138}
train stats after 36448 examples: {'rewards_train/chosen': '0.059837', 'rewards_train/rejected': '0.0016689', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058168', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-149.96', 'loss/train': '0.66803', 'examples_per_second': '32.33', 'grad_norm': '26.375', 'counters/examples': 36448, 'counters/updates': 1139}
train stats after 36480 examples: {'rewards_train/chosen': '0.033177', 'rewards_train/rejected': '-0.024428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057604', 'logps_train/rejected': '-93.016', 'logps_train/chosen': '-126.58', 'loss/train': '0.67033', 'examples_per_second': '32.073', 'grad_norm': '22.75', 'counters/examples': 36480, 'counters/updates': 1140}
train stats after 36512 examples: {'rewards_train/chosen': '0.045123', 'rewards_train/rejected': '0.048382', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0032583', 'logps_train/rejected': '-114.56', 'logps_train/chosen': '-119.57', 'loss/train': '0.70455', 'examples_per_second': '32.677', 'grad_norm': '29.5', 'counters/examples': 36512, 'counters/updates': 1141}
train stats after 36544 examples: {'rewards_train/chosen': '0.028487', 'rewards_train/rejected': '0.0057601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022727', 'logps_train/rejected': '-145.7', 'logps_train/chosen': '-171.51', 'loss/train': '0.68567', 'examples_per_second': '30.415', 'grad_norm': '33', 'counters/examples': 36544, 'counters/updates': 1142}
skipping logging after 36576 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '0.019091', 'rewards_train/rejected': '0.01017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0089204', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-118.19', 'loss/train': '0.69255', 'examples_per_second': '30.496', 'grad_norm': '32.75', 'counters/examples': 36608, 'counters/updates': 1144}
train stats after 36640 examples: {'rewards_train/chosen': '0.036787', 'rewards_train/rejected': '-0.016261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053047', 'logps_train/rejected': '-140.5', 'logps_train/chosen': '-115.62', 'loss/train': '0.6709', 'examples_per_second': '31.462', 'grad_norm': '31.5', 'counters/examples': 36640, 'counters/updates': 1145}
train stats after 36672 examples: {'rewards_train/chosen': '0.047631', 'rewards_train/rejected': '-0.00027499', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047906', 'logps_train/rejected': '-120.55', 'logps_train/chosen': '-162.6', 'loss/train': '0.67607', 'examples_per_second': '30.944', 'grad_norm': '31.25', 'counters/examples': 36672, 'counters/updates': 1146}
train stats after 36704 examples: {'rewards_train/chosen': '0.014503', 'rewards_train/rejected': '0.0061517', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0083516', 'logps_train/rejected': '-72.911', 'logps_train/chosen': '-158.05', 'loss/train': '0.69216', 'examples_per_second': '30.553', 'grad_norm': '25.5', 'counters/examples': 36704, 'counters/updates': 1147}
train stats after 36736 examples: {'rewards_train/chosen': '0.013165', 'rewards_train/rejected': '0.0053451', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0078203', 'logps_train/rejected': '-118.25', 'logps_train/chosen': '-140.74', 'loss/train': '0.69296', 'examples_per_second': '31.549', 'grad_norm': '27.5', 'counters/examples': 36736, 'counters/updates': 1148}
skipping logging after 36768 examples to avoid logging too frequently
train stats after 36800 examples: {'rewards_train/chosen': '0.042337', 'rewards_train/rejected': '-0.059442', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10178', 'logps_train/rejected': '-90.256', 'logps_train/chosen': '-153.3', 'loss/train': '0.64995', 'examples_per_second': '30.914', 'grad_norm': '25.5', 'counters/examples': 36800, 'counters/updates': 1150}
skipping logging after 36832 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '0.036688', 'rewards_train/rejected': '-0.018428', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055116', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-163.19', 'loss/train': '0.67175', 'examples_per_second': '30.47', 'grad_norm': '53', 'counters/examples': 36864, 'counters/updates': 1152}
train stats after 36896 examples: {'rewards_train/chosen': '0.012177', 'rewards_train/rejected': '0.031331', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019154', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-131.13', 'loss/train': '0.71315', 'examples_per_second': '30.862', 'grad_norm': '30', 'counters/examples': 36896, 'counters/updates': 1153}
train stats after 36928 examples: {'rewards_train/chosen': '-0.013646', 'rewards_train/rejected': '-0.029311', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015665', 'logps_train/rejected': '-119.1', 'logps_train/chosen': '-139.58', 'loss/train': '0.69036', 'examples_per_second': '31.159', 'grad_norm': '25.25', 'counters/examples': 36928, 'counters/updates': 1154}
train stats after 36960 examples: {'rewards_train/chosen': '0.043962', 'rewards_train/rejected': '3.168e-05', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04393', 'logps_train/rejected': '-147.28', 'logps_train/chosen': '-110.8', 'loss/train': '0.675', 'examples_per_second': '31.785', 'grad_norm': '27.25', 'counters/examples': 36960, 'counters/updates': 1155}
train stats after 36992 examples: {'rewards_train/chosen': '0.012722', 'rewards_train/rejected': '0.023474', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010751', 'logps_train/rejected': '-138.52', 'logps_train/chosen': '-155.81', 'loss/train': '0.70694', 'examples_per_second': '31.62', 'grad_norm': '30.375', 'counters/examples': 36992, 'counters/updates': 1156}
train stats after 37024 examples: {'rewards_train/chosen': '0.080173', 'rewards_train/rejected': '-0.054867', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13504', 'logps_train/rejected': '-153.77', 'logps_train/chosen': '-171', 'loss/train': '0.6372', 'examples_per_second': '31.72', 'grad_norm': '31.375', 'counters/examples': 37024, 'counters/updates': 1157}
train stats after 37056 examples: {'rewards_train/chosen': '0.0234', 'rewards_train/rejected': '0.016688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.006712', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-139.26', 'loss/train': '0.69468', 'examples_per_second': '31.599', 'grad_norm': '30.5', 'counters/examples': 37056, 'counters/updates': 1158}
skipping logging after 37088 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '-0.010386', 'rewards_train/rejected': '-0.040943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030558', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-159.48', 'loss/train': '0.68791', 'examples_per_second': '36.498', 'grad_norm': '28', 'counters/examples': 37120, 'counters/updates': 1160}
skipping logging after 37152 examples to avoid logging too frequently
train stats after 37184 examples: {'rewards_train/chosen': '-0.0036405', 'rewards_train/rejected': '-0.022157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.018517', 'logps_train/rejected': '-99.652', 'logps_train/chosen': '-171.69', 'loss/train': '0.69135', 'examples_per_second': '31.11', 'grad_norm': '28.375', 'counters/examples': 37184, 'counters/updates': 1162}
train stats after 37216 examples: {'rewards_train/chosen': '0.0099788', 'rewards_train/rejected': '-0.0089873', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018966', 'logps_train/rejected': '-83.46', 'logps_train/chosen': '-107.61', 'loss/train': '0.68732', 'examples_per_second': '31.602', 'grad_norm': '27.75', 'counters/examples': 37216, 'counters/updates': 1163}
skipping logging after 37248 examples to avoid logging too frequently
train stats after 37280 examples: {'rewards_train/chosen': '0.055805', 'rewards_train/rejected': '-0.0098084', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065614', 'logps_train/rejected': '-92.525', 'logps_train/chosen': '-166.22', 'loss/train': '0.66684', 'examples_per_second': '30.727', 'grad_norm': '30.625', 'counters/examples': 37280, 'counters/updates': 1165}
train stats after 37312 examples: {'rewards_train/chosen': '0.034966', 'rewards_train/rejected': '0.026655', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0083102', 'logps_train/rejected': '-169.16', 'logps_train/chosen': '-168.49', 'loss/train': '0.69454', 'examples_per_second': '31.573', 'grad_norm': '30.875', 'counters/examples': 37312, 'counters/updates': 1166}
skipping logging after 37344 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '-0.041679', 'rewards_train/rejected': '0.013621', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0553', 'logps_train/rejected': '-104.08', 'logps_train/chosen': '-153.19', 'loss/train': '0.72646', 'examples_per_second': '30.911', 'grad_norm': '32', 'counters/examples': 37376, 'counters/updates': 1168}
train stats after 37408 examples: {'rewards_train/chosen': '0.089086', 'rewards_train/rejected': '0.028212', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060874', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-161.48', 'loss/train': '0.66787', 'examples_per_second': '30.008', 'grad_norm': '27.625', 'counters/examples': 37408, 'counters/updates': 1169}
train stats after 37440 examples: {'rewards_train/chosen': '0.0088085', 'rewards_train/rejected': '0.014272', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0054631', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-170.49', 'loss/train': '0.70151', 'examples_per_second': '31.257', 'grad_norm': '31.5', 'counters/examples': 37440, 'counters/updates': 1170}
train stats after 37472 examples: {'rewards_train/chosen': '0.026426', 'rewards_train/rejected': '0.067078', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040652', 'logps_train/rejected': '-139.22', 'logps_train/chosen': '-139.65', 'loss/train': '0.71959', 'examples_per_second': '30.448', 'grad_norm': '32.5', 'counters/examples': 37472, 'counters/updates': 1171}
skipping logging after 37504 examples to avoid logging too frequently
train stats after 37536 examples: {'rewards_train/chosen': '0.017559', 'rewards_train/rejected': '-0.030707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048266', 'logps_train/rejected': '-148.69', 'logps_train/chosen': '-143.96', 'loss/train': '0.67758', 'examples_per_second': '32.482', 'grad_norm': '36.25', 'counters/examples': 37536, 'counters/updates': 1173}
train stats after 37568 examples: {'rewards_train/chosen': '0.046063', 'rewards_train/rejected': '0.02876', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.017303', 'logps_train/rejected': '-134.01', 'logps_train/chosen': '-106.79', 'loss/train': '0.6954', 'examples_per_second': '30.346', 'grad_norm': '26.25', 'counters/examples': 37568, 'counters/updates': 1174}
train stats after 37600 examples: {'rewards_train/chosen': '0.0056215', 'rewards_train/rejected': '-0.0061857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.011807', 'logps_train/rejected': '-121.88', 'logps_train/chosen': '-173.98', 'loss/train': '0.69452', 'examples_per_second': '30.23', 'grad_norm': '31', 'counters/examples': 37600, 'counters/updates': 1175}
train stats after 37632 examples: {'rewards_train/chosen': '-0.00040577', 'rewards_train/rejected': '0.018578', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018984', 'logps_train/rejected': '-109.73', 'logps_train/chosen': '-114.41', 'loss/train': '0.70635', 'examples_per_second': '30.426', 'grad_norm': '24.875', 'counters/examples': 37632, 'counters/updates': 1176}
skipping logging after 37664 examples to avoid logging too frequently
train stats after 37696 examples: {'rewards_train/chosen': '0.021689', 'rewards_train/rejected': '-0.021784', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043473', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-167.52', 'loss/train': '0.67434', 'examples_per_second': '34.254', 'grad_norm': '29.25', 'counters/examples': 37696, 'counters/updates': 1178}
train stats after 37728 examples: {'rewards_train/chosen': '0.036804', 'rewards_train/rejected': '-0.0041402', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040944', 'logps_train/rejected': '-112.48', 'logps_train/chosen': '-113.59', 'loss/train': '0.67656', 'examples_per_second': '33.09', 'grad_norm': '27.625', 'counters/examples': 37728, 'counters/updates': 1179}
train stats after 37760 examples: {'rewards_train/chosen': '0.017742', 'rewards_train/rejected': '-0.0438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061542', 'logps_train/rejected': '-112.1', 'logps_train/chosen': '-188.19', 'loss/train': '0.6672', 'examples_per_second': '31.539', 'grad_norm': '29.875', 'counters/examples': 37760, 'counters/updates': 1180}
train stats after 37792 examples: {'rewards_train/chosen': '-0.021343', 'rewards_train/rejected': '-0.014193', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0071497', 'logps_train/rejected': '-150.63', 'logps_train/chosen': '-118.61', 'loss/train': '0.70548', 'examples_per_second': '31.571', 'grad_norm': '31.75', 'counters/examples': 37792, 'counters/updates': 1181}
train stats after 37824 examples: {'rewards_train/chosen': '0.074', 'rewards_train/rejected': '-0.015801', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089801', 'logps_train/rejected': '-161.75', 'logps_train/chosen': '-163.69', 'loss/train': '0.65597', 'examples_per_second': '31.519', 'grad_norm': '29.5', 'counters/examples': 37824, 'counters/updates': 1182}
train stats after 37856 examples: {'rewards_train/chosen': '0.011311', 'rewards_train/rejected': '-0.028406', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039717', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-182.35', 'loss/train': '0.68087', 'examples_per_second': '31.546', 'grad_norm': '31.125', 'counters/examples': 37856, 'counters/updates': 1183}
skipping logging after 37888 examples to avoid logging too frequently
train stats after 37920 examples: {'rewards_train/chosen': '-0.021245', 'rewards_train/rejected': '-0.05785', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036605', 'logps_train/rejected': '-148.51', 'logps_train/chosen': '-142.56', 'loss/train': '0.68338', 'examples_per_second': '32.187', 'grad_norm': '30.875', 'counters/examples': 37920, 'counters/updates': 1185}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 37984 examples: {'rewards_train/chosen': '0.018218', 'rewards_train/rejected': '0.011502', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.006715', 'logps_train/rejected': '-133.12', 'logps_train/chosen': '-143.89', 'loss/train': '0.69553', 'examples_per_second': '31.543', 'grad_norm': '32.25', 'counters/examples': 37984, 'counters/updates': 1187}
train stats after 38016 examples: {'rewards_train/chosen': '0.023897', 'rewards_train/rejected': '-0.036071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059968', 'logps_train/rejected': '-119.65', 'logps_train/chosen': '-171.33', 'loss/train': '0.6693', 'examples_per_second': '31.796', 'grad_norm': '26.625', 'counters/examples': 38016, 'counters/updates': 1188}
train stats after 38048 examples: {'rewards_train/chosen': '0.029305', 'rewards_train/rejected': '-0.0073744', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036679', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-128.72', 'loss/train': '0.67915', 'examples_per_second': '31.5', 'grad_norm': '29.125', 'counters/examples': 38048, 'counters/updates': 1189}
train stats after 38080 examples: {'rewards_train/chosen': '0.025232', 'rewards_train/rejected': '0.016143', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0090889', 'logps_train/rejected': '-86.95', 'logps_train/chosen': '-154.6', 'loss/train': '0.69124', 'examples_per_second': '30.146', 'grad_norm': '27.5', 'counters/examples': 38080, 'counters/updates': 1190}
train stats after 38112 examples: {'rewards_train/chosen': '0.040943', 'rewards_train/rejected': '-0.02811', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069053', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-145.49', 'loss/train': '0.66297', 'examples_per_second': '32.07', 'grad_norm': '24.875', 'counters/examples': 38112, 'counters/updates': 1191}
train stats after 38144 examples: {'rewards_train/chosen': '-0.010689', 'rewards_train/rejected': '0.031366', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.042056', 'logps_train/rejected': '-123.44', 'logps_train/chosen': '-121.79', 'loss/train': '0.7202', 'examples_per_second': '31.379', 'grad_norm': '27.125', 'counters/examples': 38144, 'counters/updates': 1192}
train stats after 38176 examples: {'rewards_train/chosen': '0.038152', 'rewards_train/rejected': '0.016809', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021343', 'logps_train/rejected': '-103.21', 'logps_train/chosen': '-122.79', 'loss/train': '0.68529', 'examples_per_second': '32.14', 'grad_norm': '23.5', 'counters/examples': 38176, 'counters/updates': 1193}
train stats after 38208 examples: {'rewards_train/chosen': '-0.03062', 'rewards_train/rejected': '-0.020134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010486', 'logps_train/rejected': '-104.9', 'logps_train/chosen': '-175.4', 'loss/train': '0.70429', 'examples_per_second': '31.8', 'grad_norm': '29', 'counters/examples': 38208, 'counters/updates': 1194}
train stats after 38240 examples: {'rewards_train/chosen': '-0.032806', 'rewards_train/rejected': '-0.0093337', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023473', 'logps_train/rejected': '-105.89', 'logps_train/chosen': '-135.08', 'loss/train': '0.70881', 'examples_per_second': '30.146', 'grad_norm': '27.25', 'counters/examples': 38240, 'counters/updates': 1195}
train stats after 38272 examples: {'rewards_train/chosen': '0.043253', 'rewards_train/rejected': '0.047306', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0040531', 'logps_train/rejected': '-133.25', 'logps_train/chosen': '-153.04', 'loss/train': '0.70323', 'examples_per_second': '30.53', 'grad_norm': '31.875', 'counters/examples': 38272, 'counters/updates': 1196}
train stats after 38304 examples: {'rewards_train/chosen': '0.033534', 'rewards_train/rejected': '0.037419', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0038851', 'logps_train/rejected': '-160.07', 'logps_train/chosen': '-151.08', 'loss/train': '0.70153', 'examples_per_second': '31.483', 'grad_norm': '30.5', 'counters/examples': 38304, 'counters/updates': 1197}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38368 examples: {'rewards_train/chosen': '0.061062', 'rewards_train/rejected': '0.0062308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054831', 'logps_train/rejected': '-133.5', 'logps_train/chosen': '-159.4', 'loss/train': '0.67442', 'examples_per_second': '31.551', 'grad_norm': '35', 'counters/examples': 38368, 'counters/updates': 1199}
train stats after 38400 examples: {'rewards_train/chosen': '0.001209', 'rewards_train/rejected': '-0.023054', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024263', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-143.02', 'loss/train': '0.68709', 'examples_per_second': '31.968', 'grad_norm': '26.75', 'counters/examples': 38400, 'counters/updates': 1200}
train stats after 38432 examples: {'rewards_train/chosen': '-0.048256', 'rewards_train/rejected': '-0.0058361', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04242', 'logps_train/rejected': '-142.39', 'logps_train/chosen': '-156.67', 'loss/train': '0.72343', 'examples_per_second': '31.432', 'grad_norm': '32.5', 'counters/examples': 38432, 'counters/updates': 1201}
train stats after 38464 examples: {'rewards_train/chosen': '0.017993', 'rewards_train/rejected': '0.032651', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014657', 'logps_train/rejected': '-99.24', 'logps_train/chosen': '-170.35', 'loss/train': '0.70614', 'examples_per_second': '31.615', 'grad_norm': '29.375', 'counters/examples': 38464, 'counters/updates': 1202}
train stats after 38496 examples: {'rewards_train/chosen': '0.030456', 'rewards_train/rejected': '0.019479', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010977', 'logps_train/rejected': '-162.2', 'logps_train/chosen': '-163.75', 'loss/train': '0.69559', 'examples_per_second': '30.591', 'grad_norm': '32.25', 'counters/examples': 38496, 'counters/updates': 1203}
train stats after 38528 examples: {'rewards_train/chosen': '0.0586', 'rewards_train/rejected': '0.028373', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.030227', 'logps_train/rejected': '-104.08', 'logps_train/chosen': '-134.58', 'loss/train': '0.68437', 'examples_per_second': '30.898', 'grad_norm': '26.375', 'counters/examples': 38528, 'counters/updates': 1204}
train stats after 38560 examples: {'rewards_train/chosen': '0.018531', 'rewards_train/rejected': '-0.011173', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029704', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-113.52', 'loss/train': '0.68376', 'examples_per_second': '30.886', 'grad_norm': '25.125', 'counters/examples': 38560, 'counters/updates': 1205}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38624 examples: {'rewards_train/chosen': '0.029245', 'rewards_train/rejected': '-0.0029137', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032159', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-136.04', 'loss/train': '0.68071', 'examples_per_second': '30.229', 'grad_norm': '28', 'counters/examples': 38624, 'counters/updates': 1207}
train stats after 38656 examples: {'rewards_train/chosen': '0.033318', 'rewards_train/rejected': '0.0018348', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031483', 'logps_train/rejected': '-103', 'logps_train/chosen': '-155.31', 'loss/train': '0.68294', 'examples_per_second': '31.755', 'grad_norm': '28.625', 'counters/examples': 38656, 'counters/updates': 1208}
train stats after 38688 examples: {'rewards_train/chosen': '0.0019066', 'rewards_train/rejected': '0.041784', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.039877', 'logps_train/rejected': '-107.63', 'logps_train/chosen': '-95.714', 'loss/train': '0.72022', 'examples_per_second': '32.913', 'grad_norm': '28.75', 'counters/examples': 38688, 'counters/updates': 1209}
train stats after 38720 examples: {'rewards_train/chosen': '0.01837', 'rewards_train/rejected': '0.0068434', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011527', 'logps_train/rejected': '-142.45', 'logps_train/chosen': '-160.14', 'loss/train': '0.69213', 'examples_per_second': '30.728', 'grad_norm': '41', 'counters/examples': 38720, 'counters/updates': 1210}
train stats after 38752 examples: {'rewards_train/chosen': '-0.041188', 'rewards_train/rejected': '0.024974', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.066162', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-154.27', 'loss/train': '0.73898', 'examples_per_second': '30.475', 'grad_norm': '35.5', 'counters/examples': 38752, 'counters/updates': 1211}
train stats after 38784 examples: {'rewards_train/chosen': '0.014882', 'rewards_train/rejected': '0.014088', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0007939', 'logps_train/rejected': '-150.79', 'logps_train/chosen': '-150.61', 'loss/train': '0.69708', 'examples_per_second': '30.055', 'grad_norm': '27.625', 'counters/examples': 38784, 'counters/updates': 1212}
train stats after 38816 examples: {'rewards_train/chosen': '0.045618', 'rewards_train/rejected': '0.022657', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02296', 'logps_train/rejected': '-122.86', 'logps_train/chosen': '-170.42', 'loss/train': '0.68713', 'examples_per_second': '31.511', 'grad_norm': '28.375', 'counters/examples': 38816, 'counters/updates': 1213}
train stats after 38848 examples: {'rewards_train/chosen': '0.0019223', 'rewards_train/rejected': '0.0065711', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0046488', 'logps_train/rejected': '-162.84', 'logps_train/chosen': '-178.42', 'loss/train': '0.7113', 'examples_per_second': '30.508', 'grad_norm': '133', 'counters/examples': 38848, 'counters/updates': 1214}
train stats after 38880 examples: {'rewards_train/chosen': '0.041922', 'rewards_train/rejected': '-0.0047343', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046656', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-157.19', 'loss/train': '0.67319', 'examples_per_second': '31.578', 'grad_norm': '34', 'counters/examples': 38880, 'counters/updates': 1215}
train stats after 38912 examples: {'rewards_train/chosen': '0.0098309', 'rewards_train/rejected': '-0.019411', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029242', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-122.21', 'loss/train': '0.68335', 'examples_per_second': '31.526', 'grad_norm': '27.5', 'counters/examples': 38912, 'counters/updates': 1216}
train stats after 38944 examples: {'rewards_train/chosen': '0.015342', 'rewards_train/rejected': '-0.0022616', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017604', 'logps_train/rejected': '-119.18', 'logps_train/chosen': '-154.55', 'loss/train': '0.69022', 'examples_per_second': '30.884', 'grad_norm': '28.5', 'counters/examples': 38944, 'counters/updates': 1217}
train stats after 38976 examples: {'rewards_train/chosen': '0.020514', 'rewards_train/rejected': '0.027988', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0074741', 'logps_train/rejected': '-126.44', 'logps_train/chosen': '-179.91', 'loss/train': '0.70527', 'examples_per_second': '31.509', 'grad_norm': '30.875', 'counters/examples': 38976, 'counters/updates': 1218}
train stats after 39008 examples: {'rewards_train/chosen': '-0.038221', 'rewards_train/rejected': '0.016602', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.054823', 'logps_train/rejected': '-146.6', 'logps_train/chosen': '-134.5', 'loss/train': '0.72618', 'examples_per_second': '32.617', 'grad_norm': '28.25', 'counters/examples': 39008, 'counters/updates': 1219}
train stats after 39040 examples: {'rewards_train/chosen': '-0.01557', 'rewards_train/rejected': '-0.036627', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.021056', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-126.35', 'loss/train': '0.68674', 'examples_per_second': '31.599', 'grad_norm': '34.75', 'counters/examples': 39040, 'counters/updates': 1220}
skipping logging after 39072 examples to avoid logging too frequently
train stats after 39104 examples: {'rewards_train/chosen': '0.040376', 'rewards_train/rejected': '0.006852', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033524', 'logps_train/rejected': '-93.844', 'logps_train/chosen': '-199.38', 'loss/train': '0.68216', 'examples_per_second': '32.595', 'grad_norm': '30', 'counters/examples': 39104, 'counters/updates': 1222}
train stats after 39136 examples: {'rewards_train/chosen': '0.014691', 'rewards_train/rejected': '0.0047055', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0099855', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-153.29', 'loss/train': '0.69393', 'examples_per_second': '31.137', 'grad_norm': '27.375', 'counters/examples': 39136, 'counters/updates': 1223}
skipping logging after 39168 examples to avoid logging too frequently
train stats after 39200 examples: {'rewards_train/chosen': '0.026906', 'rewards_train/rejected': '0.023274', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0036326', 'logps_train/rejected': '-138.83', 'logps_train/chosen': '-133.23', 'loss/train': '0.69599', 'examples_per_second': '36.21', 'grad_norm': '26.375', 'counters/examples': 39200, 'counters/updates': 1225}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39264 examples: {'rewards_train/chosen': '0.030787', 'rewards_train/rejected': '0.014726', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016061', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-138.53', 'loss/train': '0.68926', 'examples_per_second': '31.724', 'grad_norm': '41.25', 'counters/examples': 39264, 'counters/updates': 1227}
skipping logging after 39296 examples to avoid logging too frequently
train stats after 39328 examples: {'rewards_train/chosen': '0.037401', 'rewards_train/rejected': '-0.02003', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057431', 'logps_train/rejected': '-121.81', 'logps_train/chosen': '-146.32', 'loss/train': '0.66926', 'examples_per_second': '32.244', 'grad_norm': '26.5', 'counters/examples': 39328, 'counters/updates': 1229}
train stats after 39360 examples: {'rewards_train/chosen': '0.044631', 'rewards_train/rejected': '0.025125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019506', 'logps_train/rejected': '-144.25', 'logps_train/chosen': '-124.87', 'loss/train': '0.68696', 'examples_per_second': '32.882', 'grad_norm': '25.375', 'counters/examples': 39360, 'counters/updates': 1230}
skipping logging after 39392 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '0.033374', 'rewards_train/rejected': '0.026193', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0071812', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-153.35', 'loss/train': '0.69448', 'examples_per_second': '31.539', 'grad_norm': '27.25', 'counters/examples': 39424, 'counters/updates': 1232}
train stats after 39456 examples: {'rewards_train/chosen': '0.056239', 'rewards_train/rejected': '0.039148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017091', 'logps_train/rejected': '-87.059', 'logps_train/chosen': '-137.41', 'loss/train': '0.68793', 'examples_per_second': '31.525', 'grad_norm': '25.125', 'counters/examples': 39456, 'counters/updates': 1233}
train stats after 39488 examples: {'rewards_train/chosen': '0.07834', 'rewards_train/rejected': '-0.0052763', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083616', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-115.17', 'loss/train': '0.66017', 'examples_per_second': '30.155', 'grad_norm': '26.125', 'counters/examples': 39488, 'counters/updates': 1234}
train stats after 39520 examples: {'rewards_train/chosen': '0.046731', 'rewards_train/rejected': '0.028335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018395', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-144.67', 'loss/train': '0.68947', 'examples_per_second': '30.059', 'grad_norm': '32', 'counters/examples': 39520, 'counters/updates': 1235}
train stats after 39552 examples: {'rewards_train/chosen': '-0.016011', 'rewards_train/rejected': '-0.014544', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0014665', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-108.84', 'loss/train': '0.69712', 'examples_per_second': '30.955', 'grad_norm': '25.125', 'counters/examples': 39552, 'counters/updates': 1236}
train stats after 39584 examples: {'rewards_train/chosen': '0.034671', 'rewards_train/rejected': '0.018126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016545', 'logps_train/rejected': '-123.71', 'logps_train/chosen': '-133.33', 'loss/train': '0.69012', 'examples_per_second': '31.183', 'grad_norm': '30.75', 'counters/examples': 39584, 'counters/updates': 1237}
train stats after 39616 examples: {'rewards_train/chosen': '-0.0059911', 'rewards_train/rejected': '-0.030923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024932', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-139.32', 'loss/train': '0.68425', 'examples_per_second': '33.074', 'grad_norm': '26.5', 'counters/examples': 39616, 'counters/updates': 1238}
train stats after 39648 examples: {'rewards_train/chosen': '0.060521', 'rewards_train/rejected': '0.011944', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048576', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-136.61', 'loss/train': '0.67283', 'examples_per_second': '31.617', 'grad_norm': '26.75', 'counters/examples': 39648, 'counters/updates': 1239}
train stats after 39680 examples: {'rewards_train/chosen': '0.048849', 'rewards_train/rejected': '0.013496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035354', 'logps_train/rejected': '-112.93', 'logps_train/chosen': '-153.53', 'loss/train': '0.68013', 'examples_per_second': '32.495', 'grad_norm': '30.875', 'counters/examples': 39680, 'counters/updates': 1240}
train stats after 39712 examples: {'rewards_train/chosen': '-0.012184', 'rewards_train/rejected': '0.011826', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02401', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-155.61', 'loss/train': '0.71327', 'examples_per_second': '32.502', 'grad_norm': '39.5', 'counters/examples': 39712, 'counters/updates': 1241}
train stats after 39744 examples: {'rewards_train/chosen': '0.074442', 'rewards_train/rejected': '0.024485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049957', 'logps_train/rejected': '-107.82', 'logps_train/chosen': '-166.6', 'loss/train': '0.67722', 'examples_per_second': '31.817', 'grad_norm': '27.375', 'counters/examples': 39744, 'counters/updates': 1242}
train stats after 39776 examples: {'rewards_train/chosen': '-0.02464', 'rewards_train/rejected': '-0.027916', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0032768', 'logps_train/rejected': '-150.94', 'logps_train/chosen': '-155.68', 'loss/train': '0.70555', 'examples_per_second': '32.102', 'grad_norm': '36.75', 'counters/examples': 39776, 'counters/updates': 1243}
train stats after 39808 examples: {'rewards_train/chosen': '0.05418', 'rewards_train/rejected': '-0.015573', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.069753', 'logps_train/rejected': '-98.904', 'logps_train/chosen': '-124.71', 'loss/train': '0.66309', 'examples_per_second': '32.508', 'grad_norm': '24.125', 'counters/examples': 39808, 'counters/updates': 1244}
train stats after 39840 examples: {'rewards_train/chosen': '0.046965', 'rewards_train/rejected': '-0.0084465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055412', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-115.9', 'loss/train': '0.67052', 'examples_per_second': '31.921', 'grad_norm': '25', 'counters/examples': 39840, 'counters/updates': 1245}
train stats after 39872 examples: {'rewards_train/chosen': '-0.0047641', 'rewards_train/rejected': '0.0023394', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0071036', 'logps_train/rejected': '-105.91', 'logps_train/chosen': '-112.69', 'loss/train': '0.70049', 'examples_per_second': '30.281', 'grad_norm': '24', 'counters/examples': 39872, 'counters/updates': 1246}
train stats after 39904 examples: {'rewards_train/chosen': '0.018264', 'rewards_train/rejected': '-0.029909', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048174', 'logps_train/rejected': '-156.22', 'logps_train/chosen': '-174.84', 'loss/train': '0.67483', 'examples_per_second': '31.687', 'grad_norm': '31', 'counters/examples': 39904, 'counters/updates': 1247}
train stats after 39936 examples: {'rewards_train/chosen': '0.022059', 'rewards_train/rejected': '0.001542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020517', 'logps_train/rejected': '-93.293', 'logps_train/chosen': '-143.23', 'loss/train': '0.68576', 'examples_per_second': '30.427', 'grad_norm': '25', 'counters/examples': 39936, 'counters/updates': 1248}
train stats after 39968 examples: {'rewards_train/chosen': '0.026621', 'rewards_train/rejected': '0.064962', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.03834', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-111.27', 'loss/train': '0.71857', 'examples_per_second': '30.614', 'grad_norm': '27.5', 'counters/examples': 39968, 'counters/updates': 1249}
train stats after 40000 examples: {'rewards_train/chosen': '0.10694', 'rewards_train/rejected': '-0.020505', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12745', 'logps_train/rejected': '-144.51', 'logps_train/chosen': '-125.68', 'loss/train': '0.63565', 'examples_per_second': '30.423', 'grad_norm': '26.625', 'counters/examples': 40000, 'counters/updates': 1250}
train stats after 40032 examples: {'rewards_train/chosen': '0.036798', 'rewards_train/rejected': '0.01187', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024928', 'logps_train/rejected': '-111.63', 'logps_train/chosen': '-146.98', 'loss/train': '0.68529', 'examples_per_second': '32.677', 'grad_norm': '25.625', 'counters/examples': 40032, 'counters/updates': 1251}
train stats after 40064 examples: {'rewards_train/chosen': '0.045978', 'rewards_train/rejected': '-0.010121', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056099', 'logps_train/rejected': '-108.23', 'logps_train/chosen': '-145.3', 'loss/train': '0.6694', 'examples_per_second': '30.239', 'grad_norm': '25.875', 'counters/examples': 40064, 'counters/updates': 1252}
skipping logging after 40096 examples to avoid logging too frequently
train stats after 40128 examples: {'rewards_train/chosen': '0.023592', 'rewards_train/rejected': '-0.010729', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034321', 'logps_train/rejected': '-137.51', 'logps_train/chosen': '-157.88', 'loss/train': '0.68553', 'examples_per_second': '31.529', 'grad_norm': '40', 'counters/examples': 40128, 'counters/updates': 1254}
skipping logging after 40160 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '0.031851', 'rewards_train/rejected': '0.048944', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.017093', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-156.33', 'loss/train': '0.70537', 'examples_per_second': '32.468', 'grad_norm': '27.5', 'counters/examples': 40192, 'counters/updates': 1256}
train stats after 40224 examples: {'rewards_train/chosen': '-0.01469', 'rewards_train/rejected': '0.0018287', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016518', 'logps_train/rejected': '-126.62', 'logps_train/chosen': '-153.44', 'loss/train': '0.70735', 'examples_per_second': '31.525', 'grad_norm': '32.75', 'counters/examples': 40224, 'counters/updates': 1257}
train stats after 40256 examples: {'rewards_train/chosen': '-0.0058644', 'rewards_train/rejected': '0.039205', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.045069', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-160.29', 'loss/train': '0.72131', 'examples_per_second': '30.754', 'grad_norm': '33.75', 'counters/examples': 40256, 'counters/updates': 1258}
train stats after 40288 examples: {'rewards_train/chosen': '0.03589', 'rewards_train/rejected': '-0.011063', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046953', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-138.63', 'loss/train': '0.67749', 'examples_per_second': '31.54', 'grad_norm': '29.875', 'counters/examples': 40288, 'counters/updates': 1259}
train stats after 40320 examples: {'rewards_train/chosen': '0.0029008', 'rewards_train/rejected': '-0.0056936', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0085944', 'logps_train/rejected': '-110.64', 'logps_train/chosen': '-129.28', 'loss/train': '0.69187', 'examples_per_second': '32.138', 'grad_norm': '26.5', 'counters/examples': 40320, 'counters/updates': 1260}
skipping logging after 40352 examples to avoid logging too frequently
train stats after 40384 examples: {'rewards_train/chosen': '0.063428', 'rewards_train/rejected': '0.021812', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041615', 'logps_train/rejected': '-106.09', 'logps_train/chosen': '-127.28', 'loss/train': '0.67786', 'examples_per_second': '32.801', 'grad_norm': '25.625', 'counters/examples': 40384, 'counters/updates': 1262}
train stats after 40416 examples: {'rewards_train/chosen': '-0.012368', 'rewards_train/rejected': '-0.012304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-6.4002e-05', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-127.41', 'loss/train': '0.69698', 'examples_per_second': '30.608', 'grad_norm': '27', 'counters/examples': 40416, 'counters/updates': 1263}
train stats after 40448 examples: {'rewards_train/chosen': '0.053424', 'rewards_train/rejected': '-0.008342', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061766', 'logps_train/rejected': '-163.81', 'logps_train/chosen': '-135.37', 'loss/train': '0.67104', 'examples_per_second': '32.379', 'grad_norm': '29.75', 'counters/examples': 40448, 'counters/updates': 1264}
train stats after 40480 examples: {'rewards_train/chosen': '0.041717', 'rewards_train/rejected': '-0.025507', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.067224', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-137.79', 'loss/train': '0.66395', 'examples_per_second': '31.36', 'grad_norm': '31.25', 'counters/examples': 40480, 'counters/updates': 1265}
train stats after 40512 examples: {'rewards_train/chosen': '0.04555', 'rewards_train/rejected': '0.056586', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011036', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-134.56', 'loss/train': '0.70434', 'examples_per_second': '32.474', 'grad_norm': '27.75', 'counters/examples': 40512, 'counters/updates': 1266}
train stats after 40544 examples: {'rewards_train/chosen': '-0.043453', 'rewards_train/rejected': '0.024877', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.06833', 'logps_train/rejected': '-120.82', 'logps_train/chosen': '-148.63', 'loss/train': '0.73685', 'examples_per_second': '31.476', 'grad_norm': '31.5', 'counters/examples': 40544, 'counters/updates': 1267}
train stats after 40576 examples: {'rewards_train/chosen': '0.023838', 'rewards_train/rejected': '0.013098', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01074', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-142.12', 'loss/train': '0.69186', 'examples_per_second': '30.24', 'grad_norm': '30.875', 'counters/examples': 40576, 'counters/updates': 1268}
train stats after 40608 examples: {'rewards_train/chosen': '0.022921', 'rewards_train/rejected': '-0.013816', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.036737', 'logps_train/rejected': '-133.53', 'logps_train/chosen': '-176.48', 'loss/train': '0.68032', 'examples_per_second': '30.091', 'grad_norm': '29.75', 'counters/examples': 40608, 'counters/updates': 1269}
train stats after 40640 examples: {'rewards_train/chosen': '0.063802', 'rewards_train/rejected': '0.0082644', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055537', 'logps_train/rejected': '-97.881', 'logps_train/chosen': '-108.37', 'loss/train': '0.67337', 'examples_per_second': '32.15', 'grad_norm': '22.75', 'counters/examples': 40640, 'counters/updates': 1270}
train stats after 40672 examples: {'rewards_train/chosen': '0.023188', 'rewards_train/rejected': '-0.0030112', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026199', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-130.08', 'loss/train': '0.68415', 'examples_per_second': '31.616', 'grad_norm': '28.375', 'counters/examples': 40672, 'counters/updates': 1271}
train stats after 40704 examples: {'rewards_train/chosen': '0.049921', 'rewards_train/rejected': '0.067336', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017415', 'logps_train/rejected': '-97.973', 'logps_train/chosen': '-129.73', 'loss/train': '0.7069', 'examples_per_second': '32.839', 'grad_norm': '30.625', 'counters/examples': 40704, 'counters/updates': 1272}
skipping logging after 40736 examples to avoid logging too frequently
train stats after 40768 examples: {'rewards_train/chosen': '0.010164', 'rewards_train/rejected': '-0.0052179', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015382', 'logps_train/rejected': '-123.15', 'logps_train/chosen': '-155.18', 'loss/train': '0.68917', 'examples_per_second': '33.062', 'grad_norm': '28.25', 'counters/examples': 40768, 'counters/updates': 1274}
skipping logging after 40800 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.039033', 'rewards_train/rejected': '-0.0011308', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037902', 'logps_train/rejected': '-87.563', 'logps_train/chosen': '-118.82', 'loss/train': '0.71575', 'examples_per_second': '35.7', 'grad_norm': '24.375', 'counters/examples': 40832, 'counters/updates': 1276}
skipping logging after 40864 examples to avoid logging too frequently
train stats after 40896 examples: {'rewards_train/chosen': '-0.0064078', 'rewards_train/rejected': '0.0054564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011864', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-158.88', 'loss/train': '0.71013', 'examples_per_second': '31.429', 'grad_norm': '56.25', 'counters/examples': 40896, 'counters/updates': 1278}
train stats after 40928 examples: {'rewards_train/chosen': '0.049423', 'rewards_train/rejected': '-0.023116', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07254', 'logps_train/rejected': '-139.09', 'logps_train/chosen': '-121.51', 'loss/train': '0.66063', 'examples_per_second': '31.851', 'grad_norm': '29.5', 'counters/examples': 40928, 'counters/updates': 1279}
train stats after 40960 examples: {'rewards_train/chosen': '0.041371', 'rewards_train/rejected': '-0.013843', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055214', 'logps_train/rejected': '-118.8', 'logps_train/chosen': '-120.08', 'loss/train': '0.66874', 'examples_per_second': '24.316', 'grad_norm': '24.25', 'counters/examples': 40960, 'counters/updates': 1280}
train stats after 40992 examples: {'rewards_train/chosen': '-0.0077362', 'rewards_train/rejected': '-0.029084', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.021348', 'logps_train/rejected': '-99.132', 'logps_train/chosen': '-120.06', 'loss/train': '0.68548', 'examples_per_second': '31.366', 'grad_norm': '23.25', 'counters/examples': 40992, 'counters/updates': 1281}
train stats after 41024 examples: {'rewards_train/chosen': '0.031461', 'rewards_train/rejected': '0.0057717', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025689', 'logps_train/rejected': '-118.91', 'logps_train/chosen': '-137.21', 'loss/train': '0.68426', 'examples_per_second': '31.594', 'grad_norm': '25.875', 'counters/examples': 41024, 'counters/updates': 1282}
train stats after 41056 examples: {'rewards_train/chosen': '0.051826', 'rewards_train/rejected': '0.034073', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017754', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-173.49', 'loss/train': '0.69047', 'examples_per_second': '24.223', 'grad_norm': '34', 'counters/examples': 41056, 'counters/updates': 1283}
train stats after 41088 examples: {'rewards_train/chosen': '-0.0080584', 'rewards_train/rejected': '-0.031378', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02332', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-142.87', 'loss/train': '0.68682', 'examples_per_second': '30.527', 'grad_norm': '34.25', 'counters/examples': 41088, 'counters/updates': 1284}
skipping logging after 41120 examples to avoid logging too frequently
train stats after 41152 examples: {'rewards_train/chosen': '0.010788', 'rewards_train/rejected': '0.015255', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0044668', 'logps_train/rejected': '-102.72', 'logps_train/chosen': '-140.27', 'loss/train': '0.69829', 'examples_per_second': '34.708', 'grad_norm': '25.375', 'counters/examples': 41152, 'counters/updates': 1286}
skipping logging after 41184 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '0.056183', 'rewards_train/rejected': '-0.015355', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.071538', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-121.13', 'loss/train': '0.6647', 'examples_per_second': '37.199', 'grad_norm': '24.625', 'counters/examples': 41216, 'counters/updates': 1288}
train stats after 41248 examples: {'rewards_train/chosen': '-0.017266', 'rewards_train/rejected': '-0.00060511', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016661', 'logps_train/rejected': '-126.11', 'logps_train/chosen': '-151.07', 'loss/train': '0.70538', 'examples_per_second': '33.25', 'grad_norm': '29.25', 'counters/examples': 41248, 'counters/updates': 1289}
train stats after 41280 examples: {'rewards_train/chosen': '0.0064829', 'rewards_train/rejected': '0.0006699', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.005813', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-141.22', 'loss/train': '0.69504', 'examples_per_second': '33.1', 'grad_norm': '27', 'counters/examples': 41280, 'counters/updates': 1290}
train stats after 41312 examples: {'rewards_train/chosen': '0.055795', 'rewards_train/rejected': '0.034763', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.021032', 'logps_train/rejected': '-120.8', 'logps_train/chosen': '-139.36', 'loss/train': '0.69045', 'examples_per_second': '32.694', 'grad_norm': '30.5', 'counters/examples': 41312, 'counters/updates': 1291}
skipping logging after 41344 examples to avoid logging too frequently
train stats after 41376 examples: {'rewards_train/chosen': '-0.0040065', 'rewards_train/rejected': '0.0080389', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012045', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-106.62', 'loss/train': '0.70407', 'examples_per_second': '31.076', 'grad_norm': '29.125', 'counters/examples': 41376, 'counters/updates': 1293}
train stats after 41408 examples: {'rewards_train/chosen': '-0.013626', 'rewards_train/rejected': '0.0053638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.01899', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-127.15', 'loss/train': '0.70654', 'examples_per_second': '32.179', 'grad_norm': '25.875', 'counters/examples': 41408, 'counters/updates': 1294}
train stats after 41440 examples: {'rewards_train/chosen': '-0.021573', 'rewards_train/rejected': '-0.015346', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.0062266', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-134.71', 'loss/train': '0.70611', 'examples_per_second': '31.322', 'grad_norm': '28', 'counters/examples': 41440, 'counters/updates': 1295}
train stats after 41472 examples: {'rewards_train/chosen': '0.015358', 'rewards_train/rejected': '-0.071661', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08702', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-174.85', 'loss/train': '0.66113', 'examples_per_second': '30.863', 'grad_norm': '29.75', 'counters/examples': 41472, 'counters/updates': 1296}
skipping logging after 41504 examples to avoid logging too frequently
train stats after 41536 examples: {'rewards_train/chosen': '0.026602', 'rewards_train/rejected': '-0.012134', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038736', 'logps_train/rejected': '-153.89', 'logps_train/chosen': '-147.15', 'loss/train': '0.68015', 'examples_per_second': '32.768', 'grad_norm': '27.125', 'counters/examples': 41536, 'counters/updates': 1298}
train stats after 41568 examples: {'rewards_train/chosen': '0.026696', 'rewards_train/rejected': '0.02555', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.001146', 'logps_train/rejected': '-132.9', 'logps_train/chosen': '-121.37', 'loss/train': '0.69633', 'examples_per_second': '30.516', 'grad_norm': '26.25', 'counters/examples': 41568, 'counters/updates': 1299}
train stats after 41600 examples: {'rewards_train/chosen': '0.020729', 'rewards_train/rejected': '0.024039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0033094', 'logps_train/rejected': '-144.91', 'logps_train/chosen': '-131.78', 'loss/train': '0.70103', 'examples_per_second': '30.891', 'grad_norm': '29.375', 'counters/examples': 41600, 'counters/updates': 1300}
train stats after 41632 examples: {'rewards_train/chosen': '0.029242', 'rewards_train/rejected': '0.031816', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0025747', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-139.53', 'loss/train': '0.69911', 'examples_per_second': '30.554', 'grad_norm': '29', 'counters/examples': 41632, 'counters/updates': 1301}
train stats after 41664 examples: {'rewards_train/chosen': '-0.0012387', 'rewards_train/rejected': '0.04159', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042829', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-123.19', 'loss/train': '0.72007', 'examples_per_second': '31.592', 'grad_norm': '29.125', 'counters/examples': 41664, 'counters/updates': 1302}
train stats after 41696 examples: {'rewards_train/chosen': '0.0021573', 'rewards_train/rejected': '-0.057545', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059702', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-154.87', 'loss/train': '0.67189', 'examples_per_second': '31.618', 'grad_norm': '24.875', 'counters/examples': 41696, 'counters/updates': 1303}
train stats after 41728 examples: {'rewards_train/chosen': '-0.010556', 'rewards_train/rejected': '0.010172', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020728', 'logps_train/rejected': '-105.88', 'logps_train/chosen': '-129.17', 'loss/train': '0.70808', 'examples_per_second': '30.265', 'grad_norm': '26.25', 'counters/examples': 41728, 'counters/updates': 1304}
skipping logging after 41760 examples to avoid logging too frequently
train stats after 41792 examples: {'rewards_train/chosen': '0.048132', 'rewards_train/rejected': '-0.033912', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082044', 'logps_train/rejected': '-158.1', 'logps_train/chosen': '-135.39', 'loss/train': '0.65837', 'examples_per_second': '31.59', 'grad_norm': '27.5', 'counters/examples': 41792, 'counters/updates': 1306}
train stats after 41824 examples: {'rewards_train/chosen': '0.0086309', 'rewards_train/rejected': '0.0044342', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0041966', 'logps_train/rejected': '-104.03', 'logps_train/chosen': '-123.32', 'loss/train': '0.69655', 'examples_per_second': '32.642', 'grad_norm': '25.625', 'counters/examples': 41824, 'counters/updates': 1307}
train stats after 41856 examples: {'rewards_train/chosen': '0.033254', 'rewards_train/rejected': '-0.0098759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04313', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-150.05', 'loss/train': '0.67642', 'examples_per_second': '31.375', 'grad_norm': '47.25', 'counters/examples': 41856, 'counters/updates': 1308}
train stats after 41888 examples: {'rewards_train/chosen': '-0.032078', 'rewards_train/rejected': '0.01589', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047968', 'logps_train/rejected': '-90.779', 'logps_train/chosen': '-124.76', 'loss/train': '0.72188', 'examples_per_second': '32.147', 'grad_norm': '25.375', 'counters/examples': 41888, 'counters/updates': 1309}
train stats after 41920 examples: {'rewards_train/chosen': '0.053501', 'rewards_train/rejected': '-0.0057944', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059295', 'logps_train/rejected': '-137.96', 'logps_train/chosen': '-137.13', 'loss/train': '0.67013', 'examples_per_second': '24.248', 'grad_norm': '28.25', 'counters/examples': 41920, 'counters/updates': 1310}
train stats after 41952 examples: {'rewards_train/chosen': '0.065905', 'rewards_train/rejected': '0.050739', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015166', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-145.44', 'loss/train': '0.69435', 'examples_per_second': '31.251', 'grad_norm': '27.25', 'counters/examples': 41952, 'counters/updates': 1311}
skipping logging after 41984 examples to avoid logging too frequently
train stats after 42016 examples: {'rewards_train/chosen': '0.028907', 'rewards_train/rejected': '0.01435', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014557', 'logps_train/rejected': '-106.32', 'logps_train/chosen': '-131.52', 'loss/train': '0.68955', 'examples_per_second': '30.312', 'grad_norm': '27.875', 'counters/examples': 42016, 'counters/updates': 1313}
train stats after 42048 examples: {'rewards_train/chosen': '0.025253', 'rewards_train/rejected': '0.0069705', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018283', 'logps_train/rejected': '-135.68', 'logps_train/chosen': '-139.97', 'loss/train': '0.68989', 'examples_per_second': '32.993', 'grad_norm': '28.875', 'counters/examples': 42048, 'counters/updates': 1314}
train stats after 42080 examples: {'rewards_train/chosen': '-0.0029779', 'rewards_train/rejected': '-0.02233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019352', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-112.56', 'loss/train': '0.6881', 'examples_per_second': '31.464', 'grad_norm': '30.25', 'counters/examples': 42080, 'counters/updates': 1315}
train stats after 42112 examples: {'rewards_train/chosen': '-0.0081991', 'rewards_train/rejected': '5.5972e-05', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0082551', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-126.81', 'loss/train': '0.70344', 'examples_per_second': '31.561', 'grad_norm': '27.625', 'counters/examples': 42112, 'counters/updates': 1316}
train stats after 42144 examples: {'rewards_train/chosen': '0.023284', 'rewards_train/rejected': '-0.039169', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062453', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-147.41', 'loss/train': '0.66986', 'examples_per_second': '31.562', 'grad_norm': '28.5', 'counters/examples': 42144, 'counters/updates': 1317}
train stats after 42176 examples: {'rewards_train/chosen': '0.010166', 'rewards_train/rejected': '0.019267', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0091009', 'logps_train/rejected': '-171.6', 'logps_train/chosen': '-167.13', 'loss/train': '0.70272', 'examples_per_second': '31.548', 'grad_norm': '47', 'counters/examples': 42176, 'counters/updates': 1318}
train stats after 42208 examples: {'rewards_train/chosen': '0.010246', 'rewards_train/rejected': '-0.021762', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032008', 'logps_train/rejected': '-96.039', 'logps_train/chosen': '-149.97', 'loss/train': '0.68641', 'examples_per_second': '30.054', 'grad_norm': '26', 'counters/examples': 42208, 'counters/updates': 1319}
train stats after 42240 examples: {'rewards_train/chosen': '-0.010423', 'rewards_train/rejected': '0.014222', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.024644', 'logps_train/rejected': '-110.98', 'logps_train/chosen': '-150.6', 'loss/train': '0.70843', 'examples_per_second': '30.602', 'grad_norm': '30.25', 'counters/examples': 42240, 'counters/updates': 1320}
train stats after 42272 examples: {'rewards_train/chosen': '0.068541', 'rewards_train/rejected': '0.032686', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.035855', 'logps_train/rejected': '-118.91', 'logps_train/chosen': '-130.86', 'loss/train': '0.67989', 'examples_per_second': '30.841', 'grad_norm': '27', 'counters/examples': 42272, 'counters/updates': 1321}
train stats after 42304 examples: {'rewards_train/chosen': '0.0079386', 'rewards_train/rejected': '0.0084931', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00055458', 'logps_train/rejected': '-138.11', 'logps_train/chosen': '-124.4', 'loss/train': '0.70212', 'examples_per_second': '32.684', 'grad_norm': '27.25', 'counters/examples': 42304, 'counters/updates': 1322}
skipping logging after 42336 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '0.026148', 'rewards_train/rejected': '0.022797', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0033514', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-128.04', 'loss/train': '0.69655', 'examples_per_second': '36.253', 'grad_norm': '28.5', 'counters/examples': 42368, 'counters/updates': 1324}
train stats after 42400 examples: {'rewards_train/chosen': '0.023969', 'rewards_train/rejected': '0.038211', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014242', 'logps_train/rejected': '-140.35', 'logps_train/chosen': '-140.96', 'loss/train': '0.70432', 'examples_per_second': '32.035', 'grad_norm': '32.75', 'counters/examples': 42400, 'counters/updates': 1325}
train stats after 42432 examples: {'rewards_train/chosen': '0.020444', 'rewards_train/rejected': '-0.018079', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038523', 'logps_train/rejected': '-87.682', 'logps_train/chosen': '-116.03', 'loss/train': '0.67709', 'examples_per_second': '32.435', 'grad_norm': '24.25', 'counters/examples': 42432, 'counters/updates': 1326}
train stats after 42464 examples: {'rewards_train/chosen': '0.071228', 'rewards_train/rejected': '0.029438', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04179', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-147.13', 'loss/train': '0.67848', 'examples_per_second': '31.343', 'grad_norm': '27.5', 'counters/examples': 42464, 'counters/updates': 1327}
train stats after 42496 examples: {'rewards_train/chosen': '0.039212', 'rewards_train/rejected': '0.0029609', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036251', 'logps_train/rejected': '-98.603', 'logps_train/chosen': '-126.07', 'loss/train': '0.67768', 'examples_per_second': '31.345', 'grad_norm': '21.5', 'counters/examples': 42496, 'counters/updates': 1328}
train stats after 42528 examples: {'rewards_train/chosen': '0.017162', 'rewards_train/rejected': '-0.0019666', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019129', 'logps_train/rejected': '-109.96', 'logps_train/chosen': '-145.87', 'loss/train': '0.6893', 'examples_per_second': '32.129', 'grad_norm': '27.375', 'counters/examples': 42528, 'counters/updates': 1329}
train stats after 42560 examples: {'rewards_train/chosen': '0.050254', 'rewards_train/rejected': '0.018419', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031835', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-129.99', 'loss/train': '0.68405', 'examples_per_second': '31.581', 'grad_norm': '29.375', 'counters/examples': 42560, 'counters/updates': 1330}
train stats after 42592 examples: {'rewards_train/chosen': '-0.0037454', 'rewards_train/rejected': '0.02761', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031355', 'logps_train/rejected': '-117.94', 'logps_train/chosen': '-138.59', 'loss/train': '0.71941', 'examples_per_second': '32.935', 'grad_norm': '30.875', 'counters/examples': 42592, 'counters/updates': 1331}
train stats after 42624 examples: {'rewards_train/chosen': '0.022627', 'rewards_train/rejected': '0.036349', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013721', 'logps_train/rejected': '-106.08', 'logps_train/chosen': '-133.97', 'loss/train': '0.70737', 'examples_per_second': '30.776', 'grad_norm': '27', 'counters/examples': 42624, 'counters/updates': 1332}
skipping logging after 42656 examples to avoid logging too frequently
train stats after 42688 examples: {'rewards_train/chosen': '0.039352', 'rewards_train/rejected': '0.043286', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0039337', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-142.32', 'loss/train': '0.70062', 'examples_per_second': '33.395', 'grad_norm': '30.625', 'counters/examples': 42688, 'counters/updates': 1334}
skipping logging after 42720 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '0.023052', 'rewards_train/rejected': '0.019534', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0035177', 'logps_train/rejected': '-122.32', 'logps_train/chosen': '-160.35', 'loss/train': '0.69597', 'examples_per_second': '31.656', 'grad_norm': '27.125', 'counters/examples': 42752, 'counters/updates': 1336}
train stats after 42784 examples: {'rewards_train/chosen': '0.0029721', 'rewards_train/rejected': '-0.0052994', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0082715', 'logps_train/rejected': '-127.82', 'logps_train/chosen': '-118.18', 'loss/train': '0.69345', 'examples_per_second': '30.01', 'grad_norm': '25.5', 'counters/examples': 42784, 'counters/updates': 1337}
train stats after 42816 examples: {'rewards_train/chosen': '0.00041759', 'rewards_train/rejected': '0.02648', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026062', 'logps_train/rejected': '-148.37', 'logps_train/chosen': '-161.13', 'loss/train': '0.71567', 'examples_per_second': '32.562', 'grad_norm': '35.25', 'counters/examples': 42816, 'counters/updates': 1338}
train stats after 42848 examples: {'rewards_train/chosen': '0.03555', 'rewards_train/rejected': '0.0035762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.031973', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-100.09', 'loss/train': '0.67971', 'examples_per_second': '32.576', 'grad_norm': '25.125', 'counters/examples': 42848, 'counters/updates': 1339}
skipping logging after 42880 examples to avoid logging too frequently
train stats after 42912 examples: {'rewards_train/chosen': '0.068624', 'rewards_train/rejected': '-0.014401', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083026', 'logps_train/rejected': '-116.44', 'logps_train/chosen': '-130.4', 'loss/train': '0.65475', 'examples_per_second': '32.003', 'grad_norm': '28.125', 'counters/examples': 42912, 'counters/updates': 1341}
train stats after 42944 examples: {'rewards_train/chosen': '0.027697', 'rewards_train/rejected': '0.02693', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00076775', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-134.08', 'loss/train': '0.70035', 'examples_per_second': '31.585', 'grad_norm': '25.625', 'counters/examples': 42944, 'counters/updates': 1342}
train stats after 42976 examples: {'rewards_train/chosen': '0.041032', 'rewards_train/rejected': '0.012595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028437', 'logps_train/rejected': '-129.68', 'logps_train/chosen': '-149.77', 'loss/train': '0.68365', 'examples_per_second': '31.654', 'grad_norm': '29.125', 'counters/examples': 42976, 'counters/updates': 1343}
train stats after 43008 examples: {'rewards_train/chosen': '0.072359', 'rewards_train/rejected': '-0.038313', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11067', 'logps_train/rejected': '-138.48', 'logps_train/chosen': '-171.72', 'loss/train': '0.64754', 'examples_per_second': '31.58', 'grad_norm': '30.25', 'counters/examples': 43008, 'counters/updates': 1344}
train stats after 43040 examples: {'rewards_train/chosen': '0.057689', 'rewards_train/rejected': '0.0008388', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05685', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-143.54', 'loss/train': '0.67204', 'examples_per_second': '31.679', 'grad_norm': '28.5', 'counters/examples': 43040, 'counters/updates': 1345}
train stats after 43072 examples: {'rewards_train/chosen': '0.065294', 'rewards_train/rejected': '-0.027135', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092429', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-154.04', 'loss/train': '0.65366', 'examples_per_second': '32.307', 'grad_norm': '28.5', 'counters/examples': 43072, 'counters/updates': 1346}
train stats after 43104 examples: {'rewards_train/chosen': '-0.039899', 'rewards_train/rejected': '-0.00060358', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.039295', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-120.63', 'loss/train': '0.71544', 'examples_per_second': '31.107', 'grad_norm': '28.5', 'counters/examples': 43104, 'counters/updates': 1347}
train stats after 43136 examples: {'rewards_train/chosen': '0.041832', 'rewards_train/rejected': '-0.043416', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085248', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-154.06', 'loss/train': '0.65831', 'examples_per_second': '32.985', 'grad_norm': '32.5', 'counters/examples': 43136, 'counters/updates': 1348}
skipping logging after 43168 examples to avoid logging too frequently
train stats after 43200 examples: {'rewards_train/chosen': '0.11143', 'rewards_train/rejected': '0.011562', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099872', 'logps_train/rejected': '-123.78', 'logps_train/chosen': '-145.12', 'loss/train': '0.6493', 'examples_per_second': '31.511', 'grad_norm': '25.75', 'counters/examples': 43200, 'counters/updates': 1350}
train stats after 43232 examples: {'rewards_train/chosen': '-0.0060401', 'rewards_train/rejected': '0.005574', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011614', 'logps_train/rejected': '-89.054', 'logps_train/chosen': '-136.53', 'loss/train': '0.70313', 'examples_per_second': '30.976', 'grad_norm': '26', 'counters/examples': 43232, 'counters/updates': 1351}
train stats after 43264 examples: {'rewards_train/chosen': '0.031336', 'rewards_train/rejected': '0.012887', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018449', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-156.14', 'loss/train': '0.69036', 'examples_per_second': '30.48', 'grad_norm': '31.125', 'counters/examples': 43264, 'counters/updates': 1352}
skipping logging after 43296 examples to avoid logging too frequently
train stats after 43328 examples: {'rewards_train/chosen': '0.038687', 'rewards_train/rejected': '-0.040286', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078973', 'logps_train/rejected': '-137.77', 'logps_train/chosen': '-166.38', 'loss/train': '0.6587', 'examples_per_second': '30.66', 'grad_norm': '31.625', 'counters/examples': 43328, 'counters/updates': 1354}
skipping logging after 43360 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '0.027999', 'rewards_train/rejected': '0.035149', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0071495', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-154.78', 'loss/train': '0.70308', 'examples_per_second': '30.607', 'grad_norm': '30.875', 'counters/examples': 43392, 'counters/updates': 1356}
train stats after 43424 examples: {'rewards_train/chosen': '0.049028', 'rewards_train/rejected': '0.010353', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038675', 'logps_train/rejected': '-137.05', 'logps_train/chosen': '-126.81', 'loss/train': '0.67967', 'examples_per_second': '32.195', 'grad_norm': '28.75', 'counters/examples': 43424, 'counters/updates': 1357}
train stats after 43456 examples: {'rewards_train/chosen': '0.0078408', 'rewards_train/rejected': '-0.046706', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054547', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-127.11', 'loss/train': '0.67115', 'examples_per_second': '31.688', 'grad_norm': '38.5', 'counters/examples': 43456, 'counters/updates': 1358}
train stats after 43488 examples: {'rewards_train/chosen': '0.045752', 'rewards_train/rejected': '-0.0074968', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053249', 'logps_train/rejected': '-111.74', 'logps_train/chosen': '-113.47', 'loss/train': '0.67279', 'examples_per_second': '30.149', 'grad_norm': '23', 'counters/examples': 43488, 'counters/updates': 1359}
train stats after 43520 examples: {'rewards_train/chosen': '0.0070475', 'rewards_train/rejected': '-0.0007432', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0077907', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-153.76', 'loss/train': '0.69686', 'examples_per_second': '31.455', 'grad_norm': '27.375', 'counters/examples': 43520, 'counters/updates': 1360}
train stats after 43552 examples: {'rewards_train/chosen': '0.039208', 'rewards_train/rejected': '-0.019595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058803', 'logps_train/rejected': '-158.85', 'logps_train/chosen': '-118.54', 'loss/train': '0.67254', 'examples_per_second': '31.672', 'grad_norm': '27.75', 'counters/examples': 43552, 'counters/updates': 1361}
train stats after 43584 examples: {'rewards_train/chosen': '0.085911', 'rewards_train/rejected': '0.0059431', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079968', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-148.15', 'loss/train': '0.65877', 'examples_per_second': '31.682', 'grad_norm': '28.75', 'counters/examples': 43584, 'counters/updates': 1362}
train stats after 43616 examples: {'rewards_train/chosen': '0.012235', 'rewards_train/rejected': '0.0067066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0055281', 'logps_train/rejected': '-75.277', 'logps_train/chosen': '-98.63', 'loss/train': '0.69545', 'examples_per_second': '30.908', 'grad_norm': '21.5', 'counters/examples': 43616, 'counters/updates': 1363}
skipping logging after 43648 examples to avoid logging too frequently
train stats after 43680 examples: {'rewards_train/chosen': '0.063654', 'rewards_train/rejected': '0.01127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052385', 'logps_train/rejected': '-105.84', 'logps_train/chosen': '-119.21', 'loss/train': '0.67247', 'examples_per_second': '35.938', 'grad_norm': '24.875', 'counters/examples': 43680, 'counters/updates': 1365}
train stats after 43712 examples: {'rewards_train/chosen': '0.079446', 'rewards_train/rejected': '0.025407', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054039', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-168.34', 'loss/train': '0.67202', 'examples_per_second': '31.707', 'grad_norm': '35', 'counters/examples': 43712, 'counters/updates': 1366}
train stats after 43744 examples: {'rewards_train/chosen': '0.028', 'rewards_train/rejected': '0.037118', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0091185', 'logps_train/rejected': '-90.571', 'logps_train/chosen': '-134.96', 'loss/train': '0.70006', 'examples_per_second': '31.674', 'grad_norm': '28.25', 'counters/examples': 43744, 'counters/updates': 1367}
train stats after 43776 examples: {'rewards_train/chosen': '0.086333', 'rewards_train/rejected': '0.014342', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.071991', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-150.64', 'loss/train': '0.66509', 'examples_per_second': '33.182', 'grad_norm': '30.5', 'counters/examples': 43776, 'counters/updates': 1368}
skipping logging after 43808 examples to avoid logging too frequently
train stats after 43840 examples: {'rewards_train/chosen': '0.0059868', 'rewards_train/rejected': '-0.030754', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036741', 'logps_train/rejected': '-158.8', 'logps_train/chosen': '-153.85', 'loss/train': '0.68184', 'examples_per_second': '31.325', 'grad_norm': '28.75', 'counters/examples': 43840, 'counters/updates': 1370}
skipping logging after 43872 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '0.053117', 'rewards_train/rejected': '0.0055694', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047548', 'logps_train/rejected': '-91.489', 'logps_train/chosen': '-113.73', 'loss/train': '0.67598', 'examples_per_second': '34.343', 'grad_norm': '24.25', 'counters/examples': 43904, 'counters/updates': 1372}
train stats after 43936 examples: {'rewards_train/chosen': '0.03346', 'rewards_train/rejected': '-0.029894', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063354', 'logps_train/rejected': '-155.69', 'logps_train/chosen': '-186.85', 'loss/train': '0.66713', 'examples_per_second': '30.892', 'grad_norm': '33.75', 'counters/examples': 43936, 'counters/updates': 1373}
train stats after 43968 examples: {'rewards_train/chosen': '0.031356', 'rewards_train/rejected': '0.037152', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0057965', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-124.76', 'loss/train': '0.69859', 'examples_per_second': '32.977', 'grad_norm': '30', 'counters/examples': 43968, 'counters/updates': 1374}
train stats after 44000 examples: {'rewards_train/chosen': '0.035653', 'rewards_train/rejected': '-0.00033284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035986', 'logps_train/rejected': '-143.24', 'logps_train/chosen': '-165.52', 'loss/train': '0.68266', 'examples_per_second': '31.398', 'grad_norm': '28.875', 'counters/examples': 44000, 'counters/updates': 1375}
train stats after 44032 examples: {'rewards_train/chosen': '0.01998', 'rewards_train/rejected': '0.024918', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0049384', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-135.13', 'loss/train': '0.70242', 'examples_per_second': '31.917', 'grad_norm': '27.75', 'counters/examples': 44032, 'counters/updates': 1376}
skipping logging after 44064 examples to avoid logging too frequently
train stats after 44096 examples: {'rewards_train/chosen': '0.039531', 'rewards_train/rejected': '0.0041655', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035366', 'logps_train/rejected': '-96.316', 'logps_train/chosen': '-103.47', 'loss/train': '0.67966', 'examples_per_second': '31.029', 'grad_norm': '26.375', 'counters/examples': 44096, 'counters/updates': 1378}
train stats after 44128 examples: {'rewards_train/chosen': '0.007022', 'rewards_train/rejected': '0.01786', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010838', 'logps_train/rejected': '-138.16', 'logps_train/chosen': '-188.02', 'loss/train': '0.70858', 'examples_per_second': '31.242', 'grad_norm': '32.5', 'counters/examples': 44128, 'counters/updates': 1379}
train stats after 44160 examples: {'rewards_train/chosen': '0.0087026', 'rewards_train/rejected': '-0.021959', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030661', 'logps_train/rejected': '-106.22', 'logps_train/chosen': '-119.94', 'loss/train': '0.68552', 'examples_per_second': '31.766', 'grad_norm': '24.625', 'counters/examples': 44160, 'counters/updates': 1380}
train stats after 44192 examples: {'rewards_train/chosen': '0.056423', 'rewards_train/rejected': '-0.017832', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074255', 'logps_train/rejected': '-86.494', 'logps_train/chosen': '-142.39', 'loss/train': '0.66531', 'examples_per_second': '31.666', 'grad_norm': '23.75', 'counters/examples': 44192, 'counters/updates': 1381}
train stats after 44224 examples: {'rewards_train/chosen': '0.06183', 'rewards_train/rejected': '0.01255', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04928', 'logps_train/rejected': '-100.11', 'logps_train/chosen': '-167.38', 'loss/train': '0.67503', 'examples_per_second': '31.691', 'grad_norm': '27.875', 'counters/examples': 44224, 'counters/updates': 1382}
train stats after 44256 examples: {'rewards_train/chosen': '0.016192', 'rewards_train/rejected': '-0.019383', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035576', 'logps_train/rejected': '-132.59', 'logps_train/chosen': '-142.5', 'loss/train': '0.68263', 'examples_per_second': '32.772', 'grad_norm': '27.125', 'counters/examples': 44256, 'counters/updates': 1383}
train stats after 44288 examples: {'rewards_train/chosen': '0.047088', 'rewards_train/rejected': '0.069237', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.022149', 'logps_train/rejected': '-129.68', 'logps_train/chosen': '-118.65', 'loss/train': '0.71208', 'examples_per_second': '32.062', 'grad_norm': '29.125', 'counters/examples': 44288, 'counters/updates': 1384}
skipping logging after 44320 examples to avoid logging too frequently
train stats after 44352 examples: {'rewards_train/chosen': '0.05806', 'rewards_train/rejected': '0.0019583', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056102', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-126.03', 'loss/train': '0.66929', 'examples_per_second': '30.175', 'grad_norm': '26.625', 'counters/examples': 44352, 'counters/updates': 1386}
train stats after 44384 examples: {'rewards_train/chosen': '0.049239', 'rewards_train/rejected': '-0.01826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067499', 'logps_train/rejected': '-118.12', 'logps_train/chosen': '-128.19', 'loss/train': '0.66402', 'examples_per_second': '31.64', 'grad_norm': '24', 'counters/examples': 44384, 'counters/updates': 1387}
train stats after 44416 examples: {'rewards_train/chosen': '0.031418', 'rewards_train/rejected': '-0.012594', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044012', 'logps_train/rejected': '-112.77', 'logps_train/chosen': '-148.92', 'loss/train': '0.67547', 'examples_per_second': '30.18', 'grad_norm': '27.625', 'counters/examples': 44416, 'counters/updates': 1388}
train stats after 44448 examples: {'rewards_train/chosen': '0.044697', 'rewards_train/rejected': '0.019663', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025034', 'logps_train/rejected': '-116', 'logps_train/chosen': '-140.14', 'loss/train': '0.68812', 'examples_per_second': '32.372', 'grad_norm': '25.875', 'counters/examples': 44448, 'counters/updates': 1389}
skipping logging after 44480 examples to avoid logging too frequently
train stats after 44512 examples: {'rewards_train/chosen': '0.044167', 'rewards_train/rejected': '0.023703', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020464', 'logps_train/rejected': '-93.572', 'logps_train/chosen': '-131.53', 'loss/train': '0.68778', 'examples_per_second': '40.382', 'grad_norm': '24.625', 'counters/examples': 44512, 'counters/updates': 1391}
train stats after 44544 examples: {'rewards_train/chosen': '0.00076817', 'rewards_train/rejected': '-0.056196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056964', 'logps_train/rejected': '-135.52', 'logps_train/chosen': '-119.05', 'loss/train': '0.67182', 'examples_per_second': '31.604', 'grad_norm': '27.25', 'counters/examples': 44544, 'counters/updates': 1392}
train stats after 44576 examples: {'rewards_train/chosen': '0.071031', 'rewards_train/rejected': '0.020203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050828', 'logps_train/rejected': '-127.48', 'logps_train/chosen': '-167.15', 'loss/train': '0.67654', 'examples_per_second': '31.677', 'grad_norm': '29.75', 'counters/examples': 44576, 'counters/updates': 1393}
train stats after 44608 examples: {'rewards_train/chosen': '0.047839', 'rewards_train/rejected': '0.0065964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041243', 'logps_train/rejected': '-104.24', 'logps_train/chosen': '-171.13', 'loss/train': '0.67806', 'examples_per_second': '30.81', 'grad_norm': '32.75', 'counters/examples': 44608, 'counters/updates': 1394}
skipping logging after 44640 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '0.050921', 'rewards_train/rejected': '0.033404', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017517', 'logps_train/rejected': '-123.34', 'logps_train/chosen': '-141.85', 'loss/train': '0.69693', 'examples_per_second': '31.684', 'grad_norm': '32.5', 'counters/examples': 44672, 'counters/updates': 1396}
skipping logging after 44704 examples to avoid logging too frequently
train stats after 44736 examples: {'rewards_train/chosen': '0.022823', 'rewards_train/rejected': '-0.032486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055309', 'logps_train/rejected': '-117.19', 'logps_train/chosen': '-140.9', 'loss/train': '0.66993', 'examples_per_second': '31.085', 'grad_norm': '25.5', 'counters/examples': 44736, 'counters/updates': 1398}
train stats after 44768 examples: {'rewards_train/chosen': '0.051828', 'rewards_train/rejected': '0.031398', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02043', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-132.44', 'loss/train': '0.68858', 'examples_per_second': '31.711', 'grad_norm': '28.875', 'counters/examples': 44768, 'counters/updates': 1399}
train stats after 44800 examples: {'rewards_train/chosen': '-0.00016408', 'rewards_train/rejected': '0.0024801', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0026442', 'logps_train/rejected': '-144.79', 'logps_train/chosen': '-190.39', 'loss/train': '0.70163', 'examples_per_second': '31.631', 'grad_norm': '33.25', 'counters/examples': 44800, 'counters/updates': 1400}
train stats after 44832 examples: {'rewards_train/chosen': '0.021812', 'rewards_train/rejected': '0.014383', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0074291', 'logps_train/rejected': '-147.63', 'logps_train/chosen': '-137.21', 'loss/train': '0.69592', 'examples_per_second': '30.312', 'grad_norm': '29.125', 'counters/examples': 44832, 'counters/updates': 1401}
train stats after 44864 examples: {'rewards_train/chosen': '0.1052', 'rewards_train/rejected': '0.019136', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086066', 'logps_train/rejected': '-148.17', 'logps_train/chosen': '-144.09', 'loss/train': '0.6601', 'examples_per_second': '32.248', 'grad_norm': '26.625', 'counters/examples': 44864, 'counters/updates': 1402}
train stats after 44896 examples: {'rewards_train/chosen': '0.0027588', 'rewards_train/rejected': '-0.004615', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0073737', 'logps_train/rejected': '-102.27', 'logps_train/chosen': '-140.76', 'loss/train': '0.69098', 'examples_per_second': '31.668', 'grad_norm': '30', 'counters/examples': 44896, 'counters/updates': 1403}
train stats after 44928 examples: {'rewards_train/chosen': '-0.030194', 'rewards_train/rejected': '-0.0021564', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028038', 'logps_train/rejected': '-178.71', 'logps_train/chosen': '-154.68', 'loss/train': '0.71235', 'examples_per_second': '31.37', 'grad_norm': '30.625', 'counters/examples': 44928, 'counters/updates': 1404}
train stats after 44960 examples: {'rewards_train/chosen': '0.052079', 'rewards_train/rejected': '0.024317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027762', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-131.37', 'loss/train': '0.68294', 'examples_per_second': '31.907', 'grad_norm': '29.375', 'counters/examples': 44960, 'counters/updates': 1405}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45024 examples: {'rewards_train/chosen': '0.014685', 'rewards_train/rejected': '-0.023874', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038558', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-161.2', 'loss/train': '0.6818', 'examples_per_second': '31.68', 'grad_norm': '31.875', 'counters/examples': 45024, 'counters/updates': 1407}
train stats after 45056 examples: {'rewards_train/chosen': '0.031774', 'rewards_train/rejected': '0.034115', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0023405', 'logps_train/rejected': '-129.7', 'logps_train/chosen': '-126.87', 'loss/train': '0.70031', 'examples_per_second': '32.504', 'grad_norm': '28.125', 'counters/examples': 45056, 'counters/updates': 1408}
train stats after 45088 examples: {'rewards_train/chosen': '0.093156', 'rewards_train/rejected': '0.013895', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079261', 'logps_train/rejected': '-104.52', 'logps_train/chosen': '-133.47', 'loss/train': '0.66346', 'examples_per_second': '30.201', 'grad_norm': '25.875', 'counters/examples': 45088, 'counters/updates': 1409}
train stats after 45120 examples: {'rewards_train/chosen': '0.015639', 'rewards_train/rejected': '0.0064793', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0091596', 'logps_train/rejected': '-99.662', 'logps_train/chosen': '-115.37', 'loss/train': '0.69262', 'examples_per_second': '32.843', 'grad_norm': '24.25', 'counters/examples': 45120, 'counters/updates': 1410}
train stats after 45152 examples: {'rewards_train/chosen': '0.01589', 'rewards_train/rejected': '-0.0019673', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017857', 'logps_train/rejected': '-109.09', 'logps_train/chosen': '-114.86', 'loss/train': '0.6904', 'examples_per_second': '32.634', 'grad_norm': '24.125', 'counters/examples': 45152, 'counters/updates': 1411}
train stats after 45184 examples: {'rewards_train/chosen': '0.017697', 'rewards_train/rejected': '-0.00035654', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018054', 'logps_train/rejected': '-104.79', 'logps_train/chosen': '-136.78', 'loss/train': '0.68963', 'examples_per_second': '30.912', 'grad_norm': '25.5', 'counters/examples': 45184, 'counters/updates': 1412}
train stats after 45216 examples: {'rewards_train/chosen': '0.0060185', 'rewards_train/rejected': '0.045855', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.039837', 'logps_train/rejected': '-135.34', 'logps_train/chosen': '-133.13', 'loss/train': '0.71607', 'examples_per_second': '30.746', 'grad_norm': '28.125', 'counters/examples': 45216, 'counters/updates': 1413}
train stats after 45248 examples: {'rewards_train/chosen': '0.0053846', 'rewards_train/rejected': '-0.025211', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030595', 'logps_train/rejected': '-134.4', 'logps_train/chosen': '-114.82', 'loss/train': '0.68341', 'examples_per_second': '32.639', 'grad_norm': '29.25', 'counters/examples': 45248, 'counters/updates': 1414}
train stats after 45280 examples: {'rewards_train/chosen': '0.006265', 'rewards_train/rejected': '0.034715', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02845', 'logps_train/rejected': '-99.06', 'logps_train/chosen': '-123.06', 'loss/train': '0.71608', 'examples_per_second': '31.178', 'grad_norm': '27.375', 'counters/examples': 45280, 'counters/updates': 1415}
train stats after 45312 examples: {'rewards_train/chosen': '0.013779', 'rewards_train/rejected': '-0.014935', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028714', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-158.92', 'loss/train': '0.68314', 'examples_per_second': '32.231', 'grad_norm': '28.5', 'counters/examples': 45312, 'counters/updates': 1416}
skipping logging after 45344 examples to avoid logging too frequently
train stats after 45376 examples: {'rewards_train/chosen': '-0.004781', 'rewards_train/rejected': '-0.016936', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012155', 'logps_train/rejected': '-101.55', 'logps_train/chosen': '-133.03', 'loss/train': '0.69082', 'examples_per_second': '31.683', 'grad_norm': '31.625', 'counters/examples': 45376, 'counters/updates': 1418}
train stats after 45408 examples: {'rewards_train/chosen': '0.044451', 'rewards_train/rejected': '0.038347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.006104', 'logps_train/rejected': '-125.56', 'logps_train/chosen': '-131.08', 'loss/train': '0.6947', 'examples_per_second': '32.405', 'grad_norm': '29.75', 'counters/examples': 45408, 'counters/updates': 1419}
train stats after 45440 examples: {'rewards_train/chosen': '0.019639', 'rewards_train/rejected': '0.02302', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0033815', 'logps_train/rejected': '-104.2', 'logps_train/chosen': '-112.78', 'loss/train': '0.69806', 'examples_per_second': '31.358', 'grad_norm': '24.875', 'counters/examples': 45440, 'counters/updates': 1420}
train stats after 45472 examples: {'rewards_train/chosen': '0.060881', 'rewards_train/rejected': '0.015259', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045623', 'logps_train/rejected': '-104.56', 'logps_train/chosen': '-110.74', 'loss/train': '0.67566', 'examples_per_second': '30.731', 'grad_norm': '24.5', 'counters/examples': 45472, 'counters/updates': 1421}
train stats after 45504 examples: {'rewards_train/chosen': '0.044799', 'rewards_train/rejected': '0.056125', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011327', 'logps_train/rejected': '-133.08', 'logps_train/chosen': '-187.58', 'loss/train': '0.71398', 'examples_per_second': '32.644', 'grad_norm': '34', 'counters/examples': 45504, 'counters/updates': 1422}
skipping logging after 45536 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '0.024837', 'rewards_train/rejected': '0.0080423', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016794', 'logps_train/rejected': '-115', 'logps_train/chosen': '-112.83', 'loss/train': '0.68917', 'examples_per_second': '33.172', 'grad_norm': '24.5', 'counters/examples': 45568, 'counters/updates': 1424}
train stats after 45600 examples: {'rewards_train/chosen': '0.035789', 'rewards_train/rejected': '-0.05206', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087848', 'logps_train/rejected': '-99.763', 'logps_train/chosen': '-139.55', 'loss/train': '0.65807', 'examples_per_second': '31.702', 'grad_norm': '26', 'counters/examples': 45600, 'counters/updates': 1425}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45664 examples: {'rewards_train/chosen': '-0.022323', 'rewards_train/rejected': '0.052851', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.075174', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-133.99', 'loss/train': '0.73767', 'examples_per_second': '31.681', 'grad_norm': '29.125', 'counters/examples': 45664, 'counters/updates': 1427}
train stats after 45696 examples: {'rewards_train/chosen': '0.049289', 'rewards_train/rejected': '0.016694', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032595', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-123.33', 'loss/train': '0.68236', 'examples_per_second': '32.759', 'grad_norm': '29.125', 'counters/examples': 45696, 'counters/updates': 1428}
train stats after 45728 examples: {'rewards_train/chosen': '0.054102', 'rewards_train/rejected': '-0.010456', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064558', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-166.52', 'loss/train': '0.66558', 'examples_per_second': '32.064', 'grad_norm': '26.625', 'counters/examples': 45728, 'counters/updates': 1429}
train stats after 45760 examples: {'rewards_train/chosen': '0.032277', 'rewards_train/rejected': '-0.0054435', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03772', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-155.6', 'loss/train': '0.67913', 'examples_per_second': '30.837', 'grad_norm': '25.125', 'counters/examples': 45760, 'counters/updates': 1430}
train stats after 45792 examples: {'rewards_train/chosen': '0.091321', 'rewards_train/rejected': '0.019154', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072167', 'logps_train/rejected': '-85.073', 'logps_train/chosen': '-122.98', 'loss/train': '0.66976', 'examples_per_second': '31.535', 'grad_norm': '26.125', 'counters/examples': 45792, 'counters/updates': 1431}
train stats after 45824 examples: {'rewards_train/chosen': '0.024817', 'rewards_train/rejected': '-0.011596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036413', 'logps_train/rejected': '-122.51', 'logps_train/chosen': '-136.11', 'loss/train': '0.67932', 'examples_per_second': '31.589', 'grad_norm': '30.375', 'counters/examples': 45824, 'counters/updates': 1432}
train stats after 45856 examples: {'rewards_train/chosen': '0.057292', 'rewards_train/rejected': '0.010314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046979', 'logps_train/rejected': '-137.69', 'logps_train/chosen': '-156.85', 'loss/train': '0.67648', 'examples_per_second': '31.575', 'grad_norm': '28.5', 'counters/examples': 45856, 'counters/updates': 1433}
train stats after 45888 examples: {'rewards_train/chosen': '0.058526', 'rewards_train/rejected': '0.00010129', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058425', 'logps_train/rejected': '-133.37', 'logps_train/chosen': '-149.18', 'loss/train': '0.67176', 'examples_per_second': '30.103', 'grad_norm': '31.125', 'counters/examples': 45888, 'counters/updates': 1434}
train stats after 45920 examples: {'rewards_train/chosen': '0.034636', 'rewards_train/rejected': '0.055152', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020517', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-174.64', 'loss/train': '0.71077', 'examples_per_second': '32.398', 'grad_norm': '31.375', 'counters/examples': 45920, 'counters/updates': 1435}
train stats after 45952 examples: {'rewards_train/chosen': '0.05518', 'rewards_train/rejected': '0.025223', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029957', 'logps_train/rejected': '-92.028', 'logps_train/chosen': '-104.69', 'loss/train': '0.68205', 'examples_per_second': '32.822', 'grad_norm': '22.75', 'counters/examples': 45952, 'counters/updates': 1436}
train stats after 45984 examples: {'rewards_train/chosen': '0.015396', 'rewards_train/rejected': '0.058393', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042997', 'logps_train/rejected': '-141.54', 'logps_train/chosen': '-178.72', 'loss/train': '0.71904', 'examples_per_second': '31.624', 'grad_norm': '33.5', 'counters/examples': 45984, 'counters/updates': 1437}
train stats after 46016 examples: {'rewards_train/chosen': '0.026727', 'rewards_train/rejected': '0.041221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.014495', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-154.8', 'loss/train': '0.70941', 'examples_per_second': '31.766', 'grad_norm': '55', 'counters/examples': 46016, 'counters/updates': 1438}
train stats after 46048 examples: {'rewards_train/chosen': '0.090105', 'rewards_train/rejected': '0.021665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068439', 'logps_train/rejected': '-142.87', 'logps_train/chosen': '-123.24', 'loss/train': '0.66608', 'examples_per_second': '30.224', 'grad_norm': '28.25', 'counters/examples': 46048, 'counters/updates': 1439}
train stats after 46080 examples: {'rewards_train/chosen': '0.042298', 'rewards_train/rejected': '0.021154', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021143', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-140.05', 'loss/train': '0.68612', 'examples_per_second': '32.734', 'grad_norm': '28.5', 'counters/examples': 46080, 'counters/updates': 1440}
skipping logging after 46112 examples to avoid logging too frequently
train stats after 46144 examples: {'rewards_train/chosen': '-0.0026094', 'rewards_train/rejected': '0.048859', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.051468', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-179.98', 'loss/train': '0.72205', 'examples_per_second': '31.632', 'grad_norm': '35', 'counters/examples': 46144, 'counters/updates': 1442}
train stats after 46176 examples: {'rewards_train/chosen': '0.063861', 'rewards_train/rejected': '0.0072485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056612', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-149.92', 'loss/train': '0.66904', 'examples_per_second': '32.675', 'grad_norm': '27.125', 'counters/examples': 46176, 'counters/updates': 1443}
train stats after 46208 examples: {'rewards_train/chosen': '0.022708', 'rewards_train/rejected': '-0.034907', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057616', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-153.83', 'loss/train': '0.6681', 'examples_per_second': '31.506', 'grad_norm': '26.25', 'counters/examples': 46208, 'counters/updates': 1444}
train stats after 46240 examples: {'rewards_train/chosen': '0.028274', 'rewards_train/rejected': '0.06089', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032617', 'logps_train/rejected': '-137.11', 'logps_train/chosen': '-121.4', 'loss/train': '0.71407', 'examples_per_second': '31.484', 'grad_norm': '27.5', 'counters/examples': 46240, 'counters/updates': 1445}
train stats after 46272 examples: {'rewards_train/chosen': '0.095282', 'rewards_train/rejected': '0.090095', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0051874', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-154.39', 'loss/train': '0.69676', 'examples_per_second': '31.425', 'grad_norm': '30.5', 'counters/examples': 46272, 'counters/updates': 1446}
skipping logging after 46304 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '0.035795', 'rewards_train/rejected': '-0.0030755', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03887', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-146.91', 'loss/train': '0.68132', 'examples_per_second': '31.84', 'grad_norm': '28', 'counters/examples': 46336, 'counters/updates': 1448}
skipping logging after 46368 examples to avoid logging too frequently
train stats after 46400 examples: {'rewards_train/chosen': '0.032152', 'rewards_train/rejected': '-0.00064463', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032796', 'logps_train/rejected': '-186.22', 'logps_train/chosen': '-150', 'loss/train': '0.68179', 'examples_per_second': '33.072', 'grad_norm': '40.25', 'counters/examples': 46400, 'counters/updates': 1450}
train stats after 46432 examples: {'rewards_train/chosen': '-0.020145', 'rewards_train/rejected': '0.035608', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.055753', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-118.83', 'loss/train': '0.7271', 'examples_per_second': '24.168', 'grad_norm': '30.5', 'counters/examples': 46432, 'counters/updates': 1451}
train stats after 46464 examples: {'rewards_train/chosen': '0.02359', 'rewards_train/rejected': '-0.0046007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028191', 'logps_train/rejected': '-103.82', 'logps_train/chosen': '-110.45', 'loss/train': '0.68289', 'examples_per_second': '32.297', 'grad_norm': '23.875', 'counters/examples': 46464, 'counters/updates': 1452}
train stats after 46496 examples: {'rewards_train/chosen': '0.0087008', 'rewards_train/rejected': '-0.10624', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11494', 'logps_train/rejected': '-153.77', 'logps_train/chosen': '-164.61', 'loss/train': '0.64297', 'examples_per_second': '31.596', 'grad_norm': '30.625', 'counters/examples': 46496, 'counters/updates': 1453}
train stats after 46528 examples: {'rewards_train/chosen': '0.0069716', 'rewards_train/rejected': '-0.0066346', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.013606', 'logps_train/rejected': '-91.862', 'logps_train/chosen': '-143.65', 'loss/train': '0.69115', 'examples_per_second': '27.576', 'grad_norm': '26', 'counters/examples': 46528, 'counters/updates': 1454}
train stats after 46560 examples: {'rewards_train/chosen': '0.029058', 'rewards_train/rejected': '0.029858', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0007996', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-121.84', 'loss/train': '0.70069', 'examples_per_second': '30.148', 'grad_norm': '36.5', 'counters/examples': 46560, 'counters/updates': 1455}
train stats after 46592 examples: {'rewards_train/chosen': '-0.043185', 'rewards_train/rejected': '-0.033825', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0093598', 'logps_train/rejected': '-110.53', 'logps_train/chosen': '-145.44', 'loss/train': '0.70437', 'examples_per_second': '31.682', 'grad_norm': '29.375', 'counters/examples': 46592, 'counters/updates': 1456}
train stats after 46624 examples: {'rewards_train/chosen': '0.040642', 'rewards_train/rejected': '0.012309', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028332', 'logps_train/rejected': '-104.66', 'logps_train/chosen': '-151.65', 'loss/train': '0.6836', 'examples_per_second': '31.68', 'grad_norm': '26.75', 'counters/examples': 46624, 'counters/updates': 1457}
train stats after 46656 examples: {'rewards_train/chosen': '0.010625', 'rewards_train/rejected': '0.025214', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014589', 'logps_train/rejected': '-90.05', 'logps_train/chosen': '-117.81', 'loss/train': '0.70543', 'examples_per_second': '31.167', 'grad_norm': '24.875', 'counters/examples': 46656, 'counters/updates': 1458}
skipping logging after 46688 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '0.049712', 'rewards_train/rejected': '0.055507', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0057951', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-185.01', 'loss/train': '0.70139', 'examples_per_second': '30.68', 'grad_norm': '30.25', 'counters/examples': 46720, 'counters/updates': 1460}
train stats after 46752 examples: {'rewards_train/chosen': '0.029423', 'rewards_train/rejected': '-0.008755', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.038178', 'logps_train/rejected': '-104.43', 'logps_train/chosen': '-150.22', 'loss/train': '0.67767', 'examples_per_second': '31.653', 'grad_norm': '29.375', 'counters/examples': 46752, 'counters/updates': 1461}
train stats after 46784 examples: {'rewards_train/chosen': '0.043121', 'rewards_train/rejected': '-0.0078335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050954', 'logps_train/rejected': '-161.31', 'logps_train/chosen': '-150.75', 'loss/train': '0.67825', 'examples_per_second': '32.578', 'grad_norm': '29.125', 'counters/examples': 46784, 'counters/updates': 1462}
train stats after 46816 examples: {'rewards_train/chosen': '0.052113', 'rewards_train/rejected': '-0.017159', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069272', 'logps_train/rejected': '-96.672', 'logps_train/chosen': '-143.49', 'loss/train': '0.66199', 'examples_per_second': '30.322', 'grad_norm': '24.875', 'counters/examples': 46816, 'counters/updates': 1463}
skipping logging after 46848 examples to avoid logging too frequently
train stats after 46880 examples: {'rewards_train/chosen': '0.069479', 'rewards_train/rejected': '0.0012823', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068197', 'logps_train/rejected': '-124.74', 'logps_train/chosen': '-146.55', 'loss/train': '0.66374', 'examples_per_second': '30.058', 'grad_norm': '40.75', 'counters/examples': 46880, 'counters/updates': 1465}
train stats after 46912 examples: {'rewards_train/chosen': '-0.037377', 'rewards_train/rejected': '0.011008', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.048385', 'logps_train/rejected': '-140.07', 'logps_train/chosen': '-183.6', 'loss/train': '0.73611', 'examples_per_second': '31.677', 'grad_norm': '46', 'counters/examples': 46912, 'counters/updates': 1466}
train stats after 46944 examples: {'rewards_train/chosen': '0.076512', 'rewards_train/rejected': '-0.017645', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094157', 'logps_train/rejected': '-101.91', 'logps_train/chosen': '-139.57', 'loss/train': '0.65304', 'examples_per_second': '30.646', 'grad_norm': '24.75', 'counters/examples': 46944, 'counters/updates': 1467}
train stats after 46976 examples: {'rewards_train/chosen': '0.027736', 'rewards_train/rejected': '0.0096521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018084', 'logps_train/rejected': '-111.06', 'logps_train/chosen': '-147.4', 'loss/train': '0.6906', 'examples_per_second': '32.389', 'grad_norm': '26.875', 'counters/examples': 46976, 'counters/updates': 1468}
train stats after 47008 examples: {'rewards_train/chosen': '0.067283', 'rewards_train/rejected': '0.037699', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.029585', 'logps_train/rejected': '-152.11', 'logps_train/chosen': '-158.81', 'loss/train': '0.68157', 'examples_per_second': '31.899', 'grad_norm': '28.125', 'counters/examples': 47008, 'counters/updates': 1469}
train stats after 47040 examples: {'rewards_train/chosen': '0.04207', 'rewards_train/rejected': '0.030283', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011787', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-134.11', 'loss/train': '0.68961', 'examples_per_second': '30.186', 'grad_norm': '31.25', 'counters/examples': 47040, 'counters/updates': 1470}
train stats after 47072 examples: {'rewards_train/chosen': '0.01531', 'rewards_train/rejected': '-0.027044', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042354', 'logps_train/rejected': '-114.96', 'logps_train/chosen': '-142.05', 'loss/train': '0.67938', 'examples_per_second': '32.004', 'grad_norm': '26.625', 'counters/examples': 47072, 'counters/updates': 1471}
train stats after 47104 examples: {'rewards_train/chosen': '0.018466', 'rewards_train/rejected': '-0.004497', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.022963', 'logps_train/rejected': '-127.02', 'logps_train/chosen': '-154.83', 'loss/train': '0.68628', 'examples_per_second': '31.63', 'grad_norm': '28.875', 'counters/examples': 47104, 'counters/updates': 1472}
train stats after 47136 examples: {'rewards_train/chosen': '0.0089353', 'rewards_train/rejected': '0.05928', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.050344', 'logps_train/rejected': '-110.62', 'logps_train/chosen': '-141.88', 'loss/train': '0.72296', 'examples_per_second': '31.042', 'grad_norm': '29.625', 'counters/examples': 47136, 'counters/updates': 1473}
train stats after 47168 examples: {'rewards_train/chosen': '-0.0027697', 'rewards_train/rejected': '-0.01953', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01676', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-130.4', 'loss/train': '0.68963', 'examples_per_second': '32.94', 'grad_norm': '29.125', 'counters/examples': 47168, 'counters/updates': 1474}
train stats after 47200 examples: {'rewards_train/chosen': '0.015659', 'rewards_train/rejected': '-0.028345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044003', 'logps_train/rejected': '-100.1', 'logps_train/chosen': '-106.01', 'loss/train': '0.67426', 'examples_per_second': '31.678', 'grad_norm': '31.125', 'counters/examples': 47200, 'counters/updates': 1475}
skipping logging after 47232 examples to avoid logging too frequently
train stats after 47264 examples: {'rewards_train/chosen': '0.033432', 'rewards_train/rejected': '0.021266', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012165', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-162.99', 'loss/train': '0.69337', 'examples_per_second': '30.714', 'grad_norm': '32', 'counters/examples': 47264, 'counters/updates': 1477}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47328 examples: {'rewards_train/chosen': '0.036475', 'rewards_train/rejected': '0.04544', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0089644', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-167.8', 'loss/train': '0.70061', 'examples_per_second': '31.793', 'grad_norm': '33', 'counters/examples': 47328, 'counters/updates': 1479}
skipping logging after 47360 examples to avoid logging too frequently
train stats after 47392 examples: {'rewards_train/chosen': '0.035902', 'rewards_train/rejected': '0.016945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018957', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-148.54', 'loss/train': '0.6871', 'examples_per_second': '31.344', 'grad_norm': '25.625', 'counters/examples': 47392, 'counters/updates': 1481}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47456 examples: {'rewards_train/chosen': '0.064368', 'rewards_train/rejected': '0.020808', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04356', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-142.04', 'loss/train': '0.67874', 'examples_per_second': '31.689', 'grad_norm': '26.75', 'counters/examples': 47456, 'counters/updates': 1483}
train stats after 47488 examples: {'rewards_train/chosen': '-0.036833', 'rewards_train/rejected': '-0.058247', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021415', 'logps_train/rejected': '-142.16', 'logps_train/chosen': '-169.07', 'loss/train': '0.68883', 'examples_per_second': '24.748', 'grad_norm': '31.5', 'counters/examples': 47488, 'counters/updates': 1484}
train stats after 47520 examples: {'rewards_train/chosen': '0.017394', 'rewards_train/rejected': '0.018154', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00076013', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-132.03', 'loss/train': '0.69724', 'examples_per_second': '31.866', 'grad_norm': '26.5', 'counters/examples': 47520, 'counters/updates': 1485}
train stats after 47552 examples: {'rewards_train/chosen': '0.011161', 'rewards_train/rejected': '0.029155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017994', 'logps_train/rejected': '-123.44', 'logps_train/chosen': '-143.56', 'loss/train': '0.71079', 'examples_per_second': '31.634', 'grad_norm': '34.5', 'counters/examples': 47552, 'counters/updates': 1486}
train stats after 47584 examples: {'rewards_train/chosen': '0.079895', 'rewards_train/rejected': '0.044613', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035282', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-151.18', 'loss/train': '0.6808', 'examples_per_second': '30.096', 'grad_norm': '35', 'counters/examples': 47584, 'counters/updates': 1487}
skipping logging after 47616 examples to avoid logging too frequently
train stats after 47648 examples: {'rewards_train/chosen': '0.033158', 'rewards_train/rejected': '0.0027474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030411', 'logps_train/rejected': '-99.381', 'logps_train/chosen': '-112.95', 'loss/train': '0.68153', 'examples_per_second': '32.908', 'grad_norm': '23.25', 'counters/examples': 47648, 'counters/updates': 1489}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47712 examples: {'rewards_train/chosen': '0.0050251', 'rewards_train/rejected': '0.025533', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020508', 'logps_train/rejected': '-110.91', 'logps_train/chosen': '-132.88', 'loss/train': '0.70934', 'examples_per_second': '38.235', 'grad_norm': '26.75', 'counters/examples': 47712, 'counters/updates': 1491}
train stats after 47744 examples: {'rewards_train/chosen': '0.026402', 'rewards_train/rejected': '0.068902', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0425', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-120.1', 'loss/train': '0.72087', 'examples_per_second': '31.554', 'grad_norm': '32.25', 'counters/examples': 47744, 'counters/updates': 1492}
skipping logging after 47776 examples to avoid logging too frequently
train stats after 47808 examples: {'rewards_train/chosen': '0.028262', 'rewards_train/rejected': '0.017581', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.01068', 'logps_train/rejected': '-166.48', 'logps_train/chosen': '-146.86', 'loss/train': '0.6954', 'examples_per_second': '30.397', 'grad_norm': '32.25', 'counters/examples': 47808, 'counters/updates': 1494}
train stats after 47840 examples: {'rewards_train/chosen': '0.06968', 'rewards_train/rejected': '-0.015312', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084992', 'logps_train/rejected': '-140.03', 'logps_train/chosen': '-114.37', 'loss/train': '0.65882', 'examples_per_second': '30.003', 'grad_norm': '39.5', 'counters/examples': 47840, 'counters/updates': 1495}
skipping logging after 47872 examples to avoid logging too frequently
train stats after 47904 examples: {'rewards_train/chosen': '0.016526', 'rewards_train/rejected': '0.052063', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035537', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-163.66', 'loss/train': '0.71742', 'examples_per_second': '30.176', 'grad_norm': '32.25', 'counters/examples': 47904, 'counters/updates': 1497}
train stats after 47936 examples: {'rewards_train/chosen': '-0.0016875', 'rewards_train/rejected': '0.028353', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.030041', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-133.88', 'loss/train': '0.71332', 'examples_per_second': '31.628', 'grad_norm': '28.375', 'counters/examples': 47936, 'counters/updates': 1498}
train stats after 47968 examples: {'rewards_train/chosen': '0.025252', 'rewards_train/rejected': '-0.049097', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07435', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-157.52', 'loss/train': '0.65985', 'examples_per_second': '30.536', 'grad_norm': '28.125', 'counters/examples': 47968, 'counters/updates': 1499}
skipping logging after 48000 examples to avoid logging too frequently
Running evaluation after 48000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.21it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 48000: {'rewards_eval/chosen': '0.043785', 'rewards_eval/rejected': '0.010637', 'rewards_eval/accuracies': '0.55859', 'rewards_eval/margins': '0.033148', 'logps_eval/rejected': '-118.51', 'logps_eval/chosen': '-139', 'loss/eval': '0.68305'}
skipping save for non epoch
train stats after 48032 examples: {'rewards_train/chosen': '0.077946', 'rewards_train/rejected': '-0.0036048', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081551', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-128.46', 'loss/train': '0.6608', 'examples_per_second': '38.203', 'grad_norm': '31.75', 'counters/examples': 48032, 'counters/updates': 1501}
train stats after 48064 examples: {'rewards_train/chosen': '0.0060407', 'rewards_train/rejected': '0.025764', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019724', 'logps_train/rejected': '-108.46', 'logps_train/chosen': '-166.8', 'loss/train': '0.71313', 'examples_per_second': '32.155', 'grad_norm': '46.25', 'counters/examples': 48064, 'counters/updates': 1502}
skipping logging after 48096 examples to avoid logging too frequently
train stats after 48128 examples: {'rewards_train/chosen': '0.035545', 'rewards_train/rejected': '-0.016028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051572', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-128.83', 'loss/train': '0.6704', 'examples_per_second': '31.762', 'grad_norm': '26.75', 'counters/examples': 48128, 'counters/updates': 1504}
train stats after 48160 examples: {'rewards_train/chosen': '0.021247', 'rewards_train/rejected': '0.002801', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018446', 'logps_train/rejected': '-100.67', 'logps_train/chosen': '-132.62', 'loss/train': '0.68758', 'examples_per_second': '30.967', 'grad_norm': '24.5', 'counters/examples': 48160, 'counters/updates': 1505}
train stats after 48192 examples: {'rewards_train/chosen': '0.034102', 'rewards_train/rejected': '0.035991', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0018893', 'logps_train/rejected': '-99.351', 'logps_train/chosen': '-110.95', 'loss/train': '0.69986', 'examples_per_second': '31.376', 'grad_norm': '24', 'counters/examples': 48192, 'counters/updates': 1506}
train stats after 48224 examples: {'rewards_train/chosen': '0.018473', 'rewards_train/rejected': '-0.0011093', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019583', 'logps_train/rejected': '-126.67', 'logps_train/chosen': '-108.09', 'loss/train': '0.68652', 'examples_per_second': '31.045', 'grad_norm': '26.625', 'counters/examples': 48224, 'counters/updates': 1507}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48288 examples: {'rewards_train/chosen': '-0.020239', 'rewards_train/rejected': '0.014004', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.034243', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-127.34', 'loss/train': '0.71903', 'examples_per_second': '31.274', 'grad_norm': '30.375', 'counters/examples': 48288, 'counters/updates': 1509}
train stats after 48320 examples: {'rewards_train/chosen': '0.059559', 'rewards_train/rejected': '-0.047264', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10682', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-132.29', 'loss/train': '0.64741', 'examples_per_second': '30.609', 'grad_norm': '25.375', 'counters/examples': 48320, 'counters/updates': 1510}
train stats after 48352 examples: {'rewards_train/chosen': '0.041519', 'rewards_train/rejected': '-0.011965', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053484', 'logps_train/rejected': '-74.295', 'logps_train/chosen': '-135.96', 'loss/train': '0.67101', 'examples_per_second': '31.258', 'grad_norm': '25.5', 'counters/examples': 48352, 'counters/updates': 1511}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48416 examples: {'rewards_train/chosen': '0.013471', 'rewards_train/rejected': '0.019817', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.006346', 'logps_train/rejected': '-121.7', 'logps_train/chosen': '-123.23', 'loss/train': '0.69901', 'examples_per_second': '32.559', 'grad_norm': '29.875', 'counters/examples': 48416, 'counters/updates': 1513}
train stats after 48448 examples: {'rewards_train/chosen': '0.10224', 'rewards_train/rejected': '0.0075341', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094705', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-151.99', 'loss/train': '0.65509', 'examples_per_second': '31.639', 'grad_norm': '28.125', 'counters/examples': 48448, 'counters/updates': 1514}
train stats after 48480 examples: {'rewards_train/chosen': '0.03268', 'rewards_train/rejected': '0.045445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012766', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-162.88', 'loss/train': '0.70308', 'examples_per_second': '31.634', 'grad_norm': '28.125', 'counters/examples': 48480, 'counters/updates': 1515}
train stats after 48512 examples: {'rewards_train/chosen': '0.056506', 'rewards_train/rejected': '-0.01735', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073855', 'logps_train/rejected': '-126.07', 'logps_train/chosen': '-125.16', 'loss/train': '0.66426', 'examples_per_second': '31.324', 'grad_norm': '26.75', 'counters/examples': 48512, 'counters/updates': 1516}
skipping logging after 48544 examples to avoid logging too frequently
train stats after 48576 examples: {'rewards_train/chosen': '0.052367', 'rewards_train/rejected': '-0.00712', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059487', 'logps_train/rejected': '-153.31', 'logps_train/chosen': '-148.78', 'loss/train': '0.66972', 'examples_per_second': '31.649', 'grad_norm': '31.5', 'counters/examples': 48576, 'counters/updates': 1518}
skipping logging after 48608 examples to avoid logging too frequently
train stats after 48640 examples: {'rewards_train/chosen': '0.0064527', 'rewards_train/rejected': '-0.0020543', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0085069', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-119.96', 'loss/train': '0.69425', 'examples_per_second': '32.079', 'grad_norm': '29.125', 'counters/examples': 48640, 'counters/updates': 1520}
train stats after 48672 examples: {'rewards_train/chosen': '0.052456', 'rewards_train/rejected': '0.0060976', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046358', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-134.67', 'loss/train': '0.67441', 'examples_per_second': '31.642', 'grad_norm': '31', 'counters/examples': 48672, 'counters/updates': 1521}
train stats after 48704 examples: {'rewards_train/chosen': '0.031072', 'rewards_train/rejected': '-0.033092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064164', 'logps_train/rejected': '-91.276', 'logps_train/chosen': '-127.68', 'loss/train': '0.66639', 'examples_per_second': '31.666', 'grad_norm': '22.25', 'counters/examples': 48704, 'counters/updates': 1522}
skipping logging after 48736 examples to avoid logging too frequently
train stats after 48768 examples: {'rewards_train/chosen': '0.044229', 'rewards_train/rejected': '0.020792', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023438', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-161.26', 'loss/train': '0.68621', 'examples_per_second': '32.296', 'grad_norm': '28.75', 'counters/examples': 48768, 'counters/updates': 1524}
train stats after 48800 examples: {'rewards_train/chosen': '0.015743', 'rewards_train/rejected': '0.016706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00096303', 'logps_train/rejected': '-142.54', 'logps_train/chosen': '-168.23', 'loss/train': '0.69977', 'examples_per_second': '31.64', 'grad_norm': '29.625', 'counters/examples': 48800, 'counters/updates': 1525}
train stats after 48832 examples: {'rewards_train/chosen': '0.12215', 'rewards_train/rejected': '0.034227', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087924', 'logps_train/rejected': '-150.49', 'logps_train/chosen': '-161.5', 'loss/train': '0.65741', 'examples_per_second': '31.552', 'grad_norm': '34', 'counters/examples': 48832, 'counters/updates': 1526}
train stats after 48864 examples: {'rewards_train/chosen': '0.039136', 'rewards_train/rejected': '-0.015709', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054845', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-134.61', 'loss/train': '0.66809', 'examples_per_second': '31.779', 'grad_norm': '26.5', 'counters/examples': 48864, 'counters/updates': 1527}
train stats after 48896 examples: {'rewards_train/chosen': '-0.042871', 'rewards_train/rejected': '0.025685', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.068557', 'logps_train/rejected': '-143.06', 'logps_train/chosen': '-153.64', 'loss/train': '0.73727', 'examples_per_second': '31.645', 'grad_norm': '34.75', 'counters/examples': 48896, 'counters/updates': 1528}
train stats after 48928 examples: {'rewards_train/chosen': '-0.050532', 'rewards_train/rejected': '0.027152', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.077684', 'logps_train/rejected': '-127.14', 'logps_train/chosen': '-178.22', 'loss/train': '0.7413', 'examples_per_second': '31.552', 'grad_norm': '33.75', 'counters/examples': 48928, 'counters/updates': 1529}
train stats after 48960 examples: {'rewards_train/chosen': '0.086389', 'rewards_train/rejected': '-0.010198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096586', 'logps_train/rejected': '-95.952', 'logps_train/chosen': '-152.17', 'loss/train': '0.65058', 'examples_per_second': '32.645', 'grad_norm': '23.875', 'counters/examples': 48960, 'counters/updates': 1530}
train stats after 48992 examples: {'rewards_train/chosen': '0.053209', 'rewards_train/rejected': '0.025193', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028015', 'logps_train/rejected': '-141.42', 'logps_train/chosen': '-106.17', 'loss/train': '0.68797', 'examples_per_second': '31.393', 'grad_norm': '36.5', 'counters/examples': 48992, 'counters/updates': 1531}
train stats after 49024 examples: {'rewards_train/chosen': '0.061574', 'rewards_train/rejected': '-0.019832', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081406', 'logps_train/rejected': '-106.96', 'logps_train/chosen': '-126.24', 'loss/train': '0.66077', 'examples_per_second': '31.926', 'grad_norm': '24.125', 'counters/examples': 49024, 'counters/updates': 1532}
skipping logging after 49056 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '0.016097', 'rewards_train/rejected': '-0.004308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020405', 'logps_train/rejected': '-108.4', 'logps_train/chosen': '-127.54', 'loss/train': '0.68648', 'examples_per_second': '31.51', 'grad_norm': '27.375', 'counters/examples': 49088, 'counters/updates': 1534}
train stats after 49120 examples: {'rewards_train/chosen': '-0.023491', 'rewards_train/rejected': '-0.015612', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0078797', 'logps_train/rejected': '-125.19', 'logps_train/chosen': '-142.88', 'loss/train': '0.6998', 'examples_per_second': '32.077', 'grad_norm': '26.125', 'counters/examples': 49120, 'counters/updates': 1535}
train stats after 49152 examples: {'rewards_train/chosen': '0.024639', 'rewards_train/rejected': '0.010936', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.013703', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-152.66', 'loss/train': '0.68988', 'examples_per_second': '31.345', 'grad_norm': '27.125', 'counters/examples': 49152, 'counters/updates': 1536}
train stats after 49184 examples: {'rewards_train/chosen': '0.035861', 'rewards_train/rejected': '0.0028936', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032968', 'logps_train/rejected': '-157.46', 'logps_train/chosen': '-176.13', 'loss/train': '0.68094', 'examples_per_second': '31.594', 'grad_norm': '30.25', 'counters/examples': 49184, 'counters/updates': 1537}
train stats after 49216 examples: {'rewards_train/chosen': '0.014723', 'rewards_train/rejected': '0.011683', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0030399', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-176.64', 'loss/train': '0.70002', 'examples_per_second': '31.649', 'grad_norm': '32.5', 'counters/examples': 49216, 'counters/updates': 1538}
train stats after 49248 examples: {'rewards_train/chosen': '-0.0064922', 'rewards_train/rejected': '-0.0033465', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0031457', 'logps_train/rejected': '-92.642', 'logps_train/chosen': '-159.96', 'loss/train': '0.69979', 'examples_per_second': '33.117', 'grad_norm': '36.75', 'counters/examples': 49248, 'counters/updates': 1539}
train stats after 49280 examples: {'rewards_train/chosen': '0.092941', 'rewards_train/rejected': '0.033806', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059135', 'logps_train/rejected': '-98.729', 'logps_train/chosen': '-126.35', 'loss/train': '0.6697', 'examples_per_second': '31.205', 'grad_norm': '29', 'counters/examples': 49280, 'counters/updates': 1540}
train stats after 49312 examples: {'rewards_train/chosen': '0.059275', 'rewards_train/rejected': '0.044781', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014493', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-141.16', 'loss/train': '0.6913', 'examples_per_second': '31.432', 'grad_norm': '36.5', 'counters/examples': 49312, 'counters/updates': 1541}
train stats after 49344 examples: {'rewards_train/chosen': '0.053186', 'rewards_train/rejected': '0.052884', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00030201', 'logps_train/rejected': '-103.63', 'logps_train/chosen': '-148.39', 'loss/train': '0.69773', 'examples_per_second': '30.436', 'grad_norm': '28', 'counters/examples': 49344, 'counters/updates': 1542}
train stats after 49376 examples: {'rewards_train/chosen': '0.056013', 'rewards_train/rejected': '0.045052', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010962', 'logps_train/rejected': '-150.65', 'logps_train/chosen': '-168.29', 'loss/train': '0.69592', 'examples_per_second': '32.513', 'grad_norm': '31.25', 'counters/examples': 49376, 'counters/updates': 1543}
train stats after 49408 examples: {'rewards_train/chosen': '0.037089', 'rewards_train/rejected': '-0.0035824', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040672', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-148.83', 'loss/train': '0.67719', 'examples_per_second': '31.631', 'grad_norm': '27.875', 'counters/examples': 49408, 'counters/updates': 1544}
train stats after 49440 examples: {'rewards_train/chosen': '-0.011802', 'rewards_train/rejected': '0.036088', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047891', 'logps_train/rejected': '-102.4', 'logps_train/chosen': '-136.82', 'loss/train': '0.7233', 'examples_per_second': '32.564', 'grad_norm': '27.25', 'counters/examples': 49440, 'counters/updates': 1545}
skipping logging after 49472 examples to avoid logging too frequently
train stats after 49504 examples: {'rewards_train/chosen': '0.036718', 'rewards_train/rejected': '0.047937', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011219', 'logps_train/rejected': '-177.23', 'logps_train/chosen': '-128.62', 'loss/train': '0.70236', 'examples_per_second': '31.558', 'grad_norm': '32', 'counters/examples': 49504, 'counters/updates': 1547}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49568 examples: {'rewards_train/chosen': '0.032915', 'rewards_train/rejected': '0.0050457', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027869', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-153.79', 'loss/train': '0.68507', 'examples_per_second': '31.636', 'grad_norm': '29.375', 'counters/examples': 49568, 'counters/updates': 1549}
train stats after 49600 examples: {'rewards_train/chosen': '0.013587', 'rewards_train/rejected': '-0.015715', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029302', 'logps_train/rejected': '-103.59', 'logps_train/chosen': '-136.18', 'loss/train': '0.68378', 'examples_per_second': '30.066', 'grad_norm': '30.25', 'counters/examples': 49600, 'counters/updates': 1550}
skipping logging after 49632 examples to avoid logging too frequently
train stats after 49664 examples: {'rewards_train/chosen': '0.011293', 'rewards_train/rejected': '0.028585', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.017292', 'logps_train/rejected': '-105.07', 'logps_train/chosen': '-126.43', 'loss/train': '0.71039', 'examples_per_second': '34.43', 'grad_norm': '27.5', 'counters/examples': 49664, 'counters/updates': 1552}
skipping logging after 49696 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '0.027816', 'rewards_train/rejected': '0.02483', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0029853', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-126.3', 'loss/train': '0.694', 'examples_per_second': '31.575', 'grad_norm': '28.375', 'counters/examples': 49728, 'counters/updates': 1554}
skipping logging after 49760 examples to avoid logging too frequently
train stats after 49792 examples: {'rewards_train/chosen': '0.064333', 'rewards_train/rejected': '-0.010444', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074777', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-165.66', 'loss/train': '0.66247', 'examples_per_second': '31.564', 'grad_norm': '26.375', 'counters/examples': 49792, 'counters/updates': 1556}
train stats after 49824 examples: {'rewards_train/chosen': '0.045736', 'rewards_train/rejected': '-0.0027154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048451', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-133.48', 'loss/train': '0.67829', 'examples_per_second': '31.233', 'grad_norm': '25.125', 'counters/examples': 49824, 'counters/updates': 1557}
skipping logging after 49856 examples to avoid logging too frequently
train stats after 49888 examples: {'rewards_train/chosen': '0.005904', 'rewards_train/rejected': '0.031695', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025791', 'logps_train/rejected': '-132.17', 'logps_train/chosen': '-134.99', 'loss/train': '0.7095', 'examples_per_second': '32.132', 'grad_norm': '27', 'counters/examples': 49888, 'counters/updates': 1559}
train stats after 49920 examples: {'rewards_train/chosen': '0.10741', 'rewards_train/rejected': '-0.0052367', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11265', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-151.66', 'loss/train': '0.64198', 'examples_per_second': '29.917', 'grad_norm': '24.25', 'counters/examples': 49920, 'counters/updates': 1560}
train stats after 49952 examples: {'rewards_train/chosen': '0.017562', 'rewards_train/rejected': '-0.00056635', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018129', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-129.76', 'loss/train': '0.6888', 'examples_per_second': '31.82', 'grad_norm': '31.75', 'counters/examples': 49952, 'counters/updates': 1561}
train stats after 49984 examples: {'rewards_train/chosen': '0.078729', 'rewards_train/rejected': '0.027089', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05164', 'logps_train/rejected': '-96.439', 'logps_train/chosen': '-132.06', 'loss/train': '0.67531', 'examples_per_second': '31.417', 'grad_norm': '26', 'counters/examples': 49984, 'counters/updates': 1562}
train stats after 50016 examples: {'rewards_train/chosen': '0.048791', 'rewards_train/rejected': '0.056386', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0075955', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-108.71', 'loss/train': '0.70148', 'examples_per_second': '31.513', 'grad_norm': '26.875', 'counters/examples': 50016, 'counters/updates': 1563}
train stats after 50048 examples: {'rewards_train/chosen': '-0.022627', 'rewards_train/rejected': '-0.0093171', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01331', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-144.57', 'loss/train': '0.70487', 'examples_per_second': '31.623', 'grad_norm': '30.75', 'counters/examples': 50048, 'counters/updates': 1564}
train stats after 50080 examples: {'rewards_train/chosen': '0.092934', 'rewards_train/rejected': '0.0096173', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083317', 'logps_train/rejected': '-130.67', 'logps_train/chosen': '-149.96', 'loss/train': '0.65888', 'examples_per_second': '31.629', 'grad_norm': '28.625', 'counters/examples': 50080, 'counters/updates': 1565}
train stats after 50112 examples: {'rewards_train/chosen': '0.010963', 'rewards_train/rejected': '0.035887', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024924', 'logps_train/rejected': '-140.35', 'logps_train/chosen': '-164.1', 'loss/train': '0.70977', 'examples_per_second': '30.134', 'grad_norm': '29.25', 'counters/examples': 50112, 'counters/updates': 1566}
train stats after 50144 examples: {'rewards_train/chosen': '0.011274', 'rewards_train/rejected': '-0.041503', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052777', 'logps_train/rejected': '-135.11', 'logps_train/chosen': '-118.94', 'loss/train': '0.67074', 'examples_per_second': '31.657', 'grad_norm': '25.75', 'counters/examples': 50144, 'counters/updates': 1567}
train stats after 50176 examples: {'rewards_train/chosen': '0.084924', 'rewards_train/rejected': '0.023315', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061609', 'logps_train/rejected': '-108.14', 'logps_train/chosen': '-126.81', 'loss/train': '0.66998', 'examples_per_second': '32.827', 'grad_norm': '25.5', 'counters/examples': 50176, 'counters/updates': 1568}
skipping logging after 50208 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '0.039639', 'rewards_train/rejected': '-0.008392', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048031', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-124.07', 'loss/train': '0.67796', 'examples_per_second': '31.881', 'grad_norm': '26.875', 'counters/examples': 50240, 'counters/updates': 1570}
skipping logging after 50272 examples to avoid logging too frequently
train stats after 50304 examples: {'rewards_train/chosen': '-0.025002', 'rewards_train/rejected': '-0.0031627', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.021839', 'logps_train/rejected': '-103.83', 'logps_train/chosen': '-156.15', 'loss/train': '0.70864', 'examples_per_second': '31.178', 'grad_norm': '28.25', 'counters/examples': 50304, 'counters/updates': 1572}
train stats after 50336 examples: {'rewards_train/chosen': '0.06209', 'rewards_train/rejected': '0.0039161', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058174', 'logps_train/rejected': '-108.53', 'logps_train/chosen': '-163.66', 'loss/train': '0.67104', 'examples_per_second': '31.296', 'grad_norm': '30', 'counters/examples': 50336, 'counters/updates': 1573}
skipping logging after 50368 examples to avoid logging too frequently
train stats after 50400 examples: {'rewards_train/chosen': '0.018513', 'rewards_train/rejected': '0.022455', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0039422', 'logps_train/rejected': '-103.89', 'logps_train/chosen': '-117.84', 'loss/train': '0.69681', 'examples_per_second': '31.383', 'grad_norm': '27.25', 'counters/examples': 50400, 'counters/updates': 1575}
train stats after 50432 examples: {'rewards_train/chosen': '0.036502', 'rewards_train/rejected': '0.027064', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0094378', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-150.08', 'loss/train': '0.69466', 'examples_per_second': '31.488', 'grad_norm': '33.25', 'counters/examples': 50432, 'counters/updates': 1576}
train stats after 50464 examples: {'rewards_train/chosen': '0.048829', 'rewards_train/rejected': '0.061443', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012614', 'logps_train/rejected': '-143.99', 'logps_train/chosen': '-133.31', 'loss/train': '0.70335', 'examples_per_second': '31.293', 'grad_norm': '28.75', 'counters/examples': 50464, 'counters/updates': 1577}
train stats after 50496 examples: {'rewards_train/chosen': '0.057302', 'rewards_train/rejected': '-0.020503', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077805', 'logps_train/rejected': '-163.79', 'logps_train/chosen': '-169.78', 'loss/train': '0.65955', 'examples_per_second': '31.61', 'grad_norm': '33.75', 'counters/examples': 50496, 'counters/updates': 1578}
train stats after 50528 examples: {'rewards_train/chosen': '0.064343', 'rewards_train/rejected': '0.025048', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039295', 'logps_train/rejected': '-135.34', 'logps_train/chosen': '-131.75', 'loss/train': '0.6792', 'examples_per_second': '32.804', 'grad_norm': '26.25', 'counters/examples': 50528, 'counters/updates': 1579}
train stats after 50560 examples: {'rewards_train/chosen': '0.0462', 'rewards_train/rejected': '0.021881', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024319', 'logps_train/rejected': '-96.995', 'logps_train/chosen': '-103.73', 'loss/train': '0.68665', 'examples_per_second': '32.692', 'grad_norm': '21.375', 'counters/examples': 50560, 'counters/updates': 1580}
skipping logging after 50592 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '0.098998', 'rewards_train/rejected': '-0.0048534', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10385', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-119.07', 'loss/train': '0.64737', 'examples_per_second': '32.572', 'grad_norm': '22.75', 'counters/examples': 50624, 'counters/updates': 1582}
train stats after 50656 examples: {'rewards_train/chosen': '0.044436', 'rewards_train/rejected': '0.060603', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016167', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-127.48', 'loss/train': '0.70475', 'examples_per_second': '30.563', 'grad_norm': '25.375', 'counters/examples': 50656, 'counters/updates': 1583}
train stats after 50688 examples: {'rewards_train/chosen': '-0.00055513', 'rewards_train/rejected': '0.046863', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047418', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-139.51', 'loss/train': '0.72196', 'examples_per_second': '31.476', 'grad_norm': '29.5', 'counters/examples': 50688, 'counters/updates': 1584}
train stats after 50720 examples: {'rewards_train/chosen': '0.073301', 'rewards_train/rejected': '0.04264', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030661', 'logps_train/rejected': '-147.85', 'logps_train/chosen': '-144.24', 'loss/train': '0.68507', 'examples_per_second': '31.82', 'grad_norm': '29.625', 'counters/examples': 50720, 'counters/updates': 1585}
train stats after 50752 examples: {'rewards_train/chosen': '0.0012193', 'rewards_train/rejected': '0.02677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.02555', 'logps_train/rejected': '-131.35', 'logps_train/chosen': '-125.51', 'loss/train': '0.71865', 'examples_per_second': '31.427', 'grad_norm': '31.5', 'counters/examples': 50752, 'counters/updates': 1586}
train stats after 50784 examples: {'rewards_train/chosen': '0.025182', 'rewards_train/rejected': '0.027445', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.002263', 'logps_train/rejected': '-135.59', 'logps_train/chosen': '-134.46', 'loss/train': '0.69927', 'examples_per_second': '31.2', 'grad_norm': '28', 'counters/examples': 50784, 'counters/updates': 1587}
train stats after 50816 examples: {'rewards_train/chosen': '0.0087039', 'rewards_train/rejected': '0.04427', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035566', 'logps_train/rejected': '-119.46', 'logps_train/chosen': '-150.77', 'loss/train': '0.71432', 'examples_per_second': '31.023', 'grad_norm': '27.625', 'counters/examples': 50816, 'counters/updates': 1588}
train stats after 50848 examples: {'rewards_train/chosen': '0.068581', 'rewards_train/rejected': '0.013848', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054733', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-163.11', 'loss/train': '0.66912', 'examples_per_second': '31.606', 'grad_norm': '28.75', 'counters/examples': 50848, 'counters/updates': 1589}
train stats after 50880 examples: {'rewards_train/chosen': '0.012932', 'rewards_train/rejected': '-0.043096', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056028', 'logps_train/rejected': '-132.24', 'logps_train/chosen': '-138.07', 'loss/train': '0.67543', 'examples_per_second': '30.98', 'grad_norm': '25.625', 'counters/examples': 50880, 'counters/updates': 1590}
train stats after 50912 examples: {'rewards_train/chosen': '-0.028246', 'rewards_train/rejected': '0.019432', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047677', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-131.95', 'loss/train': '0.72397', 'examples_per_second': '30.41', 'grad_norm': '33.5', 'counters/examples': 50912, 'counters/updates': 1591}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 50976 examples: {'rewards_train/chosen': '0.06403', 'rewards_train/rejected': '0.0010203', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06301', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-187.48', 'loss/train': '0.66535', 'examples_per_second': '31.713', 'grad_norm': '25.5', 'counters/examples': 50976, 'counters/updates': 1593}
skipping logging after 51008 examples to avoid logging too frequently
train stats after 51040 examples: {'rewards_train/chosen': '0.024476', 'rewards_train/rejected': '0.04709', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022614', 'logps_train/rejected': '-113.68', 'logps_train/chosen': '-151.42', 'loss/train': '0.71031', 'examples_per_second': '31.489', 'grad_norm': '28.875', 'counters/examples': 51040, 'counters/updates': 1595}
train stats after 51072 examples: {'rewards_train/chosen': '0.025371', 'rewards_train/rejected': '-0.010343', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035714', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-147.36', 'loss/train': '0.68146', 'examples_per_second': '31.171', 'grad_norm': '30.75', 'counters/examples': 51072, 'counters/updates': 1596}
train stats after 51104 examples: {'rewards_train/chosen': '-0.011262', 'rewards_train/rejected': '0.00089233', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012154', 'logps_train/rejected': '-149.39', 'logps_train/chosen': '-184.18', 'loss/train': '0.70304', 'examples_per_second': '30.668', 'grad_norm': '31.25', 'counters/examples': 51104, 'counters/updates': 1597}
train stats after 51136 examples: {'rewards_train/chosen': '0.06419', 'rewards_train/rejected': '0.011806', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052384', 'logps_train/rejected': '-90.408', 'logps_train/chosen': '-111.92', 'loss/train': '0.6728', 'examples_per_second': '31.593', 'grad_norm': '24.625', 'counters/examples': 51136, 'counters/updates': 1598}
train stats after 51168 examples: {'rewards_train/chosen': '0.055641', 'rewards_train/rejected': '0.0084124', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047229', 'logps_train/rejected': '-140.12', 'logps_train/chosen': '-156.35', 'loss/train': '0.67546', 'examples_per_second': '30.932', 'grad_norm': '30', 'counters/examples': 51168, 'counters/updates': 1599}
train stats after 51200 examples: {'rewards_train/chosen': '0.032104', 'rewards_train/rejected': '0.016611', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015493', 'logps_train/rejected': '-99.135', 'logps_train/chosen': '-137.24', 'loss/train': '0.69173', 'examples_per_second': '30.575', 'grad_norm': '28.25', 'counters/examples': 51200, 'counters/updates': 1600}
train stats after 51232 examples: {'rewards_train/chosen': '0.040856', 'rewards_train/rejected': '-0.011025', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051882', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-150.01', 'loss/train': '0.67223', 'examples_per_second': '31.579', 'grad_norm': '28', 'counters/examples': 51232, 'counters/updates': 1601}
skipping logging after 51264 examples to avoid logging too frequently
train stats after 51296 examples: {'rewards_train/chosen': '0.10894', 'rewards_train/rejected': '0.027919', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081016', 'logps_train/rejected': '-131.43', 'logps_train/chosen': '-138.11', 'loss/train': '0.6626', 'examples_per_second': '31.576', 'grad_norm': '27.125', 'counters/examples': 51296, 'counters/updates': 1603}
train stats after 51328 examples: {'rewards_train/chosen': '0.044211', 'rewards_train/rejected': '-0.014622', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058834', 'logps_train/rejected': '-142.05', 'logps_train/chosen': '-140.56', 'loss/train': '0.67044', 'examples_per_second': '30.215', 'grad_norm': '29', 'counters/examples': 51328, 'counters/updates': 1604}
train stats after 51360 examples: {'rewards_train/chosen': '0.020107', 'rewards_train/rejected': '-0.016351', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036458', 'logps_train/rejected': '-133.78', 'logps_train/chosen': '-170.65', 'loss/train': '0.6833', 'examples_per_second': '29.917', 'grad_norm': '29.875', 'counters/examples': 51360, 'counters/updates': 1605}
train stats after 51392 examples: {'rewards_train/chosen': '0.029806', 'rewards_train/rejected': '0.017918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011888', 'logps_train/rejected': '-97.058', 'logps_train/chosen': '-130.89', 'loss/train': '0.69074', 'examples_per_second': '30.263', 'grad_norm': '31.625', 'counters/examples': 51392, 'counters/updates': 1606}
train stats after 51424 examples: {'rewards_train/chosen': '0.04359', 'rewards_train/rejected': '0.02541', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01818', 'logps_train/rejected': '-119.07', 'logps_train/chosen': '-109.79', 'loss/train': '0.69003', 'examples_per_second': '32.47', 'grad_norm': '26.25', 'counters/examples': 51424, 'counters/updates': 1607}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51488 examples: {'rewards_train/chosen': '0.041376', 'rewards_train/rejected': '0.048307', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0069318', 'logps_train/rejected': '-78.538', 'logps_train/chosen': '-158.42', 'loss/train': '0.7008', 'examples_per_second': '30.404', 'grad_norm': '26.875', 'counters/examples': 51488, 'counters/updates': 1609}
train stats after 51520 examples: {'rewards_train/chosen': '0.029123', 'rewards_train/rejected': '0.068548', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.039424', 'logps_train/rejected': '-155.55', 'logps_train/chosen': '-132.86', 'loss/train': '0.7166', 'examples_per_second': '31.522', 'grad_norm': '33.5', 'counters/examples': 51520, 'counters/updates': 1610}
train stats after 51552 examples: {'rewards_train/chosen': '0.09663', 'rewards_train/rejected': '0.0059862', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090644', 'logps_train/rejected': '-128.72', 'logps_train/chosen': '-118.32', 'loss/train': '0.65512', 'examples_per_second': '30.294', 'grad_norm': '25', 'counters/examples': 51552, 'counters/updates': 1611}
train stats after 51584 examples: {'rewards_train/chosen': '0.020536', 'rewards_train/rejected': '0.043182', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.022646', 'logps_train/rejected': '-110.16', 'logps_train/chosen': '-136.07', 'loss/train': '0.71175', 'examples_per_second': '31.733', 'grad_norm': '32.25', 'counters/examples': 51584, 'counters/updates': 1612}
train stats after 51616 examples: {'rewards_train/chosen': '0.018924', 'rewards_train/rejected': '0.038827', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019903', 'logps_train/rejected': '-102.09', 'logps_train/chosen': '-140.86', 'loss/train': '0.70854', 'examples_per_second': '32.178', 'grad_norm': '28.75', 'counters/examples': 51616, 'counters/updates': 1613}
train stats after 51648 examples: {'rewards_train/chosen': '0.023551', 'rewards_train/rejected': '-0.0019938', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025545', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-149.07', 'loss/train': '0.68907', 'examples_per_second': '31.596', 'grad_norm': '28.375', 'counters/examples': 51648, 'counters/updates': 1614}
train stats after 51680 examples: {'rewards_train/chosen': '-0.0028213', 'rewards_train/rejected': '0.011213', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014034', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-157.6', 'loss/train': '0.71179', 'examples_per_second': '31.04', 'grad_norm': '31.375', 'counters/examples': 51680, 'counters/updates': 1615}
train stats after 51712 examples: {'rewards_train/chosen': '0.069013', 'rewards_train/rejected': '0.0010448', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067968', 'logps_train/rejected': '-117.24', 'logps_train/chosen': '-173.46', 'loss/train': '0.66866', 'examples_per_second': '31.297', 'grad_norm': '30.375', 'counters/examples': 51712, 'counters/updates': 1616}
train stats after 51744 examples: {'rewards_train/chosen': '0.064275', 'rewards_train/rejected': '-0.013964', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07824', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-124.82', 'loss/train': '0.66474', 'examples_per_second': '31.604', 'grad_norm': '24', 'counters/examples': 51744, 'counters/updates': 1617}
train stats after 51776 examples: {'rewards_train/chosen': '0.070089', 'rewards_train/rejected': '0.042663', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027427', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-120.46', 'loss/train': '0.68468', 'examples_per_second': '30.867', 'grad_norm': '27.875', 'counters/examples': 51776, 'counters/updates': 1618}
train stats after 51808 examples: {'rewards_train/chosen': '0.10871', 'rewards_train/rejected': '0.085651', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02306', 'logps_train/rejected': '-187.1', 'logps_train/chosen': '-173.13', 'loss/train': '0.69077', 'examples_per_second': '30.738', 'grad_norm': '33.5', 'counters/examples': 51808, 'counters/updates': 1619}
train stats after 51840 examples: {'rewards_train/chosen': '0.023038', 'rewards_train/rejected': '-0.00055427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.023592', 'logps_train/rejected': '-97.921', 'logps_train/chosen': '-143.14', 'loss/train': '0.68697', 'examples_per_second': '30.729', 'grad_norm': '31.75', 'counters/examples': 51840, 'counters/updates': 1620}
train stats after 51872 examples: {'rewards_train/chosen': '0.084457', 'rewards_train/rejected': '0.049857', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0346', 'logps_train/rejected': '-118.27', 'logps_train/chosen': '-148.39', 'loss/train': '0.68333', 'examples_per_second': '32.49', 'grad_norm': '27.875', 'counters/examples': 51872, 'counters/updates': 1621}
train stats after 51904 examples: {'rewards_train/chosen': '0.022617', 'rewards_train/rejected': '-0.04686', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069477', 'logps_train/rejected': '-106.98', 'logps_train/chosen': '-126.12', 'loss/train': '0.66474', 'examples_per_second': '25.616', 'grad_norm': '30', 'counters/examples': 51904, 'counters/updates': 1622}
train stats after 51936 examples: {'rewards_train/chosen': '0.076303', 'rewards_train/rejected': '0.027314', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048989', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-128.89', 'loss/train': '0.67647', 'examples_per_second': '30.559', 'grad_norm': '31.875', 'counters/examples': 51936, 'counters/updates': 1623}
train stats after 51968 examples: {'rewards_train/chosen': '0.0127', 'rewards_train/rejected': '0.0013982', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011302', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-151.75', 'loss/train': '0.69258', 'examples_per_second': '30.048', 'grad_norm': '44', 'counters/examples': 51968, 'counters/updates': 1624}
train stats after 52000 examples: {'rewards_train/chosen': '0.023861', 'rewards_train/rejected': '-0.0086636', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032525', 'logps_train/rejected': '-121.07', 'logps_train/chosen': '-150.97', 'loss/train': '0.68234', 'examples_per_second': '25.412', 'grad_norm': '26.875', 'counters/examples': 52000, 'counters/updates': 1625}
skipping logging after 52032 examples to avoid logging too frequently
train stats after 52064 examples: {'rewards_train/chosen': '0.042836', 'rewards_train/rejected': '-0.0079275', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050763', 'logps_train/rejected': '-105.03', 'logps_train/chosen': '-123.93', 'loss/train': '0.67348', 'examples_per_second': '30.938', 'grad_norm': '31.375', 'counters/examples': 52064, 'counters/updates': 1627}
train stats after 52096 examples: {'rewards_train/chosen': '0.047885', 'rewards_train/rejected': '-0.0036753', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051561', 'logps_train/rejected': '-116.5', 'logps_train/chosen': '-171.57', 'loss/train': '0.67295', 'examples_per_second': '31.489', 'grad_norm': '27.5', 'counters/examples': 52096, 'counters/updates': 1628}
train stats after 52128 examples: {'rewards_train/chosen': '0.04321', 'rewards_train/rejected': '0.043651', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0004403', 'logps_train/rejected': '-146.94', 'logps_train/chosen': '-144.11', 'loss/train': '0.69796', 'examples_per_second': '31.38', 'grad_norm': '29.375', 'counters/examples': 52128, 'counters/updates': 1629}
train stats after 52160 examples: {'rewards_train/chosen': '0.038943', 'rewards_train/rejected': '0.0049932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03395', 'logps_train/rejected': '-142.46', 'logps_train/chosen': '-137.47', 'loss/train': '0.67944', 'examples_per_second': '33.018', 'grad_norm': '28.75', 'counters/examples': 52160, 'counters/updates': 1630}
train stats after 52192 examples: {'rewards_train/chosen': '0.021004', 'rewards_train/rejected': '0.0043752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016629', 'logps_train/rejected': '-88.138', 'logps_train/chosen': '-116.85', 'loss/train': '0.6889', 'examples_per_second': '30.94', 'grad_norm': '26.25', 'counters/examples': 52192, 'counters/updates': 1631}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52256 examples: {'rewards_train/chosen': '0.038278', 'rewards_train/rejected': '0.02588', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012398', 'logps_train/rejected': '-115.16', 'logps_train/chosen': '-165.83', 'loss/train': '0.69116', 'examples_per_second': '33.45', 'grad_norm': '28.875', 'counters/examples': 52256, 'counters/updates': 1633}
train stats after 52288 examples: {'rewards_train/chosen': '0.069264', 'rewards_train/rejected': '0.041101', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028163', 'logps_train/rejected': '-166.01', 'logps_train/chosen': '-174.36', 'loss/train': '0.68694', 'examples_per_second': '31.139', 'grad_norm': '33', 'counters/examples': 52288, 'counters/updates': 1634}
train stats after 52320 examples: {'rewards_train/chosen': '0.037639', 'rewards_train/rejected': '0.020353', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017286', 'logps_train/rejected': '-140.3', 'logps_train/chosen': '-127.04', 'loss/train': '0.68986', 'examples_per_second': '30.955', 'grad_norm': '28.125', 'counters/examples': 52320, 'counters/updates': 1635}
train stats after 52352 examples: {'rewards_train/chosen': '0.023432', 'rewards_train/rejected': '-0.019825', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043258', 'logps_train/rejected': '-113.38', 'logps_train/chosen': '-110.69', 'loss/train': '0.67448', 'examples_per_second': '31.619', 'grad_norm': '25.125', 'counters/examples': 52352, 'counters/updates': 1636}
train stats after 52384 examples: {'rewards_train/chosen': '0.031291', 'rewards_train/rejected': '0.039581', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0082904', 'logps_train/rejected': '-96.632', 'logps_train/chosen': '-119.5', 'loss/train': '0.70079', 'examples_per_second': '31.596', 'grad_norm': '26.125', 'counters/examples': 52384, 'counters/updates': 1637}
skipping logging after 52416 examples to avoid logging too frequently
train stats after 52448 examples: {'rewards_train/chosen': '0.01401', 'rewards_train/rejected': '-0.020904', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034914', 'logps_train/rejected': '-107.47', 'logps_train/chosen': '-140.11', 'loss/train': '0.6831', 'examples_per_second': '30.901', 'grad_norm': '29.375', 'counters/examples': 52448, 'counters/updates': 1639}
train stats after 52480 examples: {'rewards_train/chosen': '0.062465', 'rewards_train/rejected': '0.010954', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051511', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-199.23', 'loss/train': '0.67288', 'examples_per_second': '31.749', 'grad_norm': '28.625', 'counters/examples': 52480, 'counters/updates': 1640}
skipping logging after 52512 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '0.044367', 'rewards_train/rejected': '-0.015843', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06021', 'logps_train/rejected': '-105.1', 'logps_train/chosen': '-86.247', 'loss/train': '0.66895', 'examples_per_second': '30.434', 'grad_norm': '25.625', 'counters/examples': 52544, 'counters/updates': 1642}
train stats after 52576 examples: {'rewards_train/chosen': '0.035178', 'rewards_train/rejected': '-0.0041658', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039343', 'logps_train/rejected': '-83.623', 'logps_train/chosen': '-112.29', 'loss/train': '0.67541', 'examples_per_second': '32.294', 'grad_norm': '23.875', 'counters/examples': 52576, 'counters/updates': 1643}
train stats after 52608 examples: {'rewards_train/chosen': '0.055666', 'rewards_train/rejected': '0.030615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025051', 'logps_train/rejected': '-159.11', 'logps_train/chosen': '-141.08', 'loss/train': '0.68691', 'examples_per_second': '31.33', 'grad_norm': '29.5', 'counters/examples': 52608, 'counters/updates': 1644}
skipping logging after 52640 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '0.051311', 'rewards_train/rejected': '0.037956', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013355', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-142.87', 'loss/train': '0.69122', 'examples_per_second': '30.588', 'grad_norm': '30.125', 'counters/examples': 52672, 'counters/updates': 1646}
train stats after 52704 examples: {'rewards_train/chosen': '0.054782', 'rewards_train/rejected': '0.015722', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03906', 'logps_train/rejected': '-121.09', 'logps_train/chosen': '-108.38', 'loss/train': '0.67683', 'examples_per_second': '31.569', 'grad_norm': '31.5', 'counters/examples': 52704, 'counters/updates': 1647}
train stats after 52736 examples: {'rewards_train/chosen': '0.02589', 'rewards_train/rejected': '0.013431', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012459', 'logps_train/rejected': '-110.22', 'logps_train/chosen': '-122.16', 'loss/train': '0.68927', 'examples_per_second': '29.873', 'grad_norm': '25.625', 'counters/examples': 52736, 'counters/updates': 1648}
train stats after 52768 examples: {'rewards_train/chosen': '0.037053', 'rewards_train/rejected': '-0.018457', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05551', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-151.47', 'loss/train': '0.67021', 'examples_per_second': '29.954', 'grad_norm': '29.625', 'counters/examples': 52768, 'counters/updates': 1649}
train stats after 52800 examples: {'rewards_train/chosen': '0.074887', 'rewards_train/rejected': '0.059233', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015653', 'logps_train/rejected': '-146.85', 'logps_train/chosen': '-158.85', 'loss/train': '0.68823', 'examples_per_second': '31.557', 'grad_norm': '32.75', 'counters/examples': 52800, 'counters/updates': 1650}
train stats after 52832 examples: {'rewards_train/chosen': '0.064091', 'rewards_train/rejected': '0.0091498', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054941', 'logps_train/rejected': '-139.96', 'logps_train/chosen': '-162.78', 'loss/train': '0.67822', 'examples_per_second': '30.084', 'grad_norm': '26.75', 'counters/examples': 52832, 'counters/updates': 1651}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52896 examples: {'rewards_train/chosen': '0.077747', 'rewards_train/rejected': '0.034764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042984', 'logps_train/rejected': '-156.26', 'logps_train/chosen': '-169.26', 'loss/train': '0.67756', 'examples_per_second': '31.263', 'grad_norm': '35.5', 'counters/examples': 52896, 'counters/updates': 1653}
train stats after 52928 examples: {'rewards_train/chosen': '0.42063', 'rewards_train/rejected': '-0.024157', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44478', 'logps_train/rejected': '-138.99', 'logps_train/chosen': '-140.92', 'loss/train': '0.65143', 'examples_per_second': '29.504', 'grad_norm': '26.125', 'counters/examples': 52928, 'counters/updates': 1654}
train stats after 52960 examples: {'rewards_train/chosen': '0.031711', 'rewards_train/rejected': '-0.016335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048046', 'logps_train/rejected': '-149.57', 'logps_train/chosen': '-119.4', 'loss/train': '0.67266', 'examples_per_second': '31.413', 'grad_norm': '27', 'counters/examples': 52960, 'counters/updates': 1655}
train stats after 52992 examples: {'rewards_train/chosen': '-0.0033484', 'rewards_train/rejected': '0.013491', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.01684', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-222.28', 'loss/train': '0.70404', 'examples_per_second': '31.43', 'grad_norm': '35', 'counters/examples': 52992, 'counters/updates': 1656}
train stats after 53024 examples: {'rewards_train/chosen': '0.062439', 'rewards_train/rejected': '0.011814', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050624', 'logps_train/rejected': '-143.68', 'logps_train/chosen': '-138.07', 'loss/train': '0.67344', 'examples_per_second': '30.639', 'grad_norm': '29.625', 'counters/examples': 53024, 'counters/updates': 1657}
train stats after 53056 examples: {'rewards_train/chosen': '0.032812', 'rewards_train/rejected': '-0.020961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053773', 'logps_train/rejected': '-94.309', 'logps_train/chosen': '-126.58', 'loss/train': '0.67142', 'examples_per_second': '24.734', 'grad_norm': '23.125', 'counters/examples': 53056, 'counters/updates': 1658}
train stats after 53088 examples: {'rewards_train/chosen': '-0.0062378', 'rewards_train/rejected': '-0.072522', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066284', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-114.37', 'loss/train': '0.66372', 'examples_per_second': '32.563', 'grad_norm': '26.375', 'counters/examples': 53088, 'counters/updates': 1659}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53152 examples: {'rewards_train/chosen': '0.043943', 'rewards_train/rejected': '0.01487', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029073', 'logps_train/rejected': '-89.913', 'logps_train/chosen': '-109.08', 'loss/train': '0.68124', 'examples_per_second': '30.393', 'grad_norm': '25.375', 'counters/examples': 53152, 'counters/updates': 1661}
train stats after 53184 examples: {'rewards_train/chosen': '0.046406', 'rewards_train/rejected': '0.033783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012623', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-113.73', 'loss/train': '0.69128', 'examples_per_second': '31.577', 'grad_norm': '25.625', 'counters/examples': 53184, 'counters/updates': 1662}
train stats after 53216 examples: {'rewards_train/chosen': '0.041878', 'rewards_train/rejected': '-0.00046629', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042344', 'logps_train/rejected': '-87.376', 'logps_train/chosen': '-123.32', 'loss/train': '0.67538', 'examples_per_second': '32.959', 'grad_norm': '27.25', 'counters/examples': 53216, 'counters/updates': 1663}
train stats after 53248 examples: {'rewards_train/chosen': '0.037112', 'rewards_train/rejected': '0.028396', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0087162', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-142.35', 'loss/train': '0.69838', 'examples_per_second': '31.207', 'grad_norm': '28.625', 'counters/examples': 53248, 'counters/updates': 1664}
train stats after 53280 examples: {'rewards_train/chosen': '0.030761', 'rewards_train/rejected': '0.017007', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013755', 'logps_train/rejected': '-159.25', 'logps_train/chosen': '-138.5', 'loss/train': '0.69344', 'examples_per_second': '31.585', 'grad_norm': '29.125', 'counters/examples': 53280, 'counters/updates': 1665}
skipping logging after 53312 examples to avoid logging too frequently
train stats after 53344 examples: {'rewards_train/chosen': '0.065902', 'rewards_train/rejected': '0.0605', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0054026', 'logps_train/rejected': '-165.06', 'logps_train/chosen': '-139.69', 'loss/train': '0.69687', 'examples_per_second': '31.771', 'grad_norm': '30.875', 'counters/examples': 53344, 'counters/updates': 1667}
train stats after 53376 examples: {'rewards_train/chosen': '-0.0089593', 'rewards_train/rejected': '-0.040391', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031432', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-123.09', 'loss/train': '0.68156', 'examples_per_second': '32.293', 'grad_norm': '24.375', 'counters/examples': 53376, 'counters/updates': 1668}
train stats after 53408 examples: {'rewards_train/chosen': '0.054346', 'rewards_train/rejected': '-0.017979', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072325', 'logps_train/rejected': '-91.81', 'logps_train/chosen': '-128.77', 'loss/train': '0.66292', 'examples_per_second': '32.674', 'grad_norm': '25.875', 'counters/examples': 53408, 'counters/updates': 1669}
train stats after 53440 examples: {'rewards_train/chosen': '0.034007', 'rewards_train/rejected': '0.033885', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00012188', 'logps_train/rejected': '-136.51', 'logps_train/chosen': '-129.36', 'loss/train': '0.69557', 'examples_per_second': '31.489', 'grad_norm': '28.875', 'counters/examples': 53440, 'counters/updates': 1670}
train stats after 53472 examples: {'rewards_train/chosen': '-0.0064689', 'rewards_train/rejected': '0.033033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.039502', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-160.2', 'loss/train': '0.71764', 'examples_per_second': '30.576', 'grad_norm': '29.875', 'counters/examples': 53472, 'counters/updates': 1671}
train stats after 53504 examples: {'rewards_train/chosen': '0.048049', 'rewards_train/rejected': '0.0083368', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.039712', 'logps_train/rejected': '-124.66', 'logps_train/chosen': '-120.74', 'loss/train': '0.67802', 'examples_per_second': '30.847', 'grad_norm': '24.5', 'counters/examples': 53504, 'counters/updates': 1672}
skipping logging after 53536 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '0.020816', 'rewards_train/rejected': '-0.050516', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.071332', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-130.44', 'loss/train': '0.66166', 'examples_per_second': '34.048', 'grad_norm': '32.75', 'counters/examples': 53568, 'counters/updates': 1674}
train stats after 53600 examples: {'rewards_train/chosen': '0.011478', 'rewards_train/rejected': '-0.021308', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032786', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-135.43', 'loss/train': '0.68176', 'examples_per_second': '32.553', 'grad_norm': '31.25', 'counters/examples': 53600, 'counters/updates': 1675}
train stats after 53632 examples: {'rewards_train/chosen': '0.076089', 'rewards_train/rejected': '-0.006222', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082311', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-113.38', 'loss/train': '0.65539', 'examples_per_second': '32.368', 'grad_norm': '25', 'counters/examples': 53632, 'counters/updates': 1676}
train stats after 53664 examples: {'rewards_train/chosen': '0.0089044', 'rewards_train/rejected': '0.0029301', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0059744', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-167.42', 'loss/train': '0.69815', 'examples_per_second': '31.274', 'grad_norm': '31.125', 'counters/examples': 53664, 'counters/updates': 1677}
train stats after 53696 examples: {'rewards_train/chosen': '-0.027085', 'rewards_train/rejected': '0.013944', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.041029', 'logps_train/rejected': '-115.87', 'logps_train/chosen': '-132.29', 'loss/train': '0.72001', 'examples_per_second': '30.809', 'grad_norm': '40', 'counters/examples': 53696, 'counters/updates': 1678}
train stats after 53728 examples: {'rewards_train/chosen': '0.039545', 'rewards_train/rejected': '-0.019074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058619', 'logps_train/rejected': '-144.06', 'logps_train/chosen': '-131.64', 'loss/train': '0.67005', 'examples_per_second': '30.686', 'grad_norm': '27.5', 'counters/examples': 53728, 'counters/updates': 1679}
train stats after 53760 examples: {'rewards_train/chosen': '0.11103', 'rewards_train/rejected': '0.030027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081002', 'logps_train/rejected': '-96.71', 'logps_train/chosen': '-166.76', 'loss/train': '0.66177', 'examples_per_second': '31.591', 'grad_norm': '27.625', 'counters/examples': 53760, 'counters/updates': 1680}
train stats after 53792 examples: {'rewards_train/chosen': '0.039881', 'rewards_train/rejected': '0.045166', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0052847', 'logps_train/rejected': '-171.09', 'logps_train/chosen': '-179.35', 'loss/train': '0.707', 'examples_per_second': '32.272', 'grad_norm': '34.75', 'counters/examples': 53792, 'counters/updates': 1681}
train stats after 53824 examples: {'rewards_train/chosen': '0.069174', 'rewards_train/rejected': '0.027122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042052', 'logps_train/rejected': '-151.44', 'logps_train/chosen': '-189.9', 'loss/train': '0.67483', 'examples_per_second': '31.555', 'grad_norm': '29.375', 'counters/examples': 53824, 'counters/updates': 1682}
train stats after 53856 examples: {'rewards_train/chosen': '0.036501', 'rewards_train/rejected': '-0.025935', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062435', 'logps_train/rejected': '-78.478', 'logps_train/chosen': '-152.86', 'loss/train': '0.66634', 'examples_per_second': '31.424', 'grad_norm': '25', 'counters/examples': 53856, 'counters/updates': 1683}
train stats after 53888 examples: {'rewards_train/chosen': '0.053597', 'rewards_train/rejected': '0.024479', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029118', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-146.15', 'loss/train': '0.68461', 'examples_per_second': '31.751', 'grad_norm': '26.875', 'counters/examples': 53888, 'counters/updates': 1684}
skipping logging after 53920 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '0.12068', 'rewards_train/rejected': '0.028935', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091749', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-172.91', 'loss/train': '0.65393', 'examples_per_second': '31.516', 'grad_norm': '27.25', 'counters/examples': 53952, 'counters/updates': 1686}
train stats after 53984 examples: {'rewards_train/chosen': '0.019771', 'rewards_train/rejected': '0.0078069', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011964', 'logps_train/rejected': '-142.96', 'logps_train/chosen': '-150.6', 'loss/train': '0.69394', 'examples_per_second': '30.975', 'grad_norm': '39', 'counters/examples': 53984, 'counters/updates': 1687}
train stats after 54016 examples: {'rewards_train/chosen': '0.016687', 'rewards_train/rejected': '-0.010917', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027604', 'logps_train/rejected': '-111.64', 'logps_train/chosen': '-128.46', 'loss/train': '0.68619', 'examples_per_second': '32.974', 'grad_norm': '25.5', 'counters/examples': 54016, 'counters/updates': 1688}
train stats after 54048 examples: {'rewards_train/chosen': '0.062044', 'rewards_train/rejected': '-0.0051633', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067208', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-135.42', 'loss/train': '0.6731', 'examples_per_second': '31.711', 'grad_norm': '27.375', 'counters/examples': 54048, 'counters/updates': 1689}
train stats after 54080 examples: {'rewards_train/chosen': '0.066716', 'rewards_train/rejected': '0.021324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045392', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-158.84', 'loss/train': '0.68151', 'examples_per_second': '31.546', 'grad_norm': '30.125', 'counters/examples': 54080, 'counters/updates': 1690}
skipping logging after 54112 examples to avoid logging too frequently
train stats after 54144 examples: {'rewards_train/chosen': '0.043834', 'rewards_train/rejected': '0.02432', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019514', 'logps_train/rejected': '-95.3', 'logps_train/chosen': '-156.73', 'loss/train': '0.69195', 'examples_per_second': '30.708', 'grad_norm': '28.875', 'counters/examples': 54144, 'counters/updates': 1692}
train stats after 54176 examples: {'rewards_train/chosen': '0.025375', 'rewards_train/rejected': '0.01756', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0078149', 'logps_train/rejected': '-146.2', 'logps_train/chosen': '-123.78', 'loss/train': '0.70175', 'examples_per_second': '31.389', 'grad_norm': '36.25', 'counters/examples': 54176, 'counters/updates': 1693}
train stats after 54208 examples: {'rewards_train/chosen': '0.0092078', 'rewards_train/rejected': '0.01461', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0054019', 'logps_train/rejected': '-137.18', 'logps_train/chosen': '-123.49', 'loss/train': '0.70471', 'examples_per_second': '32.377', 'grad_norm': '32', 'counters/examples': 54208, 'counters/updates': 1694}
skipping logging after 54240 examples to avoid logging too frequently
train stats after 54272 examples: {'rewards_train/chosen': '0.091115', 'rewards_train/rejected': '-0.010449', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10156', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-166.16', 'loss/train': '0.65347', 'examples_per_second': '34.247', 'grad_norm': '27.875', 'counters/examples': 54272, 'counters/updates': 1696}
train stats after 54304 examples: {'rewards_train/chosen': '-0.034359', 'rewards_train/rejected': '0.0081127', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.042472', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-142.55', 'loss/train': '0.7226', 'examples_per_second': '31.541', 'grad_norm': '30.625', 'counters/examples': 54304, 'counters/updates': 1697}
train stats after 54336 examples: {'rewards_train/chosen': '0.048543', 'rewards_train/rejected': '-0.018722', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067264', 'logps_train/rejected': '-123.71', 'logps_train/chosen': '-149.34', 'loss/train': '0.66502', 'examples_per_second': '31.497', 'grad_norm': '26.75', 'counters/examples': 54336, 'counters/updates': 1698}
train stats after 54368 examples: {'rewards_train/chosen': '0.069334', 'rewards_train/rejected': '0.048954', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020379', 'logps_train/rejected': '-99.469', 'logps_train/chosen': '-177.68', 'loss/train': '0.69028', 'examples_per_second': '30.946', 'grad_norm': '26.5', 'counters/examples': 54368, 'counters/updates': 1699}
train stats after 54400 examples: {'rewards_train/chosen': '0.0099987', 'rewards_train/rejected': '0.008894', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0011048', 'logps_train/rejected': '-131.1', 'logps_train/chosen': '-140.29', 'loss/train': '0.6966', 'examples_per_second': '31.528', 'grad_norm': '27', 'counters/examples': 54400, 'counters/updates': 1700}
train stats after 54432 examples: {'rewards_train/chosen': '0.017229', 'rewards_train/rejected': '0.069207', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.051978', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-186.1', 'loss/train': '0.72634', 'examples_per_second': '31.502', 'grad_norm': '46', 'counters/examples': 54432, 'counters/updates': 1701}
train stats after 54464 examples: {'rewards_train/chosen': '0.086508', 'rewards_train/rejected': '-0.0099958', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096504', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-169.42', 'loss/train': '0.65504', 'examples_per_second': '31.555', 'grad_norm': '29.75', 'counters/examples': 54464, 'counters/updates': 1702}
train stats after 54496 examples: {'rewards_train/chosen': '0.059101', 'rewards_train/rejected': '0.017127', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041974', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-152.99', 'loss/train': '0.67717', 'examples_per_second': '30.018', 'grad_norm': '30.625', 'counters/examples': 54496, 'counters/updates': 1703}
train stats after 54528 examples: {'rewards_train/chosen': '0.033826', 'rewards_train/rejected': '0.091562', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.057736', 'logps_train/rejected': '-161.59', 'logps_train/chosen': '-125.91', 'loss/train': '0.72975', 'examples_per_second': '31.431', 'grad_norm': '34.25', 'counters/examples': 54528, 'counters/updates': 1704}
train stats after 54560 examples: {'rewards_train/chosen': '0.064401', 'rewards_train/rejected': '0.024801', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0396', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-197.68', 'loss/train': '0.6787', 'examples_per_second': '31.53', 'grad_norm': '30.625', 'counters/examples': 54560, 'counters/updates': 1705}
train stats after 54592 examples: {'rewards_train/chosen': '0.05808', 'rewards_train/rejected': '-0.0457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10378', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-159.04', 'loss/train': '0.64857', 'examples_per_second': '30.279', 'grad_norm': '27.375', 'counters/examples': 54592, 'counters/updates': 1706}
train stats after 54624 examples: {'rewards_train/chosen': '0.047898', 'rewards_train/rejected': '0.076212', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028314', 'logps_train/rejected': '-123.92', 'logps_train/chosen': '-140.96', 'loss/train': '0.71059', 'examples_per_second': '31.548', 'grad_norm': '31', 'counters/examples': 54624, 'counters/updates': 1707}
train stats after 54656 examples: {'rewards_train/chosen': '0.086922', 'rewards_train/rejected': '0.048346', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038576', 'logps_train/rejected': '-115.26', 'logps_train/chosen': '-134.13', 'loss/train': '0.68166', 'examples_per_second': '31.569', 'grad_norm': '27', 'counters/examples': 54656, 'counters/updates': 1708}
train stats after 54688 examples: {'rewards_train/chosen': '0.031832', 'rewards_train/rejected': '-0.024372', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056204', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-169.21', 'loss/train': '0.67487', 'examples_per_second': '32.349', 'grad_norm': '31.75', 'counters/examples': 54688, 'counters/updates': 1709}
train stats after 54720 examples: {'rewards_train/chosen': '0.074003', 'rewards_train/rejected': '0.019605', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054398', 'logps_train/rejected': '-112.91', 'logps_train/chosen': '-116.64', 'loss/train': '0.67043', 'examples_per_second': '30.153', 'grad_norm': '30.25', 'counters/examples': 54720, 'counters/updates': 1710}
skipping logging after 54752 examples to avoid logging too frequently
train stats after 54784 examples: {'rewards_train/chosen': '0.026379', 'rewards_train/rejected': '-0.0028006', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02918', 'logps_train/rejected': '-130.85', 'logps_train/chosen': '-147.66', 'loss/train': '0.68992', 'examples_per_second': '32.383', 'grad_norm': '29.875', 'counters/examples': 54784, 'counters/updates': 1712}
train stats after 54816 examples: {'rewards_train/chosen': '0.071352', 'rewards_train/rejected': '-0.0035438', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074895', 'logps_train/rejected': '-173.56', 'logps_train/chosen': '-119', 'loss/train': '0.66512', 'examples_per_second': '32.294', 'grad_norm': '29', 'counters/examples': 54816, 'counters/updates': 1713}
train stats after 54848 examples: {'rewards_train/chosen': '0.05809', 'rewards_train/rejected': '0.027263', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030827', 'logps_train/rejected': '-125.24', 'logps_train/chosen': '-162.43', 'loss/train': '0.68668', 'examples_per_second': '30.914', 'grad_norm': '28.75', 'counters/examples': 54848, 'counters/updates': 1714}
skipping logging after 54880 examples to avoid logging too frequently
train stats after 54912 examples: {'rewards_train/chosen': '0.027931', 'rewards_train/rejected': '0.033231', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0052998', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-155.79', 'loss/train': '0.69995', 'examples_per_second': '33.103', 'grad_norm': '33', 'counters/examples': 54912, 'counters/updates': 1716}
train stats after 54944 examples: {'rewards_train/chosen': '0.053227', 'rewards_train/rejected': '0.009085', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044142', 'logps_train/rejected': '-131.35', 'logps_train/chosen': '-130.22', 'loss/train': '0.67558', 'examples_per_second': '32.372', 'grad_norm': '27.25', 'counters/examples': 54944, 'counters/updates': 1717}
train stats after 54976 examples: {'rewards_train/chosen': '0.11944', 'rewards_train/rejected': '0.053569', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065872', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-145.23', 'loss/train': '0.66966', 'examples_per_second': '31.637', 'grad_norm': '25.5', 'counters/examples': 54976, 'counters/updates': 1718}
skipping logging after 55008 examples to avoid logging too frequently
train stats after 55040 examples: {'rewards_train/chosen': '0.048532', 'rewards_train/rejected': '0.07559', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027058', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-159.22', 'loss/train': '0.71304', 'examples_per_second': '32.688', 'grad_norm': '37.75', 'counters/examples': 55040, 'counters/updates': 1720}
train stats after 55072 examples: {'rewards_train/chosen': '0.019984', 'rewards_train/rejected': '0.0096893', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010295', 'logps_train/rejected': '-86.249', 'logps_train/chosen': '-105.57', 'loss/train': '0.69166', 'examples_per_second': '31.617', 'grad_norm': '26.25', 'counters/examples': 55072, 'counters/updates': 1721}
train stats after 55104 examples: {'rewards_train/chosen': '0.037567', 'rewards_train/rejected': '0.014375', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023192', 'logps_train/rejected': '-104.83', 'logps_train/chosen': '-106.69', 'loss/train': '0.68475', 'examples_per_second': '32.945', 'grad_norm': '24.5', 'counters/examples': 55104, 'counters/updates': 1722}
train stats after 55136 examples: {'rewards_train/chosen': '0.0031865', 'rewards_train/rejected': '-0.033642', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036828', 'logps_train/rejected': '-119.84', 'logps_train/chosen': '-142.97', 'loss/train': '0.68085', 'examples_per_second': '31.512', 'grad_norm': '26.875', 'counters/examples': 55136, 'counters/updates': 1723}
train stats after 55168 examples: {'rewards_train/chosen': '-0.0096306', 'rewards_train/rejected': '0.02946', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.03909', 'logps_train/rejected': '-132', 'logps_train/chosen': '-156.16', 'loss/train': '0.71901', 'examples_per_second': '30.224', 'grad_norm': '30.5', 'counters/examples': 55168, 'counters/updates': 1724}
train stats after 55200 examples: {'rewards_train/chosen': '0.081681', 'rewards_train/rejected': '0.027319', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054362', 'logps_train/rejected': '-93.127', 'logps_train/chosen': '-166.63', 'loss/train': '0.67324', 'examples_per_second': '31.248', 'grad_norm': '25.625', 'counters/examples': 55200, 'counters/updates': 1725}
train stats after 55232 examples: {'rewards_train/chosen': '0.012223', 'rewards_train/rejected': '-0.01093', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023153', 'logps_train/rejected': '-168.91', 'logps_train/chosen': '-164.68', 'loss/train': '0.68716', 'examples_per_second': '31.713', 'grad_norm': '31.125', 'counters/examples': 55232, 'counters/updates': 1726}
train stats after 55264 examples: {'rewards_train/chosen': '0.01931', 'rewards_train/rejected': '0.018389', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00092092', 'logps_train/rejected': '-108.52', 'logps_train/chosen': '-142.85', 'loss/train': '0.69873', 'examples_per_second': '31.023', 'grad_norm': '33.25', 'counters/examples': 55264, 'counters/updates': 1727}
train stats after 55296 examples: {'rewards_train/chosen': '0.056564', 'rewards_train/rejected': '-0.033948', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090512', 'logps_train/rejected': '-98.653', 'logps_train/chosen': '-135.31', 'loss/train': '0.65478', 'examples_per_second': '31.123', 'grad_norm': '27.375', 'counters/examples': 55296, 'counters/updates': 1728}
skipping logging after 55328 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '0.0081945', 'rewards_train/rejected': '0.027608', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019414', 'logps_train/rejected': '-146.75', 'logps_train/chosen': '-147.64', 'loss/train': '0.70995', 'examples_per_second': '29.885', 'grad_norm': '40.5', 'counters/examples': 55360, 'counters/updates': 1730}
train stats after 55392 examples: {'rewards_train/chosen': '0.0537', 'rewards_train/rejected': '0.034127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019573', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-118.83', 'loss/train': '0.68671', 'examples_per_second': '29.94', 'grad_norm': '28.375', 'counters/examples': 55392, 'counters/updates': 1731}
train stats after 55424 examples: {'rewards_train/chosen': '-0.013432', 'rewards_train/rejected': '0.030748', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044179', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-132.44', 'loss/train': '0.72301', 'examples_per_second': '32.457', 'grad_norm': '30.375', 'counters/examples': 55424, 'counters/updates': 1732}
train stats after 55456 examples: {'rewards_train/chosen': '0.11758', 'rewards_train/rejected': '0.085507', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032069', 'logps_train/rejected': '-175.54', 'logps_train/chosen': '-204.42', 'loss/train': '0.68694', 'examples_per_second': '30.566', 'grad_norm': '34.75', 'counters/examples': 55456, 'counters/updates': 1733}
train stats after 55488 examples: {'rewards_train/chosen': '0.10426', 'rewards_train/rejected': '-0.0053793', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10964', 'logps_train/rejected': '-130.97', 'logps_train/chosen': '-153.53', 'loss/train': '0.6458', 'examples_per_second': '30.701', 'grad_norm': '26.25', 'counters/examples': 55488, 'counters/updates': 1734}
skipping logging after 55520 examples to avoid logging too frequently
train stats after 55552 examples: {'rewards_train/chosen': '0.060279', 'rewards_train/rejected': '0.046909', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013369', 'logps_train/rejected': '-157.89', 'logps_train/chosen': '-172.2', 'loss/train': '0.69246', 'examples_per_second': '31.57', 'grad_norm': '30.5', 'counters/examples': 55552, 'counters/updates': 1736}
skipping logging after 55584 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '0.050025', 'rewards_train/rejected': '0.0058301', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044195', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-148.36', 'loss/train': '0.67948', 'examples_per_second': '30.198', 'grad_norm': '31.5', 'counters/examples': 55616, 'counters/updates': 1738}
skipping logging after 55648 examples to avoid logging too frequently
train stats after 55680 examples: {'rewards_train/chosen': '0.023499', 'rewards_train/rejected': '0.017255', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0062441', 'logps_train/rejected': '-151.7', 'logps_train/chosen': '-127.58', 'loss/train': '0.69335', 'examples_per_second': '31.581', 'grad_norm': '29.375', 'counters/examples': 55680, 'counters/updates': 1740}
train stats after 55712 examples: {'rewards_train/chosen': '0.052518', 'rewards_train/rejected': '-0.010117', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062634', 'logps_train/rejected': '-119', 'logps_train/chosen': '-161.71', 'loss/train': '0.66985', 'examples_per_second': '30.47', 'grad_norm': '29.25', 'counters/examples': 55712, 'counters/updates': 1741}
train stats after 55744 examples: {'rewards_train/chosen': '0.042578', 'rewards_train/rejected': '0.042227', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00035146', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-126.81', 'loss/train': '0.69681', 'examples_per_second': '31.682', 'grad_norm': '28', 'counters/examples': 55744, 'counters/updates': 1742}
train stats after 55776 examples: {'rewards_train/chosen': '0.050718', 'rewards_train/rejected': '0.057683', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0069644', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-154.91', 'loss/train': '0.70071', 'examples_per_second': '31.628', 'grad_norm': '28', 'counters/examples': 55776, 'counters/updates': 1743}
train stats after 55808 examples: {'rewards_train/chosen': '0.038829', 'rewards_train/rejected': '-0.021351', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06018', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-147.44', 'loss/train': '0.66837', 'examples_per_second': '31.6', 'grad_norm': '27.625', 'counters/examples': 55808, 'counters/updates': 1744}
skipping logging after 55840 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '0.064252', 'rewards_train/rejected': '0.079985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015732', 'logps_train/rejected': '-147.94', 'logps_train/chosen': '-141.21', 'loss/train': '0.70489', 'examples_per_second': '29.973', 'grad_norm': '30.5', 'counters/examples': 55872, 'counters/updates': 1746}
train stats after 55904 examples: {'rewards_train/chosen': '0.088652', 'rewards_train/rejected': '-0.025018', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11367', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-167.76', 'loss/train': '0.64238', 'examples_per_second': '31.966', 'grad_norm': '28.125', 'counters/examples': 55904, 'counters/updates': 1747}
train stats after 55936 examples: {'rewards_train/chosen': '0.039579', 'rewards_train/rejected': '0.014078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025501', 'logps_train/rejected': '-140.36', 'logps_train/chosen': '-122.75', 'loss/train': '0.6827', 'examples_per_second': '31.258', 'grad_norm': '33.5', 'counters/examples': 55936, 'counters/updates': 1748}
train stats after 55968 examples: {'rewards_train/chosen': '0.077469', 'rewards_train/rejected': '0.00021609', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077253', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-145.65', 'loss/train': '0.65811', 'examples_per_second': '31.148', 'grad_norm': '24.125', 'counters/examples': 55968, 'counters/updates': 1749}
train stats after 56000 examples: {'rewards_train/chosen': '0.035528', 'rewards_train/rejected': '-0.011235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046764', 'logps_train/rejected': '-111.15', 'logps_train/chosen': '-157.77', 'loss/train': '0.67661', 'examples_per_second': '31.58', 'grad_norm': '28.375', 'counters/examples': 56000, 'counters/updates': 1750}
train stats after 56032 examples: {'rewards_train/chosen': '0.004992', 'rewards_train/rejected': '0.016447', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011455', 'logps_train/rejected': '-86.019', 'logps_train/chosen': '-111.82', 'loss/train': '0.70118', 'examples_per_second': '32.489', 'grad_norm': '23.5', 'counters/examples': 56032, 'counters/updates': 1751}
train stats after 56064 examples: {'rewards_train/chosen': '0.058593', 'rewards_train/rejected': '-0.021095', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079688', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-160.23', 'loss/train': '0.65914', 'examples_per_second': '30.16', 'grad_norm': '27.125', 'counters/examples': 56064, 'counters/updates': 1752}
train stats after 56096 examples: {'rewards_train/chosen': '0.089913', 'rewards_train/rejected': '-0.0077907', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097704', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-162.21', 'loss/train': '0.65105', 'examples_per_second': '30.336', 'grad_norm': '28.75', 'counters/examples': 56096, 'counters/updates': 1753}
train stats after 56128 examples: {'rewards_train/chosen': '0.048668', 'rewards_train/rejected': '0.072934', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024266', 'logps_train/rejected': '-114.93', 'logps_train/chosen': '-142.07', 'loss/train': '0.71299', 'examples_per_second': '31.264', 'grad_norm': '32.5', 'counters/examples': 56128, 'counters/updates': 1754}
train stats after 56160 examples: {'rewards_train/chosen': '0.057856', 'rewards_train/rejected': '-0.013205', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071061', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-140.32', 'loss/train': '0.66448', 'examples_per_second': '32.041', 'grad_norm': '27.5', 'counters/examples': 56160, 'counters/updates': 1755}
train stats after 56192 examples: {'rewards_train/chosen': '0.049622', 'rewards_train/rejected': '0.010424', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039197', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-165.57', 'loss/train': '0.67873', 'examples_per_second': '31.347', 'grad_norm': '28.375', 'counters/examples': 56192, 'counters/updates': 1756}
skipping logging after 56224 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '0.079072', 'rewards_train/rejected': '0.081913', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0028408', 'logps_train/rejected': '-171.09', 'logps_train/chosen': '-139.72', 'loss/train': '0.7004', 'examples_per_second': '30.947', 'grad_norm': '28.5', 'counters/examples': 56256, 'counters/updates': 1758}
train stats after 56288 examples: {'rewards_train/chosen': '-0.022705', 'rewards_train/rejected': '-0.008678', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014027', 'logps_train/rejected': '-118.14', 'logps_train/chosen': '-129.95', 'loss/train': '0.70859', 'examples_per_second': '33.033', 'grad_norm': '29.5', 'counters/examples': 56288, 'counters/updates': 1759}
train stats after 56320 examples: {'rewards_train/chosen': '0.076416', 'rewards_train/rejected': '0.037207', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039209', 'logps_train/rejected': '-102', 'logps_train/chosen': '-136.47', 'loss/train': '0.67978', 'examples_per_second': '31.109', 'grad_norm': '24.375', 'counters/examples': 56320, 'counters/updates': 1760}
train stats after 56352 examples: {'rewards_train/chosen': '0.045585', 'rewards_train/rejected': '0.061634', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016049', 'logps_train/rejected': '-118.48', 'logps_train/chosen': '-144.19', 'loss/train': '0.70729', 'examples_per_second': '31.667', 'grad_norm': '26', 'counters/examples': 56352, 'counters/updates': 1761}
train stats after 56384 examples: {'rewards_train/chosen': '0.084252', 'rewards_train/rejected': '0.094446', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.010194', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-166.08', 'loss/train': '0.70417', 'examples_per_second': '31.658', 'grad_norm': '32.75', 'counters/examples': 56384, 'counters/updates': 1762}
skipping logging after 56416 examples to avoid logging too frequently
train stats after 56448 examples: {'rewards_train/chosen': '0.082587', 'rewards_train/rejected': '0.01847', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064117', 'logps_train/rejected': '-96.801', 'logps_train/chosen': '-135.45', 'loss/train': '0.66721', 'examples_per_second': '33.486', 'grad_norm': '25.75', 'counters/examples': 56448, 'counters/updates': 1764}
train stats after 56480 examples: {'rewards_train/chosen': '0.034438', 'rewards_train/rejected': '0.01292', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021518', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-138.43', 'loss/train': '0.68513', 'examples_per_second': '30.457', 'grad_norm': '25.375', 'counters/examples': 56480, 'counters/updates': 1765}
skipping logging after 56512 examples to avoid logging too frequently
train stats after 56544 examples: {'rewards_train/chosen': '0.082165', 'rewards_train/rejected': '0.10967', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027503', 'logps_train/rejected': '-136.4', 'logps_train/chosen': '-148.45', 'loss/train': '0.71175', 'examples_per_second': '32.075', 'grad_norm': '29', 'counters/examples': 56544, 'counters/updates': 1767}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56608 examples: {'rewards_train/chosen': '0.046526', 'rewards_train/rejected': '0.0064018', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040124', 'logps_train/rejected': '-103.24', 'logps_train/chosen': '-126.12', 'loss/train': '0.67803', 'examples_per_second': '33.353', 'grad_norm': '25', 'counters/examples': 56608, 'counters/updates': 1769}
skipping logging after 56640 examples to avoid logging too frequently
train stats after 56672 examples: {'rewards_train/chosen': '0.042151', 'rewards_train/rejected': '0.020812', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021338', 'logps_train/rejected': '-97.973', 'logps_train/chosen': '-124.59', 'loss/train': '0.68668', 'examples_per_second': '29.952', 'grad_norm': '26.375', 'counters/examples': 56672, 'counters/updates': 1771}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56736 examples: {'rewards_train/chosen': '-0.018885', 'rewards_train/rejected': '0.0324', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.051286', 'logps_train/rejected': '-99.692', 'logps_train/chosen': '-129.27', 'loss/train': '0.72239', 'examples_per_second': '32.173', 'grad_norm': '27.25', 'counters/examples': 56736, 'counters/updates': 1773}
train stats after 56768 examples: {'rewards_train/chosen': '0.069351', 'rewards_train/rejected': '0.037131', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03222', 'logps_train/rejected': '-132.51', 'logps_train/chosen': '-157.04', 'loss/train': '0.68065', 'examples_per_second': '31.615', 'grad_norm': '28.875', 'counters/examples': 56768, 'counters/updates': 1774}
train stats after 56800 examples: {'rewards_train/chosen': '0.077429', 'rewards_train/rejected': '0.0049587', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07247', 'logps_train/rejected': '-143.39', 'logps_train/chosen': '-145.98', 'loss/train': '0.66502', 'examples_per_second': '33.257', 'grad_norm': '26.25', 'counters/examples': 56800, 'counters/updates': 1775}
train stats after 56832 examples: {'rewards_train/chosen': '0.0536', 'rewards_train/rejected': '0.057279', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0036796', 'logps_train/rejected': '-83.374', 'logps_train/chosen': '-109.72', 'loss/train': '0.6964', 'examples_per_second': '32.011', 'grad_norm': '28.25', 'counters/examples': 56832, 'counters/updates': 1776}
skipping logging after 56864 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '0.069408', 'rewards_train/rejected': '-0.0079164', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.077325', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-136.11', 'loss/train': '0.65889', 'examples_per_second': '34.116', 'grad_norm': '25.625', 'counters/examples': 56896, 'counters/updates': 1778}
train stats after 56928 examples: {'rewards_train/chosen': '0.10232', 'rewards_train/rejected': '0.062118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040199', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-137.93', 'loss/train': '0.67926', 'examples_per_second': '31.506', 'grad_norm': '25.625', 'counters/examples': 56928, 'counters/updates': 1779}
train stats after 56960 examples: {'rewards_train/chosen': '0.066744', 'rewards_train/rejected': '0.042474', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02427', 'logps_train/rejected': '-126.45', 'logps_train/chosen': '-137.41', 'loss/train': '0.68358', 'examples_per_second': '31.58', 'grad_norm': '25.875', 'counters/examples': 56960, 'counters/updates': 1780}
train stats after 56992 examples: {'rewards_train/chosen': '0.070315', 'rewards_train/rejected': '0.015897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054417', 'logps_train/rejected': '-135.19', 'logps_train/chosen': '-152.56', 'loss/train': '0.67183', 'examples_per_second': '30.558', 'grad_norm': '28.125', 'counters/examples': 56992, 'counters/updates': 1781}
train stats after 57024 examples: {'rewards_train/chosen': '0.0068089', 'rewards_train/rejected': '0.019083', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012274', 'logps_train/rejected': '-134.68', 'logps_train/chosen': '-125.69', 'loss/train': '0.70252', 'examples_per_second': '32.418', 'grad_norm': '26.5', 'counters/examples': 57024, 'counters/updates': 1782}
skipping logging after 57056 examples to avoid logging too frequently
train stats after 57088 examples: {'rewards_train/chosen': '0.051056', 'rewards_train/rejected': '-0.0034231', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05448', 'logps_train/rejected': '-104.29', 'logps_train/chosen': '-172.26', 'loss/train': '0.67391', 'examples_per_second': '31.598', 'grad_norm': '27.5', 'counters/examples': 57088, 'counters/updates': 1784}
train stats after 57120 examples: {'rewards_train/chosen': '0.071464', 'rewards_train/rejected': '0.040567', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030897', 'logps_train/rejected': '-165.94', 'logps_train/chosen': '-147.96', 'loss/train': '0.68696', 'examples_per_second': '31.599', 'grad_norm': '31.625', 'counters/examples': 57120, 'counters/updates': 1785}
train stats after 57152 examples: {'rewards_train/chosen': '0.069867', 'rewards_train/rejected': '0.0184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051467', 'logps_train/rejected': '-113.09', 'logps_train/chosen': '-104.81', 'loss/train': '0.67392', 'examples_per_second': '30.448', 'grad_norm': '24.75', 'counters/examples': 57152, 'counters/updates': 1786}
train stats after 57184 examples: {'rewards_train/chosen': '0.051898', 'rewards_train/rejected': '0.011494', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040405', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-123.08', 'loss/train': '0.68033', 'examples_per_second': '30.118', 'grad_norm': '31', 'counters/examples': 57184, 'counters/updates': 1787}
train stats after 57216 examples: {'rewards_train/chosen': '0.059954', 'rewards_train/rejected': '0.030745', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029209', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-174.3', 'loss/train': '0.68608', 'examples_per_second': '31.539', 'grad_norm': '29.75', 'counters/examples': 57216, 'counters/updates': 1788}
skipping logging after 57248 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '0.016978', 'rewards_train/rejected': '-0.035055', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052033', 'logps_train/rejected': '-104.24', 'logps_train/chosen': '-126.23', 'loss/train': '0.67162', 'examples_per_second': '31.523', 'grad_norm': '25.875', 'counters/examples': 57280, 'counters/updates': 1790}
train stats after 57312 examples: {'rewards_train/chosen': '0.052489', 'rewards_train/rejected': '-0.00028942', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052779', 'logps_train/rejected': '-140.16', 'logps_train/chosen': '-116.98', 'loss/train': '0.67138', 'examples_per_second': '31.6', 'grad_norm': '25.875', 'counters/examples': 57312, 'counters/updates': 1791}
train stats after 57344 examples: {'rewards_train/chosen': '0.10868', 'rewards_train/rejected': '0.011974', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096708', 'logps_train/rejected': '-91.856', 'logps_train/chosen': '-126.4', 'loss/train': '0.65285', 'examples_per_second': '30.386', 'grad_norm': '24.375', 'counters/examples': 57344, 'counters/updates': 1792}
train stats after 57376 examples: {'rewards_train/chosen': '0.06096', 'rewards_train/rejected': '-0.008725', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069685', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-152.81', 'loss/train': '0.66386', 'examples_per_second': '24.02', 'grad_norm': '24.375', 'counters/examples': 57376, 'counters/updates': 1793}
train stats after 57408 examples: {'rewards_train/chosen': '0.084767', 'rewards_train/rejected': '0.062782', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021985', 'logps_train/rejected': '-181.65', 'logps_train/chosen': '-165.98', 'loss/train': '0.68651', 'examples_per_second': '31.598', 'grad_norm': '46.5', 'counters/examples': 57408, 'counters/updates': 1794}
skipping logging after 57440 examples to avoid logging too frequently
train stats after 57472 examples: {'rewards_train/chosen': '0.066826', 'rewards_train/rejected': '-0.0067739', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0736', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-114.78', 'loss/train': '0.66776', 'examples_per_second': '24.3', 'grad_norm': '28.625', 'counters/examples': 57472, 'counters/updates': 1796}
train stats after 57504 examples: {'rewards_train/chosen': '0.046071', 'rewards_train/rejected': '0.019078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026993', 'logps_train/rejected': '-146.44', 'logps_train/chosen': '-153.81', 'loss/train': '0.68537', 'examples_per_second': '31.716', 'grad_norm': '29.875', 'counters/examples': 57504, 'counters/updates': 1797}
train stats after 57536 examples: {'rewards_train/chosen': '0.04239', 'rewards_train/rejected': '0.0070847', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035306', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-121.08', 'loss/train': '0.68018', 'examples_per_second': '31.948', 'grad_norm': '24.875', 'counters/examples': 57536, 'counters/updates': 1798}
train stats after 57568 examples: {'rewards_train/chosen': '0.043322', 'rewards_train/rejected': '0.045666', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0023442', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-158.74', 'loss/train': '0.69938', 'examples_per_second': '31.203', 'grad_norm': '28.5', 'counters/examples': 57568, 'counters/updates': 1799}
train stats after 57600 examples: {'rewards_train/chosen': '0.041977', 'rewards_train/rejected': '-0.032245', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074222', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-150.34', 'loss/train': '0.66428', 'examples_per_second': '32.458', 'grad_norm': '28', 'counters/examples': 57600, 'counters/updates': 1800}
train stats after 57632 examples: {'rewards_train/chosen': '0.092499', 'rewards_train/rejected': '0.050378', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042121', 'logps_train/rejected': '-123.73', 'logps_train/chosen': '-141.82', 'loss/train': '0.68452', 'examples_per_second': '32.256', 'grad_norm': '45', 'counters/examples': 57632, 'counters/updates': 1801}
train stats after 57664 examples: {'rewards_train/chosen': '0.064737', 'rewards_train/rejected': '0.044133', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020604', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-125.67', 'loss/train': '0.68528', 'examples_per_second': '30.547', 'grad_norm': '26.125', 'counters/examples': 57664, 'counters/updates': 1802}
train stats after 57696 examples: {'rewards_train/chosen': '0.081655', 'rewards_train/rejected': '0.024337', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057319', 'logps_train/rejected': '-104.85', 'logps_train/chosen': '-166.45', 'loss/train': '0.67791', 'examples_per_second': '30.546', 'grad_norm': '31.25', 'counters/examples': 57696, 'counters/updates': 1803}
train stats after 57728 examples: {'rewards_train/chosen': '0.039035', 'rewards_train/rejected': '0.0016693', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037365', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-138.61', 'loss/train': '0.67935', 'examples_per_second': '31.461', 'grad_norm': '26.25', 'counters/examples': 57728, 'counters/updates': 1804}
train stats after 57760 examples: {'rewards_train/chosen': '0.025813', 'rewards_train/rejected': '0.0088794', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016934', 'logps_train/rejected': '-147.21', 'logps_train/chosen': '-151.4', 'loss/train': '0.69307', 'examples_per_second': '30.066', 'grad_norm': '30', 'counters/examples': 57760, 'counters/updates': 1805}
train stats after 57792 examples: {'rewards_train/chosen': '0.052581', 'rewards_train/rejected': '0.044677', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0079043', 'logps_train/rejected': '-155.13', 'logps_train/chosen': '-177.68', 'loss/train': '0.69566', 'examples_per_second': '31.564', 'grad_norm': '29.25', 'counters/examples': 57792, 'counters/updates': 1806}
train stats after 57824 examples: {'rewards_train/chosen': '0.098452', 'rewards_train/rejected': '0.039923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058529', 'logps_train/rejected': '-133.41', 'logps_train/chosen': '-181.99', 'loss/train': '0.67098', 'examples_per_second': '30.083', 'grad_norm': '31.25', 'counters/examples': 57824, 'counters/updates': 1807}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57888 examples: {'rewards_train/chosen': '0.099131', 'rewards_train/rejected': '0.0063597', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.092771', 'logps_train/rejected': '-97.677', 'logps_train/chosen': '-133.75', 'loss/train': '0.65196', 'examples_per_second': '31.223', 'grad_norm': '27', 'counters/examples': 57888, 'counters/updates': 1809}
skipping logging after 57920 examples to avoid logging too frequently
train stats after 57952 examples: {'rewards_train/chosen': '0.014124', 'rewards_train/rejected': '0.012522', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0016017', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-125.65', 'loss/train': '0.69736', 'examples_per_second': '35.589', 'grad_norm': '24.25', 'counters/examples': 57952, 'counters/updates': 1811}
train stats after 57984 examples: {'rewards_train/chosen': '0.024505', 'rewards_train/rejected': '-0.002674', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027179', 'logps_train/rejected': '-120.25', 'logps_train/chosen': '-131.27', 'loss/train': '0.68466', 'examples_per_second': '31.245', 'grad_norm': '24.25', 'counters/examples': 57984, 'counters/updates': 1812}
skipping logging after 58016 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '0.034413', 'rewards_train/rejected': '0.020232', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.014181', 'logps_train/rejected': '-99.92', 'logps_train/chosen': '-122.41', 'loss/train': '0.68856', 'examples_per_second': '36.35', 'grad_norm': '24.375', 'counters/examples': 58048, 'counters/updates': 1814}
train stats after 58080 examples: {'rewards_train/chosen': '0.028979', 'rewards_train/rejected': '0.018742', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010236', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-162.51', 'loss/train': '0.69509', 'examples_per_second': '30.033', 'grad_norm': '28.875', 'counters/examples': 58080, 'counters/updates': 1815}
train stats after 58112 examples: {'rewards_train/chosen': '0.04339', 'rewards_train/rejected': '0.058238', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.014848', 'logps_train/rejected': '-103.18', 'logps_train/chosen': '-114.9', 'loss/train': '0.70597', 'examples_per_second': '31.489', 'grad_norm': '25.25', 'counters/examples': 58112, 'counters/updates': 1816}
train stats after 58144 examples: {'rewards_train/chosen': '0.11213', 'rewards_train/rejected': '0.023425', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088702', 'logps_train/rejected': '-109.23', 'logps_train/chosen': '-144.58', 'loss/train': '0.66913', 'examples_per_second': '30.282', 'grad_norm': '25.25', 'counters/examples': 58144, 'counters/updates': 1817}
skipping logging after 58176 examples to avoid logging too frequently
train stats after 58208 examples: {'rewards_train/chosen': '0.060557', 'rewards_train/rejected': '0.032414', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028143', 'logps_train/rejected': '-95.592', 'logps_train/chosen': '-151.88', 'loss/train': '0.68461', 'examples_per_second': '34.99', 'grad_norm': '24.875', 'counters/examples': 58208, 'counters/updates': 1819}
train stats after 58240 examples: {'rewards_train/chosen': '0.075224', 'rewards_train/rejected': '0.001388', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073836', 'logps_train/rejected': '-98.076', 'logps_train/chosen': '-135.21', 'loss/train': '0.6611', 'examples_per_second': '31.637', 'grad_norm': '23.375', 'counters/examples': 58240, 'counters/updates': 1820}
train stats after 58272 examples: {'rewards_train/chosen': '0.050544', 'rewards_train/rejected': '0.036048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014496', 'logps_train/rejected': '-100.54', 'logps_train/chosen': '-149.24', 'loss/train': '0.69406', 'examples_per_second': '31.646', 'grad_norm': '28.375', 'counters/examples': 58272, 'counters/updates': 1821}
train stats after 58304 examples: {'rewards_train/chosen': '0.10445', 'rewards_train/rejected': '-0.043829', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14828', 'logps_train/rejected': '-145.93', 'logps_train/chosen': '-134.05', 'loss/train': '0.62569', 'examples_per_second': '32.593', 'grad_norm': '30.25', 'counters/examples': 58304, 'counters/updates': 1822}
train stats after 58336 examples: {'rewards_train/chosen': '0.029834', 'rewards_train/rejected': '0.010795', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019039', 'logps_train/rejected': '-125.72', 'logps_train/chosen': '-171.06', 'loss/train': '0.68955', 'examples_per_second': '31.632', 'grad_norm': '32.25', 'counters/examples': 58336, 'counters/updates': 1823}
train stats after 58368 examples: {'rewards_train/chosen': '0.048583', 'rewards_train/rejected': '0.015615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032968', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-138.1', 'loss/train': '0.68264', 'examples_per_second': '31.214', 'grad_norm': '28.375', 'counters/examples': 58368, 'counters/updates': 1824}
skipping logging after 58400 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '0.083532', 'rewards_train/rejected': '0.019022', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06451', 'logps_train/rejected': '-101', 'logps_train/chosen': '-122.77', 'loss/train': '0.66835', 'examples_per_second': '33.085', 'grad_norm': '25.5', 'counters/examples': 58432, 'counters/updates': 1826}
skipping logging after 58464 examples to avoid logging too frequently
train stats after 58496 examples: {'rewards_train/chosen': '0.10508', 'rewards_train/rejected': '-0.014516', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1196', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-141.99', 'loss/train': '0.64018', 'examples_per_second': '30.527', 'grad_norm': '24.625', 'counters/examples': 58496, 'counters/updates': 1828}
train stats after 58528 examples: {'rewards_train/chosen': '0.047514', 'rewards_train/rejected': '0.082439', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034925', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-143.81', 'loss/train': '0.71594', 'examples_per_second': '30.835', 'grad_norm': '27.75', 'counters/examples': 58528, 'counters/updates': 1829}
train stats after 58560 examples: {'rewards_train/chosen': '0.033946', 'rewards_train/rejected': '0.0087215', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025225', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-163.92', 'loss/train': '0.68682', 'examples_per_second': '31.634', 'grad_norm': '30.625', 'counters/examples': 58560, 'counters/updates': 1830}
train stats after 58592 examples: {'rewards_train/chosen': '0.11002', 'rewards_train/rejected': '0.023396', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086628', 'logps_train/rejected': '-129', 'logps_train/chosen': '-137.67', 'loss/train': '0.65897', 'examples_per_second': '31.645', 'grad_norm': '28.875', 'counters/examples': 58592, 'counters/updates': 1831}
train stats after 58624 examples: {'rewards_train/chosen': '0.043602', 'rewards_train/rejected': '0.017134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026469', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-141.69', 'loss/train': '0.68658', 'examples_per_second': '25.611', 'grad_norm': '26.25', 'counters/examples': 58624, 'counters/updates': 1832}
train stats after 58656 examples: {'rewards_train/chosen': '0.049378', 'rewards_train/rejected': '0.036565', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012813', 'logps_train/rejected': '-132.17', 'logps_train/chosen': '-132.02', 'loss/train': '0.68962', 'examples_per_second': '29.941', 'grad_norm': '25.875', 'counters/examples': 58656, 'counters/updates': 1833}
train stats after 58688 examples: {'rewards_train/chosen': '0.11078', 'rewards_train/rejected': '0.021427', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089352', 'logps_train/rejected': '-141.55', 'logps_train/chosen': '-125.3', 'loss/train': '0.65731', 'examples_per_second': '30.237', 'grad_norm': '26', 'counters/examples': 58688, 'counters/updates': 1834}
train stats after 58720 examples: {'rewards_train/chosen': '0.063594', 'rewards_train/rejected': '-0.033869', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097463', 'logps_train/rejected': '-107.55', 'logps_train/chosen': '-135.38', 'loss/train': '0.65205', 'examples_per_second': '32.072', 'grad_norm': '25.5', 'counters/examples': 58720, 'counters/updates': 1835}
train stats after 58752 examples: {'rewards_train/chosen': '0.096491', 'rewards_train/rejected': '0.015609', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080882', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-125.33', 'loss/train': '0.66011', 'examples_per_second': '30.576', 'grad_norm': '36', 'counters/examples': 58752, 'counters/updates': 1836}
train stats after 58784 examples: {'rewards_train/chosen': '0.01398', 'rewards_train/rejected': '0.041907', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027926', 'logps_train/rejected': '-120.98', 'logps_train/chosen': '-129.38', 'loss/train': '0.71117', 'examples_per_second': '30.541', 'grad_norm': '26.375', 'counters/examples': 58784, 'counters/updates': 1837}
train stats after 58816 examples: {'rewards_train/chosen': '0.065784', 'rewards_train/rejected': '0.098985', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0332', 'logps_train/rejected': '-173.17', 'logps_train/chosen': '-171.45', 'loss/train': '0.7169', 'examples_per_second': '31.473', 'grad_norm': '30.25', 'counters/examples': 58816, 'counters/updates': 1838}
train stats after 58848 examples: {'rewards_train/chosen': '0.052156', 'rewards_train/rejected': '0.037619', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014537', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-113.5', 'loss/train': '0.69293', 'examples_per_second': '30.122', 'grad_norm': '27.375', 'counters/examples': 58848, 'counters/updates': 1839}
train stats after 58880 examples: {'rewards_train/chosen': '0.055965', 'rewards_train/rejected': '-0.032007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087972', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-142.9', 'loss/train': '0.65811', 'examples_per_second': '29.822', 'grad_norm': '25.875', 'counters/examples': 58880, 'counters/updates': 1840}
skipping logging after 58912 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '0.064303', 'rewards_train/rejected': '0.056341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.007962', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-142.27', 'loss/train': '0.69561', 'examples_per_second': '31.691', 'grad_norm': '33.5', 'counters/examples': 58944, 'counters/updates': 1842}
train stats after 58976 examples: {'rewards_train/chosen': '0.028817', 'rewards_train/rejected': '0.078801', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.049984', 'logps_train/rejected': '-168.33', 'logps_train/chosen': '-181.31', 'loss/train': '0.72422', 'examples_per_second': '31.278', 'grad_norm': '31.625', 'counters/examples': 58976, 'counters/updates': 1843}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59040 examples: {'rewards_train/chosen': '0.077127', 'rewards_train/rejected': '0.033698', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043429', 'logps_train/rejected': '-168.92', 'logps_train/chosen': '-127', 'loss/train': '0.67733', 'examples_per_second': '30.92', 'grad_norm': '28.125', 'counters/examples': 59040, 'counters/updates': 1845}
train stats after 59072 examples: {'rewards_train/chosen': '0.11246', 'rewards_train/rejected': '0.02778', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084682', 'logps_train/rejected': '-104.31', 'logps_train/chosen': '-128.9', 'loss/train': '0.6572', 'examples_per_second': '30.588', 'grad_norm': '29', 'counters/examples': 59072, 'counters/updates': 1846}
skipping logging after 59104 examples to avoid logging too frequently
train stats after 59136 examples: {'rewards_train/chosen': '0.098535', 'rewards_train/rejected': '0.030354', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068181', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-163.89', 'loss/train': '0.66574', 'examples_per_second': '31.816', 'grad_norm': '28.25', 'counters/examples': 59136, 'counters/updates': 1848}
train stats after 59168 examples: {'rewards_train/chosen': '0.054291', 'rewards_train/rejected': '-0.0068027', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061094', 'logps_train/rejected': '-147.22', 'logps_train/chosen': '-142.07', 'loss/train': '0.66751', 'examples_per_second': '31.619', 'grad_norm': '27.375', 'counters/examples': 59168, 'counters/updates': 1849}
train stats after 59200 examples: {'rewards_train/chosen': '0.061549', 'rewards_train/rejected': '0.048012', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013537', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-127.74', 'loss/train': '0.68879', 'examples_per_second': '30.723', 'grad_norm': '24.875', 'counters/examples': 59200, 'counters/updates': 1850}
train stats after 59232 examples: {'rewards_train/chosen': '0.096472', 'rewards_train/rejected': '0.028434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068038', 'logps_train/rejected': '-128', 'logps_train/chosen': '-120.41', 'loss/train': '0.66904', 'examples_per_second': '30.412', 'grad_norm': '24.875', 'counters/examples': 59232, 'counters/updates': 1851}
train stats after 59264 examples: {'rewards_train/chosen': '0.060433', 'rewards_train/rejected': '0.065884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0054516', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-164.1', 'loss/train': '0.69943', 'examples_per_second': '30.425', 'grad_norm': '27.875', 'counters/examples': 59264, 'counters/updates': 1852}
skipping logging after 59296 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '0.086412', 'rewards_train/rejected': '0.015639', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070773', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-127.09', 'loss/train': '0.66501', 'examples_per_second': '30.549', 'grad_norm': '27.5', 'counters/examples': 59328, 'counters/updates': 1854}
train stats after 59360 examples: {'rewards_train/chosen': '0.017553', 'rewards_train/rejected': '0.013806', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0037475', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-147.05', 'loss/train': '0.69757', 'examples_per_second': '31.622', 'grad_norm': '34', 'counters/examples': 59360, 'counters/updates': 1855}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59424 examples: {'rewards_train/chosen': '0.09156', 'rewards_train/rejected': '-0.0034584', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095018', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-156.03', 'loss/train': '0.65264', 'examples_per_second': '31.125', 'grad_norm': '37', 'counters/examples': 59424, 'counters/updates': 1857}
train stats after 59456 examples: {'rewards_train/chosen': '0.054283', 'rewards_train/rejected': '0.042973', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01131', 'logps_train/rejected': '-165.53', 'logps_train/chosen': '-158.74', 'loss/train': '0.69535', 'examples_per_second': '30.898', 'grad_norm': '31.125', 'counters/examples': 59456, 'counters/updates': 1858}
train stats after 59488 examples: {'rewards_train/chosen': '0.022628', 'rewards_train/rejected': '0.036125', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.013497', 'logps_train/rejected': '-206.9', 'logps_train/chosen': '-167.31', 'loss/train': '0.7077', 'examples_per_second': '31.55', 'grad_norm': '34.25', 'counters/examples': 59488, 'counters/updates': 1859}
train stats after 59520 examples: {'rewards_train/chosen': '0.053576', 'rewards_train/rejected': '0.042088', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011487', 'logps_train/rejected': '-126.08', 'logps_train/chosen': '-132.08', 'loss/train': '0.69262', 'examples_per_second': '30.373', 'grad_norm': '26.125', 'counters/examples': 59520, 'counters/updates': 1860}
train stats after 59552 examples: {'rewards_train/chosen': '0.041782', 'rewards_train/rejected': '-0.017307', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05909', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-111.66', 'loss/train': '0.66841', 'examples_per_second': '31.91', 'grad_norm': '23.625', 'counters/examples': 59552, 'counters/updates': 1861}
train stats after 59584 examples: {'rewards_train/chosen': '0.096743', 'rewards_train/rejected': '0.037254', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059489', 'logps_train/rejected': '-157.59', 'logps_train/chosen': '-155.98', 'loss/train': '0.67145', 'examples_per_second': '31.623', 'grad_norm': '28.25', 'counters/examples': 59584, 'counters/updates': 1862}
train stats after 59616 examples: {'rewards_train/chosen': '0.10039', 'rewards_train/rejected': '-0.021518', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12191', 'logps_train/rejected': '-80.61', 'logps_train/chosen': '-141.73', 'loss/train': '0.64182', 'examples_per_second': '31.922', 'grad_norm': '29.375', 'counters/examples': 59616, 'counters/updates': 1863}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59680 examples: {'rewards_train/chosen': '0.073034', 'rewards_train/rejected': '0.023416', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049618', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-152.87', 'loss/train': '0.67969', 'examples_per_second': '31.62', 'grad_norm': '26.375', 'counters/examples': 59680, 'counters/updates': 1865}
skipping logging after 59712 examples to avoid logging too frequently
train stats after 59744 examples: {'rewards_train/chosen': '0.055377', 'rewards_train/rejected': '0.015564', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039813', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-122.21', 'loss/train': '0.68101', 'examples_per_second': '30.369', 'grad_norm': '35.5', 'counters/examples': 59744, 'counters/updates': 1867}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59808 examples: {'rewards_train/chosen': '0.10398', 'rewards_train/rejected': '-0.0068375', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11082', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-153.07', 'loss/train': '0.64979', 'examples_per_second': '33.514', 'grad_norm': '28.75', 'counters/examples': 59808, 'counters/updates': 1869}
train stats after 59840 examples: {'rewards_train/chosen': '0.021804', 'rewards_train/rejected': '0.0031362', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018668', 'logps_train/rejected': '-107.73', 'logps_train/chosen': '-116.76', 'loss/train': '0.68937', 'examples_per_second': '32.325', 'grad_norm': '25', 'counters/examples': 59840, 'counters/updates': 1870}
train stats after 59872 examples: {'rewards_train/chosen': '0.039967', 'rewards_train/rejected': '0.032395', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0075725', 'logps_train/rejected': '-120.93', 'logps_train/chosen': '-123.27', 'loss/train': '0.69372', 'examples_per_second': '30.093', 'grad_norm': '31', 'counters/examples': 59872, 'counters/updates': 1871}
train stats after 59904 examples: {'rewards_train/chosen': '0.15746', 'rewards_train/rejected': '0.00023656', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15722', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-136.84', 'loss/train': '0.63802', 'examples_per_second': '29.963', 'grad_norm': '30.625', 'counters/examples': 59904, 'counters/updates': 1872}
train stats after 59936 examples: {'rewards_train/chosen': '0.033984', 'rewards_train/rejected': '0.04294', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0089552', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-139.96', 'loss/train': '0.70187', 'examples_per_second': '31.417', 'grad_norm': '26', 'counters/examples': 59936, 'counters/updates': 1873}
train stats after 59968 examples: {'rewards_train/chosen': '-0.010088', 'rewards_train/rejected': '-0.0058525', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0042356', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-129.07', 'loss/train': '0.69915', 'examples_per_second': '30.017', 'grad_norm': '29.5', 'counters/examples': 59968, 'counters/updates': 1874}
train stats after 60000 examples: {'rewards_train/chosen': '0.036319', 'rewards_train/rejected': '0.014477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021842', 'logps_train/rejected': '-145.46', 'logps_train/chosen': '-157.82', 'loss/train': '0.68753', 'examples_per_second': '29.846', 'grad_norm': '28.25', 'counters/examples': 60000, 'counters/updates': 1875}
Running evaluation after 60000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.74it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.88it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.90it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.94it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.88it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.94it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.86it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.83it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.86it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.88it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.81it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.86it/s]
eval after 60000: {'rewards_eval/chosen': '0.074541', 'rewards_eval/rejected': '0.026999', 'rewards_eval/accuracies': '0.58203', 'rewards_eval/margins': '0.047542', 'logps_eval/rejected': '-118.34', 'logps_eval/chosen': '-138.69', 'loss/eval': '0.6761'}
skipping save for non epoch
train stats after 60032 examples: {'rewards_train/chosen': '0.069785', 'rewards_train/rejected': '-0.013402', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083188', 'logps_train/rejected': '-145.8', 'logps_train/chosen': '-167.06', 'loss/train': '0.65689', 'examples_per_second': '31.406', 'grad_norm': '28.25', 'counters/examples': 60032, 'counters/updates': 1876}
train stats after 60064 examples: {'rewards_train/chosen': '0.066018', 'rewards_train/rejected': '0.047635', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018384', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-170.49', 'loss/train': '0.68949', 'examples_per_second': '32.758', 'grad_norm': '28.375', 'counters/examples': 60064, 'counters/updates': 1877}
train stats after 60096 examples: {'rewards_train/chosen': '0.080526', 'rewards_train/rejected': '-0.015655', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09618', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-142.05', 'loss/train': '0.65409', 'examples_per_second': '31.414', 'grad_norm': '25.625', 'counters/examples': 60096, 'counters/updates': 1878}
train stats after 60128 examples: {'rewards_train/chosen': '0.076163', 'rewards_train/rejected': '0.045264', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030899', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-150.3', 'loss/train': '0.68469', 'examples_per_second': '31.311', 'grad_norm': '26.75', 'counters/examples': 60128, 'counters/updates': 1879}
train stats after 60160 examples: {'rewards_train/chosen': '0.047543', 'rewards_train/rejected': '-0.042936', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090479', 'logps_train/rejected': '-114.16', 'logps_train/chosen': '-159.27', 'loss/train': '0.65384', 'examples_per_second': '32.466', 'grad_norm': '27.75', 'counters/examples': 60160, 'counters/updates': 1880}
train stats after 60192 examples: {'rewards_train/chosen': '0.08131', 'rewards_train/rejected': '-0.012099', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.093409', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-114.12', 'loss/train': '0.65264', 'examples_per_second': '32.263', 'grad_norm': '25.625', 'counters/examples': 60192, 'counters/updates': 1881}
train stats after 60224 examples: {'rewards_train/chosen': '0.037071', 'rewards_train/rejected': '0.0091326', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027938', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-184.27', 'loss/train': '0.68916', 'examples_per_second': '31.983', 'grad_norm': '29.625', 'counters/examples': 60224, 'counters/updates': 1882}
train stats after 60256 examples: {'rewards_train/chosen': '0.058146', 'rewards_train/rejected': '0.045167', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012979', 'logps_train/rejected': '-98.645', 'logps_train/chosen': '-114.21', 'loss/train': '0.6908', 'examples_per_second': '30.812', 'grad_norm': '23.5', 'counters/examples': 60256, 'counters/updates': 1883}
train stats after 60288 examples: {'rewards_train/chosen': '0.089884', 'rewards_train/rejected': '0.028523', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06136', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-137.05', 'loss/train': '0.66514', 'examples_per_second': '29.727', 'grad_norm': '26', 'counters/examples': 60288, 'counters/updates': 1884}
train stats after 60320 examples: {'rewards_train/chosen': '0.1229', 'rewards_train/rejected': '0.051973', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.070931', 'logps_train/rejected': '-141.06', 'logps_train/chosen': '-125.52', 'loss/train': '0.66363', 'examples_per_second': '30.262', 'grad_norm': '27.125', 'counters/examples': 60320, 'counters/updates': 1885}
train stats after 60352 examples: {'rewards_train/chosen': '0.040209', 'rewards_train/rejected': '0.03722', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0029895', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-130.36', 'loss/train': '0.69355', 'examples_per_second': '30.746', 'grad_norm': '24.875', 'counters/examples': 60352, 'counters/updates': 1886}
train stats after 60384 examples: {'rewards_train/chosen': '0.077123', 'rewards_train/rejected': '-0.016277', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093401', 'logps_train/rejected': '-107.63', 'logps_train/chosen': '-143.79', 'loss/train': '0.65386', 'examples_per_second': '31.458', 'grad_norm': '25.375', 'counters/examples': 60384, 'counters/updates': 1887}
train stats after 60416 examples: {'rewards_train/chosen': '0.11974', 'rewards_train/rejected': '0.059552', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.060184', 'logps_train/rejected': '-132.11', 'logps_train/chosen': '-149.5', 'loss/train': '0.67225', 'examples_per_second': '31.622', 'grad_norm': '28.625', 'counters/examples': 60416, 'counters/updates': 1888}
train stats after 60448 examples: {'rewards_train/chosen': '-0.021889', 'rewards_train/rejected': '0.024505', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046394', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-142.34', 'loss/train': '0.72438', 'examples_per_second': '31.286', 'grad_norm': '40', 'counters/examples': 60448, 'counters/updates': 1889}
train stats after 60480 examples: {'rewards_train/chosen': '0.0066299', 'rewards_train/rejected': '-0.018606', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025236', 'logps_train/rejected': '-94.339', 'logps_train/chosen': '-106.62', 'loss/train': '0.68684', 'examples_per_second': '31.811', 'grad_norm': '25.375', 'counters/examples': 60480, 'counters/updates': 1890}
train stats after 60512 examples: {'rewards_train/chosen': '0.10689', 'rewards_train/rejected': '0.027261', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.079633', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-135.94', 'loss/train': '0.65999', 'examples_per_second': '31.645', 'grad_norm': '27.875', 'counters/examples': 60512, 'counters/updates': 1891}
train stats after 60544 examples: {'rewards_train/chosen': '0.057993', 'rewards_train/rejected': '0.068731', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010738', 'logps_train/rejected': '-103.53', 'logps_train/chosen': '-112.86', 'loss/train': '0.70404', 'examples_per_second': '32.693', 'grad_norm': '24.5', 'counters/examples': 60544, 'counters/updates': 1892}
train stats after 60576 examples: {'rewards_train/chosen': '0.013948', 'rewards_train/rejected': '-0.015332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02928', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-135.58', 'loss/train': '0.68351', 'examples_per_second': '31.046', 'grad_norm': '25.25', 'counters/examples': 60576, 'counters/updates': 1893}
train stats after 60608 examples: {'rewards_train/chosen': '0.053422', 'rewards_train/rejected': '0.046417', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0070045', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-139.69', 'loss/train': '0.69374', 'examples_per_second': '30.156', 'grad_norm': '27.625', 'counters/examples': 60608, 'counters/updates': 1894}
train stats after 60640 examples: {'rewards_train/chosen': '0.041065', 'rewards_train/rejected': '0.010524', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030542', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-132.59', 'loss/train': '0.68429', 'examples_per_second': '30.456', 'grad_norm': '25.25', 'counters/examples': 60640, 'counters/updates': 1895}
train stats after 60672 examples: {'rewards_train/chosen': '0.04485', 'rewards_train/rejected': '0.031076', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013774', 'logps_train/rejected': '-76.898', 'logps_train/chosen': '-148.84', 'loss/train': '0.69284', 'examples_per_second': '31.905', 'grad_norm': '28.375', 'counters/examples': 60672, 'counters/updates': 1896}
train stats after 60704 examples: {'rewards_train/chosen': '0.042244', 'rewards_train/rejected': '0.033501', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0087432', 'logps_train/rejected': '-154.88', 'logps_train/chosen': '-177.27', 'loss/train': '0.69972', 'examples_per_second': '29.865', 'grad_norm': '31', 'counters/examples': 60704, 'counters/updates': 1897}
train stats after 60736 examples: {'rewards_train/chosen': '0.016714', 'rewards_train/rejected': '0.042938', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.026225', 'logps_train/rejected': '-135.69', 'logps_train/chosen': '-119.57', 'loss/train': '0.70994', 'examples_per_second': '30.667', 'grad_norm': '26.125', 'counters/examples': 60736, 'counters/updates': 1898}
train stats after 60768 examples: {'rewards_train/chosen': '0.052897', 'rewards_train/rejected': '0.022286', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030611', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-133.13', 'loss/train': '0.68172', 'examples_per_second': '30', 'grad_norm': '25.875', 'counters/examples': 60768, 'counters/updates': 1899}
train stats after 60800 examples: {'rewards_train/chosen': '0.073097', 'rewards_train/rejected': '0.0077224', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065374', 'logps_train/rejected': '-108.92', 'logps_train/chosen': '-115.11', 'loss/train': '0.66614', 'examples_per_second': '32.265', 'grad_norm': '25.75', 'counters/examples': 60800, 'counters/updates': 1900}
train stats after 60832 examples: {'rewards_train/chosen': '0.026945', 'rewards_train/rejected': '0.022549', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0043959', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-106.62', 'loss/train': '0.69626', 'examples_per_second': '31.628', 'grad_norm': '25.625', 'counters/examples': 60832, 'counters/updates': 1901}
train stats after 60864 examples: {'rewards_train/chosen': '0.13643', 'rewards_train/rejected': '0.025574', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11086', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-146.64', 'loss/train': '0.64662', 'examples_per_second': '30.792', 'grad_norm': '33', 'counters/examples': 60864, 'counters/updates': 1902}
train stats after 60896 examples: {'rewards_train/chosen': '0.054032', 'rewards_train/rejected': '0.026475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027557', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-141.33', 'loss/train': '0.68589', 'examples_per_second': '31.488', 'grad_norm': '29.625', 'counters/examples': 60896, 'counters/updates': 1903}
train stats after 60928 examples: {'rewards_train/chosen': '0.012928', 'rewards_train/rejected': '0.016197', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0032682', 'logps_train/rejected': '-133.3', 'logps_train/chosen': '-149.69', 'loss/train': '0.70209', 'examples_per_second': '30.167', 'grad_norm': '31.375', 'counters/examples': 60928, 'counters/updates': 1904}
train stats after 60960 examples: {'rewards_train/chosen': '0.088887', 'rewards_train/rejected': '0.053092', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035795', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-136.95', 'loss/train': '0.68425', 'examples_per_second': '30.656', 'grad_norm': '28.75', 'counters/examples': 60960, 'counters/updates': 1905}
train stats after 60992 examples: {'rewards_train/chosen': '0.037244', 'rewards_train/rejected': '-0.012697', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.049941', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-119.81', 'loss/train': '0.67318', 'examples_per_second': '33.182', 'grad_norm': '25.875', 'counters/examples': 60992, 'counters/updates': 1906}
skipping logging after 61024 examples to avoid logging too frequently
train stats after 61056 examples: {'rewards_train/chosen': '0.098207', 'rewards_train/rejected': '0.080975', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017232', 'logps_train/rejected': '-114.87', 'logps_train/chosen': '-123.04', 'loss/train': '0.69304', 'examples_per_second': '34.506', 'grad_norm': '24.625', 'counters/examples': 61056, 'counters/updates': 1908}
train stats after 61088 examples: {'rewards_train/chosen': '0.12693', 'rewards_train/rejected': '0.059977', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066951', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-158.76', 'loss/train': '0.66573', 'examples_per_second': '30.634', 'grad_norm': '27.25', 'counters/examples': 61088, 'counters/updates': 1909}
train stats after 61120 examples: {'rewards_train/chosen': '0.023596', 'rewards_train/rejected': '-0.026892', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050488', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-136.52', 'loss/train': '0.67733', 'examples_per_second': '32.537', 'grad_norm': '27.875', 'counters/examples': 61120, 'counters/updates': 1910}
train stats after 61152 examples: {'rewards_train/chosen': '0.029855', 'rewards_train/rejected': '0.069125', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03927', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-165.49', 'loss/train': '0.71813', 'examples_per_second': '30.114', 'grad_norm': '36', 'counters/examples': 61152, 'counters/updates': 1911}
train stats after 61184 examples: {'rewards_train/chosen': '0.065322', 'rewards_train/rejected': '-0.014646', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079968', 'logps_train/rejected': '-149.78', 'logps_train/chosen': '-138.9', 'loss/train': '0.65952', 'examples_per_second': '30.161', 'grad_norm': '26.125', 'counters/examples': 61184, 'counters/updates': 1912}
train stats after 61216 examples: {'rewards_train/chosen': '0.056086', 'rewards_train/rejected': '0.01484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041246', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-153.13', 'loss/train': '0.67803', 'examples_per_second': '30.604', 'grad_norm': '26.625', 'counters/examples': 61216, 'counters/updates': 1913}
train stats after 61248 examples: {'rewards_train/chosen': '0.053903', 'rewards_train/rejected': '0.0067992', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047104', 'logps_train/rejected': '-115.55', 'logps_train/chosen': '-133.44', 'loss/train': '0.67421', 'examples_per_second': '32.548', 'grad_norm': '25.875', 'counters/examples': 61248, 'counters/updates': 1914}
skipping logging after 61280 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '0.0029772', 'rewards_train/rejected': '-0.0032269', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0062042', 'logps_train/rejected': '-111.05', 'logps_train/chosen': '-153.54', 'loss/train': '0.69901', 'examples_per_second': '30.376', 'grad_norm': '33', 'counters/examples': 61312, 'counters/updates': 1916}
train stats after 61344 examples: {'rewards_train/chosen': '0.0078524', 'rewards_train/rejected': '0.016896', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0090433', 'logps_train/rejected': '-106.68', 'logps_train/chosen': '-133.21', 'loss/train': '0.70448', 'examples_per_second': '30.983', 'grad_norm': '27.375', 'counters/examples': 61344, 'counters/updates': 1917}
train stats after 61376 examples: {'rewards_train/chosen': '0.093897', 'rewards_train/rejected': '0.053002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040895', 'logps_train/rejected': '-135.42', 'logps_train/chosen': '-141.29', 'loss/train': '0.67885', 'examples_per_second': '30.046', 'grad_norm': '30.5', 'counters/examples': 61376, 'counters/updates': 1918}
skipping logging after 61408 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '0.075877', 'rewards_train/rejected': '0.033655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042222', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-131.84', 'loss/train': '0.67794', 'examples_per_second': '31.697', 'grad_norm': '35.25', 'counters/examples': 61440, 'counters/updates': 1920}
train stats after 61472 examples: {'rewards_train/chosen': '0.080839', 'rewards_train/rejected': '0.0018725', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078967', 'logps_train/rejected': '-123.99', 'logps_train/chosen': '-181.95', 'loss/train': '0.66021', 'examples_per_second': '29.829', 'grad_norm': '28.25', 'counters/examples': 61472, 'counters/updates': 1921}
train stats after 61504 examples: {'rewards_train/chosen': '0.016337', 'rewards_train/rejected': '0.02517', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0088331', 'logps_train/rejected': '-134.14', 'logps_train/chosen': '-109.06', 'loss/train': '0.70157', 'examples_per_second': '30.361', 'grad_norm': '26.5', 'counters/examples': 61504, 'counters/updates': 1922}
train stats after 61536 examples: {'rewards_train/chosen': '0.071932', 'rewards_train/rejected': '0.048225', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023707', 'logps_train/rejected': '-150.28', 'logps_train/chosen': '-183.46', 'loss/train': '0.6872', 'examples_per_second': '30.127', 'grad_norm': '31.125', 'counters/examples': 61536, 'counters/updates': 1923}
train stats after 61568 examples: {'rewards_train/chosen': '0.050674', 'rewards_train/rejected': '0.019205', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031469', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-155.85', 'loss/train': '0.68306', 'examples_per_second': '31.689', 'grad_norm': '29.125', 'counters/examples': 61568, 'counters/updates': 1924}
train stats after 61600 examples: {'rewards_train/chosen': '0.071626', 'rewards_train/rejected': '0.045954', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025672', 'logps_train/rejected': '-141.57', 'logps_train/chosen': '-138.82', 'loss/train': '0.6867', 'examples_per_second': '31.666', 'grad_norm': '27.125', 'counters/examples': 61600, 'counters/updates': 1925}
train stats after 61632 examples: {'rewards_train/chosen': '0.11044', 'rewards_train/rejected': '0.02333', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087113', 'logps_train/rejected': '-115.92', 'logps_train/chosen': '-128.13', 'loss/train': '0.6543', 'examples_per_second': '32.238', 'grad_norm': '39.75', 'counters/examples': 61632, 'counters/updates': 1926}
train stats after 61664 examples: {'rewards_train/chosen': '0.0551', 'rewards_train/rejected': '0.03238', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02272', 'logps_train/rejected': '-92.209', 'logps_train/chosen': '-107.5', 'loss/train': '0.68687', 'examples_per_second': '31.66', 'grad_norm': '22.375', 'counters/examples': 61664, 'counters/updates': 1927}
train stats after 61696 examples: {'rewards_train/chosen': '0.066498', 'rewards_train/rejected': '0.0025973', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.063901', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-103', 'loss/train': '0.66903', 'examples_per_second': '32.169', 'grad_norm': '29.625', 'counters/examples': 61696, 'counters/updates': 1928}
train stats after 61728 examples: {'rewards_train/chosen': '0.082537', 'rewards_train/rejected': '0.020615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061921', 'logps_train/rejected': '-148.95', 'logps_train/chosen': '-171.24', 'loss/train': '0.67088', 'examples_per_second': '31.906', 'grad_norm': '31', 'counters/examples': 61728, 'counters/updates': 1929}
train stats after 61760 examples: {'rewards_train/chosen': '0.075842', 'rewards_train/rejected': '0.045684', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030158', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-119.15', 'loss/train': '0.68218', 'examples_per_second': '33.122', 'grad_norm': '26.875', 'counters/examples': 61760, 'counters/updates': 1930}
train stats after 61792 examples: {'rewards_train/chosen': '0.082013', 'rewards_train/rejected': '-0.016777', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.09879', 'logps_train/rejected': '-90.819', 'logps_train/chosen': '-143.61', 'loss/train': '0.65127', 'examples_per_second': '32.118', 'grad_norm': '23.75', 'counters/examples': 61792, 'counters/updates': 1931}
train stats after 61824 examples: {'rewards_train/chosen': '0.054592', 'rewards_train/rejected': '0.039164', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015428', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-157.02', 'loss/train': '0.68991', 'examples_per_second': '31.531', 'grad_norm': '31.125', 'counters/examples': 61824, 'counters/updates': 1932}
train stats after 61856 examples: {'rewards_train/chosen': '0.082529', 'rewards_train/rejected': '0.11182', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02929', 'logps_train/rejected': '-132.12', 'logps_train/chosen': '-145.15', 'loss/train': '0.71596', 'examples_per_second': '31.428', 'grad_norm': '31.25', 'counters/examples': 61856, 'counters/updates': 1933}
train stats after 61888 examples: {'rewards_train/chosen': '0.044028', 'rewards_train/rejected': '-0.0062569', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050285', 'logps_train/rejected': '-95.535', 'logps_train/chosen': '-112.32', 'loss/train': '0.67456', 'examples_per_second': '31.719', 'grad_norm': '22.125', 'counters/examples': 61888, 'counters/updates': 1934}
train stats after 61920 examples: {'rewards_train/chosen': '0.046475', 'rewards_train/rejected': '0.001902', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044573', 'logps_train/rejected': '-100.1', 'logps_train/chosen': '-138.65', 'loss/train': '0.67287', 'examples_per_second': '31.762', 'grad_norm': '26.125', 'counters/examples': 61920, 'counters/updates': 1935}
train stats after 61952 examples: {'rewards_train/chosen': '0.060955', 'rewards_train/rejected': '0.038266', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022689', 'logps_train/rejected': '-139.71', 'logps_train/chosen': '-166.42', 'loss/train': '0.68898', 'examples_per_second': '30.827', 'grad_norm': '29.5', 'counters/examples': 61952, 'counters/updates': 1936}
train stats after 61984 examples: {'rewards_train/chosen': '0.065237', 'rewards_train/rejected': '0.023405', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041831', 'logps_train/rejected': '-134.6', 'logps_train/chosen': '-176.93', 'loss/train': '0.68082', 'examples_per_second': '32.2', 'grad_norm': '33', 'counters/examples': 61984, 'counters/updates': 1937}
train stats after 62016 examples: {'rewards_train/chosen': '0.12515', 'rewards_train/rejected': '0.060502', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064649', 'logps_train/rejected': '-112.16', 'logps_train/chosen': '-117.28', 'loss/train': '0.66913', 'examples_per_second': '31.58', 'grad_norm': '25.5', 'counters/examples': 62016, 'counters/updates': 1938}
train stats after 62048 examples: {'rewards_train/chosen': '0.098473', 'rewards_train/rejected': '0.0031873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095286', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-182.22', 'loss/train': '0.65257', 'examples_per_second': '30.992', 'grad_norm': '33', 'counters/examples': 62048, 'counters/updates': 1939}
train stats after 62080 examples: {'rewards_train/chosen': '0.068447', 'rewards_train/rejected': '0.033607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034841', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-146.4', 'loss/train': '0.68492', 'examples_per_second': '30.244', 'grad_norm': '30.625', 'counters/examples': 62080, 'counters/updates': 1940}
skipping logging after 62112 examples to avoid logging too frequently
train stats after 62144 examples: {'rewards_train/chosen': '0.087464', 'rewards_train/rejected': '0.045523', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041941', 'logps_train/rejected': '-94.989', 'logps_train/chosen': '-148.65', 'loss/train': '0.67646', 'examples_per_second': '31.607', 'grad_norm': '29.5', 'counters/examples': 62144, 'counters/updates': 1942}
train stats after 62176 examples: {'rewards_train/chosen': '0.059725', 'rewards_train/rejected': '0.082984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.023259', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-130.74', 'loss/train': '0.71068', 'examples_per_second': '30.199', 'grad_norm': '30', 'counters/examples': 62176, 'counters/updates': 1943}
train stats after 62208 examples: {'rewards_train/chosen': '0.085956', 'rewards_train/rejected': '0.027237', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05872', 'logps_train/rejected': '-125.72', 'logps_train/chosen': '-148.23', 'loss/train': '0.66987', 'examples_per_second': '31.313', 'grad_norm': '28.75', 'counters/examples': 62208, 'counters/updates': 1944}
train stats after 62240 examples: {'rewards_train/chosen': '0.047797', 'rewards_train/rejected': '0.014734', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033063', 'logps_train/rejected': '-146.46', 'logps_train/chosen': '-146.25', 'loss/train': '0.68241', 'examples_per_second': '31.671', 'grad_norm': '27.75', 'counters/examples': 62240, 'counters/updates': 1945}
train stats after 62272 examples: {'rewards_train/chosen': '0.029115', 'rewards_train/rejected': '0.063004', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033889', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-133.97', 'loss/train': '0.71486', 'examples_per_second': '31.865', 'grad_norm': '26.125', 'counters/examples': 62272, 'counters/updates': 1946}
train stats after 62304 examples: {'rewards_train/chosen': '0.08051', 'rewards_train/rejected': '0.081231', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00072098', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-152.9', 'loss/train': '0.69972', 'examples_per_second': '31.688', 'grad_norm': '29.75', 'counters/examples': 62304, 'counters/updates': 1947}
train stats after 62336 examples: {'rewards_train/chosen': '0.080502', 'rewards_train/rejected': '0.046304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034198', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-115.94', 'loss/train': '0.68075', 'examples_per_second': '31.195', 'grad_norm': '27.125', 'counters/examples': 62336, 'counters/updates': 1948}
skipping logging after 62368 examples to avoid logging too frequently
train stats after 62400 examples: {'rewards_train/chosen': '0.0036663', 'rewards_train/rejected': '-0.022085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025751', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-147.02', 'loss/train': '0.68441', 'examples_per_second': '32.186', 'grad_norm': '33.75', 'counters/examples': 62400, 'counters/updates': 1950}
train stats after 62432 examples: {'rewards_train/chosen': '0.035448', 'rewards_train/rejected': '-0.012885', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048332', 'logps_train/rejected': '-147.01', 'logps_train/chosen': '-156.33', 'loss/train': '0.6721', 'examples_per_second': '31.598', 'grad_norm': '29', 'counters/examples': 62432, 'counters/updates': 1951}
train stats after 62464 examples: {'rewards_train/chosen': '0.10943', 'rewards_train/rejected': '0.067762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041667', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-160.62', 'loss/train': '0.68241', 'examples_per_second': '30.333', 'grad_norm': '28.75', 'counters/examples': 62464, 'counters/updates': 1952}
train stats after 62496 examples: {'rewards_train/chosen': '0.01684', 'rewards_train/rejected': '0.00062137', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016219', 'logps_train/rejected': '-113.57', 'logps_train/chosen': '-134.39', 'loss/train': '0.69206', 'examples_per_second': '32.792', 'grad_norm': '33.75', 'counters/examples': 62496, 'counters/updates': 1953}
train stats after 62528 examples: {'rewards_train/chosen': '0.070931', 'rewards_train/rejected': '0.046443', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024488', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-149.06', 'loss/train': '0.68672', 'examples_per_second': '32.166', 'grad_norm': '28.375', 'counters/examples': 62528, 'counters/updates': 1954}
skipping logging after 62560 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '0.11149', 'rewards_train/rejected': '0.058907', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052585', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-158.97', 'loss/train': '0.67181', 'examples_per_second': '31.486', 'grad_norm': '26.75', 'counters/examples': 62592, 'counters/updates': 1956}
train stats after 62624 examples: {'rewards_train/chosen': '0.094573', 'rewards_train/rejected': '0.031233', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063339', 'logps_train/rejected': '-150', 'logps_train/chosen': '-156.44', 'loss/train': '0.66801', 'examples_per_second': '32.319', 'grad_norm': '28.75', 'counters/examples': 62624, 'counters/updates': 1957}
train stats after 62656 examples: {'rewards_train/chosen': '0.10554', 'rewards_train/rejected': '-0.02313', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12867', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-153.88', 'loss/train': '0.63973', 'examples_per_second': '33.172', 'grad_norm': '28.375', 'counters/examples': 62656, 'counters/updates': 1958}
train stats after 62688 examples: {'rewards_train/chosen': '0.076345', 'rewards_train/rejected': '0.0079334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068411', 'logps_train/rejected': '-129.73', 'logps_train/chosen': '-118.78', 'loss/train': '0.66406', 'examples_per_second': '31.081', 'grad_norm': '25.5', 'counters/examples': 62688, 'counters/updates': 1959}
train stats after 62720 examples: {'rewards_train/chosen': '0.043961', 'rewards_train/rejected': '0.044961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00099993', 'logps_train/rejected': '-108.56', 'logps_train/chosen': '-140.51', 'loss/train': '0.70569', 'examples_per_second': '32.752', 'grad_norm': '26', 'counters/examples': 62720, 'counters/updates': 1960}
train stats after 62752 examples: {'rewards_train/chosen': '0.055007', 'rewards_train/rejected': '0.048179', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0068273', 'logps_train/rejected': '-106.18', 'logps_train/chosen': '-127.31', 'loss/train': '0.69581', 'examples_per_second': '31.804', 'grad_norm': '26.125', 'counters/examples': 62752, 'counters/updates': 1961}
train stats after 62784 examples: {'rewards_train/chosen': '0.011897', 'rewards_train/rejected': '-0.034842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046738', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-134.23', 'loss/train': '0.67524', 'examples_per_second': '32.272', 'grad_norm': '30.875', 'counters/examples': 62784, 'counters/updates': 1962}
train stats after 62816 examples: {'rewards_train/chosen': '0.070268', 'rewards_train/rejected': '0.035638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.03463', 'logps_train/rejected': '-167.28', 'logps_train/chosen': '-129.19', 'loss/train': '0.68338', 'examples_per_second': '31.143', 'grad_norm': '28.875', 'counters/examples': 62816, 'counters/updates': 1963}
train stats after 62848 examples: {'rewards_train/chosen': '0.079742', 'rewards_train/rejected': '-0.033596', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11334', 'logps_train/rejected': '-123.52', 'logps_train/chosen': '-95.756', 'loss/train': '0.64192', 'examples_per_second': '21.863', 'grad_norm': '22', 'counters/examples': 62848, 'counters/updates': 1964}
skipping logging after 62880 examples to avoid logging too frequently
train stats after 62912 examples: {'rewards_train/chosen': '0.037423', 'rewards_train/rejected': '0.015204', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02222', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-132.34', 'loss/train': '0.6879', 'examples_per_second': '31.368', 'grad_norm': '25.25', 'counters/examples': 62912, 'counters/updates': 1966}
train stats after 62944 examples: {'rewards_train/chosen': '0.084774', 'rewards_train/rejected': '0.042304', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042471', 'logps_train/rejected': '-140.51', 'logps_train/chosen': '-142.87', 'loss/train': '0.68241', 'examples_per_second': '24.312', 'grad_norm': '30', 'counters/examples': 62944, 'counters/updates': 1967}
train stats after 62976 examples: {'rewards_train/chosen': '0.067332', 'rewards_train/rejected': '-0.021137', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088469', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-163.01', 'loss/train': '0.65513', 'examples_per_second': '31.697', 'grad_norm': '29.75', 'counters/examples': 62976, 'counters/updates': 1968}
skipping logging after 63008 examples to avoid logging too frequently
train stats after 63040 examples: {'rewards_train/chosen': '0.047715', 'rewards_train/rejected': '0.029144', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018572', 'logps_train/rejected': '-148.58', 'logps_train/chosen': '-148.95', 'loss/train': '0.69862', 'examples_per_second': '32.643', 'grad_norm': '30.125', 'counters/examples': 63040, 'counters/updates': 1970}
train stats after 63072 examples: {'rewards_train/chosen': '0.013827', 'rewards_train/rejected': '0.0043206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0095065', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-137.05', 'loss/train': '0.69709', 'examples_per_second': '30.818', 'grad_norm': '27.25', 'counters/examples': 63072, 'counters/updates': 1971}
train stats after 63104 examples: {'rewards_train/chosen': '0.075619', 'rewards_train/rejected': '0.03341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042209', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-131.64', 'loss/train': '0.67888', 'examples_per_second': '30.819', 'grad_norm': '28.875', 'counters/examples': 63104, 'counters/updates': 1972}
train stats after 63136 examples: {'rewards_train/chosen': '0.071058', 'rewards_train/rejected': '0.087231', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016173', 'logps_train/rejected': '-106.39', 'logps_train/chosen': '-133.45', 'loss/train': '0.70566', 'examples_per_second': '30.121', 'grad_norm': '28.375', 'counters/examples': 63136, 'counters/updates': 1973}
train stats after 63168 examples: {'rewards_train/chosen': '0.026657', 'rewards_train/rejected': '0.043792', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.017136', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-114.56', 'loss/train': '0.70784', 'examples_per_second': '31.379', 'grad_norm': '28.625', 'counters/examples': 63168, 'counters/updates': 1974}
train stats after 63200 examples: {'rewards_train/chosen': '0.061064', 'rewards_train/rejected': '0.01933', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041734', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-136.61', 'loss/train': '0.67854', 'examples_per_second': '31.063', 'grad_norm': '25.375', 'counters/examples': 63200, 'counters/updates': 1975}
skipping logging after 63232 examples to avoid logging too frequently
train stats after 63264 examples: {'rewards_train/chosen': '0.031479', 'rewards_train/rejected': '-0.0040545', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035534', 'logps_train/rejected': '-141.16', 'logps_train/chosen': '-139.68', 'loss/train': '0.68234', 'examples_per_second': '31.542', 'grad_norm': '30.125', 'counters/examples': 63264, 'counters/updates': 1977}
train stats after 63296 examples: {'rewards_train/chosen': '0.0022225', 'rewards_train/rejected': '-0.0076497', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0098722', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-153.15', 'loss/train': '0.69106', 'examples_per_second': '32.076', 'grad_norm': '32', 'counters/examples': 63296, 'counters/updates': 1978}
train stats after 63328 examples: {'rewards_train/chosen': '0.032061', 'rewards_train/rejected': '0.029062', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0029991', 'logps_train/rejected': '-132.11', 'logps_train/chosen': '-127.15', 'loss/train': '0.69597', 'examples_per_second': '31.458', 'grad_norm': '30.25', 'counters/examples': 63328, 'counters/updates': 1979}
skipping logging after 63360 examples to avoid logging too frequently
train stats after 63392 examples: {'rewards_train/chosen': '0.02858', 'rewards_train/rejected': '9.939e-05', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02848', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-166.01', 'loss/train': '0.68411', 'examples_per_second': '34.389', 'grad_norm': '27.125', 'counters/examples': 63392, 'counters/updates': 1981}
train stats after 63424 examples: {'rewards_train/chosen': '0.058331', 'rewards_train/rejected': '0.017301', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041031', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-131.85', 'loss/train': '0.67669', 'examples_per_second': '32.262', 'grad_norm': '23.25', 'counters/examples': 63424, 'counters/updates': 1982}
train stats after 63456 examples: {'rewards_train/chosen': '0.044053', 'rewards_train/rejected': '-0.0076163', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051669', 'logps_train/rejected': '-122.9', 'logps_train/chosen': '-118.79', 'loss/train': '0.67252', 'examples_per_second': '31.607', 'grad_norm': '28', 'counters/examples': 63456, 'counters/updates': 1983}
train stats after 63488 examples: {'rewards_train/chosen': '0.069545', 'rewards_train/rejected': '0.071767', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0022219', 'logps_train/rejected': '-116', 'logps_train/chosen': '-138.33', 'loss/train': '0.69714', 'examples_per_second': '30.291', 'grad_norm': '26.875', 'counters/examples': 63488, 'counters/updates': 1984}
train stats after 63520 examples: {'rewards_train/chosen': '0.065712', 'rewards_train/rejected': '0.022124', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043588', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-129.86', 'loss/train': '0.67843', 'examples_per_second': '32.708', 'grad_norm': '29.75', 'counters/examples': 63520, 'counters/updates': 1985}
train stats after 63552 examples: {'rewards_train/chosen': '0.071309', 'rewards_train/rejected': '0.030574', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040735', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-130.62', 'loss/train': '0.67993', 'examples_per_second': '31.337', 'grad_norm': '27.125', 'counters/examples': 63552, 'counters/updates': 1986}
train stats after 63584 examples: {'rewards_train/chosen': '0.041457', 'rewards_train/rejected': '0.013067', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02839', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-128.43', 'loss/train': '0.68238', 'examples_per_second': '31.669', 'grad_norm': '27.5', 'counters/examples': 63584, 'counters/updates': 1987}
train stats after 63616 examples: {'rewards_train/chosen': '0.11254', 'rewards_train/rejected': '0.031869', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.080673', 'logps_train/rejected': '-118.71', 'logps_train/chosen': '-182.48', 'loss/train': '0.65639', 'examples_per_second': '31.359', 'grad_norm': '28.625', 'counters/examples': 63616, 'counters/updates': 1988}
skipping logging after 63648 examples to avoid logging too frequently
train stats after 63680 examples: {'rewards_train/chosen': '0.12285', 'rewards_train/rejected': '0.074494', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048359', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-132.37', 'loss/train': '0.67306', 'examples_per_second': '30.769', 'grad_norm': '29.5', 'counters/examples': 63680, 'counters/updates': 1990}
train stats after 63712 examples: {'rewards_train/chosen': '0.069192', 'rewards_train/rejected': '0.0067355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062456', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-150.21', 'loss/train': '0.66999', 'examples_per_second': '31.989', 'grad_norm': '29.125', 'counters/examples': 63712, 'counters/updates': 1991}
train stats after 63744 examples: {'rewards_train/chosen': '0.10622', 'rewards_train/rejected': '0.02592', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080301', 'logps_train/rejected': '-129.16', 'logps_train/chosen': '-198.03', 'loss/train': '0.66072', 'examples_per_second': '32.775', 'grad_norm': '31.75', 'counters/examples': 63744, 'counters/updates': 1992}
train stats after 63776 examples: {'rewards_train/chosen': '0.010889', 'rewards_train/rejected': '0.020658', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0097692', 'logps_train/rejected': '-152.59', 'logps_train/chosen': '-154.91', 'loss/train': '0.71019', 'examples_per_second': '31.794', 'grad_norm': '32.5', 'counters/examples': 63776, 'counters/updates': 1993}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63840 examples: {'rewards_train/chosen': '0.03324', 'rewards_train/rejected': '0.061702', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.028463', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-143.07', 'loss/train': '0.71043', 'examples_per_second': '31.323', 'grad_norm': '26.75', 'counters/examples': 63840, 'counters/updates': 1995}
train stats after 63872 examples: {'rewards_train/chosen': '-0.0026113', 'rewards_train/rejected': '0.045352', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.047963', 'logps_train/rejected': '-140.35', 'logps_train/chosen': '-134.07', 'loss/train': '0.72674', 'examples_per_second': '32.869', 'grad_norm': '34', 'counters/examples': 63872, 'counters/updates': 1996}
train stats after 63904 examples: {'rewards_train/chosen': '0.039835', 'rewards_train/rejected': '0.019874', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019962', 'logps_train/rejected': '-119.37', 'logps_train/chosen': '-113.69', 'loss/train': '0.6906', 'examples_per_second': '31.268', 'grad_norm': '25.625', 'counters/examples': 63904, 'counters/updates': 1997}
train stats after 63936 examples: {'rewards_train/chosen': '0.082052', 'rewards_train/rejected': '0.022744', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.059308', 'logps_train/rejected': '-131.04', 'logps_train/chosen': '-150.21', 'loss/train': '0.67115', 'examples_per_second': '30.917', 'grad_norm': '26.875', 'counters/examples': 63936, 'counters/updates': 1998}
train stats after 63968 examples: {'rewards_train/chosen': '0.05806', 'rewards_train/rejected': '0.025598', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032462', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-154.91', 'loss/train': '0.68104', 'examples_per_second': '31.684', 'grad_norm': '26.875', 'counters/examples': 63968, 'counters/updates': 1999}
train stats after 64000 examples: {'rewards_train/chosen': '0.096922', 'rewards_train/rejected': '0.075753', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.021169', 'logps_train/rejected': '-120.48', 'logps_train/chosen': '-160.4', 'loss/train': '0.68909', 'examples_per_second': '32.088', 'grad_norm': '34.25', 'counters/examples': 64000, 'counters/updates': 2000}
train stats after 64032 examples: {'rewards_train/chosen': '0.064953', 'rewards_train/rejected': '0.011176', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053777', 'logps_train/rejected': '-99.241', 'logps_train/chosen': '-142.68', 'loss/train': '0.66956', 'examples_per_second': '31.771', 'grad_norm': '27', 'counters/examples': 64032, 'counters/updates': 2001}
train stats after 64064 examples: {'rewards_train/chosen': '0.070555', 'rewards_train/rejected': '0.036773', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033782', 'logps_train/rejected': '-147.16', 'logps_train/chosen': '-159.14', 'loss/train': '0.68692', 'examples_per_second': '31.465', 'grad_norm': '28.75', 'counters/examples': 64064, 'counters/updates': 2002}
skipping logging after 64096 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '0.041303', 'rewards_train/rejected': '0.060946', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019643', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-124.77', 'loss/train': '0.71096', 'examples_per_second': '31.619', 'grad_norm': '30.375', 'counters/examples': 64128, 'counters/updates': 2004}
train stats after 64160 examples: {'rewards_train/chosen': '0.035088', 'rewards_train/rejected': '0.042145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0070569', 'logps_train/rejected': '-129.5', 'logps_train/chosen': '-130.81', 'loss/train': '0.70492', 'examples_per_second': '32.346', 'grad_norm': '27.625', 'counters/examples': 64160, 'counters/updates': 2005}
train stats after 64192 examples: {'rewards_train/chosen': '0.095714', 'rewards_train/rejected': '-0.0098402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10555', 'logps_train/rejected': '-129.68', 'logps_train/chosen': '-142.91', 'loss/train': '0.64892', 'examples_per_second': '25.006', 'grad_norm': '46', 'counters/examples': 64192, 'counters/updates': 2006}
train stats after 64224 examples: {'rewards_train/chosen': '0.064746', 'rewards_train/rejected': '0.0038792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060867', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-134.9', 'loss/train': '0.67161', 'examples_per_second': '31.628', 'grad_norm': '26.125', 'counters/examples': 64224, 'counters/updates': 2007}
skipping logging after 64256 examples to avoid logging too frequently
train stats after 64288 examples: {'rewards_train/chosen': '0.062629', 'rewards_train/rejected': '0.04256', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020069', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-152.86', 'loss/train': '0.68849', 'examples_per_second': '31.554', 'grad_norm': '28.125', 'counters/examples': 64288, 'counters/updates': 2009}
train stats after 64320 examples: {'rewards_train/chosen': '0.034078', 'rewards_train/rejected': '0.034438', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00035924', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-121.72', 'loss/train': '0.69699', 'examples_per_second': '32.456', 'grad_norm': '27.875', 'counters/examples': 64320, 'counters/updates': 2010}
train stats after 64352 examples: {'rewards_train/chosen': '0.10996', 'rewards_train/rejected': '0.054895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055066', 'logps_train/rejected': '-143.68', 'logps_train/chosen': '-147.01', 'loss/train': '0.67487', 'examples_per_second': '30.57', 'grad_norm': '26.25', 'counters/examples': 64352, 'counters/updates': 2011}
train stats after 64384 examples: {'rewards_train/chosen': '0.046956', 'rewards_train/rejected': '0.054798', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.007842', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-143.75', 'loss/train': '0.70049', 'examples_per_second': '31.332', 'grad_norm': '27', 'counters/examples': 64384, 'counters/updates': 2012}
skipping logging after 64416 examples to avoid logging too frequently
train stats after 64448 examples: {'rewards_train/chosen': '0.090855', 'rewards_train/rejected': '0.032177', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058678', 'logps_train/rejected': '-177.89', 'logps_train/chosen': '-195.45', 'loss/train': '0.67365', 'examples_per_second': '32.18', 'grad_norm': '31.5', 'counters/examples': 64448, 'counters/updates': 2014}
train stats after 64480 examples: {'rewards_train/chosen': '0.052519', 'rewards_train/rejected': '0.0017499', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050769', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-112.13', 'loss/train': '0.67692', 'examples_per_second': '31.616', 'grad_norm': '24.125', 'counters/examples': 64480, 'counters/updates': 2015}
skipping logging after 64512 examples to avoid logging too frequently
train stats after 64544 examples: {'rewards_train/chosen': '0.071878', 'rewards_train/rejected': '-0.007199', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079077', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-135.48', 'loss/train': '0.65774', 'examples_per_second': '32.461', 'grad_norm': '26.375', 'counters/examples': 64544, 'counters/updates': 2017}
train stats after 64576 examples: {'rewards_train/chosen': '0.14079', 'rewards_train/rejected': '0.049939', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.090854', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-163.9', 'loss/train': '0.65651', 'examples_per_second': '31.411', 'grad_norm': '30.875', 'counters/examples': 64576, 'counters/updates': 2018}
skipping logging after 64608 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '0.070994', 'rewards_train/rejected': '0.11769', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046695', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-126.84', 'loss/train': '0.72062', 'examples_per_second': '31.203', 'grad_norm': '34.5', 'counters/examples': 64640, 'counters/updates': 2020}
skipping logging after 64672 examples to avoid logging too frequently
train stats after 64704 examples: {'rewards_train/chosen': '0.033671', 'rewards_train/rejected': '0.057047', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023376', 'logps_train/rejected': '-91.73', 'logps_train/chosen': '-100.6', 'loss/train': '0.70941', 'examples_per_second': '35.785', 'grad_norm': '23.625', 'counters/examples': 64704, 'counters/updates': 2022}
train stats after 64736 examples: {'rewards_train/chosen': '0.084825', 'rewards_train/rejected': '0.0053564', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.079469', 'logps_train/rejected': '-121.75', 'logps_train/chosen': '-133.18', 'loss/train': '0.65877', 'examples_per_second': '30.612', 'grad_norm': '29.625', 'counters/examples': 64736, 'counters/updates': 2023}
skipping logging after 64768 examples to avoid logging too frequently
train stats after 64800 examples: {'rewards_train/chosen': '0.050359', 'rewards_train/rejected': '0.016678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033681', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-117.46', 'loss/train': '0.68237', 'examples_per_second': '32.39', 'grad_norm': '25.75', 'counters/examples': 64800, 'counters/updates': 2025}
train stats after 64832 examples: {'rewards_train/chosen': '0.096722', 'rewards_train/rejected': '-0.051033', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14775', 'logps_train/rejected': '-108.97', 'logps_train/chosen': '-148.49', 'loss/train': '0.62602', 'examples_per_second': '30.6', 'grad_norm': '23.75', 'counters/examples': 64832, 'counters/updates': 2026}
train stats after 64864 examples: {'rewards_train/chosen': '0.076201', 'rewards_train/rejected': '0.057367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018834', 'logps_train/rejected': '-137.9', 'logps_train/chosen': '-160.14', 'loss/train': '0.68912', 'examples_per_second': '31.769', 'grad_norm': '29.125', 'counters/examples': 64864, 'counters/updates': 2027}
train stats after 64896 examples: {'rewards_train/chosen': '0.10353', 'rewards_train/rejected': '0.055038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048487', 'logps_train/rejected': '-177.08', 'logps_train/chosen': '-181.38', 'loss/train': '0.67689', 'examples_per_second': '31.103', 'grad_norm': '42.25', 'counters/examples': 64896, 'counters/updates': 2028}
skipping logging after 64928 examples to avoid logging too frequently
train stats after 64960 examples: {'rewards_train/chosen': '0.12407', 'rewards_train/rejected': '0.072989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051082', 'logps_train/rejected': '-106.28', 'logps_train/chosen': '-114.78', 'loss/train': '0.67251', 'examples_per_second': '31.502', 'grad_norm': '30.875', 'counters/examples': 64960, 'counters/updates': 2030}
skipping logging after 64992 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '0.065547', 'rewards_train/rejected': '0.064668', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00087914', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-122.15', 'loss/train': '0.70055', 'examples_per_second': '30.479', 'grad_norm': '27.625', 'counters/examples': 65024, 'counters/updates': 2032}
train stats after 65056 examples: {'rewards_train/chosen': '0.062643', 'rewards_train/rejected': '0.0051798', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057463', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-150.53', 'loss/train': '0.67349', 'examples_per_second': '31.027', 'grad_norm': '25.75', 'counters/examples': 65056, 'counters/updates': 2033}
train stats after 65088 examples: {'rewards_train/chosen': '0.11515', 'rewards_train/rejected': '0.079866', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035284', 'logps_train/rejected': '-132.64', 'logps_train/chosen': '-139.3', 'loss/train': '0.68808', 'examples_per_second': '30.953', 'grad_norm': '26.625', 'counters/examples': 65088, 'counters/updates': 2034}
train stats after 65120 examples: {'rewards_train/chosen': '0.15492', 'rewards_train/rejected': '-0.021589', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17651', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-159.16', 'loss/train': '0.6164', 'examples_per_second': '31.695', 'grad_norm': '24.5', 'counters/examples': 65120, 'counters/updates': 2035}
train stats after 65152 examples: {'rewards_train/chosen': '0.1134', 'rewards_train/rejected': '0.0036217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10978', 'logps_train/rejected': '-97.715', 'logps_train/chosen': '-131.24', 'loss/train': '0.64286', 'examples_per_second': '31.121', 'grad_norm': '21.375', 'counters/examples': 65152, 'counters/updates': 2036}
train stats after 65184 examples: {'rewards_train/chosen': '0.05406', 'rewards_train/rejected': '0.038174', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015885', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-154.61', 'loss/train': '0.69139', 'examples_per_second': '31.652', 'grad_norm': '27.375', 'counters/examples': 65184, 'counters/updates': 2037}
train stats after 65216 examples: {'rewards_train/chosen': '0.062852', 'rewards_train/rejected': '0.056143', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0067087', 'logps_train/rejected': '-116.79', 'logps_train/chosen': '-158.87', 'loss/train': '0.69671', 'examples_per_second': '30.875', 'grad_norm': '31.875', 'counters/examples': 65216, 'counters/updates': 2038}
train stats after 65248 examples: {'rewards_train/chosen': '0.086076', 'rewards_train/rejected': '0.0042632', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081813', 'logps_train/rejected': '-113.81', 'logps_train/chosen': '-144.31', 'loss/train': '0.65932', 'examples_per_second': '32.124', 'grad_norm': '26.5', 'counters/examples': 65248, 'counters/updates': 2039}
train stats after 65280 examples: {'rewards_train/chosen': '0.12351', 'rewards_train/rejected': '0.0084197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11509', 'logps_train/rejected': '-105.86', 'logps_train/chosen': '-129.28', 'loss/train': '0.64409', 'examples_per_second': '31.652', 'grad_norm': '26.375', 'counters/examples': 65280, 'counters/updates': 2040}
skipping logging after 65312 examples to avoid logging too frequently
train stats after 65344 examples: {'rewards_train/chosen': '0.12015', 'rewards_train/rejected': '0.016613', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10354', 'logps_train/rejected': '-109.23', 'logps_train/chosen': '-165.26', 'loss/train': '0.64681', 'examples_per_second': '30.227', 'grad_norm': '27.625', 'counters/examples': 65344, 'counters/updates': 2042}
train stats after 65376 examples: {'rewards_train/chosen': '0.13541', 'rewards_train/rejected': '-0.015834', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15125', 'logps_train/rejected': '-96.451', 'logps_train/chosen': '-154.26', 'loss/train': '0.62525', 'examples_per_second': '30.174', 'grad_norm': '22.625', 'counters/examples': 65376, 'counters/updates': 2043}
train stats after 65408 examples: {'rewards_train/chosen': '0.1483', 'rewards_train/rejected': '-0.016578', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16488', 'logps_train/rejected': '-142.09', 'logps_train/chosen': '-183.85', 'loss/train': '0.62227', 'examples_per_second': '31.656', 'grad_norm': '28.375', 'counters/examples': 65408, 'counters/updates': 2044}
train stats after 65440 examples: {'rewards_train/chosen': '0.062289', 'rewards_train/rejected': '0.022535', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039754', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-148.58', 'loss/train': '0.67835', 'examples_per_second': '31.604', 'grad_norm': '29.75', 'counters/examples': 65440, 'counters/updates': 2045}
train stats after 65472 examples: {'rewards_train/chosen': '0.10266', 'rewards_train/rejected': '0.021406', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08125', 'logps_train/rejected': '-115.63', 'logps_train/chosen': '-170.43', 'loss/train': '0.66022', 'examples_per_second': '30.334', 'grad_norm': '27.25', 'counters/examples': 65472, 'counters/updates': 2046}
skipping logging after 65504 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '0.086143', 'rewards_train/rejected': '0.087453', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0013098', 'logps_train/rejected': '-124.98', 'logps_train/chosen': '-119.37', 'loss/train': '0.69821', 'examples_per_second': '32.29', 'grad_norm': '28.875', 'counters/examples': 65536, 'counters/updates': 2048}
train stats after 65568 examples: {'rewards_train/chosen': '0.075774', 'rewards_train/rejected': '-0.0089669', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084741', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-146.42', 'loss/train': '0.6578', 'examples_per_second': '30.736', 'grad_norm': '24', 'counters/examples': 65568, 'counters/updates': 2049}
train stats after 65600 examples: {'rewards_train/chosen': '0.15261', 'rewards_train/rejected': '0.01921', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1334', 'logps_train/rejected': '-104.52', 'logps_train/chosen': '-147.89', 'loss/train': '0.64094', 'examples_per_second': '31.684', 'grad_norm': '22.625', 'counters/examples': 65600, 'counters/updates': 2050}
train stats after 65632 examples: {'rewards_train/chosen': '0.065788', 'rewards_train/rejected': '0.013138', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052651', 'logps_train/rejected': '-89.806', 'logps_train/chosen': '-129.22', 'loss/train': '0.67377', 'examples_per_second': '32.944', 'grad_norm': '24.5', 'counters/examples': 65632, 'counters/updates': 2051}
train stats after 65664 examples: {'rewards_train/chosen': '0.046268', 'rewards_train/rejected': '0.063082', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016814', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-180.84', 'loss/train': '0.70812', 'examples_per_second': '32.146', 'grad_norm': '29.5', 'counters/examples': 65664, 'counters/updates': 2052}
train stats after 65696 examples: {'rewards_train/chosen': '0.033369', 'rewards_train/rejected': '0.029343', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0040266', 'logps_train/rejected': '-89.793', 'logps_train/chosen': '-112.85', 'loss/train': '0.6973', 'examples_per_second': '30.738', 'grad_norm': '23', 'counters/examples': 65696, 'counters/updates': 2053}
skipping logging after 65728 examples to avoid logging too frequently
train stats after 65760 examples: {'rewards_train/chosen': '0.04932', 'rewards_train/rejected': '0.055044', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0057244', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-107.54', 'loss/train': '0.70272', 'examples_per_second': '31.137', 'grad_norm': '25', 'counters/examples': 65760, 'counters/updates': 2055}
train stats after 65792 examples: {'rewards_train/chosen': '0.035594', 'rewards_train/rejected': '0.024411', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011183', 'logps_train/rejected': '-102.55', 'logps_train/chosen': '-121.35', 'loss/train': '0.69188', 'examples_per_second': '33.051', 'grad_norm': '26.25', 'counters/examples': 65792, 'counters/updates': 2056}
train stats after 65824 examples: {'rewards_train/chosen': '0.064324', 'rewards_train/rejected': '0.061099', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0032258', 'logps_train/rejected': '-142.51', 'logps_train/chosen': '-175.25', 'loss/train': '0.69862', 'examples_per_second': '31.25', 'grad_norm': '29.625', 'counters/examples': 65824, 'counters/updates': 2057}
train stats after 65856 examples: {'rewards_train/chosen': '0.018823', 'rewards_train/rejected': '0.075227', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.056403', 'logps_train/rejected': '-140.1', 'logps_train/chosen': '-143.6', 'loss/train': '0.74456', 'examples_per_second': '30.681', 'grad_norm': '69.5', 'counters/examples': 65856, 'counters/updates': 2058}
train stats after 65888 examples: {'rewards_train/chosen': '0.081948', 'rewards_train/rejected': '0.023862', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058085', 'logps_train/rejected': '-80.514', 'logps_train/chosen': '-120.09', 'loss/train': '0.66875', 'examples_per_second': '31.656', 'grad_norm': '26.5', 'counters/examples': 65888, 'counters/updates': 2059}
train stats after 65920 examples: {'rewards_train/chosen': '0.090257', 'rewards_train/rejected': '0.054244', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036013', 'logps_train/rejected': '-156.75', 'logps_train/chosen': '-155.65', 'loss/train': '0.68952', 'examples_per_second': '31.236', 'grad_norm': '34', 'counters/examples': 65920, 'counters/updates': 2060}
train stats after 65952 examples: {'rewards_train/chosen': '0.12711', 'rewards_train/rejected': '0.073589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053519', 'logps_train/rejected': '-82.026', 'logps_train/chosen': '-135.1', 'loss/train': '0.67023', 'examples_per_second': '31.614', 'grad_norm': '26', 'counters/examples': 65952, 'counters/updates': 2061}
train stats after 65984 examples: {'rewards_train/chosen': '0.088474', 'rewards_train/rejected': '0.063142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025332', 'logps_train/rejected': '-134.42', 'logps_train/chosen': '-140.66', 'loss/train': '0.68824', 'examples_per_second': '30.156', 'grad_norm': '28.875', 'counters/examples': 65984, 'counters/updates': 2062}
train stats after 66016 examples: {'rewards_train/chosen': '0.082068', 'rewards_train/rejected': '0.035777', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046291', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-143.5', 'loss/train': '0.67347', 'examples_per_second': '31.155', 'grad_norm': '26', 'counters/examples': 66016, 'counters/updates': 2063}
train stats after 66048 examples: {'rewards_train/chosen': '0.062557', 'rewards_train/rejected': '-0.00063248', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063189', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-161.53', 'loss/train': '0.66663', 'examples_per_second': '31.587', 'grad_norm': '26.625', 'counters/examples': 66048, 'counters/updates': 2064}
train stats after 66080 examples: {'rewards_train/chosen': '0.10107', 'rewards_train/rejected': '0.055507', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045564', 'logps_train/rejected': '-125.51', 'logps_train/chosen': '-125.24', 'loss/train': '0.67659', 'examples_per_second': '30.762', 'grad_norm': '27.25', 'counters/examples': 66080, 'counters/updates': 2065}
train stats after 66112 examples: {'rewards_train/chosen': '0.078073', 'rewards_train/rejected': '0.022155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055918', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-120.77', 'loss/train': '0.66877', 'examples_per_second': '31.256', 'grad_norm': '25.375', 'counters/examples': 66112, 'counters/updates': 2066}
train stats after 66144 examples: {'rewards_train/chosen': '0.11525', 'rewards_train/rejected': '0.092981', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022268', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-129.98', 'loss/train': '0.68942', 'examples_per_second': '30.91', 'grad_norm': '24.5', 'counters/examples': 66144, 'counters/updates': 2067}
skipping logging after 66176 examples to avoid logging too frequently
train stats after 66208 examples: {'rewards_train/chosen': '0.066578', 'rewards_train/rejected': '0.069326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0027484', 'logps_train/rejected': '-106.79', 'logps_train/chosen': '-124.44', 'loss/train': '0.69858', 'examples_per_second': '32.803', 'grad_norm': '23.875', 'counters/examples': 66208, 'counters/updates': 2069}
train stats after 66240 examples: {'rewards_train/chosen': '0.089747', 'rewards_train/rejected': '0.08023', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0095176', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-145.63', 'loss/train': '0.70967', 'examples_per_second': '31.823', 'grad_norm': '27.75', 'counters/examples': 66240, 'counters/updates': 2070}
train stats after 66272 examples: {'rewards_train/chosen': '0.057997', 'rewards_train/rejected': '0.076752', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018756', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-158.56', 'loss/train': '0.70783', 'examples_per_second': '30.265', 'grad_norm': '29.875', 'counters/examples': 66272, 'counters/updates': 2071}
train stats after 66304 examples: {'rewards_train/chosen': '0.032251', 'rewards_train/rejected': '0.027289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.004962', 'logps_train/rejected': '-114.52', 'logps_train/chosen': '-125.27', 'loss/train': '0.69419', 'examples_per_second': '30.07', 'grad_norm': '33.75', 'counters/examples': 66304, 'counters/updates': 2072}
skipping logging after 66336 examples to avoid logging too frequently
train stats after 66368 examples: {'rewards_train/chosen': '0.047032', 'rewards_train/rejected': '0.027345', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019687', 'logps_train/rejected': '-142.75', 'logps_train/chosen': '-184.59', 'loss/train': '0.69025', 'examples_per_second': '32.708', 'grad_norm': '31', 'counters/examples': 66368, 'counters/updates': 2074}
train stats after 66400 examples: {'rewards_train/chosen': '0.055112', 'rewards_train/rejected': '0.05206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.003052', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-141.89', 'loss/train': '0.69642', 'examples_per_second': '30.673', 'grad_norm': '31.125', 'counters/examples': 66400, 'counters/updates': 2075}
train stats after 66432 examples: {'rewards_train/chosen': '-0.0037682', 'rewards_train/rejected': '0.016216', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019984', 'logps_train/rejected': '-115.38', 'logps_train/chosen': '-132.53', 'loss/train': '0.70721', 'examples_per_second': '31.738', 'grad_norm': '38', 'counters/examples': 66432, 'counters/updates': 2076}
train stats after 66464 examples: {'rewards_train/chosen': '0.067624', 'rewards_train/rejected': '0.072262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0046377', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-168.03', 'loss/train': '0.70255', 'examples_per_second': '31.575', 'grad_norm': '30.375', 'counters/examples': 66464, 'counters/updates': 2077}
train stats after 66496 examples: {'rewards_train/chosen': '0.053398', 'rewards_train/rejected': '0.026598', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0268', 'logps_train/rejected': '-103.51', 'logps_train/chosen': '-133.9', 'loss/train': '0.68559', 'examples_per_second': '31.655', 'grad_norm': '26.375', 'counters/examples': 66496, 'counters/updates': 2078}
train stats after 66528 examples: {'rewards_train/chosen': '0.0077604', 'rewards_train/rejected': '0.072256', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.064496', 'logps_train/rejected': '-118.52', 'logps_train/chosen': '-165.82', 'loss/train': '0.73253', 'examples_per_second': '31.776', 'grad_norm': '30', 'counters/examples': 66528, 'counters/updates': 2079}
train stats after 66560 examples: {'rewards_train/chosen': '0.12606', 'rewards_train/rejected': '0.022807', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10326', 'logps_train/rejected': '-107.01', 'logps_train/chosen': '-128.3', 'loss/train': '0.64707', 'examples_per_second': '30.448', 'grad_norm': '23.125', 'counters/examples': 66560, 'counters/updates': 2080}
train stats after 66592 examples: {'rewards_train/chosen': '0.030348', 'rewards_train/rejected': '-0.037963', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068312', 'logps_train/rejected': '-118.73', 'logps_train/chosen': '-117.84', 'loss/train': '0.66647', 'examples_per_second': '31.808', 'grad_norm': '25', 'counters/examples': 66592, 'counters/updates': 2081}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66656 examples: {'rewards_train/chosen': '0.038459', 'rewards_train/rejected': '-0.059713', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.098172', 'logps_train/rejected': '-83.049', 'logps_train/chosen': '-97.317', 'loss/train': '0.64899', 'examples_per_second': '32.814', 'grad_norm': '20.375', 'counters/examples': 66656, 'counters/updates': 2083}
skipping logging after 66688 examples to avoid logging too frequently
train stats after 66720 examples: {'rewards_train/chosen': '0.11931', 'rewards_train/rejected': '0.10968', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.009624', 'logps_train/rejected': '-126.88', 'logps_train/chosen': '-146.19', 'loss/train': '0.7114', 'examples_per_second': '31.331', 'grad_norm': '46.25', 'counters/examples': 66720, 'counters/updates': 2085}
train stats after 66752 examples: {'rewards_train/chosen': '0.11171', 'rewards_train/rejected': '0.082872', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028835', 'logps_train/rejected': '-160.99', 'logps_train/chosen': '-169.83', 'loss/train': '0.69107', 'examples_per_second': '31.606', 'grad_norm': '32', 'counters/examples': 66752, 'counters/updates': 2086}
skipping logging after 66784 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '0.15735', 'rewards_train/rejected': '0.051089', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10626', 'logps_train/rejected': '-153.18', 'logps_train/chosen': '-152.24', 'loss/train': '0.65009', 'examples_per_second': '31.245', 'grad_norm': '31.25', 'counters/examples': 66816, 'counters/updates': 2088}
train stats after 66848 examples: {'rewards_train/chosen': '-0.0071124', 'rewards_train/rejected': '0.013755', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020868', 'logps_train/rejected': '-105.24', 'logps_train/chosen': '-131.08', 'loss/train': '0.71072', 'examples_per_second': '33.079', 'grad_norm': '41.75', 'counters/examples': 66848, 'counters/updates': 2089}
train stats after 66880 examples: {'rewards_train/chosen': '0.099343', 'rewards_train/rejected': '0.055808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043535', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-130.86', 'loss/train': '0.67588', 'examples_per_second': '31.628', 'grad_norm': '26.5', 'counters/examples': 66880, 'counters/updates': 2090}
skipping logging after 66912 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '0.056469', 'rewards_train/rejected': '0.011581', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044888', 'logps_train/rejected': '-108.48', 'logps_train/chosen': '-135.55', 'loss/train': '0.67402', 'examples_per_second': '32.704', 'grad_norm': '27.375', 'counters/examples': 66944, 'counters/updates': 2092}
train stats after 66976 examples: {'rewards_train/chosen': '0.089314', 'rewards_train/rejected': '-0.0073601', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096674', 'logps_train/rejected': '-105.35', 'logps_train/chosen': '-133.79', 'loss/train': '0.65147', 'examples_per_second': '32.71', 'grad_norm': '27.875', 'counters/examples': 66976, 'counters/updates': 2093}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67040 examples: {'rewards_train/chosen': '0.11758', 'rewards_train/rejected': '0.066988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050597', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-138.87', 'loss/train': '0.67265', 'examples_per_second': '31.393', 'grad_norm': '25.75', 'counters/examples': 67040, 'counters/updates': 2095}
train stats after 67072 examples: {'rewards_train/chosen': '0.029454', 'rewards_train/rejected': '0.028484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00096966', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-110.99', 'loss/train': '0.69736', 'examples_per_second': '31.216', 'grad_norm': '27.5', 'counters/examples': 67072, 'counters/updates': 2096}
train stats after 67104 examples: {'rewards_train/chosen': '0.068464', 'rewards_train/rejected': '0.054132', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014332', 'logps_train/rejected': '-109.71', 'logps_train/chosen': '-135.04', 'loss/train': '0.69297', 'examples_per_second': '31.784', 'grad_norm': '30', 'counters/examples': 67104, 'counters/updates': 2097}
train stats after 67136 examples: {'rewards_train/chosen': '0.095073', 'rewards_train/rejected': '0.043648', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051426', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-147.85', 'loss/train': '0.67874', 'examples_per_second': '30.647', 'grad_norm': '27.375', 'counters/examples': 67136, 'counters/updates': 2098}
train stats after 67168 examples: {'rewards_train/chosen': '0.069188', 'rewards_train/rejected': '0.069262', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-7.4226e-05', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-118.58', 'loss/train': '0.70234', 'examples_per_second': '31.579', 'grad_norm': '32.75', 'counters/examples': 67168, 'counters/updates': 2099}
train stats after 67200 examples: {'rewards_train/chosen': '0.035558', 'rewards_train/rejected': '0.037885', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0023267', 'logps_train/rejected': '-103.43', 'logps_train/chosen': '-107.82', 'loss/train': '0.70199', 'examples_per_second': '30.574', 'grad_norm': '29.625', 'counters/examples': 67200, 'counters/updates': 2100}
skipping logging after 67232 examples to avoid logging too frequently
train stats after 67264 examples: {'rewards_train/chosen': '0.057226', 'rewards_train/rejected': '0.10548', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048254', 'logps_train/rejected': '-112.8', 'logps_train/chosen': '-132.11', 'loss/train': '0.72285', 'examples_per_second': '31.554', 'grad_norm': '30.625', 'counters/examples': 67264, 'counters/updates': 2102}
train stats after 67296 examples: {'rewards_train/chosen': '0.044257', 'rewards_train/rejected': '0.021169', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023088', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-105.33', 'loss/train': '0.68623', 'examples_per_second': '32.308', 'grad_norm': '24.5', 'counters/examples': 67296, 'counters/updates': 2103}
train stats after 67328 examples: {'rewards_train/chosen': '0.048435', 'rewards_train/rejected': '0.056653', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0082186', 'logps_train/rejected': '-139.77', 'logps_train/chosen': '-120.56', 'loss/train': '0.70798', 'examples_per_second': '32.132', 'grad_norm': '29.5', 'counters/examples': 67328, 'counters/updates': 2104}
skipping logging after 67360 examples to avoid logging too frequently
train stats after 67392 examples: {'rewards_train/chosen': '0.046662', 'rewards_train/rejected': '0.0068212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03984', 'logps_train/rejected': '-97.319', 'logps_train/chosen': '-118.58', 'loss/train': '0.67698', 'examples_per_second': '36.149', 'grad_norm': '23.375', 'counters/examples': 67392, 'counters/updates': 2106}
train stats after 67424 examples: {'rewards_train/chosen': '0.10901', 'rewards_train/rejected': '0.043', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06601', 'logps_train/rejected': '-137.13', 'logps_train/chosen': '-141.34', 'loss/train': '0.67123', 'examples_per_second': '31.577', 'grad_norm': '29.5', 'counters/examples': 67424, 'counters/updates': 2107}
skipping logging after 67456 examples to avoid logging too frequently
train stats after 67488 examples: {'rewards_train/chosen': '0.11364', 'rewards_train/rejected': '0.042752', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070892', 'logps_train/rejected': '-130', 'logps_train/chosen': '-151.26', 'loss/train': '0.66461', 'examples_per_second': '30.946', 'grad_norm': '26', 'counters/examples': 67488, 'counters/updates': 2109}
train stats after 67520 examples: {'rewards_train/chosen': '0.040026', 'rewards_train/rejected': '0.08904', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049014', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-128', 'loss/train': '0.7237', 'examples_per_second': '31.555', 'grad_norm': '27.375', 'counters/examples': 67520, 'counters/updates': 2110}
train stats after 67552 examples: {'rewards_train/chosen': '0.066179', 'rewards_train/rejected': '0.0089821', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057197', 'logps_train/rejected': '-92.158', 'logps_train/chosen': '-148.24', 'loss/train': '0.67098', 'examples_per_second': '31.103', 'grad_norm': '24.25', 'counters/examples': 67552, 'counters/updates': 2111}
train stats after 67584 examples: {'rewards_train/chosen': '0.091379', 'rewards_train/rejected': '0.065983', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025395', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-120.59', 'loss/train': '0.68973', 'examples_per_second': '32.664', 'grad_norm': '26.375', 'counters/examples': 67584, 'counters/updates': 2112}
train stats after 67616 examples: {'rewards_train/chosen': '0.042252', 'rewards_train/rejected': '0.086123', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.04387', 'logps_train/rejected': '-166.11', 'logps_train/chosen': '-144.99', 'loss/train': '0.72101', 'examples_per_second': '31.555', 'grad_norm': '31.5', 'counters/examples': 67616, 'counters/updates': 2113}
train stats after 67648 examples: {'rewards_train/chosen': '0.02972', 'rewards_train/rejected': '-0.019091', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048811', 'logps_train/rejected': '-87.083', 'logps_train/chosen': '-152.66', 'loss/train': '0.6744', 'examples_per_second': '32.005', 'grad_norm': '25.375', 'counters/examples': 67648, 'counters/updates': 2114}
train stats after 67680 examples: {'rewards_train/chosen': '0.081659', 'rewards_train/rejected': '0.035375', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046284', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-136.28', 'loss/train': '0.67571', 'examples_per_second': '30.601', 'grad_norm': '25.625', 'counters/examples': 67680, 'counters/updates': 2115}
train stats after 67712 examples: {'rewards_train/chosen': '0.083156', 'rewards_train/rejected': '0.030529', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052627', 'logps_train/rejected': '-160.12', 'logps_train/chosen': '-164.4', 'loss/train': '0.67823', 'examples_per_second': '31.395', 'grad_norm': '33', 'counters/examples': 67712, 'counters/updates': 2116}
train stats after 67744 examples: {'rewards_train/chosen': '0.045845', 'rewards_train/rejected': '0.028908', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016937', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-133', 'loss/train': '0.70141', 'examples_per_second': '31.621', 'grad_norm': '26.875', 'counters/examples': 67744, 'counters/updates': 2117}
train stats after 67776 examples: {'rewards_train/chosen': '0.043599', 'rewards_train/rejected': '0.020641', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022958', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-126.51', 'loss/train': '0.68737', 'examples_per_second': '31.608', 'grad_norm': '25.625', 'counters/examples': 67776, 'counters/updates': 2118}
train stats after 67808 examples: {'rewards_train/chosen': '0.10007', 'rewards_train/rejected': '0.040786', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059279', 'logps_train/rejected': '-160.12', 'logps_train/chosen': '-157.54', 'loss/train': '0.67082', 'examples_per_second': '31.615', 'grad_norm': '28.875', 'counters/examples': 67808, 'counters/updates': 2119}
train stats after 67840 examples: {'rewards_train/chosen': '0.013052', 'rewards_train/rejected': '0.012549', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00050316', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-108.29', 'loss/train': '0.69845', 'examples_per_second': '31.627', 'grad_norm': '27.25', 'counters/examples': 67840, 'counters/updates': 2120}
skipping logging after 67872 examples to avoid logging too frequently
train stats after 67904 examples: {'rewards_train/chosen': '0.14487', 'rewards_train/rejected': '0.051391', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093477', 'logps_train/rejected': '-86.565', 'logps_train/chosen': '-114.56', 'loss/train': '0.65147', 'examples_per_second': '34.497', 'grad_norm': '22.125', 'counters/examples': 67904, 'counters/updates': 2122}
train stats after 67936 examples: {'rewards_train/chosen': '0.02612', 'rewards_train/rejected': '0.056527', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030407', 'logps_train/rejected': '-124.25', 'logps_train/chosen': '-130.43', 'loss/train': '0.71283', 'examples_per_second': '32.674', 'grad_norm': '26.875', 'counters/examples': 67936, 'counters/updates': 2123}
train stats after 67968 examples: {'rewards_train/chosen': '0.10342', 'rewards_train/rejected': '0.024743', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078674', 'logps_train/rejected': '-105.69', 'logps_train/chosen': '-213.89', 'loss/train': '0.66148', 'examples_per_second': '31.212', 'grad_norm': '30.875', 'counters/examples': 67968, 'counters/updates': 2124}
skipping logging after 68000 examples to avoid logging too frequently
train stats after 68032 examples: {'rewards_train/chosen': '0.051305', 'rewards_train/rejected': '-0.021976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073281', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-146.09', 'loss/train': '0.6621', 'examples_per_second': '31.534', 'grad_norm': '28', 'counters/examples': 68032, 'counters/updates': 2126}
train stats after 68064 examples: {'rewards_train/chosen': '0.049422', 'rewards_train/rejected': '-0.03881', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088232', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-174.05', 'loss/train': '0.65361', 'examples_per_second': '30.606', 'grad_norm': '27.5', 'counters/examples': 68064, 'counters/updates': 2127}
train stats after 68096 examples: {'rewards_train/chosen': '0.042062', 'rewards_train/rejected': '0.017451', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024611', 'logps_train/rejected': '-121.71', 'logps_train/chosen': '-144.82', 'loss/train': '0.68343', 'examples_per_second': '30.091', 'grad_norm': '29', 'counters/examples': 68096, 'counters/updates': 2128}
skipping logging after 68128 examples to avoid logging too frequently
train stats after 68160 examples: {'rewards_train/chosen': '-0.020384', 'rewards_train/rejected': '0.023885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.044269', 'logps_train/rejected': '-123.33', 'logps_train/chosen': '-161.93', 'loss/train': '0.72589', 'examples_per_second': '32.412', 'grad_norm': '33.5', 'counters/examples': 68160, 'counters/updates': 2130}
train stats after 68192 examples: {'rewards_train/chosen': '0.090458', 'rewards_train/rejected': '0.066706', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023752', 'logps_train/rejected': '-144.66', 'logps_train/chosen': '-143.93', 'loss/train': '0.69102', 'examples_per_second': '30.103', 'grad_norm': '33.75', 'counters/examples': 68192, 'counters/updates': 2131}
train stats after 68224 examples: {'rewards_train/chosen': '0.12659', 'rewards_train/rejected': '0.0088442', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11775', 'logps_train/rejected': '-117.03', 'logps_train/chosen': '-132.83', 'loss/train': '0.64194', 'examples_per_second': '30.33', 'grad_norm': '24.375', 'counters/examples': 68224, 'counters/updates': 2132}
train stats after 68256 examples: {'rewards_train/chosen': '0.067917', 'rewards_train/rejected': '0.028667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03925', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-133.64', 'loss/train': '0.68057', 'examples_per_second': '30.392', 'grad_norm': '25.75', 'counters/examples': 68256, 'counters/updates': 2133}
train stats after 68288 examples: {'rewards_train/chosen': '0.035894', 'rewards_train/rejected': '0.062784', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.02689', 'logps_train/rejected': '-112.46', 'logps_train/chosen': '-139.65', 'loss/train': '0.7133', 'examples_per_second': '32.012', 'grad_norm': '28.875', 'counters/examples': 68288, 'counters/updates': 2134}
train stats after 68320 examples: {'rewards_train/chosen': '0.015795', 'rewards_train/rejected': '0.093834', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.078039', 'logps_train/rejected': '-167.98', 'logps_train/chosen': '-159.65', 'loss/train': '0.73934', 'examples_per_second': '23.813', 'grad_norm': '38.5', 'counters/examples': 68320, 'counters/updates': 2135}
train stats after 68352 examples: {'rewards_train/chosen': '0.12216', 'rewards_train/rejected': '0.031854', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090307', 'logps_train/rejected': '-142.63', 'logps_train/chosen': '-148.11', 'loss/train': '0.6629', 'examples_per_second': '31.551', 'grad_norm': '30.25', 'counters/examples': 68352, 'counters/updates': 2136}
train stats after 68384 examples: {'rewards_train/chosen': '0.046503', 'rewards_train/rejected': '-0.0083091', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054812', 'logps_train/rejected': '-134.92', 'logps_train/chosen': '-164.13', 'loss/train': '0.67199', 'examples_per_second': '32.93', 'grad_norm': '27.625', 'counters/examples': 68384, 'counters/updates': 2137}
train stats after 68416 examples: {'rewards_train/chosen': '0.088148', 'rewards_train/rejected': '0.036802', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051346', 'logps_train/rejected': '-135.79', 'logps_train/chosen': '-177.9', 'loss/train': '0.67766', 'examples_per_second': '24.226', 'grad_norm': '28.625', 'counters/examples': 68416, 'counters/updates': 2138}
train stats after 68448 examples: {'rewards_train/chosen': '0.047259', 'rewards_train/rejected': '0.086507', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.039248', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-100.42', 'loss/train': '0.71701', 'examples_per_second': '32.035', 'grad_norm': '25.5', 'counters/examples': 68448, 'counters/updates': 2139}
skipping logging after 68480 examples to avoid logging too frequently
train stats after 68512 examples: {'rewards_train/chosen': '0.10035', 'rewards_train/rejected': '0.066787', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033565', 'logps_train/rejected': '-145.33', 'logps_train/chosen': '-147.74', 'loss/train': '0.68322', 'examples_per_second': '30.056', 'grad_norm': '27.375', 'counters/examples': 68512, 'counters/updates': 2141}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68576 examples: {'rewards_train/chosen': '0.062015', 'rewards_train/rejected': '0.037787', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024228', 'logps_train/rejected': '-151.02', 'logps_train/chosen': '-141', 'loss/train': '0.6857', 'examples_per_second': '30.628', 'grad_norm': '27.125', 'counters/examples': 68576, 'counters/updates': 2143}
train stats after 68608 examples: {'rewards_train/chosen': '0.069011', 'rewards_train/rejected': '0.041024', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027987', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-157', 'loss/train': '0.68578', 'examples_per_second': '31.55', 'grad_norm': '30.5', 'counters/examples': 68608, 'counters/updates': 2144}
skipping logging after 68640 examples to avoid logging too frequently
train stats after 68672 examples: {'rewards_train/chosen': '0.070293', 'rewards_train/rejected': '0.019235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051059', 'logps_train/rejected': '-109.19', 'logps_train/chosen': '-120.1', 'loss/train': '0.67246', 'examples_per_second': '30.635', 'grad_norm': '23.875', 'counters/examples': 68672, 'counters/updates': 2146}
skipping logging after 68704 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '0.060503', 'rewards_train/rejected': '0.069726', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0092227', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-173.07', 'loss/train': '0.70529', 'examples_per_second': '31.599', 'grad_norm': '32.25', 'counters/examples': 68736, 'counters/updates': 2148}
skipping logging after 68768 examples to avoid logging too frequently
train stats after 68800 examples: {'rewards_train/chosen': '0.034914', 'rewards_train/rejected': '-0.0066533', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041567', 'logps_train/rejected': '-146.35', 'logps_train/chosen': '-135.54', 'loss/train': '0.6772', 'examples_per_second': '32.69', 'grad_norm': '27.375', 'counters/examples': 68800, 'counters/updates': 2150}
skipping logging after 68832 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '0.051783', 'rewards_train/rejected': '0.019683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032099', 'logps_train/rejected': '-120.35', 'logps_train/chosen': '-146.24', 'loss/train': '0.68051', 'examples_per_second': '30.348', 'grad_norm': '26.25', 'counters/examples': 68864, 'counters/updates': 2152}
train stats after 68896 examples: {'rewards_train/chosen': '0.046181', 'rewards_train/rejected': '0.047628', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0014471', 'logps_train/rejected': '-110.94', 'logps_train/chosen': '-149.01', 'loss/train': '0.70859', 'examples_per_second': '30.839', 'grad_norm': '27.25', 'counters/examples': 68896, 'counters/updates': 2153}
train stats after 68928 examples: {'rewards_train/chosen': '0.063473', 'rewards_train/rejected': '0.058572', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0049009', 'logps_train/rejected': '-124.56', 'logps_train/chosen': '-169.82', 'loss/train': '0.69768', 'examples_per_second': '31.435', 'grad_norm': '30.5', 'counters/examples': 68928, 'counters/updates': 2154}
skipping logging after 68960 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '0.0095987', 'rewards_train/rejected': '0.10458', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.094986', 'logps_train/rejected': '-150.71', 'logps_train/chosen': '-156.3', 'loss/train': '0.74738', 'examples_per_second': '31.717', 'grad_norm': '30.25', 'counters/examples': 68992, 'counters/updates': 2156}
train stats after 69024 examples: {'rewards_train/chosen': '0.14165', 'rewards_train/rejected': '0.007108', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13454', 'logps_train/rejected': '-177.49', 'logps_train/chosen': '-168.49', 'loss/train': '0.63849', 'examples_per_second': '30.114', 'grad_norm': '27.5', 'counters/examples': 69024, 'counters/updates': 2157}
train stats after 69056 examples: {'rewards_train/chosen': '0.064046', 'rewards_train/rejected': '0.02898', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035066', 'logps_train/rejected': '-105.95', 'logps_train/chosen': '-123.21', 'loss/train': '0.68105', 'examples_per_second': '32.665', 'grad_norm': '25.25', 'counters/examples': 69056, 'counters/updates': 2158}
train stats after 69088 examples: {'rewards_train/chosen': '0.049331', 'rewards_train/rejected': '0.065996', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016665', 'logps_train/rejected': '-166.35', 'logps_train/chosen': '-143.94', 'loss/train': '0.71461', 'examples_per_second': '30.105', 'grad_norm': '33.5', 'counters/examples': 69088, 'counters/updates': 2159}
train stats after 69120 examples: {'rewards_train/chosen': '0.059613', 'rewards_train/rejected': '0.019559', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040054', 'logps_train/rejected': '-118.84', 'logps_train/chosen': '-122.65', 'loss/train': '0.6828', 'examples_per_second': '30.587', 'grad_norm': '27.125', 'counters/examples': 69120, 'counters/updates': 2160}
skipping logging after 69152 examples to avoid logging too frequently
train stats after 69184 examples: {'rewards_train/chosen': '0.12846', 'rewards_train/rejected': '0.065166', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063291', 'logps_train/rejected': '-176.43', 'logps_train/chosen': '-195.62', 'loss/train': '0.67017', 'examples_per_second': '31.454', 'grad_norm': '33', 'counters/examples': 69184, 'counters/updates': 2162}
skipping logging after 69216 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '0.038121', 'rewards_train/rejected': '0.053286', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.015165', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-101.18', 'loss/train': '0.706', 'examples_per_second': '30.741', 'grad_norm': '26', 'counters/examples': 69248, 'counters/updates': 2164}
train stats after 69280 examples: {'rewards_train/chosen': '0.089962', 'rewards_train/rejected': '0.015975', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.073987', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-124.92', 'loss/train': '0.66162', 'examples_per_second': '31.9', 'grad_norm': '25.5', 'counters/examples': 69280, 'counters/updates': 2165}
train stats after 69312 examples: {'rewards_train/chosen': '0.1417', 'rewards_train/rejected': '0.032246', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10946', 'logps_train/rejected': '-174.84', 'logps_train/chosen': '-179.88', 'loss/train': '0.64681', 'examples_per_second': '31.63', 'grad_norm': '29', 'counters/examples': 69312, 'counters/updates': 2166}
train stats after 69344 examples: {'rewards_train/chosen': '0.1234', 'rewards_train/rejected': '0.0090974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11431', 'logps_train/rejected': '-125.61', 'logps_train/chosen': '-150.01', 'loss/train': '0.64602', 'examples_per_second': '30.063', 'grad_norm': '24.875', 'counters/examples': 69344, 'counters/updates': 2167}
train stats after 69376 examples: {'rewards_train/chosen': '0.059888', 'rewards_train/rejected': '0.017171', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.042717', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-109.84', 'loss/train': '0.67939', 'examples_per_second': '32.63', 'grad_norm': '24.875', 'counters/examples': 69376, 'counters/updates': 2168}
train stats after 69408 examples: {'rewards_train/chosen': '0.08253', 'rewards_train/rejected': '0.076531', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0059985', 'logps_train/rejected': '-117.35', 'logps_train/chosen': '-148.89', 'loss/train': '0.70086', 'examples_per_second': '30.69', 'grad_norm': '27.25', 'counters/examples': 69408, 'counters/updates': 2169}
train stats after 69440 examples: {'rewards_train/chosen': '0.09308', 'rewards_train/rejected': '0.084545', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0085346', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-131.55', 'loss/train': '0.69929', 'examples_per_second': '30.857', 'grad_norm': '29.5', 'counters/examples': 69440, 'counters/updates': 2170}
train stats after 69472 examples: {'rewards_train/chosen': '0.084601', 'rewards_train/rejected': '0.025733', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.058868', 'logps_train/rejected': '-150.01', 'logps_train/chosen': '-180.74', 'loss/train': '0.66996', 'examples_per_second': '31.051', 'grad_norm': '31.125', 'counters/examples': 69472, 'counters/updates': 2171}
skipping logging after 69504 examples to avoid logging too frequently
train stats after 69536 examples: {'rewards_train/chosen': '0.10753', 'rewards_train/rejected': '0.069952', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037574', 'logps_train/rejected': '-105.64', 'logps_train/chosen': '-171.27', 'loss/train': '0.67816', 'examples_per_second': '30.615', 'grad_norm': '30.625', 'counters/examples': 69536, 'counters/updates': 2173}
train stats after 69568 examples: {'rewards_train/chosen': '0.07956', 'rewards_train/rejected': '0.017958', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061602', 'logps_train/rejected': '-147.57', 'logps_train/chosen': '-148.73', 'loss/train': '0.67263', 'examples_per_second': '31.292', 'grad_norm': '28.125', 'counters/examples': 69568, 'counters/updates': 2174}
train stats after 69600 examples: {'rewards_train/chosen': '0.053067', 'rewards_train/rejected': '0.031372', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021695', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-142.73', 'loss/train': '0.68838', 'examples_per_second': '32.268', 'grad_norm': '27.375', 'counters/examples': 69600, 'counters/updates': 2175}
skipping logging after 69632 examples to avoid logging too frequently
train stats after 69664 examples: {'rewards_train/chosen': '0.049335', 'rewards_train/rejected': '0.037193', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012142', 'logps_train/rejected': '-74.675', 'logps_train/chosen': '-132.61', 'loss/train': '0.68956', 'examples_per_second': '31.583', 'grad_norm': '24.875', 'counters/examples': 69664, 'counters/updates': 2177}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69728 examples: {'rewards_train/chosen': '0.083168', 'rewards_train/rejected': '0.065441', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017727', 'logps_train/rejected': '-159.55', 'logps_train/chosen': '-161.67', 'loss/train': '0.69271', 'examples_per_second': '31.963', 'grad_norm': '31.625', 'counters/examples': 69728, 'counters/updates': 2179}
train stats after 69760 examples: {'rewards_train/chosen': '0.027886', 'rewards_train/rejected': '-0.0038523', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031739', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-106.01', 'loss/train': '0.68761', 'examples_per_second': '25.819', 'grad_norm': '29.75', 'counters/examples': 69760, 'counters/updates': 2180}
train stats after 69792 examples: {'rewards_train/chosen': '0.059923', 'rewards_train/rejected': '0.012387', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047537', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-123.95', 'loss/train': '0.6725', 'examples_per_second': '32.26', 'grad_norm': '28.125', 'counters/examples': 69792, 'counters/updates': 2181}
train stats after 69824 examples: {'rewards_train/chosen': '0.064422', 'rewards_train/rejected': '0.029418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035004', 'logps_train/rejected': '-130.3', 'logps_train/chosen': '-146.88', 'loss/train': '0.6813', 'examples_per_second': '31.402', 'grad_norm': '29.25', 'counters/examples': 69824, 'counters/updates': 2182}
train stats after 69856 examples: {'rewards_train/chosen': '0.11904', 'rewards_train/rejected': '0.0043056', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11474', 'logps_train/rejected': '-113.04', 'logps_train/chosen': '-136.33', 'loss/train': '0.6433', 'examples_per_second': '31.18', 'grad_norm': '26.25', 'counters/examples': 69856, 'counters/updates': 2183}
skipping logging after 69888 examples to avoid logging too frequently
train stats after 69920 examples: {'rewards_train/chosen': '0.079309', 'rewards_train/rejected': '-0.020429', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.099738', 'logps_train/rejected': '-92.46', 'logps_train/chosen': '-150.02', 'loss/train': '0.6498', 'examples_per_second': '31.838', 'grad_norm': '23.75', 'counters/examples': 69920, 'counters/updates': 2185}
train stats after 69952 examples: {'rewards_train/chosen': '0.12038', 'rewards_train/rejected': '0.050332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070044', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-148.49', 'loss/train': '0.66778', 'examples_per_second': '30.138', 'grad_norm': '27', 'counters/examples': 69952, 'counters/updates': 2186}
skipping logging after 69984 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '0.079725', 'rewards_train/rejected': '0.035633', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044092', 'logps_train/rejected': '-87.835', 'logps_train/chosen': '-109.07', 'loss/train': '0.67599', 'examples_per_second': '32.615', 'grad_norm': '21.375', 'counters/examples': 70016, 'counters/updates': 2188}
train stats after 70048 examples: {'rewards_train/chosen': '0.090143', 'rewards_train/rejected': '0.047071', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.043072', 'logps_train/rejected': '-146.11', 'logps_train/chosen': '-143.64', 'loss/train': '0.675', 'examples_per_second': '30.118', 'grad_norm': '27.625', 'counters/examples': 70048, 'counters/updates': 2189}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70112 examples: {'rewards_train/chosen': '0.062226', 'rewards_train/rejected': '-0.020187', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082413', 'logps_train/rejected': '-100.64', 'logps_train/chosen': '-102.52', 'loss/train': '0.65649', 'examples_per_second': '38.167', 'grad_norm': '22.125', 'counters/examples': 70112, 'counters/updates': 2191}
train stats after 70144 examples: {'rewards_train/chosen': '0.0084548', 'rewards_train/rejected': '0.053907', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.045453', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-186.61', 'loss/train': '0.72345', 'examples_per_second': '29.895', 'grad_norm': '40.5', 'counters/examples': 70144, 'counters/updates': 2192}
train stats after 70176 examples: {'rewards_train/chosen': '0.069119', 'rewards_train/rejected': '0.019024', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050095', 'logps_train/rejected': '-102.43', 'logps_train/chosen': '-153.84', 'loss/train': '0.6739', 'examples_per_second': '32.44', 'grad_norm': '24.875', 'counters/examples': 70176, 'counters/updates': 2193}
train stats after 70208 examples: {'rewards_train/chosen': '0.06226', 'rewards_train/rejected': '0.067795', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0055355', 'logps_train/rejected': '-144.17', 'logps_train/chosen': '-121.22', 'loss/train': '0.70087', 'examples_per_second': '31.027', 'grad_norm': '29.375', 'counters/examples': 70208, 'counters/updates': 2194}
train stats after 70240 examples: {'rewards_train/chosen': '0.10196', 'rewards_train/rejected': '0.036337', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065622', 'logps_train/rejected': '-112.03', 'logps_train/chosen': '-136.24', 'loss/train': '0.66597', 'examples_per_second': '30.153', 'grad_norm': '25.625', 'counters/examples': 70240, 'counters/updates': 2195}
train stats after 70272 examples: {'rewards_train/chosen': '0.041675', 'rewards_train/rejected': '0.0078358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033839', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-129.49', 'loss/train': '0.68083', 'examples_per_second': '32.37', 'grad_norm': '24.625', 'counters/examples': 70272, 'counters/updates': 2196}
skipping logging after 70304 examples to avoid logging too frequently
train stats after 70336 examples: {'rewards_train/chosen': '0.12386', 'rewards_train/rejected': '0.0032523', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1206', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-153.17', 'loss/train': '0.64018', 'examples_per_second': '35.891', 'grad_norm': '26.375', 'counters/examples': 70336, 'counters/updates': 2198}
skipping logging after 70368 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '0.1322', 'rewards_train/rejected': '0.01454', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11766', 'logps_train/rejected': '-112.26', 'logps_train/chosen': '-148.25', 'loss/train': '0.64128', 'examples_per_second': '30.311', 'grad_norm': '23.5', 'counters/examples': 70400, 'counters/updates': 2200}
skipping logging after 70432 examples to avoid logging too frequently
train stats after 70464 examples: {'rewards_train/chosen': '0.052574', 'rewards_train/rejected': '0.020647', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031926', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-146.51', 'loss/train': '0.68174', 'examples_per_second': '29.975', 'grad_norm': '26.875', 'counters/examples': 70464, 'counters/updates': 2202}
train stats after 70496 examples: {'rewards_train/chosen': '0.081443', 'rewards_train/rejected': '-0.011417', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09286', 'logps_train/rejected': '-129.88', 'logps_train/chosen': '-169.7', 'loss/train': '0.6547', 'examples_per_second': '30.073', 'grad_norm': '27.75', 'counters/examples': 70496, 'counters/updates': 2203}
train stats after 70528 examples: {'rewards_train/chosen': '0.072827', 'rewards_train/rejected': '0.038377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034451', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-147.8', 'loss/train': '0.68446', 'examples_per_second': '30.722', 'grad_norm': '26.75', 'counters/examples': 70528, 'counters/updates': 2204}
train stats after 70560 examples: {'rewards_train/chosen': '0.1462', 'rewards_train/rejected': '0.085798', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060404', 'logps_train/rejected': '-115.51', 'logps_train/chosen': '-154.64', 'loss/train': '0.67149', 'examples_per_second': '29.851', 'grad_norm': '28.5', 'counters/examples': 70560, 'counters/updates': 2205}
train stats after 70592 examples: {'rewards_train/chosen': '0.10173', 'rewards_train/rejected': '0.040587', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061141', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-138', 'loss/train': '0.67125', 'examples_per_second': '32.478', 'grad_norm': '29.25', 'counters/examples': 70592, 'counters/updates': 2206}
train stats after 70624 examples: {'rewards_train/chosen': '0.15449', 'rewards_train/rejected': '0.20743', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.052941', 'logps_train/rejected': '-127.89', 'logps_train/chosen': '-155.25', 'loss/train': '0.77796', 'examples_per_second': '31.495', 'grad_norm': '56.5', 'counters/examples': 70624, 'counters/updates': 2207}
train stats after 70656 examples: {'rewards_train/chosen': '0.051072', 'rewards_train/rejected': '0.049022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0020502', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-163.52', 'loss/train': '0.69892', 'examples_per_second': '31.255', 'grad_norm': '28.25', 'counters/examples': 70656, 'counters/updates': 2208}
skipping logging after 70688 examples to avoid logging too frequently
train stats after 70720 examples: {'rewards_train/chosen': '0.10424', 'rewards_train/rejected': '0.054552', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049692', 'logps_train/rejected': '-112.71', 'logps_train/chosen': '-149.2', 'loss/train': '0.67502', 'examples_per_second': '32.116', 'grad_norm': '26.5', 'counters/examples': 70720, 'counters/updates': 2210}
train stats after 70752 examples: {'rewards_train/chosen': '0.10237', 'rewards_train/rejected': '0.022893', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.079476', 'logps_train/rejected': '-102.23', 'logps_train/chosen': '-133.77', 'loss/train': '0.66059', 'examples_per_second': '31.564', 'grad_norm': '24.25', 'counters/examples': 70752, 'counters/updates': 2211}
train stats after 70784 examples: {'rewards_train/chosen': '0.090723', 'rewards_train/rejected': '0.006878', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083845', 'logps_train/rejected': '-107.61', 'logps_train/chosen': '-160.69', 'loss/train': '0.65483', 'examples_per_second': '30.847', 'grad_norm': '25.375', 'counters/examples': 70784, 'counters/updates': 2212}
skipping logging after 70816 examples to avoid logging too frequently
train stats after 70848 examples: {'rewards_train/chosen': '0.080825', 'rewards_train/rejected': '0.068438', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012388', 'logps_train/rejected': '-125.95', 'logps_train/chosen': '-146.79', 'loss/train': '0.69104', 'examples_per_second': '31.056', 'grad_norm': '30.125', 'counters/examples': 70848, 'counters/updates': 2214}
train stats after 70880 examples: {'rewards_train/chosen': '0.1078', 'rewards_train/rejected': '0.040926', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066878', 'logps_train/rejected': '-125.08', 'logps_train/chosen': '-169.95', 'loss/train': '0.66601', 'examples_per_second': '31.48', 'grad_norm': '27.25', 'counters/examples': 70880, 'counters/updates': 2215}
train stats after 70912 examples: {'rewards_train/chosen': '0.06736', 'rewards_train/rejected': '0.020849', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.046511', 'logps_train/rejected': '-114.79', 'logps_train/chosen': '-148.22', 'loss/train': '0.67556', 'examples_per_second': '32.459', 'grad_norm': '25.5', 'counters/examples': 70912, 'counters/updates': 2216}
skipping logging after 70944 examples to avoid logging too frequently
train stats after 70976 examples: {'rewards_train/chosen': '0.083889', 'rewards_train/rejected': '0.13105', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.047158', 'logps_train/rejected': '-154.78', 'logps_train/chosen': '-140.1', 'loss/train': '0.72447', 'examples_per_second': '31.532', 'grad_norm': '27', 'counters/examples': 70976, 'counters/updates': 2218}
skipping logging after 71008 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '0.056887', 'rewards_train/rejected': '0.00065695', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05623', 'logps_train/rejected': '-121.64', 'logps_train/chosen': '-142.32', 'loss/train': '0.67138', 'examples_per_second': '31.611', 'grad_norm': '26', 'counters/examples': 71040, 'counters/updates': 2220}
skipping logging after 71072 examples to avoid logging too frequently
train stats after 71104 examples: {'rewards_train/chosen': '0.092922', 'rewards_train/rejected': '-0.019897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11282', 'logps_train/rejected': '-108.12', 'logps_train/chosen': '-118.02', 'loss/train': '0.64368', 'examples_per_second': '32.118', 'grad_norm': '22.875', 'counters/examples': 71104, 'counters/updates': 2222}
skipping logging after 71136 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '0.040031', 'rewards_train/rejected': '-0.021053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061084', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-131.87', 'loss/train': '0.66876', 'examples_per_second': '34.252', 'grad_norm': '27', 'counters/examples': 71168, 'counters/updates': 2224}
train stats after 71200 examples: {'rewards_train/chosen': '0.042821', 'rewards_train/rejected': '0.043161', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0003399', 'logps_train/rejected': '-145.09', 'logps_train/chosen': '-161.31', 'loss/train': '0.69941', 'examples_per_second': '30.722', 'grad_norm': '28.375', 'counters/examples': 71200, 'counters/updates': 2225}
train stats after 71232 examples: {'rewards_train/chosen': '0.051147', 'rewards_train/rejected': '0.041868', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0092788', 'logps_train/rejected': '-81.956', 'logps_train/chosen': '-139.36', 'loss/train': '0.69988', 'examples_per_second': '32.421', 'grad_norm': '28.125', 'counters/examples': 71232, 'counters/updates': 2226}
train stats after 71264 examples: {'rewards_train/chosen': '0.099818', 'rewards_train/rejected': '0.031082', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068736', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-171.76', 'loss/train': '0.66913', 'examples_per_second': '31.26', 'grad_norm': '25.875', 'counters/examples': 71264, 'counters/updates': 2227}
train stats after 71296 examples: {'rewards_train/chosen': '0.094967', 'rewards_train/rejected': '0.075684', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019284', 'logps_train/rejected': '-107.39', 'logps_train/chosen': '-149.12', 'loss/train': '0.68835', 'examples_per_second': '30.799', 'grad_norm': '26.125', 'counters/examples': 71296, 'counters/updates': 2228}
train stats after 71328 examples: {'rewards_train/chosen': '0.050142', 'rewards_train/rejected': '-0.0016485', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05179', 'logps_train/rejected': '-146.79', 'logps_train/chosen': '-155.65', 'loss/train': '0.67538', 'examples_per_second': '32.974', 'grad_norm': '28.875', 'counters/examples': 71328, 'counters/updates': 2229}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71392 examples: {'rewards_train/chosen': '0.13129', 'rewards_train/rejected': '0.067225', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064069', 'logps_train/rejected': '-166.02', 'logps_train/chosen': '-131.44', 'loss/train': '0.67125', 'examples_per_second': '30.116', 'grad_norm': '27.375', 'counters/examples': 71392, 'counters/updates': 2231}
train stats after 71424 examples: {'rewards_train/chosen': '0.0074476', 'rewards_train/rejected': '0.037952', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.030504', 'logps_train/rejected': '-105.19', 'logps_train/chosen': '-102.9', 'loss/train': '0.71646', 'examples_per_second': '32.46', 'grad_norm': '57', 'counters/examples': 71424, 'counters/updates': 2232}
train stats after 71456 examples: {'rewards_train/chosen': '0.081088', 'rewards_train/rejected': '0.0032544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077833', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-143.98', 'loss/train': '0.65942', 'examples_per_second': '32.086', 'grad_norm': '25.5', 'counters/examples': 71456, 'counters/updates': 2233}
train stats after 71488 examples: {'rewards_train/chosen': '0.11923', 'rewards_train/rejected': '0.034698', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084531', 'logps_train/rejected': '-144.78', 'logps_train/chosen': '-178.89', 'loss/train': '0.65611', 'examples_per_second': '31.051', 'grad_norm': '30.375', 'counters/examples': 71488, 'counters/updates': 2234}
train stats after 71520 examples: {'rewards_train/chosen': '0.022082', 'rewards_train/rejected': '-0.0094164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031499', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-123.68', 'loss/train': '0.68226', 'examples_per_second': '30.633', 'grad_norm': '29', 'counters/examples': 71520, 'counters/updates': 2235}
train stats after 71552 examples: {'rewards_train/chosen': '0.037032', 'rewards_train/rejected': '0.01634', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020692', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-122.82', 'loss/train': '0.68749', 'examples_per_second': '32.317', 'grad_norm': '27.75', 'counters/examples': 71552, 'counters/updates': 2236}
train stats after 71584 examples: {'rewards_train/chosen': '-0.037234', 'rewards_train/rejected': '-0.011469', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025765', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-161.39', 'loss/train': '0.71177', 'examples_per_second': '30.546', 'grad_norm': '30.5', 'counters/examples': 71584, 'counters/updates': 2237}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71648 examples: {'rewards_train/chosen': '0.054769', 'rewards_train/rejected': '0.0061147', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048655', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-149.29', 'loss/train': '0.67398', 'examples_per_second': '31.608', 'grad_norm': '28.125', 'counters/examples': 71648, 'counters/updates': 2239}
train stats after 71680 examples: {'rewards_train/chosen': '0.092931', 'rewards_train/rejected': '0.027717', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065214', 'logps_train/rejected': '-112.81', 'logps_train/chosen': '-144.69', 'loss/train': '0.66846', 'examples_per_second': '31.504', 'grad_norm': '26.375', 'counters/examples': 71680, 'counters/updates': 2240}
train stats after 71712 examples: {'rewards_train/chosen': '0.046846', 'rewards_train/rejected': '-0.016443', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063288', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-125.03', 'loss/train': '0.66595', 'examples_per_second': '32.492', 'grad_norm': '26.875', 'counters/examples': 71712, 'counters/updates': 2241}
train stats after 71744 examples: {'rewards_train/chosen': '0.13169', 'rewards_train/rejected': '0.017586', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11411', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-182.75', 'loss/train': '0.64431', 'examples_per_second': '32.921', 'grad_norm': '29.125', 'counters/examples': 71744, 'counters/updates': 2242}
train stats after 71776 examples: {'rewards_train/chosen': '0.069329', 'rewards_train/rejected': '0.042584', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026744', 'logps_train/rejected': '-95.339', 'logps_train/chosen': '-137.85', 'loss/train': '0.68568', 'examples_per_second': '30.774', 'grad_norm': '28.875', 'counters/examples': 71776, 'counters/updates': 2243}
skipping logging after 71808 examples to avoid logging too frequently
train stats after 71840 examples: {'rewards_train/chosen': '0.047485', 'rewards_train/rejected': '0.01249', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034995', 'logps_train/rejected': '-101.86', 'logps_train/chosen': '-198.04', 'loss/train': '0.68548', 'examples_per_second': '31.907', 'grad_norm': '33', 'counters/examples': 71840, 'counters/updates': 2245}
train stats after 71872 examples: {'rewards_train/chosen': '0.024587', 'rewards_train/rejected': '-0.010468', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035055', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-152.68', 'loss/train': '0.68358', 'examples_per_second': '31.254', 'grad_norm': '46', 'counters/examples': 71872, 'counters/updates': 2246}
train stats after 71904 examples: {'rewards_train/chosen': '0.064529', 'rewards_train/rejected': '-0.00031486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064844', 'logps_train/rejected': '-108.98', 'logps_train/chosen': '-129.21', 'loss/train': '0.66719', 'examples_per_second': '32.478', 'grad_norm': '25.375', 'counters/examples': 71904, 'counters/updates': 2247}
train stats after 71936 examples: {'rewards_train/chosen': '0.034867', 'rewards_train/rejected': '-0.091111', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.12598', 'logps_train/rejected': '-112.8', 'logps_train/chosen': '-163.09', 'loss/train': '0.6412', 'examples_per_second': '31.533', 'grad_norm': '29.125', 'counters/examples': 71936, 'counters/updates': 2248}
skipping logging after 71968 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '0.11664', 'rewards_train/rejected': '0.012102', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10453', 'logps_train/rejected': '-145.46', 'logps_train/chosen': '-142.04', 'loss/train': '0.65133', 'examples_per_second': '32.503', 'grad_norm': '26.875', 'counters/examples': 72000, 'counters/updates': 2250}
Running evaluation after 72000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 72000: {'rewards_eval/chosen': '0.077227', 'rewards_eval/rejected': '0.036444', 'rewards_eval/accuracies': '0.5625', 'rewards_eval/margins': '0.040783', 'logps_eval/rejected': '-118.25', 'logps_eval/chosen': '-138.66', 'loss/eval': '0.68019'}
skipping save for non epoch
train stats after 72032 examples: {'rewards_train/chosen': '0.082421', 'rewards_train/rejected': '0.032575', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049846', 'logps_train/rejected': '-164.36', 'logps_train/chosen': '-153', 'loss/train': '0.67438', 'examples_per_second': '30.568', 'grad_norm': '28.875', 'counters/examples': 72032, 'counters/updates': 2251}
train stats after 72064 examples: {'rewards_train/chosen': '0.093807', 'rewards_train/rejected': '0.0068193', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.086987', 'logps_train/rejected': '-90.357', 'logps_train/chosen': '-133.34', 'loss/train': '0.65543', 'examples_per_second': '29.883', 'grad_norm': '27.75', 'counters/examples': 72064, 'counters/updates': 2252}
train stats after 72096 examples: {'rewards_train/chosen': '0.044098', 'rewards_train/rejected': '0.044138', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-4.0661e-05', 'logps_train/rejected': '-134.53', 'logps_train/chosen': '-183.13', 'loss/train': '0.69872', 'examples_per_second': '32.854', 'grad_norm': '31.375', 'counters/examples': 72096, 'counters/updates': 2253}
skipping logging after 72128 examples to avoid logging too frequently
train stats after 72160 examples: {'rewards_train/chosen': '0.032146', 'rewards_train/rejected': '0.014228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017918', 'logps_train/rejected': '-105.73', 'logps_train/chosen': '-114.81', 'loss/train': '0.69035', 'examples_per_second': '31.85', 'grad_norm': '25.625', 'counters/examples': 72160, 'counters/updates': 2255}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72224 examples: {'rewards_train/chosen': '0.11585', 'rewards_train/rejected': '0.031018', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084832', 'logps_train/rejected': '-98.981', 'logps_train/chosen': '-162.48', 'loss/train': '0.65551', 'examples_per_second': '31.589', 'grad_norm': '24.5', 'counters/examples': 72224, 'counters/updates': 2257}
skipping logging after 72256 examples to avoid logging too frequently
train stats after 72288 examples: {'rewards_train/chosen': '0.062245', 'rewards_train/rejected': '0.045691', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016554', 'logps_train/rejected': '-118', 'logps_train/chosen': '-156.21', 'loss/train': '0.69268', 'examples_per_second': '31.576', 'grad_norm': '42.75', 'counters/examples': 72288, 'counters/updates': 2259}
train stats after 72320 examples: {'rewards_train/chosen': '0.13274', 'rewards_train/rejected': '0.034228', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098511', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-167.43', 'loss/train': '0.65225', 'examples_per_second': '32.227', 'grad_norm': '26.625', 'counters/examples': 72320, 'counters/updates': 2260}
skipping logging after 72352 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '0.069662', 'rewards_train/rejected': '0.059585', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010078', 'logps_train/rejected': '-155.77', 'logps_train/chosen': '-145.89', 'loss/train': '0.69491', 'examples_per_second': '32.625', 'grad_norm': '28.25', 'counters/examples': 72384, 'counters/updates': 2262}
train stats after 72416 examples: {'rewards_train/chosen': '0.10539', 'rewards_train/rejected': '0.073082', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03231', 'logps_train/rejected': '-109.03', 'logps_train/chosen': '-128.49', 'loss/train': '0.68069', 'examples_per_second': '31.604', 'grad_norm': '29.5', 'counters/examples': 72416, 'counters/updates': 2263}
train stats after 72448 examples: {'rewards_train/chosen': '0.11326', 'rewards_train/rejected': '0.030162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083095', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-139.07', 'loss/train': '0.65869', 'examples_per_second': '32.096', 'grad_norm': '29', 'counters/examples': 72448, 'counters/updates': 2264}
train stats after 72480 examples: {'rewards_train/chosen': '0.072336', 'rewards_train/rejected': '0.042123', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030212', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-138.85', 'loss/train': '0.68139', 'examples_per_second': '30.2', 'grad_norm': '25.75', 'counters/examples': 72480, 'counters/updates': 2265}
train stats after 72512 examples: {'rewards_train/chosen': '0.059407', 'rewards_train/rejected': '0.015825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043582', 'logps_train/rejected': '-104.7', 'logps_train/chosen': '-130.5', 'loss/train': '0.67564', 'examples_per_second': '31.549', 'grad_norm': '25.375', 'counters/examples': 72512, 'counters/updates': 2266}
skipping logging after 72544 examples to avoid logging too frequently
train stats after 72576 examples: {'rewards_train/chosen': '0.092415', 'rewards_train/rejected': '0.081313', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011102', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-163.63', 'loss/train': '0.69594', 'examples_per_second': '30.52', 'grad_norm': '35', 'counters/examples': 72576, 'counters/updates': 2268}
train stats after 72608 examples: {'rewards_train/chosen': '0.066892', 'rewards_train/rejected': '0.049312', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01758', 'logps_train/rejected': '-109.48', 'logps_train/chosen': '-128.41', 'loss/train': '0.6911', 'examples_per_second': '32.663', 'grad_norm': '25.875', 'counters/examples': 72608, 'counters/updates': 2269}
train stats after 72640 examples: {'rewards_train/chosen': '0.056108', 'rewards_train/rejected': '-0.008584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064692', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-126.73', 'loss/train': '0.66521', 'examples_per_second': '31.996', 'grad_norm': '34.5', 'counters/examples': 72640, 'counters/updates': 2270}
train stats after 72672 examples: {'rewards_train/chosen': '0.087237', 'rewards_train/rejected': '0.046175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041063', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-128.03', 'loss/train': '0.67788', 'examples_per_second': '32.107', 'grad_norm': '28', 'counters/examples': 72672, 'counters/updates': 2271}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72736 examples: {'rewards_train/chosen': '0.095918', 'rewards_train/rejected': '0.0098328', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086085', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-125.6', 'loss/train': '0.66014', 'examples_per_second': '32.284', 'grad_norm': '28.75', 'counters/examples': 72736, 'counters/updates': 2273}
train stats after 72768 examples: {'rewards_train/chosen': '0.13159', 'rewards_train/rejected': '0.015125', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11646', 'logps_train/rejected': '-123.6', 'logps_train/chosen': '-131.43', 'loss/train': '0.6491', 'examples_per_second': '31.579', 'grad_norm': '26.5', 'counters/examples': 72768, 'counters/updates': 2274}
skipping logging after 72800 examples to avoid logging too frequently
train stats after 72832 examples: {'rewards_train/chosen': '0.070574', 'rewards_train/rejected': '0.074104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0035302', 'logps_train/rejected': '-108.85', 'logps_train/chosen': '-140.34', 'loss/train': '0.7006', 'examples_per_second': '30.741', 'grad_norm': '28.5', 'counters/examples': 72832, 'counters/updates': 2276}
skipping logging after 72864 examples to avoid logging too frequently
train stats after 72896 examples: {'rewards_train/chosen': '0.092709', 'rewards_train/rejected': '0.062927', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029782', 'logps_train/rejected': '-147.65', 'logps_train/chosen': '-144.71', 'loss/train': '0.68779', 'examples_per_second': '31.828', 'grad_norm': '34.75', 'counters/examples': 72896, 'counters/updates': 2278}
train stats after 72928 examples: {'rewards_train/chosen': '0.11343', 'rewards_train/rejected': '-0.012234', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12566', 'logps_train/rejected': '-91.241', 'logps_train/chosen': '-129.95', 'loss/train': '0.63876', 'examples_per_second': '33.025', 'grad_norm': '23.875', 'counters/examples': 72928, 'counters/updates': 2279}
train stats after 72960 examples: {'rewards_train/chosen': '0.035163', 'rewards_train/rejected': '0.033551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0016125', 'logps_train/rejected': '-154.6', 'logps_train/chosen': '-107.27', 'loss/train': '0.70305', 'examples_per_second': '30.612', 'grad_norm': '28.5', 'counters/examples': 72960, 'counters/updates': 2280}
train stats after 72992 examples: {'rewards_train/chosen': '0.072539', 'rewards_train/rejected': '0.10184', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029306', 'logps_train/rejected': '-181.24', 'logps_train/chosen': '-148.89', 'loss/train': '0.71596', 'examples_per_second': '32.637', 'grad_norm': '30.625', 'counters/examples': 72992, 'counters/updates': 2281}
train stats after 73024 examples: {'rewards_train/chosen': '0.077366', 'rewards_train/rejected': '0.029639', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047727', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-137.92', 'loss/train': '0.67366', 'examples_per_second': '31.544', 'grad_norm': '25.625', 'counters/examples': 73024, 'counters/updates': 2282}
skipping logging after 73056 examples to avoid logging too frequently
train stats after 73088 examples: {'rewards_train/chosen': '0.045568', 'rewards_train/rejected': '0.036417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0091511', 'logps_train/rejected': '-147.39', 'logps_train/chosen': '-143.9', 'loss/train': '0.69297', 'examples_per_second': '30.566', 'grad_norm': '29', 'counters/examples': 73088, 'counters/updates': 2284}
train stats after 73120 examples: {'rewards_train/chosen': '0.018147', 'rewards_train/rejected': '0.030857', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01271', 'logps_train/rejected': '-132.78', 'logps_train/chosen': '-123.78', 'loss/train': '0.7072', 'examples_per_second': '31.495', 'grad_norm': '28.75', 'counters/examples': 73120, 'counters/updates': 2285}
train stats after 73152 examples: {'rewards_train/chosen': '0.099729', 'rewards_train/rejected': '0.091681', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0080479', 'logps_train/rejected': '-155.03', 'logps_train/chosen': '-179.02', 'loss/train': '0.69634', 'examples_per_second': '33.1', 'grad_norm': '32.25', 'counters/examples': 73152, 'counters/updates': 2286}
train stats after 73184 examples: {'rewards_train/chosen': '0.096949', 'rewards_train/rejected': '0.04389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053059', 'logps_train/rejected': '-108.89', 'logps_train/chosen': '-115.65', 'loss/train': '0.67084', 'examples_per_second': '32.657', 'grad_norm': '22.125', 'counters/examples': 73184, 'counters/updates': 2287}
train stats after 73216 examples: {'rewards_train/chosen': '0.023811', 'rewards_train/rejected': '0.028601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0047902', 'logps_train/rejected': '-112.72', 'logps_train/chosen': '-132.17', 'loss/train': '0.70352', 'examples_per_second': '31.562', 'grad_norm': '31.875', 'counters/examples': 73216, 'counters/updates': 2288}
train stats after 73248 examples: {'rewards_train/chosen': '0.055579', 'rewards_train/rejected': '-0.02874', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.084319', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-134.06', 'loss/train': '0.65804', 'examples_per_second': '31.581', 'grad_norm': '32.25', 'counters/examples': 73248, 'counters/updates': 2289}
skipping logging after 73280 examples to avoid logging too frequently
train stats after 73312 examples: {'rewards_train/chosen': '0.026857', 'rewards_train/rejected': '0.052173', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.025316', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-149.19', 'loss/train': '0.71571', 'examples_per_second': '37.665', 'grad_norm': '34.25', 'counters/examples': 73312, 'counters/updates': 2291}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73376 examples: {'rewards_train/chosen': '-0.0085546', 'rewards_train/rejected': '-0.045976', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037422', 'logps_train/rejected': '-125.42', 'logps_train/chosen': '-134.82', 'loss/train': '0.68089', 'examples_per_second': '33.465', 'grad_norm': '27.25', 'counters/examples': 73376, 'counters/updates': 2293}
train stats after 73408 examples: {'rewards_train/chosen': '0.10567', 'rewards_train/rejected': '0.027143', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078529', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-142.64', 'loss/train': '0.65774', 'examples_per_second': '30.65', 'grad_norm': '33.25', 'counters/examples': 73408, 'counters/updates': 2294}
skipping logging after 73440 examples to avoid logging too frequently
train stats after 73472 examples: {'rewards_train/chosen': '0.082628', 'rewards_train/rejected': '0.0342', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048428', 'logps_train/rejected': '-101.91', 'logps_train/chosen': '-118.81', 'loss/train': '0.67286', 'examples_per_second': '31.624', 'grad_norm': '21.75', 'counters/examples': 73472, 'counters/updates': 2296}
train stats after 73504 examples: {'rewards_train/chosen': '0.16838', 'rewards_train/rejected': '0.11797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05041', 'logps_train/rejected': '-146.63', 'logps_train/chosen': '-140.73', 'loss/train': '0.67537', 'examples_per_second': '30.612', 'grad_norm': '35.75', 'counters/examples': 73504, 'counters/updates': 2297}
train stats after 73536 examples: {'rewards_train/chosen': '0.061558', 'rewards_train/rejected': '0.011684', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049874', 'logps_train/rejected': '-88.535', 'logps_train/chosen': '-138.92', 'loss/train': '0.67266', 'examples_per_second': '31.912', 'grad_norm': '24.75', 'counters/examples': 73536, 'counters/updates': 2298}
train stats after 73568 examples: {'rewards_train/chosen': '0.067243', 'rewards_train/rejected': '0.024809', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042434', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-157.03', 'loss/train': '0.67833', 'examples_per_second': '31.586', 'grad_norm': '25.375', 'counters/examples': 73568, 'counters/updates': 2299}
train stats after 73600 examples: {'rewards_train/chosen': '0.065844', 'rewards_train/rejected': '0.052153', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013691', 'logps_train/rejected': '-147.47', 'logps_train/chosen': '-163.67', 'loss/train': '0.6912', 'examples_per_second': '32.076', 'grad_norm': '35.25', 'counters/examples': 73600, 'counters/updates': 2300}
train stats after 73632 examples: {'rewards_train/chosen': '0.073101', 'rewards_train/rejected': '0.052126', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020975', 'logps_train/rejected': '-107.79', 'logps_train/chosen': '-138.81', 'loss/train': '0.69039', 'examples_per_second': '32.367', 'grad_norm': '30.375', 'counters/examples': 73632, 'counters/updates': 2301}
train stats after 73664 examples: {'rewards_train/chosen': '0.057553', 'rewards_train/rejected': '-0.0050851', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062638', 'logps_train/rejected': '-172.41', 'logps_train/chosen': '-156.27', 'loss/train': '0.6681', 'examples_per_second': '31.535', 'grad_norm': '30.5', 'counters/examples': 73664, 'counters/updates': 2302}
train stats after 73696 examples: {'rewards_train/chosen': '0.12304', 'rewards_train/rejected': '0.0729', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050138', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-130.96', 'loss/train': '0.67148', 'examples_per_second': '30.528', 'grad_norm': '27.75', 'counters/examples': 73696, 'counters/updates': 2303}
train stats after 73728 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '0.059653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076123', 'logps_train/rejected': '-141.42', 'logps_train/chosen': '-132.23', 'loss/train': '0.66163', 'examples_per_second': '30.961', 'grad_norm': '27.375', 'counters/examples': 73728, 'counters/updates': 2304}
train stats after 73760 examples: {'rewards_train/chosen': '0.11823', 'rewards_train/rejected': '0.103', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015233', 'logps_train/rejected': '-140.05', 'logps_train/chosen': '-141.44', 'loss/train': '0.69312', 'examples_per_second': '31.525', 'grad_norm': '26.5', 'counters/examples': 73760, 'counters/updates': 2305}
train stats after 73792 examples: {'rewards_train/chosen': '0.021355', 'rewards_train/rejected': '0.053719', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032364', 'logps_train/rejected': '-119.05', 'logps_train/chosen': '-123.72', 'loss/train': '0.71371', 'examples_per_second': '24.161', 'grad_norm': '24.125', 'counters/examples': 73792, 'counters/updates': 2306}
train stats after 73824 examples: {'rewards_train/chosen': '0.1285', 'rewards_train/rejected': '0.056017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072482', 'logps_train/rejected': '-146.28', 'logps_train/chosen': '-154.61', 'loss/train': '0.66506', 'examples_per_second': '31.44', 'grad_norm': '28.5', 'counters/examples': 73824, 'counters/updates': 2307}
train stats after 73856 examples: {'rewards_train/chosen': '0.039854', 'rewards_train/rejected': '0.0062916', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033563', 'logps_train/rejected': '-99.801', 'logps_train/chosen': '-130.72', 'loss/train': '0.6801', 'examples_per_second': '31.972', 'grad_norm': '26.75', 'counters/examples': 73856, 'counters/updates': 2308}
train stats after 73888 examples: {'rewards_train/chosen': '0.11318', 'rewards_train/rejected': '0.041043', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072132', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-156.45', 'loss/train': '0.66279', 'examples_per_second': '23.623', 'grad_norm': '30', 'counters/examples': 73888, 'counters/updates': 2309}
train stats after 73920 examples: {'rewards_train/chosen': '0.072215', 'rewards_train/rejected': '0.11193', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.039719', 'logps_train/rejected': '-115.91', 'logps_train/chosen': '-134.62', 'loss/train': '0.71975', 'examples_per_second': '31.979', 'grad_norm': '28.75', 'counters/examples': 73920, 'counters/updates': 2310}
train stats after 73952 examples: {'rewards_train/chosen': '0.094775', 'rewards_train/rejected': '0.082123', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012651', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-159.86', 'loss/train': '0.69473', 'examples_per_second': '30.118', 'grad_norm': '28.375', 'counters/examples': 73952, 'counters/updates': 2311}
train stats after 73984 examples: {'rewards_train/chosen': '0.039314', 'rewards_train/rejected': '0.01692', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022394', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-119.95', 'loss/train': '0.68584', 'examples_per_second': '30.419', 'grad_norm': '24.875', 'counters/examples': 73984, 'counters/updates': 2312}
skipping logging after 74016 examples to avoid logging too frequently
train stats after 74048 examples: {'rewards_train/chosen': '0.047115', 'rewards_train/rejected': '0.051849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0047339', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-135.96', 'loss/train': '0.70243', 'examples_per_second': '34.387', 'grad_norm': '26.75', 'counters/examples': 74048, 'counters/updates': 2314}
train stats after 74080 examples: {'rewards_train/chosen': '-0.02486', 'rewards_train/rejected': '0.0098706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034731', 'logps_train/rejected': '-101.83', 'logps_train/chosen': '-116.14', 'loss/train': '0.7175', 'examples_per_second': '31.332', 'grad_norm': '24.875', 'counters/examples': 74080, 'counters/updates': 2315}
train stats after 74112 examples: {'rewards_train/chosen': '0.062611', 'rewards_train/rejected': '0.022505', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040105', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-146.34', 'loss/train': '0.67972', 'examples_per_second': '29.97', 'grad_norm': '30.875', 'counters/examples': 74112, 'counters/updates': 2316}
skipping logging after 74144 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': '0.10106', 'rewards_train/rejected': '0.015873', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085187', 'logps_train/rejected': '-86.57', 'logps_train/chosen': '-126.14', 'loss/train': '0.65875', 'examples_per_second': '30.998', 'grad_norm': '22.375', 'counters/examples': 74176, 'counters/updates': 2318}
train stats after 74208 examples: {'rewards_train/chosen': '0.067904', 'rewards_train/rejected': '0.06001', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0078941', 'logps_train/rejected': '-107.29', 'logps_train/chosen': '-128.44', 'loss/train': '0.69839', 'examples_per_second': '31.848', 'grad_norm': '24.125', 'counters/examples': 74208, 'counters/updates': 2319}
train stats after 74240 examples: {'rewards_train/chosen': '0.11663', 'rewards_train/rejected': '0.023637', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092989', 'logps_train/rejected': '-100.35', 'logps_train/chosen': '-153.19', 'loss/train': '0.65531', 'examples_per_second': '31.501', 'grad_norm': '23.25', 'counters/examples': 74240, 'counters/updates': 2320}
skipping logging after 74272 examples to avoid logging too frequently
train stats after 74304 examples: {'rewards_train/chosen': '0.066286', 'rewards_train/rejected': '0.022382', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043903', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-139.57', 'loss/train': '0.67654', 'examples_per_second': '34.45', 'grad_norm': '31.125', 'counters/examples': 74304, 'counters/updates': 2322}
train stats after 74336 examples: {'rewards_train/chosen': '0.08055', 'rewards_train/rejected': '0.037898', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042652', 'logps_train/rejected': '-106.09', 'logps_train/chosen': '-151.27', 'loss/train': '0.67922', 'examples_per_second': '32.846', 'grad_norm': '26.125', 'counters/examples': 74336, 'counters/updates': 2323}
train stats after 74368 examples: {'rewards_train/chosen': '0.16269', 'rewards_train/rejected': '0.023736', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13895', 'logps_train/rejected': '-123.18', 'logps_train/chosen': '-167.92', 'loss/train': '0.63277', 'examples_per_second': '31.976', 'grad_norm': '25', 'counters/examples': 74368, 'counters/updates': 2324}
train stats after 74400 examples: {'rewards_train/chosen': '0.093929', 'rewards_train/rejected': '0.03388', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060049', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-155.79', 'loss/train': '0.67404', 'examples_per_second': '30.514', 'grad_norm': '25.375', 'counters/examples': 74400, 'counters/updates': 2325}
train stats after 74432 examples: {'rewards_train/chosen': '0.072238', 'rewards_train/rejected': '-0.0069497', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079188', 'logps_train/rejected': '-137.87', 'logps_train/chosen': '-117.22', 'loss/train': '0.65862', 'examples_per_second': '30.964', 'grad_norm': '25.5', 'counters/examples': 74432, 'counters/updates': 2326}
train stats after 74464 examples: {'rewards_train/chosen': '0.12051', 'rewards_train/rejected': '0.036552', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083962', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-159', 'loss/train': '0.65996', 'examples_per_second': '31.769', 'grad_norm': '30.625', 'counters/examples': 74464, 'counters/updates': 2327}
train stats after 74496 examples: {'rewards_train/chosen': '0.12058', 'rewards_train/rejected': '0.053314', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067265', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-134.36', 'loss/train': '0.66517', 'examples_per_second': '31.582', 'grad_norm': '27.25', 'counters/examples': 74496, 'counters/updates': 2328}
train stats after 74528 examples: {'rewards_train/chosen': '0.044994', 'rewards_train/rejected': '-0.0039562', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04895', 'logps_train/rejected': '-104.54', 'logps_train/chosen': '-95.109', 'loss/train': '0.67645', 'examples_per_second': '30.279', 'grad_norm': '24', 'counters/examples': 74528, 'counters/updates': 2329}
train stats after 74560 examples: {'rewards_train/chosen': '0.099231', 'rewards_train/rejected': '0.026375', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072856', 'logps_train/rejected': '-95.574', 'logps_train/chosen': '-134.67', 'loss/train': '0.6622', 'examples_per_second': '31.684', 'grad_norm': '23.125', 'counters/examples': 74560, 'counters/updates': 2330}
skipping logging after 74592 examples to avoid logging too frequently
train stats after 74624 examples: {'rewards_train/chosen': '0.058323', 'rewards_train/rejected': '0.036585', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021739', 'logps_train/rejected': '-110.13', 'logps_train/chosen': '-125.65', 'loss/train': '0.68783', 'examples_per_second': '30.891', 'grad_norm': '23.25', 'counters/examples': 74624, 'counters/updates': 2332}
train stats after 74656 examples: {'rewards_train/chosen': '0.088011', 'rewards_train/rejected': '0.049336', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038675', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-137.99', 'loss/train': '0.67907', 'examples_per_second': '31.459', 'grad_norm': '26.125', 'counters/examples': 74656, 'counters/updates': 2333}
skipping logging after 74688 examples to avoid logging too frequently
train stats after 74720 examples: {'rewards_train/chosen': '0.091509', 'rewards_train/rejected': '0.11168', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020168', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-154.54', 'loss/train': '0.70971', 'examples_per_second': '31.631', 'grad_norm': '29.5', 'counters/examples': 74720, 'counters/updates': 2335}
train stats after 74752 examples: {'rewards_train/chosen': '0.099103', 'rewards_train/rejected': '0.0093648', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089738', 'logps_train/rejected': '-122.77', 'logps_train/chosen': '-149.27', 'loss/train': '0.65479', 'examples_per_second': '31.379', 'grad_norm': '29.5', 'counters/examples': 74752, 'counters/updates': 2336}
train stats after 74784 examples: {'rewards_train/chosen': '0.090643', 'rewards_train/rejected': '0.020099', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.070544', 'logps_train/rejected': '-149.79', 'logps_train/chosen': '-132.57', 'loss/train': '0.6671', 'examples_per_second': '31.321', 'grad_norm': '32.25', 'counters/examples': 74784, 'counters/updates': 2337}
train stats after 74816 examples: {'rewards_train/chosen': '0.091139', 'rewards_train/rejected': '0.078277', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012862', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-122.4', 'loss/train': '0.69271', 'examples_per_second': '31.614', 'grad_norm': '24.625', 'counters/examples': 74816, 'counters/updates': 2338}
train stats after 74848 examples: {'rewards_train/chosen': '0.050328', 'rewards_train/rejected': '0.02049', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029838', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-145.89', 'loss/train': '0.68214', 'examples_per_second': '30.756', 'grad_norm': '26.125', 'counters/examples': 74848, 'counters/updates': 2339}
train stats after 74880 examples: {'rewards_train/chosen': '0.056208', 'rewards_train/rejected': '0.00073636', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055472', 'logps_train/rejected': '-76.662', 'logps_train/chosen': '-124.97', 'loss/train': '0.67048', 'examples_per_second': '31.633', 'grad_norm': '23.25', 'counters/examples': 74880, 'counters/updates': 2340}
train stats after 74912 examples: {'rewards_train/chosen': '0.065282', 'rewards_train/rejected': '0.059215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0060666', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-122.96', 'loss/train': '0.69816', 'examples_per_second': '31.614', 'grad_norm': '27.375', 'counters/examples': 74912, 'counters/updates': 2341}
train stats after 74944 examples: {'rewards_train/chosen': '0.1019', 'rewards_train/rejected': '0.0056385', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096257', 'logps_train/rejected': '-155.72', 'logps_train/chosen': '-135.32', 'loss/train': '0.65133', 'examples_per_second': '33.084', 'grad_norm': '30.375', 'counters/examples': 74944, 'counters/updates': 2342}
train stats after 74976 examples: {'rewards_train/chosen': '0.062084', 'rewards_train/rejected': '0.03435', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027734', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-143.2', 'loss/train': '0.69091', 'examples_per_second': '32.679', 'grad_norm': '26.25', 'counters/examples': 74976, 'counters/updates': 2343}
train stats after 75008 examples: {'rewards_train/chosen': '0.12369', 'rewards_train/rejected': '0.052465', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071225', 'logps_train/rejected': '-105.96', 'logps_train/chosen': '-132.43', 'loss/train': '0.66357', 'examples_per_second': '31.406', 'grad_norm': '26.5', 'counters/examples': 75008, 'counters/updates': 2344}
skipping logging after 75040 examples to avoid logging too frequently
train stats after 75072 examples: {'rewards_train/chosen': '0.041441', 'rewards_train/rejected': '0.091297', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.049856', 'logps_train/rejected': '-165.46', 'logps_train/chosen': '-158.88', 'loss/train': '0.73245', 'examples_per_second': '31.339', 'grad_norm': '31', 'counters/examples': 75072, 'counters/updates': 2346}
train stats after 75104 examples: {'rewards_train/chosen': '0.04601', 'rewards_train/rejected': '-0.0072045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053215', 'logps_train/rejected': '-94.87', 'logps_train/chosen': '-108.21', 'loss/train': '0.67145', 'examples_per_second': '31.241', 'grad_norm': '24.125', 'counters/examples': 75104, 'counters/updates': 2347}
train stats after 75136 examples: {'rewards_train/chosen': '0.047773', 'rewards_train/rejected': '0.058003', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010229', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-128.56', 'loss/train': '0.70632', 'examples_per_second': '30.152', 'grad_norm': '26.75', 'counters/examples': 75136, 'counters/updates': 2348}
train stats after 75168 examples: {'rewards_train/chosen': '0.076795', 'rewards_train/rejected': '0.0041254', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07267', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-141.48', 'loss/train': '0.66525', 'examples_per_second': '30.422', 'grad_norm': '29.875', 'counters/examples': 75168, 'counters/updates': 2349}
skipping logging after 75200 examples to avoid logging too frequently
train stats after 75232 examples: {'rewards_train/chosen': '0.090601', 'rewards_train/rejected': '0.04657', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.04403', 'logps_train/rejected': '-108.26', 'logps_train/chosen': '-164.03', 'loss/train': '0.67793', 'examples_per_second': '31.291', 'grad_norm': '28.5', 'counters/examples': 75232, 'counters/updates': 2351}
train stats after 75264 examples: {'rewards_train/chosen': '0.054012', 'rewards_train/rejected': '0.044531', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0094811', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-139.23', 'loss/train': '0.69725', 'examples_per_second': '30.482', 'grad_norm': '30.875', 'counters/examples': 75264, 'counters/updates': 2352}
skipping logging after 75296 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': '0.067167', 'rewards_train/rejected': '0.055202', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011965', 'logps_train/rejected': '-131.14', 'logps_train/chosen': '-160.02', 'loss/train': '0.68963', 'examples_per_second': '24.079', 'grad_norm': '27.375', 'counters/examples': 75328, 'counters/updates': 2354}
train stats after 75360 examples: {'rewards_train/chosen': '0.089887', 'rewards_train/rejected': '0.044151', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045736', 'logps_train/rejected': '-95.303', 'logps_train/chosen': '-127.25', 'loss/train': '0.6787', 'examples_per_second': '32.691', 'grad_norm': '25.25', 'counters/examples': 75360, 'counters/updates': 2355}
train stats after 75392 examples: {'rewards_train/chosen': '0.021731', 'rewards_train/rejected': '0.01162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010111', 'logps_train/rejected': '-111.22', 'logps_train/chosen': '-116.67', 'loss/train': '0.69122', 'examples_per_second': '30.053', 'grad_norm': '25.75', 'counters/examples': 75392, 'counters/updates': 2356}
skipping logging after 75424 examples to avoid logging too frequently
train stats after 75456 examples: {'rewards_train/chosen': '0.060107', 'rewards_train/rejected': '0.019554', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040553', 'logps_train/rejected': '-136.81', 'logps_train/chosen': '-155.66', 'loss/train': '0.68068', 'examples_per_second': '36.039', 'grad_norm': '33.75', 'counters/examples': 75456, 'counters/updates': 2358}
train stats after 75488 examples: {'rewards_train/chosen': '0.074529', 'rewards_train/rejected': '0.045486', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029044', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-147.96', 'loss/train': '0.68425', 'examples_per_second': '30.555', 'grad_norm': '29.5', 'counters/examples': 75488, 'counters/updates': 2359}
train stats after 75520 examples: {'rewards_train/chosen': '0.080229', 'rewards_train/rejected': '0.0065417', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073688', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-177.86', 'loss/train': '0.66421', 'examples_per_second': '31.578', 'grad_norm': '28.375', 'counters/examples': 75520, 'counters/updates': 2360}
train stats after 75552 examples: {'rewards_train/chosen': '0.12509', 'rewards_train/rejected': '0.025413', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099677', 'logps_train/rejected': '-141.28', 'logps_train/chosen': '-117.37', 'loss/train': '0.6497', 'examples_per_second': '30.591', 'grad_norm': '26.375', 'counters/examples': 75552, 'counters/updates': 2361}
train stats after 75584 examples: {'rewards_train/chosen': '0.099165', 'rewards_train/rejected': '0.069229', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029937', 'logps_train/rejected': '-114.52', 'logps_train/chosen': '-116.03', 'loss/train': '0.68187', 'examples_per_second': '30.194', 'grad_norm': '24', 'counters/examples': 75584, 'counters/updates': 2362}
train stats after 75616 examples: {'rewards_train/chosen': '0.12589', 'rewards_train/rejected': '0.0088449', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11705', 'logps_train/rejected': '-102.9', 'logps_train/chosen': '-122.83', 'loss/train': '0.64119', 'examples_per_second': '32.848', 'grad_norm': '21.125', 'counters/examples': 75616, 'counters/updates': 2363}
train stats after 75648 examples: {'rewards_train/chosen': '0.11563', 'rewards_train/rejected': '0.052151', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063481', 'logps_train/rejected': '-147.85', 'logps_train/chosen': '-163.9', 'loss/train': '0.66959', 'examples_per_second': '31.811', 'grad_norm': '29', 'counters/examples': 75648, 'counters/updates': 2364}
train stats after 75680 examples: {'rewards_train/chosen': '0.093131', 'rewards_train/rejected': '0.072336', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020794', 'logps_train/rejected': '-120.36', 'logps_train/chosen': '-120.49', 'loss/train': '0.68955', 'examples_per_second': '31.184', 'grad_norm': '31.5', 'counters/examples': 75680, 'counters/updates': 2365}
skipping logging after 75712 examples to avoid logging too frequently
train stats after 75744 examples: {'rewards_train/chosen': '0.084337', 'rewards_train/rejected': '0.019991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064346', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-135.53', 'loss/train': '0.6703', 'examples_per_second': '34.233', 'grad_norm': '25.5', 'counters/examples': 75744, 'counters/updates': 2367}
train stats after 75776 examples: {'rewards_train/chosen': '0.097035', 'rewards_train/rejected': '0.032606', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064429', 'logps_train/rejected': '-102.54', 'logps_train/chosen': '-147.13', 'loss/train': '0.67207', 'examples_per_second': '30.175', 'grad_norm': '27.75', 'counters/examples': 75776, 'counters/updates': 2368}
train stats after 75808 examples: {'rewards_train/chosen': '0.1604', 'rewards_train/rejected': '0.025696', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1347', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-174.57', 'loss/train': '0.64079', 'examples_per_second': '31.502', 'grad_norm': '30', 'counters/examples': 75808, 'counters/updates': 2369}
train stats after 75840 examples: {'rewards_train/chosen': '0.068639', 'rewards_train/rejected': '0.14256', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.073924', 'logps_train/rejected': '-156.46', 'logps_train/chosen': '-131.04', 'loss/train': '0.73503', 'examples_per_second': '31.045', 'grad_norm': '34', 'counters/examples': 75840, 'counters/updates': 2370}
train stats after 75872 examples: {'rewards_train/chosen': '0.067882', 'rewards_train/rejected': '0.037154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030728', 'logps_train/rejected': '-92.927', 'logps_train/chosen': '-124.7', 'loss/train': '0.6829', 'examples_per_second': '31.428', 'grad_norm': '23.125', 'counters/examples': 75872, 'counters/updates': 2371}
train stats after 75904 examples: {'rewards_train/chosen': '0.044367', 'rewards_train/rejected': '0.067295', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.022928', 'logps_train/rejected': '-116.49', 'logps_train/chosen': '-146.8', 'loss/train': '0.71145', 'examples_per_second': '31.42', 'grad_norm': '28.625', 'counters/examples': 75904, 'counters/updates': 2372}
train stats after 75936 examples: {'rewards_train/chosen': '0.13209', 'rewards_train/rejected': '0.039366', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092725', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-143.82', 'loss/train': '0.65613', 'examples_per_second': '31.491', 'grad_norm': '28.25', 'counters/examples': 75936, 'counters/updates': 2373}
train stats after 75968 examples: {'rewards_train/chosen': '0.02059', 'rewards_train/rejected': '0.031132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010542', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-130.85', 'loss/train': '0.70435', 'examples_per_second': '31.373', 'grad_norm': '27.75', 'counters/examples': 75968, 'counters/updates': 2374}
train stats after 76000 examples: {'rewards_train/chosen': '0.069126', 'rewards_train/rejected': '0.017482', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051644', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-114.13', 'loss/train': '0.6718', 'examples_per_second': '31.545', 'grad_norm': '26.5', 'counters/examples': 76000, 'counters/updates': 2375}
train stats after 76032 examples: {'rewards_train/chosen': '0.041455', 'rewards_train/rejected': '0.021377', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020078', 'logps_train/rejected': '-156.57', 'logps_train/chosen': '-182.8', 'loss/train': '0.69044', 'examples_per_second': '32.936', 'grad_norm': '29.875', 'counters/examples': 76032, 'counters/updates': 2376}
train stats after 76064 examples: {'rewards_train/chosen': '0.096732', 'rewards_train/rejected': '0.057217', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039515', 'logps_train/rejected': '-151.96', 'logps_train/chosen': '-155.79', 'loss/train': '0.68135', 'examples_per_second': '30.074', 'grad_norm': '30.875', 'counters/examples': 76064, 'counters/updates': 2377}
train stats after 76096 examples: {'rewards_train/chosen': '0.080432', 'rewards_train/rejected': '0.043624', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036808', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-118.18', 'loss/train': '0.68173', 'examples_per_second': '31.49', 'grad_norm': '24', 'counters/examples': 76096, 'counters/updates': 2378}
skipping logging after 76128 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '0.049241', 'rewards_train/rejected': '0.0058547', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043386', 'logps_train/rejected': '-104.96', 'logps_train/chosen': '-116.41', 'loss/train': '0.67594', 'examples_per_second': '34.641', 'grad_norm': '28.375', 'counters/examples': 76160, 'counters/updates': 2380}
skipping logging after 76192 examples to avoid logging too frequently
train stats after 76224 examples: {'rewards_train/chosen': '0.044151', 'rewards_train/rejected': '0.0436', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.00055075', 'logps_train/rejected': '-171.28', 'logps_train/chosen': '-161.96', 'loss/train': '0.6991', 'examples_per_second': '31.154', 'grad_norm': '31.75', 'counters/examples': 76224, 'counters/updates': 2382}
skipping logging after 76256 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '0.069425', 'rewards_train/rejected': '0.038157', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031268', 'logps_train/rejected': '-110.4', 'logps_train/chosen': '-117.14', 'loss/train': '0.68131', 'examples_per_second': '31.725', 'grad_norm': '24.5', 'counters/examples': 76288, 'counters/updates': 2384}
train stats after 76320 examples: {'rewards_train/chosen': '0.031765', 'rewards_train/rejected': '0.046342', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014576', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-144.48', 'loss/train': '0.70438', 'examples_per_second': '31.211', 'grad_norm': '25.625', 'counters/examples': 76320, 'counters/updates': 2385}
train stats after 76352 examples: {'rewards_train/chosen': '0.097012', 'rewards_train/rejected': '-0.024061', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12107', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-181.85', 'loss/train': '0.63864', 'examples_per_second': '30.301', 'grad_norm': '32', 'counters/examples': 76352, 'counters/updates': 2386}
train stats after 76384 examples: {'rewards_train/chosen': '0.10912', 'rewards_train/rejected': '0.0052031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10391', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-118.27', 'loss/train': '0.64838', 'examples_per_second': '31.879', 'grad_norm': '23.25', 'counters/examples': 76384, 'counters/updates': 2387}
train stats after 76416 examples: {'rewards_train/chosen': '0.093981', 'rewards_train/rejected': '0.074775', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019207', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-157.24', 'loss/train': '0.69184', 'examples_per_second': '31.539', 'grad_norm': '30', 'counters/examples': 76416, 'counters/updates': 2388}
train stats after 76448 examples: {'rewards_train/chosen': '0.053778', 'rewards_train/rejected': '0.046578', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0072004', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-144.18', 'loss/train': '0.69565', 'examples_per_second': '31.195', 'grad_norm': '28', 'counters/examples': 76448, 'counters/updates': 2389}
train stats after 76480 examples: {'rewards_train/chosen': '0.065089', 'rewards_train/rejected': '0.026468', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038621', 'logps_train/rejected': '-111.67', 'logps_train/chosen': '-142.33', 'loss/train': '0.67662', 'examples_per_second': '30.438', 'grad_norm': '49.25', 'counters/examples': 76480, 'counters/updates': 2390}
train stats after 76512 examples: {'rewards_train/chosen': '0.11512', 'rewards_train/rejected': '0.051056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06406', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-136', 'loss/train': '0.6664', 'examples_per_second': '30.208', 'grad_norm': '25.125', 'counters/examples': 76512, 'counters/updates': 2391}
train stats after 76544 examples: {'rewards_train/chosen': '0.067493', 'rewards_train/rejected': '0.048771', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018722', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-133.7', 'loss/train': '0.68803', 'examples_per_second': '31.557', 'grad_norm': '27.75', 'counters/examples': 76544, 'counters/updates': 2392}
skipping logging after 76576 examples to avoid logging too frequently
train stats after 76608 examples: {'rewards_train/chosen': '0.033308', 'rewards_train/rejected': '0.02765', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0056577', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-147.38', 'loss/train': '0.6955', 'examples_per_second': '35.898', 'grad_norm': '31', 'counters/examples': 76608, 'counters/updates': 2394}
train stats after 76640 examples: {'rewards_train/chosen': '0.075954', 'rewards_train/rejected': '-0.0017323', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.077686', 'logps_train/rejected': '-111.05', 'logps_train/chosen': '-106.34', 'loss/train': '0.66172', 'examples_per_second': '29.993', 'grad_norm': '26.125', 'counters/examples': 76640, 'counters/updates': 2395}
train stats after 76672 examples: {'rewards_train/chosen': '0.15424', 'rewards_train/rejected': '0.069739', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084497', 'logps_train/rejected': '-144.3', 'logps_train/chosen': '-162.97', 'loss/train': '0.65579', 'examples_per_second': '31.502', 'grad_norm': '28', 'counters/examples': 76672, 'counters/updates': 2396}
train stats after 76704 examples: {'rewards_train/chosen': '0.11697', 'rewards_train/rejected': '0.028745', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088229', 'logps_train/rejected': '-118.37', 'logps_train/chosen': '-155.91', 'loss/train': '0.65748', 'examples_per_second': '31.504', 'grad_norm': '26', 'counters/examples': 76704, 'counters/updates': 2397}
train stats after 76736 examples: {'rewards_train/chosen': '0.10936', 'rewards_train/rejected': '0.059262', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050095', 'logps_train/rejected': '-102.71', 'logps_train/chosen': '-110.31', 'loss/train': '0.67684', 'examples_per_second': '32.122', 'grad_norm': '24.375', 'counters/examples': 76736, 'counters/updates': 2398}
train stats after 76768 examples: {'rewards_train/chosen': '0.082854', 'rewards_train/rejected': '0.054709', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028145', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-141.25', 'loss/train': '0.68518', 'examples_per_second': '31.533', 'grad_norm': '30.125', 'counters/examples': 76768, 'counters/updates': 2399}
train stats after 76800 examples: {'rewards_train/chosen': '0.033459', 'rewards_train/rejected': '0.047055', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013597', 'logps_train/rejected': '-132.31', 'logps_train/chosen': '-138.66', 'loss/train': '0.70442', 'examples_per_second': '31.477', 'grad_norm': '31.75', 'counters/examples': 76800, 'counters/updates': 2400}
skipping logging after 76832 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': '0.12216', 'rewards_train/rejected': '0.051404', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070755', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-124.75', 'loss/train': '0.66219', 'examples_per_second': '31.184', 'grad_norm': '26.25', 'counters/examples': 76864, 'counters/updates': 2402}
train stats after 76896 examples: {'rewards_train/chosen': '0.13763', 'rewards_train/rejected': '0.043794', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093832', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-153.56', 'loss/train': '0.65854', 'examples_per_second': '31.559', 'grad_norm': '27', 'counters/examples': 76896, 'counters/updates': 2403}
skipping logging after 76928 examples to avoid logging too frequently
train stats after 76960 examples: {'rewards_train/chosen': '0.091249', 'rewards_train/rejected': '0.077016', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014233', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-156.88', 'loss/train': '0.69581', 'examples_per_second': '30.083', 'grad_norm': '29.125', 'counters/examples': 76960, 'counters/updates': 2405}
train stats after 76992 examples: {'rewards_train/chosen': '0.04061', 'rewards_train/rejected': '-0.062706', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10332', 'logps_train/rejected': '-98.86', 'logps_train/chosen': '-136.23', 'loss/train': '0.6483', 'examples_per_second': '32.461', 'grad_norm': '26.625', 'counters/examples': 76992, 'counters/updates': 2406}
train stats after 77024 examples: {'rewards_train/chosen': '0.11369', 'rewards_train/rejected': '0.030548', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083145', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-144.44', 'loss/train': '0.65785', 'examples_per_second': '32.927', 'grad_norm': '28.375', 'counters/examples': 77024, 'counters/updates': 2407}
train stats after 77056 examples: {'rewards_train/chosen': '0.041126', 'rewards_train/rejected': '-0.0039566', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045083', 'logps_train/rejected': '-110.34', 'logps_train/chosen': '-124.39', 'loss/train': '0.67467', 'examples_per_second': '31.202', 'grad_norm': '25.625', 'counters/examples': 77056, 'counters/updates': 2408}
train stats after 77088 examples: {'rewards_train/chosen': '0.014955', 'rewards_train/rejected': '0.043917', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028962', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-146.06', 'loss/train': '0.71756', 'examples_per_second': '32.444', 'grad_norm': '31.375', 'counters/examples': 77088, 'counters/updates': 2409}
skipping logging after 77120 examples to avoid logging too frequently
train stats after 77152 examples: {'rewards_train/chosen': '0.093946', 'rewards_train/rejected': '0.081056', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012889', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-147.97', 'loss/train': '0.69325', 'examples_per_second': '31.06', 'grad_norm': '27.625', 'counters/examples': 77152, 'counters/updates': 2411}
train stats after 77184 examples: {'rewards_train/chosen': '0.12254', 'rewards_train/rejected': '0.044541', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077998', 'logps_train/rejected': '-104.76', 'logps_train/chosen': '-161.71', 'loss/train': '0.66175', 'examples_per_second': '30.947', 'grad_norm': '26.125', 'counters/examples': 77184, 'counters/updates': 2412}
train stats after 77216 examples: {'rewards_train/chosen': '0.084879', 'rewards_train/rejected': '0.020347', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064532', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-147.57', 'loss/train': '0.66582', 'examples_per_second': '31.409', 'grad_norm': '25.5', 'counters/examples': 77216, 'counters/updates': 2413}
train stats after 77248 examples: {'rewards_train/chosen': '0.15496', 'rewards_train/rejected': '0.028446', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12652', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-160.49', 'loss/train': '0.6384', 'examples_per_second': '30.011', 'grad_norm': '28.125', 'counters/examples': 77248, 'counters/updates': 2414}
train stats after 77280 examples: {'rewards_train/chosen': '0.071002', 'rewards_train/rejected': '0.031815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039188', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-116.47', 'loss/train': '0.67781', 'examples_per_second': '31.187', 'grad_norm': '26.625', 'counters/examples': 77280, 'counters/updates': 2415}
train stats after 77312 examples: {'rewards_train/chosen': '0.064049', 'rewards_train/rejected': '0.026591', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037458', 'logps_train/rejected': '-152.74', 'logps_train/chosen': '-151.86', 'loss/train': '0.68711', 'examples_per_second': '30.001', 'grad_norm': '29.75', 'counters/examples': 77312, 'counters/updates': 2416}
train stats after 77344 examples: {'rewards_train/chosen': '0.12782', 'rewards_train/rejected': '0.071837', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055983', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-129.34', 'loss/train': '0.66946', 'examples_per_second': '31.442', 'grad_norm': '27.375', 'counters/examples': 77344, 'counters/updates': 2417}
train stats after 77376 examples: {'rewards_train/chosen': '0.10038', 'rewards_train/rejected': '0.053244', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047138', 'logps_train/rejected': '-120.07', 'logps_train/chosen': '-123.94', 'loss/train': '0.67749', 'examples_per_second': '31.743', 'grad_norm': '25.75', 'counters/examples': 77376, 'counters/updates': 2418}
train stats after 77408 examples: {'rewards_train/chosen': '0.060718', 'rewards_train/rejected': '-0.014327', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075046', 'logps_train/rejected': '-103.14', 'logps_train/chosen': '-122.78', 'loss/train': '0.66258', 'examples_per_second': '31.543', 'grad_norm': '26.5', 'counters/examples': 77408, 'counters/updates': 2419}
skipping logging after 77440 examples to avoid logging too frequently
train stats after 77472 examples: {'rewards_train/chosen': '0.066716', 'rewards_train/rejected': '0.0833', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016584', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-109.81', 'loss/train': '0.70565', 'examples_per_second': '32.556', 'grad_norm': '25.5', 'counters/examples': 77472, 'counters/updates': 2421}
train stats after 77504 examples: {'rewards_train/chosen': '0.061241', 'rewards_train/rejected': '0.04499', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016252', 'logps_train/rejected': '-109.7', 'logps_train/chosen': '-117.13', 'loss/train': '0.69036', 'examples_per_second': '31.301', 'grad_norm': '25', 'counters/examples': 77504, 'counters/updates': 2422}
skipping logging after 77536 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '0.059309', 'rewards_train/rejected': '0.047171', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012138', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-127.04', 'loss/train': '0.69139', 'examples_per_second': '30.138', 'grad_norm': '27.25', 'counters/examples': 77568, 'counters/updates': 2424}
train stats after 77600 examples: {'rewards_train/chosen': '0.093859', 'rewards_train/rejected': '-0.04709', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14095', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-145.72', 'loss/train': '0.63116', 'examples_per_second': '31.046', 'grad_norm': '25.75', 'counters/examples': 77600, 'counters/updates': 2425}
train stats after 77632 examples: {'rewards_train/chosen': '0.09419', 'rewards_train/rejected': '0.066167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028023', 'logps_train/rejected': '-122.7', 'logps_train/chosen': '-131.15', 'loss/train': '0.68766', 'examples_per_second': '30.508', 'grad_norm': '35.5', 'counters/examples': 77632, 'counters/updates': 2426}
train stats after 77664 examples: {'rewards_train/chosen': '0.11995', 'rewards_train/rejected': '0.051155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068796', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-133.65', 'loss/train': '0.6642', 'examples_per_second': '31.948', 'grad_norm': '24', 'counters/examples': 77664, 'counters/updates': 2427}
train stats after 77696 examples: {'rewards_train/chosen': '0.097558', 'rewards_train/rejected': '0.061868', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03569', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-150', 'loss/train': '0.68281', 'examples_per_second': '31.51', 'grad_norm': '27', 'counters/examples': 77696, 'counters/updates': 2428}
train stats after 77728 examples: {'rewards_train/chosen': '0.079732', 'rewards_train/rejected': '0.042042', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03769', 'logps_train/rejected': '-145.12', 'logps_train/chosen': '-131.94', 'loss/train': '0.67821', 'examples_per_second': '32.588', 'grad_norm': '27.375', 'counters/examples': 77728, 'counters/updates': 2429}
train stats after 77760 examples: {'rewards_train/chosen': '0.08354', 'rewards_train/rejected': '0.064783', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018758', 'logps_train/rejected': '-117.88', 'logps_train/chosen': '-162.45', 'loss/train': '0.69055', 'examples_per_second': '31.325', 'grad_norm': '29.875', 'counters/examples': 77760, 'counters/updates': 2430}
skipping logging after 77792 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '0.073038', 'rewards_train/rejected': '0.076969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0039305', 'logps_train/rejected': '-139.23', 'logps_train/chosen': '-166.06', 'loss/train': '0.7068', 'examples_per_second': '30.007', 'grad_norm': '33.25', 'counters/examples': 77824, 'counters/updates': 2432}
train stats after 77856 examples: {'rewards_train/chosen': '0.14268', 'rewards_train/rejected': '0.032527', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11016', 'logps_train/rejected': '-109.85', 'logps_train/chosen': '-132.66', 'loss/train': '0.64478', 'examples_per_second': '30.545', 'grad_norm': '25', 'counters/examples': 77856, 'counters/updates': 2433}
train stats after 77888 examples: {'rewards_train/chosen': '0.10886', 'rewards_train/rejected': '0.019927', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088935', 'logps_train/rejected': '-109.8', 'logps_train/chosen': '-127.97', 'loss/train': '0.65503', 'examples_per_second': '30.447', 'grad_norm': '28.125', 'counters/examples': 77888, 'counters/updates': 2434}
train stats after 77920 examples: {'rewards_train/chosen': '0.046385', 'rewards_train/rejected': '0.03844', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0079445', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-151.78', 'loss/train': '0.69367', 'examples_per_second': '32.278', 'grad_norm': '30.5', 'counters/examples': 77920, 'counters/updates': 2435}
train stats after 77952 examples: {'rewards_train/chosen': '0.017142', 'rewards_train/rejected': '0.067779', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.050637', 'logps_train/rejected': '-115.19', 'logps_train/chosen': '-139.33', 'loss/train': '0.72357', 'examples_per_second': '32.319', 'grad_norm': '27.375', 'counters/examples': 77952, 'counters/updates': 2436}
train stats after 77984 examples: {'rewards_train/chosen': '0.067673', 'rewards_train/rejected': '0.026829', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040844', 'logps_train/rejected': '-98.415', 'logps_train/chosen': '-109.8', 'loss/train': '0.67999', 'examples_per_second': '30.039', 'grad_norm': '26.625', 'counters/examples': 77984, 'counters/updates': 2437}
train stats after 78016 examples: {'rewards_train/chosen': '0.10691', 'rewards_train/rejected': '0.046823', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06009', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-161.14', 'loss/train': '0.6706', 'examples_per_second': '30.985', 'grad_norm': '28', 'counters/examples': 78016, 'counters/updates': 2438}
train stats after 78048 examples: {'rewards_train/chosen': '0.057791', 'rewards_train/rejected': '0.0366', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021191', 'logps_train/rejected': '-102.23', 'logps_train/chosen': '-120.47', 'loss/train': '0.68715', 'examples_per_second': '31.3', 'grad_norm': '24.375', 'counters/examples': 78048, 'counters/updates': 2439}
train stats after 78080 examples: {'rewards_train/chosen': '0.13718', 'rewards_train/rejected': '0.025625', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11156', 'logps_train/rejected': '-161.4', 'logps_train/chosen': '-143.76', 'loss/train': '0.64211', 'examples_per_second': '31.592', 'grad_norm': '28.125', 'counters/examples': 78080, 'counters/updates': 2440}
skipping logging after 78112 examples to avoid logging too frequently
train stats after 78144 examples: {'rewards_train/chosen': '0.02355', 'rewards_train/rejected': '0.0040628', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.019487', 'logps_train/rejected': '-125.67', 'logps_train/chosen': '-162.75', 'loss/train': '0.69011', 'examples_per_second': '36.312', 'grad_norm': '29.375', 'counters/examples': 78144, 'counters/updates': 2442}
train stats after 78176 examples: {'rewards_train/chosen': '0.093537', 'rewards_train/rejected': '0.053252', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040286', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-146.02', 'loss/train': '0.67766', 'examples_per_second': '30.078', 'grad_norm': '26.875', 'counters/examples': 78176, 'counters/updates': 2443}
train stats after 78208 examples: {'rewards_train/chosen': '0.079902', 'rewards_train/rejected': '0.028071', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051831', 'logps_train/rejected': '-146.16', 'logps_train/chosen': '-143.89', 'loss/train': '0.67872', 'examples_per_second': '31.23', 'grad_norm': '28.625', 'counters/examples': 78208, 'counters/updates': 2444}
train stats after 78240 examples: {'rewards_train/chosen': '0.092512', 'rewards_train/rejected': '0.073131', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019381', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-132.92', 'loss/train': '0.6918', 'examples_per_second': '30.031', 'grad_norm': '29.75', 'counters/examples': 78240, 'counters/updates': 2445}
train stats after 78272 examples: {'rewards_train/chosen': '0.16133', 'rewards_train/rejected': '0.050475', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11086', 'logps_train/rejected': '-97.783', 'logps_train/chosen': '-153.08', 'loss/train': '0.64511', 'examples_per_second': '32.639', 'grad_norm': '28.625', 'counters/examples': 78272, 'counters/updates': 2446}
skipping logging after 78304 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '0.095739', 'rewards_train/rejected': '0.055971', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039768', 'logps_train/rejected': '-133.63', 'logps_train/chosen': '-149.99', 'loss/train': '0.67788', 'examples_per_second': '31.526', 'grad_norm': '27.375', 'counters/examples': 78336, 'counters/updates': 2448}
train stats after 78368 examples: {'rewards_train/chosen': '0.10698', 'rewards_train/rejected': '-0.0086024', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11558', 'logps_train/rejected': '-163.96', 'logps_train/chosen': '-125.79', 'loss/train': '0.64954', 'examples_per_second': '30.957', 'grad_norm': '26.125', 'counters/examples': 78368, 'counters/updates': 2449}
skipping logging after 78400 examples to avoid logging too frequently
train stats after 78432 examples: {'rewards_train/chosen': '0.09139', 'rewards_train/rejected': '0.082197', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0091927', 'logps_train/rejected': '-109.29', 'logps_train/chosen': '-129.79', 'loss/train': '0.69452', 'examples_per_second': '33.871', 'grad_norm': '28.875', 'counters/examples': 78432, 'counters/updates': 2451}
train stats after 78464 examples: {'rewards_train/chosen': '0.084394', 'rewards_train/rejected': '0.072945', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011449', 'logps_train/rejected': '-108.02', 'logps_train/chosen': '-111.79', 'loss/train': '0.69251', 'examples_per_second': '31.469', 'grad_norm': '23.875', 'counters/examples': 78464, 'counters/updates': 2452}
train stats after 78496 examples: {'rewards_train/chosen': '0.15011', 'rewards_train/rejected': '0.012497', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13761', 'logps_train/rejected': '-93.527', 'logps_train/chosen': '-145.9', 'loss/train': '0.63112', 'examples_per_second': '31.678', 'grad_norm': '26.25', 'counters/examples': 78496, 'counters/updates': 2453}
skipping logging after 78528 examples to avoid logging too frequently
train stats after 78560 examples: {'rewards_train/chosen': '0.11755', 'rewards_train/rejected': '0.020945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096609', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-104.52', 'loss/train': '0.65183', 'examples_per_second': '31.497', 'grad_norm': '22.75', 'counters/examples': 78560, 'counters/updates': 2455}
skipping logging after 78592 examples to avoid logging too frequently
train stats after 78624 examples: {'rewards_train/chosen': '0.071746', 'rewards_train/rejected': '-0.018777', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.090523', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-125.35', 'loss/train': '0.65261', 'examples_per_second': '31.136', 'grad_norm': '27.375', 'counters/examples': 78624, 'counters/updates': 2457}
train stats after 78656 examples: {'rewards_train/chosen': '0.16848', 'rewards_train/rejected': '-0.0071672', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17565', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-182.15', 'loss/train': '0.61748', 'examples_per_second': '31.549', 'grad_norm': '27.75', 'counters/examples': 78656, 'counters/updates': 2458}
train stats after 78688 examples: {'rewards_train/chosen': '0.063896', 'rewards_train/rejected': '0.061473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.002423', 'logps_train/rejected': '-96.493', 'logps_train/chosen': '-126.14', 'loss/train': '0.6979', 'examples_per_second': '31.468', 'grad_norm': '31.5', 'counters/examples': 78688, 'counters/updates': 2459}
train stats after 78720 examples: {'rewards_train/chosen': '0.09871', 'rewards_train/rejected': '0.15812', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.059407', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-173.59', 'loss/train': '0.7292', 'examples_per_second': '30.622', 'grad_norm': '29.375', 'counters/examples': 78720, 'counters/updates': 2460}
train stats after 78752 examples: {'rewards_train/chosen': '0.13024', 'rewards_train/rejected': '0.036733', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093509', 'logps_train/rejected': '-88.256', 'logps_train/chosen': '-164.39', 'loss/train': '0.65757', 'examples_per_second': '31.073', 'grad_norm': '24.75', 'counters/examples': 78752, 'counters/updates': 2461}
train stats after 78784 examples: {'rewards_train/chosen': '0.13451', 'rewards_train/rejected': '0.032557', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10195', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-128.41', 'loss/train': '0.65505', 'examples_per_second': '31.496', 'grad_norm': '31.875', 'counters/examples': 78784, 'counters/updates': 2462}
train stats after 78816 examples: {'rewards_train/chosen': '0.092068', 'rewards_train/rejected': '0.057777', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034291', 'logps_train/rejected': '-123.53', 'logps_train/chosen': '-165.98', 'loss/train': '0.68227', 'examples_per_second': '32.848', 'grad_norm': '25.75', 'counters/examples': 78816, 'counters/updates': 2463}
train stats after 78848 examples: {'rewards_train/chosen': '0.18575', 'rewards_train/rejected': '0.07854', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10721', 'logps_train/rejected': '-163.92', 'logps_train/chosen': '-140.09', 'loss/train': '0.65315', 'examples_per_second': '33.199', 'grad_norm': '28.125', 'counters/examples': 78848, 'counters/updates': 2464}
train stats after 78880 examples: {'rewards_train/chosen': '0.06086', 'rewards_train/rejected': '0.022542', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038319', 'logps_train/rejected': '-94.14', 'logps_train/chosen': '-118.89', 'loss/train': '0.68155', 'examples_per_second': '32.301', 'grad_norm': '24.5', 'counters/examples': 78880, 'counters/updates': 2465}
train stats after 78912 examples: {'rewards_train/chosen': '0.063106', 'rewards_train/rejected': '0.018192', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044913', 'logps_train/rejected': '-126.4', 'logps_train/chosen': '-152.82', 'loss/train': '0.67378', 'examples_per_second': '31.779', 'grad_norm': '26', 'counters/examples': 78912, 'counters/updates': 2466}
train stats after 78944 examples: {'rewards_train/chosen': '0.11837', 'rewards_train/rejected': '0.064976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053395', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-143.08', 'loss/train': '0.67631', 'examples_per_second': '30.981', 'grad_norm': '28.25', 'counters/examples': 78944, 'counters/updates': 2467}
train stats after 78976 examples: {'rewards_train/chosen': '0.10921', 'rewards_train/rejected': '0.043479', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.065729', 'logps_train/rejected': '-125.45', 'logps_train/chosen': '-132.37', 'loss/train': '0.66781', 'examples_per_second': '30.053', 'grad_norm': '26.125', 'counters/examples': 78976, 'counters/updates': 2468}
train stats after 79008 examples: {'rewards_train/chosen': '0.080976', 'rewards_train/rejected': '0.093564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012588', 'logps_train/rejected': '-93.003', 'logps_train/chosen': '-111.13', 'loss/train': '0.70667', 'examples_per_second': '31.204', 'grad_norm': '24.625', 'counters/examples': 79008, 'counters/updates': 2469}
train stats after 79040 examples: {'rewards_train/chosen': '0.10237', 'rewards_train/rejected': '0.053151', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04922', 'logps_train/rejected': '-113.3', 'logps_train/chosen': '-148.55', 'loss/train': '0.6738', 'examples_per_second': '31.045', 'grad_norm': '25.5', 'counters/examples': 79040, 'counters/updates': 2470}
skipping logging after 79072 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '0.072844', 'rewards_train/rejected': '0.039824', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03302', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-143.45', 'loss/train': '0.68188', 'examples_per_second': '34.823', 'grad_norm': '28.875', 'counters/examples': 79104, 'counters/updates': 2472}
skipping logging after 79136 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': '0.04783', 'rewards_train/rejected': '0.025782', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022048', 'logps_train/rejected': '-102.9', 'logps_train/chosen': '-159.71', 'loss/train': '0.68786', 'examples_per_second': '31.694', 'grad_norm': '26.25', 'counters/examples': 79168, 'counters/updates': 2474}
train stats after 79200 examples: {'rewards_train/chosen': '0.1317', 'rewards_train/rejected': '0.052275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079422', 'logps_train/rejected': '-150', 'logps_train/chosen': '-191.96', 'loss/train': '0.66354', 'examples_per_second': '30.55', 'grad_norm': '30.5', 'counters/examples': 79200, 'counters/updates': 2475}
skipping logging after 79232 examples to avoid logging too frequently
train stats after 79264 examples: {'rewards_train/chosen': '0.062043', 'rewards_train/rejected': '0.061128', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00091485', 'logps_train/rejected': '-116.59', 'logps_train/chosen': '-137.64', 'loss/train': '0.70012', 'examples_per_second': '24.271', 'grad_norm': '36', 'counters/examples': 79264, 'counters/updates': 2477}
train stats after 79296 examples: {'rewards_train/chosen': '0.033217', 'rewards_train/rejected': '0.058465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.025249', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-144.87', 'loss/train': '0.71246', 'examples_per_second': '30.09', 'grad_norm': '28.375', 'counters/examples': 79296, 'counters/updates': 2478}
train stats after 79328 examples: {'rewards_train/chosen': '0.091014', 'rewards_train/rejected': '0.071307', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.019708', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-137.73', 'loss/train': '0.69075', 'examples_per_second': '31.047', 'grad_norm': '26.25', 'counters/examples': 79328, 'counters/updates': 2479}
train stats after 79360 examples: {'rewards_train/chosen': '0.038768', 'rewards_train/rejected': '0.041354', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0025861', 'logps_train/rejected': '-108.71', 'logps_train/chosen': '-124.88', 'loss/train': '0.70013', 'examples_per_second': '27.586', 'grad_norm': '35.25', 'counters/examples': 79360, 'counters/updates': 2480}
train stats after 79392 examples: {'rewards_train/chosen': '0.081974', 'rewards_train/rejected': '0.088455', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0064812', 'logps_train/rejected': '-129.32', 'logps_train/chosen': '-128.76', 'loss/train': '0.70405', 'examples_per_second': '31.16', 'grad_norm': '45', 'counters/examples': 79392, 'counters/updates': 2481}
train stats after 79424 examples: {'rewards_train/chosen': '0.10311', 'rewards_train/rejected': '0.062469', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040637', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-138.31', 'loss/train': '0.6783', 'examples_per_second': '32.119', 'grad_norm': '23.875', 'counters/examples': 79424, 'counters/updates': 2482}
train stats after 79456 examples: {'rewards_train/chosen': '0.072471', 'rewards_train/rejected': '0.011744', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060727', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-137.1', 'loss/train': '0.66787', 'examples_per_second': '32.093', 'grad_norm': '26.5', 'counters/examples': 79456, 'counters/updates': 2483}
train stats after 79488 examples: {'rewards_train/chosen': '0.086794', 'rewards_train/rejected': '0.038479', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048315', 'logps_train/rejected': '-110.42', 'logps_train/chosen': '-149.56', 'loss/train': '0.67275', 'examples_per_second': '30.956', 'grad_norm': '26.125', 'counters/examples': 79488, 'counters/updates': 2484}
train stats after 79520 examples: {'rewards_train/chosen': '0.12279', 'rewards_train/rejected': '0.095583', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027208', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-132.04', 'loss/train': '0.68715', 'examples_per_second': '31.494', 'grad_norm': '30.375', 'counters/examples': 79520, 'counters/updates': 2485}
train stats after 79552 examples: {'rewards_train/chosen': '0.097871', 'rewards_train/rejected': '0.062223', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035648', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-100.09', 'loss/train': '0.68006', 'examples_per_second': '33.176', 'grad_norm': '24.375', 'counters/examples': 79552, 'counters/updates': 2486}
train stats after 79584 examples: {'rewards_train/chosen': '0.048736', 'rewards_train/rejected': '0.055997', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0072603', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-185.85', 'loss/train': '0.70139', 'examples_per_second': '31.505', 'grad_norm': '27.375', 'counters/examples': 79584, 'counters/updates': 2487}
train stats after 79616 examples: {'rewards_train/chosen': '0.14064', 'rewards_train/rejected': '-0.052389', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19303', 'logps_train/rejected': '-165.39', 'logps_train/chosen': '-166.51', 'loss/train': '0.61717', 'examples_per_second': '31.368', 'grad_norm': '30', 'counters/examples': 79616, 'counters/updates': 2488}
train stats after 79648 examples: {'rewards_train/chosen': '0.10945', 'rewards_train/rejected': '0.034112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075338', 'logps_train/rejected': '-138.39', 'logps_train/chosen': '-154.37', 'loss/train': '0.66432', 'examples_per_second': '31.844', 'grad_norm': '28.875', 'counters/examples': 79648, 'counters/updates': 2489}
train stats after 79680 examples: {'rewards_train/chosen': '0.056679', 'rewards_train/rejected': '0.050094', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0065848', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-141.14', 'loss/train': '0.69284', 'examples_per_second': '31.489', 'grad_norm': '26.625', 'counters/examples': 79680, 'counters/updates': 2490}
train stats after 79712 examples: {'rewards_train/chosen': '0.090874', 'rewards_train/rejected': '0.022862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068012', 'logps_train/rejected': '-120.28', 'logps_train/chosen': '-156.42', 'loss/train': '0.67006', 'examples_per_second': '31.119', 'grad_norm': '25.25', 'counters/examples': 79712, 'counters/updates': 2491}
train stats after 79744 examples: {'rewards_train/chosen': '0.069489', 'rewards_train/rejected': '0.049232', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020257', 'logps_train/rejected': '-126.98', 'logps_train/chosen': '-179.21', 'loss/train': '0.69138', 'examples_per_second': '30.553', 'grad_norm': '29.75', 'counters/examples': 79744, 'counters/updates': 2492}
train stats after 79776 examples: {'rewards_train/chosen': '0.060158', 'rewards_train/rejected': '0.071962', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011804', 'logps_train/rejected': '-156.73', 'logps_train/chosen': '-142.59', 'loss/train': '0.70618', 'examples_per_second': '31.437', 'grad_norm': '35', 'counters/examples': 79776, 'counters/updates': 2493}
train stats after 79808 examples: {'rewards_train/chosen': '0.097394', 'rewards_train/rejected': '0.065363', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032031', 'logps_train/rejected': '-142.2', 'logps_train/chosen': '-162.54', 'loss/train': '0.68126', 'examples_per_second': '32.444', 'grad_norm': '26.875', 'counters/examples': 79808, 'counters/updates': 2494}
train stats after 79840 examples: {'rewards_train/chosen': '0.11506', 'rewards_train/rejected': '0.02474', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090316', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-136.89', 'loss/train': '0.65891', 'examples_per_second': '30.784', 'grad_norm': '25.75', 'counters/examples': 79840, 'counters/updates': 2495}
train stats after 79872 examples: {'rewards_train/chosen': '0.060014', 'rewards_train/rejected': '0.078944', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018929', 'logps_train/rejected': '-155.02', 'logps_train/chosen': '-112.88', 'loss/train': '0.70891', 'examples_per_second': '31.433', 'grad_norm': '29.5', 'counters/examples': 79872, 'counters/updates': 2496}
train stats after 79904 examples: {'rewards_train/chosen': '0.14416', 'rewards_train/rejected': '0.093482', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050679', 'logps_train/rejected': '-141.5', 'logps_train/chosen': '-138.92', 'loss/train': '0.67397', 'examples_per_second': '30.039', 'grad_norm': '35', 'counters/examples': 79904, 'counters/updates': 2497}
train stats after 79936 examples: {'rewards_train/chosen': '0.087369', 'rewards_train/rejected': '0.016897', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070472', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-154.73', 'loss/train': '0.66474', 'examples_per_second': '31.506', 'grad_norm': '28.875', 'counters/examples': 79936, 'counters/updates': 2498}
train stats after 79968 examples: {'rewards_train/chosen': '0.092291', 'rewards_train/rejected': '0.04503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047261', 'logps_train/rejected': '-120.8', 'logps_train/chosen': '-118.88', 'loss/train': '0.67572', 'examples_per_second': '31.469', 'grad_norm': '24.375', 'counters/examples': 79968, 'counters/updates': 2499}
train stats after 80000 examples: {'rewards_train/chosen': '0.14577', 'rewards_train/rejected': '0.064798', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080975', 'logps_train/rejected': '-169.05', 'logps_train/chosen': '-146.63', 'loss/train': '0.66084', 'examples_per_second': '31.431', 'grad_norm': '34.75', 'counters/examples': 80000, 'counters/updates': 2500}
train stats after 80032 examples: {'rewards_train/chosen': '0.14158', 'rewards_train/rejected': '0.054956', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08662', 'logps_train/rejected': '-116.26', 'logps_train/chosen': '-133.44', 'loss/train': '0.65525', 'examples_per_second': '30.278', 'grad_norm': '26.625', 'counters/examples': 80032, 'counters/updates': 2501}
train stats after 80064 examples: {'rewards_train/chosen': '0.061043', 'rewards_train/rejected': '0.11131', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.050262', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-154.75', 'loss/train': '0.72553', 'examples_per_second': '33.05', 'grad_norm': '28.375', 'counters/examples': 80064, 'counters/updates': 2502}
skipping logging after 80096 examples to avoid logging too frequently
train stats after 80128 examples: {'rewards_train/chosen': '0.038849', 'rewards_train/rejected': '0.048261', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.009411', 'logps_train/rejected': '-113.07', 'logps_train/chosen': '-126.26', 'loss/train': '0.70154', 'examples_per_second': '34.202', 'grad_norm': '27.5', 'counters/examples': 80128, 'counters/updates': 2504}
skipping logging after 80160 examples to avoid logging too frequently
train stats after 80192 examples: {'rewards_train/chosen': '0.044942', 'rewards_train/rejected': '-0.005261', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050203', 'logps_train/rejected': '-101.14', 'logps_train/chosen': '-117.71', 'loss/train': '0.67252', 'examples_per_second': '31.785', 'grad_norm': '24.875', 'counters/examples': 80192, 'counters/updates': 2506}
train stats after 80224 examples: {'rewards_train/chosen': '0.08936', 'rewards_train/rejected': '0.050215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039145', 'logps_train/rejected': '-110.28', 'logps_train/chosen': '-134.89', 'loss/train': '0.67986', 'examples_per_second': '32.828', 'grad_norm': '24.625', 'counters/examples': 80224, 'counters/updates': 2507}
train stats after 80256 examples: {'rewards_train/chosen': '0.048895', 'rewards_train/rejected': '0.078782', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.029886', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-142.34', 'loss/train': '0.71402', 'examples_per_second': '31.486', 'grad_norm': '28.75', 'counters/examples': 80256, 'counters/updates': 2508}
train stats after 80288 examples: {'rewards_train/chosen': '0.14118', 'rewards_train/rejected': '0.0093006', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13188', 'logps_train/rejected': '-111.23', 'logps_train/chosen': '-115.8', 'loss/train': '0.63487', 'examples_per_second': '32.198', 'grad_norm': '22.5', 'counters/examples': 80288, 'counters/updates': 2509}
train stats after 80320 examples: {'rewards_train/chosen': '0.11653', 'rewards_train/rejected': '0.076901', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039633', 'logps_train/rejected': '-165.93', 'logps_train/chosen': '-135.63', 'loss/train': '0.67917', 'examples_per_second': '30.356', 'grad_norm': '28.5', 'counters/examples': 80320, 'counters/updates': 2510}
train stats after 80352 examples: {'rewards_train/chosen': '0.16278', 'rewards_train/rejected': '0.058416', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10436', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-161.49', 'loss/train': '0.64805', 'examples_per_second': '29.945', 'grad_norm': '23.75', 'counters/examples': 80352, 'counters/updates': 2511}
skipping logging after 80384 examples to avoid logging too frequently
train stats after 80416 examples: {'rewards_train/chosen': '0.086307', 'rewards_train/rejected': '0.10456', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01825', 'logps_train/rejected': '-112.38', 'logps_train/chosen': '-119.65', 'loss/train': '0.70952', 'examples_per_second': '33.555', 'grad_norm': '27.25', 'counters/examples': 80416, 'counters/updates': 2513}
train stats after 80448 examples: {'rewards_train/chosen': '0.043608', 'rewards_train/rejected': '0.091653', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.048045', 'logps_train/rejected': '-128.05', 'logps_train/chosen': '-120.99', 'loss/train': '0.72396', 'examples_per_second': '31.574', 'grad_norm': '27.25', 'counters/examples': 80448, 'counters/updates': 2514}
train stats after 80480 examples: {'rewards_train/chosen': '0.033432', 'rewards_train/rejected': '0.0070351', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026397', 'logps_train/rejected': '-92.684', 'logps_train/chosen': '-120.52', 'loss/train': '0.6848', 'examples_per_second': '31.598', 'grad_norm': '23.375', 'counters/examples': 80480, 'counters/updates': 2515}
skipping logging after 80512 examples to avoid logging too frequently
train stats after 80544 examples: {'rewards_train/chosen': '0.12178', 'rewards_train/rejected': '0.083055', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038722', 'logps_train/rejected': '-108.1', 'logps_train/chosen': '-107.79', 'loss/train': '0.67912', 'examples_per_second': '30.194', 'grad_norm': '22.75', 'counters/examples': 80544, 'counters/updates': 2517}
train stats after 80576 examples: {'rewards_train/chosen': '0.10047', 'rewards_train/rejected': '0.043289', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057177', 'logps_train/rejected': '-91.747', 'logps_train/chosen': '-107.7', 'loss/train': '0.67074', 'examples_per_second': '30.538', 'grad_norm': '24.625', 'counters/examples': 80576, 'counters/updates': 2518}
train stats after 80608 examples: {'rewards_train/chosen': '0.1082', 'rewards_train/rejected': '0.0052929', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10291', 'logps_train/rejected': '-139.32', 'logps_train/chosen': '-164.78', 'loss/train': '0.65233', 'examples_per_second': '31.09', 'grad_norm': '37', 'counters/examples': 80608, 'counters/updates': 2519}
train stats after 80640 examples: {'rewards_train/chosen': '0.088695', 'rewards_train/rejected': '0.065448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023247', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-126.54', 'loss/train': '0.6901', 'examples_per_second': '30.691', 'grad_norm': '36.75', 'counters/examples': 80640, 'counters/updates': 2520}
train stats after 80672 examples: {'rewards_train/chosen': '0.1304', 'rewards_train/rejected': '0.049178', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081217', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-128.88', 'loss/train': '0.66066', 'examples_per_second': '31.056', 'grad_norm': '28.25', 'counters/examples': 80672, 'counters/updates': 2521}
train stats after 80704 examples: {'rewards_train/chosen': '0.12958', 'rewards_train/rejected': '0.024358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10522', 'logps_train/rejected': '-98.719', 'logps_train/chosen': '-166.2', 'loss/train': '0.65191', 'examples_per_second': '31.568', 'grad_norm': '23.75', 'counters/examples': 80704, 'counters/updates': 2522}
train stats after 80736 examples: {'rewards_train/chosen': '0.064518', 'rewards_train/rejected': '0.063988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00052972', 'logps_train/rejected': '-100.17', 'logps_train/chosen': '-128.72', 'loss/train': '0.6975', 'examples_per_second': '32.842', 'grad_norm': '26.5', 'counters/examples': 80736, 'counters/updates': 2523}
train stats after 80768 examples: {'rewards_train/chosen': '0.10365', 'rewards_train/rejected': '0.059286', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044364', 'logps_train/rejected': '-123.2', 'logps_train/chosen': '-128.58', 'loss/train': '0.67813', 'examples_per_second': '30.554', 'grad_norm': '26', 'counters/examples': 80768, 'counters/updates': 2524}
train stats after 80800 examples: {'rewards_train/chosen': '0.12167', 'rewards_train/rejected': '0.01645', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10522', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-143.48', 'loss/train': '0.65198', 'examples_per_second': '32.997', 'grad_norm': '33.5', 'counters/examples': 80800, 'counters/updates': 2525}
train stats after 80832 examples: {'rewards_train/chosen': '0.11453', 'rewards_train/rejected': '0.034304', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080222', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-145.15', 'loss/train': '0.66013', 'examples_per_second': '31.468', 'grad_norm': '27.75', 'counters/examples': 80832, 'counters/updates': 2526}
train stats after 80864 examples: {'rewards_train/chosen': '0.10212', 'rewards_train/rejected': '0.077841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024274', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-139.74', 'loss/train': '0.6861', 'examples_per_second': '31.515', 'grad_norm': '26.375', 'counters/examples': 80864, 'counters/updates': 2527}
train stats after 80896 examples: {'rewards_train/chosen': '0.042118', 'rewards_train/rejected': '0.054639', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012521', 'logps_train/rejected': '-116.23', 'logps_train/chosen': '-135.11', 'loss/train': '0.70477', 'examples_per_second': '24.765', 'grad_norm': '25.75', 'counters/examples': 80896, 'counters/updates': 2528}
train stats after 80928 examples: {'rewards_train/chosen': '0.061098', 'rewards_train/rejected': '0.026498', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0346', 'logps_train/rejected': '-154.05', 'logps_train/chosen': '-162.08', 'loss/train': '0.68064', 'examples_per_second': '31.07', 'grad_norm': '28.5', 'counters/examples': 80928, 'counters/updates': 2529}
train stats after 80960 examples: {'rewards_train/chosen': '0.090526', 'rewards_train/rejected': '0.047247', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.043279', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-155.96', 'loss/train': '0.67834', 'examples_per_second': '31.521', 'grad_norm': '27.625', 'counters/examples': 80960, 'counters/updates': 2530}
skipping logging after 80992 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '0.021252', 'rewards_train/rejected': '-0.0062577', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02751', 'logps_train/rejected': '-101.86', 'logps_train/chosen': '-149.19', 'loss/train': '0.68638', 'examples_per_second': '34.253', 'grad_norm': '26.75', 'counters/examples': 81024, 'counters/updates': 2532}
train stats after 81056 examples: {'rewards_train/chosen': '0.061161', 'rewards_train/rejected': '0.028042', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.033119', 'logps_train/rejected': '-117.92', 'logps_train/chosen': '-132.17', 'loss/train': '0.68163', 'examples_per_second': '30.899', 'grad_norm': '27.125', 'counters/examples': 81056, 'counters/updates': 2533}
train stats after 81088 examples: {'rewards_train/chosen': '0.073453', 'rewards_train/rejected': '0.012778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060676', 'logps_train/rejected': '-140.6', 'logps_train/chosen': '-154.17', 'loss/train': '0.66757', 'examples_per_second': '31.511', 'grad_norm': '27.75', 'counters/examples': 81088, 'counters/updates': 2534}
train stats after 81120 examples: {'rewards_train/chosen': '0.046635', 'rewards_train/rejected': '0.052158', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0055232', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-143.89', 'loss/train': '0.70445', 'examples_per_second': '31.727', 'grad_norm': '27.875', 'counters/examples': 81120, 'counters/updates': 2535}
train stats after 81152 examples: {'rewards_train/chosen': '0.13327', 'rewards_train/rejected': '0.025038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10823', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-145.94', 'loss/train': '0.64802', 'examples_per_second': '30.774', 'grad_norm': '26.75', 'counters/examples': 81152, 'counters/updates': 2536}
skipping logging after 81184 examples to avoid logging too frequently
train stats after 81216 examples: {'rewards_train/chosen': '0.097187', 'rewards_train/rejected': '-0.0030129', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1002', 'logps_train/rejected': '-105.25', 'logps_train/chosen': '-142.64', 'loss/train': '0.65151', 'examples_per_second': '31.806', 'grad_norm': '25.375', 'counters/examples': 81216, 'counters/updates': 2538}
train stats after 81248 examples: {'rewards_train/chosen': '0.091697', 'rewards_train/rejected': '0.05007', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041627', 'logps_train/rejected': '-146.47', 'logps_train/chosen': '-173.15', 'loss/train': '0.68172', 'examples_per_second': '31.839', 'grad_norm': '32', 'counters/examples': 81248, 'counters/updates': 2539}
train stats after 81280 examples: {'rewards_train/chosen': '0.13192', 'rewards_train/rejected': '0.040435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091486', 'logps_train/rejected': '-120.29', 'logps_train/chosen': '-114.26', 'loss/train': '0.65658', 'examples_per_second': '31.592', 'grad_norm': '24.125', 'counters/examples': 81280, 'counters/updates': 2540}
train stats after 81312 examples: {'rewards_train/chosen': '0.11168', 'rewards_train/rejected': '0.019065', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092616', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-132.97', 'loss/train': '0.65318', 'examples_per_second': '30.568', 'grad_norm': '25.5', 'counters/examples': 81312, 'counters/updates': 2541}
skipping logging after 81344 examples to avoid logging too frequently
train stats after 81376 examples: {'rewards_train/chosen': '0.07263', 'rewards_train/rejected': '0.0020112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070619', 'logps_train/rejected': '-102.36', 'logps_train/chosen': '-119.67', 'loss/train': '0.66198', 'examples_per_second': '30.986', 'grad_norm': '29.25', 'counters/examples': 81376, 'counters/updates': 2543}
train stats after 81408 examples: {'rewards_train/chosen': '0.097391', 'rewards_train/rejected': '0.075089', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022302', 'logps_train/rejected': '-144.17', 'logps_train/chosen': '-147.65', 'loss/train': '0.68946', 'examples_per_second': '31.298', 'grad_norm': '26.625', 'counters/examples': 81408, 'counters/updates': 2544}
skipping logging after 81440 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': '0.17983', 'rewards_train/rejected': '0.055538', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1243', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-173.45', 'loss/train': '0.64283', 'examples_per_second': '30.564', 'grad_norm': '27.625', 'counters/examples': 81472, 'counters/updates': 2546}
train stats after 81504 examples: {'rewards_train/chosen': '0.028149', 'rewards_train/rejected': '-0.01147', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039619', 'logps_train/rejected': '-125.34', 'logps_train/chosen': '-127.13', 'loss/train': '0.67869', 'examples_per_second': '31.72', 'grad_norm': '25.75', 'counters/examples': 81504, 'counters/updates': 2547}
skipping logging after 81536 examples to avoid logging too frequently
train stats after 81568 examples: {'rewards_train/chosen': '0.027782', 'rewards_train/rejected': '-0.0049973', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032779', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-96.048', 'loss/train': '0.68225', 'examples_per_second': '38.55', 'grad_norm': '23.375', 'counters/examples': 81568, 'counters/updates': 2549}
train stats after 81600 examples: {'rewards_train/chosen': '0.086961', 'rewards_train/rejected': '0.1074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020444', 'logps_train/rejected': '-129.32', 'logps_train/chosen': '-116.44', 'loss/train': '0.71197', 'examples_per_second': '30.157', 'grad_norm': '24.875', 'counters/examples': 81600, 'counters/updates': 2550}
train stats after 81632 examples: {'rewards_train/chosen': '0.12479', 'rewards_train/rejected': '0.032949', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091841', 'logps_train/rejected': '-118.75', 'logps_train/chosen': '-151.46', 'loss/train': '0.65848', 'examples_per_second': '31.585', 'grad_norm': '26.625', 'counters/examples': 81632, 'counters/updates': 2551}
train stats after 81664 examples: {'rewards_train/chosen': '0.1322', 'rewards_train/rejected': '0.041043', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.09116', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-125.08', 'loss/train': '0.65142', 'examples_per_second': '30.558', 'grad_norm': '24.25', 'counters/examples': 81664, 'counters/updates': 2552}
train stats after 81696 examples: {'rewards_train/chosen': '0.074789', 'rewards_train/rejected': '0.05579', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018999', 'logps_train/rejected': '-104.58', 'logps_train/chosen': '-99.559', 'loss/train': '0.68883', 'examples_per_second': '32.993', 'grad_norm': '24.25', 'counters/examples': 81696, 'counters/updates': 2553}
train stats after 81728 examples: {'rewards_train/chosen': '0.13516', 'rewards_train/rejected': '0.043746', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091416', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-167.88', 'loss/train': '0.65541', 'examples_per_second': '31.592', 'grad_norm': '24.5', 'counters/examples': 81728, 'counters/updates': 2554}
train stats after 81760 examples: {'rewards_train/chosen': '0.18659', 'rewards_train/rejected': '0.040671', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14592', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-171.6', 'loss/train': '0.63146', 'examples_per_second': '30.836', 'grad_norm': '27.125', 'counters/examples': 81760, 'counters/updates': 2555}
train stats after 81792 examples: {'rewards_train/chosen': '0.059018', 'rewards_train/rejected': '0.08815', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029132', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-135.76', 'loss/train': '0.72028', 'examples_per_second': '31.922', 'grad_norm': '35', 'counters/examples': 81792, 'counters/updates': 2556}
train stats after 81824 examples: {'rewards_train/chosen': '0.066836', 'rewards_train/rejected': '0.038449', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028387', 'logps_train/rejected': '-120.92', 'logps_train/chosen': '-151.95', 'loss/train': '0.68507', 'examples_per_second': '31.459', 'grad_norm': '25.75', 'counters/examples': 81824, 'counters/updates': 2557}
train stats after 81856 examples: {'rewards_train/chosen': '0.10297', 'rewards_train/rejected': '0.035192', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067778', 'logps_train/rejected': '-96.475', 'logps_train/chosen': '-94.885', 'loss/train': '0.6657', 'examples_per_second': '30.955', 'grad_norm': '23.5', 'counters/examples': 81856, 'counters/updates': 2558}
train stats after 81888 examples: {'rewards_train/chosen': '0.10003', 'rewards_train/rejected': '0.067467', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032564', 'logps_train/rejected': '-149.58', 'logps_train/chosen': '-156.18', 'loss/train': '0.68984', 'examples_per_second': '31.566', 'grad_norm': '28.75', 'counters/examples': 81888, 'counters/updates': 2559}
train stats after 81920 examples: {'rewards_train/chosen': '0.077321', 'rewards_train/rejected': '0.070473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0068473', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-127.56', 'loss/train': '0.69815', 'examples_per_second': '30.89', 'grad_norm': '28.75', 'counters/examples': 81920, 'counters/updates': 2560}
train stats after 81952 examples: {'rewards_train/chosen': '0.082549', 'rewards_train/rejected': '0.022129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06042', 'logps_train/rejected': '-97.588', 'logps_train/chosen': '-129.09', 'loss/train': '0.66968', 'examples_per_second': '32.575', 'grad_norm': '26.25', 'counters/examples': 81952, 'counters/updates': 2561}
train stats after 81984 examples: {'rewards_train/chosen': '0.099438', 'rewards_train/rejected': '0.097961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0014762', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-180.54', 'loss/train': '0.70564', 'examples_per_second': '31.601', 'grad_norm': '30.5', 'counters/examples': 81984, 'counters/updates': 2562}
train stats after 82016 examples: {'rewards_train/chosen': '0.14118', 'rewards_train/rejected': '0.013809', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12737', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-152.65', 'loss/train': '0.6358', 'examples_per_second': '31.589', 'grad_norm': '29.25', 'counters/examples': 82016, 'counters/updates': 2563}
skipping logging after 82048 examples to avoid logging too frequently
train stats after 82080 examples: {'rewards_train/chosen': '0.034311', 'rewards_train/rejected': '0.041111', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0068003', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-114.6', 'loss/train': '0.70403', 'examples_per_second': '32.198', 'grad_norm': '23.875', 'counters/examples': 82080, 'counters/updates': 2565}
skipping logging after 82112 examples to avoid logging too frequently
train stats after 82144 examples: {'rewards_train/chosen': '0.1373', 'rewards_train/rejected': '0.063711', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073593', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-122.7', 'loss/train': '0.66236', 'examples_per_second': '31.512', 'grad_norm': '29.375', 'counters/examples': 82144, 'counters/updates': 2567}
train stats after 82176 examples: {'rewards_train/chosen': '0.067697', 'rewards_train/rejected': '0.069184', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0014864', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-147.82', 'loss/train': '0.7012', 'examples_per_second': '31.453', 'grad_norm': '26.75', 'counters/examples': 82176, 'counters/updates': 2568}
train stats after 82208 examples: {'rewards_train/chosen': '0.1592', 'rewards_train/rejected': '0.092221', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06698', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-159.79', 'loss/train': '0.66787', 'examples_per_second': '32.654', 'grad_norm': '28.25', 'counters/examples': 82208, 'counters/updates': 2569}
train stats after 82240 examples: {'rewards_train/chosen': '0.050572', 'rewards_train/rejected': '0.042745', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0078274', 'logps_train/rejected': '-86.583', 'logps_train/chosen': '-183.94', 'loss/train': '0.69837', 'examples_per_second': '32.512', 'grad_norm': '29', 'counters/examples': 82240, 'counters/updates': 2570}
train stats after 82272 examples: {'rewards_train/chosen': '0.060008', 'rewards_train/rejected': '0.03684', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023168', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-134.18', 'loss/train': '0.68708', 'examples_per_second': '32.178', 'grad_norm': '32.5', 'counters/examples': 82272, 'counters/updates': 2571}
train stats after 82304 examples: {'rewards_train/chosen': '0.13531', 'rewards_train/rejected': '0.01623', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11908', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-149.38', 'loss/train': '0.64305', 'examples_per_second': '31.754', 'grad_norm': '24.25', 'counters/examples': 82304, 'counters/updates': 2572}
skipping logging after 82336 examples to avoid logging too frequently
train stats after 82368 examples: {'rewards_train/chosen': '0.088633', 'rewards_train/rejected': '0.020796', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067837', 'logps_train/rejected': '-108.99', 'logps_train/chosen': '-146.31', 'loss/train': '0.6702', 'examples_per_second': '32.461', 'grad_norm': '27.75', 'counters/examples': 82368, 'counters/updates': 2574}
train stats after 82400 examples: {'rewards_train/chosen': '0.079103', 'rewards_train/rejected': '0.053136', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025968', 'logps_train/rejected': '-169.35', 'logps_train/chosen': '-164.91', 'loss/train': '0.68826', 'examples_per_second': '31.576', 'grad_norm': '29.75', 'counters/examples': 82400, 'counters/updates': 2575}
train stats after 82432 examples: {'rewards_train/chosen': '0.057052', 'rewards_train/rejected': '0.076691', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019639', 'logps_train/rejected': '-149.51', 'logps_train/chosen': '-148.65', 'loss/train': '0.7106', 'examples_per_second': '32.246', 'grad_norm': '34.5', 'counters/examples': 82432, 'counters/updates': 2576}
train stats after 82464 examples: {'rewards_train/chosen': '0.080556', 'rewards_train/rejected': '0.059606', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02095', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-125.45', 'loss/train': '0.69501', 'examples_per_second': '30.79', 'grad_norm': '30.125', 'counters/examples': 82464, 'counters/updates': 2577}
train stats after 82496 examples: {'rewards_train/chosen': '0.12117', 'rewards_train/rejected': '0.10754', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013633', 'logps_train/rejected': '-161.44', 'logps_train/chosen': '-164.66', 'loss/train': '0.69256', 'examples_per_second': '31.565', 'grad_norm': '29.25', 'counters/examples': 82496, 'counters/updates': 2578}
train stats after 82528 examples: {'rewards_train/chosen': '0.034887', 'rewards_train/rejected': '0.0053257', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029561', 'logps_train/rejected': '-106.85', 'logps_train/chosen': '-151.01', 'loss/train': '0.68619', 'examples_per_second': '30.619', 'grad_norm': '25.125', 'counters/examples': 82528, 'counters/updates': 2579}
train stats after 82560 examples: {'rewards_train/chosen': '0.10565', 'rewards_train/rejected': '0.030989', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.07466', 'logps_train/rejected': '-104.93', 'logps_train/chosen': '-155.4', 'loss/train': '0.66743', 'examples_per_second': '30.703', 'grad_norm': '24.125', 'counters/examples': 82560, 'counters/updates': 2580}
train stats after 82592 examples: {'rewards_train/chosen': '0.091189', 'rewards_train/rejected': '0.034062', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.057126', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-154.85', 'loss/train': '0.67651', 'examples_per_second': '31.348', 'grad_norm': '32.25', 'counters/examples': 82592, 'counters/updates': 2581}
skipping logging after 82624 examples to avoid logging too frequently
train stats after 82656 examples: {'rewards_train/chosen': '0.10697', 'rewards_train/rejected': '0.044224', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062741', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-115.55', 'loss/train': '0.66805', 'examples_per_second': '31.465', 'grad_norm': '28.75', 'counters/examples': 82656, 'counters/updates': 2583}
train stats after 82688 examples: {'rewards_train/chosen': '0.15408', 'rewards_train/rejected': '0.06575', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088326', 'logps_train/rejected': '-113.92', 'logps_train/chosen': '-144.46', 'loss/train': '0.65734', 'examples_per_second': '30.144', 'grad_norm': '47.75', 'counters/examples': 82688, 'counters/updates': 2584}
train stats after 82720 examples: {'rewards_train/chosen': '0.083489', 'rewards_train/rejected': '-0.0033506', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086839', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-122.47', 'loss/train': '0.65853', 'examples_per_second': '31.775', 'grad_norm': '22.875', 'counters/examples': 82720, 'counters/updates': 2585}
train stats after 82752 examples: {'rewards_train/chosen': '0.071077', 'rewards_train/rejected': '0.023418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047659', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-146.5', 'loss/train': '0.67973', 'examples_per_second': '30.743', 'grad_norm': '28.875', 'counters/examples': 82752, 'counters/updates': 2586}
train stats after 82784 examples: {'rewards_train/chosen': '0.046827', 'rewards_train/rejected': '0.016104', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030723', 'logps_train/rejected': '-116.7', 'logps_train/chosen': '-133.63', 'loss/train': '0.68492', 'examples_per_second': '31.619', 'grad_norm': '25.375', 'counters/examples': 82784, 'counters/updates': 2587}
train stats after 82816 examples: {'rewards_train/chosen': '0.049685', 'rewards_train/rejected': '0.013276', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036409', 'logps_train/rejected': '-103.13', 'logps_train/chosen': '-128.95', 'loss/train': '0.68016', 'examples_per_second': '32.488', 'grad_norm': '25.125', 'counters/examples': 82816, 'counters/updates': 2588}
train stats after 82848 examples: {'rewards_train/chosen': '0.12958', 'rewards_train/rejected': '0.053571', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076009', 'logps_train/rejected': '-109.65', 'logps_train/chosen': '-158.31', 'loss/train': '0.66178', 'examples_per_second': '31.796', 'grad_norm': '28', 'counters/examples': 82848, 'counters/updates': 2589}
train stats after 82880 examples: {'rewards_train/chosen': '0.085389', 'rewards_train/rejected': '0.012809', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07258', 'logps_train/rejected': '-118.97', 'logps_train/chosen': '-125.61', 'loss/train': '0.67302', 'examples_per_second': '31.613', 'grad_norm': '43.25', 'counters/examples': 82880, 'counters/updates': 2590}
train stats after 82912 examples: {'rewards_train/chosen': '0.046083', 'rewards_train/rejected': '-0.0027726', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048855', 'logps_train/rejected': '-129.84', 'logps_train/chosen': '-148.31', 'loss/train': '0.67324', 'examples_per_second': '31.616', 'grad_norm': '27.125', 'counters/examples': 82912, 'counters/updates': 2591}
train stats after 82944 examples: {'rewards_train/chosen': '0.05529', 'rewards_train/rejected': '0.10307', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.047775', 'logps_train/rejected': '-165.8', 'logps_train/chosen': '-169.87', 'loss/train': '0.72463', 'examples_per_second': '31.596', 'grad_norm': '30.625', 'counters/examples': 82944, 'counters/updates': 2592}
train stats after 82976 examples: {'rewards_train/chosen': '0.11869', 'rewards_train/rejected': '0.077096', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041597', 'logps_train/rejected': '-151.16', 'logps_train/chosen': '-171.19', 'loss/train': '0.67938', 'examples_per_second': '31.541', 'grad_norm': '31', 'counters/examples': 82976, 'counters/updates': 2593}
skipping logging after 83008 examples to avoid logging too frequently
train stats after 83040 examples: {'rewards_train/chosen': '0.15101', 'rewards_train/rejected': '0.02329', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.12772', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-150.28', 'loss/train': '0.63806', 'examples_per_second': '31.693', 'grad_norm': '26.25', 'counters/examples': 83040, 'counters/updates': 2595}
train stats after 83072 examples: {'rewards_train/chosen': '0.11523', 'rewards_train/rejected': '0.01993', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095295', 'logps_train/rejected': '-137.34', 'logps_train/chosen': '-140.99', 'loss/train': '0.65362', 'examples_per_second': '30.179', 'grad_norm': '26.625', 'counters/examples': 83072, 'counters/updates': 2596}
train stats after 83104 examples: {'rewards_train/chosen': '0.098416', 'rewards_train/rejected': '0.049916', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0485', 'logps_train/rejected': '-118.77', 'logps_train/chosen': '-126.6', 'loss/train': '0.67182', 'examples_per_second': '30.217', 'grad_norm': '24.25', 'counters/examples': 83104, 'counters/updates': 2597}
train stats after 83136 examples: {'rewards_train/chosen': '0.071588', 'rewards_train/rejected': '0.062151', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0094374', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-121.98', 'loss/train': '0.69456', 'examples_per_second': '30.944', 'grad_norm': '25.875', 'counters/examples': 83136, 'counters/updates': 2598}
train stats after 83168 examples: {'rewards_train/chosen': '0.084261', 'rewards_train/rejected': '0.031361', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0529', 'logps_train/rejected': '-146.54', 'logps_train/chosen': '-132.51', 'loss/train': '0.67476', 'examples_per_second': '30.272', 'grad_norm': '27.125', 'counters/examples': 83168, 'counters/updates': 2599}
skipping logging after 83200 examples to avoid logging too frequently
train stats after 83232 examples: {'rewards_train/chosen': '0.075384', 'rewards_train/rejected': '0.056304', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.01908', 'logps_train/rejected': '-100.84', 'logps_train/chosen': '-105.9', 'loss/train': '0.69148', 'examples_per_second': '33.914', 'grad_norm': '23.125', 'counters/examples': 83232, 'counters/updates': 2601}
train stats after 83264 examples: {'rewards_train/chosen': '0.023689', 'rewards_train/rejected': '0.0064776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017211', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-138.06', 'loss/train': '0.68879', 'examples_per_second': '31.493', 'grad_norm': '29.75', 'counters/examples': 83264, 'counters/updates': 2602}
train stats after 83296 examples: {'rewards_train/chosen': '0.11888', 'rewards_train/rejected': '0.030095', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088789', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-145.88', 'loss/train': '0.65692', 'examples_per_second': '32.443', 'grad_norm': '30.625', 'counters/examples': 83296, 'counters/updates': 2603}
skipping logging after 83328 examples to avoid logging too frequently
train stats after 83360 examples: {'rewards_train/chosen': '0.10812', 'rewards_train/rejected': '0.02908', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079044', 'logps_train/rejected': '-83.946', 'logps_train/chosen': '-154.14', 'loss/train': '0.65953', 'examples_per_second': '31.591', 'grad_norm': '31.625', 'counters/examples': 83360, 'counters/updates': 2605}
skipping logging after 83392 examples to avoid logging too frequently
train stats after 83424 examples: {'rewards_train/chosen': '0.10253', 'rewards_train/rejected': '-0.013293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11582', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-146.95', 'loss/train': '0.64564', 'examples_per_second': '31.111', 'grad_norm': '26.375', 'counters/examples': 83424, 'counters/updates': 2607}
train stats after 83456 examples: {'rewards_train/chosen': '0.10139', 'rewards_train/rejected': '-0.0083952', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10978', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-148.38', 'loss/train': '0.64419', 'examples_per_second': '31.618', 'grad_norm': '27.75', 'counters/examples': 83456, 'counters/updates': 2608}
skipping logging after 83488 examples to avoid logging too frequently
train stats after 83520 examples: {'rewards_train/chosen': '0.087292', 'rewards_train/rejected': '0.061118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026174', 'logps_train/rejected': '-179.81', 'logps_train/chosen': '-154.26', 'loss/train': '0.68565', 'examples_per_second': '32.65', 'grad_norm': '30.25', 'counters/examples': 83520, 'counters/updates': 2610}
train stats after 83552 examples: {'rewards_train/chosen': '0.092729', 'rewards_train/rejected': '0.1054', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012672', 'logps_train/rejected': '-144.34', 'logps_train/chosen': '-129.39', 'loss/train': '0.70954', 'examples_per_second': '33.338', 'grad_norm': '28.125', 'counters/examples': 83552, 'counters/updates': 2611}
skipping logging after 83584 examples to avoid logging too frequently
train stats after 83616 examples: {'rewards_train/chosen': '0.11896', 'rewards_train/rejected': '0.019791', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099166', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-127.74', 'loss/train': '0.65012', 'examples_per_second': '33.541', 'grad_norm': '25.75', 'counters/examples': 83616, 'counters/updates': 2613}
skipping logging after 83648 examples to avoid logging too frequently
train stats after 83680 examples: {'rewards_train/chosen': '0.056356', 'rewards_train/rejected': '0.040115', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016241', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-120.4', 'loss/train': '0.68846', 'examples_per_second': '32.217', 'grad_norm': '26.75', 'counters/examples': 83680, 'counters/updates': 2615}
train stats after 83712 examples: {'rewards_train/chosen': '0.14325', 'rewards_train/rejected': '0.10551', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.037736', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-140.36', 'loss/train': '0.67999', 'examples_per_second': '31.132', 'grad_norm': '25', 'counters/examples': 83712, 'counters/updates': 2616}
train stats after 83744 examples: {'rewards_train/chosen': '0.13123', 'rewards_train/rejected': '0.016769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11446', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-159.04', 'loss/train': '0.64349', 'examples_per_second': '30.457', 'grad_norm': '27', 'counters/examples': 83744, 'counters/updates': 2617}
skipping logging after 83776 examples to avoid logging too frequently
train stats after 83808 examples: {'rewards_train/chosen': '0.038612', 'rewards_train/rejected': '0.016156', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022456', 'logps_train/rejected': '-86.821', 'logps_train/chosen': '-87.485', 'loss/train': '0.68656', 'examples_per_second': '33.504', 'grad_norm': '23.75', 'counters/examples': 83808, 'counters/updates': 2619}
skipping logging after 83840 examples to avoid logging too frequently
train stats after 83872 examples: {'rewards_train/chosen': '0.047782', 'rewards_train/rejected': '0.045748', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0020343', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-139.28', 'loss/train': '0.69782', 'examples_per_second': '30.426', 'grad_norm': '28.25', 'counters/examples': 83872, 'counters/updates': 2621}
train stats after 83904 examples: {'rewards_train/chosen': '0.1047', 'rewards_train/rejected': '-0.0016706', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10637', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-141.24', 'loss/train': '0.64832', 'examples_per_second': '31.948', 'grad_norm': '25.75', 'counters/examples': 83904, 'counters/updates': 2622}
train stats after 83936 examples: {'rewards_train/chosen': '0.073822', 'rewards_train/rejected': '0.070131', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.003691', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-143.82', 'loss/train': '0.70129', 'examples_per_second': '32.377', 'grad_norm': '28.25', 'counters/examples': 83936, 'counters/updates': 2623}
train stats after 83968 examples: {'rewards_train/chosen': '0.092623', 'rewards_train/rejected': '0.021949', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070674', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-149.98', 'loss/train': '0.66462', 'examples_per_second': '31.085', 'grad_norm': '28.75', 'counters/examples': 83968, 'counters/updates': 2624}
skipping logging after 84000 examples to avoid logging too frequently
Running evaluation after 84000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.18it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 84000: {'rewards_eval/chosen': '0.098802', 'rewards_eval/rejected': '0.041817', 'rewards_eval/accuracies': '0.61328', 'rewards_eval/margins': '0.056985', 'logps_eval/rejected': '-118.19', 'logps_eval/chosen': '-138.45', 'loss/eval': '0.67283'}
skipping save for non epoch
train stats after 84032 examples: {'rewards_train/chosen': '0.080647', 'rewards_train/rejected': '0.10815', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.027498', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-161.2', 'loss/train': '0.71982', 'examples_per_second': '31.415', 'grad_norm': '28.375', 'counters/examples': 84032, 'counters/updates': 2626}
train stats after 84064 examples: {'rewards_train/chosen': '0.12279', 'rewards_train/rejected': '0.02303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099765', 'logps_train/rejected': '-139.32', 'logps_train/chosen': '-162.2', 'loss/train': '0.65054', 'examples_per_second': '30.6', 'grad_norm': '26.5', 'counters/examples': 84064, 'counters/updates': 2627}
train stats after 84096 examples: {'rewards_train/chosen': '0.087313', 'rewards_train/rejected': '0.0068507', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080463', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-154.22', 'loss/train': '0.66058', 'examples_per_second': '31.667', 'grad_norm': '29.75', 'counters/examples': 84096, 'counters/updates': 2628}
train stats after 84128 examples: {'rewards_train/chosen': '0.1225', 'rewards_train/rejected': '0.016732', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10576', 'logps_train/rejected': '-81.142', 'logps_train/chosen': '-138.71', 'loss/train': '0.65055', 'examples_per_second': '31.76', 'grad_norm': '22.5', 'counters/examples': 84128, 'counters/updates': 2629}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84192 examples: {'rewards_train/chosen': '0.092439', 'rewards_train/rejected': '0.085547', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0068921', 'logps_train/rejected': '-136.06', 'logps_train/chosen': '-102.02', 'loss/train': '0.69487', 'examples_per_second': '31.591', 'grad_norm': '27.25', 'counters/examples': 84192, 'counters/updates': 2631}
train stats after 84224 examples: {'rewards_train/chosen': '0.067135', 'rewards_train/rejected': '0.0018775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065257', 'logps_train/rejected': '-106.13', 'logps_train/chosen': '-164.69', 'loss/train': '0.66726', 'examples_per_second': '31.607', 'grad_norm': '29.125', 'counters/examples': 84224, 'counters/updates': 2632}
train stats after 84256 examples: {'rewards_train/chosen': '0.033959', 'rewards_train/rejected': '0.085985', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.052027', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-100.08', 'loss/train': '0.72692', 'examples_per_second': '30.095', 'grad_norm': '29.25', 'counters/examples': 84256, 'counters/updates': 2633}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84320 examples: {'rewards_train/chosen': '0.10636', 'rewards_train/rejected': '0.036896', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069467', 'logps_train/rejected': '-117.23', 'logps_train/chosen': '-129.72', 'loss/train': '0.66571', 'examples_per_second': '31.548', 'grad_norm': '22.875', 'counters/examples': 84320, 'counters/updates': 2635}
train stats after 84352 examples: {'rewards_train/chosen': '0.12155', 'rewards_train/rejected': '0.060026', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.061526', 'logps_train/rejected': '-108.47', 'logps_train/chosen': '-151.22', 'loss/train': '0.67601', 'examples_per_second': '29.988', 'grad_norm': '25.5', 'counters/examples': 84352, 'counters/updates': 2636}
train stats after 84384 examples: {'rewards_train/chosen': '0.12299', 'rewards_train/rejected': '0.084752', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038243', 'logps_train/rejected': '-118.78', 'logps_train/chosen': '-170.77', 'loss/train': '0.68329', 'examples_per_second': '32.4', 'grad_norm': '28.875', 'counters/examples': 84384, 'counters/updates': 2637}
train stats after 84416 examples: {'rewards_train/chosen': '0.14172', 'rewards_train/rejected': '0.076639', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06508', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-144.15', 'loss/train': '0.67115', 'examples_per_second': '30.314', 'grad_norm': '29.5', 'counters/examples': 84416, 'counters/updates': 2638}
train stats after 84448 examples: {'rewards_train/chosen': '0.10257', 'rewards_train/rejected': '0.047516', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055051', 'logps_train/rejected': '-113.78', 'logps_train/chosen': '-153.56', 'loss/train': '0.67837', 'examples_per_second': '30.735', 'grad_norm': '34.75', 'counters/examples': 84448, 'counters/updates': 2639}
train stats after 84480 examples: {'rewards_train/chosen': '0.074965', 'rewards_train/rejected': '0.041692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033273', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-133.59', 'loss/train': '0.68149', 'examples_per_second': '30.111', 'grad_norm': '26.625', 'counters/examples': 84480, 'counters/updates': 2640}
skipping logging after 84512 examples to avoid logging too frequently
train stats after 84544 examples: {'rewards_train/chosen': '0.1118', 'rewards_train/rejected': '0.082146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029649', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-165.16', 'loss/train': '0.68843', 'examples_per_second': '31.478', 'grad_norm': '27.25', 'counters/examples': 84544, 'counters/updates': 2642}
skipping logging after 84576 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '0.077028', 'rewards_train/rejected': '0.053607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023422', 'logps_train/rejected': '-113.84', 'logps_train/chosen': '-105.92', 'loss/train': '0.68573', 'examples_per_second': '31.489', 'grad_norm': '26.75', 'counters/examples': 84608, 'counters/updates': 2644}
train stats after 84640 examples: {'rewards_train/chosen': '0.090321', 'rewards_train/rejected': '-0.0010685', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09139', 'logps_train/rejected': '-108.69', 'logps_train/chosen': '-125.16', 'loss/train': '0.65538', 'examples_per_second': '31.931', 'grad_norm': '23.25', 'counters/examples': 84640, 'counters/updates': 2645}
train stats after 84672 examples: {'rewards_train/chosen': '0.10142', 'rewards_train/rejected': '0.0094287', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091992', 'logps_train/rejected': '-122.25', 'logps_train/chosen': '-124.86', 'loss/train': '0.65361', 'examples_per_second': '30.545', 'grad_norm': '24.875', 'counters/examples': 84672, 'counters/updates': 2646}
skipping logging after 84704 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '0.099681', 'rewards_train/rejected': '0.030219', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069462', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-139.07', 'loss/train': '0.66683', 'examples_per_second': '21.901', 'grad_norm': '27.25', 'counters/examples': 84736, 'counters/updates': 2648}
train stats after 84768 examples: {'rewards_train/chosen': '0.054751', 'rewards_train/rejected': '0.05983', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0050789', 'logps_train/rejected': '-89.905', 'logps_train/chosen': '-131.33', 'loss/train': '0.70213', 'examples_per_second': '30.911', 'grad_norm': '24.375', 'counters/examples': 84768, 'counters/updates': 2649}
train stats after 84800 examples: {'rewards_train/chosen': '0.15616', 'rewards_train/rejected': '0.050989', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-130.64', 'loss/train': '0.64738', 'examples_per_second': '31.222', 'grad_norm': '24.375', 'counters/examples': 84800, 'counters/updates': 2650}
train stats after 84832 examples: {'rewards_train/chosen': '0.099146', 'rewards_train/rejected': '0.041538', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057608', 'logps_train/rejected': '-92.394', 'logps_train/chosen': '-113.52', 'loss/train': '0.6696', 'examples_per_second': '23.973', 'grad_norm': '31', 'counters/examples': 84832, 'counters/updates': 2651}
train stats after 84864 examples: {'rewards_train/chosen': '0.13753', 'rewards_train/rejected': '0.10433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033203', 'logps_train/rejected': '-137.57', 'logps_train/chosen': '-137.2', 'loss/train': '0.68016', 'examples_per_second': '30.266', 'grad_norm': '27.5', 'counters/examples': 84864, 'counters/updates': 2652}
train stats after 84896 examples: {'rewards_train/chosen': '0.10607', 'rewards_train/rejected': '0.10555', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00051843', 'logps_train/rejected': '-145.47', 'logps_train/chosen': '-141.61', 'loss/train': '0.69949', 'examples_per_second': '31.627', 'grad_norm': '30.125', 'counters/examples': 84896, 'counters/updates': 2653}
train stats after 84928 examples: {'rewards_train/chosen': '0.14168', 'rewards_train/rejected': '0.01008', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1316', 'logps_train/rejected': '-95.732', 'logps_train/chosen': '-136.65', 'loss/train': '0.63823', 'examples_per_second': '31.835', 'grad_norm': '24.125', 'counters/examples': 84928, 'counters/updates': 2654}
train stats after 84960 examples: {'rewards_train/chosen': '0.031846', 'rewards_train/rejected': '0.072063', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040217', 'logps_train/rejected': '-125.91', 'logps_train/chosen': '-129.61', 'loss/train': '0.72091', 'examples_per_second': '31.536', 'grad_norm': '25.25', 'counters/examples': 84960, 'counters/updates': 2655}
skipping logging after 84992 examples to avoid logging too frequently
train stats after 85024 examples: {'rewards_train/chosen': '0.04268', 'rewards_train/rejected': '-0.0030424', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045722', 'logps_train/rejected': '-111.64', 'logps_train/chosen': '-130.9', 'loss/train': '0.67575', 'examples_per_second': '36.602', 'grad_norm': '25.375', 'counters/examples': 85024, 'counters/updates': 2657}
train stats after 85056 examples: {'rewards_train/chosen': '0.084721', 'rewards_train/rejected': '0.050674', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034047', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-152.52', 'loss/train': '0.68295', 'examples_per_second': '31.589', 'grad_norm': '27.125', 'counters/examples': 85056, 'counters/updates': 2658}
train stats after 85088 examples: {'rewards_train/chosen': '0.093459', 'rewards_train/rejected': '0.082136', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.011323', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-109.97', 'loss/train': '0.69283', 'examples_per_second': '30.512', 'grad_norm': '24.25', 'counters/examples': 85088, 'counters/updates': 2659}
train stats after 85120 examples: {'rewards_train/chosen': '0.13981', 'rewards_train/rejected': '0.034001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10581', 'logps_train/rejected': '-132.12', 'logps_train/chosen': '-174.46', 'loss/train': '0.64774', 'examples_per_second': '31.624', 'grad_norm': '29.875', 'counters/examples': 85120, 'counters/updates': 2660}
train stats after 85152 examples: {'rewards_train/chosen': '0.067891', 'rewards_train/rejected': '0.065609', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0022824', 'logps_train/rejected': '-126', 'logps_train/chosen': '-117.33', 'loss/train': '0.69804', 'examples_per_second': '31.622', 'grad_norm': '26.25', 'counters/examples': 85152, 'counters/updates': 2661}
train stats after 85184 examples: {'rewards_train/chosen': '0.064055', 'rewards_train/rejected': '0.019579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044476', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-120.2', 'loss/train': '0.67715', 'examples_per_second': '31.746', 'grad_norm': '27.125', 'counters/examples': 85184, 'counters/updates': 2662}
train stats after 85216 examples: {'rewards_train/chosen': '0.16445', 'rewards_train/rejected': '0.08577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078677', 'logps_train/rejected': '-118.44', 'logps_train/chosen': '-131.8', 'loss/train': '0.6672', 'examples_per_second': '31.506', 'grad_norm': '27.375', 'counters/examples': 85216, 'counters/updates': 2663}
train stats after 85248 examples: {'rewards_train/chosen': '0.075603', 'rewards_train/rejected': '-0.0080032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083606', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-143.24', 'loss/train': '0.66358', 'examples_per_second': '32.562', 'grad_norm': '24.125', 'counters/examples': 85248, 'counters/updates': 2664}
train stats after 85280 examples: {'rewards_train/chosen': '0.078482', 'rewards_train/rejected': '0.059502', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.01898', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-144.37', 'loss/train': '0.69383', 'examples_per_second': '30.044', 'grad_norm': '33.75', 'counters/examples': 85280, 'counters/updates': 2665}
train stats after 85312 examples: {'rewards_train/chosen': '0.12122', 'rewards_train/rejected': '0.047375', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073845', 'logps_train/rejected': '-142.18', 'logps_train/chosen': '-155.73', 'loss/train': '0.66527', 'examples_per_second': '31.967', 'grad_norm': '28.875', 'counters/examples': 85312, 'counters/updates': 2666}
skipping logging after 85344 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '0.11043', 'rewards_train/rejected': '0.026787', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083643', 'logps_train/rejected': '-164.26', 'logps_train/chosen': '-142.07', 'loss/train': '0.66339', 'examples_per_second': '32.526', 'grad_norm': '26.625', 'counters/examples': 85376, 'counters/updates': 2668}
train stats after 85408 examples: {'rewards_train/chosen': '0.19965', 'rewards_train/rejected': '0.019319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18033', 'logps_train/rejected': '-98.315', 'logps_train/chosen': '-163.08', 'loss/train': '0.64445', 'examples_per_second': '31.638', 'grad_norm': '27.5', 'counters/examples': 85408, 'counters/updates': 2669}
train stats after 85440 examples: {'rewards_train/chosen': '0.08091', 'rewards_train/rejected': '0.033686', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047224', 'logps_train/rejected': '-103.96', 'logps_train/chosen': '-137.65', 'loss/train': '0.67567', 'examples_per_second': '32.783', 'grad_norm': '25.375', 'counters/examples': 85440, 'counters/updates': 2670}
train stats after 85472 examples: {'rewards_train/chosen': '0.034391', 'rewards_train/rejected': '-0.014057', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048449', 'logps_train/rejected': '-108.46', 'logps_train/chosen': '-158.92', 'loss/train': '0.67654', 'examples_per_second': '32.003', 'grad_norm': '25.875', 'counters/examples': 85472, 'counters/updates': 2671}
skipping logging after 85504 examples to avoid logging too frequently
train stats after 85536 examples: {'rewards_train/chosen': '0.14861', 'rewards_train/rejected': '0.053568', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095039', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-154.54', 'loss/train': '0.65206', 'examples_per_second': '31.929', 'grad_norm': '31.25', 'counters/examples': 85536, 'counters/updates': 2673}
train stats after 85568 examples: {'rewards_train/chosen': '0.05821', 'rewards_train/rejected': '0.022103', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036107', 'logps_train/rejected': '-92.511', 'logps_train/chosen': '-105.89', 'loss/train': '0.67912', 'examples_per_second': '31.638', 'grad_norm': '21.375', 'counters/examples': 85568, 'counters/updates': 2674}
train stats after 85600 examples: {'rewards_train/chosen': '0.1031', 'rewards_train/rejected': '0.014497', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088606', 'logps_train/rejected': '-115.82', 'logps_train/chosen': '-132.87', 'loss/train': '0.65999', 'examples_per_second': '31.637', 'grad_norm': '25', 'counters/examples': 85600, 'counters/updates': 2675}
train stats after 85632 examples: {'rewards_train/chosen': '0.090291', 'rewards_train/rejected': '0.11803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.027742', 'logps_train/rejected': '-143.69', 'logps_train/chosen': '-150.52', 'loss/train': '0.71475', 'examples_per_second': '31.451', 'grad_norm': '28.625', 'counters/examples': 85632, 'counters/updates': 2676}
train stats after 85664 examples: {'rewards_train/chosen': '0.16593', 'rewards_train/rejected': '0.14077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025165', 'logps_train/rejected': '-146.41', 'logps_train/chosen': '-197.64', 'loss/train': '0.68891', 'examples_per_second': '31.153', 'grad_norm': '30', 'counters/examples': 85664, 'counters/updates': 2677}
train stats after 85696 examples: {'rewards_train/chosen': '0.14285', 'rewards_train/rejected': '0.06206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080786', 'logps_train/rejected': '-113.04', 'logps_train/chosen': '-144.76', 'loss/train': '0.6735', 'examples_per_second': '32.876', 'grad_norm': '28.5', 'counters/examples': 85696, 'counters/updates': 2678}
skipping logging after 85728 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '0.097068', 'rewards_train/rejected': '0.015066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082002', 'logps_train/rejected': '-132.59', 'logps_train/chosen': '-138.44', 'loss/train': '0.65859', 'examples_per_second': '31.626', 'grad_norm': '37.75', 'counters/examples': 85760, 'counters/updates': 2680}
skipping logging after 85792 examples to avoid logging too frequently
train stats after 85824 examples: {'rewards_train/chosen': '0.14316', 'rewards_train/rejected': '-0.005214', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14837', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-151.01', 'loss/train': '0.62777', 'examples_per_second': '30.173', 'grad_norm': '22.875', 'counters/examples': 85824, 'counters/updates': 2682}
train stats after 85856 examples: {'rewards_train/chosen': '0.1723', 'rewards_train/rejected': '0.063254', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10905', 'logps_train/rejected': '-143.77', 'logps_train/chosen': '-161.67', 'loss/train': '0.64804', 'examples_per_second': '30.177', 'grad_norm': '28', 'counters/examples': 85856, 'counters/updates': 2683}
train stats after 85888 examples: {'rewards_train/chosen': '0.11241', 'rewards_train/rejected': '0.048257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064158', 'logps_train/rejected': '-106.62', 'logps_train/chosen': '-154.59', 'loss/train': '0.66773', 'examples_per_second': '32.596', 'grad_norm': '28.625', 'counters/examples': 85888, 'counters/updates': 2684}
train stats after 85920 examples: {'rewards_train/chosen': '0.081484', 'rewards_train/rejected': '0.038109', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043375', 'logps_train/rejected': '-119.3', 'logps_train/chosen': '-156.28', 'loss/train': '0.67873', 'examples_per_second': '30.733', 'grad_norm': '26.375', 'counters/examples': 85920, 'counters/updates': 2685}
train stats after 85952 examples: {'rewards_train/chosen': '0.076574', 'rewards_train/rejected': '0.078825', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0022504', 'logps_train/rejected': '-126.62', 'logps_train/chosen': '-148.98', 'loss/train': '0.70238', 'examples_per_second': '31.361', 'grad_norm': '31.875', 'counters/examples': 85952, 'counters/updates': 2686}
train stats after 85984 examples: {'rewards_train/chosen': '0.04684', 'rewards_train/rejected': '0.041668', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0051719', 'logps_train/rejected': '-143.67', 'logps_train/chosen': '-124.18', 'loss/train': '0.69712', 'examples_per_second': '31.453', 'grad_norm': '47.5', 'counters/examples': 85984, 'counters/updates': 2687}
train stats after 86016 examples: {'rewards_train/chosen': '0.068612', 'rewards_train/rejected': '0.024457', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044155', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-146.47', 'loss/train': '0.6774', 'examples_per_second': '31.373', 'grad_norm': '29.875', 'counters/examples': 86016, 'counters/updates': 2688}
train stats after 86048 examples: {'rewards_train/chosen': '0.087048', 'rewards_train/rejected': '0.021308', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06574', 'logps_train/rejected': '-148.99', 'logps_train/chosen': '-138.33', 'loss/train': '0.67095', 'examples_per_second': '31.644', 'grad_norm': '25.25', 'counters/examples': 86048, 'counters/updates': 2689}
train stats after 86080 examples: {'rewards_train/chosen': '0.13544', 'rewards_train/rejected': '0.075735', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059709', 'logps_train/rejected': '-175.74', 'logps_train/chosen': '-189.8', 'loss/train': '0.6741', 'examples_per_second': '30.305', 'grad_norm': '34.5', 'counters/examples': 86080, 'counters/updates': 2690}
train stats after 86112 examples: {'rewards_train/chosen': '0.092238', 'rewards_train/rejected': '0.073549', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018689', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-121.47', 'loss/train': '0.68754', 'examples_per_second': '31.265', 'grad_norm': '24.75', 'counters/examples': 86112, 'counters/updates': 2691}
skipping logging after 86144 examples to avoid logging too frequently
train stats after 86176 examples: {'rewards_train/chosen': '0.1242', 'rewards_train/rejected': '0.024181', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10002', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-137.87', 'loss/train': '0.65217', 'examples_per_second': '32.323', 'grad_norm': '29.125', 'counters/examples': 86176, 'counters/updates': 2693}
train stats after 86208 examples: {'rewards_train/chosen': '0.14515', 'rewards_train/rejected': '0.042765', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10238', 'logps_train/rejected': '-99.907', 'logps_train/chosen': '-123.62', 'loss/train': '0.64822', 'examples_per_second': '32.431', 'grad_norm': '24.375', 'counters/examples': 86208, 'counters/updates': 2694}
train stats after 86240 examples: {'rewards_train/chosen': '0.074638', 'rewards_train/rejected': '0.025877', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04876', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-114.67', 'loss/train': '0.67575', 'examples_per_second': '31.7', 'grad_norm': '25.5', 'counters/examples': 86240, 'counters/updates': 2695}
skipping logging after 86272 examples to avoid logging too frequently
train stats after 86304 examples: {'rewards_train/chosen': '0.027734', 'rewards_train/rejected': '0.04063', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012896', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-110.4', 'loss/train': '0.70618', 'examples_per_second': '30.885', 'grad_norm': '27.125', 'counters/examples': 86304, 'counters/updates': 2697}
train stats after 86336 examples: {'rewards_train/chosen': '0.088334', 'rewards_train/rejected': '0.063639', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024694', 'logps_train/rejected': '-149.83', 'logps_train/chosen': '-107.79', 'loss/train': '0.68812', 'examples_per_second': '32.818', 'grad_norm': '24.875', 'counters/examples': 86336, 'counters/updates': 2698}
train stats after 86368 examples: {'rewards_train/chosen': '0.1825', 'rewards_train/rejected': '0.055098', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12741', 'logps_train/rejected': '-151.24', 'logps_train/chosen': '-167.74', 'loss/train': '0.63857', 'examples_per_second': '31.631', 'grad_norm': '28.75', 'counters/examples': 86368, 'counters/updates': 2699}
train stats after 86400 examples: {'rewards_train/chosen': '0.024489', 'rewards_train/rejected': '0.053362', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.028873', 'logps_train/rejected': '-114.16', 'logps_train/chosen': '-121.32', 'loss/train': '0.71226', 'examples_per_second': '30.64', 'grad_norm': '25.875', 'counters/examples': 86400, 'counters/updates': 2700}
train stats after 86432 examples: {'rewards_train/chosen': '0.10074', 'rewards_train/rejected': '0.075545', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025195', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-133.55', 'loss/train': '0.6868', 'examples_per_second': '31.613', 'grad_norm': '25.5', 'counters/examples': 86432, 'counters/updates': 2701}
train stats after 86464 examples: {'rewards_train/chosen': '0.062064', 'rewards_train/rejected': '-0.0038115', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065875', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-160.94', 'loss/train': '0.66552', 'examples_per_second': '24.648', 'grad_norm': '24.875', 'counters/examples': 86464, 'counters/updates': 2702}
train stats after 86496 examples: {'rewards_train/chosen': '0.11707', 'rewards_train/rejected': '0.074333', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042741', 'logps_train/rejected': '-123.18', 'logps_train/chosen': '-155.41', 'loss/train': '0.68069', 'examples_per_second': '31.617', 'grad_norm': '29.625', 'counters/examples': 86496, 'counters/updates': 2703}
train stats after 86528 examples: {'rewards_train/chosen': '0.056135', 'rewards_train/rejected': '0.058362', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0022277', 'logps_train/rejected': '-117.31', 'logps_train/chosen': '-123.05', 'loss/train': '0.70038', 'examples_per_second': '31.483', 'grad_norm': '25.75', 'counters/examples': 86528, 'counters/updates': 2704}
train stats after 86560 examples: {'rewards_train/chosen': '0.086665', 'rewards_train/rejected': '0.030034', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056631', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-143.08', 'loss/train': '0.66923', 'examples_per_second': '31.531', 'grad_norm': '26.625', 'counters/examples': 86560, 'counters/updates': 2705}
train stats after 86592 examples: {'rewards_train/chosen': '0.025434', 'rewards_train/rejected': '0.065748', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040314', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-124.21', 'loss/train': '0.72181', 'examples_per_second': '36.189', 'grad_norm': '27.625', 'counters/examples': 86592, 'counters/updates': 2706}
train stats after 86624 examples: {'rewards_train/chosen': '0.07253', 'rewards_train/rejected': '0.074982', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0024527', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-166.53', 'loss/train': '0.69897', 'examples_per_second': '31.472', 'grad_norm': '29.75', 'counters/examples': 86624, 'counters/updates': 2707}
train stats after 86656 examples: {'rewards_train/chosen': '0.10731', 'rewards_train/rejected': '0.061842', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045472', 'logps_train/rejected': '-173.71', 'logps_train/chosen': '-175.22', 'loss/train': '0.6749', 'examples_per_second': '31.636', 'grad_norm': '34.5', 'counters/examples': 86656, 'counters/updates': 2708}
train stats after 86688 examples: {'rewards_train/chosen': '0.069024', 'rewards_train/rejected': '0.0072108', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061813', 'logps_train/rejected': '-105.84', 'logps_train/chosen': '-113.3', 'loss/train': '0.66865', 'examples_per_second': '31.062', 'grad_norm': '25.375', 'counters/examples': 86688, 'counters/updates': 2709}
train stats after 86720 examples: {'rewards_train/chosen': '0.079248', 'rewards_train/rejected': '0.043497', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035751', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-117.16', 'loss/train': '0.68337', 'examples_per_second': '30.146', 'grad_norm': '25.125', 'counters/examples': 86720, 'counters/updates': 2710}
train stats after 86752 examples: {'rewards_train/chosen': '0.14558', 'rewards_train/rejected': '0.036379', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1092', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-125.19', 'loss/train': '0.64505', 'examples_per_second': '30.274', 'grad_norm': '22.875', 'counters/examples': 86752, 'counters/updates': 2711}
train stats after 86784 examples: {'rewards_train/chosen': '0.076939', 'rewards_train/rejected': '0.1174', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040461', 'logps_train/rejected': '-145.6', 'logps_train/chosen': '-116.03', 'loss/train': '0.7207', 'examples_per_second': '30.035', 'grad_norm': '32.5', 'counters/examples': 86784, 'counters/updates': 2712}
train stats after 86816 examples: {'rewards_train/chosen': '0.040533', 'rewards_train/rejected': '0.03655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0039828', 'logps_train/rejected': '-86.827', 'logps_train/chosen': '-150.26', 'loss/train': '0.69844', 'examples_per_second': '31.619', 'grad_norm': '29', 'counters/examples': 86816, 'counters/updates': 2713}
train stats after 86848 examples: {'rewards_train/chosen': '0.071826', 'rewards_train/rejected': '-0.003714', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07554', 'logps_train/rejected': '-125.63', 'logps_train/chosen': '-134.26', 'loss/train': '0.66192', 'examples_per_second': '32.392', 'grad_norm': '26.25', 'counters/examples': 86848, 'counters/updates': 2714}
train stats after 86880 examples: {'rewards_train/chosen': '0.16549', 'rewards_train/rejected': '0.067785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097704', 'logps_train/rejected': '-160.51', 'logps_train/chosen': '-197.96', 'loss/train': '0.65746', 'examples_per_second': '31.597', 'grad_norm': '33.25', 'counters/examples': 86880, 'counters/updates': 2715}
train stats after 86912 examples: {'rewards_train/chosen': '0.097705', 'rewards_train/rejected': '0.0080639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089641', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-113.29', 'loss/train': '0.6591', 'examples_per_second': '32.844', 'grad_norm': '25.375', 'counters/examples': 86912, 'counters/updates': 2716}
train stats after 86944 examples: {'rewards_train/chosen': '0.087385', 'rewards_train/rejected': '0.036945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050439', 'logps_train/rejected': '-148.27', 'logps_train/chosen': '-145.62', 'loss/train': '0.67641', 'examples_per_second': '31.319', 'grad_norm': '28.75', 'counters/examples': 86944, 'counters/updates': 2717}
skipping logging after 86976 examples to avoid logging too frequently
train stats after 87008 examples: {'rewards_train/chosen': '0.077636', 'rewards_train/rejected': '0.070141', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0074954', 'logps_train/rejected': '-174.29', 'logps_train/chosen': '-146.22', 'loss/train': '0.69971', 'examples_per_second': '31.352', 'grad_norm': '33.75', 'counters/examples': 87008, 'counters/updates': 2719}
train stats after 87040 examples: {'rewards_train/chosen': '0.11561', 'rewards_train/rejected': '0.019047', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096568', 'logps_train/rejected': '-108.84', 'logps_train/chosen': '-140.32', 'loss/train': '0.65308', 'examples_per_second': '33.17', 'grad_norm': '28.625', 'counters/examples': 87040, 'counters/updates': 2720}
train stats after 87072 examples: {'rewards_train/chosen': '0.081568', 'rewards_train/rejected': '0.019298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06227', 'logps_train/rejected': '-140.99', 'logps_train/chosen': '-101.12', 'loss/train': '0.66759', 'examples_per_second': '31.585', 'grad_norm': '24.5', 'counters/examples': 87072, 'counters/updates': 2721}
train stats after 87104 examples: {'rewards_train/chosen': '0.087924', 'rewards_train/rejected': '0.030055', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05787', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-156.2', 'loss/train': '0.67015', 'examples_per_second': '31.44', 'grad_norm': '29', 'counters/examples': 87104, 'counters/updates': 2722}
train stats after 87136 examples: {'rewards_train/chosen': '0.15429', 'rewards_train/rejected': '0.11787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036423', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-124.7', 'loss/train': '0.68003', 'examples_per_second': '30.134', 'grad_norm': '26.5', 'counters/examples': 87136, 'counters/updates': 2723}
train stats after 87168 examples: {'rewards_train/chosen': '0.044796', 'rewards_train/rejected': '0.026238', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018558', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-127.18', 'loss/train': '0.69231', 'examples_per_second': '30.946', 'grad_norm': '27.875', 'counters/examples': 87168, 'counters/updates': 2724}
train stats after 87200 examples: {'rewards_train/chosen': '0.096627', 'rewards_train/rejected': '0.11232', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015689', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-137.26', 'loss/train': '0.70842', 'examples_per_second': '32.221', 'grad_norm': '26.25', 'counters/examples': 87200, 'counters/updates': 2725}
skipping logging after 87232 examples to avoid logging too frequently
train stats after 87264 examples: {'rewards_train/chosen': '0.090012', 'rewards_train/rejected': '0.026673', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063339', 'logps_train/rejected': '-148.46', 'logps_train/chosen': '-167.1', 'loss/train': '0.6699', 'examples_per_second': '31.517', 'grad_norm': '25.375', 'counters/examples': 87264, 'counters/updates': 2727}
skipping logging after 87296 examples to avoid logging too frequently
train stats after 87328 examples: {'rewards_train/chosen': '0.079143', 'rewards_train/rejected': '0.083174', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0040307', 'logps_train/rejected': '-124.03', 'logps_train/chosen': '-121.89', 'loss/train': '0.70568', 'examples_per_second': '32.073', 'grad_norm': '29.625', 'counters/examples': 87328, 'counters/updates': 2729}
train stats after 87360 examples: {'rewards_train/chosen': '0.13034', 'rewards_train/rejected': '-0.046943', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17728', 'logps_train/rejected': '-85.512', 'logps_train/chosen': '-132.33', 'loss/train': '0.61587', 'examples_per_second': '30.191', 'grad_norm': '20.625', 'counters/examples': 87360, 'counters/updates': 2730}
train stats after 87392 examples: {'rewards_train/chosen': '0.1013', 'rewards_train/rejected': '0.016107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085194', 'logps_train/rejected': '-106.02', 'logps_train/chosen': '-141.96', 'loss/train': '0.66128', 'examples_per_second': '32.063', 'grad_norm': '24.625', 'counters/examples': 87392, 'counters/updates': 2731}
train stats after 87424 examples: {'rewards_train/chosen': '0.0306', 'rewards_train/rejected': '0.054119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.023519', 'logps_train/rejected': '-146.19', 'logps_train/chosen': '-112.24', 'loss/train': '0.71103', 'examples_per_second': '32.088', 'grad_norm': '27.25', 'counters/examples': 87424, 'counters/updates': 2732}
train stats after 87456 examples: {'rewards_train/chosen': '0.12508', 'rewards_train/rejected': '0.043606', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08147', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-162.12', 'loss/train': '0.65889', 'examples_per_second': '32.652', 'grad_norm': '26.5', 'counters/examples': 87456, 'counters/updates': 2733}
skipping logging after 87488 examples to avoid logging too frequently
train stats after 87520 examples: {'rewards_train/chosen': '0.040214', 'rewards_train/rejected': '0.034661', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0055527', 'logps_train/rejected': '-126.5', 'logps_train/chosen': '-141.01', 'loss/train': '0.6971', 'examples_per_second': '31.49', 'grad_norm': '29.875', 'counters/examples': 87520, 'counters/updates': 2735}
train stats after 87552 examples: {'rewards_train/chosen': '0.12178', 'rewards_train/rejected': '0.052345', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069435', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-140.25', 'loss/train': '0.66537', 'examples_per_second': '32.168', 'grad_norm': '27.125', 'counters/examples': 87552, 'counters/updates': 2736}
train stats after 87584 examples: {'rewards_train/chosen': '0.1007', 'rewards_train/rejected': '0.08405', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016652', 'logps_train/rejected': '-139.41', 'logps_train/chosen': '-142.7', 'loss/train': '0.69039', 'examples_per_second': '31.224', 'grad_norm': '29.375', 'counters/examples': 87584, 'counters/updates': 2737}
train stats after 87616 examples: {'rewards_train/chosen': '0.090828', 'rewards_train/rejected': '0.02619', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064638', 'logps_train/rejected': '-133.27', 'logps_train/chosen': '-124.6', 'loss/train': '0.66648', 'examples_per_second': '30.598', 'grad_norm': '26.25', 'counters/examples': 87616, 'counters/updates': 2738}
train stats after 87648 examples: {'rewards_train/chosen': '0.10535', 'rewards_train/rejected': '0.0093876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09596', 'logps_train/rejected': '-91.124', 'logps_train/chosen': '-119.83', 'loss/train': '0.65638', 'examples_per_second': '31.543', 'grad_norm': '21.75', 'counters/examples': 87648, 'counters/updates': 2739}
train stats after 87680 examples: {'rewards_train/chosen': '0.099353', 'rewards_train/rejected': '0.077656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.021697', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-133.22', 'loss/train': '0.69236', 'examples_per_second': '31.547', 'grad_norm': '31.125', 'counters/examples': 87680, 'counters/updates': 2740}
skipping logging after 87712 examples to avoid logging too frequently
train stats after 87744 examples: {'rewards_train/chosen': '0.15735', 'rewards_train/rejected': '0.093139', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064213', 'logps_train/rejected': '-99.659', 'logps_train/chosen': '-169.23', 'loss/train': '0.66732', 'examples_per_second': '32.434', 'grad_norm': '27', 'counters/examples': 87744, 'counters/updates': 2742}
train stats after 87776 examples: {'rewards_train/chosen': '0.067898', 'rewards_train/rejected': '0.031756', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036142', 'logps_train/rejected': '-113.84', 'logps_train/chosen': '-120.68', 'loss/train': '0.67982', 'examples_per_second': '31.682', 'grad_norm': '23', 'counters/examples': 87776, 'counters/updates': 2743}
skipping logging after 87808 examples to avoid logging too frequently
train stats after 87840 examples: {'rewards_train/chosen': '0.10186', 'rewards_train/rejected': '-0.055801', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15766', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-129.85', 'loss/train': '0.62368', 'examples_per_second': '39.225', 'grad_norm': '24.5', 'counters/examples': 87840, 'counters/updates': 2745}
skipping logging after 87872 examples to avoid logging too frequently
train stats after 87904 examples: {'rewards_train/chosen': '0.095909', 'rewards_train/rejected': '0.045547', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050362', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-113.26', 'loss/train': '0.67437', 'examples_per_second': '31.6', 'grad_norm': '24.375', 'counters/examples': 87904, 'counters/updates': 2747}
train stats after 87936 examples: {'rewards_train/chosen': '0.10871', 'rewards_train/rejected': '0.070689', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038025', 'logps_train/rejected': '-117.94', 'logps_train/chosen': '-126.25', 'loss/train': '0.67884', 'examples_per_second': '30.047', 'grad_norm': '26.75', 'counters/examples': 87936, 'counters/updates': 2748}
train stats after 87968 examples: {'rewards_train/chosen': '0.12161', 'rewards_train/rejected': '0.053473', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068134', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-131.91', 'loss/train': '0.66873', 'examples_per_second': '30.439', 'grad_norm': '27.25', 'counters/examples': 87968, 'counters/updates': 2749}
skipping logging after 88000 examples to avoid logging too frequently
train stats after 88032 examples: {'rewards_train/chosen': '0.094719', 'rewards_train/rejected': '-0.0020126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096732', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-144.21', 'loss/train': '0.65356', 'examples_per_second': '31.501', 'grad_norm': '27.625', 'counters/examples': 88032, 'counters/updates': 2751}
train stats after 88064 examples: {'rewards_train/chosen': '0.10163', 'rewards_train/rejected': '0.029651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071976', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-159.46', 'loss/train': '0.66207', 'examples_per_second': '32.308', 'grad_norm': '26.25', 'counters/examples': 88064, 'counters/updates': 2752}
train stats after 88096 examples: {'rewards_train/chosen': '0.045201', 'rewards_train/rejected': '0.010732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034469', 'logps_train/rejected': '-94.973', 'logps_train/chosen': '-120.97', 'loss/train': '0.68247', 'examples_per_second': '32.472', 'grad_norm': '25.25', 'counters/examples': 88096, 'counters/updates': 2753}
train stats after 88128 examples: {'rewards_train/chosen': '0.015351', 'rewards_train/rejected': '0.028753', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013402', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-127.71', 'loss/train': '0.70579', 'examples_per_second': '31.608', 'grad_norm': '25.375', 'counters/examples': 88128, 'counters/updates': 2754}
train stats after 88160 examples: {'rewards_train/chosen': '0.14878', 'rewards_train/rejected': '0.048293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10048', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-175.29', 'loss/train': '0.65434', 'examples_per_second': '31.571', 'grad_norm': '28.75', 'counters/examples': 88160, 'counters/updates': 2755}
train stats after 88192 examples: {'rewards_train/chosen': '0.1207', 'rewards_train/rejected': '0.041652', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079046', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-110.33', 'loss/train': '0.66253', 'examples_per_second': '30.038', 'grad_norm': '24', 'counters/examples': 88192, 'counters/updates': 2756}
train stats after 88224 examples: {'rewards_train/chosen': '0.071867', 'rewards_train/rejected': '-0.006677', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078544', 'logps_train/rejected': '-94.299', 'logps_train/chosen': '-121.77', 'loss/train': '0.66281', 'examples_per_second': '31.596', 'grad_norm': '25.375', 'counters/examples': 88224, 'counters/updates': 2757}
train stats after 88256 examples: {'rewards_train/chosen': '0.056702', 'rewards_train/rejected': '-0.0010613', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057764', 'logps_train/rejected': '-100.38', 'logps_train/chosen': '-137.1', 'loss/train': '0.67183', 'examples_per_second': '32.161', 'grad_norm': '26.125', 'counters/examples': 88256, 'counters/updates': 2758}
train stats after 88288 examples: {'rewards_train/chosen': '0.099695', 'rewards_train/rejected': '0.02881', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070885', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-152.36', 'loss/train': '0.67134', 'examples_per_second': '31.132', 'grad_norm': '35.75', 'counters/examples': 88288, 'counters/updates': 2759}
train stats after 88320 examples: {'rewards_train/chosen': '0.084912', 'rewards_train/rejected': '0.048568', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036344', 'logps_train/rejected': '-141.16', 'logps_train/chosen': '-124.96', 'loss/train': '0.68088', 'examples_per_second': '31.032', 'grad_norm': '26.75', 'counters/examples': 88320, 'counters/updates': 2760}
train stats after 88352 examples: {'rewards_train/chosen': '0.048817', 'rewards_train/rejected': '0.094204', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045387', 'logps_train/rejected': '-111.41', 'logps_train/chosen': '-93.367', 'loss/train': '0.72274', 'examples_per_second': '31.385', 'grad_norm': '29.375', 'counters/examples': 88352, 'counters/updates': 2761}
train stats after 88384 examples: {'rewards_train/chosen': '0.056724', 'rewards_train/rejected': '0.037963', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018762', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-127.01', 'loss/train': '0.68868', 'examples_per_second': '30.125', 'grad_norm': '26.25', 'counters/examples': 88384, 'counters/updates': 2762}
train stats after 88416 examples: {'rewards_train/chosen': '0.14848', 'rewards_train/rejected': '-0.01061', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15909', 'logps_train/rejected': '-121.81', 'logps_train/chosen': '-168.21', 'loss/train': '0.62286', 'examples_per_second': '31.551', 'grad_norm': '26.125', 'counters/examples': 88416, 'counters/updates': 2763}
train stats after 88448 examples: {'rewards_train/chosen': '0.054649', 'rewards_train/rejected': '-0.036473', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091122', 'logps_train/rejected': '-114.47', 'logps_train/chosen': '-138.98', 'loss/train': '0.6528', 'examples_per_second': '32.772', 'grad_norm': '23.5', 'counters/examples': 88448, 'counters/updates': 2764}
train stats after 88480 examples: {'rewards_train/chosen': '0.15983', 'rewards_train/rejected': '0.022752', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13708', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-159.3', 'loss/train': '0.63902', 'examples_per_second': '31.007', 'grad_norm': '27.375', 'counters/examples': 88480, 'counters/updates': 2765}
train stats after 88512 examples: {'rewards_train/chosen': '0.073653', 'rewards_train/rejected': '0.019849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053804', 'logps_train/rejected': '-147.66', 'logps_train/chosen': '-166.45', 'loss/train': '0.67244', 'examples_per_second': '30.054', 'grad_norm': '28.125', 'counters/examples': 88512, 'counters/updates': 2766}
train stats after 88544 examples: {'rewards_train/chosen': '0.12233', 'rewards_train/rejected': '0.0068842', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11545', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-133.36', 'loss/train': '0.64309', 'examples_per_second': '31.361', 'grad_norm': '29.125', 'counters/examples': 88544, 'counters/updates': 2767}
train stats after 88576 examples: {'rewards_train/chosen': '0.11335', 'rewards_train/rejected': '0.0040577', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10929', 'logps_train/rejected': '-111.57', 'logps_train/chosen': '-155.84', 'loss/train': '0.64802', 'examples_per_second': '31.653', 'grad_norm': '26.625', 'counters/examples': 88576, 'counters/updates': 2768}
train stats after 88608 examples: {'rewards_train/chosen': '0.10906', 'rewards_train/rejected': '0.026453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082607', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-139.78', 'loss/train': '0.66392', 'examples_per_second': '32.91', 'grad_norm': '30', 'counters/examples': 88608, 'counters/updates': 2769}
train stats after 88640 examples: {'rewards_train/chosen': '0.13207', 'rewards_train/rejected': '0.031642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10043', 'logps_train/rejected': '-115.27', 'logps_train/chosen': '-141.09', 'loss/train': '0.653', 'examples_per_second': '30.536', 'grad_norm': '24.625', 'counters/examples': 88640, 'counters/updates': 2770}
skipping logging after 88672 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '0.11538', 'rewards_train/rejected': '0.022707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09267', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-127.06', 'loss/train': '0.65642', 'examples_per_second': '34.097', 'grad_norm': '23.125', 'counters/examples': 88704, 'counters/updates': 2772}
skipping logging after 88736 examples to avoid logging too frequently
train stats after 88768 examples: {'rewards_train/chosen': '0.10635', 'rewards_train/rejected': '0.060338', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.046012', 'logps_train/rejected': '-131.98', 'logps_train/chosen': '-127.31', 'loss/train': '0.67695', 'examples_per_second': '31.598', 'grad_norm': '27.625', 'counters/examples': 88768, 'counters/updates': 2774}
train stats after 88800 examples: {'rewards_train/chosen': '0.05196', 'rewards_train/rejected': '0.033465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018495', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-129.75', 'loss/train': '0.69084', 'examples_per_second': '30.248', 'grad_norm': '29.625', 'counters/examples': 88800, 'counters/updates': 2775}
train stats after 88832 examples: {'rewards_train/chosen': '0.10307', 'rewards_train/rejected': '0.019101', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083967', 'logps_train/rejected': '-92.895', 'logps_train/chosen': '-143.29', 'loss/train': '0.66011', 'examples_per_second': '32.821', 'grad_norm': '23.125', 'counters/examples': 88832, 'counters/updates': 2776}
train stats after 88864 examples: {'rewards_train/chosen': '0.12667', 'rewards_train/rejected': '0.065855', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060815', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-147.33', 'loss/train': '0.66991', 'examples_per_second': '31.602', 'grad_norm': '31.75', 'counters/examples': 88864, 'counters/updates': 2777}
train stats after 88896 examples: {'rewards_train/chosen': '0.15605', 'rewards_train/rejected': '0.053414', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10263', 'logps_train/rejected': '-116.99', 'logps_train/chosen': '-131.54', 'loss/train': '0.6496', 'examples_per_second': '30.937', 'grad_norm': '25.5', 'counters/examples': 88896, 'counters/updates': 2778}
train stats after 88928 examples: {'rewards_train/chosen': '0.064198', 'rewards_train/rejected': '0.049656', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014543', 'logps_train/rejected': '-147.05', 'logps_train/chosen': '-149.41', 'loss/train': '0.69085', 'examples_per_second': '30.51', 'grad_norm': '28.625', 'counters/examples': 88928, 'counters/updates': 2779}
skipping logging after 88960 examples to avoid logging too frequently
train stats after 88992 examples: {'rewards_train/chosen': '0.067685', 'rewards_train/rejected': '0.058126', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0095599', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-134.46', 'loss/train': '0.69655', 'examples_per_second': '31.502', 'grad_norm': '27.375', 'counters/examples': 88992, 'counters/updates': 2781}
train stats after 89024 examples: {'rewards_train/chosen': '0.046596', 'rewards_train/rejected': '0.012551', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034045', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-120.48', 'loss/train': '0.6817', 'examples_per_second': '32.936', 'grad_norm': '23.875', 'counters/examples': 89024, 'counters/updates': 2782}
train stats after 89056 examples: {'rewards_train/chosen': '0.066456', 'rewards_train/rejected': '-0.052811', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11927', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-131.06', 'loss/train': '0.64371', 'examples_per_second': '31.605', 'grad_norm': '23.5', 'counters/examples': 89056, 'counters/updates': 2783}
train stats after 89088 examples: {'rewards_train/chosen': '0.12822', 'rewards_train/rejected': '0.017973', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11025', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-164.93', 'loss/train': '0.64426', 'examples_per_second': '32.008', 'grad_norm': '27.125', 'counters/examples': 89088, 'counters/updates': 2784}
train stats after 89120 examples: {'rewards_train/chosen': '0.064887', 'rewards_train/rejected': '0.039828', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025059', 'logps_train/rejected': '-106.88', 'logps_train/chosen': '-114.39', 'loss/train': '0.68713', 'examples_per_second': '32.043', 'grad_norm': '23.75', 'counters/examples': 89120, 'counters/updates': 2785}
train stats after 89152 examples: {'rewards_train/chosen': '0.012328', 'rewards_train/rejected': '0.04555', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033223', 'logps_train/rejected': '-136.33', 'logps_train/chosen': '-166.83', 'loss/train': '0.71545', 'examples_per_second': '30.493', 'grad_norm': '32', 'counters/examples': 89152, 'counters/updates': 2786}
train stats after 89184 examples: {'rewards_train/chosen': '0.11896', 'rewards_train/rejected': '0.027903', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091055', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-189.07', 'loss/train': '0.65841', 'examples_per_second': '30.013', 'grad_norm': '28.5', 'counters/examples': 89184, 'counters/updates': 2787}
train stats after 89216 examples: {'rewards_train/chosen': '0.075548', 'rewards_train/rejected': '0.039206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036341', 'logps_train/rejected': '-107.21', 'logps_train/chosen': '-151.79', 'loss/train': '0.68025', 'examples_per_second': '31.658', 'grad_norm': '30.625', 'counters/examples': 89216, 'counters/updates': 2788}
skipping logging after 89248 examples to avoid logging too frequently
train stats after 89280 examples: {'rewards_train/chosen': '0.15139', 'rewards_train/rejected': '0.063326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088062', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-141.67', 'loss/train': '0.66088', 'examples_per_second': '31.643', 'grad_norm': '29.125', 'counters/examples': 89280, 'counters/updates': 2790}
train stats after 89312 examples: {'rewards_train/chosen': '0.075499', 'rewards_train/rejected': '0.14245', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.066949', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-133.02', 'loss/train': '0.73054', 'examples_per_second': '31.134', 'grad_norm': '29.375', 'counters/examples': 89312, 'counters/updates': 2791}
train stats after 89344 examples: {'rewards_train/chosen': '0.06479', 'rewards_train/rejected': '0.081534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016745', 'logps_train/rejected': '-138.59', 'logps_train/chosen': '-181.88', 'loss/train': '0.70881', 'examples_per_second': '31.363', 'grad_norm': '33.5', 'counters/examples': 89344, 'counters/updates': 2792}
train stats after 89376 examples: {'rewards_train/chosen': '0.14413', 'rewards_train/rejected': '0.039585', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10455', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-184.57', 'loss/train': '0.64812', 'examples_per_second': '31.619', 'grad_norm': '28.5', 'counters/examples': 89376, 'counters/updates': 2793}
train stats after 89408 examples: {'rewards_train/chosen': '0.15425', 'rewards_train/rejected': '0.053017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10124', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-127.89', 'loss/train': '0.65458', 'examples_per_second': '31.97', 'grad_norm': '22.875', 'counters/examples': 89408, 'counters/updates': 2794}
train stats after 89440 examples: {'rewards_train/chosen': '0.17581', 'rewards_train/rejected': '0.049201', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12661', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-147.73', 'loss/train': '0.63798', 'examples_per_second': '32.685', 'grad_norm': '25', 'counters/examples': 89440, 'counters/updates': 2795}
train stats after 89472 examples: {'rewards_train/chosen': '0.087584', 'rewards_train/rejected': '0.08602', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0015636', 'logps_train/rejected': '-109.59', 'logps_train/chosen': '-133.79', 'loss/train': '0.69821', 'examples_per_second': '32.401', 'grad_norm': '26.875', 'counters/examples': 89472, 'counters/updates': 2796}
train stats after 89504 examples: {'rewards_train/chosen': '0.15413', 'rewards_train/rejected': '0.060198', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093934', 'logps_train/rejected': '-145.94', 'logps_train/chosen': '-155.2', 'loss/train': '0.65448', 'examples_per_second': '31.652', 'grad_norm': '27.125', 'counters/examples': 89504, 'counters/updates': 2797}
train stats after 89536 examples: {'rewards_train/chosen': '0.075018', 'rewards_train/rejected': '0.056425', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018594', 'logps_train/rejected': '-120.08', 'logps_train/chosen': '-151.69', 'loss/train': '0.69012', 'examples_per_second': '31.57', 'grad_norm': '27.25', 'counters/examples': 89536, 'counters/updates': 2798}
train stats after 89568 examples: {'rewards_train/chosen': '0.10819', 'rewards_train/rejected': '0.018476', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.08971', 'logps_train/rejected': '-86.687', 'logps_train/chosen': '-136.65', 'loss/train': '0.65317', 'examples_per_second': '30.759', 'grad_norm': '26.25', 'counters/examples': 89568, 'counters/updates': 2799}
train stats after 89600 examples: {'rewards_train/chosen': '0.027642', 'rewards_train/rejected': '0.11876', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.091115', 'logps_train/rejected': '-150.65', 'logps_train/chosen': '-125.67', 'loss/train': '0.74868', 'examples_per_second': '30.924', 'grad_norm': '31.75', 'counters/examples': 89600, 'counters/updates': 2800}
skipping logging after 89632 examples to avoid logging too frequently
train stats after 89664 examples: {'rewards_train/chosen': '0.10546', 'rewards_train/rejected': '0.087871', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017589', 'logps_train/rejected': '-127.81', 'logps_train/chosen': '-144.6', 'loss/train': '0.6924', 'examples_per_second': '32.316', 'grad_norm': '27.125', 'counters/examples': 89664, 'counters/updates': 2802}
skipping logging after 89696 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '0.17362', 'rewards_train/rejected': '0.023594', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.15003', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-160.4', 'loss/train': '0.6258', 'examples_per_second': '31.583', 'grad_norm': '30.75', 'counters/examples': 89728, 'counters/updates': 2804}
skipping logging after 89760 examples to avoid logging too frequently
train stats after 89792 examples: {'rewards_train/chosen': '0.05009', 'rewards_train/rejected': '0.068621', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01853', 'logps_train/rejected': '-169.34', 'logps_train/chosen': '-122.09', 'loss/train': '0.70712', 'examples_per_second': '31.621', 'grad_norm': '29.75', 'counters/examples': 89792, 'counters/updates': 2806}
train stats after 89824 examples: {'rewards_train/chosen': '0.07263', 'rewards_train/rejected': '0.045674', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026956', 'logps_train/rejected': '-135.44', 'logps_train/chosen': '-126.65', 'loss/train': '0.68662', 'examples_per_second': '30.007', 'grad_norm': '29.5', 'counters/examples': 89824, 'counters/updates': 2807}
train stats after 89856 examples: {'rewards_train/chosen': '0.1007', 'rewards_train/rejected': '-0.014039', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11474', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-152.03', 'loss/train': '0.64385', 'examples_per_second': '31.531', 'grad_norm': '26.625', 'counters/examples': 89856, 'counters/updates': 2808}
train stats after 89888 examples: {'rewards_train/chosen': '0.11206', 'rewards_train/rejected': '0.15261', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040553', 'logps_train/rejected': '-149.44', 'logps_train/chosen': '-127.92', 'loss/train': '0.72838', 'examples_per_second': '30.597', 'grad_norm': '39', 'counters/examples': 89888, 'counters/updates': 2809}
train stats after 89920 examples: {'rewards_train/chosen': '0.089306', 'rewards_train/rejected': '0.073993', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015313', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-146.16', 'loss/train': '0.69122', 'examples_per_second': '30.38', 'grad_norm': '28', 'counters/examples': 89920, 'counters/updates': 2810}
train stats after 89952 examples: {'rewards_train/chosen': '0.19474', 'rewards_train/rejected': '0.12818', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066563', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-165.4', 'loss/train': '0.6786', 'examples_per_second': '31.635', 'grad_norm': '45.75', 'counters/examples': 89952, 'counters/updates': 2811}
train stats after 89984 examples: {'rewards_train/chosen': '0.10793', 'rewards_train/rejected': '0.025491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082437', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-177.96', 'loss/train': '0.65912', 'examples_per_second': '32.142', 'grad_norm': '33.25', 'counters/examples': 89984, 'counters/updates': 2812}
train stats after 90016 examples: {'rewards_train/chosen': '0.068488', 'rewards_train/rejected': '0.0055915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062897', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-147.05', 'loss/train': '0.67136', 'examples_per_second': '31.464', 'grad_norm': '28.125', 'counters/examples': 90016, 'counters/updates': 2813}
train stats after 90048 examples: {'rewards_train/chosen': '0.092232', 'rewards_train/rejected': '0.067835', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024397', 'logps_train/rejected': '-162.54', 'logps_train/chosen': '-133.75', 'loss/train': '0.69316', 'examples_per_second': '30.458', 'grad_norm': '39', 'counters/examples': 90048, 'counters/updates': 2814}
train stats after 90080 examples: {'rewards_train/chosen': '0.041085', 'rewards_train/rejected': '0.089865', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.04878', 'logps_train/rejected': '-152.49', 'logps_train/chosen': '-155.1', 'loss/train': '0.72326', 'examples_per_second': '33.069', 'grad_norm': '28.75', 'counters/examples': 90080, 'counters/updates': 2815}
train stats after 90112 examples: {'rewards_train/chosen': '0.068912', 'rewards_train/rejected': '-0.044915', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11383', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-152.31', 'loss/train': '0.64404', 'examples_per_second': '31.664', 'grad_norm': '25.75', 'counters/examples': 90112, 'counters/updates': 2816}
train stats after 90144 examples: {'rewards_train/chosen': '0.087261', 'rewards_train/rejected': '0.050793', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036468', 'logps_train/rejected': '-141.36', 'logps_train/chosen': '-143.26', 'loss/train': '0.68495', 'examples_per_second': '33.137', 'grad_norm': '29.375', 'counters/examples': 90144, 'counters/updates': 2817}
train stats after 90176 examples: {'rewards_train/chosen': '0.12477', 'rewards_train/rejected': '0.067818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056948', 'logps_train/rejected': '-121.6', 'logps_train/chosen': '-147.4', 'loss/train': '0.67155', 'examples_per_second': '30.785', 'grad_norm': '35.25', 'counters/examples': 90176, 'counters/updates': 2818}
train stats after 90208 examples: {'rewards_train/chosen': '0.12203', 'rewards_train/rejected': '0.029623', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092402', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-150.88', 'loss/train': '0.65825', 'examples_per_second': '25.134', 'grad_norm': '30.875', 'counters/examples': 90208, 'counters/updates': 2819}
train stats after 90240 examples: {'rewards_train/chosen': '0.13289', 'rewards_train/rejected': '0.045274', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087614', 'logps_train/rejected': '-102.19', 'logps_train/chosen': '-180.05', 'loss/train': '0.6563', 'examples_per_second': '30.717', 'grad_norm': '32.75', 'counters/examples': 90240, 'counters/updates': 2820}
train stats after 90272 examples: {'rewards_train/chosen': '0.067017', 'rewards_train/rejected': '0.041898', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025119', 'logps_train/rejected': '-151.5', 'logps_train/chosen': '-126.46', 'loss/train': '0.68619', 'examples_per_second': '30.995', 'grad_norm': '27', 'counters/examples': 90272, 'counters/updates': 2821}
train stats after 90304 examples: {'rewards_train/chosen': '0.12694', 'rewards_train/rejected': '0.071316', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055627', 'logps_train/rejected': '-125.32', 'logps_train/chosen': '-142.14', 'loss/train': '0.67985', 'examples_per_second': '23.747', 'grad_norm': '30', 'counters/examples': 90304, 'counters/updates': 2822}
train stats after 90336 examples: {'rewards_train/chosen': '0.13215', 'rewards_train/rejected': '0.055103', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077044', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-135.05', 'loss/train': '0.66917', 'examples_per_second': '31.16', 'grad_norm': '28.875', 'counters/examples': 90336, 'counters/updates': 2823}
train stats after 90368 examples: {'rewards_train/chosen': '0.074361', 'rewards_train/rejected': '0.055926', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.018436', 'logps_train/rejected': '-101.78', 'logps_train/chosen': '-113.48', 'loss/train': '0.68862', 'examples_per_second': '31.855', 'grad_norm': '22.75', 'counters/examples': 90368, 'counters/updates': 2824}
train stats after 90400 examples: {'rewards_train/chosen': '0.056705', 'rewards_train/rejected': '-0.013789', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070494', 'logps_train/rejected': '-104.62', 'logps_train/chosen': '-124.72', 'loss/train': '0.66523', 'examples_per_second': '32.848', 'grad_norm': '25.875', 'counters/examples': 90400, 'counters/updates': 2825}
skipping logging after 90432 examples to avoid logging too frequently
train stats after 90464 examples: {'rewards_train/chosen': '0.12682', 'rewards_train/rejected': '0.039429', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087391', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-132.02', 'loss/train': '0.65539', 'examples_per_second': '30.218', 'grad_norm': '24.125', 'counters/examples': 90464, 'counters/updates': 2827}
train stats after 90496 examples: {'rewards_train/chosen': '0.0067488', 'rewards_train/rejected': '0.065171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.058423', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-115.04', 'loss/train': '0.73204', 'examples_per_second': '30.172', 'grad_norm': '28', 'counters/examples': 90496, 'counters/updates': 2828}
train stats after 90528 examples: {'rewards_train/chosen': '0.058819', 'rewards_train/rejected': '9.1098e-05', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058728', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-150.85', 'loss/train': '0.67473', 'examples_per_second': '30.023', 'grad_norm': '32.25', 'counters/examples': 90528, 'counters/updates': 2829}
train stats after 90560 examples: {'rewards_train/chosen': '0.095884', 'rewards_train/rejected': '0.014569', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081316', 'logps_train/rejected': '-113.94', 'logps_train/chosen': '-154.78', 'loss/train': '0.65952', 'examples_per_second': '30.086', 'grad_norm': '27.5', 'counters/examples': 90560, 'counters/updates': 2830}
train stats after 90592 examples: {'rewards_train/chosen': '0.078018', 'rewards_train/rejected': '-0.013981', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091999', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-114.71', 'loss/train': '0.65448', 'examples_per_second': '31.834', 'grad_norm': '26.75', 'counters/examples': 90592, 'counters/updates': 2831}
train stats after 90624 examples: {'rewards_train/chosen': '0.081595', 'rewards_train/rejected': '0.084138', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.002543', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-107.79', 'loss/train': '0.70054', 'examples_per_second': '32.053', 'grad_norm': '25.75', 'counters/examples': 90624, 'counters/updates': 2832}
skipping logging after 90656 examples to avoid logging too frequently
train stats after 90688 examples: {'rewards_train/chosen': '0.053518', 'rewards_train/rejected': '0.026616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026902', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-119.47', 'loss/train': '0.68576', 'examples_per_second': '34.22', 'grad_norm': '27.875', 'counters/examples': 90688, 'counters/updates': 2834}
train stats after 90720 examples: {'rewards_train/chosen': '0.16495', 'rewards_train/rejected': '0.095455', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069499', 'logps_train/rejected': '-151.02', 'logps_train/chosen': '-163.25', 'loss/train': '0.6698', 'examples_per_second': '31.589', 'grad_norm': '28.5', 'counters/examples': 90720, 'counters/updates': 2835}
train stats after 90752 examples: {'rewards_train/chosen': '0.036554', 'rewards_train/rejected': '0.023422', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013133', 'logps_train/rejected': '-75.29', 'logps_train/chosen': '-108.99', 'loss/train': '0.69082', 'examples_per_second': '32.253', 'grad_norm': '23.75', 'counters/examples': 90752, 'counters/updates': 2836}
skipping logging after 90784 examples to avoid logging too frequently
train stats after 90816 examples: {'rewards_train/chosen': '0.10191', 'rewards_train/rejected': '0.022615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079299', 'logps_train/rejected': '-166.86', 'logps_train/chosen': '-141.18', 'loss/train': '0.65971', 'examples_per_second': '31.434', 'grad_norm': '28', 'counters/examples': 90816, 'counters/updates': 2838}
train stats after 90848 examples: {'rewards_train/chosen': '0.062714', 'rewards_train/rejected': '0.023135', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039579', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-154.55', 'loss/train': '0.6953', 'examples_per_second': '31.05', 'grad_norm': '27.375', 'counters/examples': 90848, 'counters/updates': 2839}
train stats after 90880 examples: {'rewards_train/chosen': '0.14434', 'rewards_train/rejected': '0.063551', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080786', 'logps_train/rejected': '-185.21', 'logps_train/chosen': '-172.38', 'loss/train': '0.6599', 'examples_per_second': '31.477', 'grad_norm': '27.5', 'counters/examples': 90880, 'counters/updates': 2840}
train stats after 90912 examples: {'rewards_train/chosen': '0.14566', 'rewards_train/rejected': '0.055518', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090146', 'logps_train/rejected': '-131.18', 'logps_train/chosen': '-166.15', 'loss/train': '0.65582', 'examples_per_second': '31.107', 'grad_norm': '26.375', 'counters/examples': 90912, 'counters/updates': 2841}
train stats after 90944 examples: {'rewards_train/chosen': '0.13415', 'rewards_train/rejected': '0.0017025', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13245', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-148.43', 'loss/train': '0.63795', 'examples_per_second': '33.061', 'grad_norm': '26.625', 'counters/examples': 90944, 'counters/updates': 2842}
train stats after 90976 examples: {'rewards_train/chosen': '0.069753', 'rewards_train/rejected': '0.06151', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0082436', 'logps_train/rejected': '-115.67', 'logps_train/chosen': '-126.88', 'loss/train': '0.69355', 'examples_per_second': '31.522', 'grad_norm': '25', 'counters/examples': 90976, 'counters/updates': 2843}
train stats after 91008 examples: {'rewards_train/chosen': '0.070957', 'rewards_train/rejected': '0.048811', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022145', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-127.41', 'loss/train': '0.69177', 'examples_per_second': '31.087', 'grad_norm': '28.75', 'counters/examples': 91008, 'counters/updates': 2844}
train stats after 91040 examples: {'rewards_train/chosen': '0.036969', 'rewards_train/rejected': '0.034558', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0024107', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-118.97', 'loss/train': '0.69763', 'examples_per_second': '32.268', 'grad_norm': '25', 'counters/examples': 91040, 'counters/updates': 2845}
train stats after 91072 examples: {'rewards_train/chosen': '0.071935', 'rewards_train/rejected': '0.011727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060208', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-144.9', 'loss/train': '0.67305', 'examples_per_second': '32.21', 'grad_norm': '26.25', 'counters/examples': 91072, 'counters/updates': 2846}
train stats after 91104 examples: {'rewards_train/chosen': '0.095061', 'rewards_train/rejected': '0.067794', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027267', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-141.3', 'loss/train': '0.68685', 'examples_per_second': '32.522', 'grad_norm': '31.125', 'counters/examples': 91104, 'counters/updates': 2847}
train stats after 91136 examples: {'rewards_train/chosen': '0.079249', 'rewards_train/rejected': '0.038233', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041016', 'logps_train/rejected': '-96.153', 'logps_train/chosen': '-139.72', 'loss/train': '0.68049', 'examples_per_second': '31.379', 'grad_norm': '25', 'counters/examples': 91136, 'counters/updates': 2848}
train stats after 91168 examples: {'rewards_train/chosen': '0.13978', 'rewards_train/rejected': '0.084265', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055512', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-130.07', 'loss/train': '0.67043', 'examples_per_second': '32.069', 'grad_norm': '24.75', 'counters/examples': 91168, 'counters/updates': 2849}
skipping logging after 91200 examples to avoid logging too frequently
train stats after 91232 examples: {'rewards_train/chosen': '0.084156', 'rewards_train/rejected': '0.024308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059848', 'logps_train/rejected': '-142.83', 'logps_train/chosen': '-148.42', 'loss/train': '0.66935', 'examples_per_second': '32.553', 'grad_norm': '27.5', 'counters/examples': 91232, 'counters/updates': 2851}
train stats after 91264 examples: {'rewards_train/chosen': '0.10228', 'rewards_train/rejected': '0.021945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080336', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-129.68', 'loss/train': '0.65846', 'examples_per_second': '31.046', 'grad_norm': '25.375', 'counters/examples': 91264, 'counters/updates': 2852}
train stats after 91296 examples: {'rewards_train/chosen': '0.063652', 'rewards_train/rejected': '0.040032', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023619', 'logps_train/rejected': '-136.53', 'logps_train/chosen': '-128.38', 'loss/train': '0.69201', 'examples_per_second': '30.024', 'grad_norm': '29.125', 'counters/examples': 91296, 'counters/updates': 2853}
train stats after 91328 examples: {'rewards_train/chosen': '0.056463', 'rewards_train/rejected': '0.034459', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022004', 'logps_train/rejected': '-103.62', 'logps_train/chosen': '-135.02', 'loss/train': '0.68967', 'examples_per_second': '31.623', 'grad_norm': '24.875', 'counters/examples': 91328, 'counters/updates': 2854}
train stats after 91360 examples: {'rewards_train/chosen': '0.099753', 'rewards_train/rejected': '0.083844', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015909', 'logps_train/rejected': '-134.15', 'logps_train/chosen': '-131.6', 'loss/train': '0.6916', 'examples_per_second': '32.19', 'grad_norm': '28.25', 'counters/examples': 91360, 'counters/updates': 2855}
train stats after 91392 examples: {'rewards_train/chosen': '0.084907', 'rewards_train/rejected': '0.044061', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040845', 'logps_train/rejected': '-116.34', 'logps_train/chosen': '-140.91', 'loss/train': '0.67765', 'examples_per_second': '30.556', 'grad_norm': '24', 'counters/examples': 91392, 'counters/updates': 2856}
skipping logging after 91424 examples to avoid logging too frequently
train stats after 91456 examples: {'rewards_train/chosen': '-0.00039432', 'rewards_train/rejected': '0.019009', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019403', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-160.19', 'loss/train': '0.71036', 'examples_per_second': '31.555', 'grad_norm': '27.875', 'counters/examples': 91456, 'counters/updates': 2858}
train stats after 91488 examples: {'rewards_train/chosen': '0.087811', 'rewards_train/rejected': '-0.0047054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092516', 'logps_train/rejected': '-136.7', 'logps_train/chosen': '-115.73', 'loss/train': '0.65423', 'examples_per_second': '32.609', 'grad_norm': '25.625', 'counters/examples': 91488, 'counters/updates': 2859}
skipping logging after 91520 examples to avoid logging too frequently
train stats after 91552 examples: {'rewards_train/chosen': '0.082958', 'rewards_train/rejected': '0.043979', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038978', 'logps_train/rejected': '-129.88', 'logps_train/chosen': '-136.79', 'loss/train': '0.69021', 'examples_per_second': '31.224', 'grad_norm': '27.75', 'counters/examples': 91552, 'counters/updates': 2861}
train stats after 91584 examples: {'rewards_train/chosen': '0.076558', 'rewards_train/rejected': '0.068972', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0075861', 'logps_train/rejected': '-158.14', 'logps_train/chosen': '-147.26', 'loss/train': '0.69495', 'examples_per_second': '31.625', 'grad_norm': '41.25', 'counters/examples': 91584, 'counters/updates': 2862}
train stats after 91616 examples: {'rewards_train/chosen': '0.0059608', 'rewards_train/rejected': '0.023341', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01738', 'logps_train/rejected': '-110.25', 'logps_train/chosen': '-114.87', 'loss/train': '0.70941', 'examples_per_second': '32.239', 'grad_norm': '28', 'counters/examples': 91616, 'counters/updates': 2863}
train stats after 91648 examples: {'rewards_train/chosen': '0.076745', 'rewards_train/rejected': '0.033097', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043648', 'logps_train/rejected': '-106.76', 'logps_train/chosen': '-130.81', 'loss/train': '0.67668', 'examples_per_second': '30.964', 'grad_norm': '37.5', 'counters/examples': 91648, 'counters/updates': 2864}
skipping logging after 91680 examples to avoid logging too frequently
train stats after 91712 examples: {'rewards_train/chosen': '0.042947', 'rewards_train/rejected': '0.012709', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030239', 'logps_train/rejected': '-103.39', 'logps_train/chosen': '-140.2', 'loss/train': '0.68334', 'examples_per_second': '31.572', 'grad_norm': '27.875', 'counters/examples': 91712, 'counters/updates': 2866}
train stats after 91744 examples: {'rewards_train/chosen': '0.059835', 'rewards_train/rejected': '0.078198', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018363', 'logps_train/rejected': '-114.52', 'logps_train/chosen': '-128.85', 'loss/train': '0.70872', 'examples_per_second': '31.287', 'grad_norm': '27.75', 'counters/examples': 91744, 'counters/updates': 2867}
train stats after 91776 examples: {'rewards_train/chosen': '0.14643', 'rewards_train/rejected': '0.032303', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11413', 'logps_train/rejected': '-89.446', 'logps_train/chosen': '-136.61', 'loss/train': '0.64228', 'examples_per_second': '31.097', 'grad_norm': '25.125', 'counters/examples': 91776, 'counters/updates': 2868}
train stats after 91808 examples: {'rewards_train/chosen': '0.10507', 'rewards_train/rejected': '0.043442', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061631', 'logps_train/rejected': '-102.95', 'logps_train/chosen': '-148.33', 'loss/train': '0.66953', 'examples_per_second': '32.344', 'grad_norm': '25.125', 'counters/examples': 91808, 'counters/updates': 2869}
skipping logging after 91840 examples to avoid logging too frequently
train stats after 91872 examples: {'rewards_train/chosen': '0.073081', 'rewards_train/rejected': '0.06261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01047', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-118.86', 'loss/train': '0.69611', 'examples_per_second': '31.232', 'grad_norm': '32.25', 'counters/examples': 91872, 'counters/updates': 2871}
train stats after 91904 examples: {'rewards_train/chosen': '0.083878', 'rewards_train/rejected': '0.02078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063098', 'logps_train/rejected': '-121.33', 'logps_train/chosen': '-155.52', 'loss/train': '0.66832', 'examples_per_second': '32.494', 'grad_norm': '27.625', 'counters/examples': 91904, 'counters/updates': 2872}
train stats after 91936 examples: {'rewards_train/chosen': '0.062739', 'rewards_train/rejected': '-0.00063569', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063374', 'logps_train/rejected': '-84.665', 'logps_train/chosen': '-115.91', 'loss/train': '0.66675', 'examples_per_second': '30.887', 'grad_norm': '25.375', 'counters/examples': 91936, 'counters/updates': 2873}
train stats after 91968 examples: {'rewards_train/chosen': '0.15047', 'rewards_train/rejected': '0.06415', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086317', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-162.47', 'loss/train': '0.65464', 'examples_per_second': '31.611', 'grad_norm': '28', 'counters/examples': 91968, 'counters/updates': 2874}
train stats after 92000 examples: {'rewards_train/chosen': '0.10944', 'rewards_train/rejected': '0.083865', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025578', 'logps_train/rejected': '-157.18', 'logps_train/chosen': '-147', 'loss/train': '0.6864', 'examples_per_second': '30.256', 'grad_norm': '28.75', 'counters/examples': 92000, 'counters/updates': 2875}
train stats after 92032 examples: {'rewards_train/chosen': '0.086541', 'rewards_train/rejected': '-0.019818', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10636', 'logps_train/rejected': '-106.59', 'logps_train/chosen': '-137.91', 'loss/train': '0.64633', 'examples_per_second': '26.928', 'grad_norm': '25.875', 'counters/examples': 92032, 'counters/updates': 2876}
train stats after 92064 examples: {'rewards_train/chosen': '0.0051064', 'rewards_train/rejected': '0.019609', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014503', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-146.67', 'loss/train': '0.70904', 'examples_per_second': '32.154', 'grad_norm': '30.125', 'counters/examples': 92064, 'counters/updates': 2877}
train stats after 92096 examples: {'rewards_train/chosen': '0.14678', 'rewards_train/rejected': '0.069715', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077062', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-137.28', 'loss/train': '0.66288', 'examples_per_second': '32.088', 'grad_norm': '29', 'counters/examples': 92096, 'counters/updates': 2878}
train stats after 92128 examples: {'rewards_train/chosen': '0.11015', 'rewards_train/rejected': '0.05911', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05104', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-149.12', 'loss/train': '0.67502', 'examples_per_second': '31.665', 'grad_norm': '27.125', 'counters/examples': 92128, 'counters/updates': 2879}
skipping logging after 92160 examples to avoid logging too frequently
train stats after 92192 examples: {'rewards_train/chosen': '0.12632', 'rewards_train/rejected': '0.063884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062437', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-161.69', 'loss/train': '0.6678', 'examples_per_second': '31.133', 'grad_norm': '27.25', 'counters/examples': 92192, 'counters/updates': 2881}
train stats after 92224 examples: {'rewards_train/chosen': '0.10863', 'rewards_train/rejected': '0.13776', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029132', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-146.39', 'loss/train': '0.71098', 'examples_per_second': '31.546', 'grad_norm': '27.75', 'counters/examples': 92224, 'counters/updates': 2882}
skipping logging after 92256 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '0.074484', 'rewards_train/rejected': '0.015981', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058503', 'logps_train/rejected': '-101.23', 'logps_train/chosen': '-143.47', 'loss/train': '0.66947', 'examples_per_second': '31.632', 'grad_norm': '25.125', 'counters/examples': 92288, 'counters/updates': 2884}
train stats after 92320 examples: {'rewards_train/chosen': '0.098518', 'rewards_train/rejected': '0.021415', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077104', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-179.76', 'loss/train': '0.66164', 'examples_per_second': '32.191', 'grad_norm': '29.875', 'counters/examples': 92320, 'counters/updates': 2885}
train stats after 92352 examples: {'rewards_train/chosen': '0.040727', 'rewards_train/rejected': '0.054488', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013761', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-177.87', 'loss/train': '0.70914', 'examples_per_second': '30.731', 'grad_norm': '29.75', 'counters/examples': 92352, 'counters/updates': 2886}
train stats after 92384 examples: {'rewards_train/chosen': '0.064808', 'rewards_train/rejected': '0.086159', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02135', 'logps_train/rejected': '-149.03', 'logps_train/chosen': '-124.69', 'loss/train': '0.71142', 'examples_per_second': '31.591', 'grad_norm': '27.5', 'counters/examples': 92384, 'counters/updates': 2887}
skipping logging after 92416 examples to avoid logging too frequently
train stats after 92448 examples: {'rewards_train/chosen': '0.045081', 'rewards_train/rejected': '-0.0018031', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046884', 'logps_train/rejected': '-140.08', 'logps_train/chosen': '-142.67', 'loss/train': '0.67823', 'examples_per_second': '31.623', 'grad_norm': '25.125', 'counters/examples': 92448, 'counters/updates': 2889}
train stats after 92480 examples: {'rewards_train/chosen': '0.12797', 'rewards_train/rejected': '0.021434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10653', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-154.78', 'loss/train': '0.64811', 'examples_per_second': '31.648', 'grad_norm': '26.125', 'counters/examples': 92480, 'counters/updates': 2890}
skipping logging after 92512 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '0.14566', 'rewards_train/rejected': '0.043153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10251', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-143.13', 'loss/train': '0.64974', 'examples_per_second': '31.849', 'grad_norm': '24.375', 'counters/examples': 92544, 'counters/updates': 2892}
train stats after 92576 examples: {'rewards_train/chosen': '0.085232', 'rewards_train/rejected': '0.014774', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070458', 'logps_train/rejected': '-84.177', 'logps_train/chosen': '-129.72', 'loss/train': '0.66217', 'examples_per_second': '31.197', 'grad_norm': '23.25', 'counters/examples': 92576, 'counters/updates': 2893}
train stats after 92608 examples: {'rewards_train/chosen': '0.12914', 'rewards_train/rejected': '0.04822', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08092', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-167.16', 'loss/train': '0.66234', 'examples_per_second': '30.189', 'grad_norm': '28', 'counters/examples': 92608, 'counters/updates': 2894}
train stats after 92640 examples: {'rewards_train/chosen': '0.15554', 'rewards_train/rejected': '0.0085884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14696', 'logps_train/rejected': '-108.6', 'logps_train/chosen': '-142.95', 'loss/train': '0.64282', 'examples_per_second': '30.096', 'grad_norm': '32.25', 'counters/examples': 92640, 'counters/updates': 2895}
skipping logging after 92672 examples to avoid logging too frequently
train stats after 92704 examples: {'rewards_train/chosen': '0.054795', 'rewards_train/rejected': '0.050181', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0046132', 'logps_train/rejected': '-133.93', 'logps_train/chosen': '-148.2', 'loss/train': '0.69664', 'examples_per_second': '32.034', 'grad_norm': '27.5', 'counters/examples': 92704, 'counters/updates': 2897}
skipping logging after 92736 examples to avoid logging too frequently
train stats after 92768 examples: {'rewards_train/chosen': '0.056397', 'rewards_train/rejected': '0.029021', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027376', 'logps_train/rejected': '-110.43', 'logps_train/chosen': '-109.46', 'loss/train': '0.6845', 'examples_per_second': '31.255', 'grad_norm': '23.375', 'counters/examples': 92768, 'counters/updates': 2899}
skipping logging after 92800 examples to avoid logging too frequently
train stats after 92832 examples: {'rewards_train/chosen': '0.11738', 'rewards_train/rejected': '0.030595', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086788', 'logps_train/rejected': '-151.22', 'logps_train/chosen': '-168.89', 'loss/train': '0.66072', 'examples_per_second': '34.006', 'grad_norm': '29.75', 'counters/examples': 92832, 'counters/updates': 2901}
train stats after 92864 examples: {'rewards_train/chosen': '0.15863', 'rewards_train/rejected': '0.033844', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12479', 'logps_train/rejected': '-98.476', 'logps_train/chosen': '-151.73', 'loss/train': '0.64095', 'examples_per_second': '30.347', 'grad_norm': '24.75', 'counters/examples': 92864, 'counters/updates': 2902}
train stats after 92896 examples: {'rewards_train/chosen': '0.041723', 'rewards_train/rejected': '0.018918', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022805', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-100.53', 'loss/train': '0.68609', 'examples_per_second': '31.492', 'grad_norm': '28.5', 'counters/examples': 92896, 'counters/updates': 2903}
train stats after 92928 examples: {'rewards_train/chosen': '0.11823', 'rewards_train/rejected': '-0.040963', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15919', 'logps_train/rejected': '-99.381', 'logps_train/chosen': '-133.76', 'loss/train': '0.62358', 'examples_per_second': '30.889', 'grad_norm': '22.25', 'counters/examples': 92928, 'counters/updates': 2904}
train stats after 92960 examples: {'rewards_train/chosen': '0.13329', 'rewards_train/rejected': '0.027445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10584', 'logps_train/rejected': '-81.667', 'logps_train/chosen': '-148.69', 'loss/train': '0.65361', 'examples_per_second': '31.113', 'grad_norm': '24.125', 'counters/examples': 92960, 'counters/updates': 2905}
skipping logging after 92992 examples to avoid logging too frequently
train stats after 93024 examples: {'rewards_train/chosen': '0.084549', 'rewards_train/rejected': '0.052941', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031607', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-166.06', 'loss/train': '0.685', 'examples_per_second': '30.979', 'grad_norm': '28.125', 'counters/examples': 93024, 'counters/updates': 2907}
skipping logging after 93056 examples to avoid logging too frequently
train stats after 93088 examples: {'rewards_train/chosen': '0.044611', 'rewards_train/rejected': '0.019665', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024945', 'logps_train/rejected': '-169.36', 'logps_train/chosen': '-149.08', 'loss/train': '0.6903', 'examples_per_second': '33.514', 'grad_norm': '30.375', 'counters/examples': 93088, 'counters/updates': 2909}
train stats after 93120 examples: {'rewards_train/chosen': '0.082191', 'rewards_train/rejected': '0.09047', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0082789', 'logps_train/rejected': '-143.21', 'logps_train/chosen': '-150.28', 'loss/train': '0.70424', 'examples_per_second': '30.298', 'grad_norm': '29.125', 'counters/examples': 93120, 'counters/updates': 2910}
train stats after 93152 examples: {'rewards_train/chosen': '0.069185', 'rewards_train/rejected': '-0.020895', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090081', 'logps_train/rejected': '-75.902', 'logps_train/chosen': '-120.38', 'loss/train': '0.65452', 'examples_per_second': '31.552', 'grad_norm': '26.25', 'counters/examples': 93152, 'counters/updates': 2911}
skipping logging after 93184 examples to avoid logging too frequently
train stats after 93216 examples: {'rewards_train/chosen': '0.078387', 'rewards_train/rejected': '0.04929', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.029097', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-171.46', 'loss/train': '0.68573', 'examples_per_second': '33.283', 'grad_norm': '31.875', 'counters/examples': 93216, 'counters/updates': 2913}
train stats after 93248 examples: {'rewards_train/chosen': '0.12558', 'rewards_train/rejected': '0.050861', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07472', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-122.34', 'loss/train': '0.66376', 'examples_per_second': '31.373', 'grad_norm': '25.125', 'counters/examples': 93248, 'counters/updates': 2914}
train stats after 93280 examples: {'rewards_train/chosen': '0.10562', 'rewards_train/rejected': '0.076804', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028817', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-162.73', 'loss/train': '0.68649', 'examples_per_second': '31.64', 'grad_norm': '25.625', 'counters/examples': 93280, 'counters/updates': 2915}
train stats after 93312 examples: {'rewards_train/chosen': '0.12902', 'rewards_train/rejected': '0.063518', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.065501', 'logps_train/rejected': '-112.64', 'logps_train/chosen': '-99.869', 'loss/train': '0.66688', 'examples_per_second': '31.359', 'grad_norm': '21.375', 'counters/examples': 93312, 'counters/updates': 2916}
train stats after 93344 examples: {'rewards_train/chosen': '0.085956', 'rewards_train/rejected': '0.077056', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0089004', 'logps_train/rejected': '-130.02', 'logps_train/chosen': '-155.31', 'loss/train': '0.70231', 'examples_per_second': '31.615', 'grad_norm': '29', 'counters/examples': 93344, 'counters/updates': 2917}
train stats after 93376 examples: {'rewards_train/chosen': '0.063487', 'rewards_train/rejected': '0.082911', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019424', 'logps_train/rejected': '-163.71', 'logps_train/chosen': '-148.01', 'loss/train': '0.70981', 'examples_per_second': '31.56', 'grad_norm': '29.5', 'counters/examples': 93376, 'counters/updates': 2918}
train stats after 93408 examples: {'rewards_train/chosen': '0.11913', 'rewards_train/rejected': '0.050202', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.068931', 'logps_train/rejected': '-148.61', 'logps_train/chosen': '-168.57', 'loss/train': '0.6646', 'examples_per_second': '31.552', 'grad_norm': '29.25', 'counters/examples': 93408, 'counters/updates': 2919}
train stats after 93440 examples: {'rewards_train/chosen': '0.15791', 'rewards_train/rejected': '0.087503', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070407', 'logps_train/rejected': '-105.66', 'logps_train/chosen': '-152.6', 'loss/train': '0.67083', 'examples_per_second': '31.463', 'grad_norm': '28.375', 'counters/examples': 93440, 'counters/updates': 2920}
train stats after 93472 examples: {'rewards_train/chosen': '0.090935', 'rewards_train/rejected': '0.042757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048178', 'logps_train/rejected': '-108.72', 'logps_train/chosen': '-132.66', 'loss/train': '0.67437', 'examples_per_second': '31.062', 'grad_norm': '24.25', 'counters/examples': 93472, 'counters/updates': 2921}
train stats after 93504 examples: {'rewards_train/chosen': '0.10235', 'rewards_train/rejected': '0.034037', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068311', 'logps_train/rejected': '-121.02', 'logps_train/chosen': '-150.02', 'loss/train': '0.66598', 'examples_per_second': '31.626', 'grad_norm': '25', 'counters/examples': 93504, 'counters/updates': 2922}
train stats after 93536 examples: {'rewards_train/chosen': '0.10546', 'rewards_train/rejected': '0.039716', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065745', 'logps_train/rejected': '-117.58', 'logps_train/chosen': '-176.8', 'loss/train': '0.66952', 'examples_per_second': '31.173', 'grad_norm': '29.625', 'counters/examples': 93536, 'counters/updates': 2923}
train stats after 93568 examples: {'rewards_train/chosen': '0.085443', 'rewards_train/rejected': '0.03145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053993', 'logps_train/rejected': '-177.02', 'logps_train/chosen': '-175.4', 'loss/train': '0.67904', 'examples_per_second': '31.635', 'grad_norm': '37.5', 'counters/examples': 93568, 'counters/updates': 2924}
train stats after 93600 examples: {'rewards_train/chosen': '0.069879', 'rewards_train/rejected': '0.096699', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02682', 'logps_train/rejected': '-132.64', 'logps_train/chosen': '-151.72', 'loss/train': '0.7133', 'examples_per_second': '30.099', 'grad_norm': '29.25', 'counters/examples': 93600, 'counters/updates': 2925}
train stats after 93632 examples: {'rewards_train/chosen': '0.11965', 'rewards_train/rejected': '0.0014305', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11822', 'logps_train/rejected': '-130.78', 'logps_train/chosen': '-151.98', 'loss/train': '0.64121', 'examples_per_second': '30.481', 'grad_norm': '35.25', 'counters/examples': 93632, 'counters/updates': 2926}
train stats after 93664 examples: {'rewards_train/chosen': '0.14287', 'rewards_train/rejected': '0.067427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075446', 'logps_train/rejected': '-116.77', 'logps_train/chosen': '-156.25', 'loss/train': '0.66159', 'examples_per_second': '30.11', 'grad_norm': '25.625', 'counters/examples': 93664, 'counters/updates': 2927}
train stats after 93696 examples: {'rewards_train/chosen': '0.1616', 'rewards_train/rejected': '0.085103', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0765', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-167.95', 'loss/train': '0.66232', 'examples_per_second': '31.553', 'grad_norm': '26.625', 'counters/examples': 93696, 'counters/updates': 2928}
train stats after 93728 examples: {'rewards_train/chosen': '0.022849', 'rewards_train/rejected': '0.085936', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.063088', 'logps_train/rejected': '-155.03', 'logps_train/chosen': '-109.09', 'loss/train': '0.73001', 'examples_per_second': '31.585', 'grad_norm': '33.25', 'counters/examples': 93728, 'counters/updates': 2929}
skipping logging after 93760 examples to avoid logging too frequently
train stats after 93792 examples: {'rewards_train/chosen': '0.15211', 'rewards_train/rejected': '-0.0004553', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15256', 'logps_train/rejected': '-146.16', 'logps_train/chosen': '-139.98', 'loss/train': '0.63375', 'examples_per_second': '30.095', 'grad_norm': '31.375', 'counters/examples': 93792, 'counters/updates': 2931}
train stats after 93824 examples: {'rewards_train/chosen': '0.073576', 'rewards_train/rejected': '0.022004', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051572', 'logps_train/rejected': '-91.209', 'logps_train/chosen': '-124.83', 'loss/train': '0.67608', 'examples_per_second': '31.638', 'grad_norm': '23.625', 'counters/examples': 93824, 'counters/updates': 2932}
train stats after 93856 examples: {'rewards_train/chosen': '0.11129', 'rewards_train/rejected': '0.084229', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027065', 'logps_train/rejected': '-130.05', 'logps_train/chosen': '-156.93', 'loss/train': '0.68849', 'examples_per_second': '31.933', 'grad_norm': '24.75', 'counters/examples': 93856, 'counters/updates': 2933}
skipping logging after 93888 examples to avoid logging too frequently
train stats after 93920 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '0.0143', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14068', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-152.17', 'loss/train': '0.63843', 'examples_per_second': '29.856', 'grad_norm': '28.25', 'counters/examples': 93920, 'counters/updates': 2935}
train stats after 93952 examples: {'rewards_train/chosen': '0.064184', 'rewards_train/rejected': '0.056927', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0072576', 'logps_train/rejected': '-109.51', 'logps_train/chosen': '-135.53', 'loss/train': '0.70256', 'examples_per_second': '32.984', 'grad_norm': '28.25', 'counters/examples': 93952, 'counters/updates': 2936}
train stats after 93984 examples: {'rewards_train/chosen': '0.087223', 'rewards_train/rejected': '0.083429', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0037941', 'logps_train/rejected': '-146.4', 'logps_train/chosen': '-157.54', 'loss/train': '0.70061', 'examples_per_second': '31.997', 'grad_norm': '32.5', 'counters/examples': 93984, 'counters/updates': 2937}
train stats after 94016 examples: {'rewards_train/chosen': '0.11175', 'rewards_train/rejected': '0.10131', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010444', 'logps_train/rejected': '-123.34', 'logps_train/chosen': '-139.57', 'loss/train': '0.69569', 'examples_per_second': '30.025', 'grad_norm': '26.125', 'counters/examples': 94016, 'counters/updates': 2938}
skipping logging after 94048 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '0.1006', 'rewards_train/rejected': '0.078102', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022501', 'logps_train/rejected': '-149.32', 'logps_train/chosen': '-161.18', 'loss/train': '0.69052', 'examples_per_second': '31.58', 'grad_norm': '31.375', 'counters/examples': 94080, 'counters/updates': 2940}
skipping logging after 94112 examples to avoid logging too frequently
train stats after 94144 examples: {'rewards_train/chosen': '0.12264', 'rewards_train/rejected': '-0.016368', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13901', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-176.4', 'loss/train': '0.62975', 'examples_per_second': '31.619', 'grad_norm': '27.375', 'counters/examples': 94144, 'counters/updates': 2942}
train stats after 94176 examples: {'rewards_train/chosen': '0.075525', 'rewards_train/rejected': '0.024998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050528', 'logps_train/rejected': '-140.43', 'logps_train/chosen': '-159.33', 'loss/train': '0.67314', 'examples_per_second': '31.535', 'grad_norm': '29.75', 'counters/examples': 94176, 'counters/updates': 2943}
train stats after 94208 examples: {'rewards_train/chosen': '0.15138', 'rewards_train/rejected': '0.097053', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054324', 'logps_train/rejected': '-133.68', 'logps_train/chosen': '-172.82', 'loss/train': '0.67437', 'examples_per_second': '31.551', 'grad_norm': '34', 'counters/examples': 94208, 'counters/updates': 2944}
train stats after 94240 examples: {'rewards_train/chosen': '0.037137', 'rewards_train/rejected': '0.018413', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018724', 'logps_train/rejected': '-85.66', 'logps_train/chosen': '-108.25', 'loss/train': '0.69012', 'examples_per_second': '29.923', 'grad_norm': '23.25', 'counters/examples': 94240, 'counters/updates': 2945}
train stats after 94272 examples: {'rewards_train/chosen': '0.15292', 'rewards_train/rejected': '0.07896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073962', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-159.15', 'loss/train': '0.67683', 'examples_per_second': '33.21', 'grad_norm': '29.5', 'counters/examples': 94272, 'counters/updates': 2946}
train stats after 94304 examples: {'rewards_train/chosen': '0.061482', 'rewards_train/rejected': '-0.0055925', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.067074', 'logps_train/rejected': '-99.542', 'logps_train/chosen': '-141.37', 'loss/train': '0.66779', 'examples_per_second': '32.239', 'grad_norm': '24.625', 'counters/examples': 94304, 'counters/updates': 2947}
train stats after 94336 examples: {'rewards_train/chosen': '0.030523', 'rewards_train/rejected': '0.050022', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019498', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-119.63', 'loss/train': '0.70701', 'examples_per_second': '30.652', 'grad_norm': '24.25', 'counters/examples': 94336, 'counters/updates': 2948}
train stats after 94368 examples: {'rewards_train/chosen': '0.030256', 'rewards_train/rejected': '-0.0038747', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034131', 'logps_train/rejected': '-104.96', 'logps_train/chosen': '-107.55', 'loss/train': '0.68144', 'examples_per_second': '32.293', 'grad_norm': '26.375', 'counters/examples': 94368, 'counters/updates': 2949}
skipping logging after 94400 examples to avoid logging too frequently
train stats after 94432 examples: {'rewards_train/chosen': '0.088478', 'rewards_train/rejected': '-0.016081', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10456', 'logps_train/rejected': '-113.13', 'logps_train/chosen': '-153.26', 'loss/train': '0.65075', 'examples_per_second': '31.755', 'grad_norm': '26.75', 'counters/examples': 94432, 'counters/updates': 2951}
train stats after 94464 examples: {'rewards_train/chosen': '0.10687', 'rewards_train/rejected': '0.10128', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0055863', 'logps_train/rejected': '-166.07', 'logps_train/chosen': '-151.36', 'loss/train': '0.69874', 'examples_per_second': '31.572', 'grad_norm': '29', 'counters/examples': 94464, 'counters/updates': 2952}
train stats after 94496 examples: {'rewards_train/chosen': '0.099032', 'rewards_train/rejected': '0.047026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052006', 'logps_train/rejected': '-151.52', 'logps_train/chosen': '-159.15', 'loss/train': '0.67363', 'examples_per_second': '31.295', 'grad_norm': '28.125', 'counters/examples': 94496, 'counters/updates': 2953}
train stats after 94528 examples: {'rewards_train/chosen': '0.074977', 'rewards_train/rejected': '0.043276', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0317', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-115.51', 'loss/train': '0.68506', 'examples_per_second': '30.249', 'grad_norm': '25.25', 'counters/examples': 94528, 'counters/updates': 2954}
train stats after 94560 examples: {'rewards_train/chosen': '0.091327', 'rewards_train/rejected': '0.05581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035518', 'logps_train/rejected': '-101.6', 'logps_train/chosen': '-144.27', 'loss/train': '0.68431', 'examples_per_second': '31.298', 'grad_norm': '27', 'counters/examples': 94560, 'counters/updates': 2955}
train stats after 94592 examples: {'rewards_train/chosen': '0.11296', 'rewards_train/rejected': '0.089031', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023929', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-140.87', 'loss/train': '0.69103', 'examples_per_second': '31.279', 'grad_norm': '29.75', 'counters/examples': 94592, 'counters/updates': 2956}
train stats after 94624 examples: {'rewards_train/chosen': '0.13954', 'rewards_train/rejected': '0.023849', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11569', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-179.74', 'loss/train': '0.64666', 'examples_per_second': '30.116', 'grad_norm': '28.375', 'counters/examples': 94624, 'counters/updates': 2957}
skipping logging after 94656 examples to avoid logging too frequently
train stats after 94688 examples: {'rewards_train/chosen': '0.093828', 'rewards_train/rejected': '0.11124', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017415', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-99.678', 'loss/train': '0.71069', 'examples_per_second': '31.29', 'grad_norm': '31.625', 'counters/examples': 94688, 'counters/updates': 2959}
skipping logging after 94720 examples to avoid logging too frequently
train stats after 94752 examples: {'rewards_train/chosen': '0.13674', 'rewards_train/rejected': '0.10606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030681', 'logps_train/rejected': '-147.54', 'logps_train/chosen': '-139.56', 'loss/train': '0.68225', 'examples_per_second': '31.068', 'grad_norm': '26.75', 'counters/examples': 94752, 'counters/updates': 2961}
train stats after 94784 examples: {'rewards_train/chosen': '0.083033', 'rewards_train/rejected': '0.024607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058426', 'logps_train/rejected': '-108.65', 'logps_train/chosen': '-123.88', 'loss/train': '0.66976', 'examples_per_second': '31.548', 'grad_norm': '24.625', 'counters/examples': 94784, 'counters/updates': 2962}
skipping logging after 94816 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '0.07088', 'rewards_train/rejected': '0.0060546', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064826', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-136.11', 'loss/train': '0.66641', 'examples_per_second': '32.284', 'grad_norm': '35', 'counters/examples': 94848, 'counters/updates': 2964}
train stats after 94880 examples: {'rewards_train/chosen': '0.095687', 'rewards_train/rejected': '-0.029666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12535', 'logps_train/rejected': '-95.216', 'logps_train/chosen': '-135.73', 'loss/train': '0.64156', 'examples_per_second': '31.429', 'grad_norm': '23.25', 'counters/examples': 94880, 'counters/updates': 2965}
train stats after 94912 examples: {'rewards_train/chosen': '0.09984', 'rewards_train/rejected': '0.056263', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043577', 'logps_train/rejected': '-135.2', 'logps_train/chosen': '-125.67', 'loss/train': '0.68172', 'examples_per_second': '32.156', 'grad_norm': '26.125', 'counters/examples': 94912, 'counters/updates': 2966}
skipping logging after 94944 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '0.049966', 'rewards_train/rejected': '-0.0095863', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059552', 'logps_train/rejected': '-158.45', 'logps_train/chosen': '-132.5', 'loss/train': '0.66972', 'examples_per_second': '31.551', 'grad_norm': '31.25', 'counters/examples': 94976, 'counters/updates': 2968}
train stats after 95008 examples: {'rewards_train/chosen': '0.098541', 'rewards_train/rejected': '0.042525', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056016', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-139.7', 'loss/train': '0.67728', 'examples_per_second': '32.264', 'grad_norm': '32', 'counters/examples': 95008, 'counters/updates': 2969}
train stats after 95040 examples: {'rewards_train/chosen': '0.055447', 'rewards_train/rejected': '0.057137', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0016896', 'logps_train/rejected': '-139.31', 'logps_train/chosen': '-124', 'loss/train': '0.69914', 'examples_per_second': '30.792', 'grad_norm': '34.25', 'counters/examples': 95040, 'counters/updates': 2970}
train stats after 95072 examples: {'rewards_train/chosen': '0.14249', 'rewards_train/rejected': '0.0081386', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13436', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-158.8', 'loss/train': '0.63689', 'examples_per_second': '31.353', 'grad_norm': '23.875', 'counters/examples': 95072, 'counters/updates': 2971}
train stats after 95104 examples: {'rewards_train/chosen': '0.20346', 'rewards_train/rejected': '0.1111', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092361', 'logps_train/rejected': '-102.16', 'logps_train/chosen': '-152.57', 'loss/train': '0.65694', 'examples_per_second': '31.547', 'grad_norm': '23.5', 'counters/examples': 95104, 'counters/updates': 2972}
train stats after 95136 examples: {'rewards_train/chosen': '0.045999', 'rewards_train/rejected': '0.021014', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024985', 'logps_train/rejected': '-107.53', 'logps_train/chosen': '-115.79', 'loss/train': '0.68842', 'examples_per_second': '31.89', 'grad_norm': '25', 'counters/examples': 95136, 'counters/updates': 2973}
skipping logging after 95168 examples to avoid logging too frequently
train stats after 95200 examples: {'rewards_train/chosen': '0.034782', 'rewards_train/rejected': '0.021571', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.01321', 'logps_train/rejected': '-92.305', 'logps_train/chosen': '-130.13', 'loss/train': '0.69219', 'examples_per_second': '32.464', 'grad_norm': '25', 'counters/examples': 95200, 'counters/updates': 2975}
train stats after 95232 examples: {'rewards_train/chosen': '0.070712', 'rewards_train/rejected': '0.016362', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05435', 'logps_train/rejected': '-133.63', 'logps_train/chosen': '-121.58', 'loss/train': '0.67415', 'examples_per_second': '33.165', 'grad_norm': '27.25', 'counters/examples': 95232, 'counters/updates': 2976}
train stats after 95264 examples: {'rewards_train/chosen': '0.11684', 'rewards_train/rejected': '0.023303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093534', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-126.27', 'loss/train': '0.65241', 'examples_per_second': '30.907', 'grad_norm': '23.625', 'counters/examples': 95264, 'counters/updates': 2977}
train stats after 95296 examples: {'rewards_train/chosen': '0.13453', 'rewards_train/rejected': '0.0093658', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12517', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-129.44', 'loss/train': '0.64259', 'examples_per_second': '31.59', 'grad_norm': '25.25', 'counters/examples': 95296, 'counters/updates': 2978}
train stats after 95328 examples: {'rewards_train/chosen': '0.13064', 'rewards_train/rejected': '0.10745', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023186', 'logps_train/rejected': '-151.73', 'logps_train/chosen': '-158.84', 'loss/train': '0.69474', 'examples_per_second': '32.196', 'grad_norm': '34.5', 'counters/examples': 95328, 'counters/updates': 2979}
train stats after 95360 examples: {'rewards_train/chosen': '0.13272', 'rewards_train/rejected': '0.062056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07066', 'logps_train/rejected': '-142.99', 'logps_train/chosen': '-115.21', 'loss/train': '0.66876', 'examples_per_second': '31.41', 'grad_norm': '31.25', 'counters/examples': 95360, 'counters/updates': 2980}
train stats after 95392 examples: {'rewards_train/chosen': '0.15969', 'rewards_train/rejected': '0.062157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09753', 'logps_train/rejected': '-127.62', 'logps_train/chosen': '-137.73', 'loss/train': '0.65529', 'examples_per_second': '31.556', 'grad_norm': '27.125', 'counters/examples': 95392, 'counters/updates': 2981}
train stats after 95424 examples: {'rewards_train/chosen': '0.072308', 'rewards_train/rejected': '0.049712', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022596', 'logps_train/rejected': '-148.66', 'logps_train/chosen': '-157.05', 'loss/train': '0.68972', 'examples_per_second': '30.441', 'grad_norm': '29.375', 'counters/examples': 95424, 'counters/updates': 2982}
train stats after 95456 examples: {'rewards_train/chosen': '0.060757', 'rewards_train/rejected': '0.0036864', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05707', 'logps_train/rejected': '-95.059', 'logps_train/chosen': '-132.24', 'loss/train': '0.67255', 'examples_per_second': '29.984', 'grad_norm': '26', 'counters/examples': 95456, 'counters/updates': 2983}
train stats after 95488 examples: {'rewards_train/chosen': '0.080103', 'rewards_train/rejected': '0.052649', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027454', 'logps_train/rejected': '-163.4', 'logps_train/chosen': '-134.65', 'loss/train': '0.68779', 'examples_per_second': '31.519', 'grad_norm': '27.875', 'counters/examples': 95488, 'counters/updates': 2984}
train stats after 95520 examples: {'rewards_train/chosen': '0.14689', 'rewards_train/rejected': '0.053784', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093111', 'logps_train/rejected': '-133.29', 'logps_train/chosen': '-135.99', 'loss/train': '0.65811', 'examples_per_second': '32.925', 'grad_norm': '29', 'counters/examples': 95520, 'counters/updates': 2985}
train stats after 95552 examples: {'rewards_train/chosen': '0.13409', 'rewards_train/rejected': '0.065113', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068976', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-151.82', 'loss/train': '0.66572', 'examples_per_second': '31.504', 'grad_norm': '30', 'counters/examples': 95552, 'counters/updates': 2986}
train stats after 95584 examples: {'rewards_train/chosen': '0.073803', 'rewards_train/rejected': '0.06736', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0064424', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-134.28', 'loss/train': '0.69495', 'examples_per_second': '31.127', 'grad_norm': '28.875', 'counters/examples': 95584, 'counters/updates': 2987}
train stats after 95616 examples: {'rewards_train/chosen': '0.13796', 'rewards_train/rejected': '0.062668', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075293', 'logps_train/rejected': '-126.5', 'logps_train/chosen': '-183.67', 'loss/train': '0.6633', 'examples_per_second': '31.309', 'grad_norm': '27.625', 'counters/examples': 95616, 'counters/updates': 2988}
train stats after 95648 examples: {'rewards_train/chosen': '0.084244', 'rewards_train/rejected': '0.041257', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042987', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-158.37', 'loss/train': '0.68048', 'examples_per_second': '33.106', 'grad_norm': '33.25', 'counters/examples': 95648, 'counters/updates': 2989}
train stats after 95680 examples: {'rewards_train/chosen': '0.12916', 'rewards_train/rejected': '0.032456', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.096701', 'logps_train/rejected': '-118.54', 'logps_train/chosen': '-168.61', 'loss/train': '0.65104', 'examples_per_second': '24.151', 'grad_norm': '27.125', 'counters/examples': 95680, 'counters/updates': 2990}
train stats after 95712 examples: {'rewards_train/chosen': '0.082097', 'rewards_train/rejected': '0.061885', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020212', 'logps_train/rejected': '-115.14', 'logps_train/chosen': '-122.33', 'loss/train': '0.69023', 'examples_per_second': '32.36', 'grad_norm': '23.625', 'counters/examples': 95712, 'counters/updates': 2991}
train stats after 95744 examples: {'rewards_train/chosen': '0.075287', 'rewards_train/rejected': '0.051842', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023445', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-139.45', 'loss/train': '0.69187', 'examples_per_second': '29.944', 'grad_norm': '27.375', 'counters/examples': 95744, 'counters/updates': 2992}
train stats after 95776 examples: {'rewards_train/chosen': '0.093706', 'rewards_train/rejected': '0.049625', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044081', 'logps_train/rejected': '-120.77', 'logps_train/chosen': '-138.45', 'loss/train': '0.68019', 'examples_per_second': '24.162', 'grad_norm': '26', 'counters/examples': 95776, 'counters/updates': 2993}
train stats after 95808 examples: {'rewards_train/chosen': '0.024556', 'rewards_train/rejected': '0.020322', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0042334', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-143.14', 'loss/train': '0.69831', 'examples_per_second': '30.251', 'grad_norm': '28.5', 'counters/examples': 95808, 'counters/updates': 2994}
skipping logging after 95840 examples to avoid logging too frequently
train stats after 95872 examples: {'rewards_train/chosen': '0.12539', 'rewards_train/rejected': '0.068955', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056435', 'logps_train/rejected': '-107.5', 'logps_train/chosen': '-136.33', 'loss/train': '0.67291', 'examples_per_second': '31.121', 'grad_norm': '29.125', 'counters/examples': 95872, 'counters/updates': 2996}
train stats after 95904 examples: {'rewards_train/chosen': '0.14409', 'rewards_train/rejected': '0.079485', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064602', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-160.1', 'loss/train': '0.66615', 'examples_per_second': '31.452', 'grad_norm': '28.25', 'counters/examples': 95904, 'counters/updates': 2997}
train stats after 95936 examples: {'rewards_train/chosen': '0.05914', 'rewards_train/rejected': '0.011866', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047274', 'logps_train/rejected': '-104.33', 'logps_train/chosen': '-102.66', 'loss/train': '0.67377', 'examples_per_second': '30.611', 'grad_norm': '22.125', 'counters/examples': 95936, 'counters/updates': 2998}
skipping logging after 95968 examples to avoid logging too frequently
train stats after 96000 examples: {'rewards_train/chosen': '0.069462', 'rewards_train/rejected': '0.088837', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019376', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-158.22', 'loss/train': '0.70939', 'examples_per_second': '31.389', 'grad_norm': '30.375', 'counters/examples': 96000, 'counters/updates': 3000}
Running evaluation after 96000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 96000: {'rewards_eval/chosen': '0.09819', 'rewards_eval/rejected': '0.040033', 'rewards_eval/accuracies': '0.58984', 'rewards_eval/margins': '0.058157', 'logps_eval/rejected': '-118.21', 'logps_eval/chosen': '-138.45', 'loss/eval': '0.67272'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880/step-96000...
writing checkpoint to .cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880/step-96000/policy.pt...
train stats after 96032 examples: {'rewards_train/chosen': '0.067002', 'rewards_train/rejected': '-0.022422', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089425', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-177.11', 'loss/train': '0.65739', 'examples_per_second': '23.272', 'grad_norm': '25.25', 'counters/examples': 96032, 'counters/updates': 3001}
train stats after 96064 examples: {'rewards_train/chosen': '0.15049', 'rewards_train/rejected': '0.036913', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11358', 'logps_train/rejected': '-95.111', 'logps_train/chosen': '-146.76', 'loss/train': '0.64327', 'examples_per_second': '30.257', 'grad_norm': '25.125', 'counters/examples': 96064, 'counters/updates': 3002}
train stats after 96096 examples: {'rewards_train/chosen': '0.076657', 'rewards_train/rejected': '0.0050212', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071635', 'logps_train/rejected': '-104.79', 'logps_train/chosen': '-160.35', 'loss/train': '0.66527', 'examples_per_second': '31.926', 'grad_norm': '24.75', 'counters/examples': 96096, 'counters/updates': 3003}
train stats after 96128 examples: {'rewards_train/chosen': '0.096545', 'rewards_train/rejected': '0.035891', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060654', 'logps_train/rejected': '-138.54', 'logps_train/chosen': '-153.67', 'loss/train': '0.66847', 'examples_per_second': '31.603', 'grad_norm': '26.375', 'counters/examples': 96128, 'counters/updates': 3004}
train stats after 96160 examples: {'rewards_train/chosen': '0.070617', 'rewards_train/rejected': '0.012788', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057829', 'logps_train/rejected': '-102.28', 'logps_train/chosen': '-123.14', 'loss/train': '0.67059', 'examples_per_second': '30.254', 'grad_norm': '29.5', 'counters/examples': 96160, 'counters/updates': 3005}
train stats after 96192 examples: {'rewards_train/chosen': '0.10012', 'rewards_train/rejected': '0.016098', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084025', 'logps_train/rejected': '-103.92', 'logps_train/chosen': '-121.02', 'loss/train': '0.65469', 'examples_per_second': '31.554', 'grad_norm': '30.5', 'counters/examples': 96192, 'counters/updates': 3006}
train stats after 96224 examples: {'rewards_train/chosen': '0.098673', 'rewards_train/rejected': '0.037894', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060779', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-169.43', 'loss/train': '0.66977', 'examples_per_second': '30.544', 'grad_norm': '26.625', 'counters/examples': 96224, 'counters/updates': 3007}
train stats after 96256 examples: {'rewards_train/chosen': '0.05149', 'rewards_train/rejected': '-0.016203', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067692', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-163.36', 'loss/train': '0.66695', 'examples_per_second': '32.462', 'grad_norm': '25.125', 'counters/examples': 96256, 'counters/updates': 3008}
train stats after 96288 examples: {'rewards_train/chosen': '0.14462', 'rewards_train/rejected': '0.059114', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085503', 'logps_train/rejected': '-101.9', 'logps_train/chosen': '-123.04', 'loss/train': '0.65676', 'examples_per_second': '31.533', 'grad_norm': '24.5', 'counters/examples': 96288, 'counters/updates': 3009}
train stats after 96320 examples: {'rewards_train/chosen': '0.12585', 'rewards_train/rejected': '0.05183', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074018', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-151.48', 'loss/train': '0.66281', 'examples_per_second': '31.382', 'grad_norm': '27.375', 'counters/examples': 96320, 'counters/updates': 3010}
train stats after 96352 examples: {'rewards_train/chosen': '0.094045', 'rewards_train/rejected': '0.083066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010978', 'logps_train/rejected': '-119.13', 'logps_train/chosen': '-139.5', 'loss/train': '0.69721', 'examples_per_second': '30.767', 'grad_norm': '29.5', 'counters/examples': 96352, 'counters/updates': 3011}
train stats after 96384 examples: {'rewards_train/chosen': '0.1272', 'rewards_train/rejected': '0.033131', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094067', 'logps_train/rejected': '-135.08', 'logps_train/chosen': '-178.96', 'loss/train': '0.65626', 'examples_per_second': '31.588', 'grad_norm': '29.5', 'counters/examples': 96384, 'counters/updates': 3012}
train stats after 96416 examples: {'rewards_train/chosen': '0.071707', 'rewards_train/rejected': '0.0094097', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062297', 'logps_train/rejected': '-118.81', 'logps_train/chosen': '-119.83', 'loss/train': '0.66943', 'examples_per_second': '30.218', 'grad_norm': '29', 'counters/examples': 96416, 'counters/updates': 3013}
train stats after 96448 examples: {'rewards_train/chosen': '0.10377', 'rewards_train/rejected': '0.053013', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050759', 'logps_train/rejected': '-94.402', 'logps_train/chosen': '-132.24', 'loss/train': '0.67466', 'examples_per_second': '30.827', 'grad_norm': '23.125', 'counters/examples': 96448, 'counters/updates': 3014}
train stats after 96480 examples: {'rewards_train/chosen': '0.073693', 'rewards_train/rejected': '0.059673', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014021', 'logps_train/rejected': '-117.76', 'logps_train/chosen': '-184.94', 'loss/train': '0.69262', 'examples_per_second': '31.643', 'grad_norm': '29.625', 'counters/examples': 96480, 'counters/updates': 3015}
train stats after 96512 examples: {'rewards_train/chosen': '0.15514', 'rewards_train/rejected': '0.0054785', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14967', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-144.43', 'loss/train': '0.62847', 'examples_per_second': '31.593', 'grad_norm': '25.25', 'counters/examples': 96512, 'counters/updates': 3016}
train stats after 96544 examples: {'rewards_train/chosen': '0.086555', 'rewards_train/rejected': '-0.028303', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11486', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-128.53', 'loss/train': '0.64409', 'examples_per_second': '31.764', 'grad_norm': '22.875', 'counters/examples': 96544, 'counters/updates': 3017}
skipping logging after 96576 examples to avoid logging too frequently
train stats after 96608 examples: {'rewards_train/chosen': '0.14848', 'rewards_train/rejected': '0.063509', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084975', 'logps_train/rejected': '-165.1', 'logps_train/chosen': '-149.37', 'loss/train': '0.66056', 'examples_per_second': '31.599', 'grad_norm': '27', 'counters/examples': 96608, 'counters/updates': 3019}
train stats after 96640 examples: {'rewards_train/chosen': '0.11152', 'rewards_train/rejected': '0.068202', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043322', 'logps_train/rejected': '-160.03', 'logps_train/chosen': '-142.36', 'loss/train': '0.6813', 'examples_per_second': '31.609', 'grad_norm': '31.875', 'counters/examples': 96640, 'counters/updates': 3020}
train stats after 96672 examples: {'rewards_train/chosen': '0.059076', 'rewards_train/rejected': '0.0068236', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052253', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-153.39', 'loss/train': '0.67631', 'examples_per_second': '31.643', 'grad_norm': '30.125', 'counters/examples': 96672, 'counters/updates': 3021}
train stats after 96704 examples: {'rewards_train/chosen': '0.0092335', 'rewards_train/rejected': '0.0010492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0081843', 'logps_train/rejected': '-98.561', 'logps_train/chosen': '-123.2', 'loss/train': '0.69898', 'examples_per_second': '31.294', 'grad_norm': '23.75', 'counters/examples': 96704, 'counters/updates': 3022}
train stats after 96736 examples: {'rewards_train/chosen': '0.093195', 'rewards_train/rejected': '-0.039139', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13233', 'logps_train/rejected': '-111.22', 'logps_train/chosen': '-151.26', 'loss/train': '0.64005', 'examples_per_second': '32.409', 'grad_norm': '24.25', 'counters/examples': 96736, 'counters/updates': 3023}
train stats after 96768 examples: {'rewards_train/chosen': '0.077815', 'rewards_train/rejected': '0.064311', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.013504', 'logps_train/rejected': '-153.68', 'logps_train/chosen': '-155.98', 'loss/train': '0.69049', 'examples_per_second': '31.149', 'grad_norm': '29.25', 'counters/examples': 96768, 'counters/updates': 3024}
train stats after 96800 examples: {'rewards_train/chosen': '0.065716', 'rewards_train/rejected': '0.044916', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0208', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-127.96', 'loss/train': '0.68817', 'examples_per_second': '31.168', 'grad_norm': '27.5', 'counters/examples': 96800, 'counters/updates': 3025}
skipping logging after 96832 examples to avoid logging too frequently
train stats after 96864 examples: {'rewards_train/chosen': '0.11463', 'rewards_train/rejected': '0.081431', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033202', 'logps_train/rejected': '-144.35', 'logps_train/chosen': '-150.22', 'loss/train': '0.68501', 'examples_per_second': '31.72', 'grad_norm': '49.75', 'counters/examples': 96864, 'counters/updates': 3027}
train stats after 96896 examples: {'rewards_train/chosen': '0.13244', 'rewards_train/rejected': '0.045813', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086628', 'logps_train/rejected': '-121.78', 'logps_train/chosen': '-145.9', 'loss/train': '0.65774', 'examples_per_second': '31.269', 'grad_norm': '26.25', 'counters/examples': 96896, 'counters/updates': 3028}
train stats after 96928 examples: {'rewards_train/chosen': '0.01628', 'rewards_train/rejected': '-0.048151', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064431', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-164.61', 'loss/train': '0.66989', 'examples_per_second': '32.479', 'grad_norm': '27.625', 'counters/examples': 96928, 'counters/updates': 3029}
train stats after 96960 examples: {'rewards_train/chosen': '0.063204', 'rewards_train/rejected': '0.0069928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056212', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-128.66', 'loss/train': '0.66855', 'examples_per_second': '32.527', 'grad_norm': '23.5', 'counters/examples': 96960, 'counters/updates': 3030}
train stats after 96992 examples: {'rewards_train/chosen': '0.083457', 'rewards_train/rejected': '0.056181', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027276', 'logps_train/rejected': '-132', 'logps_train/chosen': '-150.1', 'loss/train': '0.68846', 'examples_per_second': '31.255', 'grad_norm': '27.625', 'counters/examples': 96992, 'counters/updates': 3031}
train stats after 97024 examples: {'rewards_train/chosen': '0.077294', 'rewards_train/rejected': '0.059317', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017976', 'logps_train/rejected': '-110.36', 'logps_train/chosen': '-113.96', 'loss/train': '0.69189', 'examples_per_second': '31.652', 'grad_norm': '26.25', 'counters/examples': 97024, 'counters/updates': 3032}
train stats after 97056 examples: {'rewards_train/chosen': '0.06039', 'rewards_train/rejected': '0.051194', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0091965', 'logps_train/rejected': '-100.55', 'logps_train/chosen': '-131.05', 'loss/train': '0.69163', 'examples_per_second': '31.414', 'grad_norm': '25.75', 'counters/examples': 97056, 'counters/updates': 3033}
train stats after 97088 examples: {'rewards_train/chosen': '0.027219', 'rewards_train/rejected': '0.035079', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00786', 'logps_train/rejected': '-106.64', 'logps_train/chosen': '-134.49', 'loss/train': '0.70169', 'examples_per_second': '31.277', 'grad_norm': '25.5', 'counters/examples': 97088, 'counters/updates': 3034}
train stats after 97120 examples: {'rewards_train/chosen': '0.083472', 'rewards_train/rejected': '0.052017', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031455', 'logps_train/rejected': '-149.21', 'logps_train/chosen': '-132.94', 'loss/train': '0.68405', 'examples_per_second': '32.914', 'grad_norm': '28.625', 'counters/examples': 97120, 'counters/updates': 3035}
train stats after 97152 examples: {'rewards_train/chosen': '0.11158', 'rewards_train/rejected': '0.048729', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062852', 'logps_train/rejected': '-177.92', 'logps_train/chosen': '-118.19', 'loss/train': '0.67112', 'examples_per_second': '31.428', 'grad_norm': '29', 'counters/examples': 97152, 'counters/updates': 3036}
train stats after 97184 examples: {'rewards_train/chosen': '0.10112', 'rewards_train/rejected': '-0.0061898', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10731', 'logps_train/rejected': '-104.99', 'logps_train/chosen': '-128.6', 'loss/train': '0.64821', 'examples_per_second': '30.972', 'grad_norm': '23', 'counters/examples': 97184, 'counters/updates': 3037}
train stats after 97216 examples: {'rewards_train/chosen': '0.10036', 'rewards_train/rejected': '-0.013537', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1139', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-140.04', 'loss/train': '0.64726', 'examples_per_second': '33.026', 'grad_norm': '28.25', 'counters/examples': 97216, 'counters/updates': 3038}
train stats after 97248 examples: {'rewards_train/chosen': '0.049163', 'rewards_train/rejected': '0.10394', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.054773', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-110.44', 'loss/train': '0.73096', 'examples_per_second': '31.356', 'grad_norm': '28.625', 'counters/examples': 97248, 'counters/updates': 3039}
train stats after 97280 examples: {'rewards_train/chosen': '0.078374', 'rewards_train/rejected': '0.02921', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049164', 'logps_train/rejected': '-137.28', 'logps_train/chosen': '-124.15', 'loss/train': '0.68125', 'examples_per_second': '30.036', 'grad_norm': '26.75', 'counters/examples': 97280, 'counters/updates': 3040}
train stats after 97312 examples: {'rewards_train/chosen': '0.084153', 'rewards_train/rejected': '0.061198', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022954', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-160.12', 'loss/train': '0.69122', 'examples_per_second': '31.623', 'grad_norm': '28.125', 'counters/examples': 97312, 'counters/updates': 3041}
train stats after 97344 examples: {'rewards_train/chosen': '0.073434', 'rewards_train/rejected': '0.035454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037981', 'logps_train/rejected': '-97.663', 'logps_train/chosen': '-143.84', 'loss/train': '0.6801', 'examples_per_second': '30.894', 'grad_norm': '26.5', 'counters/examples': 97344, 'counters/updates': 3042}
train stats after 97376 examples: {'rewards_train/chosen': '0.15257', 'rewards_train/rejected': '0.081732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070833', 'logps_train/rejected': '-90.121', 'logps_train/chosen': '-135.83', 'loss/train': '0.67375', 'examples_per_second': '31.411', 'grad_norm': '27.25', 'counters/examples': 97376, 'counters/updates': 3043}
train stats after 97408 examples: {'rewards_train/chosen': '0.071171', 'rewards_train/rejected': '0.044056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027115', 'logps_train/rejected': '-148.28', 'logps_train/chosen': '-127.11', 'loss/train': '0.68558', 'examples_per_second': '30.676', 'grad_norm': '25.875', 'counters/examples': 97408, 'counters/updates': 3044}
train stats after 97440 examples: {'rewards_train/chosen': '0.064072', 'rewards_train/rejected': '0.034013', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03006', 'logps_train/rejected': '-186.78', 'logps_train/chosen': '-134.43', 'loss/train': '0.69182', 'examples_per_second': '30.59', 'grad_norm': '30.875', 'counters/examples': 97440, 'counters/updates': 3045}
train stats after 97472 examples: {'rewards_train/chosen': '0.081592', 'rewards_train/rejected': '0.028894', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052698', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-133.71', 'loss/train': '0.67608', 'examples_per_second': '31.649', 'grad_norm': '25.5', 'counters/examples': 97472, 'counters/updates': 3046}
train stats after 97504 examples: {'rewards_train/chosen': '0.17986', 'rewards_train/rejected': '0.026391', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15347', 'logps_train/rejected': '-103.36', 'logps_train/chosen': '-155.38', 'loss/train': '0.62856', 'examples_per_second': '30.944', 'grad_norm': '25.5', 'counters/examples': 97504, 'counters/updates': 3047}
train stats after 97536 examples: {'rewards_train/chosen': '0.12939', 'rewards_train/rejected': '0.052039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077347', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-142.47', 'loss/train': '0.66554', 'examples_per_second': '31.69', 'grad_norm': '27.25', 'counters/examples': 97536, 'counters/updates': 3048}
train stats after 97568 examples: {'rewards_train/chosen': '0.087249', 'rewards_train/rejected': '0.04921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038039', 'logps_train/rejected': '-105', 'logps_train/chosen': '-123.33', 'loss/train': '0.68275', 'examples_per_second': '32.208', 'grad_norm': '23.5', 'counters/examples': 97568, 'counters/updates': 3049}
train stats after 97600 examples: {'rewards_train/chosen': '0.091119', 'rewards_train/rejected': '0.026771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064349', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-156.57', 'loss/train': '0.66674', 'examples_per_second': '31.552', 'grad_norm': '56.25', 'counters/examples': 97600, 'counters/updates': 3050}
train stats after 97632 examples: {'rewards_train/chosen': '0.080776', 'rewards_train/rejected': '-0.02065', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10143', 'logps_train/rejected': '-108.45', 'logps_train/chosen': '-133.7', 'loss/train': '0.65161', 'examples_per_second': '24.634', 'grad_norm': '22.625', 'counters/examples': 97632, 'counters/updates': 3051}
train stats after 97664 examples: {'rewards_train/chosen': '0.10842', 'rewards_train/rejected': '0.056637', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051786', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-131.02', 'loss/train': '0.67457', 'examples_per_second': '31.597', 'grad_norm': '26.625', 'counters/examples': 97664, 'counters/updates': 3052}
train stats after 97696 examples: {'rewards_train/chosen': '0.011153', 'rewards_train/rejected': '0.05608', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.044927', 'logps_train/rejected': '-132.43', 'logps_train/chosen': '-154.52', 'loss/train': '0.72161', 'examples_per_second': '31.519', 'grad_norm': '32.25', 'counters/examples': 97696, 'counters/updates': 3053}
train stats after 97728 examples: {'rewards_train/chosen': '0.09735', 'rewards_train/rejected': '-0.0089168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10627', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-161.46', 'loss/train': '0.651', 'examples_per_second': '31.543', 'grad_norm': '29.625', 'counters/examples': 97728, 'counters/updates': 3054}
skipping logging after 97760 examples to avoid logging too frequently
train stats after 97792 examples: {'rewards_train/chosen': '0.076518', 'rewards_train/rejected': '0.02069', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055828', 'logps_train/rejected': '-141.1', 'logps_train/chosen': '-167.86', 'loss/train': '0.6791', 'examples_per_second': '31.615', 'grad_norm': '27.5', 'counters/examples': 97792, 'counters/updates': 3056}
skipping logging after 97824 examples to avoid logging too frequently
train stats after 97856 examples: {'rewards_train/chosen': '0.12609', 'rewards_train/rejected': '0.044893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081195', 'logps_train/rejected': '-138.18', 'logps_train/chosen': '-165.76', 'loss/train': '0.65978', 'examples_per_second': '31.252', 'grad_norm': '26.625', 'counters/examples': 97856, 'counters/updates': 3058}
skipping logging after 97888 examples to avoid logging too frequently
train stats after 97920 examples: {'rewards_train/chosen': '0.05318', 'rewards_train/rejected': '0.05831', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0051307', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-128.92', 'loss/train': '0.70422', 'examples_per_second': '32.449', 'grad_norm': '26.375', 'counters/examples': 97920, 'counters/updates': 3060}
train stats after 97952 examples: {'rewards_train/chosen': '-0.0057699', 'rewards_train/rejected': '0.021689', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027459', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-100.57', 'loss/train': '0.71189', 'examples_per_second': '32.459', 'grad_norm': '23.625', 'counters/examples': 97952, 'counters/updates': 3061}
train stats after 97984 examples: {'rewards_train/chosen': '0.026595', 'rewards_train/rejected': '0.045669', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019074', 'logps_train/rejected': '-114.34', 'logps_train/chosen': '-107.94', 'loss/train': '0.71394', 'examples_per_second': '31.614', 'grad_norm': '23.625', 'counters/examples': 97984, 'counters/updates': 3062}
train stats after 98016 examples: {'rewards_train/chosen': '0.1461', 'rewards_train/rejected': '0.09212', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053985', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-153.44', 'loss/train': '0.67884', 'examples_per_second': '30.616', 'grad_norm': '30.875', 'counters/examples': 98016, 'counters/updates': 3063}
train stats after 98048 examples: {'rewards_train/chosen': '0.053348', 'rewards_train/rejected': '0.0023788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050969', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-122.18', 'loss/train': '0.67321', 'examples_per_second': '32.123', 'grad_norm': '23', 'counters/examples': 98048, 'counters/updates': 3064}
train stats after 98080 examples: {'rewards_train/chosen': '0.11934', 'rewards_train/rejected': '0.10077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018576', 'logps_train/rejected': '-150.23', 'logps_train/chosen': '-134.7', 'loss/train': '0.69043', 'examples_per_second': '30.279', 'grad_norm': '26.25', 'counters/examples': 98080, 'counters/updates': 3065}
train stats after 98112 examples: {'rewards_train/chosen': '0.12898', 'rewards_train/rejected': '-0.045306', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17429', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-164.99', 'loss/train': '0.62469', 'examples_per_second': '31.599', 'grad_norm': '23.875', 'counters/examples': 98112, 'counters/updates': 3066}
train stats after 98144 examples: {'rewards_train/chosen': '0.062026', 'rewards_train/rejected': '0.051249', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010778', 'logps_train/rejected': '-136.13', 'logps_train/chosen': '-171.39', 'loss/train': '0.69645', 'examples_per_second': '30.086', 'grad_norm': '29.375', 'counters/examples': 98144, 'counters/updates': 3067}
train stats after 98176 examples: {'rewards_train/chosen': '0.17152', 'rewards_train/rejected': '0.04491', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12661', 'logps_train/rejected': '-91.815', 'logps_train/chosen': '-132.06', 'loss/train': '0.64355', 'examples_per_second': '31.665', 'grad_norm': '24.25', 'counters/examples': 98176, 'counters/updates': 3068}
train stats after 98208 examples: {'rewards_train/chosen': '0.082397', 'rewards_train/rejected': '0.088081', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0056839', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-118.28', 'loss/train': '0.70291', 'examples_per_second': '31.625', 'grad_norm': '28.875', 'counters/examples': 98208, 'counters/updates': 3069}
train stats after 98240 examples: {'rewards_train/chosen': '0.0633', 'rewards_train/rejected': '0.0023642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060936', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-128.71', 'loss/train': '0.67027', 'examples_per_second': '30.689', 'grad_norm': '24.125', 'counters/examples': 98240, 'counters/updates': 3070}
train stats after 98272 examples: {'rewards_train/chosen': '0.092471', 'rewards_train/rejected': '0.033305', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059166', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-100.3', 'loss/train': '0.66918', 'examples_per_second': '32.335', 'grad_norm': '25.625', 'counters/examples': 98272, 'counters/updates': 3071}
train stats after 98304 examples: {'rewards_train/chosen': '0.075489', 'rewards_train/rejected': '0.021655', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053834', 'logps_train/rejected': '-159.78', 'logps_train/chosen': '-122.8', 'loss/train': '0.67857', 'examples_per_second': '31.615', 'grad_norm': '33.5', 'counters/examples': 98304, 'counters/updates': 3072}
train stats after 98336 examples: {'rewards_train/chosen': '0.10854', 'rewards_train/rejected': '0.02797', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080567', 'logps_train/rejected': '-102.74', 'logps_train/chosen': '-168.98', 'loss/train': '0.65879', 'examples_per_second': '31.563', 'grad_norm': '27.5', 'counters/examples': 98336, 'counters/updates': 3073}
train stats after 98368 examples: {'rewards_train/chosen': '0.098444', 'rewards_train/rejected': '0.065471', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032972', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-176.35', 'loss/train': '0.68207', 'examples_per_second': '32.133', 'grad_norm': '32.75', 'counters/examples': 98368, 'counters/updates': 3074}
train stats after 98400 examples: {'rewards_train/chosen': '0.03726', 'rewards_train/rejected': '0.035375', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0018845', 'logps_train/rejected': '-157.87', 'logps_train/chosen': '-146.23', 'loss/train': '0.69765', 'examples_per_second': '32.933', 'grad_norm': '29.75', 'counters/examples': 98400, 'counters/updates': 3075}
train stats after 98432 examples: {'rewards_train/chosen': '0.1276', 'rewards_train/rejected': '0.070496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057102', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-163.2', 'loss/train': '0.67237', 'examples_per_second': '31.586', 'grad_norm': '29.875', 'counters/examples': 98432, 'counters/updates': 3076}
skipping logging after 98464 examples to avoid logging too frequently
train stats after 98496 examples: {'rewards_train/chosen': '0.16725', 'rewards_train/rejected': '0.071418', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09583', 'logps_train/rejected': '-108.64', 'logps_train/chosen': '-146.06', 'loss/train': '0.65428', 'examples_per_second': '30.071', 'grad_norm': '35', 'counters/examples': 98496, 'counters/updates': 3078}
train stats after 98528 examples: {'rewards_train/chosen': '0.13599', 'rewards_train/rejected': '0.030283', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1057', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-149.18', 'loss/train': '0.64843', 'examples_per_second': '30.148', 'grad_norm': '25.625', 'counters/examples': 98528, 'counters/updates': 3079}
train stats after 98560 examples: {'rewards_train/chosen': '0.15393', 'rewards_train/rejected': '0.054281', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099645', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-145.7', 'loss/train': '0.65302', 'examples_per_second': '31.55', 'grad_norm': '29.25', 'counters/examples': 98560, 'counters/updates': 3080}
skipping logging after 98592 examples to avoid logging too frequently
train stats after 98624 examples: {'rewards_train/chosen': '0.11483', 'rewards_train/rejected': '0.016538', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098287', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-141.72', 'loss/train': '0.65301', 'examples_per_second': '31.198', 'grad_norm': '26.625', 'counters/examples': 98624, 'counters/updates': 3082}
train stats after 98656 examples: {'rewards_train/chosen': '0.085663', 'rewards_train/rejected': '-0.0017944', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087457', 'logps_train/rejected': '-126.19', 'logps_train/chosen': '-144.1', 'loss/train': '0.65523', 'examples_per_second': '31.079', 'grad_norm': '24', 'counters/examples': 98656, 'counters/updates': 3083}
train stats after 98688 examples: {'rewards_train/chosen': '0.10111', 'rewards_train/rejected': '0.0055163', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095596', 'logps_train/rejected': '-112.1', 'logps_train/chosen': '-137.26', 'loss/train': '0.65336', 'examples_per_second': '31.343', 'grad_norm': '25.75', 'counters/examples': 98688, 'counters/updates': 3084}
train stats after 98720 examples: {'rewards_train/chosen': '0.093006', 'rewards_train/rejected': '0.022544', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070462', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-148', 'loss/train': '0.66767', 'examples_per_second': '30.516', 'grad_norm': '25.25', 'counters/examples': 98720, 'counters/updates': 3085}
train stats after 98752 examples: {'rewards_train/chosen': '0.024139', 'rewards_train/rejected': '0.030068', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0059286', 'logps_train/rejected': '-155.87', 'logps_train/chosen': '-133.03', 'loss/train': '0.70448', 'examples_per_second': '31.651', 'grad_norm': '28.875', 'counters/examples': 98752, 'counters/updates': 3086}
skipping logging after 98784 examples to avoid logging too frequently
train stats after 98816 examples: {'rewards_train/chosen': '0.13636', 'rewards_train/rejected': '0.042623', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093741', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-123.06', 'loss/train': '0.6548', 'examples_per_second': '31.836', 'grad_norm': '28.875', 'counters/examples': 98816, 'counters/updates': 3088}
train stats after 98848 examples: {'rewards_train/chosen': '0.13436', 'rewards_train/rejected': '0.082982', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051382', 'logps_train/rejected': '-169.78', 'logps_train/chosen': '-165.83', 'loss/train': '0.67869', 'examples_per_second': '31.683', 'grad_norm': '32.5', 'counters/examples': 98848, 'counters/updates': 3089}
train stats after 98880 examples: {'rewards_train/chosen': '0.096417', 'rewards_train/rejected': '0.055643', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.040774', 'logps_train/rejected': '-98.059', 'logps_train/chosen': '-126.35', 'loss/train': '0.67849', 'examples_per_second': '31.719', 'grad_norm': '25.25', 'counters/examples': 98880, 'counters/updates': 3090}
train stats after 98912 examples: {'rewards_train/chosen': '0.10991', 'rewards_train/rejected': '0.12673', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016814', 'logps_train/rejected': '-140.68', 'logps_train/chosen': '-156.07', 'loss/train': '0.7084', 'examples_per_second': '31.609', 'grad_norm': '28.875', 'counters/examples': 98912, 'counters/updates': 3091}
skipping logging after 98944 examples to avoid logging too frequently
train stats after 98976 examples: {'rewards_train/chosen': '0.14697', 'rewards_train/rejected': '0.061155', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085818', 'logps_train/rejected': '-107.24', 'logps_train/chosen': '-140.16', 'loss/train': '0.65614', 'examples_per_second': '34.324', 'grad_norm': '26.5', 'counters/examples': 98976, 'counters/updates': 3093}
train stats after 99008 examples: {'rewards_train/chosen': '0.098926', 'rewards_train/rejected': '0.019995', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078931', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-132.51', 'loss/train': '0.66204', 'examples_per_second': '30.517', 'grad_norm': '25.25', 'counters/examples': 99008, 'counters/updates': 3094}
train stats after 99040 examples: {'rewards_train/chosen': '0.093495', 'rewards_train/rejected': '0.041827', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051668', 'logps_train/rejected': '-138.51', 'logps_train/chosen': '-119.1', 'loss/train': '0.67316', 'examples_per_second': '31.47', 'grad_norm': '31.125', 'counters/examples': 99040, 'counters/updates': 3095}
train stats after 99072 examples: {'rewards_train/chosen': '0.079207', 'rewards_train/rejected': '0.11223', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033021', 'logps_train/rejected': '-129.28', 'logps_train/chosen': '-135.95', 'loss/train': '0.71887', 'examples_per_second': '32.592', 'grad_norm': '29.75', 'counters/examples': 99072, 'counters/updates': 3096}
train stats after 99104 examples: {'rewards_train/chosen': '0.10126', 'rewards_train/rejected': '-0.018008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11927', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-142.05', 'loss/train': '0.64421', 'examples_per_second': '30.647', 'grad_norm': '26.25', 'counters/examples': 99104, 'counters/updates': 3097}
skipping logging after 99136 examples to avoid logging too frequently
train stats after 99168 examples: {'rewards_train/chosen': '0.058315', 'rewards_train/rejected': '-0.017297', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075612', 'logps_train/rejected': '-103.1', 'logps_train/chosen': '-119.33', 'loss/train': '0.66613', 'examples_per_second': '32.584', 'grad_norm': '23.75', 'counters/examples': 99168, 'counters/updates': 3099}
train stats after 99200 examples: {'rewards_train/chosen': '0.10526', 'rewards_train/rejected': '0.067317', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037943', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-134.36', 'loss/train': '0.68455', 'examples_per_second': '30.499', 'grad_norm': '25.25', 'counters/examples': 99200, 'counters/updates': 3100}
train stats after 99232 examples: {'rewards_train/chosen': '0.094691', 'rewards_train/rejected': '0.028568', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066122', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-137.64', 'loss/train': '0.66797', 'examples_per_second': '32.748', 'grad_norm': '32.5', 'counters/examples': 99232, 'counters/updates': 3101}
train stats after 99264 examples: {'rewards_train/chosen': '0.064218', 'rewards_train/rejected': '-0.0052534', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069471', 'logps_train/rejected': '-112.78', 'logps_train/chosen': '-148.15', 'loss/train': '0.66616', 'examples_per_second': '31.01', 'grad_norm': '25.875', 'counters/examples': 99264, 'counters/updates': 3102}
train stats after 99296 examples: {'rewards_train/chosen': '0.1309', 'rewards_train/rejected': '0.080761', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05014', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-123.41', 'loss/train': '0.67808', 'examples_per_second': '31.607', 'grad_norm': '27.5', 'counters/examples': 99296, 'counters/updates': 3103}
train stats after 99328 examples: {'rewards_train/chosen': '0.092737', 'rewards_train/rejected': '0.0052034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087533', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-154.67', 'loss/train': '0.65574', 'examples_per_second': '30.664', 'grad_norm': '27.625', 'counters/examples': 99328, 'counters/updates': 3104}
train stats after 99360 examples: {'rewards_train/chosen': '0.054584', 'rewards_train/rejected': '-0.076423', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13101', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-117.4', 'loss/train': '0.64231', 'examples_per_second': '32.607', 'grad_norm': '23', 'counters/examples': 99360, 'counters/updates': 3105}
train stats after 99392 examples: {'rewards_train/chosen': '0.072929', 'rewards_train/rejected': '0.024583', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048346', 'logps_train/rejected': '-135.18', 'logps_train/chosen': '-144.95', 'loss/train': '0.67847', 'examples_per_second': '30.102', 'grad_norm': '33.25', 'counters/examples': 99392, 'counters/updates': 3106}
skipping logging after 99424 examples to avoid logging too frequently
train stats after 99456 examples: {'rewards_train/chosen': '0.081988', 'rewards_train/rejected': '0.0050076', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07698', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-136.93', 'loss/train': '0.66154', 'examples_per_second': '32.782', 'grad_norm': '25.5', 'counters/examples': 99456, 'counters/updates': 3108}
skipping logging after 99488 examples to avoid logging too frequently
train stats after 99520 examples: {'rewards_train/chosen': '0.090408', 'rewards_train/rejected': '0.031567', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058841', 'logps_train/rejected': '-132.28', 'logps_train/chosen': '-129.78', 'loss/train': '0.67201', 'examples_per_second': '32.354', 'grad_norm': '29.125', 'counters/examples': 99520, 'counters/updates': 3110}
train stats after 99552 examples: {'rewards_train/chosen': '0.056068', 'rewards_train/rejected': '-0.0251', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081168', 'logps_train/rejected': '-117.74', 'logps_train/chosen': '-124.95', 'loss/train': '0.65912', 'examples_per_second': '32.6', 'grad_norm': '25.875', 'counters/examples': 99552, 'counters/updates': 3111}
train stats after 99584 examples: {'rewards_train/chosen': '0.1302', 'rewards_train/rejected': '0.080343', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049858', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-163.75', 'loss/train': '0.67713', 'examples_per_second': '31.65', 'grad_norm': '28', 'counters/examples': 99584, 'counters/updates': 3112}
skipping logging after 99616 examples to avoid logging too frequently
train stats after 99648 examples: {'rewards_train/chosen': '0.095035', 'rewards_train/rejected': '0.028308', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066727', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-122.49', 'loss/train': '0.66937', 'examples_per_second': '32.874', 'grad_norm': '33.75', 'counters/examples': 99648, 'counters/updates': 3114}
train stats after 99680 examples: {'rewards_train/chosen': '0.13867', 'rewards_train/rejected': '0.0246', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11407', 'logps_train/rejected': '-101.02', 'logps_train/chosen': '-123.33', 'loss/train': '0.64675', 'examples_per_second': '31.517', 'grad_norm': '22.375', 'counters/examples': 99680, 'counters/updates': 3115}
skipping logging after 99712 examples to avoid logging too frequently
train stats after 99744 examples: {'rewards_train/chosen': '0.095555', 'rewards_train/rejected': '0.034274', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06128', 'logps_train/rejected': '-89.821', 'logps_train/chosen': '-112.98', 'loss/train': '0.66652', 'examples_per_second': '31.538', 'grad_norm': '21.75', 'counters/examples': 99744, 'counters/updates': 3117}
train stats after 99776 examples: {'rewards_train/chosen': '0.12633', 'rewards_train/rejected': '0.067298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059029', 'logps_train/rejected': '-144.32', 'logps_train/chosen': '-114.46', 'loss/train': '0.67841', 'examples_per_second': '30.634', 'grad_norm': '30.375', 'counters/examples': 99776, 'counters/updates': 3118}
skipping logging after 99808 examples to avoid logging too frequently
train stats after 99840 examples: {'rewards_train/chosen': '0.15803', 'rewards_train/rejected': '0.093095', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064938', 'logps_train/rejected': '-164.36', 'logps_train/chosen': '-178.8', 'loss/train': '0.67127', 'examples_per_second': '30.172', 'grad_norm': '32', 'counters/examples': 99840, 'counters/updates': 3120}
train stats after 99872 examples: {'rewards_train/chosen': '0.13499', 'rewards_train/rejected': '0.057201', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077786', 'logps_train/rejected': '-142.58', 'logps_train/chosen': '-133.93', 'loss/train': '0.66333', 'examples_per_second': '30.528', 'grad_norm': '25.75', 'counters/examples': 99872, 'counters/updates': 3121}
train stats after 99904 examples: {'rewards_train/chosen': '0.10037', 'rewards_train/rejected': '0.036765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063603', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-176.42', 'loss/train': '0.67765', 'examples_per_second': '31.649', 'grad_norm': '29.875', 'counters/examples': 99904, 'counters/updates': 3122}
train stats after 99936 examples: {'rewards_train/chosen': '0.062458', 'rewards_train/rejected': '0.053943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.008515', 'logps_train/rejected': '-168.07', 'logps_train/chosen': '-184.78', 'loss/train': '0.69629', 'examples_per_second': '32.384', 'grad_norm': '33', 'counters/examples': 99936, 'counters/updates': 3123}
skipping logging after 99968 examples to avoid logging too frequently
train stats after 100000 examples: {'rewards_train/chosen': '0.083807', 'rewards_train/rejected': '-0.0048979', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088705', 'logps_train/rejected': '-107.68', 'logps_train/chosen': '-146.36', 'loss/train': '0.65869', 'examples_per_second': '31.663', 'grad_norm': '26', 'counters/examples': 100000, 'counters/updates': 3125}
train stats after 100032 examples: {'rewards_train/chosen': '0.074604', 'rewards_train/rejected': '0.00032265', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074281', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-124.06', 'loss/train': '0.66293', 'examples_per_second': '32.907', 'grad_norm': '27.625', 'counters/examples': 100032, 'counters/updates': 3126}
skipping logging after 100064 examples to avoid logging too frequently
train stats after 100096 examples: {'rewards_train/chosen': '0.049885', 'rewards_train/rejected': '0.0023262', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047559', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-129.08', 'loss/train': '0.67618', 'examples_per_second': '33.519', 'grad_norm': '23.625', 'counters/examples': 100096, 'counters/updates': 3128}
skipping logging after 100128 examples to avoid logging too frequently
train stats after 100160 examples: {'rewards_train/chosen': '0.097856', 'rewards_train/rejected': '0.0023233', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095533', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-138.33', 'loss/train': '0.65495', 'examples_per_second': '31.473', 'grad_norm': '25.25', 'counters/examples': 100160, 'counters/updates': 3130}
train stats after 100192 examples: {'rewards_train/chosen': '0.080268', 'rewards_train/rejected': '0.032265', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048003', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-111.7', 'loss/train': '0.67563', 'examples_per_second': '31.999', 'grad_norm': '28', 'counters/examples': 100192, 'counters/updates': 3131}
skipping logging after 100224 examples to avoid logging too frequently
train stats after 100256 examples: {'rewards_train/chosen': '0.12977', 'rewards_train/rejected': '0.060636', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069131', 'logps_train/rejected': '-106.72', 'logps_train/chosen': '-131.38', 'loss/train': '0.6647', 'examples_per_second': '30.795', 'grad_norm': '26.625', 'counters/examples': 100256, 'counters/updates': 3133}
skipping logging after 100288 examples to avoid logging too frequently
train stats after 100320 examples: {'rewards_train/chosen': '0.11845', 'rewards_train/rejected': '0.080765', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037683', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-118.81', 'loss/train': '0.67978', 'examples_per_second': '31.771', 'grad_norm': '25.875', 'counters/examples': 100320, 'counters/updates': 3135}
train stats after 100352 examples: {'rewards_train/chosen': '0.16481', 'rewards_train/rejected': '0.058883', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10593', 'logps_train/rejected': '-118.4', 'logps_train/chosen': '-129.38', 'loss/train': '0.6507', 'examples_per_second': '31.204', 'grad_norm': '24.125', 'counters/examples': 100352, 'counters/updates': 3136}
train stats after 100384 examples: {'rewards_train/chosen': '0.19817', 'rewards_train/rejected': '0.018658', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17951', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-145.34', 'loss/train': '0.61845', 'examples_per_second': '30.647', 'grad_norm': '27.75', 'counters/examples': 100384, 'counters/updates': 3137}
train stats after 100416 examples: {'rewards_train/chosen': '0.085257', 'rewards_train/rejected': '-0.02634', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1116', 'logps_train/rejected': '-104.13', 'logps_train/chosen': '-113.01', 'loss/train': '0.65146', 'examples_per_second': '32.695', 'grad_norm': '25', 'counters/examples': 100416, 'counters/updates': 3138}
train stats after 100448 examples: {'rewards_train/chosen': '0.091381', 'rewards_train/rejected': '0.018829', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072552', 'logps_train/rejected': '-151.02', 'logps_train/chosen': '-143.65', 'loss/train': '0.6693', 'examples_per_second': '31.599', 'grad_norm': '29', 'counters/examples': 100448, 'counters/updates': 3139}
train stats after 100480 examples: {'rewards_train/chosen': '0.066538', 'rewards_train/rejected': '0.031041', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035497', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-110.85', 'loss/train': '0.67993', 'examples_per_second': '30.218', 'grad_norm': '24.25', 'counters/examples': 100480, 'counters/updates': 3140}
train stats after 100512 examples: {'rewards_train/chosen': '0.067943', 'rewards_train/rejected': '-0.026409', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094352', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-127.19', 'loss/train': '0.65304', 'examples_per_second': '32.194', 'grad_norm': '24.5', 'counters/examples': 100512, 'counters/updates': 3141}
train stats after 100544 examples: {'rewards_train/chosen': '0.11828', 'rewards_train/rejected': '-0.0046471', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12293', 'logps_train/rejected': '-111.5', 'logps_train/chosen': '-141.74', 'loss/train': '0.6407', 'examples_per_second': '30.912', 'grad_norm': '24.875', 'counters/examples': 100544, 'counters/updates': 3142}
train stats after 100576 examples: {'rewards_train/chosen': '0.084285', 'rewards_train/rejected': '0.06641', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017876', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-159.63', 'loss/train': '0.69138', 'examples_per_second': '33.083', 'grad_norm': '29.5', 'counters/examples': 100576, 'counters/updates': 3143}
train stats after 100608 examples: {'rewards_train/chosen': '0.079926', 'rewards_train/rejected': '0.0091138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070812', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-170.5', 'loss/train': '0.66641', 'examples_per_second': '30.667', 'grad_norm': '29.5', 'counters/examples': 100608, 'counters/updates': 3144}
train stats after 100640 examples: {'rewards_train/chosen': '0.086794', 'rewards_train/rejected': '0.043121', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043673', 'logps_train/rejected': '-105.01', 'logps_train/chosen': '-171.07', 'loss/train': '0.67872', 'examples_per_second': '31.627', 'grad_norm': '27.75', 'counters/examples': 100640, 'counters/updates': 3145}
skipping logging after 100672 examples to avoid logging too frequently
train stats after 100704 examples: {'rewards_train/chosen': '0.084622', 'rewards_train/rejected': '0.042589', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042033', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-151.55', 'loss/train': '0.68176', 'examples_per_second': '30.241', 'grad_norm': '49.75', 'counters/examples': 100704, 'counters/updates': 3147}
train stats after 100736 examples: {'rewards_train/chosen': '0.18454', 'rewards_train/rejected': '0.07181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11274', 'logps_train/rejected': '-149.1', 'logps_train/chosen': '-199.54', 'loss/train': '0.64725', 'examples_per_second': '30.64', 'grad_norm': '28.5', 'counters/examples': 100736, 'counters/updates': 3148}
train stats after 100768 examples: {'rewards_train/chosen': '0.040502', 'rewards_train/rejected': '0.037189', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0033126', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-119.12', 'loss/train': '0.69804', 'examples_per_second': '32.24', 'grad_norm': '29.125', 'counters/examples': 100768, 'counters/updates': 3149}
train stats after 100800 examples: {'rewards_train/chosen': '0.14336', 'rewards_train/rejected': '-0.033819', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17718', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-141.78', 'loss/train': '0.62231', 'examples_per_second': '31.627', 'grad_norm': '25.625', 'counters/examples': 100800, 'counters/updates': 3150}
train stats after 100832 examples: {'rewards_train/chosen': '0.1276', 'rewards_train/rejected': '0.078114', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049482', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-193.31', 'loss/train': '0.6751', 'examples_per_second': '30.602', 'grad_norm': '27.75', 'counters/examples': 100832, 'counters/updates': 3151}
train stats after 100864 examples: {'rewards_train/chosen': '0.12461', 'rewards_train/rejected': '-0.00065891', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12527', 'logps_train/rejected': '-89.859', 'logps_train/chosen': '-128.79', 'loss/train': '0.6429', 'examples_per_second': '31.261', 'grad_norm': '23.875', 'counters/examples': 100864, 'counters/updates': 3152}
train stats after 100896 examples: {'rewards_train/chosen': '0.11376', 'rewards_train/rejected': '0.056175', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057587', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-139.19', 'loss/train': '0.67101', 'examples_per_second': '30.494', 'grad_norm': '26.75', 'counters/examples': 100896, 'counters/updates': 3153}
train stats after 100928 examples: {'rewards_train/chosen': '0.10218', 'rewards_train/rejected': '0.072715', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029464', 'logps_train/rejected': '-153.11', 'logps_train/chosen': '-111.46', 'loss/train': '0.68637', 'examples_per_second': '31.115', 'grad_norm': '26', 'counters/examples': 100928, 'counters/updates': 3154}
train stats after 100960 examples: {'rewards_train/chosen': '0.060401', 'rewards_train/rejected': '-0.020598', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080999', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-154.61', 'loss/train': '0.65733', 'examples_per_second': '30.507', 'grad_norm': '25.875', 'counters/examples': 100960, 'counters/updates': 3155}
train stats after 100992 examples: {'rewards_train/chosen': '0.12216', 'rewards_train/rejected': '0.043487', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078669', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-112.89', 'loss/train': '0.66065', 'examples_per_second': '31.649', 'grad_norm': '22.375', 'counters/examples': 100992, 'counters/updates': 3156}
train stats after 101024 examples: {'rewards_train/chosen': '0.14982', 'rewards_train/rejected': '0.062967', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086848', 'logps_train/rejected': '-143.03', 'logps_train/chosen': '-135.83', 'loss/train': '0.66081', 'examples_per_second': '30.138', 'grad_norm': '27.875', 'counters/examples': 101024, 'counters/updates': 3157}
skipping logging after 101056 examples to avoid logging too frequently
train stats after 101088 examples: {'rewards_train/chosen': '0.048114', 'rewards_train/rejected': '0.050546', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0024326', 'logps_train/rejected': '-113.37', 'logps_train/chosen': '-131.16', 'loss/train': '0.69858', 'examples_per_second': '30.091', 'grad_norm': '26.125', 'counters/examples': 101088, 'counters/updates': 3159}
train stats after 101120 examples: {'rewards_train/chosen': '0.078491', 'rewards_train/rejected': '-0.010573', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089064', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-172.16', 'loss/train': '0.65407', 'examples_per_second': '32.051', 'grad_norm': '33.25', 'counters/examples': 101120, 'counters/updates': 3160}
train stats after 101152 examples: {'rewards_train/chosen': '0.043179', 'rewards_train/rejected': '0.015675', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.027504', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-134.47', 'loss/train': '0.68528', 'examples_per_second': '31.175', 'grad_norm': '25.125', 'counters/examples': 101152, 'counters/updates': 3161}
train stats after 101184 examples: {'rewards_train/chosen': '0.15417', 'rewards_train/rejected': '0.029722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12445', 'logps_train/rejected': '-108.3', 'logps_train/chosen': '-145.91', 'loss/train': '0.64276', 'examples_per_second': '31.553', 'grad_norm': '24.75', 'counters/examples': 101184, 'counters/updates': 3162}
train stats after 101216 examples: {'rewards_train/chosen': '0.14112', 'rewards_train/rejected': '0.032773', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10834', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-156.58', 'loss/train': '0.64884', 'examples_per_second': '31.33', 'grad_norm': '32.25', 'counters/examples': 101216, 'counters/updates': 3163}
skipping logging after 101248 examples to avoid logging too frequently
train stats after 101280 examples: {'rewards_train/chosen': '0.065787', 'rewards_train/rejected': '0.03989', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025897', 'logps_train/rejected': '-116.04', 'logps_train/chosen': '-130.29', 'loss/train': '0.68918', 'examples_per_second': '31.046', 'grad_norm': '30.875', 'counters/examples': 101280, 'counters/updates': 3165}
skipping logging after 101312 examples to avoid logging too frequently
train stats after 101344 examples: {'rewards_train/chosen': '0.11356', 'rewards_train/rejected': '0.033386', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08017', 'logps_train/rejected': '-81.031', 'logps_train/chosen': '-108.27', 'loss/train': '0.65743', 'examples_per_second': '30.294', 'grad_norm': '20.625', 'counters/examples': 101344, 'counters/updates': 3167}
skipping logging after 101376 examples to avoid logging too frequently
train stats after 101408 examples: {'rewards_train/chosen': '0.15313', 'rewards_train/rejected': '0.084624', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06851', 'logps_train/rejected': '-176.55', 'logps_train/chosen': '-155.62', 'loss/train': '0.66805', 'examples_per_second': '30.592', 'grad_norm': '33.5', 'counters/examples': 101408, 'counters/updates': 3169}
train stats after 101440 examples: {'rewards_train/chosen': '0.14675', 'rewards_train/rejected': '0.062899', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083854', 'logps_train/rejected': '-138.32', 'logps_train/chosen': '-115', 'loss/train': '0.66166', 'examples_per_second': '30.685', 'grad_norm': '25.5', 'counters/examples': 101440, 'counters/updates': 3170}
train stats after 101472 examples: {'rewards_train/chosen': '0.12046', 'rewards_train/rejected': '0.020863', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0996', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-148.06', 'loss/train': '0.65268', 'examples_per_second': '31.604', 'grad_norm': '27.25', 'counters/examples': 101472, 'counters/updates': 3171}
train stats after 101504 examples: {'rewards_train/chosen': '0.04578', 'rewards_train/rejected': '0.015349', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030431', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-160.97', 'loss/train': '0.68717', 'examples_per_second': '31.393', 'grad_norm': '28.875', 'counters/examples': 101504, 'counters/updates': 3172}
train stats after 101536 examples: {'rewards_train/chosen': '0.082586', 'rewards_train/rejected': '-0.0017499', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.084336', 'logps_train/rejected': '-106.72', 'logps_train/chosen': '-119.04', 'loss/train': '0.65475', 'examples_per_second': '30.392', 'grad_norm': '23.375', 'counters/examples': 101536, 'counters/updates': 3173}
train stats after 101568 examples: {'rewards_train/chosen': '0.13328', 'rewards_train/rejected': '0.0079389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12534', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-147.82', 'loss/train': '0.63889', 'examples_per_second': '30.217', 'grad_norm': '25.5', 'counters/examples': 101568, 'counters/updates': 3174}
skipping logging after 101600 examples to avoid logging too frequently
train stats after 101632 examples: {'rewards_train/chosen': '0.16222', 'rewards_train/rejected': '0.035311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12691', 'logps_train/rejected': '-133.03', 'logps_train/chosen': '-146.84', 'loss/train': '0.63984', 'examples_per_second': '30.143', 'grad_norm': '27.375', 'counters/examples': 101632, 'counters/updates': 3176}
train stats after 101664 examples: {'rewards_train/chosen': '0.07988', 'rewards_train/rejected': '0.033889', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04599', 'logps_train/rejected': '-95.586', 'logps_train/chosen': '-122.79', 'loss/train': '0.67684', 'examples_per_second': '31.672', 'grad_norm': '23.5', 'counters/examples': 101664, 'counters/updates': 3177}
train stats after 101696 examples: {'rewards_train/chosen': '0.10808', 'rewards_train/rejected': '0.079082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028998', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-104.53', 'loss/train': '0.68351', 'examples_per_second': '30.759', 'grad_norm': '23', 'counters/examples': 101696, 'counters/updates': 3178}
train stats after 101728 examples: {'rewards_train/chosen': '0.08073', 'rewards_train/rejected': '0.026789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053942', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-134.01', 'loss/train': '0.67751', 'examples_per_second': '31.09', 'grad_norm': '27.375', 'counters/examples': 101728, 'counters/updates': 3179}
train stats after 101760 examples: {'rewards_train/chosen': '0.070136', 'rewards_train/rejected': '0.048292', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021843', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-109.51', 'loss/train': '0.68901', 'examples_per_second': '31.306', 'grad_norm': '21.75', 'counters/examples': 101760, 'counters/updates': 3180}
train stats after 101792 examples: {'rewards_train/chosen': '0.096876', 'rewards_train/rejected': '0.10744', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.010569', 'logps_train/rejected': '-119.03', 'logps_train/chosen': '-125.69', 'loss/train': '0.70927', 'examples_per_second': '31.688', 'grad_norm': '30.625', 'counters/examples': 101792, 'counters/updates': 3181}
skipping logging after 101824 examples to avoid logging too frequently
train stats after 101856 examples: {'rewards_train/chosen': '0.083399', 'rewards_train/rejected': '0.040989', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04241', 'logps_train/rejected': '-103.15', 'logps_train/chosen': '-123.36', 'loss/train': '0.67779', 'examples_per_second': '34.606', 'grad_norm': '24.375', 'counters/examples': 101856, 'counters/updates': 3183}
train stats after 101888 examples: {'rewards_train/chosen': '0.096653', 'rewards_train/rejected': '0.056841', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039812', 'logps_train/rejected': '-133.6', 'logps_train/chosen': '-153.96', 'loss/train': '0.68152', 'examples_per_second': '32.635', 'grad_norm': '28.875', 'counters/examples': 101888, 'counters/updates': 3184}
train stats after 101920 examples: {'rewards_train/chosen': '0.14912', 'rewards_train/rejected': '0.091737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057385', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-172.45', 'loss/train': '0.67646', 'examples_per_second': '31.509', 'grad_norm': '26.125', 'counters/examples': 101920, 'counters/updates': 3185}
train stats after 101952 examples: {'rewards_train/chosen': '0.084438', 'rewards_train/rejected': '0.044871', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039567', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-144.15', 'loss/train': '0.68036', 'examples_per_second': '30.169', 'grad_norm': '26.875', 'counters/examples': 101952, 'counters/updates': 3186}
train stats after 101984 examples: {'rewards_train/chosen': '0.13216', 'rewards_train/rejected': '-0.010917', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14307', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-164.09', 'loss/train': '0.63057', 'examples_per_second': '31.561', 'grad_norm': '28.25', 'counters/examples': 101984, 'counters/updates': 3187}
train stats after 102016 examples: {'rewards_train/chosen': '0.050441', 'rewards_train/rejected': '0.042451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0079902', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-128.52', 'loss/train': '0.69301', 'examples_per_second': '31.46', 'grad_norm': '31.625', 'counters/examples': 102016, 'counters/updates': 3188}
skipping logging after 102048 examples to avoid logging too frequently
train stats after 102080 examples: {'rewards_train/chosen': '0.11159', 'rewards_train/rejected': '0.01887', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092725', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-151.26', 'loss/train': '0.65704', 'examples_per_second': '31.504', 'grad_norm': '30.25', 'counters/examples': 102080, 'counters/updates': 3190}
train stats after 102112 examples: {'rewards_train/chosen': '0.10353', 'rewards_train/rejected': '0.011505', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092029', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-155.9', 'loss/train': '0.66087', 'examples_per_second': '30.624', 'grad_norm': '28.375', 'counters/examples': 102112, 'counters/updates': 3191}
train stats after 102144 examples: {'rewards_train/chosen': '0.082906', 'rewards_train/rejected': '0.024722', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058184', 'logps_train/rejected': '-93.207', 'logps_train/chosen': '-107.19', 'loss/train': '0.67126', 'examples_per_second': '32.731', 'grad_norm': '23.125', 'counters/examples': 102144, 'counters/updates': 3192}
train stats after 102176 examples: {'rewards_train/chosen': '0.063934', 'rewards_train/rejected': '0.0065078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057426', 'logps_train/rejected': '-109.24', 'logps_train/chosen': '-142.6', 'loss/train': '0.67239', 'examples_per_second': '31.285', 'grad_norm': '27.25', 'counters/examples': 102176, 'counters/updates': 3193}
train stats after 102208 examples: {'rewards_train/chosen': '0.080715', 'rewards_train/rejected': '0.032086', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.048629', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-155.11', 'loss/train': '0.67889', 'examples_per_second': '31.609', 'grad_norm': '31.875', 'counters/examples': 102208, 'counters/updates': 3194}
train stats after 102240 examples: {'rewards_train/chosen': '0.044255', 'rewards_train/rejected': '-0.018122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062377', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-126.79', 'loss/train': '0.67288', 'examples_per_second': '32.575', 'grad_norm': '25.375', 'counters/examples': 102240, 'counters/updates': 3195}
train stats after 102272 examples: {'rewards_train/chosen': '0.036072', 'rewards_train/rejected': '0.020014', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016058', 'logps_train/rejected': '-82.884', 'logps_train/chosen': '-128.06', 'loss/train': '0.69006', 'examples_per_second': '31.014', 'grad_norm': '24.875', 'counters/examples': 102272, 'counters/updates': 3196}
train stats after 102304 examples: {'rewards_train/chosen': '0.061301', 'rewards_train/rejected': '0.064064', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027633', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-181.24', 'loss/train': '0.7073', 'examples_per_second': '33.138', 'grad_norm': '35.5', 'counters/examples': 102304, 'counters/updates': 3197}
skipping logging after 102336 examples to avoid logging too frequently
train stats after 102368 examples: {'rewards_train/chosen': '0.13172', 'rewards_train/rejected': '0.044864', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086853', 'logps_train/rejected': '-99.925', 'logps_train/chosen': '-120.83', 'loss/train': '0.66366', 'examples_per_second': '31.967', 'grad_norm': '24.25', 'counters/examples': 102368, 'counters/updates': 3199}
train stats after 102400 examples: {'rewards_train/chosen': '0.090982', 'rewards_train/rejected': '0.056951', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034031', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-105.42', 'loss/train': '0.68448', 'examples_per_second': '31.227', 'grad_norm': '27.125', 'counters/examples': 102400, 'counters/updates': 3200}
train stats after 102432 examples: {'rewards_train/chosen': '0.10181', 'rewards_train/rejected': '0.040118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061689', 'logps_train/rejected': '-140.56', 'logps_train/chosen': '-135.56', 'loss/train': '0.67027', 'examples_per_second': '30.205', 'grad_norm': '25.75', 'counters/examples': 102432, 'counters/updates': 3201}
train stats after 102464 examples: {'rewards_train/chosen': '0.14597', 'rewards_train/rejected': '0.033295', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11268', 'logps_train/rejected': '-137.96', 'logps_train/chosen': '-146.74', 'loss/train': '0.64634', 'examples_per_second': '30.207', 'grad_norm': '24.5', 'counters/examples': 102464, 'counters/updates': 3202}
skipping logging after 102496 examples to avoid logging too frequently
train stats after 102528 examples: {'rewards_train/chosen': '0.1127', 'rewards_train/rejected': '0.049514', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063191', 'logps_train/rejected': '-105.28', 'logps_train/chosen': '-109.02', 'loss/train': '0.67063', 'examples_per_second': '32.73', 'grad_norm': '26', 'counters/examples': 102528, 'counters/updates': 3204}
train stats after 102560 examples: {'rewards_train/chosen': '0.1336', 'rewards_train/rejected': '-0.0023503', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13595', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-131.65', 'loss/train': '0.6371', 'examples_per_second': '23.668', 'grad_norm': '50.75', 'counters/examples': 102560, 'counters/updates': 3205}
train stats after 102592 examples: {'rewards_train/chosen': '0.20774', 'rewards_train/rejected': '-0.010314', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21805', 'logps_train/rejected': '-130.67', 'logps_train/chosen': '-153.64', 'loss/train': '0.59965', 'examples_per_second': '30.176', 'grad_norm': '31.875', 'counters/examples': 102592, 'counters/updates': 3206}
train stats after 102624 examples: {'rewards_train/chosen': '0.11758', 'rewards_train/rejected': '0.044477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073106', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-160.35', 'loss/train': '0.66594', 'examples_per_second': '30.434', 'grad_norm': '32.25', 'counters/examples': 102624, 'counters/updates': 3207}
train stats after 102656 examples: {'rewards_train/chosen': '0.0035977', 'rewards_train/rejected': '-0.012522', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016119', 'logps_train/rejected': '-144.57', 'logps_train/chosen': '-157.18', 'loss/train': '0.69205', 'examples_per_second': '22.891', 'grad_norm': '29.375', 'counters/examples': 102656, 'counters/updates': 3208}
train stats after 102688 examples: {'rewards_train/chosen': '0.10181', 'rewards_train/rejected': '0.01083', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090978', 'logps_train/rejected': '-135.08', 'logps_train/chosen': '-139.4', 'loss/train': '0.65677', 'examples_per_second': '32.458', 'grad_norm': '25.25', 'counters/examples': 102688, 'counters/updates': 3209}
train stats after 102720 examples: {'rewards_train/chosen': '0.093021', 'rewards_train/rejected': '-0.037458', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13048', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-144.46', 'loss/train': '0.63456', 'examples_per_second': '31.091', 'grad_norm': '23.5', 'counters/examples': 102720, 'counters/updates': 3210}
train stats after 102752 examples: {'rewards_train/chosen': '0.11612', 'rewards_train/rejected': '-0.0089769', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12509', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-133.37', 'loss/train': '0.64262', 'examples_per_second': '31.696', 'grad_norm': '23.625', 'counters/examples': 102752, 'counters/updates': 3211}
train stats after 102784 examples: {'rewards_train/chosen': '0.094775', 'rewards_train/rejected': '0.022688', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072086', 'logps_train/rejected': '-122.52', 'logps_train/chosen': '-142.63', 'loss/train': '0.66465', 'examples_per_second': '31.013', 'grad_norm': '43', 'counters/examples': 102784, 'counters/updates': 3212}
train stats after 102816 examples: {'rewards_train/chosen': '0.15582', 'rewards_train/rejected': '0.066689', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089134', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-156.16', 'loss/train': '0.66721', 'examples_per_second': '31.686', 'grad_norm': '28.375', 'counters/examples': 102816, 'counters/updates': 3213}
train stats after 102848 examples: {'rewards_train/chosen': '0.10768', 'rewards_train/rejected': '-0.025236', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13291', 'logps_train/rejected': '-111.85', 'logps_train/chosen': '-135.14', 'loss/train': '0.63285', 'examples_per_second': '31.309', 'grad_norm': '29.25', 'counters/examples': 102848, 'counters/updates': 3214}
skipping logging after 102880 examples to avoid logging too frequently
train stats after 102912 examples: {'rewards_train/chosen': '0.12336', 'rewards_train/rejected': '0.060292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063067', 'logps_train/rejected': '-125.29', 'logps_train/chosen': '-156.02', 'loss/train': '0.66849', 'examples_per_second': '31.601', 'grad_norm': '26.5', 'counters/examples': 102912, 'counters/updates': 3216}
train stats after 102944 examples: {'rewards_train/chosen': '0.13479', 'rewards_train/rejected': '0.030695', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1041', 'logps_train/rejected': '-89.137', 'logps_train/chosen': '-121.53', 'loss/train': '0.65323', 'examples_per_second': '30.89', 'grad_norm': '20.25', 'counters/examples': 102944, 'counters/updates': 3217}
train stats after 102976 examples: {'rewards_train/chosen': '0.045881', 'rewards_train/rejected': '0.082577', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036696', 'logps_train/rejected': '-165.63', 'logps_train/chosen': '-142.81', 'loss/train': '0.7221', 'examples_per_second': '30.931', 'grad_norm': '32', 'counters/examples': 102976, 'counters/updates': 3218}
train stats after 103008 examples: {'rewards_train/chosen': '0.037316', 'rewards_train/rejected': '0.004691', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032625', 'logps_train/rejected': '-115.16', 'logps_train/chosen': '-116.91', 'loss/train': '0.6855', 'examples_per_second': '30.315', 'grad_norm': '23.875', 'counters/examples': 103008, 'counters/updates': 3219}
train stats after 103040 examples: {'rewards_train/chosen': '0.036988', 'rewards_train/rejected': '-0.0016989', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038687', 'logps_train/rejected': '-188.67', 'logps_train/chosen': '-182.31', 'loss/train': '0.68103', 'examples_per_second': '31.691', 'grad_norm': '36.75', 'counters/examples': 103040, 'counters/updates': 3220}
skipping logging after 103072 examples to avoid logging too frequently
train stats after 103104 examples: {'rewards_train/chosen': '0.037457', 'rewards_train/rejected': '0.04325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0057932', 'logps_train/rejected': '-129.13', 'logps_train/chosen': '-135.24', 'loss/train': '0.70392', 'examples_per_second': '30.102', 'grad_norm': '27.375', 'counters/examples': 103104, 'counters/updates': 3222}
skipping logging after 103136 examples to avoid logging too frequently
train stats after 103168 examples: {'rewards_train/chosen': '0.14356', 'rewards_train/rejected': '0.074238', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069324', 'logps_train/rejected': '-146.82', 'logps_train/chosen': '-166.97', 'loss/train': '0.66514', 'examples_per_second': '30.168', 'grad_norm': '29.875', 'counters/examples': 103168, 'counters/updates': 3224}
train stats after 103200 examples: {'rewards_train/chosen': '0.11698', 'rewards_train/rejected': '0.00043666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11654', 'logps_train/rejected': '-99.718', 'logps_train/chosen': '-136.63', 'loss/train': '0.64598', 'examples_per_second': '33.141', 'grad_norm': '25.375', 'counters/examples': 103200, 'counters/updates': 3225}
train stats after 103232 examples: {'rewards_train/chosen': '0.035818', 'rewards_train/rejected': '-0.014885', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.050704', 'logps_train/rejected': '-106.4', 'logps_train/chosen': '-118.45', 'loss/train': '0.67284', 'examples_per_second': '32.79', 'grad_norm': '23.25', 'counters/examples': 103232, 'counters/updates': 3226}
train stats after 103264 examples: {'rewards_train/chosen': '0.18513', 'rewards_train/rejected': '0.098213', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086919', 'logps_train/rejected': '-146.63', 'logps_train/chosen': '-150.33', 'loss/train': '0.67197', 'examples_per_second': '30.64', 'grad_norm': '33.25', 'counters/examples': 103264, 'counters/updates': 3227}
train stats after 103296 examples: {'rewards_train/chosen': '0.11967', 'rewards_train/rejected': '0.09291', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026762', 'logps_train/rejected': '-132.51', 'logps_train/chosen': '-176.07', 'loss/train': '0.68539', 'examples_per_second': '30.301', 'grad_norm': '27.25', 'counters/examples': 103296, 'counters/updates': 3228}
train stats after 103328 examples: {'rewards_train/chosen': '0.11933', 'rewards_train/rejected': '0.062716', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056613', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-134.02', 'loss/train': '0.6744', 'examples_per_second': '31.425', 'grad_norm': '28.5', 'counters/examples': 103328, 'counters/updates': 3229}
train stats after 103360 examples: {'rewards_train/chosen': '0.10467', 'rewards_train/rejected': '0.034909', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069757', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-117.12', 'loss/train': '0.66534', 'examples_per_second': '31.013', 'grad_norm': '25', 'counters/examples': 103360, 'counters/updates': 3230}
train stats after 103392 examples: {'rewards_train/chosen': '-0.017046', 'rewards_train/rejected': '-0.010348', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0066984', 'logps_train/rejected': '-96.214', 'logps_train/chosen': '-153.03', 'loss/train': '0.70713', 'examples_per_second': '32.848', 'grad_norm': '27.5', 'counters/examples': 103392, 'counters/updates': 3231}
skipping logging after 103424 examples to avoid logging too frequently
train stats after 103456 examples: {'rewards_train/chosen': '0.076969', 'rewards_train/rejected': '0.071875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.005094', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-162.25', 'loss/train': '0.70207', 'examples_per_second': '33.971', 'grad_norm': '30.125', 'counters/examples': 103456, 'counters/updates': 3233}
train stats after 103488 examples: {'rewards_train/chosen': '0.15133', 'rewards_train/rejected': '0.0094517', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14187', 'logps_train/rejected': '-166.45', 'logps_train/chosen': '-119.63', 'loss/train': '0.64837', 'examples_per_second': '31.265', 'grad_norm': '32.25', 'counters/examples': 103488, 'counters/updates': 3234}
skipping logging after 103520 examples to avoid logging too frequently
train stats after 103552 examples: {'rewards_train/chosen': '0.062526', 'rewards_train/rejected': '0.011818', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050708', 'logps_train/rejected': '-145.42', 'logps_train/chosen': '-153.58', 'loss/train': '0.67651', 'examples_per_second': '32.978', 'grad_norm': '27.75', 'counters/examples': 103552, 'counters/updates': 3236}
train stats after 103584 examples: {'rewards_train/chosen': '0.11565', 'rewards_train/rejected': '0.095502', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020144', 'logps_train/rejected': '-104.42', 'logps_train/chosen': '-154.31', 'loss/train': '0.69711', 'examples_per_second': '31.908', 'grad_norm': '28.5', 'counters/examples': 103584, 'counters/updates': 3237}
skipping logging after 103616 examples to avoid logging too frequently
train stats after 103648 examples: {'rewards_train/chosen': '0.091109', 'rewards_train/rejected': '0.002179', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.08893', 'logps_train/rejected': '-89.372', 'logps_train/chosen': '-142.38', 'loss/train': '0.65252', 'examples_per_second': '31.618', 'grad_norm': '25.5', 'counters/examples': 103648, 'counters/updates': 3239}
train stats after 103680 examples: {'rewards_train/chosen': '0.11766', 'rewards_train/rejected': '0.02472', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092944', 'logps_train/rejected': '-143.58', 'logps_train/chosen': '-184.44', 'loss/train': '0.66455', 'examples_per_second': '30.187', 'grad_norm': '28.625', 'counters/examples': 103680, 'counters/updates': 3240}
train stats after 103712 examples: {'rewards_train/chosen': '0.10093', 'rewards_train/rejected': '0.084264', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.01667', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-102.32', 'loss/train': '0.69685', 'examples_per_second': '30.964', 'grad_norm': '25.125', 'counters/examples': 103712, 'counters/updates': 3241}
skipping logging after 103744 examples to avoid logging too frequently
train stats after 103776 examples: {'rewards_train/chosen': '0.12381', 'rewards_train/rejected': '0.010317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1135', 'logps_train/rejected': '-99.635', 'logps_train/chosen': '-124.12', 'loss/train': '0.64377', 'examples_per_second': '31.538', 'grad_norm': '24.75', 'counters/examples': 103776, 'counters/updates': 3243}
train stats after 103808 examples: {'rewards_train/chosen': '0.11669', 'rewards_train/rejected': '0.036564', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080128', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-150.57', 'loss/train': '0.66593', 'examples_per_second': '32.986', 'grad_norm': '28', 'counters/examples': 103808, 'counters/updates': 3244}
train stats after 103840 examples: {'rewards_train/chosen': '0.045058', 'rewards_train/rejected': '0.043187', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0018707', 'logps_train/rejected': '-103.71', 'logps_train/chosen': '-118.15', 'loss/train': '0.70001', 'examples_per_second': '30.968', 'grad_norm': '26.375', 'counters/examples': 103840, 'counters/updates': 3245}
train stats after 103872 examples: {'rewards_train/chosen': '0.055307', 'rewards_train/rejected': '0.0083125', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046994', 'logps_train/rejected': '-107.36', 'logps_train/chosen': '-92.837', 'loss/train': '0.67402', 'examples_per_second': '31.628', 'grad_norm': '22.625', 'counters/examples': 103872, 'counters/updates': 3246}
train stats after 103904 examples: {'rewards_train/chosen': '0.16018', 'rewards_train/rejected': '0.10499', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055187', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-142.93', 'loss/train': '0.67607', 'examples_per_second': '30.807', 'grad_norm': '25.875', 'counters/examples': 103904, 'counters/updates': 3247}
train stats after 103936 examples: {'rewards_train/chosen': '0.16781', 'rewards_train/rejected': '0.084455', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083353', 'logps_train/rejected': '-198.73', 'logps_train/chosen': '-188.69', 'loss/train': '0.66395', 'examples_per_second': '30.477', 'grad_norm': '28.875', 'counters/examples': 103936, 'counters/updates': 3248}
train stats after 103968 examples: {'rewards_train/chosen': '0.075066', 'rewards_train/rejected': '0.0075861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06748', 'logps_train/rejected': '-106.15', 'logps_train/chosen': '-126.52', 'loss/train': '0.66795', 'examples_per_second': '31.515', 'grad_norm': '24.5', 'counters/examples': 103968, 'counters/updates': 3249}
train stats after 104000 examples: {'rewards_train/chosen': '0.10519', 'rewards_train/rejected': '0.0099665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095228', 'logps_train/rejected': '-121.51', 'logps_train/chosen': '-136.41', 'loss/train': '0.65206', 'examples_per_second': '30.632', 'grad_norm': '31.75', 'counters/examples': 104000, 'counters/updates': 3250}
train stats after 104032 examples: {'rewards_train/chosen': '0.13568', 'rewards_train/rejected': '0.082269', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053407', 'logps_train/rejected': '-147.17', 'logps_train/chosen': '-184.18', 'loss/train': '0.67647', 'examples_per_second': '31.287', 'grad_norm': '31.625', 'counters/examples': 104032, 'counters/updates': 3251}
train stats after 104064 examples: {'rewards_train/chosen': '0.06706', 'rewards_train/rejected': '0.058387', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0086729', 'logps_train/rejected': '-138.22', 'logps_train/chosen': '-150.46', 'loss/train': '0.69646', 'examples_per_second': '31.003', 'grad_norm': '29.625', 'counters/examples': 104064, 'counters/updates': 3252}
skipping logging after 104096 examples to avoid logging too frequently
train stats after 104128 examples: {'rewards_train/chosen': '0.1179', 'rewards_train/rejected': '0.058961', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058934', 'logps_train/rejected': '-120.69', 'logps_train/chosen': '-170.22', 'loss/train': '0.67733', 'examples_per_second': '30.055', 'grad_norm': '26', 'counters/examples': 104128, 'counters/updates': 3254}
train stats after 104160 examples: {'rewards_train/chosen': '0.090246', 'rewards_train/rejected': '-0.014706', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10495', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-130.22', 'loss/train': '0.65762', 'examples_per_second': '32.967', 'grad_norm': '26.625', 'counters/examples': 104160, 'counters/updates': 3255}
train stats after 104192 examples: {'rewards_train/chosen': '0.07929', 'rewards_train/rejected': '-4.6355e-05', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079336', 'logps_train/rejected': '-143.81', 'logps_train/chosen': '-148.68', 'loss/train': '0.66394', 'examples_per_second': '32.26', 'grad_norm': '27.375', 'counters/examples': 104192, 'counters/updates': 3256}
train stats after 104224 examples: {'rewards_train/chosen': '0.037828', 'rewards_train/rejected': '0.015184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022644', 'logps_train/rejected': '-141.29', 'logps_train/chosen': '-117.88', 'loss/train': '0.69315', 'examples_per_second': '31.67', 'grad_norm': '28.625', 'counters/examples': 104224, 'counters/updates': 3257}
train stats after 104256 examples: {'rewards_train/chosen': '0.078799', 'rewards_train/rejected': '0.048081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030718', 'logps_train/rejected': '-124.15', 'logps_train/chosen': '-134.85', 'loss/train': '0.68309', 'examples_per_second': '30.106', 'grad_norm': '26.875', 'counters/examples': 104256, 'counters/updates': 3258}
skipping logging after 104288 examples to avoid logging too frequently
train stats after 104320 examples: {'rewards_train/chosen': '0.1226', 'rewards_train/rejected': '0.071851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050746', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-188.65', 'loss/train': '0.67344', 'examples_per_second': '31.669', 'grad_norm': '30.5', 'counters/examples': 104320, 'counters/updates': 3260}
train stats after 104352 examples: {'rewards_train/chosen': '0.07661', 'rewards_train/rejected': '-0.013808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090418', 'logps_train/rejected': '-132.41', 'logps_train/chosen': '-102.55', 'loss/train': '0.65676', 'examples_per_second': '31.324', 'grad_norm': '29.375', 'counters/examples': 104352, 'counters/updates': 3261}
train stats after 104384 examples: {'rewards_train/chosen': '0.048304', 'rewards_train/rejected': '0.002239', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046065', 'logps_train/rejected': '-105.55', 'logps_train/chosen': '-113.31', 'loss/train': '0.67876', 'examples_per_second': '31.691', 'grad_norm': '24.625', 'counters/examples': 104384, 'counters/updates': 3262}
train stats after 104416 examples: {'rewards_train/chosen': '0.040197', 'rewards_train/rejected': '0.08946', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.049263', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-127.99', 'loss/train': '0.72333', 'examples_per_second': '30.77', 'grad_norm': '27.5', 'counters/examples': 104416, 'counters/updates': 3263}
train stats after 104448 examples: {'rewards_train/chosen': '0.081391', 'rewards_train/rejected': '0.04895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032441', 'logps_train/rejected': '-157.85', 'logps_train/chosen': '-127.41', 'loss/train': '0.68354', 'examples_per_second': '30.889', 'grad_norm': '32', 'counters/examples': 104448, 'counters/updates': 3264}
skipping logging after 104480 examples to avoid logging too frequently
train stats after 104512 examples: {'rewards_train/chosen': '0.082989', 'rewards_train/rejected': '0.012714', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070275', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-132.05', 'loss/train': '0.66704', 'examples_per_second': '31.595', 'grad_norm': '26.125', 'counters/examples': 104512, 'counters/updates': 3266}
train stats after 104544 examples: {'rewards_train/chosen': '0.11802', 'rewards_train/rejected': '0.0015761', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11645', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-137.89', 'loss/train': '0.64826', 'examples_per_second': '31.668', 'grad_norm': '30', 'counters/examples': 104544, 'counters/updates': 3267}
train stats after 104576 examples: {'rewards_train/chosen': '0.12987', 'rewards_train/rejected': '0.016468', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1134', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-131.15', 'loss/train': '0.64533', 'examples_per_second': '31.691', 'grad_norm': '31.5', 'counters/examples': 104576, 'counters/updates': 3268}
train stats after 104608 examples: {'rewards_train/chosen': '0.13665', 'rewards_train/rejected': '0.057282', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079372', 'logps_train/rejected': '-127.88', 'logps_train/chosen': '-153.66', 'loss/train': '0.6639', 'examples_per_second': '31.625', 'grad_norm': '28.625', 'counters/examples': 104608, 'counters/updates': 3269}
train stats after 104640 examples: {'rewards_train/chosen': '0.062935', 'rewards_train/rejected': '0.0074963', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055439', 'logps_train/rejected': '-98.257', 'logps_train/chosen': '-118.87', 'loss/train': '0.66881', 'examples_per_second': '24.046', 'grad_norm': '27.25', 'counters/examples': 104640, 'counters/updates': 3270}
train stats after 104672 examples: {'rewards_train/chosen': '0.11427', 'rewards_train/rejected': '-0.0058178', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12009', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-157.68', 'loss/train': '0.64426', 'examples_per_second': '29.895', 'grad_norm': '27', 'counters/examples': 104672, 'counters/updates': 3271}
train stats after 104704 examples: {'rewards_train/chosen': '0.12228', 'rewards_train/rejected': '0.052485', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069794', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-147.24', 'loss/train': '0.66476', 'examples_per_second': '31.658', 'grad_norm': '26.25', 'counters/examples': 104704, 'counters/updates': 3272}
train stats after 104736 examples: {'rewards_train/chosen': '0.036397', 'rewards_train/rejected': '0.027629', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.008768', 'logps_train/rejected': '-113.51', 'logps_train/chosen': '-122.68', 'loss/train': '0.69568', 'examples_per_second': '31.463', 'grad_norm': '28.125', 'counters/examples': 104736, 'counters/updates': 3273}
train stats after 104768 examples: {'rewards_train/chosen': '0.039031', 'rewards_train/rejected': '0.053912', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01488', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-152.97', 'loss/train': '0.70925', 'examples_per_second': '32.208', 'grad_norm': '32.75', 'counters/examples': 104768, 'counters/updates': 3274}
train stats after 104800 examples: {'rewards_train/chosen': '0.1186', 'rewards_train/rejected': '0.040106', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.07849', 'logps_train/rejected': '-108.47', 'logps_train/chosen': '-123.46', 'loss/train': '0.66484', 'examples_per_second': '31.668', 'grad_norm': '24.25', 'counters/examples': 104800, 'counters/updates': 3275}
train stats after 104832 examples: {'rewards_train/chosen': '0.082001', 'rewards_train/rejected': '0.060829', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021172', 'logps_train/rejected': '-105.8', 'logps_train/chosen': '-150.23', 'loss/train': '0.68801', 'examples_per_second': '31.408', 'grad_norm': '27.5', 'counters/examples': 104832, 'counters/updates': 3276}
skipping logging after 104864 examples to avoid logging too frequently
train stats after 104896 examples: {'rewards_train/chosen': '0.093098', 'rewards_train/rejected': '0.059018', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03408', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-138.16', 'loss/train': '0.68084', 'examples_per_second': '33.558', 'grad_norm': '26.875', 'counters/examples': 104896, 'counters/updates': 3278}
train stats after 104928 examples: {'rewards_train/chosen': '0.079402', 'rewards_train/rejected': '0.033793', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045609', 'logps_train/rejected': '-107.81', 'logps_train/chosen': '-140.76', 'loss/train': '0.68172', 'examples_per_second': '31.652', 'grad_norm': '25.75', 'counters/examples': 104928, 'counters/updates': 3279}
train stats after 104960 examples: {'rewards_train/chosen': '0.14226', 'rewards_train/rejected': '0.030409', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11185', 'logps_train/rejected': '-110.22', 'logps_train/chosen': '-156.51', 'loss/train': '0.6484', 'examples_per_second': '31.646', 'grad_norm': '26.375', 'counters/examples': 104960, 'counters/updates': 3280}
train stats after 104992 examples: {'rewards_train/chosen': '0.027789', 'rewards_train/rejected': '0.0064003', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021389', 'logps_train/rejected': '-151.42', 'logps_train/chosen': '-135.77', 'loss/train': '0.69056', 'examples_per_second': '33.112', 'grad_norm': '31', 'counters/examples': 104992, 'counters/updates': 3281}
train stats after 105024 examples: {'rewards_train/chosen': '0.18382', 'rewards_train/rejected': '0.02214', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16168', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-138.42', 'loss/train': '0.63525', 'examples_per_second': '30.364', 'grad_norm': '25', 'counters/examples': 105024, 'counters/updates': 3282}
train stats after 105056 examples: {'rewards_train/chosen': '0.10958', 'rewards_train/rejected': '0.0016527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10793', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-148.22', 'loss/train': '0.64894', 'examples_per_second': '30.151', 'grad_norm': '35.5', 'counters/examples': 105056, 'counters/updates': 3283}
train stats after 105088 examples: {'rewards_train/chosen': '0.082083', 'rewards_train/rejected': '0.11544', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033357', 'logps_train/rejected': '-149.83', 'logps_train/chosen': '-142.87', 'loss/train': '0.7212', 'examples_per_second': '31.667', 'grad_norm': '35', 'counters/examples': 105088, 'counters/updates': 3284}
train stats after 105120 examples: {'rewards_train/chosen': '0.11952', 'rewards_train/rejected': '0.060303', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.059217', 'logps_train/rejected': '-136.92', 'logps_train/chosen': '-133.16', 'loss/train': '0.67238', 'examples_per_second': '31.711', 'grad_norm': '36.5', 'counters/examples': 105120, 'counters/updates': 3285}
train stats after 105152 examples: {'rewards_train/chosen': '0.096436', 'rewards_train/rejected': '0.063696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03274', 'logps_train/rejected': '-132.66', 'logps_train/chosen': '-120.87', 'loss/train': '0.68059', 'examples_per_second': '30.569', 'grad_norm': '28.125', 'counters/examples': 105152, 'counters/updates': 3286}
train stats after 105184 examples: {'rewards_train/chosen': '0.095072', 'rewards_train/rejected': '0.076965', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018107', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-148.69', 'loss/train': '0.69305', 'examples_per_second': '31.695', 'grad_norm': '51.75', 'counters/examples': 105184, 'counters/updates': 3287}
train stats after 105216 examples: {'rewards_train/chosen': '0.10448', 'rewards_train/rejected': '0.084211', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020272', 'logps_train/rejected': '-140.99', 'logps_train/chosen': '-112.74', 'loss/train': '0.69083', 'examples_per_second': '33.068', 'grad_norm': '28', 'counters/examples': 105216, 'counters/updates': 3288}
train stats after 105248 examples: {'rewards_train/chosen': '0.020943', 'rewards_train/rejected': '-0.00018288', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021125', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-103.5', 'loss/train': '0.69067', 'examples_per_second': '31.1', 'grad_norm': '22.5', 'counters/examples': 105248, 'counters/updates': 3289}
train stats after 105280 examples: {'rewards_train/chosen': '0.077343', 'rewards_train/rejected': '0.012336', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065008', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-126.86', 'loss/train': '0.66888', 'examples_per_second': '31.656', 'grad_norm': '27.375', 'counters/examples': 105280, 'counters/updates': 3290}
train stats after 105312 examples: {'rewards_train/chosen': '0.14431', 'rewards_train/rejected': '0.075786', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068523', 'logps_train/rejected': '-155.66', 'logps_train/chosen': '-142.41', 'loss/train': '0.68057', 'examples_per_second': '31.651', 'grad_norm': '30.875', 'counters/examples': 105312, 'counters/updates': 3291}
train stats after 105344 examples: {'rewards_train/chosen': '0.13623', 'rewards_train/rejected': '0.031657', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10458', 'logps_train/rejected': '-105.54', 'logps_train/chosen': '-144.2', 'loss/train': '0.64968', 'examples_per_second': '30.825', 'grad_norm': '25.125', 'counters/examples': 105344, 'counters/updates': 3292}
train stats after 105376 examples: {'rewards_train/chosen': '0.039915', 'rewards_train/rejected': '0.062981', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023066', 'logps_train/rejected': '-156.18', 'logps_train/chosen': '-133.62', 'loss/train': '0.7129', 'examples_per_second': '32.322', 'grad_norm': '27.875', 'counters/examples': 105376, 'counters/updates': 3293}
skipping logging after 105408 examples to avoid logging too frequently
train stats after 105440 examples: {'rewards_train/chosen': '0.063264', 'rewards_train/rejected': '0.022436', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040827', 'logps_train/rejected': '-103.13', 'logps_train/chosen': '-124.13', 'loss/train': '0.67955', 'examples_per_second': '33.475', 'grad_norm': '24.25', 'counters/examples': 105440, 'counters/updates': 3295}
skipping logging after 105472 examples to avoid logging too frequently
train stats after 105504 examples: {'rewards_train/chosen': '0.076861', 'rewards_train/rejected': '0.0093871', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067474', 'logps_train/rejected': '-86.881', 'logps_train/chosen': '-124.34', 'loss/train': '0.66896', 'examples_per_second': '31.985', 'grad_norm': '37.5', 'counters/examples': 105504, 'counters/updates': 3297}
skipping logging after 105536 examples to avoid logging too frequently
train stats after 105568 examples: {'rewards_train/chosen': '0.048093', 'rewards_train/rejected': '-0.0012824', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049376', 'logps_train/rejected': '-104.89', 'logps_train/chosen': '-130.22', 'loss/train': '0.6776', 'examples_per_second': '31.973', 'grad_norm': '23.125', 'counters/examples': 105568, 'counters/updates': 3299}
train stats after 105600 examples: {'rewards_train/chosen': '0.15711', 'rewards_train/rejected': '0.079642', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077465', 'logps_train/rejected': '-98.31', 'logps_train/chosen': '-136.33', 'loss/train': '0.66399', 'examples_per_second': '31.681', 'grad_norm': '25.75', 'counters/examples': 105600, 'counters/updates': 3300}
train stats after 105632 examples: {'rewards_train/chosen': '0.10418', 'rewards_train/rejected': '0.04004', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064143', 'logps_train/rejected': '-94.772', 'logps_train/chosen': '-144.9', 'loss/train': '0.66881', 'examples_per_second': '32.24', 'grad_norm': '28.875', 'counters/examples': 105632, 'counters/updates': 3301}
train stats after 105664 examples: {'rewards_train/chosen': '0.052356', 'rewards_train/rejected': '0.0052277', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047128', 'logps_train/rejected': '-94.367', 'logps_train/chosen': '-103.6', 'loss/train': '0.67418', 'examples_per_second': '31.639', 'grad_norm': '25.375', 'counters/examples': 105664, 'counters/updates': 3302}
train stats after 105696 examples: {'rewards_train/chosen': '0.097885', 'rewards_train/rejected': '0.01313', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084754', 'logps_train/rejected': '-99.029', 'logps_train/chosen': '-131.07', 'loss/train': '0.66019', 'examples_per_second': '31.224', 'grad_norm': '23', 'counters/examples': 105696, 'counters/updates': 3303}
train stats after 105728 examples: {'rewards_train/chosen': '0.13459', 'rewards_train/rejected': '0.053432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081158', 'logps_train/rejected': '-97.03', 'logps_train/chosen': '-148.97', 'loss/train': '0.65889', 'examples_per_second': '31.467', 'grad_norm': '24.375', 'counters/examples': 105728, 'counters/updates': 3304}
train stats after 105760 examples: {'rewards_train/chosen': '0.11049', 'rewards_train/rejected': '0.056255', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054235', 'logps_train/rejected': '-114.34', 'logps_train/chosen': '-166.91', 'loss/train': '0.67636', 'examples_per_second': '30.573', 'grad_norm': '27.125', 'counters/examples': 105760, 'counters/updates': 3305}
train stats after 105792 examples: {'rewards_train/chosen': '0.064707', 'rewards_train/rejected': '0.065551', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00084428', 'logps_train/rejected': '-132.11', 'logps_train/chosen': '-198.54', 'loss/train': '0.69903', 'examples_per_second': '30.675', 'grad_norm': '30.25', 'counters/examples': 105792, 'counters/updates': 3306}
train stats after 105824 examples: {'rewards_train/chosen': '0.069401', 'rewards_train/rejected': '0.038692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030709', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-140.19', 'loss/train': '0.68373', 'examples_per_second': '30.293', 'grad_norm': '25.5', 'counters/examples': 105824, 'counters/updates': 3307}
train stats after 105856 examples: {'rewards_train/chosen': '0.041033', 'rewards_train/rejected': '0.048454', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0074209', 'logps_train/rejected': '-113.18', 'logps_train/chosen': '-159.74', 'loss/train': '0.7034', 'examples_per_second': '29.952', 'grad_norm': '31', 'counters/examples': 105856, 'counters/updates': 3308}
train stats after 105888 examples: {'rewards_train/chosen': '0.075421', 'rewards_train/rejected': '0.05322', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0222', 'logps_train/rejected': '-142.6', 'logps_train/chosen': '-136.26', 'loss/train': '0.69044', 'examples_per_second': '31.551', 'grad_norm': '28.5', 'counters/examples': 105888, 'counters/updates': 3309}
train stats after 105920 examples: {'rewards_train/chosen': '0.12737', 'rewards_train/rejected': '0.068656', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058716', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-164.84', 'loss/train': '0.67138', 'examples_per_second': '30.676', 'grad_norm': '27.875', 'counters/examples': 105920, 'counters/updates': 3310}
train stats after 105952 examples: {'rewards_train/chosen': '0.12351', 'rewards_train/rejected': '0.084371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039142', 'logps_train/rejected': '-139.94', 'logps_train/chosen': '-153.58', 'loss/train': '0.68102', 'examples_per_second': '31.467', 'grad_norm': '27', 'counters/examples': 105952, 'counters/updates': 3311}
skipping logging after 105984 examples to avoid logging too frequently
train stats after 106016 examples: {'rewards_train/chosen': '0.13031', 'rewards_train/rejected': '-0.040276', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17058', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-185.93', 'loss/train': '0.62365', 'examples_per_second': '31.685', 'grad_norm': '27.125', 'counters/examples': 106016, 'counters/updates': 3313}
train stats after 106048 examples: {'rewards_train/chosen': '0.15882', 'rewards_train/rejected': '-0.0099421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16876', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-148.09', 'loss/train': '0.62187', 'examples_per_second': '30.342', 'grad_norm': '25', 'counters/examples': 106048, 'counters/updates': 3314}
train stats after 106080 examples: {'rewards_train/chosen': '0.049386', 'rewards_train/rejected': '0.034968', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.014418', 'logps_train/rejected': '-107.74', 'logps_train/chosen': '-138.57', 'loss/train': '0.69286', 'examples_per_second': '31.433', 'grad_norm': '36.25', 'counters/examples': 106080, 'counters/updates': 3315}
skipping logging after 106112 examples to avoid logging too frequently
train stats after 106144 examples: {'rewards_train/chosen': '0.1162', 'rewards_train/rejected': '0.033641', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082562', 'logps_train/rejected': '-122.87', 'logps_train/chosen': '-159.07', 'loss/train': '0.65905', 'examples_per_second': '31.983', 'grad_norm': '36.75', 'counters/examples': 106144, 'counters/updates': 3317}
skipping logging after 106176 examples to avoid logging too frequently
train stats after 106208 examples: {'rewards_train/chosen': '0.14916', 'rewards_train/rejected': '0.040088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10908', 'logps_train/rejected': '-93.913', 'logps_train/chosen': '-121.14', 'loss/train': '0.64843', 'examples_per_second': '30.58', 'grad_norm': '22', 'counters/examples': 106208, 'counters/updates': 3319}
train stats after 106240 examples: {'rewards_train/chosen': '0.11485', 'rewards_train/rejected': '0.049801', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065045', 'logps_train/rejected': '-96.133', 'logps_train/chosen': '-131.28', 'loss/train': '0.66784', 'examples_per_second': '31.192', 'grad_norm': '25.125', 'counters/examples': 106240, 'counters/updates': 3320}
train stats after 106272 examples: {'rewards_train/chosen': '0.078829', 'rewards_train/rejected': '0.13015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.051323', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-131.19', 'loss/train': '0.73261', 'examples_per_second': '32.039', 'grad_norm': '41.25', 'counters/examples': 106272, 'counters/updates': 3321}
skipping logging after 106304 examples to avoid logging too frequently
train stats after 106336 examples: {'rewards_train/chosen': '0.029945', 'rewards_train/rejected': '0.031183', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0012379', 'logps_train/rejected': '-105.47', 'logps_train/chosen': '-116.86', 'loss/train': '0.69836', 'examples_per_second': '33.054', 'grad_norm': '27.75', 'counters/examples': 106336, 'counters/updates': 3323}
train stats after 106368 examples: {'rewards_train/chosen': '0.13415', 'rewards_train/rejected': '0.031593', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10256', 'logps_train/rejected': '-107.71', 'logps_train/chosen': '-143.89', 'loss/train': '0.64959', 'examples_per_second': '31.414', 'grad_norm': '25', 'counters/examples': 106368, 'counters/updates': 3324}
train stats after 106400 examples: {'rewards_train/chosen': '0.099755', 'rewards_train/rejected': '0.036484', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063271', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-135.04', 'loss/train': '0.66704', 'examples_per_second': '30.128', 'grad_norm': '26.25', 'counters/examples': 106400, 'counters/updates': 3325}
skipping logging after 106432 examples to avoid logging too frequently
train stats after 106464 examples: {'rewards_train/chosen': '0.06535', 'rewards_train/rejected': '0.045678', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.019672', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-164.18', 'loss/train': '0.69325', 'examples_per_second': '29.976', 'grad_norm': '37.5', 'counters/examples': 106464, 'counters/updates': 3327}
train stats after 106496 examples: {'rewards_train/chosen': '0.094803', 'rewards_train/rejected': '0.011055', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083749', 'logps_train/rejected': '-140.43', 'logps_train/chosen': '-130.77', 'loss/train': '0.66177', 'examples_per_second': '31.531', 'grad_norm': '30.875', 'counters/examples': 106496, 'counters/updates': 3328}
train stats after 106528 examples: {'rewards_train/chosen': '0.11243', 'rewards_train/rejected': '0.06911', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043322', 'logps_train/rejected': '-159.75', 'logps_train/chosen': '-179.62', 'loss/train': '0.6822', 'examples_per_second': '31.644', 'grad_norm': '29.375', 'counters/examples': 106528, 'counters/updates': 3329}
train stats after 106560 examples: {'rewards_train/chosen': '0.11382', 'rewards_train/rejected': '0.020893', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092928', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-161.49', 'loss/train': '0.65852', 'examples_per_second': '31.113', 'grad_norm': '31.125', 'counters/examples': 106560, 'counters/updates': 3330}
train stats after 106592 examples: {'rewards_train/chosen': '0.043287', 'rewards_train/rejected': '0.026098', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017189', 'logps_train/rejected': '-114.43', 'logps_train/chosen': '-121.86', 'loss/train': '0.69339', 'examples_per_second': '33.264', 'grad_norm': '28.875', 'counters/examples': 106592, 'counters/updates': 3331}
train stats after 106624 examples: {'rewards_train/chosen': '0.14924', 'rewards_train/rejected': '0.033287', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11595', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-114.17', 'loss/train': '0.64372', 'examples_per_second': '31.698', 'grad_norm': '22.875', 'counters/examples': 106624, 'counters/updates': 3332}
train stats after 106656 examples: {'rewards_train/chosen': '0.043362', 'rewards_train/rejected': '0.013856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029506', 'logps_train/rejected': '-124.59', 'logps_train/chosen': '-126.46', 'loss/train': '0.68555', 'examples_per_second': '32.169', 'grad_norm': '27.25', 'counters/examples': 106656, 'counters/updates': 3333}
train stats after 106688 examples: {'rewards_train/chosen': '0.051716', 'rewards_train/rejected': '-0.01462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066336', 'logps_train/rejected': '-113.3', 'logps_train/chosen': '-140.46', 'loss/train': '0.66677', 'examples_per_second': '31.671', 'grad_norm': '24.125', 'counters/examples': 106688, 'counters/updates': 3334}
train stats after 106720 examples: {'rewards_train/chosen': '0.11718', 'rewards_train/rejected': '0.029057', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088121', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-134.23', 'loss/train': '0.65869', 'examples_per_second': '30.161', 'grad_norm': '23.125', 'counters/examples': 106720, 'counters/updates': 3335}
train stats after 106752 examples: {'rewards_train/chosen': '0.08333', 'rewards_train/rejected': '0.030588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052742', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-136.4', 'loss/train': '0.67761', 'examples_per_second': '31.538', 'grad_norm': '25.5', 'counters/examples': 106752, 'counters/updates': 3336}
skipping logging after 106784 examples to avoid logging too frequently
train stats after 106816 examples: {'rewards_train/chosen': '0.020873', 'rewards_train/rejected': '0.02133', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00045753', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-104.06', 'loss/train': '0.69774', 'examples_per_second': '31.652', 'grad_norm': '30.75', 'counters/examples': 106816, 'counters/updates': 3338}
train stats after 106848 examples: {'rewards_train/chosen': '0.090165', 'rewards_train/rejected': '0.10972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019553', 'logps_train/rejected': '-165.36', 'logps_train/chosen': '-137.11', 'loss/train': '0.71054', 'examples_per_second': '31.68', 'grad_norm': '29.75', 'counters/examples': 106848, 'counters/updates': 3339}
train stats after 106880 examples: {'rewards_train/chosen': '0.18624', 'rewards_train/rejected': '0.036341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1499', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-160.5', 'loss/train': '0.6328', 'examples_per_second': '31.665', 'grad_norm': '25.125', 'counters/examples': 106880, 'counters/updates': 3340}
train stats after 106912 examples: {'rewards_train/chosen': '0.13923', 'rewards_train/rejected': '0.0048129', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13442', 'logps_train/rejected': '-108.08', 'logps_train/chosen': '-146.4', 'loss/train': '0.63409', 'examples_per_second': '30.102', 'grad_norm': '24.75', 'counters/examples': 106912, 'counters/updates': 3341}
train stats after 106944 examples: {'rewards_train/chosen': '0.072299', 'rewards_train/rejected': '0.050859', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02144', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-134.69', 'loss/train': '0.70627', 'examples_per_second': '31.689', 'grad_norm': '37.25', 'counters/examples': 106944, 'counters/updates': 3342}
train stats after 106976 examples: {'rewards_train/chosen': '0.040311', 'rewards_train/rejected': '0.001005', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039306', 'logps_train/rejected': '-87.764', 'logps_train/chosen': '-125.2', 'loss/train': '0.68267', 'examples_per_second': '31.84', 'grad_norm': '23.5', 'counters/examples': 106976, 'counters/updates': 3343}
train stats after 107008 examples: {'rewards_train/chosen': '0.067615', 'rewards_train/rejected': '-0.0072122', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074827', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-165.67', 'loss/train': '0.66307', 'examples_per_second': '31.971', 'grad_norm': '28.5', 'counters/examples': 107008, 'counters/updates': 3344}
train stats after 107040 examples: {'rewards_train/chosen': '0.064873', 'rewards_train/rejected': '0.01205', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052823', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-130.51', 'loss/train': '0.67206', 'examples_per_second': '30.273', 'grad_norm': '24.375', 'counters/examples': 107040, 'counters/updates': 3345}
skipping logging after 107072 examples to avoid logging too frequently
train stats after 107104 examples: {'rewards_train/chosen': '0.048108', 'rewards_train/rejected': '-0.011914', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060022', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-120.81', 'loss/train': '0.67332', 'examples_per_second': '32.54', 'grad_norm': '27.375', 'counters/examples': 107104, 'counters/updates': 3347}
train stats after 107136 examples: {'rewards_train/chosen': '0.10422', 'rewards_train/rejected': '0.075092', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029129', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-130.63', 'loss/train': '0.69024', 'examples_per_second': '31.676', 'grad_norm': '27.375', 'counters/examples': 107136, 'counters/updates': 3348}
train stats after 107168 examples: {'rewards_train/chosen': '0.11757', 'rewards_train/rejected': '0.064752', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052814', 'logps_train/rejected': '-112.31', 'logps_train/chosen': '-161.96', 'loss/train': '0.67249', 'examples_per_second': '32.557', 'grad_norm': '28.625', 'counters/examples': 107168, 'counters/updates': 3349}
train stats after 107200 examples: {'rewards_train/chosen': '0.08418', 'rewards_train/rejected': '0.025081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059099', 'logps_train/rejected': '-115.05', 'logps_train/chosen': '-106.65', 'loss/train': '0.67162', 'examples_per_second': '31.72', 'grad_norm': '22.625', 'counters/examples': 107200, 'counters/updates': 3350}
skipping logging after 107232 examples to avoid logging too frequently
train stats after 107264 examples: {'rewards_train/chosen': '0.075212', 'rewards_train/rejected': '-0.012358', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08757', 'logps_train/rejected': '-113.57', 'logps_train/chosen': '-122.78', 'loss/train': '0.66045', 'examples_per_second': '31.659', 'grad_norm': '23.75', 'counters/examples': 107264, 'counters/updates': 3352}
train stats after 107296 examples: {'rewards_train/chosen': '0.07975', 'rewards_train/rejected': '0.010134', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069616', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-95.939', 'loss/train': '0.6657', 'examples_per_second': '29.96', 'grad_norm': '28.875', 'counters/examples': 107296, 'counters/updates': 3353}
train stats after 107328 examples: {'rewards_train/chosen': '0.047688', 'rewards_train/rejected': '0.021381', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026306', 'logps_train/rejected': '-98.814', 'logps_train/chosen': '-126.5', 'loss/train': '0.68585', 'examples_per_second': '31.06', 'grad_norm': '23.125', 'counters/examples': 107328, 'counters/updates': 3354}
skipping logging after 107360 examples to avoid logging too frequently
train stats after 107392 examples: {'rewards_train/chosen': '0.089067', 'rewards_train/rejected': '0.099574', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010508', 'logps_train/rejected': '-143.81', 'logps_train/chosen': '-146.17', 'loss/train': '0.70696', 'examples_per_second': '31.613', 'grad_norm': '33', 'counters/examples': 107392, 'counters/updates': 3356}
train stats after 107424 examples: {'rewards_train/chosen': '0.11643', 'rewards_train/rejected': '0.051005', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065422', 'logps_train/rejected': '-151.85', 'logps_train/chosen': '-175.8', 'loss/train': '0.66911', 'examples_per_second': '31.652', 'grad_norm': '31.375', 'counters/examples': 107424, 'counters/updates': 3357}
train stats after 107456 examples: {'rewards_train/chosen': '0.1015', 'rewards_train/rejected': '0.086112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.015383', 'logps_train/rejected': '-116.67', 'logps_train/chosen': '-139.68', 'loss/train': '0.69776', 'examples_per_second': '31.329', 'grad_norm': '26.5', 'counters/examples': 107456, 'counters/updates': 3358}
train stats after 107488 examples: {'rewards_train/chosen': '0.047903', 'rewards_train/rejected': '0.050952', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.003049', 'logps_train/rejected': '-148.66', 'logps_train/chosen': '-122.48', 'loss/train': '0.70373', 'examples_per_second': '31.038', 'grad_norm': '31.75', 'counters/examples': 107488, 'counters/updates': 3359}
train stats after 107520 examples: {'rewards_train/chosen': '0.13588', 'rewards_train/rejected': '0.073649', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062233', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-160.44', 'loss/train': '0.67043', 'examples_per_second': '31.645', 'grad_norm': '25.625', 'counters/examples': 107520, 'counters/updates': 3360}
train stats after 107552 examples: {'rewards_train/chosen': '0.10001', 'rewards_train/rejected': '0.031149', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068865', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-166', 'loss/train': '0.66813', 'examples_per_second': '31.638', 'grad_norm': '47', 'counters/examples': 107552, 'counters/updates': 3361}
skipping logging after 107584 examples to avoid logging too frequently
train stats after 107616 examples: {'rewards_train/chosen': '0.11372', 'rewards_train/rejected': '0.064277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049441', 'logps_train/rejected': '-144.01', 'logps_train/chosen': '-148.08', 'loss/train': '0.67662', 'examples_per_second': '31.677', 'grad_norm': '28.375', 'counters/examples': 107616, 'counters/updates': 3363}
skipping logging after 107648 examples to avoid logging too frequently
train stats after 107680 examples: {'rewards_train/chosen': '0.046695', 'rewards_train/rejected': '0.043702', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0029933', 'logps_train/rejected': '-135.33', 'logps_train/chosen': '-115.35', 'loss/train': '0.69826', 'examples_per_second': '31.867', 'grad_norm': '31.375', 'counters/examples': 107680, 'counters/updates': 3365}
train stats after 107712 examples: {'rewards_train/chosen': '0.16015', 'rewards_train/rejected': '0.075441', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084713', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-130.44', 'loss/train': '0.66342', 'examples_per_second': '31.18', 'grad_norm': '29.375', 'counters/examples': 107712, 'counters/updates': 3366}
train stats after 107744 examples: {'rewards_train/chosen': '0.14751', 'rewards_train/rejected': '0.070341', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07717', 'logps_train/rejected': '-157.55', 'logps_train/chosen': '-109.75', 'loss/train': '0.66771', 'examples_per_second': '31.651', 'grad_norm': '32.75', 'counters/examples': 107744, 'counters/updates': 3367}
train stats after 107776 examples: {'rewards_train/chosen': '0.07762', 'rewards_train/rejected': '-0.0032903', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08091', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-139.7', 'loss/train': '0.66293', 'examples_per_second': '32.731', 'grad_norm': '25.125', 'counters/examples': 107776, 'counters/updates': 3368}
skipping logging after 107808 examples to avoid logging too frequently
train stats after 107840 examples: {'rewards_train/chosen': '0.10927', 'rewards_train/rejected': '0.072169', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037105', 'logps_train/rejected': '-141.82', 'logps_train/chosen': '-112.85', 'loss/train': '0.68403', 'examples_per_second': '31.624', 'grad_norm': '26', 'counters/examples': 107840, 'counters/updates': 3370}
skipping logging after 107872 examples to avoid logging too frequently
train stats after 107904 examples: {'rewards_train/chosen': '0.095096', 'rewards_train/rejected': '0.032808', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062288', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-138.42', 'loss/train': '0.66652', 'examples_per_second': '31.642', 'grad_norm': '25.875', 'counters/examples': 107904, 'counters/updates': 3372}
train stats after 107936 examples: {'rewards_train/chosen': '0.070657', 'rewards_train/rejected': '-0.0063283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076985', 'logps_train/rejected': '-98.696', 'logps_train/chosen': '-123.22', 'loss/train': '0.66224', 'examples_per_second': '32.325', 'grad_norm': '21.375', 'counters/examples': 107936, 'counters/updates': 3373}
skipping logging after 107968 examples to avoid logging too frequently
train stats after 108000 examples: {'rewards_train/chosen': '0.15262', 'rewards_train/rejected': '0.099678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.052938', 'logps_train/rejected': '-142.88', 'logps_train/chosen': '-143.86', 'loss/train': '0.67421', 'examples_per_second': '32.015', 'grad_norm': '30.375', 'counters/examples': 108000, 'counters/updates': 3375}
Running evaluation after 108000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.26it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 108000: {'rewards_eval/chosen': '0.1126', 'rewards_eval/rejected': '0.028389', 'rewards_eval/accuracies': '0.60938', 'rewards_eval/margins': '0.084209', 'logps_eval/rejected': '-118.33', 'logps_eval/chosen': '-138.31', 'loss/eval': '0.66087'}
skipping save for non epoch
train stats after 108032 examples: {'rewards_train/chosen': '0.009753', 'rewards_train/rejected': '0.0050278', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0047252', 'logps_train/rejected': '-112.69', 'logps_train/chosen': '-117.88', 'loss/train': '0.69922', 'examples_per_second': '33.99', 'grad_norm': '25.125', 'counters/examples': 108032, 'counters/updates': 3376}
skipping logging after 108064 examples to avoid logging too frequently
train stats after 108096 examples: {'rewards_train/chosen': '0.13722', 'rewards_train/rejected': '-0.029766', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.16699', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-129.04', 'loss/train': '0.61929', 'examples_per_second': '30.704', 'grad_norm': '24.375', 'counters/examples': 108096, 'counters/updates': 3378}
skipping logging after 108128 examples to avoid logging too frequently
train stats after 108160 examples: {'rewards_train/chosen': '0.13734', 'rewards_train/rejected': '0.0040827', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13325', 'logps_train/rejected': '-88.167', 'logps_train/chosen': '-145.49', 'loss/train': '0.63436', 'examples_per_second': '26.179', 'grad_norm': '22.125', 'counters/examples': 108160, 'counters/updates': 3380}
train stats after 108192 examples: {'rewards_train/chosen': '0.087696', 'rewards_train/rejected': '0.059195', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028501', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-151.72', 'loss/train': '0.69237', 'examples_per_second': '32.609', 'grad_norm': '30.875', 'counters/examples': 108192, 'counters/updates': 3381}
train stats after 108224 examples: {'rewards_train/chosen': '0.19485', 'rewards_train/rejected': '0.022194', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17266', 'logps_train/rejected': '-179.53', 'logps_train/chosen': '-150.4', 'loss/train': '0.61947', 'examples_per_second': '31.595', 'grad_norm': '28.875', 'counters/examples': 108224, 'counters/updates': 3382}
train stats after 108256 examples: {'rewards_train/chosen': '0.1248', 'rewards_train/rejected': '0.044861', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07994', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-140.58', 'loss/train': '0.66371', 'examples_per_second': '24.218', 'grad_norm': '31.5', 'counters/examples': 108256, 'counters/updates': 3383}
skipping logging after 108288 examples to avoid logging too frequently
train stats after 108320 examples: {'rewards_train/chosen': '0.07866', 'rewards_train/rejected': '0.038859', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039802', 'logps_train/rejected': '-107.53', 'logps_train/chosen': '-118.94', 'loss/train': '0.6783', 'examples_per_second': '31.394', 'grad_norm': '29.25', 'counters/examples': 108320, 'counters/updates': 3385}
skipping logging after 108352 examples to avoid logging too frequently
train stats after 108384 examples: {'rewards_train/chosen': '0.11701', 'rewards_train/rejected': '0.069095', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04791', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-141.65', 'loss/train': '0.68227', 'examples_per_second': '30.169', 'grad_norm': '24.625', 'counters/examples': 108384, 'counters/updates': 3387}
skipping logging after 108416 examples to avoid logging too frequently
train stats after 108448 examples: {'rewards_train/chosen': '0.12496', 'rewards_train/rejected': '0.027171', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097784', 'logps_train/rejected': '-145.83', 'logps_train/chosen': '-177.71', 'loss/train': '0.65727', 'examples_per_second': '30.155', 'grad_norm': '29', 'counters/examples': 108448, 'counters/updates': 3389}
train stats after 108480 examples: {'rewards_train/chosen': '0.10668', 'rewards_train/rejected': '0.025229', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081447', 'logps_train/rejected': '-129.07', 'logps_train/chosen': '-131.15', 'loss/train': '0.66038', 'examples_per_second': '30.984', 'grad_norm': '24.875', 'counters/examples': 108480, 'counters/updates': 3390}
train stats after 108512 examples: {'rewards_train/chosen': '0.10686', 'rewards_train/rejected': '0.12148', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014627', 'logps_train/rejected': '-154.93', 'logps_train/chosen': '-163.3', 'loss/train': '0.71177', 'examples_per_second': '31.069', 'grad_norm': '37.75', 'counters/examples': 108512, 'counters/updates': 3391}
skipping logging after 108544 examples to avoid logging too frequently
train stats after 108576 examples: {'rewards_train/chosen': '0.13553', 'rewards_train/rejected': '0.058339', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.077189', 'logps_train/rejected': '-149.88', 'logps_train/chosen': '-169.02', 'loss/train': '0.66225', 'examples_per_second': '31.559', 'grad_norm': '29.875', 'counters/examples': 108576, 'counters/updates': 3393}
train stats after 108608 examples: {'rewards_train/chosen': '0.095917', 'rewards_train/rejected': '0.085675', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010242', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-129.41', 'loss/train': '0.69279', 'examples_per_second': '30.265', 'grad_norm': '27.75', 'counters/examples': 108608, 'counters/updates': 3394}
skipping logging after 108640 examples to avoid logging too frequently
train stats after 108672 examples: {'rewards_train/chosen': '0.086905', 'rewards_train/rejected': '0.0063054', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0806', 'logps_train/rejected': '-123.27', 'logps_train/chosen': '-131.26', 'loss/train': '0.65851', 'examples_per_second': '32.133', 'grad_norm': '26.125', 'counters/examples': 108672, 'counters/updates': 3396}
train stats after 108704 examples: {'rewards_train/chosen': '0.15815', 'rewards_train/rejected': '0.11242', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045724', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-135.06', 'loss/train': '0.68693', 'examples_per_second': '31.158', 'grad_norm': '25.875', 'counters/examples': 108704, 'counters/updates': 3397}
train stats after 108736 examples: {'rewards_train/chosen': '0.13687', 'rewards_train/rejected': '0.14526', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0083907', 'logps_train/rejected': '-138.04', 'logps_train/chosen': '-159.58', 'loss/train': '0.71504', 'examples_per_second': '30.464', 'grad_norm': '30', 'counters/examples': 108736, 'counters/updates': 3398}
train stats after 108768 examples: {'rewards_train/chosen': '0.14085', 'rewards_train/rejected': '0.042732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098116', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-149.7', 'loss/train': '0.65299', 'examples_per_second': '30.261', 'grad_norm': '28.125', 'counters/examples': 108768, 'counters/updates': 3399}
train stats after 108800 examples: {'rewards_train/chosen': '0.038217', 'rewards_train/rejected': '-0.0046023', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042819', 'logps_train/rejected': '-83.871', 'logps_train/chosen': '-105.35', 'loss/train': '0.67744', 'examples_per_second': '30.704', 'grad_norm': '23.875', 'counters/examples': 108800, 'counters/updates': 3400}
skipping logging after 108832 examples to avoid logging too frequently
train stats after 108864 examples: {'rewards_train/chosen': '0.084271', 'rewards_train/rejected': '0.044328', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039944', 'logps_train/rejected': '-83.42', 'logps_train/chosen': '-115.54', 'loss/train': '0.67788', 'examples_per_second': '31.61', 'grad_norm': '22.125', 'counters/examples': 108864, 'counters/updates': 3402}
train stats after 108896 examples: {'rewards_train/chosen': '0.088151', 'rewards_train/rejected': '0.017738', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070413', 'logps_train/rejected': '-146.88', 'logps_train/chosen': '-166.94', 'loss/train': '0.66699', 'examples_per_second': '31.78', 'grad_norm': '30.375', 'counters/examples': 108896, 'counters/updates': 3403}
skipping logging after 108928 examples to avoid logging too frequently
train stats after 108960 examples: {'rewards_train/chosen': '0.094137', 'rewards_train/rejected': '0.045866', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.048271', 'logps_train/rejected': '-140.96', 'logps_train/chosen': '-127.98', 'loss/train': '0.67548', 'examples_per_second': '35.874', 'grad_norm': '25.25', 'counters/examples': 108960, 'counters/updates': 3405}
train stats after 108992 examples: {'rewards_train/chosen': '0.088189', 'rewards_train/rejected': '0.091234', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0030446', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-170.06', 'loss/train': '0.70408', 'examples_per_second': '30.685', 'grad_norm': '32.5', 'counters/examples': 108992, 'counters/updates': 3406}
train stats after 109024 examples: {'rewards_train/chosen': '0.11981', 'rewards_train/rejected': '0.019887', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099926', 'logps_train/rejected': '-114', 'logps_train/chosen': '-152.36', 'loss/train': '0.65032', 'examples_per_second': '33.104', 'grad_norm': '28.125', 'counters/examples': 109024, 'counters/updates': 3407}
train stats after 109056 examples: {'rewards_train/chosen': '0.082046', 'rewards_train/rejected': '0.0045053', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077541', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-200.54', 'loss/train': '0.6693', 'examples_per_second': '31.624', 'grad_norm': '30.5', 'counters/examples': 109056, 'counters/updates': 3408}
train stats after 109088 examples: {'rewards_train/chosen': '0.049985', 'rewards_train/rejected': '0.064973', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014989', 'logps_train/rejected': '-95.229', 'logps_train/chosen': '-149.08', 'loss/train': '0.7123', 'examples_per_second': '32.352', 'grad_norm': '30.375', 'counters/examples': 109088, 'counters/updates': 3409}
train stats after 109120 examples: {'rewards_train/chosen': '0.13769', 'rewards_train/rejected': '0.012643', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12505', 'logps_train/rejected': '-115.25', 'logps_train/chosen': '-139.29', 'loss/train': '0.6374', 'examples_per_second': '32.649', 'grad_norm': '27.5', 'counters/examples': 109120, 'counters/updates': 3410}
train stats after 109152 examples: {'rewards_train/chosen': '-0.035807', 'rewards_train/rejected': '-0.031495', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0043113', 'logps_train/rejected': '-89.968', 'logps_train/chosen': '-131.09', 'loss/train': '0.70257', 'examples_per_second': '32.694', 'grad_norm': '24.75', 'counters/examples': 109152, 'counters/updates': 3411}
skipping logging after 109184 examples to avoid logging too frequently
train stats after 109216 examples: {'rewards_train/chosen': '0.074757', 'rewards_train/rejected': '0.063418', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011339', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-108.89', 'loss/train': '0.69184', 'examples_per_second': '33.115', 'grad_norm': '23.75', 'counters/examples': 109216, 'counters/updates': 3413}
skipping logging after 109248 examples to avoid logging too frequently
train stats after 109280 examples: {'rewards_train/chosen': '0.096371', 'rewards_train/rejected': '0.035785', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.060585', 'logps_train/rejected': '-140.96', 'logps_train/chosen': '-156.39', 'loss/train': '0.6743', 'examples_per_second': '33.085', 'grad_norm': '32', 'counters/examples': 109280, 'counters/updates': 3415}
train stats after 109312 examples: {'rewards_train/chosen': '0.097418', 'rewards_train/rejected': '0.073771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023647', 'logps_train/rejected': '-126.1', 'logps_train/chosen': '-154.98', 'loss/train': '0.69117', 'examples_per_second': '31.662', 'grad_norm': '29', 'counters/examples': 109312, 'counters/updates': 3416}
train stats after 109344 examples: {'rewards_train/chosen': '0.16946', 'rewards_train/rejected': '0.055602', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11386', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-134.86', 'loss/train': '0.64708', 'examples_per_second': '30.821', 'grad_norm': '28.875', 'counters/examples': 109344, 'counters/updates': 3417}
train stats after 109376 examples: {'rewards_train/chosen': '0.075463', 'rewards_train/rejected': '0.004924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070539', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-148.64', 'loss/train': '0.66278', 'examples_per_second': '31.578', 'grad_norm': '27.875', 'counters/examples': 109376, 'counters/updates': 3418}
train stats after 109408 examples: {'rewards_train/chosen': '0.055079', 'rewards_train/rejected': '0.020195', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034884', 'logps_train/rejected': '-135.99', 'logps_train/chosen': '-125.49', 'loss/train': '0.68748', 'examples_per_second': '31.034', 'grad_norm': '26.875', 'counters/examples': 109408, 'counters/updates': 3419}
train stats after 109440 examples: {'rewards_train/chosen': '0.091954', 'rewards_train/rejected': '-0.038489', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13044', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-126.39', 'loss/train': '0.64593', 'examples_per_second': '31.623', 'grad_norm': '28.625', 'counters/examples': 109440, 'counters/updates': 3420}
train stats after 109472 examples: {'rewards_train/chosen': '0.090706', 'rewards_train/rejected': '0.012106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0786', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-142.36', 'loss/train': '0.66217', 'examples_per_second': '31.905', 'grad_norm': '24.625', 'counters/examples': 109472, 'counters/updates': 3421}
train stats after 109504 examples: {'rewards_train/chosen': '0.088753', 'rewards_train/rejected': '0.026157', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062597', 'logps_train/rejected': '-112.81', 'logps_train/chosen': '-158.8', 'loss/train': '0.6693', 'examples_per_second': '32.706', 'grad_norm': '27.375', 'counters/examples': 109504, 'counters/updates': 3422}
train stats after 109536 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '0.079454', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067373', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-148.61', 'loss/train': '0.66729', 'examples_per_second': '31.312', 'grad_norm': '26.75', 'counters/examples': 109536, 'counters/updates': 3423}
skipping logging after 109568 examples to avoid logging too frequently
train stats after 109600 examples: {'rewards_train/chosen': '0.069554', 'rewards_train/rejected': '-0.024019', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093573', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-145.94', 'loss/train': '0.65194', 'examples_per_second': '31.025', 'grad_norm': '25.125', 'counters/examples': 109600, 'counters/updates': 3425}
train stats after 109632 examples: {'rewards_train/chosen': '0.098934', 'rewards_train/rejected': '-0.012853', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11179', 'logps_train/rejected': '-118.81', 'logps_train/chosen': '-134.35', 'loss/train': '0.64793', 'examples_per_second': '31.252', 'grad_norm': '23.75', 'counters/examples': 109632, 'counters/updates': 3426}
train stats after 109664 examples: {'rewards_train/chosen': '0.082952', 'rewards_train/rejected': '0.060425', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022527', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-165.77', 'loss/train': '0.68865', 'examples_per_second': '31.57', 'grad_norm': '28.75', 'counters/examples': 109664, 'counters/updates': 3427}
train stats after 109696 examples: {'rewards_train/chosen': '0.17003', 'rewards_train/rejected': '0.12597', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044064', 'logps_train/rejected': '-176.38', 'logps_train/chosen': '-173.74', 'loss/train': '0.68047', 'examples_per_second': '30.59', 'grad_norm': '34.25', 'counters/examples': 109696, 'counters/updates': 3428}
skipping logging after 109728 examples to avoid logging too frequently
train stats after 109760 examples: {'rewards_train/chosen': '0.14567', 'rewards_train/rejected': '0.10878', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036889', 'logps_train/rejected': '-136.66', 'logps_train/chosen': '-115.79', 'loss/train': '0.68036', 'examples_per_second': '31.112', 'grad_norm': '24', 'counters/examples': 109760, 'counters/updates': 3430}
train stats after 109792 examples: {'rewards_train/chosen': '1.7942e-05', 'rewards_train/rejected': '0.030891', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030873', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-119.85', 'loss/train': '0.71506', 'examples_per_second': '31.99', 'grad_norm': '26.375', 'counters/examples': 109792, 'counters/updates': 3431}
train stats after 109824 examples: {'rewards_train/chosen': '0.15863', 'rewards_train/rejected': '-0.0014059', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16004', 'logps_train/rejected': '-102.97', 'logps_train/chosen': '-182.36', 'loss/train': '0.63095', 'examples_per_second': '31.642', 'grad_norm': '50.75', 'counters/examples': 109824, 'counters/updates': 3432}
train stats after 109856 examples: {'rewards_train/chosen': '0.075364', 'rewards_train/rejected': '-0.049048', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12441', 'logps_train/rejected': '-109.27', 'logps_train/chosen': '-151.53', 'loss/train': '0.64183', 'examples_per_second': '30.751', 'grad_norm': '25.625', 'counters/examples': 109856, 'counters/updates': 3433}
train stats after 109888 examples: {'rewards_train/chosen': '0.074641', 'rewards_train/rejected': '0.023681', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050959', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-189.92', 'loss/train': '0.67654', 'examples_per_second': '32.602', 'grad_norm': '28.125', 'counters/examples': 109888, 'counters/updates': 3434}
train stats after 109920 examples: {'rewards_train/chosen': '0.094145', 'rewards_train/rejected': '0.0065721', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.087573', 'logps_train/rejected': '-141.49', 'logps_train/chosen': '-164.53', 'loss/train': '0.65957', 'examples_per_second': '29.888', 'grad_norm': '31.75', 'counters/examples': 109920, 'counters/updates': 3435}
train stats after 109952 examples: {'rewards_train/chosen': '0.07921', 'rewards_train/rejected': '0.028181', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051029', 'logps_train/rejected': '-155.12', 'logps_train/chosen': '-131.91', 'loss/train': '0.67853', 'examples_per_second': '31.581', 'grad_norm': '32', 'counters/examples': 109952, 'counters/updates': 3436}
train stats after 109984 examples: {'rewards_train/chosen': '0.067218', 'rewards_train/rejected': '0.072749', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.005531', 'logps_train/rejected': '-153.73', 'logps_train/chosen': '-117.43', 'loss/train': '0.71198', 'examples_per_second': '31.677', 'grad_norm': '26.625', 'counters/examples': 109984, 'counters/updates': 3437}
train stats after 110016 examples: {'rewards_train/chosen': '0.081225', 'rewards_train/rejected': '0.046964', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034261', 'logps_train/rejected': '-111.55', 'logps_train/chosen': '-121.6', 'loss/train': '0.68781', 'examples_per_second': '31.821', 'grad_norm': '26.625', 'counters/examples': 110016, 'counters/updates': 3438}
train stats after 110048 examples: {'rewards_train/chosen': '0.092736', 'rewards_train/rejected': '0.034223', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058512', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-134.74', 'loss/train': '0.67145', 'examples_per_second': '31.652', 'grad_norm': '25.375', 'counters/examples': 110048, 'counters/updates': 3439}
train stats after 110080 examples: {'rewards_train/chosen': '0.055617', 'rewards_train/rejected': '0.055679', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-6.1898e-05', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-122.51', 'loss/train': '0.69879', 'examples_per_second': '30.475', 'grad_norm': '25.625', 'counters/examples': 110080, 'counters/updates': 3440}
train stats after 110112 examples: {'rewards_train/chosen': '0.14156', 'rewards_train/rejected': '0.04634', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095222', 'logps_train/rejected': '-133.01', 'logps_train/chosen': '-134.25', 'loss/train': '0.65819', 'examples_per_second': '31.65', 'grad_norm': '28.125', 'counters/examples': 110112, 'counters/updates': 3441}
train stats after 110144 examples: {'rewards_train/chosen': '0.033709', 'rewards_train/rejected': '0.0039645', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029745', 'logps_train/rejected': '-98.755', 'logps_train/chosen': '-116.23', 'loss/train': '0.68403', 'examples_per_second': '31.544', 'grad_norm': '24', 'counters/examples': 110144, 'counters/updates': 3442}
train stats after 110176 examples: {'rewards_train/chosen': '0.11921', 'rewards_train/rejected': '0.073976', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045237', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-126.88', 'loss/train': '0.6825', 'examples_per_second': '30.183', 'grad_norm': '31.75', 'counters/examples': 110176, 'counters/updates': 3443}
train stats after 110208 examples: {'rewards_train/chosen': '0.03678', 'rewards_train/rejected': '-0.012887', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049668', 'logps_train/rejected': '-104.23', 'logps_train/chosen': '-138.12', 'loss/train': '0.67884', 'examples_per_second': '31.155', 'grad_norm': '25', 'counters/examples': 110208, 'counters/updates': 3444}
train stats after 110240 examples: {'rewards_train/chosen': '0.17611', 'rewards_train/rejected': '0.024381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15172', 'logps_train/rejected': '-113.21', 'logps_train/chosen': '-142.99', 'loss/train': '0.63361', 'examples_per_second': '24.741', 'grad_norm': '27', 'counters/examples': 110240, 'counters/updates': 3445}
train stats after 110272 examples: {'rewards_train/chosen': '0.078624', 'rewards_train/rejected': '0.058249', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020376', 'logps_train/rejected': '-115.78', 'logps_train/chosen': '-140.09', 'loss/train': '0.6939', 'examples_per_second': '31.659', 'grad_norm': '28.5', 'counters/examples': 110272, 'counters/updates': 3446}
skipping logging after 110304 examples to avoid logging too frequently
train stats after 110336 examples: {'rewards_train/chosen': '0.090428', 'rewards_train/rejected': '0.011114', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.079313', 'logps_train/rejected': '-117.72', 'logps_train/chosen': '-115.3', 'loss/train': '0.65939', 'examples_per_second': '35.539', 'grad_norm': '26.125', 'counters/examples': 110336, 'counters/updates': 3448}
train stats after 110368 examples: {'rewards_train/chosen': '0.13871', 'rewards_train/rejected': '0.052259', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086447', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-136.18', 'loss/train': '0.65951', 'examples_per_second': '30.31', 'grad_norm': '28.75', 'counters/examples': 110368, 'counters/updates': 3449}
train stats after 110400 examples: {'rewards_train/chosen': '0.043929', 'rewards_train/rejected': '0.061477', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017548', 'logps_train/rejected': '-125.91', 'logps_train/chosen': '-146.88', 'loss/train': '0.71554', 'examples_per_second': '31.153', 'grad_norm': '28.75', 'counters/examples': 110400, 'counters/updates': 3450}
train stats after 110432 examples: {'rewards_train/chosen': '0.12605', 'rewards_train/rejected': '0.041079', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084975', 'logps_train/rejected': '-127.33', 'logps_train/chosen': '-141.43', 'loss/train': '0.66449', 'examples_per_second': '31.638', 'grad_norm': '28.875', 'counters/examples': 110432, 'counters/updates': 3451}
train stats after 110464 examples: {'rewards_train/chosen': '0.12227', 'rewards_train/rejected': '0.037392', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084877', 'logps_train/rejected': '-106.69', 'logps_train/chosen': '-153.1', 'loss/train': '0.66127', 'examples_per_second': '32.728', 'grad_norm': '26.875', 'counters/examples': 110464, 'counters/updates': 3452}
skipping logging after 110496 examples to avoid logging too frequently
train stats after 110528 examples: {'rewards_train/chosen': '0.13387', 'rewards_train/rejected': '0.04925', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084618', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-157.3', 'loss/train': '0.66322', 'examples_per_second': '31.632', 'grad_norm': '27.25', 'counters/examples': 110528, 'counters/updates': 3454}
train stats after 110560 examples: {'rewards_train/chosen': '0.11684', 'rewards_train/rejected': '0.075862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040974', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-135.98', 'loss/train': '0.69013', 'examples_per_second': '32.367', 'grad_norm': '49.25', 'counters/examples': 110560, 'counters/updates': 3455}
train stats after 110592 examples: {'rewards_train/chosen': '0.086846', 'rewards_train/rejected': '-0.03253', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11938', 'logps_train/rejected': '-151.66', 'logps_train/chosen': '-144.42', 'loss/train': '0.6588', 'examples_per_second': '31.857', 'grad_norm': '29.5', 'counters/examples': 110592, 'counters/updates': 3456}
skipping logging after 110624 examples to avoid logging too frequently
train stats after 110656 examples: {'rewards_train/chosen': '0.084434', 'rewards_train/rejected': '0.015188', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069246', 'logps_train/rejected': '-101.52', 'logps_train/chosen': '-140.03', 'loss/train': '0.66828', 'examples_per_second': '31.407', 'grad_norm': '23', 'counters/examples': 110656, 'counters/updates': 3458}
train stats after 110688 examples: {'rewards_train/chosen': '0.091081', 'rewards_train/rejected': '0.012598', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.078484', 'logps_train/rejected': '-147.18', 'logps_train/chosen': '-162.91', 'loss/train': '0.67147', 'examples_per_second': '31.397', 'grad_norm': '28.125', 'counters/examples': 110688, 'counters/updates': 3459}
skipping logging after 110720 examples to avoid logging too frequently
train stats after 110752 examples: {'rewards_train/chosen': '0.026086', 'rewards_train/rejected': '-0.035331', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061417', 'logps_train/rejected': '-96.387', 'logps_train/chosen': '-96.361', 'loss/train': '0.66725', 'examples_per_second': '33.542', 'grad_norm': '23.125', 'counters/examples': 110752, 'counters/updates': 3461}
train stats after 110784 examples: {'rewards_train/chosen': '0.078952', 'rewards_train/rejected': '-0.0057944', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084747', 'logps_train/rejected': '-111.43', 'logps_train/chosen': '-150.75', 'loss/train': '0.66156', 'examples_per_second': '32.971', 'grad_norm': '24.5', 'counters/examples': 110784, 'counters/updates': 3462}
train stats after 110816 examples: {'rewards_train/chosen': '0.092783', 'rewards_train/rejected': '0.097756', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0049736', 'logps_train/rejected': '-128.42', 'logps_train/chosen': '-165.89', 'loss/train': '0.70558', 'examples_per_second': '31.351', 'grad_norm': '27.75', 'counters/examples': 110816, 'counters/updates': 3463}
skipping logging after 110848 examples to avoid logging too frequently
train stats after 110880 examples: {'rewards_train/chosen': '0.10665', 'rewards_train/rejected': '0.075172', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031482', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-141.66', 'loss/train': '0.68296', 'examples_per_second': '31.598', 'grad_norm': '27', 'counters/examples': 110880, 'counters/updates': 3465}
train stats after 110912 examples: {'rewards_train/chosen': '0.079197', 'rewards_train/rejected': '0.032057', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04714', 'logps_train/rejected': '-125.43', 'logps_train/chosen': '-136.4', 'loss/train': '0.67692', 'examples_per_second': '31.603', 'grad_norm': '26', 'counters/examples': 110912, 'counters/updates': 3466}
train stats after 110944 examples: {'rewards_train/chosen': '0.13373', 'rewards_train/rejected': '0.029591', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10414', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-121.65', 'loss/train': '0.64701', 'examples_per_second': '31.642', 'grad_norm': '24.5', 'counters/examples': 110944, 'counters/updates': 3467}
skipping logging after 110976 examples to avoid logging too frequently
train stats after 111008 examples: {'rewards_train/chosen': '0.13479', 'rewards_train/rejected': '-0.015343', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15014', 'logps_train/rejected': '-127.85', 'logps_train/chosen': '-140.26', 'loss/train': '0.62864', 'examples_per_second': '32.49', 'grad_norm': '27.5', 'counters/examples': 111008, 'counters/updates': 3469}
train stats after 111040 examples: {'rewards_train/chosen': '0.061097', 'rewards_train/rejected': '0.022098', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.038999', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-149.81', 'loss/train': '0.68196', 'examples_per_second': '31.65', 'grad_norm': '27.75', 'counters/examples': 111040, 'counters/updates': 3470}
train stats after 111072 examples: {'rewards_train/chosen': '0.099327', 'rewards_train/rejected': '0.10844', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0091159', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-134.11', 'loss/train': '0.70739', 'examples_per_second': '29.947', 'grad_norm': '30.375', 'counters/examples': 111072, 'counters/updates': 3471}
skipping logging after 111104 examples to avoid logging too frequently
train stats after 111136 examples: {'rewards_train/chosen': '0.12377', 'rewards_train/rejected': '0.095174', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028596', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-129.78', 'loss/train': '0.69024', 'examples_per_second': '30.089', 'grad_norm': '32.5', 'counters/examples': 111136, 'counters/updates': 3473}
train stats after 111168 examples: {'rewards_train/chosen': '0.07799', 'rewards_train/rejected': '0.046673', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031316', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-119.38', 'loss/train': '0.68415', 'examples_per_second': '31.454', 'grad_norm': '27.25', 'counters/examples': 111168, 'counters/updates': 3474}
train stats after 111200 examples: {'rewards_train/chosen': '0.10205', 'rewards_train/rejected': '0.0077451', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094301', 'logps_train/rejected': '-147.44', 'logps_train/chosen': '-209.25', 'loss/train': '0.66474', 'examples_per_second': '31.593', 'grad_norm': '32.25', 'counters/examples': 111200, 'counters/updates': 3475}
train stats after 111232 examples: {'rewards_train/chosen': '0.10729', 'rewards_train/rejected': '0.016492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090796', 'logps_train/rejected': '-111.68', 'logps_train/chosen': '-138.96', 'loss/train': '0.65853', 'examples_per_second': '30.678', 'grad_norm': '31.25', 'counters/examples': 111232, 'counters/updates': 3476}
skipping logging after 111264 examples to avoid logging too frequently
train stats after 111296 examples: {'rewards_train/chosen': '0.052007', 'rewards_train/rejected': '0.071096', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019089', 'logps_train/rejected': '-117.72', 'logps_train/chosen': '-119.51', 'loss/train': '0.71352', 'examples_per_second': '32.12', 'grad_norm': '30', 'counters/examples': 111296, 'counters/updates': 3478}
train stats after 111328 examples: {'rewards_train/chosen': '0.1228', 'rewards_train/rejected': '0.035602', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.087201', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-164.14', 'loss/train': '0.65388', 'examples_per_second': '31.272', 'grad_norm': '25.5', 'counters/examples': 111328, 'counters/updates': 3479}
skipping logging after 111360 examples to avoid logging too frequently
train stats after 111392 examples: {'rewards_train/chosen': '0.13065', 'rewards_train/rejected': '0.058136', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072513', 'logps_train/rejected': '-157.82', 'logps_train/chosen': '-138.23', 'loss/train': '0.66709', 'examples_per_second': '31.616', 'grad_norm': '29.375', 'counters/examples': 111392, 'counters/updates': 3481}
train stats after 111424 examples: {'rewards_train/chosen': '0.15213', 'rewards_train/rejected': '0.040868', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11126', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-166.42', 'loss/train': '0.64734', 'examples_per_second': '31.622', 'grad_norm': '26', 'counters/examples': 111424, 'counters/updates': 3482}
train stats after 111456 examples: {'rewards_train/chosen': '0.18834', 'rewards_train/rejected': '0.047642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1407', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-163.61', 'loss/train': '0.6356', 'examples_per_second': '30.372', 'grad_norm': '24.625', 'counters/examples': 111456, 'counters/updates': 3483}
skipping logging after 111488 examples to avoid logging too frequently
train stats after 111520 examples: {'rewards_train/chosen': '0.096788', 'rewards_train/rejected': '0.03486', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061928', 'logps_train/rejected': '-88.348', 'logps_train/chosen': '-143.92', 'loss/train': '0.66849', 'examples_per_second': '31.592', 'grad_norm': '23.75', 'counters/examples': 111520, 'counters/updates': 3485}
train stats after 111552 examples: {'rewards_train/chosen': '0.046981', 'rewards_train/rejected': '0.0088501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038131', 'logps_train/rejected': '-100.58', 'logps_train/chosen': '-118.72', 'loss/train': '0.68435', 'examples_per_second': '32.441', 'grad_norm': '25.5', 'counters/examples': 111552, 'counters/updates': 3486}
train stats after 111584 examples: {'rewards_train/chosen': '0.09735', 'rewards_train/rejected': '0.035662', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.061688', 'logps_train/rejected': '-150.89', 'logps_train/chosen': '-123.87', 'loss/train': '0.6753', 'examples_per_second': '30.12', 'grad_norm': '28.25', 'counters/examples': 111584, 'counters/updates': 3487}
train stats after 111616 examples: {'rewards_train/chosen': '0.046403', 'rewards_train/rejected': '0.02072', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025683', 'logps_train/rejected': '-86.12', 'logps_train/chosen': '-154.68', 'loss/train': '0.68748', 'examples_per_second': '30.254', 'grad_norm': '25.25', 'counters/examples': 111616, 'counters/updates': 3488}
train stats after 111648 examples: {'rewards_train/chosen': '0.092055', 'rewards_train/rejected': '0.0050809', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086974', 'logps_train/rejected': '-99.606', 'logps_train/chosen': '-106.14', 'loss/train': '0.66147', 'examples_per_second': '32.716', 'grad_norm': '23.75', 'counters/examples': 111648, 'counters/updates': 3489}
train stats after 111680 examples: {'rewards_train/chosen': '0.12135', 'rewards_train/rejected': '-0.085974', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20732', 'logps_train/rejected': '-83.453', 'logps_train/chosen': '-147.68', 'loss/train': '0.60826', 'examples_per_second': '31.986', 'grad_norm': '22.75', 'counters/examples': 111680, 'counters/updates': 3490}
train stats after 111712 examples: {'rewards_train/chosen': '0.072494', 'rewards_train/rejected': '0.040562', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.031932', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-148.98', 'loss/train': '0.68809', 'examples_per_second': '30.052', 'grad_norm': '31.5', 'counters/examples': 111712, 'counters/updates': 3491}
train stats after 111744 examples: {'rewards_train/chosen': '0.093138', 'rewards_train/rejected': '0.0047109', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088427', 'logps_train/rejected': '-99.422', 'logps_train/chosen': '-152.1', 'loss/train': '0.65947', 'examples_per_second': '30.224', 'grad_norm': '25.125', 'counters/examples': 111744, 'counters/updates': 3492}
train stats after 111776 examples: {'rewards_train/chosen': '0.088708', 'rewards_train/rejected': '-0.01917', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10788', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-117.73', 'loss/train': '0.65095', 'examples_per_second': '31.456', 'grad_norm': '23.625', 'counters/examples': 111776, 'counters/updates': 3493}
skipping logging after 111808 examples to avoid logging too frequently
train stats after 111840 examples: {'rewards_train/chosen': '0.11487', 'rewards_train/rejected': '0.045721', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069154', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-148.06', 'loss/train': '0.6658', 'examples_per_second': '30.516', 'grad_norm': '27.125', 'counters/examples': 111840, 'counters/updates': 3495}
train stats after 111872 examples: {'rewards_train/chosen': '0.084986', 'rewards_train/rejected': '0.017292', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067694', 'logps_train/rejected': '-85.058', 'logps_train/chosen': '-124.05', 'loss/train': '0.66432', 'examples_per_second': '31.417', 'grad_norm': '21.875', 'counters/examples': 111872, 'counters/updates': 3496}
train stats after 111904 examples: {'rewards_train/chosen': '0.064557', 'rewards_train/rejected': '0.063334', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0012228', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-121.72', 'loss/train': '0.70109', 'examples_per_second': '32.414', 'grad_norm': '27.25', 'counters/examples': 111904, 'counters/updates': 3497}
train stats after 111936 examples: {'rewards_train/chosen': '0.13813', 'rewards_train/rejected': '0.09521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042925', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-110.32', 'loss/train': '0.67764', 'examples_per_second': '31.57', 'grad_norm': '23.125', 'counters/examples': 111936, 'counters/updates': 3498}
train stats after 111968 examples: {'rewards_train/chosen': '0.16021', 'rewards_train/rejected': '0.081412', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078796', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-145.84', 'loss/train': '0.66196', 'examples_per_second': '31.975', 'grad_norm': '25.75', 'counters/examples': 111968, 'counters/updates': 3499}
train stats after 112000 examples: {'rewards_train/chosen': '0.035578', 'rewards_train/rejected': '0.051046', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015469', 'logps_train/rejected': '-112.48', 'logps_train/chosen': '-117.04', 'loss/train': '0.70611', 'examples_per_second': '30.392', 'grad_norm': '27.5', 'counters/examples': 112000, 'counters/updates': 3500}
train stats after 112032 examples: {'rewards_train/chosen': '0.1956', 'rewards_train/rejected': '0.083919', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11168', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-157.44', 'loss/train': '0.65948', 'examples_per_second': '31.618', 'grad_norm': '29.25', 'counters/examples': 112032, 'counters/updates': 3501}
train stats after 112064 examples: {'rewards_train/chosen': '0.048614', 'rewards_train/rejected': '0.084413', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.035799', 'logps_train/rejected': '-161.23', 'logps_train/chosen': '-189.33', 'loss/train': '0.7219', 'examples_per_second': '30.248', 'grad_norm': '32.25', 'counters/examples': 112064, 'counters/updates': 3502}
train stats after 112096 examples: {'rewards_train/chosen': '0.14049', 'rewards_train/rejected': '0.028118', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11237', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-147.81', 'loss/train': '0.64651', 'examples_per_second': '31.045', 'grad_norm': '25.625', 'counters/examples': 112096, 'counters/updates': 3503}
train stats after 112128 examples: {'rewards_train/chosen': '0.15602', 'rewards_train/rejected': '0.025237', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13079', 'logps_train/rejected': '-119.87', 'logps_train/chosen': '-152.85', 'loss/train': '0.64251', 'examples_per_second': '31.13', 'grad_norm': '26.375', 'counters/examples': 112128, 'counters/updates': 3504}
train stats after 112160 examples: {'rewards_train/chosen': '0.17189', 'rewards_train/rejected': '0.064987', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10691', 'logps_train/rejected': '-145.51', 'logps_train/chosen': '-195.64', 'loss/train': '0.64742', 'examples_per_second': '31.425', 'grad_norm': '26.75', 'counters/examples': 112160, 'counters/updates': 3505}
train stats after 112192 examples: {'rewards_train/chosen': '0.096901', 'rewards_train/rejected': '0.092088', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0048133', 'logps_train/rejected': '-109.58', 'logps_train/chosen': '-162.59', 'loss/train': '0.69615', 'examples_per_second': '31.967', 'grad_norm': '27.5', 'counters/examples': 112192, 'counters/updates': 3506}
train stats after 112224 examples: {'rewards_train/chosen': '0.10386', 'rewards_train/rejected': '0.027181', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07668', 'logps_train/rejected': '-125.64', 'logps_train/chosen': '-137.58', 'loss/train': '0.6628', 'examples_per_second': '31.573', 'grad_norm': '24', 'counters/examples': 112224, 'counters/updates': 3507}
train stats after 112256 examples: {'rewards_train/chosen': '0.070462', 'rewards_train/rejected': '-0.022336', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092797', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-161.23', 'loss/train': '0.65768', 'examples_per_second': '33.117', 'grad_norm': '27.375', 'counters/examples': 112256, 'counters/updates': 3508}
train stats after 112288 examples: {'rewards_train/chosen': '0.088527', 'rewards_train/rejected': '-0.033159', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12169', 'logps_train/rejected': '-117.73', 'logps_train/chosen': '-112.67', 'loss/train': '0.64017', 'examples_per_second': '32.619', 'grad_norm': '23.75', 'counters/examples': 112288, 'counters/updates': 3509}
train stats after 112320 examples: {'rewards_train/chosen': '0.062297', 'rewards_train/rejected': '0.02928', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033018', 'logps_train/rejected': '-107.3', 'logps_train/chosen': '-129.02', 'loss/train': '0.68316', 'examples_per_second': '30.845', 'grad_norm': '26.375', 'counters/examples': 112320, 'counters/updates': 3510}
train stats after 112352 examples: {'rewards_train/chosen': '0.035938', 'rewards_train/rejected': '0.016626', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019312', 'logps_train/rejected': '-92.605', 'logps_train/chosen': '-108.97', 'loss/train': '0.69146', 'examples_per_second': '32.381', 'grad_norm': '22.625', 'counters/examples': 112352, 'counters/updates': 3511}
skipping logging after 112384 examples to avoid logging too frequently
train stats after 112416 examples: {'rewards_train/chosen': '0.18205', 'rewards_train/rejected': '0.053914', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12813', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-160.19', 'loss/train': '0.64024', 'examples_per_second': '31.421', 'grad_norm': '28.5', 'counters/examples': 112416, 'counters/updates': 3513}
train stats after 112448 examples: {'rewards_train/chosen': '0.099967', 'rewards_train/rejected': '0.057315', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042652', 'logps_train/rejected': '-152.23', 'logps_train/chosen': '-164.07', 'loss/train': '0.68086', 'examples_per_second': '31.588', 'grad_norm': '38.5', 'counters/examples': 112448, 'counters/updates': 3514}
train stats after 112480 examples: {'rewards_train/chosen': '0.063204', 'rewards_train/rejected': '-0.0047523', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067956', 'logps_train/rejected': '-99.196', 'logps_train/chosen': '-124.63', 'loss/train': '0.6629', 'examples_per_second': '33.052', 'grad_norm': '24.125', 'counters/examples': 112480, 'counters/updates': 3515}
train stats after 112512 examples: {'rewards_train/chosen': '0.10858', 'rewards_train/rejected': '0.022851', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085728', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-138.69', 'loss/train': '0.65705', 'examples_per_second': '30.082', 'grad_norm': '34.25', 'counters/examples': 112512, 'counters/updates': 3516}
train stats after 112544 examples: {'rewards_train/chosen': '0.032065', 'rewards_train/rejected': '0.05787', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025805', 'logps_train/rejected': '-103.56', 'logps_train/chosen': '-143.72', 'loss/train': '0.71557', 'examples_per_second': '32.109', 'grad_norm': '27.25', 'counters/examples': 112544, 'counters/updates': 3517}
train stats after 112576 examples: {'rewards_train/chosen': '0.049335', 'rewards_train/rejected': '0.095035', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0457', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-135.96', 'loss/train': '0.72725', 'examples_per_second': '31.607', 'grad_norm': '31.75', 'counters/examples': 112576, 'counters/updates': 3518}
train stats after 112608 examples: {'rewards_train/chosen': '0.081069', 'rewards_train/rejected': '0.032294', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048774', 'logps_train/rejected': '-98.07', 'logps_train/chosen': '-142.04', 'loss/train': '0.67823', 'examples_per_second': '30.053', 'grad_norm': '25.25', 'counters/examples': 112608, 'counters/updates': 3519}
train stats after 112640 examples: {'rewards_train/chosen': '0.14388', 'rewards_train/rejected': '0.049784', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.094096', 'logps_train/rejected': '-121.33', 'logps_train/chosen': '-141.07', 'loss/train': '0.654', 'examples_per_second': '31.572', 'grad_norm': '28.625', 'counters/examples': 112640, 'counters/updates': 3520}
train stats after 112672 examples: {'rewards_train/chosen': '0.079231', 'rewards_train/rejected': '0.016939', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062293', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-156.06', 'loss/train': '0.67081', 'examples_per_second': '30.626', 'grad_norm': '26.875', 'counters/examples': 112672, 'counters/updates': 3521}
skipping logging after 112704 examples to avoid logging too frequently
train stats after 112736 examples: {'rewards_train/chosen': '0.11101', 'rewards_train/rejected': '0.011841', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099167', 'logps_train/rejected': '-98.134', 'logps_train/chosen': '-140.71', 'loss/train': '0.65327', 'examples_per_second': '33.667', 'grad_norm': '25.125', 'counters/examples': 112736, 'counters/updates': 3523}
train stats after 112768 examples: {'rewards_train/chosen': '0.061096', 'rewards_train/rejected': '-0.00036446', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061461', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-116.83', 'loss/train': '0.66889', 'examples_per_second': '31.06', 'grad_norm': '26.625', 'counters/examples': 112768, 'counters/updates': 3524}
skipping logging after 112800 examples to avoid logging too frequently
train stats after 112832 examples: {'rewards_train/chosen': '0.055137', 'rewards_train/rejected': '0.075212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020075', 'logps_train/rejected': '-156.21', 'logps_train/chosen': '-147.53', 'loss/train': '0.71481', 'examples_per_second': '31.086', 'grad_norm': '27.75', 'counters/examples': 112832, 'counters/updates': 3526}
skipping logging after 112864 examples to avoid logging too frequently
train stats after 112896 examples: {'rewards_train/chosen': '0.14681', 'rewards_train/rejected': '0.10424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04257', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-174.73', 'loss/train': '0.68079', 'examples_per_second': '31.641', 'grad_norm': '29.75', 'counters/examples': 112896, 'counters/updates': 3528}
skipping logging after 112928 examples to avoid logging too frequently
train stats after 112960 examples: {'rewards_train/chosen': '0.12057', 'rewards_train/rejected': '0.015109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10546', 'logps_train/rejected': '-166.43', 'logps_train/chosen': '-142.28', 'loss/train': '0.64968', 'examples_per_second': '31.058', 'grad_norm': '26.625', 'counters/examples': 112960, 'counters/updates': 3530}
train stats after 112992 examples: {'rewards_train/chosen': '0.12243', 'rewards_train/rejected': '0.07716', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045272', 'logps_train/rejected': '-131.79', 'logps_train/chosen': '-145.96', 'loss/train': '0.68326', 'examples_per_second': '29.967', 'grad_norm': '27.375', 'counters/examples': 112992, 'counters/updates': 3531}
train stats after 113024 examples: {'rewards_train/chosen': '0.16589', 'rewards_train/rejected': '0.0033529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16254', 'logps_train/rejected': '-151.41', 'logps_train/chosen': '-201.9', 'loss/train': '0.63036', 'examples_per_second': '31.551', 'grad_norm': '28', 'counters/examples': 113024, 'counters/updates': 3532}
train stats after 113056 examples: {'rewards_train/chosen': '0.07889', 'rewards_train/rejected': '0.039902', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038988', 'logps_train/rejected': '-101.72', 'logps_train/chosen': '-117.99', 'loss/train': '0.68106', 'examples_per_second': '29.949', 'grad_norm': '24.375', 'counters/examples': 113056, 'counters/updates': 3533}
train stats after 113088 examples: {'rewards_train/chosen': '0.036827', 'rewards_train/rejected': '0.079652', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042826', 'logps_train/rejected': '-135.3', 'logps_train/chosen': '-158.89', 'loss/train': '0.72238', 'examples_per_second': '31.578', 'grad_norm': '33', 'counters/examples': 113088, 'counters/updates': 3534}
train stats after 113120 examples: {'rewards_train/chosen': '0.083497', 'rewards_train/rejected': '0.012009', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071488', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-131.56', 'loss/train': '0.66517', 'examples_per_second': '30.24', 'grad_norm': '26.125', 'counters/examples': 113120, 'counters/updates': 3535}
train stats after 113152 examples: {'rewards_train/chosen': '0.13851', 'rewards_train/rejected': '0.064403', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074108', 'logps_train/rejected': '-149.12', 'logps_train/chosen': '-174.01', 'loss/train': '0.66359', 'examples_per_second': '31.397', 'grad_norm': '29.625', 'counters/examples': 113152, 'counters/updates': 3536}
train stats after 113184 examples: {'rewards_train/chosen': '0.13677', 'rewards_train/rejected': '0.035422', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10135', 'logps_train/rejected': '-145.08', 'logps_train/chosen': '-146.31', 'loss/train': '0.65424', 'examples_per_second': '32.111', 'grad_norm': '27.75', 'counters/examples': 113184, 'counters/updates': 3537}
train stats after 113216 examples: {'rewards_train/chosen': '0.16058', 'rewards_train/rejected': '0.10995', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050626', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-170.84', 'loss/train': '0.68085', 'examples_per_second': '30.88', 'grad_norm': '30', 'counters/examples': 113216, 'counters/updates': 3538}
train stats after 113248 examples: {'rewards_train/chosen': '0.10346', 'rewards_train/rejected': '0.072138', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031318', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-124.18', 'loss/train': '0.68651', 'examples_per_second': '31.561', 'grad_norm': '26', 'counters/examples': 113248, 'counters/updates': 3539}
skipping logging after 113280 examples to avoid logging too frequently
train stats after 113312 examples: {'rewards_train/chosen': '0.053365', 'rewards_train/rejected': '-0.01536', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068725', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-147.56', 'loss/train': '0.66829', 'examples_per_second': '31.551', 'grad_norm': '25.625', 'counters/examples': 113312, 'counters/updates': 3541}
skipping logging after 113344 examples to avoid logging too frequently
train stats after 113376 examples: {'rewards_train/chosen': '0.063957', 'rewards_train/rejected': '0.044852', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019105', 'logps_train/rejected': '-98.858', 'logps_train/chosen': '-137.98', 'loss/train': '0.69188', 'examples_per_second': '31.243', 'grad_norm': '25.875', 'counters/examples': 113376, 'counters/updates': 3543}
train stats after 113408 examples: {'rewards_train/chosen': '0.13209', 'rewards_train/rejected': '0.059559', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072533', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-149.2', 'loss/train': '0.66328', 'examples_per_second': '30.183', 'grad_norm': '24.75', 'counters/examples': 113408, 'counters/updates': 3544}
train stats after 113440 examples: {'rewards_train/chosen': '0.1542', 'rewards_train/rejected': '0.095035', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059164', 'logps_train/rejected': '-96.865', 'logps_train/chosen': '-152.1', 'loss/train': '0.6734', 'examples_per_second': '30.543', 'grad_norm': '27.875', 'counters/examples': 113440, 'counters/updates': 3545}
train stats after 113472 examples: {'rewards_train/chosen': '0.10729', 'rewards_train/rejected': '0.065914', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041378', 'logps_train/rejected': '-119.83', 'logps_train/chosen': '-149.9', 'loss/train': '0.6764', 'examples_per_second': '30.579', 'grad_norm': '27.5', 'counters/examples': 113472, 'counters/updates': 3546}
train stats after 113504 examples: {'rewards_train/chosen': '0.11657', 'rewards_train/rejected': '-0.0022721', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11884', 'logps_train/rejected': '-121.16', 'logps_train/chosen': '-135.65', 'loss/train': '0.6457', 'examples_per_second': '32.994', 'grad_norm': '28.875', 'counters/examples': 113504, 'counters/updates': 3547}
train stats after 113536 examples: {'rewards_train/chosen': '0.15959', 'rewards_train/rejected': '0.054485', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1051', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-172.5', 'loss/train': '0.65135', 'examples_per_second': '31.217', 'grad_norm': '28.625', 'counters/examples': 113536, 'counters/updates': 3548}
train stats after 113568 examples: {'rewards_train/chosen': '0.13981', 'rewards_train/rejected': '0.042672', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097135', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-113.66', 'loss/train': '0.65375', 'examples_per_second': '31.607', 'grad_norm': '25.625', 'counters/examples': 113568, 'counters/updates': 3549}
train stats after 113600 examples: {'rewards_train/chosen': '0.058903', 'rewards_train/rejected': '-0.013882', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072785', 'logps_train/rejected': '-124.73', 'logps_train/chosen': '-123.59', 'loss/train': '0.66166', 'examples_per_second': '31.582', 'grad_norm': '24.375', 'counters/examples': 113600, 'counters/updates': 3550}
train stats after 113632 examples: {'rewards_train/chosen': '0.11679', 'rewards_train/rejected': '0.032505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084283', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-174.8', 'loss/train': '0.6577', 'examples_per_second': '24.317', 'grad_norm': '29.125', 'counters/examples': 113632, 'counters/updates': 3551}
train stats after 113664 examples: {'rewards_train/chosen': '0.1479', 'rewards_train/rejected': '-0.044428', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.19233', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-160.69', 'loss/train': '0.60845', 'examples_per_second': '31.933', 'grad_norm': '24.25', 'counters/examples': 113664, 'counters/updates': 3552}
train stats after 113696 examples: {'rewards_train/chosen': '0.064237', 'rewards_train/rejected': '-0.048661', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1129', 'logps_train/rejected': '-77.797', 'logps_train/chosen': '-113.48', 'loss/train': '0.64337', 'examples_per_second': '32.517', 'grad_norm': '20', 'counters/examples': 113696, 'counters/updates': 3553}
train stats after 113728 examples: {'rewards_train/chosen': '0.15285', 'rewards_train/rejected': '0.04713', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10572', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-121.53', 'loss/train': '0.64953', 'examples_per_second': '24.895', 'grad_norm': '31.125', 'counters/examples': 113728, 'counters/updates': 3554}
train stats after 113760 examples: {'rewards_train/chosen': '0.076299', 'rewards_train/rejected': '-0.019021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09532', 'logps_train/rejected': '-118.89', 'logps_train/chosen': '-123.56', 'loss/train': '0.65344', 'examples_per_second': '32.223', 'grad_norm': '24.375', 'counters/examples': 113760, 'counters/updates': 3555}
train stats after 113792 examples: {'rewards_train/chosen': '0.16055', 'rewards_train/rejected': '-0.0026995', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16325', 'logps_train/rejected': '-98.818', 'logps_train/chosen': '-151.83', 'loss/train': '0.62329', 'examples_per_second': '31.551', 'grad_norm': '22.75', 'counters/examples': 113792, 'counters/updates': 3556}
skipping logging after 113824 examples to avoid logging too frequently
train stats after 113856 examples: {'rewards_train/chosen': '0.18367', 'rewards_train/rejected': '0.10008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083591', 'logps_train/rejected': '-154.38', 'logps_train/chosen': '-157.23', 'loss/train': '0.66263', 'examples_per_second': '31.644', 'grad_norm': '30.125', 'counters/examples': 113856, 'counters/updates': 3558}
train stats after 113888 examples: {'rewards_train/chosen': '0.11298', 'rewards_train/rejected': '0.037558', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075419', 'logps_train/rejected': '-138.81', 'logps_train/chosen': '-155.78', 'loss/train': '0.66704', 'examples_per_second': '31.243', 'grad_norm': '29', 'counters/examples': 113888, 'counters/updates': 3559}
train stats after 113920 examples: {'rewards_train/chosen': '0.084412', 'rewards_train/rejected': '0.068509', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015903', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-142.15', 'loss/train': '0.69212', 'examples_per_second': '32.498', 'grad_norm': '26.625', 'counters/examples': 113920, 'counters/updates': 3560}
skipping logging after 113952 examples to avoid logging too frequently
train stats after 113984 examples: {'rewards_train/chosen': '0.082554', 'rewards_train/rejected': '0.052711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029843', 'logps_train/rejected': '-133.4', 'logps_train/chosen': '-126.37', 'loss/train': '0.68395', 'examples_per_second': '30.773', 'grad_norm': '25.75', 'counters/examples': 113984, 'counters/updates': 3562}
skipping logging after 114016 examples to avoid logging too frequently
train stats after 114048 examples: {'rewards_train/chosen': '0.069114', 'rewards_train/rejected': '-0.019198', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088313', 'logps_train/rejected': '-97.441', 'logps_train/chosen': '-108.37', 'loss/train': '0.65819', 'examples_per_second': '34.44', 'grad_norm': '22.25', 'counters/examples': 114048, 'counters/updates': 3564}
skipping logging after 114080 examples to avoid logging too frequently
train stats after 114112 examples: {'rewards_train/chosen': '0.14749', 'rewards_train/rejected': '0.085389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062097', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-125.51', 'loss/train': '0.68073', 'examples_per_second': '32.594', 'grad_norm': '27.375', 'counters/examples': 114112, 'counters/updates': 3566}
skipping logging after 114144 examples to avoid logging too frequently
train stats after 114176 examples: {'rewards_train/chosen': '0.084449', 'rewards_train/rejected': '0.031532', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052917', 'logps_train/rejected': '-121.72', 'logps_train/chosen': '-148.27', 'loss/train': '0.67092', 'examples_per_second': '34.457', 'grad_norm': '26.375', 'counters/examples': 114176, 'counters/updates': 3568}
train stats after 114208 examples: {'rewards_train/chosen': '0.048989', 'rewards_train/rejected': '0.046773', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0022158', 'logps_train/rejected': '-109.16', 'logps_train/chosen': '-101.43', 'loss/train': '0.69709', 'examples_per_second': '32.456', 'grad_norm': '26.5', 'counters/examples': 114208, 'counters/updates': 3569}
train stats after 114240 examples: {'rewards_train/chosen': '0.10425', 'rewards_train/rejected': '0.03792', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066327', 'logps_train/rejected': '-155.39', 'logps_train/chosen': '-145.12', 'loss/train': '0.67049', 'examples_per_second': '29.943', 'grad_norm': '31.125', 'counters/examples': 114240, 'counters/updates': 3570}
train stats after 114272 examples: {'rewards_train/chosen': '0.062314', 'rewards_train/rejected': '0.015411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046903', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-124.67', 'loss/train': '0.6768', 'examples_per_second': '30.882', 'grad_norm': '23.625', 'counters/examples': 114272, 'counters/updates': 3571}
train stats after 114304 examples: {'rewards_train/chosen': '0.070151', 'rewards_train/rejected': '0.0041216', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066029', 'logps_train/rejected': '-97.983', 'logps_train/chosen': '-122.82', 'loss/train': '0.6733', 'examples_per_second': '30.038', 'grad_norm': '23.875', 'counters/examples': 114304, 'counters/updates': 3572}
train stats after 114336 examples: {'rewards_train/chosen': '0.12595', 'rewards_train/rejected': '-0.0014393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12739', 'logps_train/rejected': '-106.75', 'logps_train/chosen': '-149.22', 'loss/train': '0.63967', 'examples_per_second': '31.591', 'grad_norm': '25.25', 'counters/examples': 114336, 'counters/updates': 3573}
train stats after 114368 examples: {'rewards_train/chosen': '0.15831', 'rewards_train/rejected': '0.039837', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11847', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-147.1', 'loss/train': '0.64279', 'examples_per_second': '31.565', 'grad_norm': '27', 'counters/examples': 114368, 'counters/updates': 3574}
train stats after 114400 examples: {'rewards_train/chosen': '0.037703', 'rewards_train/rejected': '-0.008253', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045956', 'logps_train/rejected': '-107.66', 'logps_train/chosen': '-133.42', 'loss/train': '0.67527', 'examples_per_second': '32.626', 'grad_norm': '27.875', 'counters/examples': 114400, 'counters/updates': 3575}
train stats after 114432 examples: {'rewards_train/chosen': '0.064179', 'rewards_train/rejected': '0.011289', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05289', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-148.81', 'loss/train': '0.67425', 'examples_per_second': '31.573', 'grad_norm': '26.125', 'counters/examples': 114432, 'counters/updates': 3576}
train stats after 114464 examples: {'rewards_train/chosen': '0.078763', 'rewards_train/rejected': '0.072323', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0064404', 'logps_train/rejected': '-151.3', 'logps_train/chosen': '-131.44', 'loss/train': '0.70291', 'examples_per_second': '29.899', 'grad_norm': '31.125', 'counters/examples': 114464, 'counters/updates': 3577}
train stats after 114496 examples: {'rewards_train/chosen': '0.16108', 'rewards_train/rejected': '0.053286', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10779', 'logps_train/rejected': '-126.95', 'logps_train/chosen': '-157.29', 'loss/train': '0.65235', 'examples_per_second': '30.918', 'grad_norm': '26.25', 'counters/examples': 114496, 'counters/updates': 3578}
skipping logging after 114528 examples to avoid logging too frequently
train stats after 114560 examples: {'rewards_train/chosen': '0.16326', 'rewards_train/rejected': '0.077032', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086224', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-202.03', 'loss/train': '0.66514', 'examples_per_second': '31.493', 'grad_norm': '31', 'counters/examples': 114560, 'counters/updates': 3580}
train stats after 114592 examples: {'rewards_train/chosen': '0.093555', 'rewards_train/rejected': '0.039671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053884', 'logps_train/rejected': '-104', 'logps_train/chosen': '-148.19', 'loss/train': '0.67513', 'examples_per_second': '31.779', 'grad_norm': '24.75', 'counters/examples': 114592, 'counters/updates': 3581}
skipping logging after 114624 examples to avoid logging too frequently
train stats after 114656 examples: {'rewards_train/chosen': '0.11771', 'rewards_train/rejected': '-0.0036016', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12131', 'logps_train/rejected': '-110.03', 'logps_train/chosen': '-121.1', 'loss/train': '0.6417', 'examples_per_second': '32.412', 'grad_norm': '24.25', 'counters/examples': 114656, 'counters/updates': 3583}
train stats after 114688 examples: {'rewards_train/chosen': '0.073554', 'rewards_train/rejected': '0.091911', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018356', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-147.04', 'loss/train': '0.70967', 'examples_per_second': '31.209', 'grad_norm': '31.375', 'counters/examples': 114688, 'counters/updates': 3584}
train stats after 114720 examples: {'rewards_train/chosen': '0.096794', 'rewards_train/rejected': '0.078067', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018726', 'logps_train/rejected': '-139.74', 'logps_train/chosen': '-142.63', 'loss/train': '0.69357', 'examples_per_second': '31.575', 'grad_norm': '27.875', 'counters/examples': 114720, 'counters/updates': 3585}
train stats after 114752 examples: {'rewards_train/chosen': '0.053374', 'rewards_train/rejected': '0.019323', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034051', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-154.07', 'loss/train': '0.68644', 'examples_per_second': '31.726', 'grad_norm': '27.625', 'counters/examples': 114752, 'counters/updates': 3586}
train stats after 114784 examples: {'rewards_train/chosen': '0.11959', 'rewards_train/rejected': '0.0082606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11133', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-150.41', 'loss/train': '0.64917', 'examples_per_second': '31.604', 'grad_norm': '25.875', 'counters/examples': 114784, 'counters/updates': 3587}
train stats after 114816 examples: {'rewards_train/chosen': '0.087028', 'rewards_train/rejected': '0.071528', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0155', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-183.7', 'loss/train': '0.69484', 'examples_per_second': '31.634', 'grad_norm': '33.25', 'counters/examples': 114816, 'counters/updates': 3588}
skipping logging after 114848 examples to avoid logging too frequently
train stats after 114880 examples: {'rewards_train/chosen': '0.091195', 'rewards_train/rejected': '0.073192', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018003', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-128.69', 'loss/train': '0.69637', 'examples_per_second': '32.29', 'grad_norm': '27.125', 'counters/examples': 114880, 'counters/updates': 3590}
train stats after 114912 examples: {'rewards_train/chosen': '0.12438', 'rewards_train/rejected': '0.0022525', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12213', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-169.26', 'loss/train': '0.64467', 'examples_per_second': '30.022', 'grad_norm': '24.375', 'counters/examples': 114912, 'counters/updates': 3591}
train stats after 114944 examples: {'rewards_train/chosen': '0.10326', 'rewards_train/rejected': '0.0073773', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09588', 'logps_train/rejected': '-104.73', 'logps_train/chosen': '-155.88', 'loss/train': '0.65685', 'examples_per_second': '32.369', 'grad_norm': '28', 'counters/examples': 114944, 'counters/updates': 3592}
train stats after 114976 examples: {'rewards_train/chosen': '0.07702', 'rewards_train/rejected': '0.010552', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066468', 'logps_train/rejected': '-106.29', 'logps_train/chosen': '-118.97', 'loss/train': '0.66703', 'examples_per_second': '31.069', 'grad_norm': '24', 'counters/examples': 114976, 'counters/updates': 3593}
train stats after 115008 examples: {'rewards_train/chosen': '0.071465', 'rewards_train/rejected': '0.031707', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039757', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-122.61', 'loss/train': '0.67963', 'examples_per_second': '31.105', 'grad_norm': '29.75', 'counters/examples': 115008, 'counters/updates': 3594}
skipping logging after 115040 examples to avoid logging too frequently
train stats after 115072 examples: {'rewards_train/chosen': '0.18135', 'rewards_train/rejected': '0.057785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12357', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-151.39', 'loss/train': '0.64688', 'examples_per_second': '31.296', 'grad_norm': '25.75', 'counters/examples': 115072, 'counters/updates': 3596}
train stats after 115104 examples: {'rewards_train/chosen': '0.073013', 'rewards_train/rejected': '0.039452', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033561', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-108.79', 'loss/train': '0.68503', 'examples_per_second': '30.596', 'grad_norm': '24.25', 'counters/examples': 115104, 'counters/updates': 3597}
skipping logging after 115136 examples to avoid logging too frequently
train stats after 115168 examples: {'rewards_train/chosen': '0.03626', 'rewards_train/rejected': '-0.0049973', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041257', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-138.72', 'loss/train': '0.67773', 'examples_per_second': '31.567', 'grad_norm': '24.875', 'counters/examples': 115168, 'counters/updates': 3599}
skipping logging after 115200 examples to avoid logging too frequently
train stats after 115232 examples: {'rewards_train/chosen': '0.052536', 'rewards_train/rejected': '0.015219', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037316', 'logps_train/rejected': '-97.152', 'logps_train/chosen': '-115.86', 'loss/train': '0.67854', 'examples_per_second': '34.812', 'grad_norm': '22', 'counters/examples': 115232, 'counters/updates': 3601}
train stats after 115264 examples: {'rewards_train/chosen': '0.098578', 'rewards_train/rejected': '-0.018414', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11699', 'logps_train/rejected': '-127.92', 'logps_train/chosen': '-159.21', 'loss/train': '0.64594', 'examples_per_second': '32.958', 'grad_norm': '28.5', 'counters/examples': 115264, 'counters/updates': 3602}
train stats after 115296 examples: {'rewards_train/chosen': '0.12513', 'rewards_train/rejected': '0.024362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10076', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-145.85', 'loss/train': '0.65129', 'examples_per_second': '31.663', 'grad_norm': '33', 'counters/examples': 115296, 'counters/updates': 3603}
train stats after 115328 examples: {'rewards_train/chosen': '0.10002', 'rewards_train/rejected': '0.019659', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080358', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-102.2', 'loss/train': '0.66008', 'examples_per_second': '31.575', 'grad_norm': '24', 'counters/examples': 115328, 'counters/updates': 3604}
train stats after 115360 examples: {'rewards_train/chosen': '0.037855', 'rewards_train/rejected': '-0.021849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059704', 'logps_train/rejected': '-110.16', 'logps_train/chosen': '-165.01', 'loss/train': '0.6709', 'examples_per_second': '32.437', 'grad_norm': '27.5', 'counters/examples': 115360, 'counters/updates': 3605}
skipping logging after 115392 examples to avoid logging too frequently
train stats after 115424 examples: {'rewards_train/chosen': '0.15073', 'rewards_train/rejected': '0.057158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093576', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-137.78', 'loss/train': '0.65494', 'examples_per_second': '29.918', 'grad_norm': '25.375', 'counters/examples': 115424, 'counters/updates': 3607}
train stats after 115456 examples: {'rewards_train/chosen': '0.098317', 'rewards_train/rejected': '-0.028297', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12661', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-147.19', 'loss/train': '0.63797', 'examples_per_second': '30.321', 'grad_norm': '23.25', 'counters/examples': 115456, 'counters/updates': 3608}
skipping logging after 115488 examples to avoid logging too frequently
train stats after 115520 examples: {'rewards_train/chosen': '0.076274', 'rewards_train/rejected': '0.12284', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.046565', 'logps_train/rejected': '-144.7', 'logps_train/chosen': '-170.93', 'loss/train': '0.72322', 'examples_per_second': '30.192', 'grad_norm': '31.125', 'counters/examples': 115520, 'counters/updates': 3610}
train stats after 115552 examples: {'rewards_train/chosen': '0.14347', 'rewards_train/rejected': '-0.024131', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16761', 'logps_train/rejected': '-113.01', 'logps_train/chosen': '-158.86', 'loss/train': '0.62389', 'examples_per_second': '32.333', 'grad_norm': '24.25', 'counters/examples': 115552, 'counters/updates': 3611}
skipping logging after 115584 examples to avoid logging too frequently
train stats after 115616 examples: {'rewards_train/chosen': '0.14229', 'rewards_train/rejected': '0.031638', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11066', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-135.42', 'loss/train': '0.64581', 'examples_per_second': '31.515', 'grad_norm': '31', 'counters/examples': 115616, 'counters/updates': 3613}
train stats after 115648 examples: {'rewards_train/chosen': '0.11052', 'rewards_train/rejected': '0.064181', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04634', 'logps_train/rejected': '-109.33', 'logps_train/chosen': '-120.61', 'loss/train': '0.67736', 'examples_per_second': '31.354', 'grad_norm': '25.25', 'counters/examples': 115648, 'counters/updates': 3614}
train stats after 115680 examples: {'rewards_train/chosen': '0.13373', 'rewards_train/rejected': '0.12899', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0047324', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-158.29', 'loss/train': '0.69777', 'examples_per_second': '30.836', 'grad_norm': '34.5', 'counters/examples': 115680, 'counters/updates': 3615}
train stats after 115712 examples: {'rewards_train/chosen': '0.068622', 'rewards_train/rejected': '0.028858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039764', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-139.58', 'loss/train': '0.68007', 'examples_per_second': '30.155', 'grad_norm': '27.375', 'counters/examples': 115712, 'counters/updates': 3616}
train stats after 115744 examples: {'rewards_train/chosen': '0.051401', 'rewards_train/rejected': '-0.01287', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064271', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-106.4', 'loss/train': '0.66914', 'examples_per_second': '31.201', 'grad_norm': '24.125', 'counters/examples': 115744, 'counters/updates': 3617}
train stats after 115776 examples: {'rewards_train/chosen': '0.15697', 'rewards_train/rejected': '0.061554', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.095417', 'logps_train/rejected': '-139.69', 'logps_train/chosen': '-193.5', 'loss/train': '0.65473', 'examples_per_second': '31.384', 'grad_norm': '30.75', 'counters/examples': 115776, 'counters/updates': 3618}
train stats after 115808 examples: {'rewards_train/chosen': '0.19056', 'rewards_train/rejected': '0.00036447', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1902', 'logps_train/rejected': '-165.33', 'logps_train/chosen': '-157.55', 'loss/train': '0.61417', 'examples_per_second': '24.276', 'grad_norm': '26.375', 'counters/examples': 115808, 'counters/updates': 3619}
train stats after 115840 examples: {'rewards_train/chosen': '0.094259', 'rewards_train/rejected': '-0.055554', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14981', 'logps_train/rejected': '-104.77', 'logps_train/chosen': '-134.46', 'loss/train': '0.62646', 'examples_per_second': '32.172', 'grad_norm': '24.875', 'counters/examples': 115840, 'counters/updates': 3620}
skipping logging after 115872 examples to avoid logging too frequently
train stats after 115904 examples: {'rewards_train/chosen': '0.13972', 'rewards_train/rejected': '0.021651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11807', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-174.1', 'loss/train': '0.64596', 'examples_per_second': '31.43', 'grad_norm': '27.375', 'counters/examples': 115904, 'counters/updates': 3622}
skipping logging after 115936 examples to avoid logging too frequently
train stats after 115968 examples: {'rewards_train/chosen': '0.10857', 'rewards_train/rejected': '-0.0014907', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11006', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-151.92', 'loss/train': '0.64641', 'examples_per_second': '30.086', 'grad_norm': '26.625', 'counters/examples': 115968, 'counters/updates': 3624}
train stats after 116000 examples: {'rewards_train/chosen': '0.10287', 'rewards_train/rejected': '0.042846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060024', 'logps_train/rejected': '-119.94', 'logps_train/chosen': '-140.78', 'loss/train': '0.67474', 'examples_per_second': '30.962', 'grad_norm': '29.625', 'counters/examples': 116000, 'counters/updates': 3625}
train stats after 116032 examples: {'rewards_train/chosen': '0.067749', 'rewards_train/rejected': '0.061757', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0059924', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-117.97', 'loss/train': '0.70102', 'examples_per_second': '30.615', 'grad_norm': '28', 'counters/examples': 116032, 'counters/updates': 3626}
train stats after 116064 examples: {'rewards_train/chosen': '0.11453', 'rewards_train/rejected': '0.061316', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053216', 'logps_train/rejected': '-107.91', 'logps_train/chosen': '-123.73', 'loss/train': '0.67162', 'examples_per_second': '31.326', 'grad_norm': '23.875', 'counters/examples': 116064, 'counters/updates': 3627}
train stats after 116096 examples: {'rewards_train/chosen': '0.1814', 'rewards_train/rejected': '0.095252', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086147', 'logps_train/rejected': '-149.1', 'logps_train/chosen': '-149.46', 'loss/train': '0.65531', 'examples_per_second': '31.516', 'grad_norm': '28', 'counters/examples': 116096, 'counters/updates': 3628}
skipping logging after 116128 examples to avoid logging too frequently
train stats after 116160 examples: {'rewards_train/chosen': '0.11416', 'rewards_train/rejected': '0.039045', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075113', 'logps_train/rejected': '-125.44', 'logps_train/chosen': '-123.07', 'loss/train': '0.66251', 'examples_per_second': '30.594', 'grad_norm': '25.875', 'counters/examples': 116160, 'counters/updates': 3630}
train stats after 116192 examples: {'rewards_train/chosen': '0.11703', 'rewards_train/rejected': '0.0071022', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10993', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-127.91', 'loss/train': '0.64751', 'examples_per_second': '30.072', 'grad_norm': '32.5', 'counters/examples': 116192, 'counters/updates': 3631}
train stats after 116224 examples: {'rewards_train/chosen': '0.18054', 'rewards_train/rejected': '0.042988', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13756', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-154.23', 'loss/train': '0.63314', 'examples_per_second': '30.744', 'grad_norm': '24.5', 'counters/examples': 116224, 'counters/updates': 3632}
skipping logging after 116256 examples to avoid logging too frequently
train stats after 116288 examples: {'rewards_train/chosen': '0.041435', 'rewards_train/rejected': '0.064081', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022647', 'logps_train/rejected': '-137.41', 'logps_train/chosen': '-140.92', 'loss/train': '0.7111', 'examples_per_second': '30.093', 'grad_norm': '31.25', 'counters/examples': 116288, 'counters/updates': 3634}
train stats after 116320 examples: {'rewards_train/chosen': '0.079601', 'rewards_train/rejected': '0.02401', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055592', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-143.71', 'loss/train': '0.67362', 'examples_per_second': '31.611', 'grad_norm': '28.375', 'counters/examples': 116320, 'counters/updates': 3635}
train stats after 116352 examples: {'rewards_train/chosen': '0.048599', 'rewards_train/rejected': '0.028755', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019844', 'logps_train/rejected': '-98.103', 'logps_train/chosen': '-125.06', 'loss/train': '0.68797', 'examples_per_second': '32.255', 'grad_norm': '26.25', 'counters/examples': 116352, 'counters/updates': 3636}
train stats after 116384 examples: {'rewards_train/chosen': '0.097371', 'rewards_train/rejected': '0.125', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.027629', 'logps_train/rejected': '-153.05', 'logps_train/chosen': '-129.95', 'loss/train': '0.72161', 'examples_per_second': '29.918', 'grad_norm': '30', 'counters/examples': 116384, 'counters/updates': 3637}
train stats after 116416 examples: {'rewards_train/chosen': '0.17603', 'rewards_train/rejected': '0.040045', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13599', 'logps_train/rejected': '-132.36', 'logps_train/chosen': '-163.55', 'loss/train': '0.64035', 'examples_per_second': '31.614', 'grad_norm': '39', 'counters/examples': 116416, 'counters/updates': 3638}
train stats after 116448 examples: {'rewards_train/chosen': '0.11082', 'rewards_train/rejected': '0.0197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091123', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-126.34', 'loss/train': '0.6609', 'examples_per_second': '31.979', 'grad_norm': '25', 'counters/examples': 116448, 'counters/updates': 3639}
train stats after 116480 examples: {'rewards_train/chosen': '0.086505', 'rewards_train/rejected': '0.028772', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057732', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-141.6', 'loss/train': '0.67213', 'examples_per_second': '30.544', 'grad_norm': '26.75', 'counters/examples': 116480, 'counters/updates': 3640}
skipping logging after 116512 examples to avoid logging too frequently
train stats after 116544 examples: {'rewards_train/chosen': '0.16625', 'rewards_train/rejected': '0.023724', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14253', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-158.63', 'loss/train': '0.64397', 'examples_per_second': '30.087', 'grad_norm': '24.625', 'counters/examples': 116544, 'counters/updates': 3642}
train stats after 116576 examples: {'rewards_train/chosen': '0.086866', 'rewards_train/rejected': '0.038254', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048612', 'logps_train/rejected': '-100.31', 'logps_train/chosen': '-121.88', 'loss/train': '0.67642', 'examples_per_second': '31.629', 'grad_norm': '28.75', 'counters/examples': 116576, 'counters/updates': 3643}
train stats after 116608 examples: {'rewards_train/chosen': '0.11372', 'rewards_train/rejected': '-0.033657', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14737', 'logps_train/rejected': '-105.62', 'logps_train/chosen': '-112.77', 'loss/train': '0.62871', 'examples_per_second': '31.851', 'grad_norm': '22.125', 'counters/examples': 116608, 'counters/updates': 3644}
train stats after 116640 examples: {'rewards_train/chosen': '0.080788', 'rewards_train/rejected': '0.0023719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078417', 'logps_train/rejected': '-107.55', 'logps_train/chosen': '-140.06', 'loss/train': '0.66012', 'examples_per_second': '31.591', 'grad_norm': '28', 'counters/examples': 116640, 'counters/updates': 3645}
train stats after 116672 examples: {'rewards_train/chosen': '0.072808', 'rewards_train/rejected': '0.016556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056253', 'logps_train/rejected': '-97.473', 'logps_train/chosen': '-114.89', 'loss/train': '0.66982', 'examples_per_second': '32.511', 'grad_norm': '23.5', 'counters/examples': 116672, 'counters/updates': 3646}
skipping logging after 116704 examples to avoid logging too frequently
train stats after 116736 examples: {'rewards_train/chosen': '0.17144', 'rewards_train/rejected': '0.027805', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14363', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-138.5', 'loss/train': '0.63437', 'examples_per_second': '30.965', 'grad_norm': '25.125', 'counters/examples': 116736, 'counters/updates': 3648}
train stats after 116768 examples: {'rewards_train/chosen': '0.13838', 'rewards_train/rejected': '0.071909', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.066469', 'logps_train/rejected': '-142.43', 'logps_train/chosen': '-112.35', 'loss/train': '0.66519', 'examples_per_second': '31.444', 'grad_norm': '24.625', 'counters/examples': 116768, 'counters/updates': 3649}
skipping logging after 116800 examples to avoid logging too frequently
train stats after 116832 examples: {'rewards_train/chosen': '0.15289', 'rewards_train/rejected': '0.054374', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098515', 'logps_train/rejected': '-123.31', 'logps_train/chosen': '-157.32', 'loss/train': '0.65216', 'examples_per_second': '31.086', 'grad_norm': '26.5', 'counters/examples': 116832, 'counters/updates': 3651}
train stats after 116864 examples: {'rewards_train/chosen': '0.097729', 'rewards_train/rejected': '0.08542', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01231', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-101.21', 'loss/train': '0.69905', 'examples_per_second': '31.552', 'grad_norm': '29.75', 'counters/examples': 116864, 'counters/updates': 3652}
train stats after 116896 examples: {'rewards_train/chosen': '0.20107', 'rewards_train/rejected': '-0.0026884', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20376', 'logps_train/rejected': '-117.78', 'logps_train/chosen': '-132.98', 'loss/train': '0.60781', 'examples_per_second': '30.286', 'grad_norm': '24.25', 'counters/examples': 116896, 'counters/updates': 3653}
train stats after 116928 examples: {'rewards_train/chosen': '0.06858', 'rewards_train/rejected': '0.040197', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028383', 'logps_train/rejected': '-132.84', 'logps_train/chosen': '-136.77', 'loss/train': '0.68649', 'examples_per_second': '31.641', 'grad_norm': '31.375', 'counters/examples': 116928, 'counters/updates': 3654}
train stats after 116960 examples: {'rewards_train/chosen': '0.090201', 'rewards_train/rejected': '0.089795', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00040582', 'logps_train/rejected': '-150.45', 'logps_train/chosen': '-131.83', 'loss/train': '0.70063', 'examples_per_second': '31.68', 'grad_norm': '47.25', 'counters/examples': 116960, 'counters/updates': 3655}
train stats after 116992 examples: {'rewards_train/chosen': '0.20498', 'rewards_train/rejected': '0.031253', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17373', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-163.18', 'loss/train': '0.6216', 'examples_per_second': '31.452', 'grad_norm': '26.125', 'counters/examples': 116992, 'counters/updates': 3656}
train stats after 117024 examples: {'rewards_train/chosen': '0.03556', 'rewards_train/rejected': '0.060862', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025302', 'logps_train/rejected': '-101.95', 'logps_train/chosen': '-146.21', 'loss/train': '0.71585', 'examples_per_second': '32.172', 'grad_norm': '26.625', 'counters/examples': 117024, 'counters/updates': 3657}
train stats after 117056 examples: {'rewards_train/chosen': '0.077772', 'rewards_train/rejected': '0.00462', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073152', 'logps_train/rejected': '-100.06', 'logps_train/chosen': '-136.56', 'loss/train': '0.66495', 'examples_per_second': '31.593', 'grad_norm': '23', 'counters/examples': 117056, 'counters/updates': 3658}
train stats after 117088 examples: {'rewards_train/chosen': '0.044188', 'rewards_train/rejected': '0.02592', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018268', 'logps_train/rejected': '-104.77', 'logps_train/chosen': '-118.77', 'loss/train': '0.6889', 'examples_per_second': '31.139', 'grad_norm': '26.375', 'counters/examples': 117088, 'counters/updates': 3659}
train stats after 117120 examples: {'rewards_train/chosen': '0.091588', 'rewards_train/rejected': '0.002895', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088693', 'logps_train/rejected': '-142.34', 'logps_train/chosen': '-158.36', 'loss/train': '0.66432', 'examples_per_second': '32.922', 'grad_norm': '26.875', 'counters/examples': 117120, 'counters/updates': 3660}
skipping logging after 117152 examples to avoid logging too frequently
train stats after 117184 examples: {'rewards_train/chosen': '0.084792', 'rewards_train/rejected': '-0.019358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10415', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-116.99', 'loss/train': '0.65019', 'examples_per_second': '30.92', 'grad_norm': '24.5', 'counters/examples': 117184, 'counters/updates': 3662}
train stats after 117216 examples: {'rewards_train/chosen': '0.006886', 'rewards_train/rejected': '0.026483', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019597', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-137.52', 'loss/train': '0.70882', 'examples_per_second': '32.184', 'grad_norm': '28.5', 'counters/examples': 117216, 'counters/updates': 3663}
train stats after 117248 examples: {'rewards_train/chosen': '0.061827', 'rewards_train/rejected': '0.012859', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048968', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-116.82', 'loss/train': '0.67429', 'examples_per_second': '30.109', 'grad_norm': '23', 'counters/examples': 117248, 'counters/updates': 3664}
train stats after 117280 examples: {'rewards_train/chosen': '0.11731', 'rewards_train/rejected': '0.030087', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087225', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-163.01', 'loss/train': '0.65966', 'examples_per_second': '30.301', 'grad_norm': '28.125', 'counters/examples': 117280, 'counters/updates': 3665}
train stats after 117312 examples: {'rewards_train/chosen': '0.12349', 'rewards_train/rejected': '-0.0030149', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12651', 'logps_train/rejected': '-116.55', 'logps_train/chosen': '-133.71', 'loss/train': '0.63926', 'examples_per_second': '31.66', 'grad_norm': '26', 'counters/examples': 117312, 'counters/updates': 3666}
train stats after 117344 examples: {'rewards_train/chosen': '0.099295', 'rewards_train/rejected': '-0.026517', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12581', 'logps_train/rejected': '-99.268', 'logps_train/chosen': '-135.91', 'loss/train': '0.63798', 'examples_per_second': '30.205', 'grad_norm': '26.875', 'counters/examples': 117344, 'counters/updates': 3667}
train stats after 117376 examples: {'rewards_train/chosen': '0.031147', 'rewards_train/rejected': '0.088241', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.057093', 'logps_train/rejected': '-123.86', 'logps_train/chosen': '-174.09', 'loss/train': '0.73604', 'examples_per_second': '32.515', 'grad_norm': '39.25', 'counters/examples': 117376, 'counters/updates': 3668}
skipping logging after 117408 examples to avoid logging too frequently
train stats after 117440 examples: {'rewards_train/chosen': '0.059711', 'rewards_train/rejected': '0.006286', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053425', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-115.66', 'loss/train': '0.67149', 'examples_per_second': '31.644', 'grad_norm': '24', 'counters/examples': 117440, 'counters/updates': 3670}
train stats after 117472 examples: {'rewards_train/chosen': '0.046845', 'rewards_train/rejected': '0.042769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0040767', 'logps_train/rejected': '-144.03', 'logps_train/chosen': '-151.35', 'loss/train': '0.69831', 'examples_per_second': '31.442', 'grad_norm': '35.25', 'counters/examples': 117472, 'counters/updates': 3671}
skipping logging after 117504 examples to avoid logging too frequently
train stats after 117536 examples: {'rewards_train/chosen': '0.067312', 'rewards_train/rejected': '0.0074379', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059874', 'logps_train/rejected': '-109.91', 'logps_train/chosen': '-147.09', 'loss/train': '0.67603', 'examples_per_second': '32.661', 'grad_norm': '29.75', 'counters/examples': 117536, 'counters/updates': 3673}
train stats after 117568 examples: {'rewards_train/chosen': '0.034986', 'rewards_train/rejected': '-0.034467', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069454', 'logps_train/rejected': '-98.948', 'logps_train/chosen': '-111.72', 'loss/train': '0.66725', 'examples_per_second': '32.823', 'grad_norm': '22.625', 'counters/examples': 117568, 'counters/updates': 3674}
train stats after 117600 examples: {'rewards_train/chosen': '0.16234', 'rewards_train/rejected': '0.067873', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094466', 'logps_train/rejected': '-100.94', 'logps_train/chosen': '-163.07', 'loss/train': '0.66003', 'examples_per_second': '31.657', 'grad_norm': '27.75', 'counters/examples': 117600, 'counters/updates': 3675}
skipping logging after 117632 examples to avoid logging too frequently
train stats after 117664 examples: {'rewards_train/chosen': '0.11029', 'rewards_train/rejected': '-0.011687', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12197', 'logps_train/rejected': '-100.22', 'logps_train/chosen': '-110.55', 'loss/train': '0.63901', 'examples_per_second': '30.697', 'grad_norm': '21.625', 'counters/examples': 117664, 'counters/updates': 3677}
train stats after 117696 examples: {'rewards_train/chosen': '0.121', 'rewards_train/rejected': '0.018406', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10259', 'logps_train/rejected': '-123.02', 'logps_train/chosen': '-166.53', 'loss/train': '0.65374', 'examples_per_second': '30.603', 'grad_norm': '28.5', 'counters/examples': 117696, 'counters/updates': 3678}
skipping logging after 117728 examples to avoid logging too frequently
train stats after 117760 examples: {'rewards_train/chosen': '0.10217', 'rewards_train/rejected': '0.03114', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071028', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-122.69', 'loss/train': '0.67071', 'examples_per_second': '31.586', 'grad_norm': '26', 'counters/examples': 117760, 'counters/updates': 3680}
skipping logging after 117792 examples to avoid logging too frequently
train stats after 117824 examples: {'rewards_train/chosen': '0.1594', 'rewards_train/rejected': '-0.027529', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18693', 'logps_train/rejected': '-107.24', 'logps_train/chosen': '-139.3', 'loss/train': '0.61263', 'examples_per_second': '30.612', 'grad_norm': '22.875', 'counters/examples': 117824, 'counters/updates': 3682}
skipping logging after 117856 examples to avoid logging too frequently
train stats after 117888 examples: {'rewards_train/chosen': '0.12451', 'rewards_train/rejected': '0.069682', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054828', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-154.05', 'loss/train': '0.66912', 'examples_per_second': '30.34', 'grad_norm': '32.5', 'counters/examples': 117888, 'counters/updates': 3684}
train stats after 117920 examples: {'rewards_train/chosen': '0.067692', 'rewards_train/rejected': '0.11004', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042344', 'logps_train/rejected': '-144.42', 'logps_train/chosen': '-120.48', 'loss/train': '0.72222', 'examples_per_second': '31.597', 'grad_norm': '29.75', 'counters/examples': 117920, 'counters/updates': 3685}
train stats after 117952 examples: {'rewards_train/chosen': '0.056804', 'rewards_train/rejected': '0.00087458', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05593', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-119.32', 'loss/train': '0.6725', 'examples_per_second': '32.332', 'grad_norm': '24.75', 'counters/examples': 117952, 'counters/updates': 3686}
train stats after 117984 examples: {'rewards_train/chosen': '0.088351', 'rewards_train/rejected': '-0.012653', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.101', 'logps_train/rejected': '-104.02', 'logps_train/chosen': '-115.22', 'loss/train': '0.64835', 'examples_per_second': '31.914', 'grad_norm': '23.75', 'counters/examples': 117984, 'counters/updates': 3687}
train stats after 118016 examples: {'rewards_train/chosen': '0.034895', 'rewards_train/rejected': '0.026021', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0088733', 'logps_train/rejected': '-118.22', 'logps_train/chosen': '-123.85', 'loss/train': '0.69927', 'examples_per_second': '31.768', 'grad_norm': '28.125', 'counters/examples': 118016, 'counters/updates': 3688}
train stats after 118048 examples: {'rewards_train/chosen': '0.051475', 'rewards_train/rejected': '0.03529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016185', 'logps_train/rejected': '-101.28', 'logps_train/chosen': '-107.19', 'loss/train': '0.69228', 'examples_per_second': '31.482', 'grad_norm': '27.5', 'counters/examples': 118048, 'counters/updates': 3689}
train stats after 118080 examples: {'rewards_train/chosen': '0.14461', 'rewards_train/rejected': '0.090972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05364', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-144.91', 'loss/train': '0.68', 'examples_per_second': '29.918', 'grad_norm': '26.25', 'counters/examples': 118080, 'counters/updates': 3690}
train stats after 118112 examples: {'rewards_train/chosen': '0.086023', 'rewards_train/rejected': '0.058377', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027646', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-138.67', 'loss/train': '0.68497', 'examples_per_second': '30.667', 'grad_norm': '26.875', 'counters/examples': 118112, 'counters/updates': 3691}
skipping logging after 118144 examples to avoid logging too frequently
train stats after 118176 examples: {'rewards_train/chosen': '0.055297', 'rewards_train/rejected': '-0.00010315', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055401', 'logps_train/rejected': '-134.43', 'logps_train/chosen': '-102.47', 'loss/train': '0.67251', 'examples_per_second': '31.653', 'grad_norm': '26.125', 'counters/examples': 118176, 'counters/updates': 3693}
train stats after 118208 examples: {'rewards_train/chosen': '0.15006', 'rewards_train/rejected': '0.098588', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051467', 'logps_train/rejected': '-157.09', 'logps_train/chosen': '-164.34', 'loss/train': '0.67481', 'examples_per_second': '31.348', 'grad_norm': '31.375', 'counters/examples': 118208, 'counters/updates': 3694}
skipping logging after 118240 examples to avoid logging too frequently
train stats after 118272 examples: {'rewards_train/chosen': '0.1722', 'rewards_train/rejected': '0.055017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11718', 'logps_train/rejected': '-93.088', 'logps_train/chosen': '-150.59', 'loss/train': '0.64554', 'examples_per_second': '30.094', 'grad_norm': '30', 'counters/examples': 118272, 'counters/updates': 3696}
skipping logging after 118304 examples to avoid logging too frequently
train stats after 118336 examples: {'rewards_train/chosen': '0.10397', 'rewards_train/rejected': '0.052551', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051421', 'logps_train/rejected': '-134.62', 'logps_train/chosen': '-128.61', 'loss/train': '0.67525', 'examples_per_second': '31.198', 'grad_norm': '26.25', 'counters/examples': 118336, 'counters/updates': 3698}
train stats after 118368 examples: {'rewards_train/chosen': '0.14294', 'rewards_train/rejected': '0.035365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10757', 'logps_train/rejected': '-110.67', 'logps_train/chosen': '-147.48', 'loss/train': '0.64789', 'examples_per_second': '31.905', 'grad_norm': '25.25', 'counters/examples': 118368, 'counters/updates': 3699}
train stats after 118400 examples: {'rewards_train/chosen': '0.089515', 'rewards_train/rejected': '0.003075', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08644', 'logps_train/rejected': '-116.67', 'logps_train/chosen': '-110.98', 'loss/train': '0.6559', 'examples_per_second': '31.933', 'grad_norm': '25.75', 'counters/examples': 118400, 'counters/updates': 3700}
skipping logging after 118432 examples to avoid logging too frequently
train stats after 118464 examples: {'rewards_train/chosen': '0.059001', 'rewards_train/rejected': '0.053222', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.005779', 'logps_train/rejected': '-157.75', 'logps_train/chosen': '-140.83', 'loss/train': '0.70145', 'examples_per_second': '31.259', 'grad_norm': '30.25', 'counters/examples': 118464, 'counters/updates': 3702}
train stats after 118496 examples: {'rewards_train/chosen': '0.17144', 'rewards_train/rejected': '0.10033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07111', 'logps_train/rejected': '-113.74', 'logps_train/chosen': '-138.56', 'loss/train': '0.67302', 'examples_per_second': '31.524', 'grad_norm': '25.875', 'counters/examples': 118496, 'counters/updates': 3703}
skipping logging after 118528 examples to avoid logging too frequently
train stats after 118560 examples: {'rewards_train/chosen': '0.047176', 'rewards_train/rejected': '-0.0016975', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048873', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-128.4', 'loss/train': '0.67492', 'examples_per_second': '33.689', 'grad_norm': '24.5', 'counters/examples': 118560, 'counters/updates': 3705}
train stats after 118592 examples: {'rewards_train/chosen': '0.068508', 'rewards_train/rejected': '-0.039838', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10835', 'logps_train/rejected': '-103.6', 'logps_train/chosen': '-100.5', 'loss/train': '0.65019', 'examples_per_second': '31.638', 'grad_norm': '24', 'counters/examples': 118592, 'counters/updates': 3706}
train stats after 118624 examples: {'rewards_train/chosen': '0.10952', 'rewards_train/rejected': '0.048726', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06079', 'logps_train/rejected': '-101.4', 'logps_train/chosen': '-142.43', 'loss/train': '0.66996', 'examples_per_second': '32.212', 'grad_norm': '25.375', 'counters/examples': 118624, 'counters/updates': 3707}
skipping logging after 118656 examples to avoid logging too frequently
train stats after 118688 examples: {'rewards_train/chosen': '0.026682', 'rewards_train/rejected': '0.036814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010132', 'logps_train/rejected': '-101.44', 'logps_train/chosen': '-123.91', 'loss/train': '0.70503', 'examples_per_second': '35.621', 'grad_norm': '25.125', 'counters/examples': 118688, 'counters/updates': 3709}
train stats after 118720 examples: {'rewards_train/chosen': '0.10888', 'rewards_train/rejected': '0.020246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088631', 'logps_train/rejected': '-156.41', 'logps_train/chosen': '-148.26', 'loss/train': '0.6562', 'examples_per_second': '33.074', 'grad_norm': '27.625', 'counters/examples': 118720, 'counters/updates': 3710}
train stats after 118752 examples: {'rewards_train/chosen': '0.11222', 'rewards_train/rejected': '0.077515', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.034709', 'logps_train/rejected': '-114.85', 'logps_train/chosen': '-161.51', 'loss/train': '0.68688', 'examples_per_second': '31.658', 'grad_norm': '26', 'counters/examples': 118752, 'counters/updates': 3711}
train stats after 118784 examples: {'rewards_train/chosen': '0.058661', 'rewards_train/rejected': '0.0070827', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051578', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-139.5', 'loss/train': '0.67383', 'examples_per_second': '31.601', 'grad_norm': '28.5', 'counters/examples': 118784, 'counters/updates': 3712}
skipping logging after 118816 examples to avoid logging too frequently
train stats after 118848 examples: {'rewards_train/chosen': '0.13388', 'rewards_train/rejected': '-0.00059076', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13447', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-159.53', 'loss/train': '0.64371', 'examples_per_second': '31.039', 'grad_norm': '28', 'counters/examples': 118848, 'counters/updates': 3714}
train stats after 118880 examples: {'rewards_train/chosen': '0.051972', 'rewards_train/rejected': '0.019328', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032643', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-136.02', 'loss/train': '0.68646', 'examples_per_second': '31.682', 'grad_norm': '27.25', 'counters/examples': 118880, 'counters/updates': 3715}
skipping logging after 118912 examples to avoid logging too frequently
train stats after 118944 examples: {'rewards_train/chosen': '0.10923', 'rewards_train/rejected': '0.060984', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048241', 'logps_train/rejected': '-90.472', 'logps_train/chosen': '-112.58', 'loss/train': '0.67296', 'examples_per_second': '33.255', 'grad_norm': '24', 'counters/examples': 118944, 'counters/updates': 3717}
train stats after 118976 examples: {'rewards_train/chosen': '0.10093', 'rewards_train/rejected': '-0.0020242', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10295', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-145.8', 'loss/train': '0.65297', 'examples_per_second': '30.283', 'grad_norm': '28.25', 'counters/examples': 118976, 'counters/updates': 3718}
train stats after 119008 examples: {'rewards_train/chosen': '0.18143', 'rewards_train/rejected': '0.032915', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14852', 'logps_train/rejected': '-86.889', 'logps_train/chosen': '-141.4', 'loss/train': '0.63105', 'examples_per_second': '31.314', 'grad_norm': '26.125', 'counters/examples': 119008, 'counters/updates': 3719}
skipping logging after 119040 examples to avoid logging too frequently
train stats after 119072 examples: {'rewards_train/chosen': '0.04838', 'rewards_train/rejected': '-0.018961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06734', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-134.54', 'loss/train': '0.6686', 'examples_per_second': '35.852', 'grad_norm': '24.375', 'counters/examples': 119072, 'counters/updates': 3721}
train stats after 119104 examples: {'rewards_train/chosen': '0.098339', 'rewards_train/rejected': '-0.021498', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11984', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-120.87', 'loss/train': '0.64335', 'examples_per_second': '26.429', 'grad_norm': '22.125', 'counters/examples': 119104, 'counters/updates': 3722}
train stats after 119136 examples: {'rewards_train/chosen': '0.080526', 'rewards_train/rejected': '-0.028292', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10882', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-141.79', 'loss/train': '0.64783', 'examples_per_second': '30.904', 'grad_norm': '26.25', 'counters/examples': 119136, 'counters/updates': 3723}
train stats after 119168 examples: {'rewards_train/chosen': '0.10928', 'rewards_train/rejected': '0.07505', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.034235', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-129.17', 'loss/train': '0.67971', 'examples_per_second': '31.535', 'grad_norm': '24.875', 'counters/examples': 119168, 'counters/updates': 3724}
train stats after 119200 examples: {'rewards_train/chosen': '0.096317', 'rewards_train/rejected': '-0.0078791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1042', 'logps_train/rejected': '-103.61', 'logps_train/chosen': '-133.85', 'loss/train': '0.64998', 'examples_per_second': '24.854', 'grad_norm': '22.875', 'counters/examples': 119200, 'counters/updates': 3725}
train stats after 119232 examples: {'rewards_train/chosen': '0.11449', 'rewards_train/rejected': '0.07026', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044234', 'logps_train/rejected': '-140.08', 'logps_train/chosen': '-154.46', 'loss/train': '0.67776', 'examples_per_second': '30.648', 'grad_norm': '28.875', 'counters/examples': 119232, 'counters/updates': 3726}
train stats after 119264 examples: {'rewards_train/chosen': '0.15943', 'rewards_train/rejected': '0.0072753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15216', 'logps_train/rejected': '-127.58', 'logps_train/chosen': '-158.36', 'loss/train': '0.63303', 'examples_per_second': '31.601', 'grad_norm': '24.75', 'counters/examples': 119264, 'counters/updates': 3727}
train stats after 119296 examples: {'rewards_train/chosen': '0.12321', 'rewards_train/rejected': '0.040462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082752', 'logps_train/rejected': '-148.05', 'logps_train/chosen': '-127.14', 'loss/train': '0.66275', 'examples_per_second': '30.137', 'grad_norm': '26.75', 'counters/examples': 119296, 'counters/updates': 3728}
train stats after 119328 examples: {'rewards_train/chosen': '0.025067', 'rewards_train/rejected': '0.014674', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010393', 'logps_train/rejected': '-100.33', 'logps_train/chosen': '-127.62', 'loss/train': '0.69833', 'examples_per_second': '32.581', 'grad_norm': '27', 'counters/examples': 119328, 'counters/updates': 3729}
skipping logging after 119360 examples to avoid logging too frequently
train stats after 119392 examples: {'rewards_train/chosen': '0.071042', 'rewards_train/rejected': '0.048834', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.022208', 'logps_train/rejected': '-140.44', 'logps_train/chosen': '-129.62', 'loss/train': '0.69098', 'examples_per_second': '33.324', 'grad_norm': '28.25', 'counters/examples': 119392, 'counters/updates': 3731}
train stats after 119424 examples: {'rewards_train/chosen': '0.12586', 'rewards_train/rejected': '0.020888', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10497', 'logps_train/rejected': '-114.2', 'logps_train/chosen': '-169.49', 'loss/train': '0.65027', 'examples_per_second': '31.604', 'grad_norm': '29.625', 'counters/examples': 119424, 'counters/updates': 3732}
skipping logging after 119456 examples to avoid logging too frequently
train stats after 119488 examples: {'rewards_train/chosen': '0.13834', 'rewards_train/rejected': '0.036425', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10191', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-150.08', 'loss/train': '0.6504', 'examples_per_second': '31.652', 'grad_norm': '24.625', 'counters/examples': 119488, 'counters/updates': 3734}
train stats after 119520 examples: {'rewards_train/chosen': '0.1194', 'rewards_train/rejected': '0.086417', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032987', 'logps_train/rejected': '-127.93', 'logps_train/chosen': '-138.66', 'loss/train': '0.69172', 'examples_per_second': '31.021', 'grad_norm': '44.5', 'counters/examples': 119520, 'counters/updates': 3735}
train stats after 119552 examples: {'rewards_train/chosen': '0.033372', 'rewards_train/rejected': '0.046517', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013145', 'logps_train/rejected': '-96.699', 'logps_train/chosen': '-145.27', 'loss/train': '0.71753', 'examples_per_second': '31.779', 'grad_norm': '27.25', 'counters/examples': 119552, 'counters/updates': 3736}
train stats after 119584 examples: {'rewards_train/chosen': '0.051182', 'rewards_train/rejected': '0.018217', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032964', 'logps_train/rejected': '-131.32', 'logps_train/chosen': '-170.1', 'loss/train': '0.68507', 'examples_per_second': '31.183', 'grad_norm': '31.5', 'counters/examples': 119584, 'counters/updates': 3737}
train stats after 119616 examples: {'rewards_train/chosen': '0.19819', 'rewards_train/rejected': '0.044365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15382', 'logps_train/rejected': '-139.73', 'logps_train/chosen': '-178.52', 'loss/train': '0.63159', 'examples_per_second': '31.024', 'grad_norm': '27.875', 'counters/examples': 119616, 'counters/updates': 3738}
train stats after 119648 examples: {'rewards_train/chosen': '0.062591', 'rewards_train/rejected': '0.0062577', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056333', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-135.51', 'loss/train': '0.6725', 'examples_per_second': '32.463', 'grad_norm': '24.625', 'counters/examples': 119648, 'counters/updates': 3739}
train stats after 119680 examples: {'rewards_train/chosen': '0.082055', 'rewards_train/rejected': '-0.020123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10218', 'logps_train/rejected': '-146.89', 'logps_train/chosen': '-144.81', 'loss/train': '0.66111', 'examples_per_second': '32.326', 'grad_norm': '29.75', 'counters/examples': 119680, 'counters/updates': 3740}
train stats after 119712 examples: {'rewards_train/chosen': '0.10827', 'rewards_train/rejected': '-0.03048', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13875', 'logps_train/rejected': '-102.95', 'logps_train/chosen': '-166.86', 'loss/train': '0.6324', 'examples_per_second': '30.677', 'grad_norm': '25', 'counters/examples': 119712, 'counters/updates': 3741}
train stats after 119744 examples: {'rewards_train/chosen': '0.15984', 'rewards_train/rejected': '0.040152', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11968', 'logps_train/rejected': '-133.68', 'logps_train/chosen': '-152.81', 'loss/train': '0.64639', 'examples_per_second': '30.43', 'grad_norm': '30.125', 'counters/examples': 119744, 'counters/updates': 3742}
train stats after 119776 examples: {'rewards_train/chosen': '0.093114', 'rewards_train/rejected': '0.017995', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075119', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-139.33', 'loss/train': '0.66955', 'examples_per_second': '32.164', 'grad_norm': '27.25', 'counters/examples': 119776, 'counters/updates': 3743}
train stats after 119808 examples: {'rewards_train/chosen': '0.12139', 'rewards_train/rejected': '0.069342', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.052047', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-146.43', 'loss/train': '0.68522', 'examples_per_second': '31.509', 'grad_norm': '32.75', 'counters/examples': 119808, 'counters/updates': 3744}
train stats after 119840 examples: {'rewards_train/chosen': '0.17047', 'rewards_train/rejected': '0.064737', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10573', 'logps_train/rejected': '-145.87', 'logps_train/chosen': '-150.97', 'loss/train': '0.64833', 'examples_per_second': '30.317', 'grad_norm': '24.625', 'counters/examples': 119840, 'counters/updates': 3745}
train stats after 119872 examples: {'rewards_train/chosen': '0.086693', 'rewards_train/rejected': '0.016004', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070689', 'logps_train/rejected': '-117.34', 'logps_train/chosen': '-138.69', 'loss/train': '0.66691', 'examples_per_second': '31.465', 'grad_norm': '24.625', 'counters/examples': 119872, 'counters/updates': 3746}
train stats after 119904 examples: {'rewards_train/chosen': '0.13727', 'rewards_train/rejected': '0.02571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11156', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-151.71', 'loss/train': '0.65007', 'examples_per_second': '30.105', 'grad_norm': '27.875', 'counters/examples': 119904, 'counters/updates': 3747}
train stats after 119936 examples: {'rewards_train/chosen': '0.10183', 'rewards_train/rejected': '0.054342', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04749', 'logps_train/rejected': '-110.58', 'logps_train/chosen': '-141.05', 'loss/train': '0.68166', 'examples_per_second': '32.135', 'grad_norm': '30.75', 'counters/examples': 119936, 'counters/updates': 3748}
skipping logging after 119968 examples to avoid logging too frequently
train stats after 120000 examples: {'rewards_train/chosen': '0.05112', 'rewards_train/rejected': '0.019714', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031406', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-118.72', 'loss/train': '0.68629', 'examples_per_second': '31.309', 'grad_norm': '27.75', 'counters/examples': 120000, 'counters/updates': 3750}
Running evaluation after 120000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 120000: {'rewards_eval/chosen': '0.11764', 'rewards_eval/rejected': '0.038294', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.079348', 'logps_eval/rejected': '-118.23', 'logps_eval/chosen': '-138.26', 'loss/eval': '0.66386'}
skipping save for non epoch
train stats after 120032 examples: {'rewards_train/chosen': '0.068646', 'rewards_train/rejected': '-0.035489', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10414', 'logps_train/rejected': '-90.226', 'logps_train/chosen': '-146.99', 'loss/train': '0.64821', 'examples_per_second': '33.855', 'grad_norm': '24.875', 'counters/examples': 120032, 'counters/updates': 3751}
skipping logging after 120064 examples to avoid logging too frequently
train stats after 120096 examples: {'rewards_train/chosen': '0.095253', 'rewards_train/rejected': '0.0095348', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085718', 'logps_train/rejected': '-93.203', 'logps_train/chosen': '-136.75', 'loss/train': '0.65681', 'examples_per_second': '32.665', 'grad_norm': '26.125', 'counters/examples': 120096, 'counters/updates': 3753}
train stats after 120128 examples: {'rewards_train/chosen': '0.061529', 'rewards_train/rejected': '0.061271', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00025755', 'logps_train/rejected': '-133.73', 'logps_train/chosen': '-149.8', 'loss/train': '0.70742', 'examples_per_second': '31.415', 'grad_norm': '31.5', 'counters/examples': 120128, 'counters/updates': 3754}
skipping logging after 120160 examples to avoid logging too frequently
train stats after 120192 examples: {'rewards_train/chosen': '0.1478', 'rewards_train/rejected': '0.035795', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11201', 'logps_train/rejected': '-118.11', 'logps_train/chosen': '-125.89', 'loss/train': '0.64527', 'examples_per_second': '32.509', 'grad_norm': '30.875', 'counters/examples': 120192, 'counters/updates': 3756}
train stats after 120224 examples: {'rewards_train/chosen': '0.17062', 'rewards_train/rejected': '0.05091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11971', 'logps_train/rejected': '-142.47', 'logps_train/chosen': '-179.25', 'loss/train': '0.65202', 'examples_per_second': '33.386', 'grad_norm': '30.5', 'counters/examples': 120224, 'counters/updates': 3757}
train stats after 120256 examples: {'rewards_train/chosen': '0.11578', 'rewards_train/rejected': '0.044428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071353', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-172.85', 'loss/train': '0.66576', 'examples_per_second': '30.023', 'grad_norm': '26.875', 'counters/examples': 120256, 'counters/updates': 3758}
train stats after 120288 examples: {'rewards_train/chosen': '0.1373', 'rewards_train/rejected': '0.041109', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096189', 'logps_train/rejected': '-105.8', 'logps_train/chosen': '-128.82', 'loss/train': '0.65896', 'examples_per_second': '30.527', 'grad_norm': '22.75', 'counters/examples': 120288, 'counters/updates': 3759}
train stats after 120320 examples: {'rewards_train/chosen': '0.16122', 'rewards_train/rejected': '0.064285', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096938', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-144.77', 'loss/train': '0.65727', 'examples_per_second': '31.039', 'grad_norm': '27.125', 'counters/examples': 120320, 'counters/updates': 3760}
train stats after 120352 examples: {'rewards_train/chosen': '0.1233', 'rewards_train/rejected': '0.050125', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073173', 'logps_train/rejected': '-130.56', 'logps_train/chosen': '-145.13', 'loss/train': '0.6706', 'examples_per_second': '30.559', 'grad_norm': '28.25', 'counters/examples': 120352, 'counters/updates': 3761}
train stats after 120384 examples: {'rewards_train/chosen': '0.081058', 'rewards_train/rejected': '0.060154', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020904', 'logps_train/rejected': '-167.38', 'logps_train/chosen': '-191.67', 'loss/train': '0.69206', 'examples_per_second': '31.508', 'grad_norm': '33.25', 'counters/examples': 120384, 'counters/updates': 3762}
train stats after 120416 examples: {'rewards_train/chosen': '0.070827', 'rewards_train/rejected': '-0.0020321', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072859', 'logps_train/rejected': '-129.44', 'logps_train/chosen': '-134.65', 'loss/train': '0.66673', 'examples_per_second': '32.232', 'grad_norm': '31.375', 'counters/examples': 120416, 'counters/updates': 3763}
train stats after 120448 examples: {'rewards_train/chosen': '0.082157', 'rewards_train/rejected': '-0.015885', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098042', 'logps_train/rejected': '-98.392', 'logps_train/chosen': '-133.35', 'loss/train': '0.65241', 'examples_per_second': '30.673', 'grad_norm': '26.75', 'counters/examples': 120448, 'counters/updates': 3764}
skipping logging after 120480 examples to avoid logging too frequently
train stats after 120512 examples: {'rewards_train/chosen': '0.13115', 'rewards_train/rejected': '0.024694', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10646', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-137.45', 'loss/train': '0.64823', 'examples_per_second': '30.316', 'grad_norm': '24.375', 'counters/examples': 120512, 'counters/updates': 3766}
train stats after 120544 examples: {'rewards_train/chosen': '0.15674', 'rewards_train/rejected': '0.055086', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10165', 'logps_train/rejected': '-132.63', 'logps_train/chosen': '-162.34', 'loss/train': '0.65378', 'examples_per_second': '31.224', 'grad_norm': '25.625', 'counters/examples': 120544, 'counters/updates': 3767}
skipping logging after 120576 examples to avoid logging too frequently
train stats after 120608 examples: {'rewards_train/chosen': '0.13103', 'rewards_train/rejected': '0.01999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11104', 'logps_train/rejected': '-182.02', 'logps_train/chosen': '-136.24', 'loss/train': '0.64763', 'examples_per_second': '31.618', 'grad_norm': '30', 'counters/examples': 120608, 'counters/updates': 3769}
train stats after 120640 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '0.03403', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088977', 'logps_train/rejected': '-103.01', 'logps_train/chosen': '-138.56', 'loss/train': '0.65704', 'examples_per_second': '31.086', 'grad_norm': '24.875', 'counters/examples': 120640, 'counters/updates': 3770}
train stats after 120672 examples: {'rewards_train/chosen': '0.083947', 'rewards_train/rejected': '0.069719', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014228', 'logps_train/rejected': '-155.85', 'logps_train/chosen': '-150.75', 'loss/train': '0.69889', 'examples_per_second': '32.644', 'grad_norm': '30.625', 'counters/examples': 120672, 'counters/updates': 3771}
train stats after 120704 examples: {'rewards_train/chosen': '0.078366', 'rewards_train/rejected': '0.0064528', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071913', 'logps_train/rejected': '-132.82', 'logps_train/chosen': '-151.53', 'loss/train': '0.66229', 'examples_per_second': '32.945', 'grad_norm': '30.375', 'counters/examples': 120704, 'counters/updates': 3772}
train stats after 120736 examples: {'rewards_train/chosen': '0.22808', 'rewards_train/rejected': '-0.0070918', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23518', 'logps_train/rejected': '-152.47', 'logps_train/chosen': '-178.35', 'loss/train': '0.60555', 'examples_per_second': '31.252', 'grad_norm': '26.5', 'counters/examples': 120736, 'counters/updates': 3773}
train stats after 120768 examples: {'rewards_train/chosen': '0.10142', 'rewards_train/rejected': '0.03531', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06611', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-148.82', 'loss/train': '0.67153', 'examples_per_second': '30.574', 'grad_norm': '32', 'counters/examples': 120768, 'counters/updates': 3774}
train stats after 120800 examples: {'rewards_train/chosen': '0.088282', 'rewards_train/rejected': '0.058356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029925', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-173.72', 'loss/train': '0.68789', 'examples_per_second': '31.337', 'grad_norm': '51.25', 'counters/examples': 120800, 'counters/updates': 3775}
skipping logging after 120832 examples to avoid logging too frequently
train stats after 120864 examples: {'rewards_train/chosen': '0.14047', 'rewards_train/rejected': '0.019018', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12145', 'logps_train/rejected': '-115.16', 'logps_train/chosen': '-144.69', 'loss/train': '0.64158', 'examples_per_second': '32.132', 'grad_norm': '26.5', 'counters/examples': 120864, 'counters/updates': 3777}
train stats after 120896 examples: {'rewards_train/chosen': '0.043255', 'rewards_train/rejected': '-0.042048', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085303', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-156.98', 'loss/train': '0.66284', 'examples_per_second': '32.359', 'grad_norm': '29.375', 'counters/examples': 120896, 'counters/updates': 3778}
skipping logging after 120928 examples to avoid logging too frequently
train stats after 120960 examples: {'rewards_train/chosen': '0.19982', 'rewards_train/rejected': '0.047228', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15259', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-151.47', 'loss/train': '0.62898', 'examples_per_second': '30.535', 'grad_norm': '25.625', 'counters/examples': 120960, 'counters/updates': 3780}
train stats after 120992 examples: {'rewards_train/chosen': '0.20149', 'rewards_train/rejected': '-0.054266', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25575', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-168.33', 'loss/train': '0.58529', 'examples_per_second': '31.559', 'grad_norm': '23.625', 'counters/examples': 120992, 'counters/updates': 3781}
skipping logging after 121024 examples to avoid logging too frequently
train stats after 121056 examples: {'rewards_train/chosen': '0.090527', 'rewards_train/rejected': '0.028577', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06195', 'logps_train/rejected': '-111.76', 'logps_train/chosen': '-146.39', 'loss/train': '0.66776', 'examples_per_second': '32.977', 'grad_norm': '24.75', 'counters/examples': 121056, 'counters/updates': 3783}
train stats after 121088 examples: {'rewards_train/chosen': '0.12227', 'rewards_train/rejected': '-0.032046', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15432', 'logps_train/rejected': '-117', 'logps_train/chosen': '-137.05', 'loss/train': '0.63525', 'examples_per_second': '32.06', 'grad_norm': '26.625', 'counters/examples': 121088, 'counters/updates': 3784}
train stats after 121120 examples: {'rewards_train/chosen': '0.15061', 'rewards_train/rejected': '-0.0067664', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15738', 'logps_train/rejected': '-107.7', 'logps_train/chosen': '-149.8', 'loss/train': '0.62859', 'examples_per_second': '30.663', 'grad_norm': '26.25', 'counters/examples': 121120, 'counters/updates': 3785}
train stats after 121152 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '-0.013643', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16048', 'logps_train/rejected': '-97.469', 'logps_train/chosen': '-138.56', 'loss/train': '0.62635', 'examples_per_second': '31.644', 'grad_norm': '23.125', 'counters/examples': 121152, 'counters/updates': 3786}
skipping logging after 121184 examples to avoid logging too frequently
train stats after 121216 examples: {'rewards_train/chosen': '0.10669', 'rewards_train/rejected': '0.054732', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051961', 'logps_train/rejected': '-152.36', 'logps_train/chosen': '-130.55', 'loss/train': '0.67337', 'examples_per_second': '31.369', 'grad_norm': '50', 'counters/examples': 121216, 'counters/updates': 3788}
train stats after 121248 examples: {'rewards_train/chosen': '0.13349', 'rewards_train/rejected': '0.011042', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12245', 'logps_train/rejected': '-103.34', 'logps_train/chosen': '-157.06', 'loss/train': '0.64638', 'examples_per_second': '31.652', 'grad_norm': '24.875', 'counters/examples': 121248, 'counters/updates': 3789}
skipping logging after 121280 examples to avoid logging too frequently
train stats after 121312 examples: {'rewards_train/chosen': '0.06354', 'rewards_train/rejected': '0.084767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.021227', 'logps_train/rejected': '-143.64', 'logps_train/chosen': '-135.83', 'loss/train': '0.71585', 'examples_per_second': '31.54', 'grad_norm': '28.625', 'counters/examples': 121312, 'counters/updates': 3791}
train stats after 121344 examples: {'rewards_train/chosen': '0.091559', 'rewards_train/rejected': '0.0063789', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08518', 'logps_train/rejected': '-114.54', 'logps_train/chosen': '-157.07', 'loss/train': '0.65994', 'examples_per_second': '31.623', 'grad_norm': '30.25', 'counters/examples': 121344, 'counters/updates': 3792}
train stats after 121376 examples: {'rewards_train/chosen': '0.097665', 'rewards_train/rejected': '0.066019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031646', 'logps_train/rejected': '-159.34', 'logps_train/chosen': '-149.44', 'loss/train': '0.68668', 'examples_per_second': '22.767', 'grad_norm': '29.375', 'counters/examples': 121376, 'counters/updates': 3793}
skipping logging after 121408 examples to avoid logging too frequently
train stats after 121440 examples: {'rewards_train/chosen': '0.095149', 'rewards_train/rejected': '0.064564', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030585', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-127.08', 'loss/train': '0.68302', 'examples_per_second': '30.472', 'grad_norm': '23', 'counters/examples': 121440, 'counters/updates': 3795}
skipping logging after 121472 examples to avoid logging too frequently
train stats after 121504 examples: {'rewards_train/chosen': '0.077872', 'rewards_train/rejected': '0.043321', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034551', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-134.8', 'loss/train': '0.68813', 'examples_per_second': '34.469', 'grad_norm': '28', 'counters/examples': 121504, 'counters/updates': 3797}
train stats after 121536 examples: {'rewards_train/chosen': '0.15521', 'rewards_train/rejected': '0.12718', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028029', 'logps_train/rejected': '-113.09', 'logps_train/chosen': '-157.22', 'loss/train': '0.68326', 'examples_per_second': '30.13', 'grad_norm': '25.75', 'counters/examples': 121536, 'counters/updates': 3798}
skipping logging after 121568 examples to avoid logging too frequently
train stats after 121600 examples: {'rewards_train/chosen': '0.035048', 'rewards_train/rejected': '0.082541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.047493', 'logps_train/rejected': '-128.22', 'logps_train/chosen': '-124.26', 'loss/train': '0.72807', 'examples_per_second': '33.759', 'grad_norm': '32.25', 'counters/examples': 121600, 'counters/updates': 3800}
train stats after 121632 examples: {'rewards_train/chosen': '0.052559', 'rewards_train/rejected': '-0.035388', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.087947', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-119.25', 'loss/train': '0.66346', 'examples_per_second': '33.065', 'grad_norm': '27.625', 'counters/examples': 121632, 'counters/updates': 3801}
train stats after 121664 examples: {'rewards_train/chosen': '0.092865', 'rewards_train/rejected': '0.061567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031297', 'logps_train/rejected': '-122.51', 'logps_train/chosen': '-128.4', 'loss/train': '0.68691', 'examples_per_second': '31.352', 'grad_norm': '29.75', 'counters/examples': 121664, 'counters/updates': 3802}
train stats after 121696 examples: {'rewards_train/chosen': '0.067796', 'rewards_train/rejected': '0.033081', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034716', 'logps_train/rejected': '-145.45', 'logps_train/chosen': '-135.53', 'loss/train': '0.68333', 'examples_per_second': '29.964', 'grad_norm': '27', 'counters/examples': 121696, 'counters/updates': 3803}
train stats after 121728 examples: {'rewards_train/chosen': '0.18459', 'rewards_train/rejected': '0.014161', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17043', 'logps_train/rejected': '-146.43', 'logps_train/chosen': '-168.09', 'loss/train': '0.62244', 'examples_per_second': '31.632', 'grad_norm': '26.125', 'counters/examples': 121728, 'counters/updates': 3804}
train stats after 121760 examples: {'rewards_train/chosen': '0.078949', 'rewards_train/rejected': '-0.009818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088767', 'logps_train/rejected': '-124.58', 'logps_train/chosen': '-146.43', 'loss/train': '0.65791', 'examples_per_second': '31.041', 'grad_norm': '28.5', 'counters/examples': 121760, 'counters/updates': 3805}
train stats after 121792 examples: {'rewards_train/chosen': '0.085293', 'rewards_train/rejected': '0.03208', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053213', 'logps_train/rejected': '-122.93', 'logps_train/chosen': '-158.92', 'loss/train': '0.68568', 'examples_per_second': '31.699', 'grad_norm': '30.375', 'counters/examples': 121792, 'counters/updates': 3806}
train stats after 121824 examples: {'rewards_train/chosen': '0.17321', 'rewards_train/rejected': '0.094337', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07887', 'logps_train/rejected': '-149.39', 'logps_train/chosen': '-170.61', 'loss/train': '0.66451', 'examples_per_second': '30.058', 'grad_norm': '33', 'counters/examples': 121824, 'counters/updates': 3807}
train stats after 121856 examples: {'rewards_train/chosen': '0.10143', 'rewards_train/rejected': '-0.01074', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11217', 'logps_train/rejected': '-96.53', 'logps_train/chosen': '-124.8', 'loss/train': '0.64539', 'examples_per_second': '32.127', 'grad_norm': '26.625', 'counters/examples': 121856, 'counters/updates': 3808}
train stats after 121888 examples: {'rewards_train/chosen': '0.048052', 'rewards_train/rejected': '-0.054568', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10262', 'logps_train/rejected': '-112.06', 'logps_train/chosen': '-129.52', 'loss/train': '0.65192', 'examples_per_second': '32.89', 'grad_norm': '24.75', 'counters/examples': 121888, 'counters/updates': 3809}
train stats after 121920 examples: {'rewards_train/chosen': '0.037581', 'rewards_train/rejected': '0.018338', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019243', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-121.45', 'loss/train': '0.68933', 'examples_per_second': '32.359', 'grad_norm': '28', 'counters/examples': 121920, 'counters/updates': 3810}
train stats after 121952 examples: {'rewards_train/chosen': '0.072468', 'rewards_train/rejected': '-0.026402', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098869', 'logps_train/rejected': '-90.575', 'logps_train/chosen': '-131.05', 'loss/train': '0.65115', 'examples_per_second': '32.473', 'grad_norm': '25.875', 'counters/examples': 121952, 'counters/updates': 3811}
train stats after 121984 examples: {'rewards_train/chosen': '0.11616', 'rewards_train/rejected': '0.055165', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060996', 'logps_train/rejected': '-131.11', 'logps_train/chosen': '-122.82', 'loss/train': '0.67074', 'examples_per_second': '30.201', 'grad_norm': '25.625', 'counters/examples': 121984, 'counters/updates': 3812}
train stats after 122016 examples: {'rewards_train/chosen': '0.10301', 'rewards_train/rejected': '0.014946', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088063', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-126.01', 'loss/train': '0.66258', 'examples_per_second': '31.58', 'grad_norm': '25.125', 'counters/examples': 122016, 'counters/updates': 3813}
train stats after 122048 examples: {'rewards_train/chosen': '0.14212', 'rewards_train/rejected': '-0.029276', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17139', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-152.96', 'loss/train': '0.62242', 'examples_per_second': '31.466', 'grad_norm': '23.75', 'counters/examples': 122048, 'counters/updates': 3814}
skipping logging after 122080 examples to avoid logging too frequently
train stats after 122112 examples: {'rewards_train/chosen': '0.10056', 'rewards_train/rejected': '0.0053422', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09522', 'logps_train/rejected': '-118.82', 'logps_train/chosen': '-121.72', 'loss/train': '0.65575', 'examples_per_second': '31.159', 'grad_norm': '23.5', 'counters/examples': 122112, 'counters/updates': 3816}
skipping logging after 122144 examples to avoid logging too frequently
train stats after 122176 examples: {'rewards_train/chosen': '0.15469', 'rewards_train/rejected': '-0.033435', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18812', 'logps_train/rejected': '-119.79', 'logps_train/chosen': '-134.57', 'loss/train': '0.61835', 'examples_per_second': '35.681', 'grad_norm': '25', 'counters/examples': 122176, 'counters/updates': 3818}
train stats after 122208 examples: {'rewards_train/chosen': '0.13743', 'rewards_train/rejected': '0.076815', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060618', 'logps_train/rejected': '-125.72', 'logps_train/chosen': '-129.05', 'loss/train': '0.67052', 'examples_per_second': '30.806', 'grad_norm': '28.25', 'counters/examples': 122208, 'counters/updates': 3819}
skipping logging after 122240 examples to avoid logging too frequently
train stats after 122272 examples: {'rewards_train/chosen': '0.11357', 'rewards_train/rejected': '0.031468', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082102', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-157.69', 'loss/train': '0.66269', 'examples_per_second': '35.99', 'grad_norm': '25.125', 'counters/examples': 122272, 'counters/updates': 3821}
skipping logging after 122304 examples to avoid logging too frequently
train stats after 122336 examples: {'rewards_train/chosen': '0.055439', 'rewards_train/rejected': '-0.012313', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067752', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-108.98', 'loss/train': '0.66654', 'examples_per_second': '31.412', 'grad_norm': '31', 'counters/examples': 122336, 'counters/updates': 3823}
train stats after 122368 examples: {'rewards_train/chosen': '0.0099183', 'rewards_train/rejected': '0.00086719', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0090512', 'logps_train/rejected': '-141', 'logps_train/chosen': '-120.98', 'loss/train': '0.69966', 'examples_per_second': '32.312', 'grad_norm': '32.5', 'counters/examples': 122368, 'counters/updates': 3824}
skipping logging after 122400 examples to avoid logging too frequently
train stats after 122432 examples: {'rewards_train/chosen': '0.063852', 'rewards_train/rejected': '0.027436', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036416', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-138.49', 'loss/train': '0.68302', 'examples_per_second': '31.373', 'grad_norm': '26.875', 'counters/examples': 122432, 'counters/updates': 3826}
train stats after 122464 examples: {'rewards_train/chosen': '0.084929', 'rewards_train/rejected': '0.12317', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.038236', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-111.07', 'loss/train': '0.72064', 'examples_per_second': '32.155', 'grad_norm': '27.125', 'counters/examples': 122464, 'counters/updates': 3827}
train stats after 122496 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '0.051425', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063399', 'logps_train/rejected': '-103.21', 'logps_train/chosen': '-144.87', 'loss/train': '0.67043', 'examples_per_second': '31.658', 'grad_norm': '24.875', 'counters/examples': 122496, 'counters/updates': 3828}
train stats after 122528 examples: {'rewards_train/chosen': '0.091423', 'rewards_train/rejected': '-0.021814', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11324', 'logps_train/rejected': '-98.845', 'logps_train/chosen': '-120.95', 'loss/train': '0.6429', 'examples_per_second': '30.52', 'grad_norm': '31', 'counters/examples': 122528, 'counters/updates': 3829}
train stats after 122560 examples: {'rewards_train/chosen': '0.086657', 'rewards_train/rejected': '0.032512', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054146', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-135.91', 'loss/train': '0.67224', 'examples_per_second': '31.244', 'grad_norm': '25.875', 'counters/examples': 122560, 'counters/updates': 3830}
train stats after 122592 examples: {'rewards_train/chosen': '0.11722', 'rewards_train/rejected': '0.0064272', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11079', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-144.48', 'loss/train': '0.64726', 'examples_per_second': '30.554', 'grad_norm': '28.875', 'counters/examples': 122592, 'counters/updates': 3831}
train stats after 122624 examples: {'rewards_train/chosen': '0.07967', 'rewards_train/rejected': '0.010849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068822', 'logps_train/rejected': '-153.63', 'logps_train/chosen': '-156.57', 'loss/train': '0.67097', 'examples_per_second': '31.64', 'grad_norm': '30.25', 'counters/examples': 122624, 'counters/updates': 3832}
skipping logging after 122656 examples to avoid logging too frequently
train stats after 122688 examples: {'rewards_train/chosen': '0.12363', 'rewards_train/rejected': '0.053145', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070481', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-151.11', 'loss/train': '0.66637', 'examples_per_second': '30.302', 'grad_norm': '28.125', 'counters/examples': 122688, 'counters/updates': 3834}
skipping logging after 122720 examples to avoid logging too frequently
train stats after 122752 examples: {'rewards_train/chosen': '0.082816', 'rewards_train/rejected': '-0.037378', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12019', 'logps_train/rejected': '-96.584', 'logps_train/chosen': '-107.02', 'loss/train': '0.64098', 'examples_per_second': '31.847', 'grad_norm': '22.125', 'counters/examples': 122752, 'counters/updates': 3836}
train stats after 122784 examples: {'rewards_train/chosen': '0.057004', 'rewards_train/rejected': '-0.039022', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096025', 'logps_train/rejected': '-86.54', 'logps_train/chosen': '-95.915', 'loss/train': '0.65932', 'examples_per_second': '31.127', 'grad_norm': '23.75', 'counters/examples': 122784, 'counters/updates': 3837}
skipping logging after 122816 examples to avoid logging too frequently
train stats after 122848 examples: {'rewards_train/chosen': '0.083741', 'rewards_train/rejected': '0.039079', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044662', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-124.93', 'loss/train': '0.67728', 'examples_per_second': '32.39', 'grad_norm': '23.5', 'counters/examples': 122848, 'counters/updates': 3839}
train stats after 122880 examples: {'rewards_train/chosen': '0.19519', 'rewards_train/rejected': '0.10595', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089238', 'logps_train/rejected': '-191.83', 'logps_train/chosen': '-196.51', 'loss/train': '0.66666', 'examples_per_second': '31.644', 'grad_norm': '40.25', 'counters/examples': 122880, 'counters/updates': 3840}
train stats after 122912 examples: {'rewards_train/chosen': '0.13957', 'rewards_train/rejected': '0.092344', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047227', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-126.15', 'loss/train': '0.67626', 'examples_per_second': '30.522', 'grad_norm': '26.625', 'counters/examples': 122912, 'counters/updates': 3841}
skipping logging after 122944 examples to avoid logging too frequently
train stats after 122976 examples: {'rewards_train/chosen': '0.15306', 'rewards_train/rejected': '0.028949', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12412', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-151.36', 'loss/train': '0.64363', 'examples_per_second': '34.444', 'grad_norm': '24.75', 'counters/examples': 122976, 'counters/updates': 3843}
skipping logging after 123008 examples to avoid logging too frequently
train stats after 123040 examples: {'rewards_train/chosen': '0.054875', 'rewards_train/rejected': '-0.012701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067576', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-128.91', 'loss/train': '0.66773', 'examples_per_second': '31.31', 'grad_norm': '23', 'counters/examples': 123040, 'counters/updates': 3845}
train stats after 123072 examples: {'rewards_train/chosen': '0.099338', 'rewards_train/rejected': '3.2363e-06', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099334', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-154.72', 'loss/train': '0.65044', 'examples_per_second': '31.859', 'grad_norm': '29.75', 'counters/examples': 123072, 'counters/updates': 3846}
train stats after 123104 examples: {'rewards_train/chosen': '0.054522', 'rewards_train/rejected': '0.11239', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.057871', 'logps_train/rejected': '-135.79', 'logps_train/chosen': '-128.56', 'loss/train': '0.73063', 'examples_per_second': '30.469', 'grad_norm': '28.375', 'counters/examples': 123104, 'counters/updates': 3847}
skipping logging after 123136 examples to avoid logging too frequently
train stats after 123168 examples: {'rewards_train/chosen': '0.14943', 'rewards_train/rejected': '-0.038264', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18769', 'logps_train/rejected': '-106.89', 'logps_train/chosen': '-138.47', 'loss/train': '0.61877', 'examples_per_second': '31.311', 'grad_norm': '22.875', 'counters/examples': 123168, 'counters/updates': 3849}
train stats after 123200 examples: {'rewards_train/chosen': '0.14167', 'rewards_train/rejected': '0.048302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093365', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-150.75', 'loss/train': '0.6597', 'examples_per_second': '31.662', 'grad_norm': '31', 'counters/examples': 123200, 'counters/updates': 3850}
train stats after 123232 examples: {'rewards_train/chosen': '0.065286', 'rewards_train/rejected': '-0.0041318', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069418', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-140.47', 'loss/train': '0.67093', 'examples_per_second': '31.595', 'grad_norm': '26.625', 'counters/examples': 123232, 'counters/updates': 3851}
train stats after 123264 examples: {'rewards_train/chosen': '0.084305', 'rewards_train/rejected': '0.0054979', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078807', 'logps_train/rejected': '-134.68', 'logps_train/chosen': '-158.78', 'loss/train': '0.65993', 'examples_per_second': '30.286', 'grad_norm': '26.25', 'counters/examples': 123264, 'counters/updates': 3852}
train stats after 123296 examples: {'rewards_train/chosen': '0.10248', 'rewards_train/rejected': '0.074393', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028092', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-119.86', 'loss/train': '0.68287', 'examples_per_second': '32.432', 'grad_norm': '26', 'counters/examples': 123296, 'counters/updates': 3853}
train stats after 123328 examples: {'rewards_train/chosen': '0.13803', 'rewards_train/rejected': '0.087368', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.050663', 'logps_train/rejected': '-139.51', 'logps_train/chosen': '-139.56', 'loss/train': '0.68324', 'examples_per_second': '31.654', 'grad_norm': '81.5', 'counters/examples': 123328, 'counters/updates': 3854}
train stats after 123360 examples: {'rewards_train/chosen': '0.091247', 'rewards_train/rejected': '0.10654', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015291', 'logps_train/rejected': '-125.48', 'logps_train/chosen': '-180.38', 'loss/train': '0.71193', 'examples_per_second': '31.307', 'grad_norm': '31', 'counters/examples': 123360, 'counters/updates': 3855}
train stats after 123392 examples: {'rewards_train/chosen': '0.13545', 'rewards_train/rejected': '0.0085024', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12694', 'logps_train/rejected': '-121.39', 'logps_train/chosen': '-171.92', 'loss/train': '0.64002', 'examples_per_second': '31.333', 'grad_norm': '26', 'counters/examples': 123392, 'counters/updates': 3856}
skipping logging after 123424 examples to avoid logging too frequently
train stats after 123456 examples: {'rewards_train/chosen': '0.10004', 'rewards_train/rejected': '0.069168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030875', 'logps_train/rejected': '-166.35', 'logps_train/chosen': '-176.6', 'loss/train': '0.68592', 'examples_per_second': '31.633', 'grad_norm': '32.25', 'counters/examples': 123456, 'counters/updates': 3858}
train stats after 123488 examples: {'rewards_train/chosen': '0.15121', 'rewards_train/rejected': '0.017871', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13334', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-195.2', 'loss/train': '0.63926', 'examples_per_second': '30.628', 'grad_norm': '28.5', 'counters/examples': 123488, 'counters/updates': 3859}
train stats after 123520 examples: {'rewards_train/chosen': '0.055563', 'rewards_train/rejected': '0.080209', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024646', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-137.73', 'loss/train': '0.70927', 'examples_per_second': '31.642', 'grad_norm': '28.875', 'counters/examples': 123520, 'counters/updates': 3860}
train stats after 123552 examples: {'rewards_train/chosen': '0.14852', 'rewards_train/rejected': '0.016778', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13174', 'logps_train/rejected': '-109.51', 'logps_train/chosen': '-128.82', 'loss/train': '0.6407', 'examples_per_second': '30.933', 'grad_norm': '27', 'counters/examples': 123552, 'counters/updates': 3861}
train stats after 123584 examples: {'rewards_train/chosen': '0.12792', 'rewards_train/rejected': '0.039265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088652', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-169.51', 'loss/train': '0.65676', 'examples_per_second': '31.304', 'grad_norm': '25.875', 'counters/examples': 123584, 'counters/updates': 3862}
train stats after 123616 examples: {'rewards_train/chosen': '0.17013', 'rewards_train/rejected': '0.016889', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15324', 'logps_train/rejected': '-111.31', 'logps_train/chosen': '-162.38', 'loss/train': '0.63066', 'examples_per_second': '31.657', 'grad_norm': '25.375', 'counters/examples': 123616, 'counters/updates': 3863}
train stats after 123648 examples: {'rewards_train/chosen': '0.083099', 'rewards_train/rejected': '0.058222', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024877', 'logps_train/rejected': '-133.59', 'logps_train/chosen': '-127.23', 'loss/train': '0.69326', 'examples_per_second': '31.411', 'grad_norm': '28.625', 'counters/examples': 123648, 'counters/updates': 3864}
train stats after 123680 examples: {'rewards_train/chosen': '0.087752', 'rewards_train/rejected': '0.068433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019319', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-125.82', 'loss/train': '0.69591', 'examples_per_second': '30.676', 'grad_norm': '29.125', 'counters/examples': 123680, 'counters/updates': 3865}
train stats after 123712 examples: {'rewards_train/chosen': '0.10198', 'rewards_train/rejected': '0.035355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06662', 'logps_train/rejected': '-126.37', 'logps_train/chosen': '-140.77', 'loss/train': '0.67169', 'examples_per_second': '31.627', 'grad_norm': '26.375', 'counters/examples': 123712, 'counters/updates': 3866}
train stats after 123744 examples: {'rewards_train/chosen': '0.004101', 'rewards_train/rejected': '0.052854', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.048753', 'logps_train/rejected': '-126.36', 'logps_train/chosen': '-136.53', 'loss/train': '0.72638', 'examples_per_second': '30.677', 'grad_norm': '27.625', 'counters/examples': 123744, 'counters/updates': 3867}
train stats after 123776 examples: {'rewards_train/chosen': '0.17746', 'rewards_train/rejected': '0.077998', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099461', 'logps_train/rejected': '-104.57', 'logps_train/chosen': '-132.78', 'loss/train': '0.65256', 'examples_per_second': '31.079', 'grad_norm': '25.875', 'counters/examples': 123776, 'counters/updates': 3868}
train stats after 123808 examples: {'rewards_train/chosen': '0.11063', 'rewards_train/rejected': '0.0054581', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-118.01', 'logps_train/chosen': '-161.11', 'loss/train': '0.65188', 'examples_per_second': '32.746', 'grad_norm': '25.375', 'counters/examples': 123808, 'counters/updates': 3869}
train stats after 123840 examples: {'rewards_train/chosen': '0.085791', 'rewards_train/rejected': '0.0065237', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079267', 'logps_train/rejected': '-103.64', 'logps_train/chosen': '-117', 'loss/train': '0.66029', 'examples_per_second': '30.423', 'grad_norm': '24.75', 'counters/examples': 123840, 'counters/updates': 3870}
train stats after 123872 examples: {'rewards_train/chosen': '0.10477', 'rewards_train/rejected': '0.004879', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099895', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-132.09', 'loss/train': '0.65101', 'examples_per_second': '30.667', 'grad_norm': '25.25', 'counters/examples': 123872, 'counters/updates': 3871}
train stats after 123904 examples: {'rewards_train/chosen': '0.13255', 'rewards_train/rejected': '0.045588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086959', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-153.17', 'loss/train': '0.662', 'examples_per_second': '32.036', 'grad_norm': '28.5', 'counters/examples': 123904, 'counters/updates': 3872}
skipping logging after 123936 examples to avoid logging too frequently
train stats after 123968 examples: {'rewards_train/chosen': '0.093597', 'rewards_train/rejected': '0.030851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062746', 'logps_train/rejected': '-104.59', 'logps_train/chosen': '-124.57', 'loss/train': '0.67129', 'examples_per_second': '33.689', 'grad_norm': '24.375', 'counters/examples': 123968, 'counters/updates': 3874}
train stats after 124000 examples: {'rewards_train/chosen': '0.10622', 'rewards_train/rejected': '0.028515', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077709', 'logps_train/rejected': '-118.91', 'logps_train/chosen': '-142.67', 'loss/train': '0.66239', 'examples_per_second': '30.154', 'grad_norm': '31.375', 'counters/examples': 124000, 'counters/updates': 3875}
skipping logging after 124032 examples to avoid logging too frequently
train stats after 124064 examples: {'rewards_train/chosen': '0.18585', 'rewards_train/rejected': '0.10375', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082102', 'logps_train/rejected': '-153.44', 'logps_train/chosen': '-170.65', 'loss/train': '0.66063', 'examples_per_second': '31.674', 'grad_norm': '28.25', 'counters/examples': 124064, 'counters/updates': 3877}
train stats after 124096 examples: {'rewards_train/chosen': '0.17793', 'rewards_train/rejected': '0.064979', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11295', 'logps_train/rejected': '-129.06', 'logps_train/chosen': '-143', 'loss/train': '0.64635', 'examples_per_second': '32.715', 'grad_norm': '27.75', 'counters/examples': 124096, 'counters/updates': 3878}
train stats after 124128 examples: {'rewards_train/chosen': '0.15147', 'rewards_train/rejected': '0.064379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087088', 'logps_train/rejected': '-122.33', 'logps_train/chosen': '-142.2', 'loss/train': '0.65723', 'examples_per_second': '31.54', 'grad_norm': '27.25', 'counters/examples': 124128, 'counters/updates': 3879}
train stats after 124160 examples: {'rewards_train/chosen': '0.13528', 'rewards_train/rejected': '0.012312', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12297', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-172.53', 'loss/train': '0.64287', 'examples_per_second': '30.669', 'grad_norm': '34.25', 'counters/examples': 124160, 'counters/updates': 3880}
train stats after 124192 examples: {'rewards_train/chosen': '0.035492', 'rewards_train/rejected': '-0.013279', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048771', 'logps_train/rejected': '-139.56', 'logps_train/chosen': '-156.7', 'loss/train': '0.67806', 'examples_per_second': '32.565', 'grad_norm': '27.125', 'counters/examples': 124192, 'counters/updates': 3881}
train stats after 124224 examples: {'rewards_train/chosen': '0.17885', 'rewards_train/rejected': '0.11067', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06818', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-174.34', 'loss/train': '0.66485', 'examples_per_second': '33.273', 'grad_norm': '33.5', 'counters/examples': 124224, 'counters/updates': 3882}
train stats after 124256 examples: {'rewards_train/chosen': '0.17297', 'rewards_train/rejected': '0.021527', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15145', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-185.66', 'loss/train': '0.6308', 'examples_per_second': '31.414', 'grad_norm': '26.5', 'counters/examples': 124256, 'counters/updates': 3883}
train stats after 124288 examples: {'rewards_train/chosen': '0.084441', 'rewards_train/rejected': '0.014582', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069859', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-139.81', 'loss/train': '0.66594', 'examples_per_second': '31.641', 'grad_norm': '25.75', 'counters/examples': 124288, 'counters/updates': 3884}
train stats after 124320 examples: {'rewards_train/chosen': '0.083614', 'rewards_train/rejected': '0.027273', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056341', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-172.16', 'loss/train': '0.67207', 'examples_per_second': '31.65', 'grad_norm': '30', 'counters/examples': 124320, 'counters/updates': 3885}
train stats after 124352 examples: {'rewards_train/chosen': '0.13581', 'rewards_train/rejected': '-0.0011387', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13695', 'logps_train/rejected': '-97.62', 'logps_train/chosen': '-132.7', 'loss/train': '0.63495', 'examples_per_second': '29.994', 'grad_norm': '25.5', 'counters/examples': 124352, 'counters/updates': 3886}
skipping logging after 124384 examples to avoid logging too frequently
train stats after 124416 examples: {'rewards_train/chosen': '0.15171', 'rewards_train/rejected': '0.026133', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12558', 'logps_train/rejected': '-115.2', 'logps_train/chosen': '-162.02', 'loss/train': '0.64098', 'examples_per_second': '33.973', 'grad_norm': '28.125', 'counters/examples': 124416, 'counters/updates': 3888}
train stats after 124448 examples: {'rewards_train/chosen': '0.12025', 'rewards_train/rejected': '0.094886', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025363', 'logps_train/rejected': '-153.63', 'logps_train/chosen': '-125.76', 'loss/train': '0.6907', 'examples_per_second': '31.679', 'grad_norm': '31.625', 'counters/examples': 124448, 'counters/updates': 3889}
train stats after 124480 examples: {'rewards_train/chosen': '0.15043', 'rewards_train/rejected': '0.073897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076532', 'logps_train/rejected': '-121.97', 'logps_train/chosen': '-153.54', 'loss/train': '0.66032', 'examples_per_second': '32.592', 'grad_norm': '27.125', 'counters/examples': 124480, 'counters/updates': 3890}
train stats after 124512 examples: {'rewards_train/chosen': '0.066045', 'rewards_train/rejected': '0.090617', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024572', 'logps_train/rejected': '-150.15', 'logps_train/chosen': '-102.48', 'loss/train': '0.71833', 'examples_per_second': '32.127', 'grad_norm': '29.375', 'counters/examples': 124512, 'counters/updates': 3891}
skipping logging after 124544 examples to avoid logging too frequently
train stats after 124576 examples: {'rewards_train/chosen': '0.029652', 'rewards_train/rejected': '-0.0036804', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033332', 'logps_train/rejected': '-105.91', 'logps_train/chosen': '-132.58', 'loss/train': '0.68026', 'examples_per_second': '27.003', 'grad_norm': '29.125', 'counters/examples': 124576, 'counters/updates': 3893}
train stats after 124608 examples: {'rewards_train/chosen': '0.065761', 'rewards_train/rejected': '-0.035387', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10115', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-136.68', 'loss/train': '0.65611', 'examples_per_second': '31.147', 'grad_norm': '24.25', 'counters/examples': 124608, 'counters/updates': 3894}
train stats after 124640 examples: {'rewards_train/chosen': '0.044949', 'rewards_train/rejected': '0.015745', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029204', 'logps_train/rejected': '-184.28', 'logps_train/chosen': '-138.64', 'loss/train': '0.69768', 'examples_per_second': '32.977', 'grad_norm': '31.75', 'counters/examples': 124640, 'counters/updates': 3895}
train stats after 124672 examples: {'rewards_train/chosen': '0.17865', 'rewards_train/rejected': '0.12512', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053525', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-151.61', 'loss/train': '0.67364', 'examples_per_second': '23.812', 'grad_norm': '26', 'counters/examples': 124672, 'counters/updates': 3896}
train stats after 124704 examples: {'rewards_train/chosen': '0.11029', 'rewards_train/rejected': '-0.0043493', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11464', 'logps_train/rejected': '-116.92', 'logps_train/chosen': '-144.13', 'loss/train': '0.64791', 'examples_per_second': '30.69', 'grad_norm': '25.875', 'counters/examples': 124704, 'counters/updates': 3897}
train stats after 124736 examples: {'rewards_train/chosen': '0.17236', 'rewards_train/rejected': '0.090614', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081743', 'logps_train/rejected': '-146.72', 'logps_train/chosen': '-172.45', 'loss/train': '0.66688', 'examples_per_second': '30.58', 'grad_norm': '30.5', 'counters/examples': 124736, 'counters/updates': 3898}
train stats after 124768 examples: {'rewards_train/chosen': '0.11847', 'rewards_train/rejected': '0.083003', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03547', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-130.17', 'loss/train': '0.68143', 'examples_per_second': '30.942', 'grad_norm': '25.125', 'counters/examples': 124768, 'counters/updates': 3899}
skipping logging after 124800 examples to avoid logging too frequently
train stats after 124832 examples: {'rewards_train/chosen': '0.13028', 'rewards_train/rejected': '0.0088031', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12148', 'logps_train/rejected': '-128.27', 'logps_train/chosen': '-131.04', 'loss/train': '0.64516', 'examples_per_second': '32.514', 'grad_norm': '25.625', 'counters/examples': 124832, 'counters/updates': 3901}
train stats after 124864 examples: {'rewards_train/chosen': '0.1193', 'rewards_train/rejected': '-0.016614', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13591', 'logps_train/rejected': '-111.85', 'logps_train/chosen': '-189.35', 'loss/train': '0.64054', 'examples_per_second': '31.617', 'grad_norm': '28.25', 'counters/examples': 124864, 'counters/updates': 3902}
train stats after 124896 examples: {'rewards_train/chosen': '0.14871', 'rewards_train/rejected': '0.051882', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.096826', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-137.92', 'loss/train': '0.65228', 'examples_per_second': '31.603', 'grad_norm': '26.75', 'counters/examples': 124896, 'counters/updates': 3903}
train stats after 124928 examples: {'rewards_train/chosen': '0.09373', 'rewards_train/rejected': '0.057442', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.036289', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-116.52', 'loss/train': '0.683', 'examples_per_second': '31.691', 'grad_norm': '24.875', 'counters/examples': 124928, 'counters/updates': 3904}
train stats after 124960 examples: {'rewards_train/chosen': '0.10429', 'rewards_train/rejected': '0.078445', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025848', 'logps_train/rejected': '-131.06', 'logps_train/chosen': '-138.9', 'loss/train': '0.68881', 'examples_per_second': '33.232', 'grad_norm': '27.375', 'counters/examples': 124960, 'counters/updates': 3905}
train stats after 124992 examples: {'rewards_train/chosen': '0.15793', 'rewards_train/rejected': '0.11077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047167', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-165.73', 'loss/train': '0.67757', 'examples_per_second': '31.702', 'grad_norm': '29.75', 'counters/examples': 124992, 'counters/updates': 3906}
train stats after 125024 examples: {'rewards_train/chosen': '0.095942', 'rewards_train/rejected': '0.044823', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051119', 'logps_train/rejected': '-127.94', 'logps_train/chosen': '-120.18', 'loss/train': '0.67732', 'examples_per_second': '31.634', 'grad_norm': '27.75', 'counters/examples': 125024, 'counters/updates': 3907}
train stats after 125056 examples: {'rewards_train/chosen': '0.16174', 'rewards_train/rejected': '0.0094253', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15232', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-167.56', 'loss/train': '0.63112', 'examples_per_second': '31.579', 'grad_norm': '24.875', 'counters/examples': 125056, 'counters/updates': 3908}
train stats after 125088 examples: {'rewards_train/chosen': '0.08511', 'rewards_train/rejected': '0.039652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045458', 'logps_train/rejected': '-132.67', 'logps_train/chosen': '-134.01', 'loss/train': '0.67713', 'examples_per_second': '31.108', 'grad_norm': '49.5', 'counters/examples': 125088, 'counters/updates': 3909}
train stats after 125120 examples: {'rewards_train/chosen': '0.22928', 'rewards_train/rejected': '0.070689', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15859', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-181.38', 'loss/train': '0.63164', 'examples_per_second': '30.01', 'grad_norm': '33', 'counters/examples': 125120, 'counters/updates': 3910}
train stats after 125152 examples: {'rewards_train/chosen': '0.2358', 'rewards_train/rejected': '0.096595', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13921', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-175.02', 'loss/train': '0.63388', 'examples_per_second': '30.382', 'grad_norm': '31.875', 'counters/examples': 125152, 'counters/updates': 3911}
skipping logging after 125184 examples to avoid logging too frequently
train stats after 125216 examples: {'rewards_train/chosen': '0.08359', 'rewards_train/rejected': '0.023768', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059822', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-131.6', 'loss/train': '0.67027', 'examples_per_second': '34.649', 'grad_norm': '23.5', 'counters/examples': 125216, 'counters/updates': 3913}
train stats after 125248 examples: {'rewards_train/chosen': '0.1539', 'rewards_train/rejected': '0.026548', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12736', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-148.51', 'loss/train': '0.63928', 'examples_per_second': '32.486', 'grad_norm': '26.75', 'counters/examples': 125248, 'counters/updates': 3914}
train stats after 125280 examples: {'rewards_train/chosen': '0.10234', 'rewards_train/rejected': '0.031422', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070921', 'logps_train/rejected': '-105.18', 'logps_train/chosen': '-122.82', 'loss/train': '0.66671', 'examples_per_second': '31.062', 'grad_norm': '24.25', 'counters/examples': 125280, 'counters/updates': 3915}
skipping logging after 125312 examples to avoid logging too frequently
train stats after 125344 examples: {'rewards_train/chosen': '0.040107', 'rewards_train/rejected': '0.00084786', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03926', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-146.31', 'loss/train': '0.68077', 'examples_per_second': '34.072', 'grad_norm': '28.875', 'counters/examples': 125344, 'counters/updates': 3917}
skipping logging after 125376 examples to avoid logging too frequently
train stats after 125408 examples: {'rewards_train/chosen': '0.018198', 'rewards_train/rejected': '0.087136', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.068938', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-117.02', 'loss/train': '0.744', 'examples_per_second': '34.336', 'grad_norm': '34.25', 'counters/examples': 125408, 'counters/updates': 3919}
train stats after 125440 examples: {'rewards_train/chosen': '0.055888', 'rewards_train/rejected': '-0.0016479', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057536', 'logps_train/rejected': '-94.571', 'logps_train/chosen': '-150.59', 'loss/train': '0.67347', 'examples_per_second': '30.432', 'grad_norm': '25.625', 'counters/examples': 125440, 'counters/updates': 3920}
train stats after 125472 examples: {'rewards_train/chosen': '0.088359', 'rewards_train/rejected': '-0.0025598', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090919', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-122.23', 'loss/train': '0.65966', 'examples_per_second': '31.605', 'grad_norm': '24.375', 'counters/examples': 125472, 'counters/updates': 3921}
train stats after 125504 examples: {'rewards_train/chosen': '0.082906', 'rewards_train/rejected': '0.021516', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06139', 'logps_train/rejected': '-102.13', 'logps_train/chosen': '-101.73', 'loss/train': '0.66741', 'examples_per_second': '30.249', 'grad_norm': '23.25', 'counters/examples': 125504, 'counters/updates': 3922}
train stats after 125536 examples: {'rewards_train/chosen': '0.11616', 'rewards_train/rejected': '0.093937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022221', 'logps_train/rejected': '-102.03', 'logps_train/chosen': '-122.29', 'loss/train': '0.68783', 'examples_per_second': '31.6', 'grad_norm': '21.75', 'counters/examples': 125536, 'counters/updates': 3923}
train stats after 125568 examples: {'rewards_train/chosen': '0.1032', 'rewards_train/rejected': '0.037091', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066108', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-153.96', 'loss/train': '0.66822', 'examples_per_second': '30.596', 'grad_norm': '25.25', 'counters/examples': 125568, 'counters/updates': 3924}
train stats after 125600 examples: {'rewards_train/chosen': '0.050422', 'rewards_train/rejected': '0.031371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019051', 'logps_train/rejected': '-105.75', 'logps_train/chosen': '-140.45', 'loss/train': '0.69473', 'examples_per_second': '31.65', 'grad_norm': '27.75', 'counters/examples': 125600, 'counters/updates': 3925}
train stats after 125632 examples: {'rewards_train/chosen': '0.085925', 'rewards_train/rejected': '-0.0021779', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088103', 'logps_train/rejected': '-95.189', 'logps_train/chosen': '-162.79', 'loss/train': '0.664', 'examples_per_second': '30.252', 'grad_norm': '23.75', 'counters/examples': 125632, 'counters/updates': 3926}
train stats after 125664 examples: {'rewards_train/chosen': '0.14265', 'rewards_train/rejected': '0.071936', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070711', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-139.08', 'loss/train': '0.66694', 'examples_per_second': '30.066', 'grad_norm': '27.625', 'counters/examples': 125664, 'counters/updates': 3927}
train stats after 125696 examples: {'rewards_train/chosen': '0.066287', 'rewards_train/rejected': '0.091534', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025247', 'logps_train/rejected': '-109.46', 'logps_train/chosen': '-129.63', 'loss/train': '0.71234', 'examples_per_second': '31.763', 'grad_norm': '25', 'counters/examples': 125696, 'counters/updates': 3928}
train stats after 125728 examples: {'rewards_train/chosen': '0.14191', 'rewards_train/rejected': '0.049683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09223', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-166.61', 'loss/train': '0.66054', 'examples_per_second': '31.583', 'grad_norm': '27.75', 'counters/examples': 125728, 'counters/updates': 3929}
train stats after 125760 examples: {'rewards_train/chosen': '0.11701', 'rewards_train/rejected': '0.0175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099513', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-155.34', 'loss/train': '0.65132', 'examples_per_second': '31.645', 'grad_norm': '28', 'counters/examples': 125760, 'counters/updates': 3930}
train stats after 125792 examples: {'rewards_train/chosen': '0.094274', 'rewards_train/rejected': '0.11019', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015918', 'logps_train/rejected': '-156.27', 'logps_train/chosen': '-173.25', 'loss/train': '0.71684', 'examples_per_second': '31.675', 'grad_norm': '29.75', 'counters/examples': 125792, 'counters/updates': 3931}
train stats after 125824 examples: {'rewards_train/chosen': '0.12003', 'rewards_train/rejected': '-0.051139', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17117', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-141.69', 'loss/train': '0.62111', 'examples_per_second': '32.297', 'grad_norm': '25.375', 'counters/examples': 125824, 'counters/updates': 3932}
train stats after 125856 examples: {'rewards_train/chosen': '0.05615', 'rewards_train/rejected': '0.083524', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027374', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-134.95', 'loss/train': '0.71338', 'examples_per_second': '31.639', 'grad_norm': '27.875', 'counters/examples': 125856, 'counters/updates': 3933}
skipping logging after 125888 examples to avoid logging too frequently
train stats after 125920 examples: {'rewards_train/chosen': '0.12342', 'rewards_train/rejected': '0.075745', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047677', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-180.46', 'loss/train': '0.68726', 'examples_per_second': '30.677', 'grad_norm': '34.5', 'counters/examples': 125920, 'counters/updates': 3935}
train stats after 125952 examples: {'rewards_train/chosen': '0.14799', 'rewards_train/rejected': '0.038179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10981', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-152.19', 'loss/train': '0.64705', 'examples_per_second': '31.431', 'grad_norm': '24.25', 'counters/examples': 125952, 'counters/updates': 3936}
skipping logging after 125984 examples to avoid logging too frequently
train stats after 126016 examples: {'rewards_train/chosen': '0.080388', 'rewards_train/rejected': '-0.0040943', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084482', 'logps_train/rejected': '-118.22', 'logps_train/chosen': '-134.66', 'loss/train': '0.65844', 'examples_per_second': '31.704', 'grad_norm': '26.75', 'counters/examples': 126016, 'counters/updates': 3938}
train stats after 126048 examples: {'rewards_train/chosen': '0.050513', 'rewards_train/rejected': '0.074565', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.024052', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-127.03', 'loss/train': '0.71259', 'examples_per_second': '31.17', 'grad_norm': '33.25', 'counters/examples': 126048, 'counters/updates': 3939}
train stats after 126080 examples: {'rewards_train/chosen': '0.14957', 'rewards_train/rejected': '0.045536', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10403', 'logps_train/rejected': '-119.07', 'logps_train/chosen': '-179.64', 'loss/train': '0.65694', 'examples_per_second': '31.487', 'grad_norm': '37.75', 'counters/examples': 126080, 'counters/updates': 3940}
skipping logging after 126112 examples to avoid logging too frequently
train stats after 126144 examples: {'rewards_train/chosen': '0.098677', 'rewards_train/rejected': '0.0083162', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090361', 'logps_train/rejected': '-116.99', 'logps_train/chosen': '-154.86', 'loss/train': '0.65797', 'examples_per_second': '30.132', 'grad_norm': '28.125', 'counters/examples': 126144, 'counters/updates': 3942}
train stats after 126176 examples: {'rewards_train/chosen': '0.070026', 'rewards_train/rejected': '0.0569', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013126', 'logps_train/rejected': '-108.65', 'logps_train/chosen': '-113.3', 'loss/train': '0.69151', 'examples_per_second': '32.284', 'grad_norm': '25.625', 'counters/examples': 126176, 'counters/updates': 3943}
skipping logging after 126208 examples to avoid logging too frequently
train stats after 126240 examples: {'rewards_train/chosen': '0.055736', 'rewards_train/rejected': '0.086788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.031052', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-137.77', 'loss/train': '0.72987', 'examples_per_second': '31.672', 'grad_norm': '28.125', 'counters/examples': 126240, 'counters/updates': 3945}
train stats after 126272 examples: {'rewards_train/chosen': '0.063603', 'rewards_train/rejected': '0.03572', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027883', 'logps_train/rejected': '-136.21', 'logps_train/chosen': '-172', 'loss/train': '0.68541', 'examples_per_second': '30.5', 'grad_norm': '28.75', 'counters/examples': 126272, 'counters/updates': 3946}
train stats after 126304 examples: {'rewards_train/chosen': '0.12794', 'rewards_train/rejected': '0.047596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080341', 'logps_train/rejected': '-147.1', 'logps_train/chosen': '-189.32', 'loss/train': '0.66907', 'examples_per_second': '30.913', 'grad_norm': '29.5', 'counters/examples': 126304, 'counters/updates': 3947}
train stats after 126336 examples: {'rewards_train/chosen': '0.10914', 'rewards_train/rejected': '0.1237', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014559', 'logps_train/rejected': '-156.77', 'logps_train/chosen': '-113.86', 'loss/train': '0.7076', 'examples_per_second': '30.497', 'grad_norm': '27.875', 'counters/examples': 126336, 'counters/updates': 3948}
train stats after 126368 examples: {'rewards_train/chosen': '0.15716', 'rewards_train/rejected': '0.0072623', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14989', 'logps_train/rejected': '-142.63', 'logps_train/chosen': '-187.35', 'loss/train': '0.63203', 'examples_per_second': '30.41', 'grad_norm': '26.625', 'counters/examples': 126368, 'counters/updates': 3949}
train stats after 126400 examples: {'rewards_train/chosen': '0.087603', 'rewards_train/rejected': '0.072231', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015372', 'logps_train/rejected': '-108.3', 'logps_train/chosen': '-180.77', 'loss/train': '0.69562', 'examples_per_second': '30.072', 'grad_norm': '34.75', 'counters/examples': 126400, 'counters/updates': 3950}
train stats after 126432 examples: {'rewards_train/chosen': '0.10857', 'rewards_train/rejected': '0.027187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081386', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-155.19', 'loss/train': '0.66109', 'examples_per_second': '31.105', 'grad_norm': '26.625', 'counters/examples': 126432, 'counters/updates': 3951}
train stats after 126464 examples: {'rewards_train/chosen': '0.098492', 'rewards_train/rejected': '0.070715', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027778', 'logps_train/rejected': '-161.11', 'logps_train/chosen': '-165.56', 'loss/train': '0.69177', 'examples_per_second': '31.63', 'grad_norm': '37', 'counters/examples': 126464, 'counters/updates': 3952}
train stats after 126496 examples: {'rewards_train/chosen': '0.091044', 'rewards_train/rejected': '0.086168', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0048753', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-124.91', 'loss/train': '0.69756', 'examples_per_second': '31.554', 'grad_norm': '26.5', 'counters/examples': 126496, 'counters/updates': 3953}
train stats after 126528 examples: {'rewards_train/chosen': '0.11491', 'rewards_train/rejected': '0.059299', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05561', 'logps_train/rejected': '-91.347', 'logps_train/chosen': '-130.65', 'loss/train': '0.67511', 'examples_per_second': '31.602', 'grad_norm': '24.875', 'counters/examples': 126528, 'counters/updates': 3954}
train stats after 126560 examples: {'rewards_train/chosen': '0.054926', 'rewards_train/rejected': '0.036828', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018098', 'logps_train/rejected': '-109.87', 'logps_train/chosen': '-117.84', 'loss/train': '0.69181', 'examples_per_second': '31.185', 'grad_norm': '23.25', 'counters/examples': 126560, 'counters/updates': 3955}
train stats after 126592 examples: {'rewards_train/chosen': '0.1355', 'rewards_train/rejected': '-0.0059206', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14142', 'logps_train/rejected': '-93.929', 'logps_train/chosen': '-118.2', 'loss/train': '0.63094', 'examples_per_second': '30.515', 'grad_norm': '22.25', 'counters/examples': 126592, 'counters/updates': 3956}
train stats after 126624 examples: {'rewards_train/chosen': '0.1775', 'rewards_train/rejected': '0.080666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096833', 'logps_train/rejected': '-106.69', 'logps_train/chosen': '-128.73', 'loss/train': '0.65513', 'examples_per_second': '30.117', 'grad_norm': '29', 'counters/examples': 126624, 'counters/updates': 3957}
train stats after 126656 examples: {'rewards_train/chosen': '0.12376', 'rewards_train/rejected': '0.010095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11367', 'logps_train/rejected': '-162.18', 'logps_train/chosen': '-159.99', 'loss/train': '0.64586', 'examples_per_second': '31.479', 'grad_norm': '29.125', 'counters/examples': 126656, 'counters/updates': 3958}
skipping logging after 126688 examples to avoid logging too frequently
train stats after 126720 examples: {'rewards_train/chosen': '0.13028', 'rewards_train/rejected': '0.10493', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025354', 'logps_train/rejected': '-141.9', 'logps_train/chosen': '-150.88', 'loss/train': '0.68785', 'examples_per_second': '35.773', 'grad_norm': '26.375', 'counters/examples': 126720, 'counters/updates': 3960}
skipping logging after 126752 examples to avoid logging too frequently
train stats after 126784 examples: {'rewards_train/chosen': '0.088698', 'rewards_train/rejected': '0.0069179', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08178', 'logps_train/rejected': '-97.689', 'logps_train/chosen': '-113.23', 'loss/train': '0.65806', 'examples_per_second': '31.489', 'grad_norm': '23', 'counters/examples': 126784, 'counters/updates': 3962}
train stats after 126816 examples: {'rewards_train/chosen': '0.089147', 'rewards_train/rejected': '0.027946', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061201', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-137.03', 'loss/train': '0.67196', 'examples_per_second': '30.487', 'grad_norm': '26.375', 'counters/examples': 126816, 'counters/updates': 3963}
train stats after 126848 examples: {'rewards_train/chosen': '0.15243', 'rewards_train/rejected': '0.068986', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083449', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-146.33', 'loss/train': '0.65999', 'examples_per_second': '30.592', 'grad_norm': '27.375', 'counters/examples': 126848, 'counters/updates': 3964}
train stats after 126880 examples: {'rewards_train/chosen': '0.14857', 'rewards_train/rejected': '0.042146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10642', 'logps_train/rejected': '-97.426', 'logps_train/chosen': '-146.87', 'loss/train': '0.64839', 'examples_per_second': '31.613', 'grad_norm': '23.625', 'counters/examples': 126880, 'counters/updates': 3965}
train stats after 126912 examples: {'rewards_train/chosen': '0.019201', 'rewards_train/rejected': '0.076798', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.057597', 'logps_train/rejected': '-135.45', 'logps_train/chosen': '-102.09', 'loss/train': '0.72649', 'examples_per_second': '31.617', 'grad_norm': '28.25', 'counters/examples': 126912, 'counters/updates': 3966}
train stats after 126944 examples: {'rewards_train/chosen': '0.16538', 'rewards_train/rejected': '0.03514', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13024', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-130.66', 'loss/train': '0.63978', 'examples_per_second': '24.122', 'grad_norm': '23.625', 'counters/examples': 126944, 'counters/updates': 3967}
train stats after 126976 examples: {'rewards_train/chosen': '0.13588', 'rewards_train/rejected': '0.08209', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053791', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-142.44', 'loss/train': '0.67221', 'examples_per_second': '31.617', 'grad_norm': '26.875', 'counters/examples': 126976, 'counters/updates': 3968}
train stats after 127008 examples: {'rewards_train/chosen': '0.073346', 'rewards_train/rejected': '0.034114', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039232', 'logps_train/rejected': '-110.23', 'logps_train/chosen': '-142.73', 'loss/train': '0.68194', 'examples_per_second': '33.061', 'grad_norm': '25.875', 'counters/examples': 127008, 'counters/updates': 3969}
train stats after 127040 examples: {'rewards_train/chosen': '0.11525', 'rewards_train/rejected': '0.019421', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095825', 'logps_train/rejected': '-101.04', 'logps_train/chosen': '-125.6', 'loss/train': '0.65307', 'examples_per_second': '31.161', 'grad_norm': '25', 'counters/examples': 127040, 'counters/updates': 3970}
train stats after 127072 examples: {'rewards_train/chosen': '0.13044', 'rewards_train/rejected': '0.085968', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044474', 'logps_train/rejected': '-153.24', 'logps_train/chosen': '-182.97', 'loss/train': '0.67686', 'examples_per_second': '31.77', 'grad_norm': '30.625', 'counters/examples': 127072, 'counters/updates': 3971}
train stats after 127104 examples: {'rewards_train/chosen': '0.12312', 'rewards_train/rejected': '0.00033919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12278', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-124.45', 'loss/train': '0.64517', 'examples_per_second': '30.591', 'grad_norm': '27.5', 'counters/examples': 127104, 'counters/updates': 3972}
train stats after 127136 examples: {'rewards_train/chosen': '0.085716', 'rewards_train/rejected': '-0.0050731', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090789', 'logps_train/rejected': '-93.403', 'logps_train/chosen': '-164.61', 'loss/train': '0.65346', 'examples_per_second': '32.791', 'grad_norm': '25.625', 'counters/examples': 127136, 'counters/updates': 3973}
skipping logging after 127168 examples to avoid logging too frequently
train stats after 127200 examples: {'rewards_train/chosen': '0.07317', 'rewards_train/rejected': '0.0907', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01753', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-119.5', 'loss/train': '0.71117', 'examples_per_second': '30.05', 'grad_norm': '30.875', 'counters/examples': 127200, 'counters/updates': 3975}
train stats after 127232 examples: {'rewards_train/chosen': '0.2112', 'rewards_train/rejected': '0.0089813', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20222', 'logps_train/rejected': '-92.943', 'logps_train/chosen': '-167.83', 'loss/train': '0.61484', 'examples_per_second': '30.769', 'grad_norm': '28.5', 'counters/examples': 127232, 'counters/updates': 3976}
train stats after 127264 examples: {'rewards_train/chosen': '0.16005', 'rewards_train/rejected': '0.082518', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077529', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-185.98', 'loss/train': '0.66317', 'examples_per_second': '31.585', 'grad_norm': '28.75', 'counters/examples': 127264, 'counters/updates': 3977}
train stats after 127296 examples: {'rewards_train/chosen': '0.18862', 'rewards_train/rejected': '0.041718', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1469', 'logps_train/rejected': '-146.46', 'logps_train/chosen': '-165.49', 'loss/train': '0.64408', 'examples_per_second': '31.567', 'grad_norm': '23.875', 'counters/examples': 127296, 'counters/updates': 3978}
train stats after 127328 examples: {'rewards_train/chosen': '0.071562', 'rewards_train/rejected': '0.032803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038759', 'logps_train/rejected': '-111.79', 'logps_train/chosen': '-136.81', 'loss/train': '0.68309', 'examples_per_second': '32.305', 'grad_norm': '41.5', 'counters/examples': 127328, 'counters/updates': 3979}
train stats after 127360 examples: {'rewards_train/chosen': '0.094014', 'rewards_train/rejected': '0.055696', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038318', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-120.3', 'loss/train': '0.67787', 'examples_per_second': '31.022', 'grad_norm': '25.5', 'counters/examples': 127360, 'counters/updates': 3980}
train stats after 127392 examples: {'rewards_train/chosen': '0.094758', 'rewards_train/rejected': '0.043773', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050986', 'logps_train/rejected': '-160.87', 'logps_train/chosen': '-179.77', 'loss/train': '0.67872', 'examples_per_second': '31.548', 'grad_norm': '31.5', 'counters/examples': 127392, 'counters/updates': 3981}
train stats after 127424 examples: {'rewards_train/chosen': '0.13653', 'rewards_train/rejected': '0.059655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076876', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-166.17', 'loss/train': '0.66595', 'examples_per_second': '30.142', 'grad_norm': '26.5', 'counters/examples': 127424, 'counters/updates': 3982}
skipping logging after 127456 examples to avoid logging too frequently
train stats after 127488 examples: {'rewards_train/chosen': '0.071915', 'rewards_train/rejected': '-0.018374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090289', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-121.35', 'loss/train': '0.65832', 'examples_per_second': '30.478', 'grad_norm': '24.625', 'counters/examples': 127488, 'counters/updates': 3984}
train stats after 127520 examples: {'rewards_train/chosen': '0.058044', 'rewards_train/rejected': '0.0088311', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049213', 'logps_train/rejected': '-150.4', 'logps_train/chosen': '-140.56', 'loss/train': '0.67707', 'examples_per_second': '32.857', 'grad_norm': '27.25', 'counters/examples': 127520, 'counters/updates': 3985}
train stats after 127552 examples: {'rewards_train/chosen': '0.096629', 'rewards_train/rejected': '0.096002', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00062676', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-129.96', 'loss/train': '0.70027', 'examples_per_second': '31.389', 'grad_norm': '26.75', 'counters/examples': 127552, 'counters/updates': 3986}
skipping logging after 127584 examples to avoid logging too frequently
train stats after 127616 examples: {'rewards_train/chosen': '0.058266', 'rewards_train/rejected': '0.052043', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0062231', 'logps_train/rejected': '-118.29', 'logps_train/chosen': '-130.92', 'loss/train': '0.69839', 'examples_per_second': '30.241', 'grad_norm': '27.375', 'counters/examples': 127616, 'counters/updates': 3988}
skipping logging after 127648 examples to avoid logging too frequently
train stats after 127680 examples: {'rewards_train/chosen': '0.18019', 'rewards_train/rejected': '0.07787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10232', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-130.8', 'loss/train': '0.66441', 'examples_per_second': '32.524', 'grad_norm': '24.375', 'counters/examples': 127680, 'counters/updates': 3990}
train stats after 127712 examples: {'rewards_train/chosen': '0.076614', 'rewards_train/rejected': '0.15385', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.077241', 'logps_train/rejected': '-142.54', 'logps_train/chosen': '-173.2', 'loss/train': '0.75191', 'examples_per_second': '31.52', 'grad_norm': '41', 'counters/examples': 127712, 'counters/updates': 3991}
train stats after 127744 examples: {'rewards_train/chosen': '0.13693', 'rewards_train/rejected': '0.039619', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097315', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-134.56', 'loss/train': '0.65789', 'examples_per_second': '30.077', 'grad_norm': '28.625', 'counters/examples': 127744, 'counters/updates': 3992}
train stats after 127776 examples: {'rewards_train/chosen': '0.081482', 'rewards_train/rejected': '0.045441', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036041', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-113.23', 'loss/train': '0.68756', 'examples_per_second': '31.564', 'grad_norm': '25', 'counters/examples': 127776, 'counters/updates': 3993}
train stats after 127808 examples: {'rewards_train/chosen': '0.13073', 'rewards_train/rejected': '-0.018359', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14909', 'logps_train/rejected': '-143.19', 'logps_train/chosen': '-141.69', 'loss/train': '0.62828', 'examples_per_second': '29.82', 'grad_norm': '28.25', 'counters/examples': 127808, 'counters/updates': 3994}
train stats after 127840 examples: {'rewards_train/chosen': '0.10381', 'rewards_train/rejected': '0.032754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071055', 'logps_train/rejected': '-139.87', 'logps_train/chosen': '-171.15', 'loss/train': '0.66482', 'examples_per_second': '32.441', 'grad_norm': '28', 'counters/examples': 127840, 'counters/updates': 3995}
train stats after 127872 examples: {'rewards_train/chosen': '0.070452', 'rewards_train/rejected': '0.05027', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020182', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-125.46', 'loss/train': '0.69392', 'examples_per_second': '31.555', 'grad_norm': '25', 'counters/examples': 127872, 'counters/updates': 3996}
train stats after 127904 examples: {'rewards_train/chosen': '0.067469', 'rewards_train/rejected': '0.017604', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049865', 'logps_train/rejected': '-83.953', 'logps_train/chosen': '-91.511', 'loss/train': '0.67391', 'examples_per_second': '32.464', 'grad_norm': '22.125', 'counters/examples': 127904, 'counters/updates': 3997}
train stats after 127936 examples: {'rewards_train/chosen': '0.033764', 'rewards_train/rejected': '-0.054322', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088086', 'logps_train/rejected': '-153.86', 'logps_train/chosen': '-155.12', 'loss/train': '0.66022', 'examples_per_second': '33.25', 'grad_norm': '28.5', 'counters/examples': 127936, 'counters/updates': 3998}
train stats after 127968 examples: {'rewards_train/chosen': '0.069485', 'rewards_train/rejected': '0.069304', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00018106', 'logps_train/rejected': '-108.92', 'logps_train/chosen': '-160.54', 'loss/train': '0.70009', 'examples_per_second': '31.564', 'grad_norm': '44.5', 'counters/examples': 127968, 'counters/updates': 3999}
train stats after 128000 examples: {'rewards_train/chosen': '0.031586', 'rewards_train/rejected': '0.098176', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06659', 'logps_train/rejected': '-106.27', 'logps_train/chosen': '-112.46', 'loss/train': '0.73221', 'examples_per_second': '30.606', 'grad_norm': '61.5', 'counters/examples': 128000, 'counters/updates': 4000}
skipping logging after 128032 examples to avoid logging too frequently
train stats after 128064 examples: {'rewards_train/chosen': '0.081941', 'rewards_train/rejected': '0.049179', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032762', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-121.91', 'loss/train': '0.68356', 'examples_per_second': '32.035', 'grad_norm': '26.125', 'counters/examples': 128064, 'counters/updates': 4002}
train stats after 128096 examples: {'rewards_train/chosen': '0.12428', 'rewards_train/rejected': '0.096517', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027765', 'logps_train/rejected': '-165.58', 'logps_train/chosen': '-126.88', 'loss/train': '0.68911', 'examples_per_second': '31.629', 'grad_norm': '29.375', 'counters/examples': 128096, 'counters/updates': 4003}
train stats after 128128 examples: {'rewards_train/chosen': '0.014467', 'rewards_train/rejected': '0.018697', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0042305', 'logps_train/rejected': '-111.6', 'logps_train/chosen': '-111.2', 'loss/train': '0.70274', 'examples_per_second': '31.411', 'grad_norm': '25.5', 'counters/examples': 128128, 'counters/updates': 4004}
train stats after 128160 examples: {'rewards_train/chosen': '0.10983', 'rewards_train/rejected': '0.055345', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054487', 'logps_train/rejected': '-97.687', 'logps_train/chosen': '-115.76', 'loss/train': '0.67418', 'examples_per_second': '31.312', 'grad_norm': '24.625', 'counters/examples': 128160, 'counters/updates': 4005}
train stats after 128192 examples: {'rewards_train/chosen': '0.096533', 'rewards_train/rejected': '0.010753', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085779', 'logps_train/rejected': '-104.83', 'logps_train/chosen': '-129.43', 'loss/train': '0.6574', 'examples_per_second': '32.074', 'grad_norm': '26.625', 'counters/examples': 128192, 'counters/updates': 4006}
train stats after 128224 examples: {'rewards_train/chosen': '0.13178', 'rewards_train/rejected': '0.015768', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11601', 'logps_train/rejected': '-120.49', 'logps_train/chosen': '-135.28', 'loss/train': '0.65134', 'examples_per_second': '31.525', 'grad_norm': '24.125', 'counters/examples': 128224, 'counters/updates': 4007}
train stats after 128256 examples: {'rewards_train/chosen': '0.10652', 'rewards_train/rejected': '-0.019798', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12631', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-165.86', 'loss/train': '0.6436', 'examples_per_second': '31.632', 'grad_norm': '26.875', 'counters/examples': 128256, 'counters/updates': 4008}
skipping logging after 128288 examples to avoid logging too frequently
train stats after 128320 examples: {'rewards_train/chosen': '0.14353', 'rewards_train/rejected': '0.02011', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12342', 'logps_train/rejected': '-121.78', 'logps_train/chosen': '-170.6', 'loss/train': '0.64477', 'examples_per_second': '30.025', 'grad_norm': '28.5', 'counters/examples': 128320, 'counters/updates': 4010}
train stats after 128352 examples: {'rewards_train/chosen': '0.099388', 'rewards_train/rejected': '-0.0041242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10351', 'logps_train/rejected': '-115.86', 'logps_train/chosen': '-118.62', 'loss/train': '0.65082', 'examples_per_second': '30.863', 'grad_norm': '22.625', 'counters/examples': 128352, 'counters/updates': 4011}
train stats after 128384 examples: {'rewards_train/chosen': '0.085238', 'rewards_train/rejected': '0.084863', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00037474', 'logps_train/rejected': '-147.72', 'logps_train/chosen': '-162.43', 'loss/train': '0.69956', 'examples_per_second': '31.582', 'grad_norm': '29.5', 'counters/examples': 128384, 'counters/updates': 4012}
train stats after 128416 examples: {'rewards_train/chosen': '0.13814', 'rewards_train/rejected': '0.046572', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091565', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-131.43', 'loss/train': '0.65677', 'examples_per_second': '31.601', 'grad_norm': '27.875', 'counters/examples': 128416, 'counters/updates': 4013}
train stats after 128448 examples: {'rewards_train/chosen': '0.075156', 'rewards_train/rejected': '-0.0041214', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079277', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-114.83', 'loss/train': '0.66411', 'examples_per_second': '30.904', 'grad_norm': '29.375', 'counters/examples': 128448, 'counters/updates': 4014}
skipping logging after 128480 examples to avoid logging too frequently
train stats after 128512 examples: {'rewards_train/chosen': '0.16024', 'rewards_train/rejected': '0.033147', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12709', 'logps_train/rejected': '-143.21', 'logps_train/chosen': '-149.64', 'loss/train': '0.64069', 'examples_per_second': '31.053', 'grad_norm': '26.25', 'counters/examples': 128512, 'counters/updates': 4016}
train stats after 128544 examples: {'rewards_train/chosen': '0.047557', 'rewards_train/rejected': '0.050012', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0024551', 'logps_train/rejected': '-129.56', 'logps_train/chosen': '-129.88', 'loss/train': '0.7008', 'examples_per_second': '32.477', 'grad_norm': '29.375', 'counters/examples': 128544, 'counters/updates': 4017}
train stats after 128576 examples: {'rewards_train/chosen': '0.09157', 'rewards_train/rejected': '0.046873', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044697', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-142.33', 'loss/train': '0.67665', 'examples_per_second': '30.811', 'grad_norm': '26.375', 'counters/examples': 128576, 'counters/updates': 4018}
train stats after 128608 examples: {'rewards_train/chosen': '0.066878', 'rewards_train/rejected': '0.032754', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034124', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-126.52', 'loss/train': '0.68738', 'examples_per_second': '31.572', 'grad_norm': '26.25', 'counters/examples': 128608, 'counters/updates': 4019}
train stats after 128640 examples: {'rewards_train/chosen': '0.085452', 'rewards_train/rejected': '0.076842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0086095', 'logps_train/rejected': '-142.04', 'logps_train/chosen': '-164.65', 'loss/train': '0.69778', 'examples_per_second': '30.222', 'grad_norm': '39.5', 'counters/examples': 128640, 'counters/updates': 4020}
train stats after 128672 examples: {'rewards_train/chosen': '0.13257', 'rewards_train/rejected': '0.12166', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010907', 'logps_train/rejected': '-160.68', 'logps_train/chosen': '-155.47', 'loss/train': '0.69667', 'examples_per_second': '31.08', 'grad_norm': '27.25', 'counters/examples': 128672, 'counters/updates': 4021}
train stats after 128704 examples: {'rewards_train/chosen': '0.051527', 'rewards_train/rejected': '-0.0037252', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055253', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-116.91', 'loss/train': '0.6726', 'examples_per_second': '31.752', 'grad_norm': '33.75', 'counters/examples': 128704, 'counters/updates': 4022}
skipping logging after 128736 examples to avoid logging too frequently
train stats after 128768 examples: {'rewards_train/chosen': '0.15635', 'rewards_train/rejected': '0.027151', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1292', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-175.57', 'loss/train': '0.64177', 'examples_per_second': '31.574', 'grad_norm': '30', 'counters/examples': 128768, 'counters/updates': 4024}
train stats after 128800 examples: {'rewards_train/chosen': '0.082855', 'rewards_train/rejected': '0.021602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061254', 'logps_train/rejected': '-98.296', 'logps_train/chosen': '-106.7', 'loss/train': '0.67289', 'examples_per_second': '30.966', 'grad_norm': '22.5', 'counters/examples': 128800, 'counters/updates': 4025}
train stats after 128832 examples: {'rewards_train/chosen': '0.096902', 'rewards_train/rejected': '0.037573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059329', 'logps_train/rejected': '-149.11', 'logps_train/chosen': '-154.67', 'loss/train': '0.67481', 'examples_per_second': '31.405', 'grad_norm': '29.875', 'counters/examples': 128832, 'counters/updates': 4026}
skipping logging after 128864 examples to avoid logging too frequently
train stats after 128896 examples: {'rewards_train/chosen': '0.12094', 'rewards_train/rejected': '-0.01262', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13356', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-140.15', 'loss/train': '0.63881', 'examples_per_second': '32.007', 'grad_norm': '32', 'counters/examples': 128896, 'counters/updates': 4028}
train stats after 128928 examples: {'rewards_train/chosen': '0.11525', 'rewards_train/rejected': '0.07805', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.037204', 'logps_train/rejected': '-156.85', 'logps_train/chosen': '-151.14', 'loss/train': '0.68231', 'examples_per_second': '30.604', 'grad_norm': '31.375', 'counters/examples': 128928, 'counters/updates': 4029}
train stats after 128960 examples: {'rewards_train/chosen': '0.064963', 'rewards_train/rejected': '0.041289', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023674', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-139.19', 'loss/train': '0.68685', 'examples_per_second': '32.094', 'grad_norm': '25.25', 'counters/examples': 128960, 'counters/updates': 4030}
train stats after 128992 examples: {'rewards_train/chosen': '0.074685', 'rewards_train/rejected': '0.0076081', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067076', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-176.16', 'loss/train': '0.67076', 'examples_per_second': '32.27', 'grad_norm': '28.5', 'counters/examples': 128992, 'counters/updates': 4031}
train stats after 129024 examples: {'rewards_train/chosen': '0.07538', 'rewards_train/rejected': '0.014201', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061179', 'logps_train/rejected': '-111.89', 'logps_train/chosen': '-100.82', 'loss/train': '0.6711', 'examples_per_second': '31.097', 'grad_norm': '32', 'counters/examples': 129024, 'counters/updates': 4032}
train stats after 129056 examples: {'rewards_train/chosen': '0.098813', 'rewards_train/rejected': '0.01136', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087453', 'logps_train/rejected': '-113.9', 'logps_train/chosen': '-124.72', 'loss/train': '0.65766', 'examples_per_second': '32.91', 'grad_norm': '24.125', 'counters/examples': 129056, 'counters/updates': 4033}
train stats after 129088 examples: {'rewards_train/chosen': '0.073303', 'rewards_train/rejected': '-0.031068', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10437', 'logps_train/rejected': '-103.24', 'logps_train/chosen': '-139.42', 'loss/train': '0.65108', 'examples_per_second': '32.167', 'grad_norm': '25.25', 'counters/examples': 129088, 'counters/updates': 4034}
train stats after 129120 examples: {'rewards_train/chosen': '0.045058', 'rewards_train/rejected': '0.050658', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0055999', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-129.64', 'loss/train': '0.70683', 'examples_per_second': '32.727', 'grad_norm': '28', 'counters/examples': 129120, 'counters/updates': 4035}
train stats after 129152 examples: {'rewards_train/chosen': '0.067403', 'rewards_train/rejected': '0.035218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032185', 'logps_train/rejected': '-97.451', 'logps_train/chosen': '-124.92', 'loss/train': '0.68528', 'examples_per_second': '31.506', 'grad_norm': '28.5', 'counters/examples': 129152, 'counters/updates': 4036}
train stats after 129184 examples: {'rewards_train/chosen': '0.087443', 'rewards_train/rejected': '0.022079', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065364', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-142.8', 'loss/train': '0.66569', 'examples_per_second': '31.458', 'grad_norm': '23.5', 'counters/examples': 129184, 'counters/updates': 4037}
skipping logging after 129216 examples to avoid logging too frequently
train stats after 129248 examples: {'rewards_train/chosen': '0.1146', 'rewards_train/rejected': '0.052312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06229', 'logps_train/rejected': '-136.26', 'logps_train/chosen': '-154.46', 'loss/train': '0.67286', 'examples_per_second': '34.386', 'grad_norm': '27', 'counters/examples': 129248, 'counters/updates': 4039}
train stats after 129280 examples: {'rewards_train/chosen': '0.11419', 'rewards_train/rejected': '0.034266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079927', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-122.83', 'loss/train': '0.66255', 'examples_per_second': '31.558', 'grad_norm': '22.5', 'counters/examples': 129280, 'counters/updates': 4040}
train stats after 129312 examples: {'rewards_train/chosen': '0.090224', 'rewards_train/rejected': '0.038564', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05166', 'logps_train/rejected': '-122.65', 'logps_train/chosen': '-143.65', 'loss/train': '0.67537', 'examples_per_second': '32.593', 'grad_norm': '26', 'counters/examples': 129312, 'counters/updates': 4041}
train stats after 129344 examples: {'rewards_train/chosen': '0.11847', 'rewards_train/rejected': '0.071122', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047346', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-124.39', 'loss/train': '0.68253', 'examples_per_second': '29.888', 'grad_norm': '26.5', 'counters/examples': 129344, 'counters/updates': 4042}
train stats after 129376 examples: {'rewards_train/chosen': '0.11297', 'rewards_train/rejected': '0.032254', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080716', 'logps_train/rejected': '-122.95', 'logps_train/chosen': '-113.34', 'loss/train': '0.66235', 'examples_per_second': '31.089', 'grad_norm': '23.125', 'counters/examples': 129376, 'counters/updates': 4043}
train stats after 129408 examples: {'rewards_train/chosen': '0.11288', 'rewards_train/rejected': '0.055752', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057126', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-148.84', 'loss/train': '0.67679', 'examples_per_second': '32.882', 'grad_norm': '27.625', 'counters/examples': 129408, 'counters/updates': 4044}
train stats after 129440 examples: {'rewards_train/chosen': '0.12043', 'rewards_train/rejected': '0.11872', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0017155', 'logps_train/rejected': '-172.87', 'logps_train/chosen': '-126.42', 'loss/train': '0.70677', 'examples_per_second': '31.572', 'grad_norm': '40', 'counters/examples': 129440, 'counters/updates': 4045}
train stats after 129472 examples: {'rewards_train/chosen': '0.13397', 'rewards_train/rejected': '0.013942', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12003', 'logps_train/rejected': '-120.88', 'logps_train/chosen': '-108.56', 'loss/train': '0.64505', 'examples_per_second': '31.602', 'grad_norm': '24.75', 'counters/examples': 129472, 'counters/updates': 4046}
train stats after 129504 examples: {'rewards_train/chosen': '0.076155', 'rewards_train/rejected': '0.11128', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.035127', 'logps_train/rejected': '-161.85', 'logps_train/chosen': '-161.24', 'loss/train': '0.7199', 'examples_per_second': '31.446', 'grad_norm': '33.75', 'counters/examples': 129504, 'counters/updates': 4047}
train stats after 129536 examples: {'rewards_train/chosen': '0.063267', 'rewards_train/rejected': '-0.019936', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083204', 'logps_train/rejected': '-111.59', 'logps_train/chosen': '-150.74', 'loss/train': '0.65711', 'examples_per_second': '31.642', 'grad_norm': '26.875', 'counters/examples': 129536, 'counters/updates': 4048}
train stats after 129568 examples: {'rewards_train/chosen': '0.055457', 'rewards_train/rejected': '0.021835', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033622', 'logps_train/rejected': '-156.32', 'logps_train/chosen': '-121.48', 'loss/train': '0.68289', 'examples_per_second': '31.634', 'grad_norm': '26.5', 'counters/examples': 129568, 'counters/updates': 4049}
train stats after 129600 examples: {'rewards_train/chosen': '0.15801', 'rewards_train/rejected': '0.12371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034298', 'logps_train/rejected': '-153.45', 'logps_train/chosen': '-181.25', 'loss/train': '0.68597', 'examples_per_second': '31.612', 'grad_norm': '30', 'counters/examples': 129600, 'counters/updates': 4050}
train stats after 129632 examples: {'rewards_train/chosen': '0.10387', 'rewards_train/rejected': '0.040137', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063736', 'logps_train/rejected': '-158.1', 'logps_train/chosen': '-158.42', 'loss/train': '0.68169', 'examples_per_second': '33.065', 'grad_norm': '31.25', 'counters/examples': 129632, 'counters/updates': 4051}
train stats after 129664 examples: {'rewards_train/chosen': '0.23604', 'rewards_train/rejected': '0.03845', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19759', 'logps_train/rejected': '-125.63', 'logps_train/chosen': '-135.72', 'loss/train': '0.61292', 'examples_per_second': '30.843', 'grad_norm': '21.75', 'counters/examples': 129664, 'counters/updates': 4052}
train stats after 129696 examples: {'rewards_train/chosen': '0.10947', 'rewards_train/rejected': '0.014326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095145', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-133.03', 'loss/train': '0.65313', 'examples_per_second': '31.52', 'grad_norm': '25.75', 'counters/examples': 129696, 'counters/updates': 4053}
skipping logging after 129728 examples to avoid logging too frequently
train stats after 129760 examples: {'rewards_train/chosen': '0.13806', 'rewards_train/rejected': '0.0186', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11946', 'logps_train/rejected': '-101.55', 'logps_train/chosen': '-143.97', 'loss/train': '0.64551', 'examples_per_second': '31.633', 'grad_norm': '25.25', 'counters/examples': 129760, 'counters/updates': 4055}
train stats after 129792 examples: {'rewards_train/chosen': '0.086541', 'rewards_train/rejected': '0.092597', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0060556', 'logps_train/rejected': '-167.85', 'logps_train/chosen': '-153.81', 'loss/train': '0.70519', 'examples_per_second': '31.501', 'grad_norm': '31.875', 'counters/examples': 129792, 'counters/updates': 4056}
train stats after 129824 examples: {'rewards_train/chosen': '0.16373', 'rewards_train/rejected': '0.11916', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044565', 'logps_train/rejected': '-149.42', 'logps_train/chosen': '-142.09', 'loss/train': '0.67746', 'examples_per_second': '31.398', 'grad_norm': '26.25', 'counters/examples': 129824, 'counters/updates': 4057}
skipping logging after 129856 examples to avoid logging too frequently
train stats after 129888 examples: {'rewards_train/chosen': '0.12932', 'rewards_train/rejected': '0.036906', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092419', 'logps_train/rejected': '-139.01', 'logps_train/chosen': '-142.96', 'loss/train': '0.66077', 'examples_per_second': '33.931', 'grad_norm': '26.375', 'counters/examples': 129888, 'counters/updates': 4059}
skipping logging after 129920 examples to avoid logging too frequently
train stats after 129952 examples: {'rewards_train/chosen': '0.12541', 'rewards_train/rejected': '0.0099517', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11545', 'logps_train/rejected': '-110.48', 'logps_train/chosen': '-118.47', 'loss/train': '0.64383', 'examples_per_second': '31.039', 'grad_norm': '28.75', 'counters/examples': 129952, 'counters/updates': 4061}
train stats after 129984 examples: {'rewards_train/chosen': '0.016062', 'rewards_train/rejected': '0.0065839', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0094781', 'logps_train/rejected': '-87.013', 'logps_train/chosen': '-133.51', 'loss/train': '0.69285', 'examples_per_second': '32.067', 'grad_norm': '23.875', 'counters/examples': 129984, 'counters/updates': 4062}
train stats after 130016 examples: {'rewards_train/chosen': '0.1048', 'rewards_train/rejected': '0.039902', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064897', 'logps_train/rejected': '-155.06', 'logps_train/chosen': '-125.41', 'loss/train': '0.67003', 'examples_per_second': '31.52', 'grad_norm': '27.25', 'counters/examples': 130016, 'counters/updates': 4063}
train stats after 130048 examples: {'rewards_train/chosen': '0.1403', 'rewards_train/rejected': '0.09931', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040987', 'logps_train/rejected': '-116.06', 'logps_train/chosen': '-106.97', 'loss/train': '0.67973', 'examples_per_second': '23.436', 'grad_norm': '26.375', 'counters/examples': 130048, 'counters/updates': 4064}
train stats after 130080 examples: {'rewards_train/chosen': '0.059903', 'rewards_train/rejected': '0.032691', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027212', 'logps_train/rejected': '-116.15', 'logps_train/chosen': '-118.07', 'loss/train': '0.68736', 'examples_per_second': '30.091', 'grad_norm': '24.625', 'counters/examples': 130080, 'counters/updates': 4065}
skipping logging after 130112 examples to avoid logging too frequently
train stats after 130144 examples: {'rewards_train/chosen': '0.043404', 'rewards_train/rejected': '-0.00093908', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044343', 'logps_train/rejected': '-100.54', 'logps_train/chosen': '-102.3', 'loss/train': '0.67904', 'examples_per_second': '29.345', 'grad_norm': '22.5', 'counters/examples': 130144, 'counters/updates': 4067}
skipping logging after 130176 examples to avoid logging too frequently
train stats after 130208 examples: {'rewards_train/chosen': '0.040388', 'rewards_train/rejected': '-0.021584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061972', 'logps_train/rejected': '-103.15', 'logps_train/chosen': '-154.27', 'loss/train': '0.67074', 'examples_per_second': '32.061', 'grad_norm': '26.125', 'counters/examples': 130208, 'counters/updates': 4069}
train stats after 130240 examples: {'rewards_train/chosen': '-0.0092619', 'rewards_train/rejected': '-0.039624', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030363', 'logps_train/rejected': '-91.578', 'logps_train/chosen': '-138.74', 'loss/train': '0.68561', 'examples_per_second': '32.493', 'grad_norm': '28.25', 'counters/examples': 130240, 'counters/updates': 4070}
skipping logging after 130272 examples to avoid logging too frequently
train stats after 130304 examples: {'rewards_train/chosen': '0.026016', 'rewards_train/rejected': '0.050734', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024718', 'logps_train/rejected': '-98.066', 'logps_train/chosen': '-127.67', 'loss/train': '0.71206', 'examples_per_second': '31.545', 'grad_norm': '24.875', 'counters/examples': 130304, 'counters/updates': 4072}
train stats after 130336 examples: {'rewards_train/chosen': '0.18955', 'rewards_train/rejected': '0.026864', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16269', 'logps_train/rejected': '-147.8', 'logps_train/chosen': '-183.24', 'loss/train': '0.62545', 'examples_per_second': '31.617', 'grad_norm': '27.25', 'counters/examples': 130336, 'counters/updates': 4073}
train stats after 130368 examples: {'rewards_train/chosen': '0.07514', 'rewards_train/rejected': '0.047417', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027723', 'logps_train/rejected': '-107.74', 'logps_train/chosen': '-91.563', 'loss/train': '0.68446', 'examples_per_second': '30.639', 'grad_norm': '23.125', 'counters/examples': 130368, 'counters/updates': 4074}
train stats after 130400 examples: {'rewards_train/chosen': '0.003494', 'rewards_train/rejected': '-0.0072697', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010764', 'logps_train/rejected': '-97.939', 'logps_train/chosen': '-102.84', 'loss/train': '0.69379', 'examples_per_second': '32.85', 'grad_norm': '25.75', 'counters/examples': 130400, 'counters/updates': 4075}
train stats after 130432 examples: {'rewards_train/chosen': '0.085758', 'rewards_train/rejected': '0.041039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044719', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-109.73', 'loss/train': '0.67772', 'examples_per_second': '32.61', 'grad_norm': '29.375', 'counters/examples': 130432, 'counters/updates': 4076}
skipping logging after 130464 examples to avoid logging too frequently
train stats after 130496 examples: {'rewards_train/chosen': '0.098068', 'rewards_train/rejected': '0.06388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034188', 'logps_train/rejected': '-141.35', 'logps_train/chosen': '-153.59', 'loss/train': '0.6831', 'examples_per_second': '31.006', 'grad_norm': '29.875', 'counters/examples': 130496, 'counters/updates': 4078}
train stats after 130528 examples: {'rewards_train/chosen': '0.054828', 'rewards_train/rejected': '-0.017291', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072119', 'logps_train/rejected': '-111.43', 'logps_train/chosen': '-163.56', 'loss/train': '0.6656', 'examples_per_second': '30.734', 'grad_norm': '25.75', 'counters/examples': 130528, 'counters/updates': 4079}
train stats after 130560 examples: {'rewards_train/chosen': '0.19794', 'rewards_train/rejected': '0.064004', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13394', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-172.27', 'loss/train': '0.63646', 'examples_per_second': '31.6', 'grad_norm': '26.375', 'counters/examples': 130560, 'counters/updates': 4080}
train stats after 130592 examples: {'rewards_train/chosen': '0.12797', 'rewards_train/rejected': '0.030578', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097392', 'logps_train/rejected': '-95.3', 'logps_train/chosen': '-99.646', 'loss/train': '0.6543', 'examples_per_second': '30.556', 'grad_norm': '24.625', 'counters/examples': 130592, 'counters/updates': 4081}
train stats after 130624 examples: {'rewards_train/chosen': '0.17892', 'rewards_train/rejected': '0.046135', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13278', 'logps_train/rejected': '-107.57', 'logps_train/chosen': '-140.68', 'loss/train': '0.63744', 'examples_per_second': '33.072', 'grad_norm': '24.5', 'counters/examples': 130624, 'counters/updates': 4082}
skipping logging after 130656 examples to avoid logging too frequently
train stats after 130688 examples: {'rewards_train/chosen': '0.14331', 'rewards_train/rejected': '0.070444', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07287', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-141.47', 'loss/train': '0.66646', 'examples_per_second': '32.45', 'grad_norm': '29.875', 'counters/examples': 130688, 'counters/updates': 4084}
train stats after 130720 examples: {'rewards_train/chosen': '0.14694', 'rewards_train/rejected': '0.17399', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027044', 'logps_train/rejected': '-137.12', 'logps_train/chosen': '-177.59', 'loss/train': '0.72018', 'examples_per_second': '31.6', 'grad_norm': '31', 'counters/examples': 130720, 'counters/updates': 4085}
train stats after 130752 examples: {'rewards_train/chosen': '0.13334', 'rewards_train/rejected': '0.036525', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096818', 'logps_train/rejected': '-108.26', 'logps_train/chosen': '-133.8', 'loss/train': '0.65625', 'examples_per_second': '31.062', 'grad_norm': '43.5', 'counters/examples': 130752, 'counters/updates': 4086}
train stats after 130784 examples: {'rewards_train/chosen': '0.12974', 'rewards_train/rejected': '0.031285', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098459', 'logps_train/rejected': '-152.02', 'logps_train/chosen': '-131.08', 'loss/train': '0.65281', 'examples_per_second': '31.716', 'grad_norm': '29.125', 'counters/examples': 130784, 'counters/updates': 4087}
train stats after 130816 examples: {'rewards_train/chosen': '0.14514', 'rewards_train/rejected': '0.048949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096194', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-124.93', 'loss/train': '0.66475', 'examples_per_second': '30.943', 'grad_norm': '27.875', 'counters/examples': 130816, 'counters/updates': 4088}
skipping logging after 130848 examples to avoid logging too frequently
train stats after 130880 examples: {'rewards_train/chosen': '0.10299', 'rewards_train/rejected': '0.089347', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013642', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-142.19', 'loss/train': '0.70131', 'examples_per_second': '30.014', 'grad_norm': '27.5', 'counters/examples': 130880, 'counters/updates': 4090}
train stats after 130912 examples: {'rewards_train/chosen': '0.14593', 'rewards_train/rejected': '0.078799', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06713', 'logps_train/rejected': '-118.08', 'logps_train/chosen': '-160.54', 'loss/train': '0.66842', 'examples_per_second': '31.392', 'grad_norm': '25.75', 'counters/examples': 130912, 'counters/updates': 4091}
skipping logging after 130944 examples to avoid logging too frequently
train stats after 130976 examples: {'rewards_train/chosen': '0.10567', 'rewards_train/rejected': '0.00021231', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10546', 'logps_train/rejected': '-98.741', 'logps_train/chosen': '-130.8', 'loss/train': '0.65402', 'examples_per_second': '30.587', 'grad_norm': '22.125', 'counters/examples': 130976, 'counters/updates': 4093}
train stats after 131008 examples: {'rewards_train/chosen': '0.082171', 'rewards_train/rejected': '0.049403', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032768', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-142.86', 'loss/train': '0.68464', 'examples_per_second': '30.017', 'grad_norm': '25.5', 'counters/examples': 131008, 'counters/updates': 4094}
train stats after 131040 examples: {'rewards_train/chosen': '0.17816', 'rewards_train/rejected': '0.063674', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11449', 'logps_train/rejected': '-179.57', 'logps_train/chosen': '-200.51', 'loss/train': '0.65459', 'examples_per_second': '31.587', 'grad_norm': '30.625', 'counters/examples': 131040, 'counters/updates': 4095}
train stats after 131072 examples: {'rewards_train/chosen': '0.081771', 'rewards_train/rejected': '-0.0097778', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091549', 'logps_train/rejected': '-147.38', 'logps_train/chosen': '-132.29', 'loss/train': '0.65558', 'examples_per_second': '31.339', 'grad_norm': '26.75', 'counters/examples': 131072, 'counters/updates': 4096}
skipping logging after 131104 examples to avoid logging too frequently
train stats after 131136 examples: {'rewards_train/chosen': '0.088737', 'rewards_train/rejected': '0.033626', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055111', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-153.41', 'loss/train': '0.676', 'examples_per_second': '30.529', 'grad_norm': '29.125', 'counters/examples': 131136, 'counters/updates': 4098}
train stats after 131168 examples: {'rewards_train/chosen': '0.063048', 'rewards_train/rejected': '0.066309', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0032613', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-153.83', 'loss/train': '0.69999', 'examples_per_second': '31.718', 'grad_norm': '28.625', 'counters/examples': 131168, 'counters/updates': 4099}
train stats after 131200 examples: {'rewards_train/chosen': '0.11108', 'rewards_train/rejected': '0.036553', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07453', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-159.79', 'loss/train': '0.67352', 'examples_per_second': '31.282', 'grad_norm': '27.5', 'counters/examples': 131200, 'counters/updates': 4100}
train stats after 131232 examples: {'rewards_train/chosen': '0.053255', 'rewards_train/rejected': '0.0040914', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049163', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-142.4', 'loss/train': '0.68167', 'examples_per_second': '30.966', 'grad_norm': '33.75', 'counters/examples': 131232, 'counters/updates': 4101}
train stats after 131264 examples: {'rewards_train/chosen': '0.11663', 'rewards_train/rejected': '-0.011512', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12814', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-167.57', 'loss/train': '0.64496', 'examples_per_second': '30.026', 'grad_norm': '27.125', 'counters/examples': 131264, 'counters/updates': 4102}
skipping logging after 131296 examples to avoid logging too frequently
train stats after 131328 examples: {'rewards_train/chosen': '0.13602', 'rewards_train/rejected': '0.084348', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051668', 'logps_train/rejected': '-171.89', 'logps_train/chosen': '-137.4', 'loss/train': '0.67387', 'examples_per_second': '31.438', 'grad_norm': '28.125', 'counters/examples': 131328, 'counters/updates': 4104}
train stats after 131360 examples: {'rewards_train/chosen': '0.15276', 'rewards_train/rejected': '0.066922', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085834', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-112.38', 'loss/train': '0.66459', 'examples_per_second': '30.07', 'grad_norm': '27.875', 'counters/examples': 131360, 'counters/updates': 4105}
train stats after 131392 examples: {'rewards_train/chosen': '0.14649', 'rewards_train/rejected': '0.079026', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067467', 'logps_train/rejected': '-138.95', 'logps_train/chosen': '-173', 'loss/train': '0.66791', 'examples_per_second': '30.53', 'grad_norm': '27.875', 'counters/examples': 131392, 'counters/updates': 4106}
train stats after 131424 examples: {'rewards_train/chosen': '0.10097', 'rewards_train/rejected': '0.018385', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082581', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-135.85', 'loss/train': '0.65734', 'examples_per_second': '31.573', 'grad_norm': '24.625', 'counters/examples': 131424, 'counters/updates': 4107}
skipping logging after 131456 examples to avoid logging too frequently
train stats after 131488 examples: {'rewards_train/chosen': '0.14341', 'rewards_train/rejected': '0.025988', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11743', 'logps_train/rejected': '-122.96', 'logps_train/chosen': '-180.41', 'loss/train': '0.64654', 'examples_per_second': '31.486', 'grad_norm': '28.625', 'counters/examples': 131488, 'counters/updates': 4109}
skipping logging after 131520 examples to avoid logging too frequently
train stats after 131552 examples: {'rewards_train/chosen': '0.038014', 'rewards_train/rejected': '0.022767', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015247', 'logps_train/rejected': '-159.98', 'logps_train/chosen': '-151.1', 'loss/train': '0.69367', 'examples_per_second': '32.754', 'grad_norm': '34.5', 'counters/examples': 131552, 'counters/updates': 4111}
train stats after 131584 examples: {'rewards_train/chosen': '0.11242', 'rewards_train/rejected': '0.053933', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05849', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-119.88', 'loss/train': '0.6694', 'examples_per_second': '30.804', 'grad_norm': '25', 'counters/examples': 131584, 'counters/updates': 4112}
skipping logging after 131616 examples to avoid logging too frequently
train stats after 131648 examples: {'rewards_train/chosen': '0.10447', 'rewards_train/rejected': '0.013074', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091394', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-108.85', 'loss/train': '0.65673', 'examples_per_second': '31.341', 'grad_norm': '24.5', 'counters/examples': 131648, 'counters/updates': 4114}
train stats after 131680 examples: {'rewards_train/chosen': '0.043218', 'rewards_train/rejected': '0.015498', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02772', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-131.02', 'loss/train': '0.68706', 'examples_per_second': '30.994', 'grad_norm': '29.875', 'counters/examples': 131680, 'counters/updates': 4115}
train stats after 131712 examples: {'rewards_train/chosen': '0.067446', 'rewards_train/rejected': '0.077186', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00974', 'logps_train/rejected': '-128.19', 'logps_train/chosen': '-144.65', 'loss/train': '0.71399', 'examples_per_second': '32.613', 'grad_norm': '29.25', 'counters/examples': 131712, 'counters/updates': 4116}
train stats after 131744 examples: {'rewards_train/chosen': '0.095883', 'rewards_train/rejected': '0.053399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042484', 'logps_train/rejected': '-85.3', 'logps_train/chosen': '-125.12', 'loss/train': '0.67669', 'examples_per_second': '30.324', 'grad_norm': '24.5', 'counters/examples': 131744, 'counters/updates': 4117}
skipping logging after 131776 examples to avoid logging too frequently
train stats after 131808 examples: {'rewards_train/chosen': '0.10001', 'rewards_train/rejected': '0.075196', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02481', 'logps_train/rejected': '-125.91', 'logps_train/chosen': '-134.2', 'loss/train': '0.68982', 'examples_per_second': '29.997', 'grad_norm': '29.75', 'counters/examples': 131808, 'counters/updates': 4119}
skipping logging after 131840 examples to avoid logging too frequently
train stats after 131872 examples: {'rewards_train/chosen': '0.12386', 'rewards_train/rejected': '0.035038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088824', 'logps_train/rejected': '-129.34', 'logps_train/chosen': '-128.7', 'loss/train': '0.65904', 'examples_per_second': '38.051', 'grad_norm': '31.875', 'counters/examples': 131872, 'counters/updates': 4121}
train stats after 131904 examples: {'rewards_train/chosen': '0.054778', 'rewards_train/rejected': '0.053086', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0016916', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-144.46', 'loss/train': '0.7071', 'examples_per_second': '32.261', 'grad_norm': '31', 'counters/examples': 131904, 'counters/updates': 4122}
skipping logging after 131936 examples to avoid logging too frequently
train stats after 131968 examples: {'rewards_train/chosen': '0.11487', 'rewards_train/rejected': '0.010002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10487', 'logps_train/rejected': '-127.37', 'logps_train/chosen': '-135.4', 'loss/train': '0.65472', 'examples_per_second': '32.826', 'grad_norm': '25.75', 'counters/examples': 131968, 'counters/updates': 4124}
skipping logging after 132000 examples to avoid logging too frequently
Running evaluation after 132000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.21it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 132000: {'rewards_eval/chosen': '0.10898', 'rewards_eval/rejected': '0.033207', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.075772', 'logps_eval/rejected': '-118.28', 'logps_eval/chosen': '-138.34', 'loss/eval': '0.66646'}
skipping save for non epoch
train stats after 132032 examples: {'rewards_train/chosen': '0.12283', 'rewards_train/rejected': '0.041386', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081447', 'logps_train/rejected': '-123.64', 'logps_train/chosen': '-160.67', 'loss/train': '0.65894', 'examples_per_second': '30.79', 'grad_norm': '26.625', 'counters/examples': 132032, 'counters/updates': 4126}
train stats after 132064 examples: {'rewards_train/chosen': '0.12414', 'rewards_train/rejected': '0.072495', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051644', 'logps_train/rejected': '-113.56', 'logps_train/chosen': '-128.49', 'loss/train': '0.68082', 'examples_per_second': '30.771', 'grad_norm': '27.25', 'counters/examples': 132064, 'counters/updates': 4127}
skipping logging after 132096 examples to avoid logging too frequently
train stats after 132128 examples: {'rewards_train/chosen': '0.09155', 'rewards_train/rejected': '0.02255', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069', 'logps_train/rejected': '-133.59', 'logps_train/chosen': '-91.298', 'loss/train': '0.66581', 'examples_per_second': '30.804', 'grad_norm': '21.875', 'counters/examples': 132128, 'counters/updates': 4129}
train stats after 132160 examples: {'rewards_train/chosen': '0.11196', 'rewards_train/rejected': '-0.009632', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1216', 'logps_train/rejected': '-124.91', 'logps_train/chosen': '-132.08', 'loss/train': '0.64528', 'examples_per_second': '31.538', 'grad_norm': '23.75', 'counters/examples': 132160, 'counters/updates': 4130}
train stats after 132192 examples: {'rewards_train/chosen': '0.083174', 'rewards_train/rejected': '0.054233', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028941', 'logps_train/rejected': '-133.91', 'logps_train/chosen': '-167.96', 'loss/train': '0.6892', 'examples_per_second': '30.835', 'grad_norm': '28.875', 'counters/examples': 132192, 'counters/updates': 4131}
train stats after 132224 examples: {'rewards_train/chosen': '0.064913', 'rewards_train/rejected': '0.013805', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051109', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-137.49', 'loss/train': '0.67338', 'examples_per_second': '31.551', 'grad_norm': '25.25', 'counters/examples': 132224, 'counters/updates': 4132}
train stats after 132256 examples: {'rewards_train/chosen': '-0.0079272', 'rewards_train/rejected': '0.0042106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012138', 'logps_train/rejected': '-137.02', 'logps_train/chosen': '-147.94', 'loss/train': '0.7082', 'examples_per_second': '32.313', 'grad_norm': '30.125', 'counters/examples': 132256, 'counters/updates': 4133}
train stats after 132288 examples: {'rewards_train/chosen': '0.20656', 'rewards_train/rejected': '0.081107', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12546', 'logps_train/rejected': '-144.36', 'logps_train/chosen': '-164.58', 'loss/train': '0.64388', 'examples_per_second': '30.077', 'grad_norm': '35.75', 'counters/examples': 132288, 'counters/updates': 4134}
train stats after 132320 examples: {'rewards_train/chosen': '0.092858', 'rewards_train/rejected': '-0.0012212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09408', 'logps_train/rejected': '-124.71', 'logps_train/chosen': '-143.86', 'loss/train': '0.65891', 'examples_per_second': '32.743', 'grad_norm': '28', 'counters/examples': 132320, 'counters/updates': 4135}
train stats after 132352 examples: {'rewards_train/chosen': '0.11602', 'rewards_train/rejected': '0.043572', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07245', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-171.35', 'loss/train': '0.66219', 'examples_per_second': '32.106', 'grad_norm': '26.25', 'counters/examples': 132352, 'counters/updates': 4136}
train stats after 132384 examples: {'rewards_train/chosen': '0.062828', 'rewards_train/rejected': '0.066349', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0035214', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-117.31', 'loss/train': '0.70464', 'examples_per_second': '31.608', 'grad_norm': '26.125', 'counters/examples': 132384, 'counters/updates': 4137}
train stats after 132416 examples: {'rewards_train/chosen': '0.074602', 'rewards_train/rejected': '0.082718', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.008116', 'logps_train/rejected': '-132.09', 'logps_train/chosen': '-187.56', 'loss/train': '0.71578', 'examples_per_second': '31.585', 'grad_norm': '33.75', 'counters/examples': 132416, 'counters/updates': 4138}
train stats after 132448 examples: {'rewards_train/chosen': '0.14242', 'rewards_train/rejected': '0.014788', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12763', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-164.85', 'loss/train': '0.64133', 'examples_per_second': '30.615', 'grad_norm': '23.625', 'counters/examples': 132448, 'counters/updates': 4139}
train stats after 132480 examples: {'rewards_train/chosen': '0.1136', 'rewards_train/rejected': '0.099986', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013616', 'logps_train/rejected': '-115.04', 'logps_train/chosen': '-146.9', 'loss/train': '0.68979', 'examples_per_second': '31.538', 'grad_norm': '26', 'counters/examples': 132480, 'counters/updates': 4140}
train stats after 132512 examples: {'rewards_train/chosen': '0.13396', 'rewards_train/rejected': '0.038129', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.095828', 'logps_train/rejected': '-137.27', 'logps_train/chosen': '-138.93', 'loss/train': '0.65926', 'examples_per_second': '23.596', 'grad_norm': '28.75', 'counters/examples': 132512, 'counters/updates': 4141}
train stats after 132544 examples: {'rewards_train/chosen': '0.099095', 'rewards_train/rejected': '0.13187', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032775', 'logps_train/rejected': '-161.76', 'logps_train/chosen': '-162.31', 'loss/train': '0.72367', 'examples_per_second': '31.332', 'grad_norm': '29', 'counters/examples': 132544, 'counters/updates': 4142}
train stats after 132576 examples: {'rewards_train/chosen': '0.091666', 'rewards_train/rejected': '0.041402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050265', 'logps_train/rejected': '-110.91', 'logps_train/chosen': '-111.03', 'loss/train': '0.67563', 'examples_per_second': '30.506', 'grad_norm': '26.875', 'counters/examples': 132576, 'counters/updates': 4143}
train stats after 132608 examples: {'rewards_train/chosen': '0.12528', 'rewards_train/rejected': '0.036108', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089171', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-127.19', 'loss/train': '0.65748', 'examples_per_second': '30.698', 'grad_norm': '26.25', 'counters/examples': 132608, 'counters/updates': 4144}
skipping logging after 132640 examples to avoid logging too frequently
train stats after 132672 examples: {'rewards_train/chosen': '0.044497', 'rewards_train/rejected': '0.022554', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021942', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-155.44', 'loss/train': '0.68992', 'examples_per_second': '31.601', 'grad_norm': '29', 'counters/examples': 132672, 'counters/updates': 4146}
train stats after 132704 examples: {'rewards_train/chosen': '0.10635', 'rewards_train/rejected': '0.056056', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.050298', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-126.27', 'loss/train': '0.67855', 'examples_per_second': '30.973', 'grad_norm': '26.75', 'counters/examples': 132704, 'counters/updates': 4147}
train stats after 132736 examples: {'rewards_train/chosen': '0.15811', 'rewards_train/rejected': '-0.0029298', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16104', 'logps_train/rejected': '-83.432', 'logps_train/chosen': '-139.94', 'loss/train': '0.62332', 'examples_per_second': '30.11', 'grad_norm': '22.125', 'counters/examples': 132736, 'counters/updates': 4148}
train stats after 132768 examples: {'rewards_train/chosen': '0.099935', 'rewards_train/rejected': '0.058123', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041812', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-147.72', 'loss/train': '0.68259', 'examples_per_second': '30.161', 'grad_norm': '28.375', 'counters/examples': 132768, 'counters/updates': 4149}
train stats after 132800 examples: {'rewards_train/chosen': '0.072112', 'rewards_train/rejected': '0.021072', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05104', 'logps_train/rejected': '-109.64', 'logps_train/chosen': '-138.94', 'loss/train': '0.67919', 'examples_per_second': '31.444', 'grad_norm': '26.25', 'counters/examples': 132800, 'counters/updates': 4150}
train stats after 132832 examples: {'rewards_train/chosen': '0.1152', 'rewards_train/rejected': '0.053665', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061536', 'logps_train/rejected': '-158.52', 'logps_train/chosen': '-189.44', 'loss/train': '0.67188', 'examples_per_second': '31.965', 'grad_norm': '29.125', 'counters/examples': 132832, 'counters/updates': 4151}
train stats after 132864 examples: {'rewards_train/chosen': '0.060132', 'rewards_train/rejected': '-0.0052173', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065349', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-147.63', 'loss/train': '0.66951', 'examples_per_second': '31.866', 'grad_norm': '28.125', 'counters/examples': 132864, 'counters/updates': 4152}
train stats after 132896 examples: {'rewards_train/chosen': '0.034484', 'rewards_train/rejected': '0.073257', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038773', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-149.84', 'loss/train': '0.7194', 'examples_per_second': '31.501', 'grad_norm': '30.875', 'counters/examples': 132896, 'counters/updates': 4153}
train stats after 132928 examples: {'rewards_train/chosen': '0.12728', 'rewards_train/rejected': '0.064705', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062571', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-136.57', 'loss/train': '0.669', 'examples_per_second': '30.102', 'grad_norm': '25.875', 'counters/examples': 132928, 'counters/updates': 4154}
train stats after 132960 examples: {'rewards_train/chosen': '0.073533', 'rewards_train/rejected': '0.029143', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044389', 'logps_train/rejected': '-129.63', 'logps_train/chosen': '-147.06', 'loss/train': '0.67594', 'examples_per_second': '30.111', 'grad_norm': '30.125', 'counters/examples': 132960, 'counters/updates': 4155}
train stats after 132992 examples: {'rewards_train/chosen': '0.069222', 'rewards_train/rejected': '0.041348', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027874', 'logps_train/rejected': '-142.4', 'logps_train/chosen': '-166.2', 'loss/train': '0.69026', 'examples_per_second': '32.187', 'grad_norm': '30.125', 'counters/examples': 132992, 'counters/updates': 4156}
train stats after 133024 examples: {'rewards_train/chosen': '0.18775', 'rewards_train/rejected': '0.051224', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13653', 'logps_train/rejected': '-140.88', 'logps_train/chosen': '-151.26', 'loss/train': '0.63549', 'examples_per_second': '31.104', 'grad_norm': '24.875', 'counters/examples': 133024, 'counters/updates': 4157}
train stats after 133056 examples: {'rewards_train/chosen': '0.14777', 'rewards_train/rejected': '0.033212', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11456', 'logps_train/rejected': '-110.04', 'logps_train/chosen': '-155.16', 'loss/train': '0.64166', 'examples_per_second': '31.538', 'grad_norm': '28.125', 'counters/examples': 133056, 'counters/updates': 4158}
skipping logging after 133088 examples to avoid logging too frequently
train stats after 133120 examples: {'rewards_train/chosen': '0.059915', 'rewards_train/rejected': '0.022359', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037556', 'logps_train/rejected': '-159.69', 'logps_train/chosen': '-150.08', 'loss/train': '0.68638', 'examples_per_second': '31.618', 'grad_norm': '33.75', 'counters/examples': 133120, 'counters/updates': 4160}
train stats after 133152 examples: {'rewards_train/chosen': '0.061123', 'rewards_train/rejected': '0.010058', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051065', 'logps_train/rejected': '-100.17', 'logps_train/chosen': '-101.08', 'loss/train': '0.67261', 'examples_per_second': '31.599', 'grad_norm': '25.125', 'counters/examples': 133152, 'counters/updates': 4161}
train stats after 133184 examples: {'rewards_train/chosen': '0.14182', 'rewards_train/rejected': '0.027574', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11425', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-117.93', 'loss/train': '0.64619', 'examples_per_second': '31.639', 'grad_norm': '29.125', 'counters/examples': 133184, 'counters/updates': 4162}
skipping logging after 133216 examples to avoid logging too frequently
train stats after 133248 examples: {'rewards_train/chosen': '0.20692', 'rewards_train/rejected': '0.042008', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16491', 'logps_train/rejected': '-113.22', 'logps_train/chosen': '-154.97', 'loss/train': '0.62321', 'examples_per_second': '31.568', 'grad_norm': '24.75', 'counters/examples': 133248, 'counters/updates': 4164}
skipping logging after 133280 examples to avoid logging too frequently
train stats after 133312 examples: {'rewards_train/chosen': '-0.0021929', 'rewards_train/rejected': '-0.002354', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00016105', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-147.01', 'loss/train': '0.70433', 'examples_per_second': '31.587', 'grad_norm': '27.875', 'counters/examples': 133312, 'counters/updates': 4166}
train stats after 133344 examples: {'rewards_train/chosen': '0.10982', 'rewards_train/rejected': '-0.018983', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12881', 'logps_train/rejected': '-146.65', 'logps_train/chosen': '-191.37', 'loss/train': '0.638', 'examples_per_second': '30.128', 'grad_norm': '29.5', 'counters/examples': 133344, 'counters/updates': 4167}
train stats after 133376 examples: {'rewards_train/chosen': '0.095394', 'rewards_train/rejected': '-0.010946', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10634', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-123.04', 'loss/train': '0.64451', 'examples_per_second': '31.663', 'grad_norm': '28.375', 'counters/examples': 133376, 'counters/updates': 4168}
train stats after 133408 examples: {'rewards_train/chosen': '0.11146', 'rewards_train/rejected': '0.020205', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091256', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-156.64', 'loss/train': '0.65426', 'examples_per_second': '31.648', 'grad_norm': '27.75', 'counters/examples': 133408, 'counters/updates': 4169}
skipping logging after 133440 examples to avoid logging too frequently
train stats after 133472 examples: {'rewards_train/chosen': '0.076071', 'rewards_train/rejected': '-0.019077', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.095148', 'logps_train/rejected': '-83.689', 'logps_train/chosen': '-110.41', 'loss/train': '0.65045', 'examples_per_second': '31.671', 'grad_norm': '21.875', 'counters/examples': 133472, 'counters/updates': 4171}
train stats after 133504 examples: {'rewards_train/chosen': '0.17268', 'rewards_train/rejected': '0.068498', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10418', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-128.06', 'loss/train': '0.64971', 'examples_per_second': '30.252', 'grad_norm': '32.5', 'counters/examples': 133504, 'counters/updates': 4172}
train stats after 133536 examples: {'rewards_train/chosen': '0.056977', 'rewards_train/rejected': '-0.023349', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080326', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-135.3', 'loss/train': '0.66183', 'examples_per_second': '31.377', 'grad_norm': '29.125', 'counters/examples': 133536, 'counters/updates': 4173}
train stats after 133568 examples: {'rewards_train/chosen': '0.089578', 'rewards_train/rejected': '0.015696', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073882', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-139.78', 'loss/train': '0.66426', 'examples_per_second': '32.066', 'grad_norm': '29.5', 'counters/examples': 133568, 'counters/updates': 4174}
train stats after 133600 examples: {'rewards_train/chosen': '0.21313', 'rewards_train/rejected': '0.098888', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11424', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-177.85', 'loss/train': '0.64476', 'examples_per_second': '30.783', 'grad_norm': '28.625', 'counters/examples': 133600, 'counters/updates': 4175}
train stats after 133632 examples: {'rewards_train/chosen': '0.081897', 'rewards_train/rejected': '-0.0012132', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08311', 'logps_train/rejected': '-115.3', 'logps_train/chosen': '-138.96', 'loss/train': '0.66051', 'examples_per_second': '31.647', 'grad_norm': '23.625', 'counters/examples': 133632, 'counters/updates': 4176}
train stats after 133664 examples: {'rewards_train/chosen': '0.10476', 'rewards_train/rejected': '-0.027674', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13243', 'logps_train/rejected': '-90.584', 'logps_train/chosen': '-121.57', 'loss/train': '0.63776', 'examples_per_second': '30.561', 'grad_norm': '26.125', 'counters/examples': 133664, 'counters/updates': 4177}
train stats after 133696 examples: {'rewards_train/chosen': '0.12452', 'rewards_train/rejected': '0.01977', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10475', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-177.59', 'loss/train': '0.6517', 'examples_per_second': '33.235', 'grad_norm': '29.5', 'counters/examples': 133696, 'counters/updates': 4178}
train stats after 133728 examples: {'rewards_train/chosen': '0.070138', 'rewards_train/rejected': '0.071634', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0014962', 'logps_train/rejected': '-112.88', 'logps_train/chosen': '-116.6', 'loss/train': '0.71254', 'examples_per_second': '31.624', 'grad_norm': '27', 'counters/examples': 133728, 'counters/updates': 4179}
train stats after 133760 examples: {'rewards_train/chosen': '0.13851', 'rewards_train/rejected': '0.055283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083226', 'logps_train/rejected': '-95.015', 'logps_train/chosen': '-163.44', 'loss/train': '0.67819', 'examples_per_second': '32.349', 'grad_norm': '24.125', 'counters/examples': 133760, 'counters/updates': 4180}
train stats after 133792 examples: {'rewards_train/chosen': '0.022325', 'rewards_train/rejected': '-0.063917', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086242', 'logps_train/rejected': '-108.31', 'logps_train/chosen': '-130.82', 'loss/train': '0.66108', 'examples_per_second': '32.737', 'grad_norm': '25.375', 'counters/examples': 133792, 'counters/updates': 4181}
train stats after 133824 examples: {'rewards_train/chosen': '0.06758', 'rewards_train/rejected': '-0.00094001', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06852', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-131.65', 'loss/train': '0.67189', 'examples_per_second': '32.162', 'grad_norm': '26', 'counters/examples': 133824, 'counters/updates': 4182}
train stats after 133856 examples: {'rewards_train/chosen': '0.11306', 'rewards_train/rejected': '0.034893', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078168', 'logps_train/rejected': '-140.3', 'logps_train/chosen': '-130.84', 'loss/train': '0.66846', 'examples_per_second': '31.57', 'grad_norm': '27.5', 'counters/examples': 133856, 'counters/updates': 4183}
train stats after 133888 examples: {'rewards_train/chosen': '0.10206', 'rewards_train/rejected': '0.076788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025268', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-148.78', 'loss/train': '0.69227', 'examples_per_second': '31.822', 'grad_norm': '27.25', 'counters/examples': 133888, 'counters/updates': 4184}
train stats after 133920 examples: {'rewards_train/chosen': '0.14893', 'rewards_train/rejected': '-0.031849', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18078', 'logps_train/rejected': '-123.99', 'logps_train/chosen': '-139.1', 'loss/train': '0.61829', 'examples_per_second': '32.643', 'grad_norm': '23.875', 'counters/examples': 133920, 'counters/updates': 4185}
train stats after 133952 examples: {'rewards_train/chosen': '0.13185', 'rewards_train/rejected': '0.033175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.098677', 'logps_train/rejected': '-119.46', 'logps_train/chosen': '-140.1', 'loss/train': '0.65269', 'examples_per_second': '32.356', 'grad_norm': '26.125', 'counters/examples': 133952, 'counters/updates': 4186}
train stats after 133984 examples: {'rewards_train/chosen': '0.093879', 'rewards_train/rejected': '-0.055345', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14922', 'logps_train/rejected': '-108.27', 'logps_train/chosen': '-101.54', 'loss/train': '0.62988', 'examples_per_second': '32.612', 'grad_norm': '22', 'counters/examples': 133984, 'counters/updates': 4187}
train stats after 134016 examples: {'rewards_train/chosen': '0.071908', 'rewards_train/rejected': '-0.025207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097114', 'logps_train/rejected': '-162.49', 'logps_train/chosen': '-147.93', 'loss/train': '0.65658', 'examples_per_second': '30.077', 'grad_norm': '29.25', 'counters/examples': 134016, 'counters/updates': 4188}
train stats after 134048 examples: {'rewards_train/chosen': '0.10794', 'rewards_train/rejected': '0.011071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096865', 'logps_train/rejected': '-107.27', 'logps_train/chosen': '-138.81', 'loss/train': '0.65767', 'examples_per_second': '31.646', 'grad_norm': '24.625', 'counters/examples': 134048, 'counters/updates': 4189}
train stats after 134080 examples: {'rewards_train/chosen': '0.15794', 'rewards_train/rejected': '0.052688', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10525', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-145.11', 'loss/train': '0.64528', 'examples_per_second': '31.477', 'grad_norm': '24.875', 'counters/examples': 134080, 'counters/updates': 4190}
train stats after 134112 examples: {'rewards_train/chosen': '0.12671', 'rewards_train/rejected': '0.014402', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11231', 'logps_train/rejected': '-111.13', 'logps_train/chosen': '-146.68', 'loss/train': '0.6499', 'examples_per_second': '31.871', 'grad_norm': '24.25', 'counters/examples': 134112, 'counters/updates': 4191}
train stats after 134144 examples: {'rewards_train/chosen': '0.099731', 'rewards_train/rejected': '0.01097', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088761', 'logps_train/rejected': '-109.72', 'logps_train/chosen': '-149.36', 'loss/train': '0.65633', 'examples_per_second': '31.598', 'grad_norm': '25.25', 'counters/examples': 134144, 'counters/updates': 4192}
train stats after 134176 examples: {'rewards_train/chosen': '0.079889', 'rewards_train/rejected': '0.032845', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047044', 'logps_train/rejected': '-102.29', 'logps_train/chosen': '-106.34', 'loss/train': '0.67629', 'examples_per_second': '30.675', 'grad_norm': '23.5', 'counters/examples': 134176, 'counters/updates': 4193}
train stats after 134208 examples: {'rewards_train/chosen': '0.13884', 'rewards_train/rejected': '0.041783', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097059', 'logps_train/rejected': '-133.78', 'logps_train/chosen': '-180.37', 'loss/train': '0.65143', 'examples_per_second': '31.06', 'grad_norm': '26.625', 'counters/examples': 134208, 'counters/updates': 4194}
train stats after 134240 examples: {'rewards_train/chosen': '0.17063', 'rewards_train/rejected': '0.022754', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14787', 'logps_train/rejected': '-122.31', 'logps_train/chosen': '-141.06', 'loss/train': '0.63801', 'examples_per_second': '31.357', 'grad_norm': '28.375', 'counters/examples': 134240, 'counters/updates': 4195}
train stats after 134272 examples: {'rewards_train/chosen': '0.16788', 'rewards_train/rejected': '0.03243', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13545', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-154.31', 'loss/train': '0.64455', 'examples_per_second': '31.643', 'grad_norm': '26.25', 'counters/examples': 134272, 'counters/updates': 4196}
train stats after 134304 examples: {'rewards_train/chosen': '0.16358', 'rewards_train/rejected': '0.027704', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13588', 'logps_train/rejected': '-132.99', 'logps_train/chosen': '-126.32', 'loss/train': '0.6344', 'examples_per_second': '31.669', 'grad_norm': '23.25', 'counters/examples': 134304, 'counters/updates': 4197}
train stats after 134336 examples: {'rewards_train/chosen': '0.095148', 'rewards_train/rejected': '0.032622', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.062526', 'logps_train/rejected': '-108.05', 'logps_train/chosen': '-142.16', 'loss/train': '0.66857', 'examples_per_second': '31.677', 'grad_norm': '26', 'counters/examples': 134336, 'counters/updates': 4198}
train stats after 134368 examples: {'rewards_train/chosen': '0.17465', 'rewards_train/rejected': '0.03958', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13507', 'logps_train/rejected': '-158.71', 'logps_train/chosen': '-131.32', 'loss/train': '0.63524', 'examples_per_second': '30.85', 'grad_norm': '26.25', 'counters/examples': 134368, 'counters/updates': 4199}
train stats after 134400 examples: {'rewards_train/chosen': '0.069022', 'rewards_train/rejected': '0.0036051', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065417', 'logps_train/rejected': '-90.317', 'logps_train/chosen': '-152.42', 'loss/train': '0.66747', 'examples_per_second': '32.808', 'grad_norm': '24', 'counters/examples': 134400, 'counters/updates': 4200}
train stats after 134432 examples: {'rewards_train/chosen': '0.077916', 'rewards_train/rejected': '-0.025757', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10367', 'logps_train/rejected': '-98.726', 'logps_train/chosen': '-119.91', 'loss/train': '0.64756', 'examples_per_second': '31.73', 'grad_norm': '22.5', 'counters/examples': 134432, 'counters/updates': 4201}
train stats after 134464 examples: {'rewards_train/chosen': '0.13132', 'rewards_train/rejected': '0.060497', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070824', 'logps_train/rejected': '-117.4', 'logps_train/chosen': '-142.63', 'loss/train': '0.67314', 'examples_per_second': '31.172', 'grad_norm': '26.75', 'counters/examples': 134464, 'counters/updates': 4202}
train stats after 134496 examples: {'rewards_train/chosen': '0.15407', 'rewards_train/rejected': '0.0026626', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15141', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-148.19', 'loss/train': '0.62676', 'examples_per_second': '30.825', 'grad_norm': '25', 'counters/examples': 134496, 'counters/updates': 4203}
train stats after 134528 examples: {'rewards_train/chosen': '0.16772', 'rewards_train/rejected': '0.056194', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11152', 'logps_train/rejected': '-137.08', 'logps_train/chosen': '-141.45', 'loss/train': '0.65168', 'examples_per_second': '30.929', 'grad_norm': '26.25', 'counters/examples': 134528, 'counters/updates': 4204}
train stats after 134560 examples: {'rewards_train/chosen': '0.082567', 'rewards_train/rejected': '-0.0010756', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083643', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-112.24', 'loss/train': '0.66124', 'examples_per_second': '31.673', 'grad_norm': '25.25', 'counters/examples': 134560, 'counters/updates': 4205}
train stats after 134592 examples: {'rewards_train/chosen': '0.17334', 'rewards_train/rejected': '0.068103', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10524', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-139.75', 'loss/train': '0.64956', 'examples_per_second': '31.632', 'grad_norm': '39.5', 'counters/examples': 134592, 'counters/updates': 4206}
train stats after 134624 examples: {'rewards_train/chosen': '0.087458', 'rewards_train/rejected': '0.054103', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033355', 'logps_train/rejected': '-142.02', 'logps_train/chosen': '-161.79', 'loss/train': '0.68735', 'examples_per_second': '31.632', 'grad_norm': '29.25', 'counters/examples': 134624, 'counters/updates': 4207}
train stats after 134656 examples: {'rewards_train/chosen': '0.14022', 'rewards_train/rejected': '0.059091', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081125', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-151.83', 'loss/train': '0.66925', 'examples_per_second': '32.162', 'grad_norm': '27.75', 'counters/examples': 134656, 'counters/updates': 4208}
train stats after 134688 examples: {'rewards_train/chosen': '0.098562', 'rewards_train/rejected': '-0.013299', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11186', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-152.05', 'loss/train': '0.65265', 'examples_per_second': '31.735', 'grad_norm': '26.125', 'counters/examples': 134688, 'counters/updates': 4209}
train stats after 134720 examples: {'rewards_train/chosen': '0.10003', 'rewards_train/rejected': '0.02693', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073101', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-145.51', 'loss/train': '0.6631', 'examples_per_second': '30.209', 'grad_norm': '24.375', 'counters/examples': 134720, 'counters/updates': 4210}
train stats after 134752 examples: {'rewards_train/chosen': '0.13106', 'rewards_train/rejected': '0.022851', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10821', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-156.97', 'loss/train': '0.65057', 'examples_per_second': '31.657', 'grad_norm': '26.625', 'counters/examples': 134752, 'counters/updates': 4211}
skipping logging after 134784 examples to avoid logging too frequently
train stats after 134816 examples: {'rewards_train/chosen': '0.16459', 'rewards_train/rejected': '0.0016581', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16293', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-136.36', 'loss/train': '0.6234', 'examples_per_second': '31.031', 'grad_norm': '23.875', 'counters/examples': 134816, 'counters/updates': 4213}
train stats after 134848 examples: {'rewards_train/chosen': '0.076684', 'rewards_train/rejected': '0.041642', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035041', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-154.49', 'loss/train': '0.69426', 'examples_per_second': '31.063', 'grad_norm': '31.75', 'counters/examples': 134848, 'counters/updates': 4214}
train stats after 134880 examples: {'rewards_train/chosen': '0.14788', 'rewards_train/rejected': '0.016303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13158', 'logps_train/rejected': '-114.66', 'logps_train/chosen': '-133.82', 'loss/train': '0.64336', 'examples_per_second': '30.288', 'grad_norm': '28.375', 'counters/examples': 134880, 'counters/updates': 4215}
skipping logging after 134912 examples to avoid logging too frequently
train stats after 134944 examples: {'rewards_train/chosen': '0.14382', 'rewards_train/rejected': '0.056082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087741', 'logps_train/rejected': '-144.84', 'logps_train/chosen': '-152.83', 'loss/train': '0.66092', 'examples_per_second': '34.393', 'grad_norm': '30.125', 'counters/examples': 134944, 'counters/updates': 4217}
train stats after 134976 examples: {'rewards_train/chosen': '0.19627', 'rewards_train/rejected': '0.11686', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079405', 'logps_train/rejected': '-105.79', 'logps_train/chosen': '-108.23', 'loss/train': '0.65891', 'examples_per_second': '31.678', 'grad_norm': '23.125', 'counters/examples': 134976, 'counters/updates': 4218}
train stats after 135008 examples: {'rewards_train/chosen': '0.017149', 'rewards_train/rejected': '0.025756', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0086076', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-131.33', 'loss/train': '0.70239', 'examples_per_second': '30.862', 'grad_norm': '25.875', 'counters/examples': 135008, 'counters/updates': 4219}
train stats after 135040 examples: {'rewards_train/chosen': '0.093885', 'rewards_train/rejected': '0.018678', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075207', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-131.72', 'loss/train': '0.66145', 'examples_per_second': '30.647', 'grad_norm': '28.375', 'counters/examples': 135040, 'counters/updates': 4220}
skipping logging after 135072 examples to avoid logging too frequently
train stats after 135104 examples: {'rewards_train/chosen': '0.12342', 'rewards_train/rejected': '0.038705', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084712', 'logps_train/rejected': '-139.91', 'logps_train/chosen': '-161.71', 'loss/train': '0.65877', 'examples_per_second': '32.341', 'grad_norm': '28', 'counters/examples': 135104, 'counters/updates': 4222}
train stats after 135136 examples: {'rewards_train/chosen': '0.0551', 'rewards_train/rejected': '0.027026', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028073', 'logps_train/rejected': '-127.12', 'logps_train/chosen': '-131.9', 'loss/train': '0.68964', 'examples_per_second': '30.206', 'grad_norm': '28.5', 'counters/examples': 135136, 'counters/updates': 4223}
skipping logging after 135168 examples to avoid logging too frequently
train stats after 135200 examples: {'rewards_train/chosen': '0.12656', 'rewards_train/rejected': '0.020096', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10646', 'logps_train/rejected': '-157.39', 'logps_train/chosen': '-133.77', 'loss/train': '0.65194', 'examples_per_second': '31.637', 'grad_norm': '29', 'counters/examples': 135200, 'counters/updates': 4225}
skipping logging after 135232 examples to avoid logging too frequently
train stats after 135264 examples: {'rewards_train/chosen': '0.14985', 'rewards_train/rejected': '0.028801', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12105', 'logps_train/rejected': '-107.66', 'logps_train/chosen': '-155.4', 'loss/train': '0.6452', 'examples_per_second': '31.641', 'grad_norm': '28', 'counters/examples': 135264, 'counters/updates': 4227}
train stats after 135296 examples: {'rewards_train/chosen': '0.11102', 'rewards_train/rejected': '0.096591', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014424', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-136.1', 'loss/train': '0.69895', 'examples_per_second': '32.21', 'grad_norm': '28.375', 'counters/examples': 135296, 'counters/updates': 4228}
train stats after 135328 examples: {'rewards_train/chosen': '0.10625', 'rewards_train/rejected': '0.10611', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00014529', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-144.35', 'loss/train': '0.7138', 'examples_per_second': '31.725', 'grad_norm': '33.25', 'counters/examples': 135328, 'counters/updates': 4229}
train stats after 135360 examples: {'rewards_train/chosen': '0.12838', 'rewards_train/rejected': '0.046297', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082083', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-155.88', 'loss/train': '0.6575', 'examples_per_second': '32.728', 'grad_norm': '25.625', 'counters/examples': 135360, 'counters/updates': 4230}
skipping logging after 135392 examples to avoid logging too frequently
train stats after 135424 examples: {'rewards_train/chosen': '0.18763', 'rewards_train/rejected': '0.041256', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14638', 'logps_train/rejected': '-110.14', 'logps_train/chosen': '-147.55', 'loss/train': '0.63833', 'examples_per_second': '30.576', 'grad_norm': '26.5', 'counters/examples': 135424, 'counters/updates': 4232}
train stats after 135456 examples: {'rewards_train/chosen': '0.079025', 'rewards_train/rejected': '-0.012041', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091066', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-112.81', 'loss/train': '0.66216', 'examples_per_second': '32.573', 'grad_norm': '24.375', 'counters/examples': 135456, 'counters/updates': 4233}
train stats after 135488 examples: {'rewards_train/chosen': '0.015948', 'rewards_train/rejected': '0.056306', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040358', 'logps_train/rejected': '-95.829', 'logps_train/chosen': '-115.95', 'loss/train': '0.72083', 'examples_per_second': '31.139', 'grad_norm': '23.875', 'counters/examples': 135488, 'counters/updates': 4234}
train stats after 135520 examples: {'rewards_train/chosen': '0.17875', 'rewards_train/rejected': '0.002977', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17577', 'logps_train/rejected': '-99.876', 'logps_train/chosen': '-147.4', 'loss/train': '0.62002', 'examples_per_second': '23.731', 'grad_norm': '23.875', 'counters/examples': 135520, 'counters/updates': 4235}
train stats after 135552 examples: {'rewards_train/chosen': '0.18531', 'rewards_train/rejected': '0.087842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097464', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-136.72', 'loss/train': '0.65513', 'examples_per_second': '31.616', 'grad_norm': '30.125', 'counters/examples': 135552, 'counters/updates': 4236}
train stats after 135584 examples: {'rewards_train/chosen': '0.12619', 'rewards_train/rejected': '0.060108', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066084', 'logps_train/rejected': '-99.831', 'logps_train/chosen': '-120.9', 'loss/train': '0.67022', 'examples_per_second': '31.489', 'grad_norm': '24.125', 'counters/examples': 135584, 'counters/updates': 4237}
train stats after 135616 examples: {'rewards_train/chosen': '0.11492', 'rewards_train/rejected': '0.034042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080878', 'logps_train/rejected': '-123.28', 'logps_train/chosen': '-130.55', 'loss/train': '0.66516', 'examples_per_second': '24.731', 'grad_norm': '28.25', 'counters/examples': 135616, 'counters/updates': 4238}
train stats after 135648 examples: {'rewards_train/chosen': '0.073901', 'rewards_train/rejected': '0.00081963', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.073081', 'logps_train/rejected': '-128.5', 'logps_train/chosen': '-146.71', 'loss/train': '0.66919', 'examples_per_second': '32.769', 'grad_norm': '29.875', 'counters/examples': 135648, 'counters/updates': 4239}
train stats after 135680 examples: {'rewards_train/chosen': '0.16581', 'rewards_train/rejected': '0.077214', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.088593', 'logps_train/rejected': '-110.4', 'logps_train/chosen': '-155.8', 'loss/train': '0.65631', 'examples_per_second': '31.65', 'grad_norm': '26.125', 'counters/examples': 135680, 'counters/updates': 4240}
train stats after 135712 examples: {'rewards_train/chosen': '0.10653', 'rewards_train/rejected': '0.079682', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026852', 'logps_train/rejected': '-152.85', 'logps_train/chosen': '-168.54', 'loss/train': '0.69095', 'examples_per_second': '31.095', 'grad_norm': '30.875', 'counters/examples': 135712, 'counters/updates': 4241}
train stats after 135744 examples: {'rewards_train/chosen': '0.051671', 'rewards_train/rejected': '-0.033577', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085248', 'logps_train/rejected': '-129.31', 'logps_train/chosen': '-128.94', 'loss/train': '0.66198', 'examples_per_second': '31.993', 'grad_norm': '26.875', 'counters/examples': 135744, 'counters/updates': 4242}
train stats after 135776 examples: {'rewards_train/chosen': '0.17079', 'rewards_train/rejected': '0.014116', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15667', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-150.09', 'loss/train': '0.63291', 'examples_per_second': '31.483', 'grad_norm': '24.75', 'counters/examples': 135776, 'counters/updates': 4243}
train stats after 135808 examples: {'rewards_train/chosen': '0.17813', 'rewards_train/rejected': '0.11551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062619', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-148.21', 'loss/train': '0.66988', 'examples_per_second': '31.362', 'grad_norm': '31.125', 'counters/examples': 135808, 'counters/updates': 4244}
train stats after 135840 examples: {'rewards_train/chosen': '0.06944', 'rewards_train/rejected': '0.054136', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015304', 'logps_train/rejected': '-143.11', 'logps_train/chosen': '-141.73', 'loss/train': '0.69079', 'examples_per_second': '30.669', 'grad_norm': '26', 'counters/examples': 135840, 'counters/updates': 4245}
train stats after 135872 examples: {'rewards_train/chosen': '0.060334', 'rewards_train/rejected': '0.023986', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.036348', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-119.63', 'loss/train': '0.68153', 'examples_per_second': '32.624', 'grad_norm': '24', 'counters/examples': 135872, 'counters/updates': 4246}
train stats after 135904 examples: {'rewards_train/chosen': '0.16884', 'rewards_train/rejected': '0.067253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10159', 'logps_train/rejected': '-92.383', 'logps_train/chosen': '-124.41', 'loss/train': '0.6536', 'examples_per_second': '32.39', 'grad_norm': '23', 'counters/examples': 135904, 'counters/updates': 4247}
train stats after 135936 examples: {'rewards_train/chosen': '0.070688', 'rewards_train/rejected': '0.027112', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043576', 'logps_train/rejected': '-103.5', 'logps_train/chosen': '-178.23', 'loss/train': '0.68094', 'examples_per_second': '30.987', 'grad_norm': '33.75', 'counters/examples': 135936, 'counters/updates': 4248}
train stats after 135968 examples: {'rewards_train/chosen': '0.037018', 'rewards_train/rejected': '-0.014191', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051209', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-133.61', 'loss/train': '0.67793', 'examples_per_second': '32.928', 'grad_norm': '26', 'counters/examples': 135968, 'counters/updates': 4249}
train stats after 136000 examples: {'rewards_train/chosen': '0.049118', 'rewards_train/rejected': '0.02392', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025198', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-123.14', 'loss/train': '0.68981', 'examples_per_second': '30.89', 'grad_norm': '25.875', 'counters/examples': 136000, 'counters/updates': 4250}
skipping logging after 136032 examples to avoid logging too frequently
train stats after 136064 examples: {'rewards_train/chosen': '0.045954', 'rewards_train/rejected': '0.090048', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.044094', 'logps_train/rejected': '-113.32', 'logps_train/chosen': '-129.92', 'loss/train': '0.72816', 'examples_per_second': '30.4', 'grad_norm': '30.375', 'counters/examples': 136064, 'counters/updates': 4252}
train stats after 136096 examples: {'rewards_train/chosen': '0.14294', 'rewards_train/rejected': '0.065074', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077863', 'logps_train/rejected': '-146.81', 'logps_train/chosen': '-178.66', 'loss/train': '0.66376', 'examples_per_second': '32.017', 'grad_norm': '28.75', 'counters/examples': 136096, 'counters/updates': 4253}
skipping logging after 136128 examples to avoid logging too frequently
train stats after 136160 examples: {'rewards_train/chosen': '0.040811', 'rewards_train/rejected': '0.0035497', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037261', 'logps_train/rejected': '-93.26', 'logps_train/chosen': '-151.15', 'loss/train': '0.68303', 'examples_per_second': '33.429', 'grad_norm': '26.5', 'counters/examples': 136160, 'counters/updates': 4255}
train stats after 136192 examples: {'rewards_train/chosen': '0.099934', 'rewards_train/rejected': '0.11587', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015933', 'logps_train/rejected': '-136.77', 'logps_train/chosen': '-146.73', 'loss/train': '0.71049', 'examples_per_second': '31.572', 'grad_norm': '31.375', 'counters/examples': 136192, 'counters/updates': 4256}
train stats after 136224 examples: {'rewards_train/chosen': '0.14627', 'rewards_train/rejected': '0.068083', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078185', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-127.74', 'loss/train': '0.68885', 'examples_per_second': '30.801', 'grad_norm': '31.25', 'counters/examples': 136224, 'counters/updates': 4257}
train stats after 136256 examples: {'rewards_train/chosen': '0.15108', 'rewards_train/rejected': '-0.011039', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16212', 'logps_train/rejected': '-101.45', 'logps_train/chosen': '-125.54', 'loss/train': '0.6284', 'examples_per_second': '31.677', 'grad_norm': '24.375', 'counters/examples': 136256, 'counters/updates': 4258}
train stats after 136288 examples: {'rewards_train/chosen': '0.084885', 'rewards_train/rejected': '0.09161', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0067254', 'logps_train/rejected': '-143.24', 'logps_train/chosen': '-141.65', 'loss/train': '0.70141', 'examples_per_second': '31.162', 'grad_norm': '26.75', 'counters/examples': 136288, 'counters/updates': 4259}
skipping logging after 136320 examples to avoid logging too frequently
train stats after 136352 examples: {'rewards_train/chosen': '0.1658', 'rewards_train/rejected': '0.096176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069622', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-168.7', 'loss/train': '0.67005', 'examples_per_second': '31.661', 'grad_norm': '28.75', 'counters/examples': 136352, 'counters/updates': 4261}
skipping logging after 136384 examples to avoid logging too frequently
train stats after 136416 examples: {'rewards_train/chosen': '0.13279', 'rewards_train/rejected': '0.08582', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046973', 'logps_train/rejected': '-142.92', 'logps_train/chosen': '-139.46', 'loss/train': '0.68023', 'examples_per_second': '31.657', 'grad_norm': '32.5', 'counters/examples': 136416, 'counters/updates': 4263}
train stats after 136448 examples: {'rewards_train/chosen': '0.085982', 'rewards_train/rejected': '0.072106', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013877', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-131.72', 'loss/train': '0.69704', 'examples_per_second': '30.227', 'grad_norm': '27.75', 'counters/examples': 136448, 'counters/updates': 4264}
train stats after 136480 examples: {'rewards_train/chosen': '0.1282', 'rewards_train/rejected': '0.049317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078886', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-159.94', 'loss/train': '0.66794', 'examples_per_second': '31.6', 'grad_norm': '26.875', 'counters/examples': 136480, 'counters/updates': 4265}
skipping logging after 136512 examples to avoid logging too frequently
train stats after 136544 examples: {'rewards_train/chosen': '0.19754', 'rewards_train/rejected': '0.02768', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16986', 'logps_train/rejected': '-125.92', 'logps_train/chosen': '-154.98', 'loss/train': '0.6245', 'examples_per_second': '31.465', 'grad_norm': '27.75', 'counters/examples': 136544, 'counters/updates': 4267}
train stats after 136576 examples: {'rewards_train/chosen': '0.14834', 'rewards_train/rejected': '0.03599', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11235', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-137.06', 'loss/train': '0.65227', 'examples_per_second': '30.333', 'grad_norm': '26', 'counters/examples': 136576, 'counters/updates': 4268}
train stats after 136608 examples: {'rewards_train/chosen': '0.074379', 'rewards_train/rejected': '0.073011', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0013679', 'logps_train/rejected': '-151.35', 'logps_train/chosen': '-124.84', 'loss/train': '0.70258', 'examples_per_second': '31.13', 'grad_norm': '26.5', 'counters/examples': 136608, 'counters/updates': 4269}
train stats after 136640 examples: {'rewards_train/chosen': '0.080346', 'rewards_train/rejected': '0.036317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04403', 'logps_train/rejected': '-111.58', 'logps_train/chosen': '-124.36', 'loss/train': '0.6808', 'examples_per_second': '31.324', 'grad_norm': '24', 'counters/examples': 136640, 'counters/updates': 4270}
train stats after 136672 examples: {'rewards_train/chosen': '0.099959', 'rewards_train/rejected': '0.02232', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077638', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-163.21', 'loss/train': '0.66318', 'examples_per_second': '30.748', 'grad_norm': '26.625', 'counters/examples': 136672, 'counters/updates': 4271}
train stats after 136704 examples: {'rewards_train/chosen': '0.07315', 'rewards_train/rejected': '0.083218', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010068', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-164.03', 'loss/train': '0.70952', 'examples_per_second': '31.824', 'grad_norm': '28.875', 'counters/examples': 136704, 'counters/updates': 4272}
skipping logging after 136736 examples to avoid logging too frequently
train stats after 136768 examples: {'rewards_train/chosen': '0.051691', 'rewards_train/rejected': '-0.0097714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061462', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-121.72', 'loss/train': '0.67329', 'examples_per_second': '33.227', 'grad_norm': '24.5', 'counters/examples': 136768, 'counters/updates': 4274}
train stats after 136800 examples: {'rewards_train/chosen': '0.12049', 'rewards_train/rejected': '0.030314', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090179', 'logps_train/rejected': '-90.582', 'logps_train/chosen': '-111.39', 'loss/train': '0.65979', 'examples_per_second': '30.69', 'grad_norm': '21.625', 'counters/examples': 136800, 'counters/updates': 4275}
train stats after 136832 examples: {'rewards_train/chosen': '0.087468', 'rewards_train/rejected': '0.060863', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026604', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-146.23', 'loss/train': '0.69026', 'examples_per_second': '32.74', 'grad_norm': '26.875', 'counters/examples': 136832, 'counters/updates': 4276}
train stats after 136864 examples: {'rewards_train/chosen': '0.11047', 'rewards_train/rejected': '0.084217', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026249', 'logps_train/rejected': '-131.6', 'logps_train/chosen': '-138.28', 'loss/train': '0.69254', 'examples_per_second': '31.028', 'grad_norm': '25.75', 'counters/examples': 136864, 'counters/updates': 4277}
train stats after 136896 examples: {'rewards_train/chosen': '0.050347', 'rewards_train/rejected': '0.0655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015153', 'logps_train/rejected': '-139.91', 'logps_train/chosen': '-135.94', 'loss/train': '0.71328', 'examples_per_second': '31.637', 'grad_norm': '28.25', 'counters/examples': 136896, 'counters/updates': 4278}
train stats after 136928 examples: {'rewards_train/chosen': '0.12578', 'rewards_train/rejected': '0.050837', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074947', 'logps_train/rejected': '-123.86', 'logps_train/chosen': '-138.69', 'loss/train': '0.66384', 'examples_per_second': '31.586', 'grad_norm': '32.75', 'counters/examples': 136928, 'counters/updates': 4279}
train stats after 136960 examples: {'rewards_train/chosen': '0.10655', 'rewards_train/rejected': '0.10318', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0033632', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-152.62', 'loss/train': '0.70231', 'examples_per_second': '33.036', 'grad_norm': '27.5', 'counters/examples': 136960, 'counters/updates': 4280}
train stats after 136992 examples: {'rewards_train/chosen': '0.10644', 'rewards_train/rejected': '0.018885', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087555', 'logps_train/rejected': '-85.838', 'logps_train/chosen': '-147.27', 'loss/train': '0.65549', 'examples_per_second': '30.199', 'grad_norm': '23', 'counters/examples': 136992, 'counters/updates': 4281}
train stats after 137024 examples: {'rewards_train/chosen': '0.10219', 'rewards_train/rejected': '0.12267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020477', 'logps_train/rejected': '-166.09', 'logps_train/chosen': '-179.08', 'loss/train': '0.71579', 'examples_per_second': '32.988', 'grad_norm': '34.25', 'counters/examples': 137024, 'counters/updates': 4282}
train stats after 137056 examples: {'rewards_train/chosen': '0.13901', 'rewards_train/rejected': '-0.019194', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15821', 'logps_train/rejected': '-121.73', 'logps_train/chosen': '-154.88', 'loss/train': '0.62796', 'examples_per_second': '31.626', 'grad_norm': '24.5', 'counters/examples': 137056, 'counters/updates': 4283}
train stats after 137088 examples: {'rewards_train/chosen': '0.12259', 'rewards_train/rejected': '0.017054', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10554', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-126.95', 'loss/train': '0.64783', 'examples_per_second': '30.612', 'grad_norm': '22.375', 'counters/examples': 137088, 'counters/updates': 4284}
train stats after 137120 examples: {'rewards_train/chosen': '0.15209', 'rewards_train/rejected': '0.0070398', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14505', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-185.94', 'loss/train': '0.63795', 'examples_per_second': '31.634', 'grad_norm': '26.75', 'counters/examples': 137120, 'counters/updates': 4285}
train stats after 137152 examples: {'rewards_train/chosen': '0.07873', 'rewards_train/rejected': '0.050234', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028495', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-164.61', 'loss/train': '0.6887', 'examples_per_second': '31.501', 'grad_norm': '28.25', 'counters/examples': 137152, 'counters/updates': 4286}
train stats after 137184 examples: {'rewards_train/chosen': '0.082387', 'rewards_train/rejected': '0.00045159', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081935', 'logps_train/rejected': '-111.48', 'logps_train/chosen': '-168.08', 'loss/train': '0.66596', 'examples_per_second': '31.657', 'grad_norm': '25', 'counters/examples': 137184, 'counters/updates': 4287}
train stats after 137216 examples: {'rewards_train/chosen': '0.15553', 'rewards_train/rejected': '0.073612', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.081916', 'logps_train/rejected': '-91.606', 'logps_train/chosen': '-132.81', 'loss/train': '0.66575', 'examples_per_second': '31.934', 'grad_norm': '23.25', 'counters/examples': 137216, 'counters/updates': 4288}
train stats after 137248 examples: {'rewards_train/chosen': '0.19169', 'rewards_train/rejected': '-0.022662', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21435', 'logps_train/rejected': '-140.86', 'logps_train/chosen': '-174.48', 'loss/train': '0.60653', 'examples_per_second': '30.127', 'grad_norm': '27.625', 'counters/examples': 137248, 'counters/updates': 4289}
skipping logging after 137280 examples to avoid logging too frequently
train stats after 137312 examples: {'rewards_train/chosen': '0.11038', 'rewards_train/rejected': '0.058199', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052184', 'logps_train/rejected': '-108.67', 'logps_train/chosen': '-137.05', 'loss/train': '0.67542', 'examples_per_second': '35.561', 'grad_norm': '25.375', 'counters/examples': 137312, 'counters/updates': 4291}
train stats after 137344 examples: {'rewards_train/chosen': '0.094492', 'rewards_train/rejected': '0.025007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069485', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-133.01', 'loss/train': '0.66566', 'examples_per_second': '30.19', 'grad_norm': '26.375', 'counters/examples': 137344, 'counters/updates': 4292}
train stats after 137376 examples: {'rewards_train/chosen': '0.12109', 'rewards_train/rejected': '0.0064712', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11462', 'logps_train/rejected': '-133.84', 'logps_train/chosen': '-166.44', 'loss/train': '0.6463', 'examples_per_second': '32.192', 'grad_norm': '26.875', 'counters/examples': 137376, 'counters/updates': 4293}
train stats after 137408 examples: {'rewards_train/chosen': '0.14018', 'rewards_train/rejected': '0.10873', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031443', 'logps_train/rejected': '-164.76', 'logps_train/chosen': '-158.21', 'loss/train': '0.69223', 'examples_per_second': '32.33', 'grad_norm': '30.875', 'counters/examples': 137408, 'counters/updates': 4294}
skipping logging after 137440 examples to avoid logging too frequently
train stats after 137472 examples: {'rewards_train/chosen': '0.11267', 'rewards_train/rejected': '0.084656', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028014', 'logps_train/rejected': '-145.48', 'logps_train/chosen': '-159.75', 'loss/train': '0.69462', 'examples_per_second': '31.674', 'grad_norm': '28.75', 'counters/examples': 137472, 'counters/updates': 4296}
train stats after 137504 examples: {'rewards_train/chosen': '0.10977', 'rewards_train/rejected': '0.022019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08775', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-128.29', 'loss/train': '0.66003', 'examples_per_second': '31.645', 'grad_norm': '26.5', 'counters/examples': 137504, 'counters/updates': 4297}
train stats after 137536 examples: {'rewards_train/chosen': '0.04335', 'rewards_train/rejected': '-0.034242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077592', 'logps_train/rejected': '-116.23', 'logps_train/chosen': '-126.36', 'loss/train': '0.66277', 'examples_per_second': '31.664', 'grad_norm': '25', 'counters/examples': 137536, 'counters/updates': 4298}
train stats after 137568 examples: {'rewards_train/chosen': '0.17596', 'rewards_train/rejected': '0.03', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14596', 'logps_train/rejected': '-155.72', 'logps_train/chosen': '-178.98', 'loss/train': '0.64136', 'examples_per_second': '30.102', 'grad_norm': '30.5', 'counters/examples': 137568, 'counters/updates': 4299}
skipping logging after 137600 examples to avoid logging too frequently
train stats after 137632 examples: {'rewards_train/chosen': '0.094076', 'rewards_train/rejected': '-0.048057', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14213', 'logps_train/rejected': '-123.46', 'logps_train/chosen': '-139.3', 'loss/train': '0.63079', 'examples_per_second': '31.735', 'grad_norm': '25.375', 'counters/examples': 137632, 'counters/updates': 4301}
train stats after 137664 examples: {'rewards_train/chosen': '0.13501', 'rewards_train/rejected': '-0.014946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14996', 'logps_train/rejected': '-81.819', 'logps_train/chosen': '-113.5', 'loss/train': '0.63299', 'examples_per_second': '32.001', 'grad_norm': '25.75', 'counters/examples': 137664, 'counters/updates': 4302}
train stats after 137696 examples: {'rewards_train/chosen': '0.19731', 'rewards_train/rejected': '0.050407', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1469', 'logps_train/rejected': '-131.88', 'logps_train/chosen': '-165.2', 'loss/train': '0.63386', 'examples_per_second': '31.695', 'grad_norm': '26.75', 'counters/examples': 137696, 'counters/updates': 4303}
train stats after 137728 examples: {'rewards_train/chosen': '0.042794', 'rewards_train/rejected': '-0.033712', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076506', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-143.15', 'loss/train': '0.66116', 'examples_per_second': '32.699', 'grad_norm': '27.875', 'counters/examples': 137728, 'counters/updates': 4304}
train stats after 137760 examples: {'rewards_train/chosen': '0.10624', 'rewards_train/rejected': '0.02796', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078277', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-123.34', 'loss/train': '0.66551', 'examples_per_second': '30.707', 'grad_norm': '24.875', 'counters/examples': 137760, 'counters/updates': 4305}
train stats after 137792 examples: {'rewards_train/chosen': '0.048454', 'rewards_train/rejected': '-0.01937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067824', 'logps_train/rejected': '-95.516', 'logps_train/chosen': '-121.32', 'loss/train': '0.66963', 'examples_per_second': '32.446', 'grad_norm': '23.375', 'counters/examples': 137792, 'counters/updates': 4306}
train stats after 137824 examples: {'rewards_train/chosen': '0.138', 'rewards_train/rejected': '0.018716', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11929', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-155.69', 'loss/train': '0.6435', 'examples_per_second': '31.639', 'grad_norm': '25.75', 'counters/examples': 137824, 'counters/updates': 4307}
train stats after 137856 examples: {'rewards_train/chosen': '0.15457', 'rewards_train/rejected': '-0.013074', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16764', 'logps_train/rejected': '-97.376', 'logps_train/chosen': '-118.68', 'loss/train': '0.61993', 'examples_per_second': '30.615', 'grad_norm': '23.875', 'counters/examples': 137856, 'counters/updates': 4308}
train stats after 137888 examples: {'rewards_train/chosen': '0.094439', 'rewards_train/rejected': '0.097137', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0026976', 'logps_train/rejected': '-122.33', 'logps_train/chosen': '-119.38', 'loss/train': '0.70422', 'examples_per_second': '32.47', 'grad_norm': '24.5', 'counters/examples': 137888, 'counters/updates': 4309}
train stats after 137920 examples: {'rewards_train/chosen': '0.20041', 'rewards_train/rejected': '0.083659', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11675', 'logps_train/rejected': '-118.85', 'logps_train/chosen': '-132.56', 'loss/train': '0.64653', 'examples_per_second': '31.391', 'grad_norm': '24.375', 'counters/examples': 137920, 'counters/updates': 4310}
train stats after 137952 examples: {'rewards_train/chosen': '0.1104', 'rewards_train/rejected': '0.060153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050249', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-144.88', 'loss/train': '0.67668', 'examples_per_second': '30.71', 'grad_norm': '29.5', 'counters/examples': 137952, 'counters/updates': 4311}
train stats after 137984 examples: {'rewards_train/chosen': '0.16385', 'rewards_train/rejected': '0.031338', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13251', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-170.43', 'loss/train': '0.64754', 'examples_per_second': '30.835', 'grad_norm': '28.625', 'counters/examples': 137984, 'counters/updates': 4312}
train stats after 138016 examples: {'rewards_train/chosen': '0.14388', 'rewards_train/rejected': '0.0021487', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14173', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-137.57', 'loss/train': '0.63337', 'examples_per_second': '30.722', 'grad_norm': '24.625', 'counters/examples': 138016, 'counters/updates': 4313}
train stats after 138048 examples: {'rewards_train/chosen': '0.14904', 'rewards_train/rejected': '0.018327', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13071', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-127.62', 'loss/train': '0.63599', 'examples_per_second': '31.049', 'grad_norm': '24.125', 'counters/examples': 138048, 'counters/updates': 4314}
train stats after 138080 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '0.018395', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096426', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-140.63', 'loss/train': '0.65496', 'examples_per_second': '23.981', 'grad_norm': '26.25', 'counters/examples': 138080, 'counters/updates': 4315}
train stats after 138112 examples: {'rewards_train/chosen': '0.093371', 'rewards_train/rejected': '0.049174', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044196', 'logps_train/rejected': '-163.05', 'logps_train/chosen': '-186.11', 'loss/train': '0.68481', 'examples_per_second': '31.159', 'grad_norm': '32.25', 'counters/examples': 138112, 'counters/updates': 4316}
train stats after 138144 examples: {'rewards_train/chosen': '0.10991', 'rewards_train/rejected': '0.027453', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082453', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-170.84', 'loss/train': '0.66214', 'examples_per_second': '32.553', 'grad_norm': '28.5', 'counters/examples': 138144, 'counters/updates': 4317}
skipping logging after 138176 examples to avoid logging too frequently
train stats after 138208 examples: {'rewards_train/chosen': '0.13173', 'rewards_train/rejected': '0.10389', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027839', 'logps_train/rejected': '-164.48', 'logps_train/chosen': '-155.6', 'loss/train': '0.68654', 'examples_per_second': '30.103', 'grad_norm': '30.25', 'counters/examples': 138208, 'counters/updates': 4319}
train stats after 138240 examples: {'rewards_train/chosen': '0.11371', 'rewards_train/rejected': '0.046648', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067061', 'logps_train/rejected': '-105.48', 'logps_train/chosen': '-128.2', 'loss/train': '0.66962', 'examples_per_second': '32.161', 'grad_norm': '24', 'counters/examples': 138240, 'counters/updates': 4320}
train stats after 138272 examples: {'rewards_train/chosen': '0.13972', 'rewards_train/rejected': '0.052493', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087229', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-93.973', 'loss/train': '0.66243', 'examples_per_second': '31.102', 'grad_norm': '23.5', 'counters/examples': 138272, 'counters/updates': 4321}
train stats after 138304 examples: {'rewards_train/chosen': '0.13702', 'rewards_train/rejected': '0.12737', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0096508', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-130.16', 'loss/train': '0.69423', 'examples_per_second': '30.298', 'grad_norm': '25.75', 'counters/examples': 138304, 'counters/updates': 4322}
train stats after 138336 examples: {'rewards_train/chosen': '0.071516', 'rewards_train/rejected': '0.059654', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011862', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-137.17', 'loss/train': '0.69984', 'examples_per_second': '31.591', 'grad_norm': '30', 'counters/examples': 138336, 'counters/updates': 4323}
train stats after 138368 examples: {'rewards_train/chosen': '0.15937', 'rewards_train/rejected': '0.066901', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092473', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-158.32', 'loss/train': '0.65931', 'examples_per_second': '31.134', 'grad_norm': '29', 'counters/examples': 138368, 'counters/updates': 4324}
train stats after 138400 examples: {'rewards_train/chosen': '0.082225', 'rewards_train/rejected': '0.023273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058952', 'logps_train/rejected': '-100.95', 'logps_train/chosen': '-120.66', 'loss/train': '0.67499', 'examples_per_second': '31.101', 'grad_norm': '26', 'counters/examples': 138400, 'counters/updates': 4325}
train stats after 138432 examples: {'rewards_train/chosen': '0.050833', 'rewards_train/rejected': '0.064922', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014089', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-103.04', 'loss/train': '0.70828', 'examples_per_second': '31.028', 'grad_norm': '27', 'counters/examples': 138432, 'counters/updates': 4326}
train stats after 138464 examples: {'rewards_train/chosen': '0.22766', 'rewards_train/rejected': '0.02779', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19987', 'logps_train/rejected': '-160.45', 'logps_train/chosen': '-173.79', 'loss/train': '0.61696', 'examples_per_second': '31.659', 'grad_norm': '28.375', 'counters/examples': 138464, 'counters/updates': 4327}
train stats after 138496 examples: {'rewards_train/chosen': '0.17726', 'rewards_train/rejected': '0.0033387', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17392', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-139.74', 'loss/train': '0.62053', 'examples_per_second': '31.633', 'grad_norm': '25.25', 'counters/examples': 138496, 'counters/updates': 4328}
train stats after 138528 examples: {'rewards_train/chosen': '0.15182', 'rewards_train/rejected': '0.11208', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039733', 'logps_train/rejected': '-161.09', 'logps_train/chosen': '-160.67', 'loss/train': '0.68251', 'examples_per_second': '31.719', 'grad_norm': '30.875', 'counters/examples': 138528, 'counters/updates': 4329}
train stats after 138560 examples: {'rewards_train/chosen': '0.062119', 'rewards_train/rejected': '0.029251', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032867', 'logps_train/rejected': '-141.74', 'logps_train/chosen': '-141.54', 'loss/train': '0.68941', 'examples_per_second': '31.622', 'grad_norm': '32.25', 'counters/examples': 138560, 'counters/updates': 4330}
train stats after 138592 examples: {'rewards_train/chosen': '0.12768', 'rewards_train/rejected': '0.10093', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026743', 'logps_train/rejected': '-126.88', 'logps_train/chosen': '-145.27', 'loss/train': '0.69146', 'examples_per_second': '30.821', 'grad_norm': '27.125', 'counters/examples': 138592, 'counters/updates': 4331}
train stats after 138624 examples: {'rewards_train/chosen': '0.09761', 'rewards_train/rejected': '0.066198', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031411', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-110.96', 'loss/train': '0.68739', 'examples_per_second': '33.242', 'grad_norm': '29.875', 'counters/examples': 138624, 'counters/updates': 4332}
train stats after 138656 examples: {'rewards_train/chosen': '0.12688', 'rewards_train/rejected': '0.0052102', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12167', 'logps_train/rejected': '-113.31', 'logps_train/chosen': '-136.29', 'loss/train': '0.6413', 'examples_per_second': '32.887', 'grad_norm': '24.125', 'counters/examples': 138656, 'counters/updates': 4333}
train stats after 138688 examples: {'rewards_train/chosen': '0.083655', 'rewards_train/rejected': '-0.015456', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099111', 'logps_train/rejected': '-97.177', 'logps_train/chosen': '-117.66', 'loss/train': '0.6507', 'examples_per_second': '30.473', 'grad_norm': '22.375', 'counters/examples': 138688, 'counters/updates': 4334}
train stats after 138720 examples: {'rewards_train/chosen': '0.095139', 'rewards_train/rejected': '0.05964', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035499', 'logps_train/rejected': '-106.91', 'logps_train/chosen': '-150.54', 'loss/train': '0.6866', 'examples_per_second': '31.367', 'grad_norm': '27.75', 'counters/examples': 138720, 'counters/updates': 4335}
train stats after 138752 examples: {'rewards_train/chosen': '0.065248', 'rewards_train/rejected': '0.026281', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038967', 'logps_train/rejected': '-125.61', 'logps_train/chosen': '-145.94', 'loss/train': '0.68162', 'examples_per_second': '31.277', 'grad_norm': '27.75', 'counters/examples': 138752, 'counters/updates': 4336}
skipping logging after 138784 examples to avoid logging too frequently
train stats after 138816 examples: {'rewards_train/chosen': '0.097078', 'rewards_train/rejected': '0.026451', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070627', 'logps_train/rejected': '-84.714', 'logps_train/chosen': '-104.84', 'loss/train': '0.66456', 'examples_per_second': '35.97', 'grad_norm': '20.625', 'counters/examples': 138816, 'counters/updates': 4338}
train stats after 138848 examples: {'rewards_train/chosen': '0.12144', 'rewards_train/rejected': '0.079347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042097', 'logps_train/rejected': '-138.62', 'logps_train/chosen': '-150.52', 'loss/train': '0.68387', 'examples_per_second': '31.547', 'grad_norm': '25.75', 'counters/examples': 138848, 'counters/updates': 4339}
train stats after 138880 examples: {'rewards_train/chosen': '0.095898', 'rewards_train/rejected': '0.058775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037123', 'logps_train/rejected': '-125.48', 'logps_train/chosen': '-132.5', 'loss/train': '0.68864', 'examples_per_second': '32.572', 'grad_norm': '25.125', 'counters/examples': 138880, 'counters/updates': 4340}
train stats after 138912 examples: {'rewards_train/chosen': '0.15318', 'rewards_train/rejected': '0.051174', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.102', 'logps_train/rejected': '-134.38', 'logps_train/chosen': '-157.65', 'loss/train': '0.66002', 'examples_per_second': '30.767', 'grad_norm': '25.75', 'counters/examples': 138912, 'counters/updates': 4341}
skipping logging after 138944 examples to avoid logging too frequently
train stats after 138976 examples: {'rewards_train/chosen': '0.052299', 'rewards_train/rejected': '0.0034124', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048886', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-139.03', 'loss/train': '0.67837', 'examples_per_second': '31.093', 'grad_norm': '27.25', 'counters/examples': 138976, 'counters/updates': 4343}
train stats after 139008 examples: {'rewards_train/chosen': '0.13945', 'rewards_train/rejected': '0.0376', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10185', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-149.56', 'loss/train': '0.6545', 'examples_per_second': '32.715', 'grad_norm': '28.125', 'counters/examples': 139008, 'counters/updates': 4344}
train stats after 139040 examples: {'rewards_train/chosen': '0.13622', 'rewards_train/rejected': '-0.011955', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14817', 'logps_train/rejected': '-82.587', 'logps_train/chosen': '-148.04', 'loss/train': '0.635', 'examples_per_second': '30.104', 'grad_norm': '22.875', 'counters/examples': 139040, 'counters/updates': 4345}
train stats after 139072 examples: {'rewards_train/chosen': '0.12827', 'rewards_train/rejected': '0.043948', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084326', 'logps_train/rejected': '-160.14', 'logps_train/chosen': '-174.35', 'loss/train': '0.66708', 'examples_per_second': '31.469', 'grad_norm': '32', 'counters/examples': 139072, 'counters/updates': 4346}
train stats after 139104 examples: {'rewards_train/chosen': '0.10503', 'rewards_train/rejected': '0.10811', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0030818', 'logps_train/rejected': '-107.7', 'logps_train/chosen': '-117.63', 'loss/train': '0.70727', 'examples_per_second': '30.59', 'grad_norm': '27.75', 'counters/examples': 139104, 'counters/updates': 4347}
train stats after 139136 examples: {'rewards_train/chosen': '0.086994', 'rewards_train/rejected': '-0.020924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10792', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-141.52', 'loss/train': '0.6473', 'examples_per_second': '32.432', 'grad_norm': '28.25', 'counters/examples': 139136, 'counters/updates': 4348}
train stats after 139168 examples: {'rewards_train/chosen': '0.070491', 'rewards_train/rejected': '0.045887', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024603', 'logps_train/rejected': '-158.74', 'logps_train/chosen': '-148.39', 'loss/train': '0.68982', 'examples_per_second': '32.163', 'grad_norm': '29.5', 'counters/examples': 139168, 'counters/updates': 4349}
train stats after 139200 examples: {'rewards_train/chosen': '0.028396', 'rewards_train/rejected': '-0.012776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041172', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-170.69', 'loss/train': '0.68492', 'examples_per_second': '32.417', 'grad_norm': '30', 'counters/examples': 139200, 'counters/updates': 4350}
train stats after 139232 examples: {'rewards_train/chosen': '0.05391', 'rewards_train/rejected': '0.057173', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0032631', 'logps_train/rejected': '-162.31', 'logps_train/chosen': '-140.19', 'loss/train': '0.70147', 'examples_per_second': '31.596', 'grad_norm': '30', 'counters/examples': 139232, 'counters/updates': 4351}
train stats after 139264 examples: {'rewards_train/chosen': '0.087265', 'rewards_train/rejected': '0.13102', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.043756', 'logps_train/rejected': '-154.58', 'logps_train/chosen': '-137.62', 'loss/train': '0.72452', 'examples_per_second': '31.612', 'grad_norm': '30.75', 'counters/examples': 139264, 'counters/updates': 4352}
train stats after 139296 examples: {'rewards_train/chosen': '0.096624', 'rewards_train/rejected': '0.049904', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046721', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-127.24', 'loss/train': '0.68293', 'examples_per_second': '31.603', 'grad_norm': '28.125', 'counters/examples': 139296, 'counters/updates': 4353}
skipping logging after 139328 examples to avoid logging too frequently
train stats after 139360 examples: {'rewards_train/chosen': '0.1083', 'rewards_train/rejected': '0.025715', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082583', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-137.39', 'loss/train': '0.65923', 'examples_per_second': '32.194', 'grad_norm': '25.75', 'counters/examples': 139360, 'counters/updates': 4355}
train stats after 139392 examples: {'rewards_train/chosen': '0.15694', 'rewards_train/rejected': '-0.011609', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16855', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-153.78', 'loss/train': '0.62449', 'examples_per_second': '32.629', 'grad_norm': '24.25', 'counters/examples': 139392, 'counters/updates': 4356}
skipping logging after 139424 examples to avoid logging too frequently
train stats after 139456 examples: {'rewards_train/chosen': '0.13403', 'rewards_train/rejected': '0.023345', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11069', 'logps_train/rejected': '-153.25', 'logps_train/chosen': '-150.26', 'loss/train': '0.6556', 'examples_per_second': '31.602', 'grad_norm': '30', 'counters/examples': 139456, 'counters/updates': 4358}
train stats after 139488 examples: {'rewards_train/chosen': '0.065151', 'rewards_train/rejected': '0.056996', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0081544', 'logps_train/rejected': '-106.57', 'logps_train/chosen': '-127.78', 'loss/train': '0.69652', 'examples_per_second': '31.059', 'grad_norm': '26', 'counters/examples': 139488, 'counters/updates': 4359}
train stats after 139520 examples: {'rewards_train/chosen': '0.13666', 'rewards_train/rejected': '0.068249', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068409', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-142.3', 'loss/train': '0.6669', 'examples_per_second': '31.538', 'grad_norm': '26.75', 'counters/examples': 139520, 'counters/updates': 4360}
train stats after 139552 examples: {'rewards_train/chosen': '0.072379', 'rewards_train/rejected': '-0.048882', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12126', 'logps_train/rejected': '-96.097', 'logps_train/chosen': '-127.65', 'loss/train': '0.64088', 'examples_per_second': '31.676', 'grad_norm': '29.25', 'counters/examples': 139552, 'counters/updates': 4361}
train stats after 139584 examples: {'rewards_train/chosen': '0.12261', 'rewards_train/rejected': '0.097112', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025501', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-125.74', 'loss/train': '0.68928', 'examples_per_second': '31.642', 'grad_norm': '31.625', 'counters/examples': 139584, 'counters/updates': 4362}
train stats after 139616 examples: {'rewards_train/chosen': '0.16395', 'rewards_train/rejected': '-0.043335', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20728', 'logps_train/rejected': '-127.6', 'logps_train/chosen': '-174.68', 'loss/train': '0.60826', 'examples_per_second': '31.934', 'grad_norm': '27.25', 'counters/examples': 139616, 'counters/updates': 4363}
skipping logging after 139648 examples to avoid logging too frequently
train stats after 139680 examples: {'rewards_train/chosen': '0.079352', 'rewards_train/rejected': '-0.0011478', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080499', 'logps_train/rejected': '-106.26', 'logps_train/chosen': '-137.97', 'loss/train': '0.66334', 'examples_per_second': '36.514', 'grad_norm': '24', 'counters/examples': 139680, 'counters/updates': 4365}
train stats after 139712 examples: {'rewards_train/chosen': '0.12405', 'rewards_train/rejected': '0.060981', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06307', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-153.85', 'loss/train': '0.66848', 'examples_per_second': '30.642', 'grad_norm': '26', 'counters/examples': 139712, 'counters/updates': 4366}
train stats after 139744 examples: {'rewards_train/chosen': '0.098753', 'rewards_train/rejected': '0.036283', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062471', 'logps_train/rejected': '-113.86', 'logps_train/chosen': '-130.59', 'loss/train': '0.66703', 'examples_per_second': '31.62', 'grad_norm': '24.75', 'counters/examples': 139744, 'counters/updates': 4367}
train stats after 139776 examples: {'rewards_train/chosen': '0.099627', 'rewards_train/rejected': '0.092546', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0070808', 'logps_train/rejected': '-118.02', 'logps_train/chosen': '-123.6', 'loss/train': '0.69646', 'examples_per_second': '33.057', 'grad_norm': '31.625', 'counters/examples': 139776, 'counters/updates': 4368}
skipping logging after 139808 examples to avoid logging too frequently
train stats after 139840 examples: {'rewards_train/chosen': '0.097997', 'rewards_train/rejected': '0.095093', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0029038', 'logps_train/rejected': '-157.85', 'logps_train/chosen': '-140.04', 'loss/train': '0.70292', 'examples_per_second': '30.353', 'grad_norm': '31.125', 'counters/examples': 139840, 'counters/updates': 4370}
train stats after 139872 examples: {'rewards_train/chosen': '0.093016', 'rewards_train/rejected': '0.072695', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.020321', 'logps_train/rejected': '-121.97', 'logps_train/chosen': '-142.7', 'loss/train': '0.69925', 'examples_per_second': '31.65', 'grad_norm': '30', 'counters/examples': 139872, 'counters/updates': 4371}
train stats after 139904 examples: {'rewards_train/chosen': '0.17648', 'rewards_train/rejected': '0.010707', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16578', 'logps_train/rejected': '-106.4', 'logps_train/chosen': '-138.9', 'loss/train': '0.62577', 'examples_per_second': '30.475', 'grad_norm': '22', 'counters/examples': 139904, 'counters/updates': 4372}
train stats after 139936 examples: {'rewards_train/chosen': '0.10516', 'rewards_train/rejected': '0.089625', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015531', 'logps_train/rejected': '-156.17', 'logps_train/chosen': '-158.7', 'loss/train': '0.69286', 'examples_per_second': '31.581', 'grad_norm': '32.5', 'counters/examples': 139936, 'counters/updates': 4373}
train stats after 139968 examples: {'rewards_train/chosen': '0.13047', 'rewards_train/rejected': '0.021812', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10866', 'logps_train/rejected': '-130.17', 'logps_train/chosen': '-134.72', 'loss/train': '0.64901', 'examples_per_second': '31.495', 'grad_norm': '27', 'counters/examples': 139968, 'counters/updates': 4374}
train stats after 140000 examples: {'rewards_train/chosen': '0.090246', 'rewards_train/rejected': '0.086894', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0033519', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-127.51', 'loss/train': '0.6989', 'examples_per_second': '31.526', 'grad_norm': '24.125', 'counters/examples': 140000, 'counters/updates': 4375}
train stats after 140032 examples: {'rewards_train/chosen': '0.12453', 'rewards_train/rejected': '0.12288', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0016484', 'logps_train/rejected': '-159.4', 'logps_train/chosen': '-122.1', 'loss/train': '0.71101', 'examples_per_second': '30.607', 'grad_norm': '29.375', 'counters/examples': 140032, 'counters/updates': 4376}
train stats after 140064 examples: {'rewards_train/chosen': '0.053002', 'rewards_train/rejected': '0.015895', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037107', 'logps_train/rejected': '-94.311', 'logps_train/chosen': '-123.05', 'loss/train': '0.68251', 'examples_per_second': '31.481', 'grad_norm': '23.375', 'counters/examples': 140064, 'counters/updates': 4377}
train stats after 140096 examples: {'rewards_train/chosen': '0.15059', 'rewards_train/rejected': '-0.0068335', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15742', 'logps_train/rejected': '-87.177', 'logps_train/chosen': '-149.21', 'loss/train': '0.6295', 'examples_per_second': '32.133', 'grad_norm': '22.75', 'counters/examples': 140096, 'counters/updates': 4378}
train stats after 140128 examples: {'rewards_train/chosen': '0.14792', 'rewards_train/rejected': '0.077065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070853', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-138.05', 'loss/train': '0.66878', 'examples_per_second': '30.133', 'grad_norm': '28', 'counters/examples': 140128, 'counters/updates': 4379}
train stats after 140160 examples: {'rewards_train/chosen': '0.15185', 'rewards_train/rejected': '0.017142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13471', 'logps_train/rejected': '-104.67', 'logps_train/chosen': '-146.48', 'loss/train': '0.63569', 'examples_per_second': '30.616', 'grad_norm': '24.5', 'counters/examples': 140160, 'counters/updates': 4380}
train stats after 140192 examples: {'rewards_train/chosen': '0.084021', 'rewards_train/rejected': '0.033214', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050807', 'logps_train/rejected': '-105.2', 'logps_train/chosen': '-123.88', 'loss/train': '0.68675', 'examples_per_second': '31.38', 'grad_norm': '25.5', 'counters/examples': 140192, 'counters/updates': 4381}
train stats after 140224 examples: {'rewards_train/chosen': '0.14646', 'rewards_train/rejected': '0.040283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10617', 'logps_train/rejected': '-108.44', 'logps_train/chosen': '-188.58', 'loss/train': '0.65218', 'examples_per_second': '31.58', 'grad_norm': '29.375', 'counters/examples': 140224, 'counters/updates': 4382}
train stats after 140256 examples: {'rewards_train/chosen': '0.091684', 'rewards_train/rejected': '0.036054', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05563', 'logps_train/rejected': '-176.77', 'logps_train/chosen': '-171.95', 'loss/train': '0.67535', 'examples_per_second': '33.062', 'grad_norm': '31.125', 'counters/examples': 140256, 'counters/updates': 4383}
train stats after 140288 examples: {'rewards_train/chosen': '0.11222', 'rewards_train/rejected': '0.068909', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043307', 'logps_train/rejected': '-148.57', 'logps_train/chosen': '-152.21', 'loss/train': '0.6852', 'examples_per_second': '31.379', 'grad_norm': '27.125', 'counters/examples': 140288, 'counters/updates': 4384}
train stats after 140320 examples: {'rewards_train/chosen': '0.099567', 'rewards_train/rejected': '0.066577', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03299', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-137.59', 'loss/train': '0.6833', 'examples_per_second': '30.599', 'grad_norm': '25.125', 'counters/examples': 140320, 'counters/updates': 4385}
skipping logging after 140352 examples to avoid logging too frequently
train stats after 140384 examples: {'rewards_train/chosen': '0.19798', 'rewards_train/rejected': '0.041659', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15633', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-169.27', 'loss/train': '0.63423', 'examples_per_second': '31.032', 'grad_norm': '25.625', 'counters/examples': 140384, 'counters/updates': 4387}
train stats after 140416 examples: {'rewards_train/chosen': '0.16215', 'rewards_train/rejected': '0.013739', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14841', 'logps_train/rejected': '-144.11', 'logps_train/chosen': '-107.2', 'loss/train': '0.63184', 'examples_per_second': '31.101', 'grad_norm': '25.25', 'counters/examples': 140416, 'counters/updates': 4388}
train stats after 140448 examples: {'rewards_train/chosen': '0.083838', 'rewards_train/rejected': '0.01944', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064398', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-149.86', 'loss/train': '0.67209', 'examples_per_second': '32.42', 'grad_norm': '30.75', 'counters/examples': 140448, 'counters/updates': 4389}
skipping logging after 140480 examples to avoid logging too frequently
train stats after 140512 examples: {'rewards_train/chosen': '0.10178', 'rewards_train/rejected': '0.00098808', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10079', 'logps_train/rejected': '-99.552', 'logps_train/chosen': '-131.27', 'loss/train': '0.65201', 'examples_per_second': '32.648', 'grad_norm': '22.5', 'counters/examples': 140512, 'counters/updates': 4391}
train stats after 140544 examples: {'rewards_train/chosen': '0.099199', 'rewards_train/rejected': '0.031238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067961', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-145.33', 'loss/train': '0.67255', 'examples_per_second': '32.335', 'grad_norm': '27.125', 'counters/examples': 140544, 'counters/updates': 4392}
train stats after 140576 examples: {'rewards_train/chosen': '0.12311', 'rewards_train/rejected': '0.10746', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015651', 'logps_train/rejected': '-147.35', 'logps_train/chosen': '-147.68', 'loss/train': '0.69336', 'examples_per_second': '30.728', 'grad_norm': '50.5', 'counters/examples': 140576, 'counters/updates': 4393}
train stats after 140608 examples: {'rewards_train/chosen': '0.13802', 'rewards_train/rejected': '0.055063', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.082959', 'logps_train/rejected': '-114.8', 'logps_train/chosen': '-115.14', 'loss/train': '0.66079', 'examples_per_second': '31.68', 'grad_norm': '26.625', 'counters/examples': 140608, 'counters/updates': 4394}
train stats after 140640 examples: {'rewards_train/chosen': '0.14606', 'rewards_train/rejected': '-0.034164', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18023', 'logps_train/rejected': '-116.25', 'logps_train/chosen': '-121.49', 'loss/train': '0.61804', 'examples_per_second': '30.083', 'grad_norm': '30.375', 'counters/examples': 140640, 'counters/updates': 4395}
train stats after 140672 examples: {'rewards_train/chosen': '0.17161', 'rewards_train/rejected': '0.075897', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095715', 'logps_train/rejected': '-172.05', 'logps_train/chosen': '-155.1', 'loss/train': '0.65584', 'examples_per_second': '32.286', 'grad_norm': '29.375', 'counters/examples': 140672, 'counters/updates': 4396}
train stats after 140704 examples: {'rewards_train/chosen': '0.19724', 'rewards_train/rejected': '0.055403', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14183', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-179.93', 'loss/train': '0.63239', 'examples_per_second': '30.984', 'grad_norm': '29.25', 'counters/examples': 140704, 'counters/updates': 4397}
train stats after 140736 examples: {'rewards_train/chosen': '0.15727', 'rewards_train/rejected': '-0.0042625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16154', 'logps_train/rejected': '-102.51', 'logps_train/chosen': '-135.1', 'loss/train': '0.62427', 'examples_per_second': '31.591', 'grad_norm': '23.125', 'counters/examples': 140736, 'counters/updates': 4398}
train stats after 140768 examples: {'rewards_train/chosen': '0.13963', 'rewards_train/rejected': '0.094144', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045487', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-134.08', 'loss/train': '0.67772', 'examples_per_second': '30.841', 'grad_norm': '26.375', 'counters/examples': 140768, 'counters/updates': 4399}
train stats after 140800 examples: {'rewards_train/chosen': '0.060186', 'rewards_train/rejected': '-0.00068315', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060869', 'logps_train/rejected': '-105.97', 'logps_train/chosen': '-148.83', 'loss/train': '0.66984', 'examples_per_second': '32.124', 'grad_norm': '24.625', 'counters/examples': 140800, 'counters/updates': 4400}
train stats after 140832 examples: {'rewards_train/chosen': '0.13275', 'rewards_train/rejected': '0.013911', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11884', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-169.15', 'loss/train': '0.64876', 'examples_per_second': '30.769', 'grad_norm': '27', 'counters/examples': 140832, 'counters/updates': 4401}
train stats after 140864 examples: {'rewards_train/chosen': '0.10021', 'rewards_train/rejected': '0.021433', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078774', 'logps_train/rejected': '-89.73', 'logps_train/chosen': '-134.77', 'loss/train': '0.66108', 'examples_per_second': '30.651', 'grad_norm': '28.875', 'counters/examples': 140864, 'counters/updates': 4402}
train stats after 140896 examples: {'rewards_train/chosen': '0.091531', 'rewards_train/rejected': '-0.01287', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1044', 'logps_train/rejected': '-79.035', 'logps_train/chosen': '-116.82', 'loss/train': '0.64759', 'examples_per_second': '32.074', 'grad_norm': '21.125', 'counters/examples': 140896, 'counters/updates': 4403}
train stats after 140928 examples: {'rewards_train/chosen': '0.1422', 'rewards_train/rejected': '0.062716', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07948', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-126.04', 'loss/train': '0.66624', 'examples_per_second': '32.146', 'grad_norm': '26.375', 'counters/examples': 140928, 'counters/updates': 4404}
train stats after 140960 examples: {'rewards_train/chosen': '0.064039', 'rewards_train/rejected': '0.14053', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.076495', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-133.14', 'loss/train': '0.74272', 'examples_per_second': '30.983', 'grad_norm': '30.25', 'counters/examples': 140960, 'counters/updates': 4405}
train stats after 140992 examples: {'rewards_train/chosen': '0.11739', 'rewards_train/rejected': '0.048035', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06935', 'logps_train/rejected': '-117.17', 'logps_train/chosen': '-151.14', 'loss/train': '0.66891', 'examples_per_second': '24.23', 'grad_norm': '26.5', 'counters/examples': 140992, 'counters/updates': 4406}
train stats after 141024 examples: {'rewards_train/chosen': '0.14677', 'rewards_train/rejected': '0.10768', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.039088', 'logps_train/rejected': '-188.53', 'logps_train/chosen': '-128.86', 'loss/train': '0.68396', 'examples_per_second': '31.574', 'grad_norm': '29.125', 'counters/examples': 141024, 'counters/updates': 4407}
train stats after 141056 examples: {'rewards_train/chosen': '0.12772', 'rewards_train/rejected': '0.045309', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082411', 'logps_train/rejected': '-98.34', 'logps_train/chosen': '-146.91', 'loss/train': '0.66036', 'examples_per_second': '30.106', 'grad_norm': '24.875', 'counters/examples': 141056, 'counters/updates': 4408}
train stats after 141088 examples: {'rewards_train/chosen': '0.020644', 'rewards_train/rejected': '0.014839', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0058047', 'logps_train/rejected': '-121.54', 'logps_train/chosen': '-141.04', 'loss/train': '0.70107', 'examples_per_second': '25.31', 'grad_norm': '27.75', 'counters/examples': 141088, 'counters/updates': 4409}
skipping logging after 141120 examples to avoid logging too frequently
train stats after 141152 examples: {'rewards_train/chosen': '0.13484', 'rewards_train/rejected': '0.065119', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069723', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-137.11', 'loss/train': '0.67475', 'examples_per_second': '31.305', 'grad_norm': '28.375', 'counters/examples': 141152, 'counters/updates': 4411}
train stats after 141184 examples: {'rewards_train/chosen': '0.069088', 'rewards_train/rejected': '0.0061744', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062914', 'logps_train/rejected': '-104.5', 'logps_train/chosen': '-123.91', 'loss/train': '0.66998', 'examples_per_second': '32.157', 'grad_norm': '23.5', 'counters/examples': 141184, 'counters/updates': 4412}
train stats after 141216 examples: {'rewards_train/chosen': '0.062176', 'rewards_train/rejected': '0.1009', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038727', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-161.04', 'loss/train': '0.71766', 'examples_per_second': '31.569', 'grad_norm': '28.375', 'counters/examples': 141216, 'counters/updates': 4413}
train stats after 141248 examples: {'rewards_train/chosen': '0.15334', 'rewards_train/rejected': '0.049593', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10375', 'logps_train/rejected': '-105.09', 'logps_train/chosen': '-138.53', 'loss/train': '0.65935', 'examples_per_second': '31.452', 'grad_norm': '26.125', 'counters/examples': 141248, 'counters/updates': 4414}
train stats after 141280 examples: {'rewards_train/chosen': '0.030844', 'rewards_train/rejected': '-0.02279', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053634', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-120.09', 'loss/train': '0.67234', 'examples_per_second': '31.182', 'grad_norm': '23', 'counters/examples': 141280, 'counters/updates': 4415}
train stats after 141312 examples: {'rewards_train/chosen': '0.075354', 'rewards_train/rejected': '0.0045077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070847', 'logps_train/rejected': '-110.25', 'logps_train/chosen': '-163.23', 'loss/train': '0.66468', 'examples_per_second': '31.418', 'grad_norm': '30.875', 'counters/examples': 141312, 'counters/updates': 4416}
train stats after 141344 examples: {'rewards_train/chosen': '0.054716', 'rewards_train/rejected': '-0.047311', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10203', 'logps_train/rejected': '-104.39', 'logps_train/chosen': '-133.51', 'loss/train': '0.64838', 'examples_per_second': '31.426', 'grad_norm': '23.375', 'counters/examples': 141344, 'counters/updates': 4417}
train stats after 141376 examples: {'rewards_train/chosen': '0.13123', 'rewards_train/rejected': '0.012961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11826', 'logps_train/rejected': '-103.67', 'logps_train/chosen': '-121.26', 'loss/train': '0.64655', 'examples_per_second': '30.214', 'grad_norm': '23.5', 'counters/examples': 141376, 'counters/updates': 4418}
train stats after 141408 examples: {'rewards_train/chosen': '0.13997', 'rewards_train/rejected': '0.10254', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037432', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-146.64', 'loss/train': '0.68385', 'examples_per_second': '31.589', 'grad_norm': '25.25', 'counters/examples': 141408, 'counters/updates': 4419}
train stats after 141440 examples: {'rewards_train/chosen': '0.12496', 'rewards_train/rejected': '0.077542', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047416', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-162.14', 'loss/train': '0.68581', 'examples_per_second': '32.667', 'grad_norm': '28.625', 'counters/examples': 141440, 'counters/updates': 4420}
train stats after 141472 examples: {'rewards_train/chosen': '0.15227', 'rewards_train/rejected': '0.084103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068163', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-116.89', 'loss/train': '0.66623', 'examples_per_second': '31.163', 'grad_norm': '24.75', 'counters/examples': 141472, 'counters/updates': 4421}
train stats after 141504 examples: {'rewards_train/chosen': '0.098025', 'rewards_train/rejected': '0.074339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023686', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-133.89', 'loss/train': '0.68669', 'examples_per_second': '31.508', 'grad_norm': '26.5', 'counters/examples': 141504, 'counters/updates': 4422}
train stats after 141536 examples: {'rewards_train/chosen': '0.12324', 'rewards_train/rejected': '0.035377', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087858', 'logps_train/rejected': '-119.27', 'logps_train/chosen': '-124.19', 'loss/train': '0.65578', 'examples_per_second': '32.218', 'grad_norm': '24.25', 'counters/examples': 141536, 'counters/updates': 4423}
skipping logging after 141568 examples to avoid logging too frequently
train stats after 141600 examples: {'rewards_train/chosen': '0.073107', 'rewards_train/rejected': '0.027757', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04535', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-147.67', 'loss/train': '0.68365', 'examples_per_second': '30.756', 'grad_norm': '30.75', 'counters/examples': 141600, 'counters/updates': 4425}
skipping logging after 141632 examples to avoid logging too frequently
train stats after 141664 examples: {'rewards_train/chosen': '0.05777', 'rewards_train/rejected': '0.083012', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.025242', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-114.8', 'loss/train': '0.71215', 'examples_per_second': '33.667', 'grad_norm': '33.75', 'counters/examples': 141664, 'counters/updates': 4427}
train stats after 141696 examples: {'rewards_train/chosen': '0.17917', 'rewards_train/rejected': '0.061351', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11782', 'logps_train/rejected': '-137.85', 'logps_train/chosen': '-121.67', 'loss/train': '0.64385', 'examples_per_second': '32.467', 'grad_norm': '27.75', 'counters/examples': 141696, 'counters/updates': 4428}
train stats after 141728 examples: {'rewards_train/chosen': '0.12264', 'rewards_train/rejected': '0.017986', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10466', 'logps_train/rejected': '-102.07', 'logps_train/chosen': '-160.58', 'loss/train': '0.64946', 'examples_per_second': '32.268', 'grad_norm': '23.875', 'counters/examples': 141728, 'counters/updates': 4429}
train stats after 141760 examples: {'rewards_train/chosen': '0.13844', 'rewards_train/rejected': '0.015476', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12297', 'logps_train/rejected': '-126.2', 'logps_train/chosen': '-135.12', 'loss/train': '0.63957', 'examples_per_second': '31.463', 'grad_norm': '25.125', 'counters/examples': 141760, 'counters/updates': 4430}
train stats after 141792 examples: {'rewards_train/chosen': '0.13514', 'rewards_train/rejected': '0.047052', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088092', 'logps_train/rejected': '-134.35', 'logps_train/chosen': '-134.53', 'loss/train': '0.6587', 'examples_per_second': '31.512', 'grad_norm': '26.5', 'counters/examples': 141792, 'counters/updates': 4431}
skipping logging after 141824 examples to avoid logging too frequently
train stats after 141856 examples: {'rewards_train/chosen': '0.11918', 'rewards_train/rejected': '0.036825', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082359', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-136.85', 'loss/train': '0.66802', 'examples_per_second': '31.543', 'grad_norm': '26.125', 'counters/examples': 141856, 'counters/updates': 4433}
train stats after 141888 examples: {'rewards_train/chosen': '0.14669', 'rewards_train/rejected': '0.029607', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11709', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-183.88', 'loss/train': '0.65116', 'examples_per_second': '31.466', 'grad_norm': '29.75', 'counters/examples': 141888, 'counters/updates': 4434}
train stats after 141920 examples: {'rewards_train/chosen': '0.050631', 'rewards_train/rejected': '0.10229', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051662', 'logps_train/rejected': '-155.18', 'logps_train/chosen': '-126.17', 'loss/train': '0.73323', 'examples_per_second': '30.659', 'grad_norm': '33.5', 'counters/examples': 141920, 'counters/updates': 4435}
train stats after 141952 examples: {'rewards_train/chosen': '0.062673', 'rewards_train/rejected': '0.013401', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049272', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-105.66', 'loss/train': '0.67741', 'examples_per_second': '31.553', 'grad_norm': '24.125', 'counters/examples': 141952, 'counters/updates': 4436}
train stats after 141984 examples: {'rewards_train/chosen': '0.14869', 'rewards_train/rejected': '0.10369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045009', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-159.11', 'loss/train': '0.69493', 'examples_per_second': '31.365', 'grad_norm': '31.25', 'counters/examples': 141984, 'counters/updates': 4437}
train stats after 142016 examples: {'rewards_train/chosen': '0.16047', 'rewards_train/rejected': '0.01991', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14056', 'logps_train/rejected': '-122.15', 'logps_train/chosen': '-149.39', 'loss/train': '0.63425', 'examples_per_second': '31.337', 'grad_norm': '24.5', 'counters/examples': 142016, 'counters/updates': 4438}
skipping logging after 142048 examples to avoid logging too frequently
train stats after 142080 examples: {'rewards_train/chosen': '0.10071', 'rewards_train/rejected': '0.01989', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080816', 'logps_train/rejected': '-121.38', 'logps_train/chosen': '-107.28', 'loss/train': '0.66341', 'examples_per_second': '31.57', 'grad_norm': '23.75', 'counters/examples': 142080, 'counters/updates': 4440}
train stats after 142112 examples: {'rewards_train/chosen': '0.042447', 'rewards_train/rejected': '-0.013248', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055694', 'logps_train/rejected': '-155.76', 'logps_train/chosen': '-170', 'loss/train': '0.67969', 'examples_per_second': '32.081', 'grad_norm': '31.25', 'counters/examples': 142112, 'counters/updates': 4441}
skipping logging after 142144 examples to avoid logging too frequently
train stats after 142176 examples: {'rewards_train/chosen': '0.22952', 'rewards_train/rejected': '0.11183', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11769', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-116.33', 'loss/train': '0.64588', 'examples_per_second': '34.01', 'grad_norm': '27.25', 'counters/examples': 142176, 'counters/updates': 4443}
train stats after 142208 examples: {'rewards_train/chosen': '0.15386', 'rewards_train/rejected': '0.014609', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13925', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-159.14', 'loss/train': '0.6357', 'examples_per_second': '30.577', 'grad_norm': '29.5', 'counters/examples': 142208, 'counters/updates': 4444}
train stats after 142240 examples: {'rewards_train/chosen': '0.13811', 'rewards_train/rejected': '-0.011425', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14954', 'logps_train/rejected': '-88.505', 'logps_train/chosen': '-150.61', 'loss/train': '0.63277', 'examples_per_second': '30.863', 'grad_norm': '25.375', 'counters/examples': 142240, 'counters/updates': 4445}
skipping logging after 142272 examples to avoid logging too frequently
train stats after 142304 examples: {'rewards_train/chosen': '0.21739', 'rewards_train/rejected': '0.068869', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14852', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-155.93', 'loss/train': '0.63279', 'examples_per_second': '31.407', 'grad_norm': '30', 'counters/examples': 142304, 'counters/updates': 4447}
skipping logging after 142336 examples to avoid logging too frequently
train stats after 142368 examples: {'rewards_train/chosen': '0.096446', 'rewards_train/rejected': '0.010604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085842', 'logps_train/rejected': '-94.61', 'logps_train/chosen': '-103.2', 'loss/train': '0.65725', 'examples_per_second': '30.877', 'grad_norm': '21.625', 'counters/examples': 142368, 'counters/updates': 4449}
train stats after 142400 examples: {'rewards_train/chosen': '0.031268', 'rewards_train/rejected': '-0.0080522', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03932', 'logps_train/rejected': '-153.38', 'logps_train/chosen': '-157.09', 'loss/train': '0.68378', 'examples_per_second': '31.732', 'grad_norm': '29.5', 'counters/examples': 142400, 'counters/updates': 4450}
train stats after 142432 examples: {'rewards_train/chosen': '0.14441', 'rewards_train/rejected': '0.087284', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057126', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-122.09', 'loss/train': '0.67323', 'examples_per_second': '31.724', 'grad_norm': '24.25', 'counters/examples': 142432, 'counters/updates': 4451}
train stats after 142464 examples: {'rewards_train/chosen': '0.085159', 'rewards_train/rejected': '0.0099504', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075209', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-132.41', 'loss/train': '0.66361', 'examples_per_second': '31.964', 'grad_norm': '25.5', 'counters/examples': 142464, 'counters/updates': 4452}
train stats after 142496 examples: {'rewards_train/chosen': '0.088377', 'rewards_train/rejected': '-0.041624', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-151.44', 'loss/train': '0.63994', 'examples_per_second': '33.148', 'grad_norm': '27.375', 'counters/examples': 142496, 'counters/updates': 4453}
train stats after 142528 examples: {'rewards_train/chosen': '0.062135', 'rewards_train/rejected': '0.038025', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02411', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-142.56', 'loss/train': '0.69343', 'examples_per_second': '32.305', 'grad_norm': '28.25', 'counters/examples': 142528, 'counters/updates': 4454}
train stats after 142560 examples: {'rewards_train/chosen': '0.15431', 'rewards_train/rejected': '0.059852', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094454', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-140.64', 'loss/train': '0.66103', 'examples_per_second': '31.276', 'grad_norm': '24.75', 'counters/examples': 142560, 'counters/updates': 4455}
train stats after 142592 examples: {'rewards_train/chosen': '0.21638', 'rewards_train/rejected': '0.026139', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19024', 'logps_train/rejected': '-137.4', 'logps_train/chosen': '-170.19', 'loss/train': '0.61574', 'examples_per_second': '30.888', 'grad_norm': '29.875', 'counters/examples': 142592, 'counters/updates': 4456}
train stats after 142624 examples: {'rewards_train/chosen': '0.15037', 'rewards_train/rejected': '0.15123', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00085672', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-177.2', 'loss/train': '0.70547', 'examples_per_second': '31.541', 'grad_norm': '31.75', 'counters/examples': 142624, 'counters/updates': 4457}
train stats after 142656 examples: {'rewards_train/chosen': '0.075583', 'rewards_train/rejected': '-0.02454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10012', 'logps_train/rejected': '-82.155', 'logps_train/chosen': '-133.44', 'loss/train': '0.65974', 'examples_per_second': '31.415', 'grad_norm': '28.125', 'counters/examples': 142656, 'counters/updates': 4458}
train stats after 142688 examples: {'rewards_train/chosen': '0.29521', 'rewards_train/rejected': '0.12405', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17116', 'logps_train/rejected': '-113.91', 'logps_train/chosen': '-152.61', 'loss/train': '0.62692', 'examples_per_second': '31.327', 'grad_norm': '26.375', 'counters/examples': 142688, 'counters/updates': 4459}
skipping logging after 142720 examples to avoid logging too frequently
train stats after 142752 examples: {'rewards_train/chosen': '0.080392', 'rewards_train/rejected': '0.091111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.010719', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-113.21', 'loss/train': '0.7123', 'examples_per_second': '33.158', 'grad_norm': '30.625', 'counters/examples': 142752, 'counters/updates': 4461}
train stats after 142784 examples: {'rewards_train/chosen': '0.095385', 'rewards_train/rejected': '-0.023249', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11863', 'logps_train/rejected': '-150.24', 'logps_train/chosen': '-120.96', 'loss/train': '0.64426', 'examples_per_second': '32.514', 'grad_norm': '24', 'counters/examples': 142784, 'counters/updates': 4462}
train stats after 142816 examples: {'rewards_train/chosen': '0.11491', 'rewards_train/rejected': '0.0085874', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10632', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-130.41', 'loss/train': '0.65026', 'examples_per_second': '31.458', 'grad_norm': '24', 'counters/examples': 142816, 'counters/updates': 4463}
train stats after 142848 examples: {'rewards_train/chosen': '0.081199', 'rewards_train/rejected': '0.010473', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070726', 'logps_train/rejected': '-107.26', 'logps_train/chosen': '-126.81', 'loss/train': '0.66373', 'examples_per_second': '31.595', 'grad_norm': '22.75', 'counters/examples': 142848, 'counters/updates': 4464}
train stats after 142880 examples: {'rewards_train/chosen': '0.10959', 'rewards_train/rejected': '0.034897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074694', 'logps_train/rejected': '-116.7', 'logps_train/chosen': '-126.52', 'loss/train': '0.6627', 'examples_per_second': '30.385', 'grad_norm': '25.125', 'counters/examples': 142880, 'counters/updates': 4465}
train stats after 142912 examples: {'rewards_train/chosen': '0.15932', 'rewards_train/rejected': '0.062281', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097038', 'logps_train/rejected': '-106.79', 'logps_train/chosen': '-129.32', 'loss/train': '0.65254', 'examples_per_second': '30.125', 'grad_norm': '24.875', 'counters/examples': 142912, 'counters/updates': 4466}
train stats after 142944 examples: {'rewards_train/chosen': '0.15903', 'rewards_train/rejected': '0.076077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082949', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-146.7', 'loss/train': '0.65966', 'examples_per_second': '31.103', 'grad_norm': '29', 'counters/examples': 142944, 'counters/updates': 4467}
train stats after 142976 examples: {'rewards_train/chosen': '0.15875', 'rewards_train/rejected': '0.058234', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10051', 'logps_train/rejected': '-115.74', 'logps_train/chosen': '-147.7', 'loss/train': '0.66138', 'examples_per_second': '31.386', 'grad_norm': '29.625', 'counters/examples': 142976, 'counters/updates': 4468}
train stats after 143008 examples: {'rewards_train/chosen': '0.097291', 'rewards_train/rejected': '0.042884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054406', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-152.91', 'loss/train': '0.6784', 'examples_per_second': '32.536', 'grad_norm': '26.375', 'counters/examples': 143008, 'counters/updates': 4469}
train stats after 143040 examples: {'rewards_train/chosen': '0.20532', 'rewards_train/rejected': '-0.0040417', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20936', 'logps_train/rejected': '-96.289', 'logps_train/chosen': '-178.59', 'loss/train': '0.6081', 'examples_per_second': '29.912', 'grad_norm': '26.25', 'counters/examples': 143040, 'counters/updates': 4470}
skipping logging after 143072 examples to avoid logging too frequently
train stats after 143104 examples: {'rewards_train/chosen': '0.079785', 'rewards_train/rejected': '0.15003', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.070245', 'logps_train/rejected': '-181.87', 'logps_train/chosen': '-149.48', 'loss/train': '0.74244', 'examples_per_second': '32.379', 'grad_norm': '32.5', 'counters/examples': 143104, 'counters/updates': 4472}
train stats after 143136 examples: {'rewards_train/chosen': '0.1141', 'rewards_train/rejected': '0.020004', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094094', 'logps_train/rejected': '-135.89', 'logps_train/chosen': '-145.21', 'loss/train': '0.65249', 'examples_per_second': '31.494', 'grad_norm': '28.75', 'counters/examples': 143136, 'counters/updates': 4473}
train stats after 143168 examples: {'rewards_train/chosen': '0.11134', 'rewards_train/rejected': '0.03293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078406', 'logps_train/rejected': '-121.29', 'logps_train/chosen': '-130.95', 'loss/train': '0.66606', 'examples_per_second': '30.211', 'grad_norm': '25.875', 'counters/examples': 143168, 'counters/updates': 4474}
train stats after 143200 examples: {'rewards_train/chosen': '0.12353', 'rewards_train/rejected': '0.12373', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00020029', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-131.93', 'loss/train': '0.70251', 'examples_per_second': '32.348', 'grad_norm': '28.375', 'counters/examples': 143200, 'counters/updates': 4475}
train stats after 143232 examples: {'rewards_train/chosen': '0.12173', 'rewards_train/rejected': '0.043154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078573', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-136.38', 'loss/train': '0.66029', 'examples_per_second': '31.566', 'grad_norm': '24.5', 'counters/examples': 143232, 'counters/updates': 4476}
train stats after 143264 examples: {'rewards_train/chosen': '0.18316', 'rewards_train/rejected': '0.084695', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098462', 'logps_train/rejected': '-132.72', 'logps_train/chosen': '-157.54', 'loss/train': '0.65169', 'examples_per_second': '31.56', 'grad_norm': '29.375', 'counters/examples': 143264, 'counters/updates': 4477}
train stats after 143296 examples: {'rewards_train/chosen': '0.16698', 'rewards_train/rejected': '0.065635', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10135', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-142.64', 'loss/train': '0.65621', 'examples_per_second': '30.088', 'grad_norm': '26.625', 'counters/examples': 143296, 'counters/updates': 4478}
train stats after 143328 examples: {'rewards_train/chosen': '0.13517', 'rewards_train/rejected': '0.044992', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090174', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-152.09', 'loss/train': '0.66162', 'examples_per_second': '31.452', 'grad_norm': '27.5', 'counters/examples': 143328, 'counters/updates': 4479}
train stats after 143360 examples: {'rewards_train/chosen': '0.18067', 'rewards_train/rejected': '0.084104', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096567', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-178.29', 'loss/train': '0.65446', 'examples_per_second': '30.684', 'grad_norm': '27.75', 'counters/examples': 143360, 'counters/updates': 4480}
train stats after 143392 examples: {'rewards_train/chosen': '0.1489', 'rewards_train/rejected': '0.044424', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10447', 'logps_train/rejected': '-113.41', 'logps_train/chosen': '-134.37', 'loss/train': '0.66046', 'examples_per_second': '31.546', 'grad_norm': '27.5', 'counters/examples': 143392, 'counters/updates': 4481}
train stats after 143424 examples: {'rewards_train/chosen': '0.12088', 'rewards_train/rejected': '0.084445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036438', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-149.59', 'loss/train': '0.68124', 'examples_per_second': '31.485', 'grad_norm': '26.375', 'counters/examples': 143424, 'counters/updates': 4482}
train stats after 143456 examples: {'rewards_train/chosen': '0.10804', 'rewards_train/rejected': '0.084786', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023255', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-151.63', 'loss/train': '0.68908', 'examples_per_second': '29.963', 'grad_norm': '27.125', 'counters/examples': 143456, 'counters/updates': 4483}
skipping logging after 143488 examples to avoid logging too frequently
train stats after 143520 examples: {'rewards_train/chosen': '0.21017', 'rewards_train/rejected': '0.021647', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18852', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-142.04', 'loss/train': '0.6142', 'examples_per_second': '34.73', 'grad_norm': '25.25', 'counters/examples': 143520, 'counters/updates': 4485}
train stats after 143552 examples: {'rewards_train/chosen': '0.11096', 'rewards_train/rejected': '0.11454', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0035813', 'logps_train/rejected': '-163.15', 'logps_train/chosen': '-177.53', 'loss/train': '0.71244', 'examples_per_second': '30.948', 'grad_norm': '29.625', 'counters/examples': 143552, 'counters/updates': 4486}
train stats after 143584 examples: {'rewards_train/chosen': '0.1376', 'rewards_train/rejected': '0.038243', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099358', 'logps_train/rejected': '-96.076', 'logps_train/chosen': '-131.91', 'loss/train': '0.65385', 'examples_per_second': '30.539', 'grad_norm': '24', 'counters/examples': 143584, 'counters/updates': 4487}
train stats after 143616 examples: {'rewards_train/chosen': '0.065107', 'rewards_train/rejected': '0.016136', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.048971', 'logps_train/rejected': '-97.951', 'logps_train/chosen': '-109.84', 'loss/train': '0.67722', 'examples_per_second': '31.716', 'grad_norm': '23.125', 'counters/examples': 143616, 'counters/updates': 4488}
train stats after 143648 examples: {'rewards_train/chosen': '0.13961', 'rewards_train/rejected': '0.069052', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070556', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-168.04', 'loss/train': '0.66729', 'examples_per_second': '23.678', 'grad_norm': '34.25', 'counters/examples': 143648, 'counters/updates': 4489}
train stats after 143680 examples: {'rewards_train/chosen': '0.13797', 'rewards_train/rejected': '0.0059335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13204', 'logps_train/rejected': '-97.685', 'logps_train/chosen': '-127.45', 'loss/train': '0.64036', 'examples_per_second': '31.682', 'grad_norm': '22.25', 'counters/examples': 143680, 'counters/updates': 4490}
train stats after 143712 examples: {'rewards_train/chosen': '0.0964', 'rewards_train/rejected': '0.026451', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.06995', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-136.53', 'loss/train': '0.66748', 'examples_per_second': '32.704', 'grad_norm': '24.625', 'counters/examples': 143712, 'counters/updates': 4491}
train stats after 143744 examples: {'rewards_train/chosen': '0.14832', 'rewards_train/rejected': '0.035409', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11291', 'logps_train/rejected': '-114.26', 'logps_train/chosen': '-156.39', 'loss/train': '0.65217', 'examples_per_second': '30.889', 'grad_norm': '27.375', 'counters/examples': 143744, 'counters/updates': 4492}
train stats after 143776 examples: {'rewards_train/chosen': '0.11929', 'rewards_train/rejected': '0.16283', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.04354', 'logps_train/rejected': '-149.32', 'logps_train/chosen': '-141.42', 'loss/train': '0.73521', 'examples_per_second': '31.302', 'grad_norm': '75.5', 'counters/examples': 143776, 'counters/updates': 4493}
skipping logging after 143808 examples to avoid logging too frequently
train stats after 143840 examples: {'rewards_train/chosen': '0.079553', 'rewards_train/rejected': '-0.035358', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11491', 'logps_train/rejected': '-97.748', 'logps_train/chosen': '-156.04', 'loss/train': '0.64791', 'examples_per_second': '32.248', 'grad_norm': '25', 'counters/examples': 143840, 'counters/updates': 4495}
train stats after 143872 examples: {'rewards_train/chosen': '0.081469', 'rewards_train/rejected': '0.0072061', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074263', 'logps_train/rejected': '-114.35', 'logps_train/chosen': '-144.51', 'loss/train': '0.66436', 'examples_per_second': '30.965', 'grad_norm': '24', 'counters/examples': 143872, 'counters/updates': 4496}
skipping logging after 143904 examples to avoid logging too frequently
train stats after 143936 examples: {'rewards_train/chosen': '0.12706', 'rewards_train/rejected': '0.064108', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062949', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-136.18', 'loss/train': '0.6725', 'examples_per_second': '30.366', 'grad_norm': '33.5', 'counters/examples': 143936, 'counters/updates': 4498}
train stats after 143968 examples: {'rewards_train/chosen': '0.14368', 'rewards_train/rejected': '0.072656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071029', 'logps_train/rejected': '-140.82', 'logps_train/chosen': '-136.29', 'loss/train': '0.66562', 'examples_per_second': '31.534', 'grad_norm': '26.125', 'counters/examples': 143968, 'counters/updates': 4499}
skipping logging after 144000 examples to avoid logging too frequently
Running evaluation after 144000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.09it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.78it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.89it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.90it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.88it/s]
eval after 144000: {'rewards_eval/chosen': '0.13835', 'rewards_eval/rejected': '0.045891', 'rewards_eval/accuracies': '0.625', 'rewards_eval/margins': '0.092461', 'logps_eval/rejected': '-118.15', 'logps_eval/chosen': '-138.05', 'loss/eval': '0.65872'}
skipping save for non epoch
train stats after 144032 examples: {'rewards_train/chosen': '0.12848', 'rewards_train/rejected': '0.11005', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018429', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-152.15', 'loss/train': '0.69536', 'examples_per_second': '31.553', 'grad_norm': '31.75', 'counters/examples': 144032, 'counters/updates': 4501}
skipping logging after 144064 examples to avoid logging too frequently
train stats after 144096 examples: {'rewards_train/chosen': '0.17283', 'rewards_train/rejected': '0.010357', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16248', 'logps_train/rejected': '-119.2', 'logps_train/chosen': '-142.91', 'loss/train': '0.6254', 'examples_per_second': '31.906', 'grad_norm': '24.125', 'counters/examples': 144096, 'counters/updates': 4503}
skipping logging after 144128 examples to avoid logging too frequently
train stats after 144160 examples: {'rewards_train/chosen': '0.18651', 'rewards_train/rejected': '0.015839', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17067', 'logps_train/rejected': '-140.45', 'logps_train/chosen': '-167.98', 'loss/train': '0.625', 'examples_per_second': '31.123', 'grad_norm': '25.75', 'counters/examples': 144160, 'counters/updates': 4505}
skipping logging after 144192 examples to avoid logging too frequently
train stats after 144224 examples: {'rewards_train/chosen': '0.097945', 'rewards_train/rejected': '-0.0059779', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10392', 'logps_train/rejected': '-92.145', 'logps_train/chosen': '-125.47', 'loss/train': '0.64648', 'examples_per_second': '37.616', 'grad_norm': '23.125', 'counters/examples': 144224, 'counters/updates': 4507}
skipping logging after 144256 examples to avoid logging too frequently
train stats after 144288 examples: {'rewards_train/chosen': '0.11735', 'rewards_train/rejected': '0.037256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080094', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-151.34', 'loss/train': '0.66634', 'examples_per_second': '36.378', 'grad_norm': '27.875', 'counters/examples': 144288, 'counters/updates': 4509}
train stats after 144320 examples: {'rewards_train/chosen': '0.11645', 'rewards_train/rejected': '0.048633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067816', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-164.23', 'loss/train': '0.66989', 'examples_per_second': '30.402', 'grad_norm': '33.75', 'counters/examples': 144320, 'counters/updates': 4510}
train stats after 144352 examples: {'rewards_train/chosen': '0.14131', 'rewards_train/rejected': '0.042792', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098515', 'logps_train/rejected': '-96.588', 'logps_train/chosen': '-162.47', 'loss/train': '0.65132', 'examples_per_second': '31.742', 'grad_norm': '25.125', 'counters/examples': 144352, 'counters/updates': 4511}
train stats after 144384 examples: {'rewards_train/chosen': '0.14925', 'rewards_train/rejected': '0.074619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07463', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-156.27', 'loss/train': '0.67173', 'examples_per_second': '31.288', 'grad_norm': '28.875', 'counters/examples': 144384, 'counters/updates': 4512}
train stats after 144416 examples: {'rewards_train/chosen': '0.16282', 'rewards_train/rejected': '0.079276', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083547', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-150.08', 'loss/train': '0.6637', 'examples_per_second': '30.551', 'grad_norm': '28.625', 'counters/examples': 144416, 'counters/updates': 4513}
train stats after 144448 examples: {'rewards_train/chosen': '0.1805', 'rewards_train/rejected': '0.056642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12386', 'logps_train/rejected': '-136.62', 'logps_train/chosen': '-116.86', 'loss/train': '0.6402', 'examples_per_second': '31.314', 'grad_norm': '23.125', 'counters/examples': 144448, 'counters/updates': 4514}
train stats after 144480 examples: {'rewards_train/chosen': '0.18411', 'rewards_train/rejected': '0.10734', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076766', 'logps_train/rejected': '-137.45', 'logps_train/chosen': '-127.6', 'loss/train': '0.66531', 'examples_per_second': '30.427', 'grad_norm': '29.375', 'counters/examples': 144480, 'counters/updates': 4515}
train stats after 144512 examples: {'rewards_train/chosen': '0.18898', 'rewards_train/rejected': '0.064699', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12428', 'logps_train/rejected': '-135.1', 'logps_train/chosen': '-140.51', 'loss/train': '0.64258', 'examples_per_second': '31.024', 'grad_norm': '30.5', 'counters/examples': 144512, 'counters/updates': 4516}
train stats after 144544 examples: {'rewards_train/chosen': '0.1471', 'rewards_train/rejected': '0.066647', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080451', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-141.89', 'loss/train': '0.66972', 'examples_per_second': '32.104', 'grad_norm': '27', 'counters/examples': 144544, 'counters/updates': 4517}
train stats after 144576 examples: {'rewards_train/chosen': '0.050271', 'rewards_train/rejected': '0.065653', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015383', 'logps_train/rejected': '-102.98', 'logps_train/chosen': '-156.37', 'loss/train': '0.7132', 'examples_per_second': '32.031', 'grad_norm': '28.875', 'counters/examples': 144576, 'counters/updates': 4518}
train stats after 144608 examples: {'rewards_train/chosen': '0.099526', 'rewards_train/rejected': '0.064155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035372', 'logps_train/rejected': '-139.95', 'logps_train/chosen': '-178.09', 'loss/train': '0.68735', 'examples_per_second': '31.518', 'grad_norm': '27.875', 'counters/examples': 144608, 'counters/updates': 4519}
train stats after 144640 examples: {'rewards_train/chosen': '0.12447', 'rewards_train/rejected': '0.056857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067612', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-126.48', 'loss/train': '0.66565', 'examples_per_second': '31.164', 'grad_norm': '23.125', 'counters/examples': 144640, 'counters/updates': 4520}
skipping logging after 144672 examples to avoid logging too frequently
train stats after 144704 examples: {'rewards_train/chosen': '0.13467', 'rewards_train/rejected': '0.040709', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.093961', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-159.77', 'loss/train': '0.65556', 'examples_per_second': '31.262', 'grad_norm': '33.5', 'counters/examples': 144704, 'counters/updates': 4522}
train stats after 144736 examples: {'rewards_train/chosen': '0.046109', 'rewards_train/rejected': '0.048024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0019147', 'logps_train/rejected': '-106.29', 'logps_train/chosen': '-132.22', 'loss/train': '0.70369', 'examples_per_second': '32.118', 'grad_norm': '26.75', 'counters/examples': 144736, 'counters/updates': 4523}
train stats after 144768 examples: {'rewards_train/chosen': '0.088869', 'rewards_train/rejected': '0.066769', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0221', 'logps_train/rejected': '-149.46', 'logps_train/chosen': '-110.09', 'loss/train': '0.68846', 'examples_per_second': '31.401', 'grad_norm': '24.875', 'counters/examples': 144768, 'counters/updates': 4524}
train stats after 144800 examples: {'rewards_train/chosen': '0.056302', 'rewards_train/rejected': '0.030713', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02559', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-127.56', 'loss/train': '0.69431', 'examples_per_second': '31.157', 'grad_norm': '27.25', 'counters/examples': 144800, 'counters/updates': 4525}
train stats after 144832 examples: {'rewards_train/chosen': '0.12774', 'rewards_train/rejected': '0.013813', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11393', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-185.83', 'loss/train': '0.64987', 'examples_per_second': '30.625', 'grad_norm': '26.875', 'counters/examples': 144832, 'counters/updates': 4526}
train stats after 144864 examples: {'rewards_train/chosen': '0.10219', 'rewards_train/rejected': '0.077451', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024742', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-156.86', 'loss/train': '0.68729', 'examples_per_second': '29.722', 'grad_norm': '25.375', 'counters/examples': 144864, 'counters/updates': 4527}
train stats after 144896 examples: {'rewards_train/chosen': '0.13348', 'rewards_train/rejected': '0.084605', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04888', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-140.46', 'loss/train': '0.68592', 'examples_per_second': '30.072', 'grad_norm': '32', 'counters/examples': 144896, 'counters/updates': 4528}
train stats after 144928 examples: {'rewards_train/chosen': '0.094022', 'rewards_train/rejected': '-0.0010541', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095077', 'logps_train/rejected': '-112.82', 'logps_train/chosen': '-137.04', 'loss/train': '0.65553', 'examples_per_second': '31.36', 'grad_norm': '25.25', 'counters/examples': 144928, 'counters/updates': 4529}
train stats after 144960 examples: {'rewards_train/chosen': '0.16512', 'rewards_train/rejected': '0.025801', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13931', 'logps_train/rejected': '-107.32', 'logps_train/chosen': '-165.23', 'loss/train': '0.6352', 'examples_per_second': '31.49', 'grad_norm': '28.125', 'counters/examples': 144960, 'counters/updates': 4530}
train stats after 144992 examples: {'rewards_train/chosen': '0.094401', 'rewards_train/rejected': '0.048115', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046286', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-156.02', 'loss/train': '0.6767', 'examples_per_second': '31.405', 'grad_norm': '28.375', 'counters/examples': 144992, 'counters/updates': 4531}
train stats after 145024 examples: {'rewards_train/chosen': '0.0902', 'rewards_train/rejected': '0.090836', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0006356', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-143.45', 'loss/train': '0.7053', 'examples_per_second': '31.561', 'grad_norm': '28.25', 'counters/examples': 145024, 'counters/updates': 4532}
train stats after 145056 examples: {'rewards_train/chosen': '0.05693', 'rewards_train/rejected': '0.02544', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031491', 'logps_train/rejected': '-128.47', 'logps_train/chosen': '-123.01', 'loss/train': '0.68327', 'examples_per_second': '30.567', 'grad_norm': '28.875', 'counters/examples': 145056, 'counters/updates': 4533}
skipping logging after 145088 examples to avoid logging too frequently
train stats after 145120 examples: {'rewards_train/chosen': '0.11222', 'rewards_train/rejected': '0.11686', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0046434', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-124.81', 'loss/train': '0.70367', 'examples_per_second': '31.543', 'grad_norm': '32.75', 'counters/examples': 145120, 'counters/updates': 4535}
train stats after 145152 examples: {'rewards_train/chosen': '0.13175', 'rewards_train/rejected': '0.048703', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083044', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-148.81', 'loss/train': '0.65818', 'examples_per_second': '32.891', 'grad_norm': '25.75', 'counters/examples': 145152, 'counters/updates': 4536}
skipping logging after 145184 examples to avoid logging too frequently
train stats after 145216 examples: {'rewards_train/chosen': '0.1978', 'rewards_train/rejected': '-0.0096827', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20748', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-143.76', 'loss/train': '0.60563', 'examples_per_second': '33.569', 'grad_norm': '25.25', 'counters/examples': 145216, 'counters/updates': 4538}
skipping logging after 145248 examples to avoid logging too frequently
train stats after 145280 examples: {'rewards_train/chosen': '0.021112', 'rewards_train/rejected': '0.0010247', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020087', 'logps_train/rejected': '-119.95', 'logps_train/chosen': '-134.3', 'loss/train': '0.69134', 'examples_per_second': '32.916', 'grad_norm': '26.5', 'counters/examples': 145280, 'counters/updates': 4540}
train stats after 145312 examples: {'rewards_train/chosen': '0.08318', 'rewards_train/rejected': '0.070563', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012617', 'logps_train/rejected': '-123.69', 'logps_train/chosen': '-139.7', 'loss/train': '0.70503', 'examples_per_second': '31.582', 'grad_norm': '25.75', 'counters/examples': 145312, 'counters/updates': 4541}
skipping logging after 145344 examples to avoid logging too frequently
train stats after 145376 examples: {'rewards_train/chosen': '0.037161', 'rewards_train/rejected': '0.0088632', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028297', 'logps_train/rejected': '-143.43', 'logps_train/chosen': '-123.35', 'loss/train': '0.6885', 'examples_per_second': '31.727', 'grad_norm': '26.5', 'counters/examples': 145376, 'counters/updates': 4543}
train stats after 145408 examples: {'rewards_train/chosen': '0.13161', 'rewards_train/rejected': '-0.032669', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16428', 'logps_train/rejected': '-144', 'logps_train/chosen': '-167.93', 'loss/train': '0.62618', 'examples_per_second': '31.583', 'grad_norm': '26.75', 'counters/examples': 145408, 'counters/updates': 4544}
train stats after 145440 examples: {'rewards_train/chosen': '0.1037', 'rewards_train/rejected': '0.045388', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058308', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-130.88', 'loss/train': '0.67729', 'examples_per_second': '32.94', 'grad_norm': '28.5', 'counters/examples': 145440, 'counters/updates': 4545}
train stats after 145472 examples: {'rewards_train/chosen': '0.10223', 'rewards_train/rejected': '0.14902', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046788', 'logps_train/rejected': '-113.07', 'logps_train/chosen': '-136.16', 'loss/train': '0.72865', 'examples_per_second': '30.91', 'grad_norm': '29.5', 'counters/examples': 145472, 'counters/updates': 4546}
train stats after 145504 examples: {'rewards_train/chosen': '0.10626', 'rewards_train/rejected': '-0.039263', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14552', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-155.85', 'loss/train': '0.63261', 'examples_per_second': '30.761', 'grad_norm': '26.25', 'counters/examples': 145504, 'counters/updates': 4547}
train stats after 145536 examples: {'rewards_train/chosen': '0.083313', 'rewards_train/rejected': '0.039631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043682', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-162.66', 'loss/train': '0.68322', 'examples_per_second': '31.555', 'grad_norm': '26', 'counters/examples': 145536, 'counters/updates': 4548}
train stats after 145568 examples: {'rewards_train/chosen': '0.094476', 'rewards_train/rejected': '0.0339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060577', 'logps_train/rejected': '-111.58', 'logps_train/chosen': '-130.46', 'loss/train': '0.67198', 'examples_per_second': '31.543', 'grad_norm': '23.625', 'counters/examples': 145568, 'counters/updates': 4549}
train stats after 145600 examples: {'rewards_train/chosen': '0.054123', 'rewards_train/rejected': '0.057763', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0036403', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-138.13', 'loss/train': '0.70572', 'examples_per_second': '31.192', 'grad_norm': '27.375', 'counters/examples': 145600, 'counters/updates': 4550}
train stats after 145632 examples: {'rewards_train/chosen': '0.10079', 'rewards_train/rejected': '0.070934', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029851', 'logps_train/rejected': '-145.57', 'logps_train/chosen': '-129.18', 'loss/train': '0.68626', 'examples_per_second': '31.545', 'grad_norm': '27.25', 'counters/examples': 145632, 'counters/updates': 4551}
skipping logging after 145664 examples to avoid logging too frequently
train stats after 145696 examples: {'rewards_train/chosen': '0.067248', 'rewards_train/rejected': '0.015786', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051463', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-138.4', 'loss/train': '0.68452', 'examples_per_second': '32.445', 'grad_norm': '26.875', 'counters/examples': 145696, 'counters/updates': 4553}
train stats after 145728 examples: {'rewards_train/chosen': '0.14607', 'rewards_train/rejected': '0.046067', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099999', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-141.97', 'loss/train': '0.6535', 'examples_per_second': '30.385', 'grad_norm': '27', 'counters/examples': 145728, 'counters/updates': 4554}
skipping logging after 145760 examples to avoid logging too frequently
train stats after 145792 examples: {'rewards_train/chosen': '0.15216', 'rewards_train/rejected': '0.011518', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14064', 'logps_train/rejected': '-146.3', 'logps_train/chosen': '-142.77', 'loss/train': '0.63604', 'examples_per_second': '33.107', 'grad_norm': '25.125', 'counters/examples': 145792, 'counters/updates': 4556}
train stats after 145824 examples: {'rewards_train/chosen': '0.10702', 'rewards_train/rejected': '-0.0086149', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11563', 'logps_train/rejected': '-94.717', 'logps_train/chosen': '-116.92', 'loss/train': '0.64323', 'examples_per_second': '31.476', 'grad_norm': '22', 'counters/examples': 145824, 'counters/updates': 4557}
train stats after 145856 examples: {'rewards_train/chosen': '0.084851', 'rewards_train/rejected': '0.019242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065609', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-111.95', 'loss/train': '0.6703', 'examples_per_second': '30.603', 'grad_norm': '39.25', 'counters/examples': 145856, 'counters/updates': 4558}
train stats after 145888 examples: {'rewards_train/chosen': '0.12738', 'rewards_train/rejected': '-0.0084416', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13582', 'logps_train/rejected': '-86.632', 'logps_train/chosen': '-140.34', 'loss/train': '0.63709', 'examples_per_second': '31.019', 'grad_norm': '28.375', 'counters/examples': 145888, 'counters/updates': 4559}
train stats after 145920 examples: {'rewards_train/chosen': '0.1515', 'rewards_train/rejected': '0.02928', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12222', 'logps_train/rejected': '-91.812', 'logps_train/chosen': '-141.71', 'loss/train': '0.64848', 'examples_per_second': '31.601', 'grad_norm': '21.875', 'counters/examples': 145920, 'counters/updates': 4560}
train stats after 145952 examples: {'rewards_train/chosen': '0.19232', 'rewards_train/rejected': '0.067544', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12477', 'logps_train/rejected': '-137.51', 'logps_train/chosen': '-150.04', 'loss/train': '0.64504', 'examples_per_second': '30.624', 'grad_norm': '25.625', 'counters/examples': 145952, 'counters/updates': 4561}
skipping logging after 145984 examples to avoid logging too frequently
train stats after 146016 examples: {'rewards_train/chosen': '0.092742', 'rewards_train/rejected': '-0.041621', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13436', 'logps_train/rejected': '-78.324', 'logps_train/chosen': '-125.03', 'loss/train': '0.63878', 'examples_per_second': '34.064', 'grad_norm': '23.75', 'counters/examples': 146016, 'counters/updates': 4563}
train stats after 146048 examples: {'rewards_train/chosen': '0.16987', 'rewards_train/rejected': '0.053676', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1162', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-155.6', 'loss/train': '0.65413', 'examples_per_second': '30.047', 'grad_norm': '28.25', 'counters/examples': 146048, 'counters/updates': 4564}
train stats after 146080 examples: {'rewards_train/chosen': '0.063562', 'rewards_train/rejected': '-0.064048', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12761', 'logps_train/rejected': '-100.53', 'logps_train/chosen': '-86.561', 'loss/train': '0.63669', 'examples_per_second': '31.878', 'grad_norm': '20.75', 'counters/examples': 146080, 'counters/updates': 4565}
train stats after 146112 examples: {'rewards_train/chosen': '0.11962', 'rewards_train/rejected': '0.028562', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091056', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-152.53', 'loss/train': '0.65809', 'examples_per_second': '32.21', 'grad_norm': '26', 'counters/examples': 146112, 'counters/updates': 4566}
train stats after 146144 examples: {'rewards_train/chosen': '0.082599', 'rewards_train/rejected': '0.024889', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05771', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-188.91', 'loss/train': '0.67439', 'examples_per_second': '31.513', 'grad_norm': '31.5', 'counters/examples': 146144, 'counters/updates': 4567}
train stats after 146176 examples: {'rewards_train/chosen': '0.17659', 'rewards_train/rejected': '0.025059', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15153', 'logps_train/rejected': '-138.23', 'logps_train/chosen': '-162.83', 'loss/train': '0.63031', 'examples_per_second': '31.565', 'grad_norm': '25.5', 'counters/examples': 146176, 'counters/updates': 4568}
train stats after 146208 examples: {'rewards_train/chosen': '0.14849', 'rewards_train/rejected': '0.036132', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11236', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-137.1', 'loss/train': '0.65238', 'examples_per_second': '31.268', 'grad_norm': '22', 'counters/examples': 146208, 'counters/updates': 4569}
skipping logging after 146240 examples to avoid logging too frequently
train stats after 146272 examples: {'rewards_train/chosen': '0.12639', 'rewards_train/rejected': '0.021118', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10527', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-140.21', 'loss/train': '0.64806', 'examples_per_second': '31.526', 'grad_norm': '25.25', 'counters/examples': 146272, 'counters/updates': 4571}
train stats after 146304 examples: {'rewards_train/chosen': '0.12248', 'rewards_train/rejected': '0.085207', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037275', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-152.34', 'loss/train': '0.68396', 'examples_per_second': '32.955', 'grad_norm': '28.875', 'counters/examples': 146304, 'counters/updates': 4572}
skipping logging after 146336 examples to avoid logging too frequently
train stats after 146368 examples: {'rewards_train/chosen': '0.065638', 'rewards_train/rejected': '0.079595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013957', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-156.66', 'loss/train': '0.71366', 'examples_per_second': '30.189', 'grad_norm': '30.625', 'counters/examples': 146368, 'counters/updates': 4574}
train stats after 146400 examples: {'rewards_train/chosen': '0.10485', 'rewards_train/rejected': '0.03825', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066604', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-107.97', 'loss/train': '0.67221', 'examples_per_second': '31.115', 'grad_norm': '28.5', 'counters/examples': 146400, 'counters/updates': 4575}
train stats after 146432 examples: {'rewards_train/chosen': '0.030227', 'rewards_train/rejected': '0.019052', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011175', 'logps_train/rejected': '-121.99', 'logps_train/chosen': '-150.68', 'loss/train': '0.69496', 'examples_per_second': '33.3', 'grad_norm': '28.75', 'counters/examples': 146432, 'counters/updates': 4576}
train stats after 146464 examples: {'rewards_train/chosen': '0.070441', 'rewards_train/rejected': '-0.030233', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10067', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-162.63', 'loss/train': '0.65517', 'examples_per_second': '24.974', 'grad_norm': '28.625', 'counters/examples': 146464, 'counters/updates': 4577}
train stats after 146496 examples: {'rewards_train/chosen': '0.068015', 'rewards_train/rejected': '0.068921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00090629', 'logps_train/rejected': '-146.74', 'logps_train/chosen': '-172.1', 'loss/train': '0.70107', 'examples_per_second': '32.879', 'grad_norm': '35.5', 'counters/examples': 146496, 'counters/updates': 4578}
train stats after 146528 examples: {'rewards_train/chosen': '0.048553', 'rewards_train/rejected': '0.013922', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034631', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-146.38', 'loss/train': '0.68331', 'examples_per_second': '32.919', 'grad_norm': '28', 'counters/examples': 146528, 'counters/updates': 4579}
train stats after 146560 examples: {'rewards_train/chosen': '0.1014', 'rewards_train/rejected': '0.045565', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055831', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-144.46', 'loss/train': '0.67574', 'examples_per_second': '23.359', 'grad_norm': '27.875', 'counters/examples': 146560, 'counters/updates': 4580}
train stats after 146592 examples: {'rewards_train/chosen': '0.0399', 'rewards_train/rejected': '0.010349', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029551', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-97.735', 'loss/train': '0.68903', 'examples_per_second': '30.044', 'grad_norm': '23.625', 'counters/examples': 146592, 'counters/updates': 4581}
skipping logging after 146624 examples to avoid logging too frequently
train stats after 146656 examples: {'rewards_train/chosen': '0.11751', 'rewards_train/rejected': '0.055262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06225', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-111.79', 'loss/train': '0.66917', 'examples_per_second': '32.324', 'grad_norm': '21.625', 'counters/examples': 146656, 'counters/updates': 4583}
train stats after 146688 examples: {'rewards_train/chosen': '0.12692', 'rewards_train/rejected': '0.034391', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092532', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-131.83', 'loss/train': '0.66209', 'examples_per_second': '30.613', 'grad_norm': '24.375', 'counters/examples': 146688, 'counters/updates': 4584}
train stats after 146720 examples: {'rewards_train/chosen': '0.10056', 'rewards_train/rejected': '0.12682', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026256', 'logps_train/rejected': '-164.69', 'logps_train/chosen': '-150.41', 'loss/train': '0.71483', 'examples_per_second': '31.556', 'grad_norm': '31.875', 'counters/examples': 146720, 'counters/updates': 4585}
train stats after 146752 examples: {'rewards_train/chosen': '0.11946', 'rewards_train/rejected': '0.093058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026397', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-140.26', 'loss/train': '0.69357', 'examples_per_second': '30.419', 'grad_norm': '27.75', 'counters/examples': 146752, 'counters/updates': 4586}
skipping logging after 146784 examples to avoid logging too frequently
train stats after 146816 examples: {'rewards_train/chosen': '0.13758', 'rewards_train/rejected': '0.015281', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1223', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-125.78', 'loss/train': '0.64311', 'examples_per_second': '31.865', 'grad_norm': '23.25', 'counters/examples': 146816, 'counters/updates': 4588}
skipping logging after 146848 examples to avoid logging too frequently
train stats after 146880 examples: {'rewards_train/chosen': '0.032488', 'rewards_train/rejected': '0.035105', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0026174', 'logps_train/rejected': '-162', 'logps_train/chosen': '-145.86', 'loss/train': '0.704', 'examples_per_second': '30.953', 'grad_norm': '29.5', 'counters/examples': 146880, 'counters/updates': 4590}
train stats after 146912 examples: {'rewards_train/chosen': '0.054077', 'rewards_train/rejected': '0.08508', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031003', 'logps_train/rejected': '-162.92', 'logps_train/chosen': '-141.9', 'loss/train': '0.71674', 'examples_per_second': '31.347', 'grad_norm': '35.75', 'counters/examples': 146912, 'counters/updates': 4591}
skipping logging after 146944 examples to avoid logging too frequently
train stats after 146976 examples: {'rewards_train/chosen': '0.077761', 'rewards_train/rejected': '0.025819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051941', 'logps_train/rejected': '-74.055', 'logps_train/chosen': '-90.761', 'loss/train': '0.67509', 'examples_per_second': '33.388', 'grad_norm': '19.75', 'counters/examples': 146976, 'counters/updates': 4593}
train stats after 147008 examples: {'rewards_train/chosen': '0.092772', 'rewards_train/rejected': '-0.052708', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14548', 'logps_train/rejected': '-156.2', 'logps_train/chosen': '-171.08', 'loss/train': '0.63704', 'examples_per_second': '31.134', 'grad_norm': '30.375', 'counters/examples': 147008, 'counters/updates': 4594}
train stats after 147040 examples: {'rewards_train/chosen': '0.10594', 'rewards_train/rejected': '0.013889', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092048', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-127.07', 'loss/train': '0.65715', 'examples_per_second': '30.794', 'grad_norm': '29.25', 'counters/examples': 147040, 'counters/updates': 4595}
train stats after 147072 examples: {'rewards_train/chosen': '0.22263', 'rewards_train/rejected': '0.0042299', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2184', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-149.21', 'loss/train': '0.60363', 'examples_per_second': '31.37', 'grad_norm': '23.5', 'counters/examples': 147072, 'counters/updates': 4596}
train stats after 147104 examples: {'rewards_train/chosen': '0.14147', 'rewards_train/rejected': '0.028198', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11327', 'logps_train/rejected': '-117.34', 'logps_train/chosen': '-133.06', 'loss/train': '0.64965', 'examples_per_second': '31.175', 'grad_norm': '22.375', 'counters/examples': 147104, 'counters/updates': 4597}
train stats after 147136 examples: {'rewards_train/chosen': '0.062955', 'rewards_train/rejected': '0.024425', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03853', 'logps_train/rejected': '-134', 'logps_train/chosen': '-151.71', 'loss/train': '0.68088', 'examples_per_second': '32.41', 'grad_norm': '29.25', 'counters/examples': 147136, 'counters/updates': 4598}
train stats after 147168 examples: {'rewards_train/chosen': '0.11815', 'rewards_train/rejected': '0.1037', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014441', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-121.84', 'loss/train': '0.69271', 'examples_per_second': '30.788', 'grad_norm': '24.875', 'counters/examples': 147168, 'counters/updates': 4599}
train stats after 147200 examples: {'rewards_train/chosen': '0.065749', 'rewards_train/rejected': '0.0074236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058325', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-137.49', 'loss/train': '0.67607', 'examples_per_second': '30.055', 'grad_norm': '26.625', 'counters/examples': 147200, 'counters/updates': 4600}
train stats after 147232 examples: {'rewards_train/chosen': '0.18595', 'rewards_train/rejected': '0.07129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11466', 'logps_train/rejected': '-114.45', 'logps_train/chosen': '-169.83', 'loss/train': '0.65216', 'examples_per_second': '31.489', 'grad_norm': '24.5', 'counters/examples': 147232, 'counters/updates': 4601}
train stats after 147264 examples: {'rewards_train/chosen': '0.1688', 'rewards_train/rejected': '0.00026871', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16853', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-134.72', 'loss/train': '0.62508', 'examples_per_second': '32.897', 'grad_norm': '23.75', 'counters/examples': 147264, 'counters/updates': 4602}
train stats after 147296 examples: {'rewards_train/chosen': '0.07224', 'rewards_train/rejected': '0.0091659', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063074', 'logps_train/rejected': '-82.683', 'logps_train/chosen': '-144.44', 'loss/train': '0.67447', 'examples_per_second': '32.62', 'grad_norm': '25.375', 'counters/examples': 147296, 'counters/updates': 4603}
train stats after 147328 examples: {'rewards_train/chosen': '0.082652', 'rewards_train/rejected': '-0.031083', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11373', 'logps_train/rejected': '-107.68', 'logps_train/chosen': '-126.01', 'loss/train': '0.64731', 'examples_per_second': '31.354', 'grad_norm': '24.375', 'counters/examples': 147328, 'counters/updates': 4604}
train stats after 147360 examples: {'rewards_train/chosen': '0.17586', 'rewards_train/rejected': '0.072824', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10304', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-155.38', 'loss/train': '0.65131', 'examples_per_second': '31.453', 'grad_norm': '28.625', 'counters/examples': 147360, 'counters/updates': 4605}
skipping logging after 147392 examples to avoid logging too frequently
train stats after 147424 examples: {'rewards_train/chosen': '0.19267', 'rewards_train/rejected': '0.097642', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095031', 'logps_train/rejected': '-213.36', 'logps_train/chosen': '-175.59', 'loss/train': '0.65785', 'examples_per_second': '31.254', 'grad_norm': '31.875', 'counters/examples': 147424, 'counters/updates': 4607}
train stats after 147456 examples: {'rewards_train/chosen': '0.18271', 'rewards_train/rejected': '0.098117', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084588', 'logps_train/rejected': '-178.06', 'logps_train/chosen': '-183.87', 'loss/train': '0.66222', 'examples_per_second': '31.522', 'grad_norm': '32.75', 'counters/examples': 147456, 'counters/updates': 4608}
train stats after 147488 examples: {'rewards_train/chosen': '0.052177', 'rewards_train/rejected': '-0.054036', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10621', 'logps_train/rejected': '-81.965', 'logps_train/chosen': '-93.151', 'loss/train': '0.65186', 'examples_per_second': '30.626', 'grad_norm': '21.625', 'counters/examples': 147488, 'counters/updates': 4609}
train stats after 147520 examples: {'rewards_train/chosen': '0.18849', 'rewards_train/rejected': '-0.0091925', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19768', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-135.74', 'loss/train': '0.6156', 'examples_per_second': '30.315', 'grad_norm': '24.75', 'counters/examples': 147520, 'counters/updates': 4610}
train stats after 147552 examples: {'rewards_train/chosen': '0.068869', 'rewards_train/rejected': '0.0075264', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061343', 'logps_train/rejected': '-101.27', 'logps_train/chosen': '-174.35', 'loss/train': '0.67558', 'examples_per_second': '33.256', 'grad_norm': '29.375', 'counters/examples': 147552, 'counters/updates': 4611}
train stats after 147584 examples: {'rewards_train/chosen': '0.1222', 'rewards_train/rejected': '0.07126', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.050944', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-164.66', 'loss/train': '0.67671', 'examples_per_second': '31.572', 'grad_norm': '26.375', 'counters/examples': 147584, 'counters/updates': 4612}
train stats after 147616 examples: {'rewards_train/chosen': '0.2001', 'rewards_train/rejected': '0.1513', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048794', 'logps_train/rejected': '-189.31', 'logps_train/chosen': '-160.85', 'loss/train': '0.68085', 'examples_per_second': '31.504', 'grad_norm': '34.25', 'counters/examples': 147616, 'counters/updates': 4613}
skipping logging after 147648 examples to avoid logging too frequently
train stats after 147680 examples: {'rewards_train/chosen': '0.10933', 'rewards_train/rejected': '0.013956', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095376', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-171.41', 'loss/train': '0.65524', 'examples_per_second': '36.14', 'grad_norm': '28.25', 'counters/examples': 147680, 'counters/updates': 4615}
train stats after 147712 examples: {'rewards_train/chosen': '0.040616', 'rewards_train/rejected': '-0.01603', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.056646', 'logps_train/rejected': '-104.5', 'logps_train/chosen': '-185.73', 'loss/train': '0.69991', 'examples_per_second': '30.832', 'grad_norm': '288', 'counters/examples': 147712, 'counters/updates': 4616}
train stats after 147744 examples: {'rewards_train/chosen': '0.20612', 'rewards_train/rejected': '0.032511', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17361', 'logps_train/rejected': '-163.17', 'logps_train/chosen': '-187.77', 'loss/train': '0.62649', 'examples_per_second': '31.538', 'grad_norm': '31', 'counters/examples': 147744, 'counters/updates': 4617}
train stats after 147776 examples: {'rewards_train/chosen': '0.077832', 'rewards_train/rejected': '0.06123', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016602', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-135.58', 'loss/train': '0.68937', 'examples_per_second': '31.146', 'grad_norm': '28', 'counters/examples': 147776, 'counters/updates': 4618}
train stats after 147808 examples: {'rewards_train/chosen': '0.19055', 'rewards_train/rejected': '0.091455', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099099', 'logps_train/rejected': '-119.52', 'logps_train/chosen': '-154.65', 'loss/train': '0.65463', 'examples_per_second': '32.394', 'grad_norm': '31.25', 'counters/examples': 147808, 'counters/updates': 4619}
train stats after 147840 examples: {'rewards_train/chosen': '0.18517', 'rewards_train/rejected': '0.024276', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16089', 'logps_train/rejected': '-121.41', 'logps_train/chosen': '-157.55', 'loss/train': '0.62449', 'examples_per_second': '31.511', 'grad_norm': '24.625', 'counters/examples': 147840, 'counters/updates': 4620}
train stats after 147872 examples: {'rewards_train/chosen': '0.13401', 'rewards_train/rejected': '0.032882', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10113', 'logps_train/rejected': '-141.84', 'logps_train/chosen': '-140.11', 'loss/train': '0.65948', 'examples_per_second': '29.863', 'grad_norm': '28', 'counters/examples': 147872, 'counters/updates': 4621}
train stats after 147904 examples: {'rewards_train/chosen': '0.13719', 'rewards_train/rejected': '-0.0055646', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14276', 'logps_train/rejected': '-98.62', 'logps_train/chosen': '-120.78', 'loss/train': '0.63436', 'examples_per_second': '31.486', 'grad_norm': '24.5', 'counters/examples': 147904, 'counters/updates': 4622}
train stats after 147936 examples: {'rewards_train/chosen': '0.13817', 'rewards_train/rejected': '0.073607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064558', 'logps_train/rejected': '-117.38', 'logps_train/chosen': '-134.59', 'loss/train': '0.67461', 'examples_per_second': '31.26', 'grad_norm': '25.875', 'counters/examples': 147936, 'counters/updates': 4623}
train stats after 147968 examples: {'rewards_train/chosen': '0.041871', 'rewards_train/rejected': '-0.014937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056808', 'logps_train/rejected': '-121.07', 'logps_train/chosen': '-118.16', 'loss/train': '0.67034', 'examples_per_second': '32.285', 'grad_norm': '25.25', 'counters/examples': 147968, 'counters/updates': 4624}
train stats after 148000 examples: {'rewards_train/chosen': '0.14607', 'rewards_train/rejected': '0.049583', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09649', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-173.5', 'loss/train': '0.6551', 'examples_per_second': '31.478', 'grad_norm': '28', 'counters/examples': 148000, 'counters/updates': 4625}
train stats after 148032 examples: {'rewards_train/chosen': '0.15695', 'rewards_train/rejected': '0.02263', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13432', 'logps_train/rejected': '-119.95', 'logps_train/chosen': '-143.95', 'loss/train': '0.6384', 'examples_per_second': '32.419', 'grad_norm': '27.75', 'counters/examples': 148032, 'counters/updates': 4626}
train stats after 148064 examples: {'rewards_train/chosen': '0.12989', 'rewards_train/rejected': '0.084606', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04528', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-128.83', 'loss/train': '0.6908', 'examples_per_second': '31.612', 'grad_norm': '30.875', 'counters/examples': 148064, 'counters/updates': 4627}
skipping logging after 148096 examples to avoid logging too frequently
train stats after 148128 examples: {'rewards_train/chosen': '0.059921', 'rewards_train/rejected': '0.048077', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011844', 'logps_train/rejected': '-132.38', 'logps_train/chosen': '-155.67', 'loss/train': '0.69754', 'examples_per_second': '31.677', 'grad_norm': '29', 'counters/examples': 148128, 'counters/updates': 4629}
train stats after 148160 examples: {'rewards_train/chosen': '0.081762', 'rewards_train/rejected': '0.020102', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.06166', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-137.56', 'loss/train': '0.67457', 'examples_per_second': '32.34', 'grad_norm': '28.75', 'counters/examples': 148160, 'counters/updates': 4630}
skipping logging after 148192 examples to avoid logging too frequently
train stats after 148224 examples: {'rewards_train/chosen': '0.10349', 'rewards_train/rejected': '0.086829', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016661', 'logps_train/rejected': '-108.96', 'logps_train/chosen': '-113.02', 'loss/train': '0.69183', 'examples_per_second': '33.215', 'grad_norm': '24', 'counters/examples': 148224, 'counters/updates': 4632}
train stats after 148256 examples: {'rewards_train/chosen': '0.12403', 'rewards_train/rejected': '0.090831', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033202', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-123.88', 'loss/train': '0.68226', 'examples_per_second': '30.364', 'grad_norm': '25.125', 'counters/examples': 148256, 'counters/updates': 4633}
skipping logging after 148288 examples to avoid logging too frequently
train stats after 148320 examples: {'rewards_train/chosen': '0.17657', 'rewards_train/rejected': '0.1248', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051763', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-131.65', 'loss/train': '0.67149', 'examples_per_second': '31.139', 'grad_norm': '23.25', 'counters/examples': 148320, 'counters/updates': 4635}
skipping logging after 148352 examples to avoid logging too frequently
train stats after 148384 examples: {'rewards_train/chosen': '0.12345', 'rewards_train/rejected': '0.071992', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05146', 'logps_train/rejected': '-144.56', 'logps_train/chosen': '-127.75', 'loss/train': '0.67144', 'examples_per_second': '31.461', 'grad_norm': '26', 'counters/examples': 148384, 'counters/updates': 4637}
train stats after 148416 examples: {'rewards_train/chosen': '0.05785', 'rewards_train/rejected': '0.056051', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0017991', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-146.1', 'loss/train': '0.70553', 'examples_per_second': '30.442', 'grad_norm': '27.875', 'counters/examples': 148416, 'counters/updates': 4638}
train stats after 148448 examples: {'rewards_train/chosen': '0.012702', 'rewards_train/rejected': '-0.046133', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058835', 'logps_train/rejected': '-118.75', 'logps_train/chosen': '-109.22', 'loss/train': '0.67297', 'examples_per_second': '33.088', 'grad_norm': '25.875', 'counters/examples': 148448, 'counters/updates': 4639}
skipping logging after 148480 examples to avoid logging too frequently
train stats after 148512 examples: {'rewards_train/chosen': '0.098765', 'rewards_train/rejected': '0.079568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019197', 'logps_train/rejected': '-145.89', 'logps_train/chosen': '-180.77', 'loss/train': '0.69409', 'examples_per_second': '31.12', 'grad_norm': '32', 'counters/examples': 148512, 'counters/updates': 4641}
train stats after 148544 examples: {'rewards_train/chosen': '0.053582', 'rewards_train/rejected': '0.008694', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044888', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-135.76', 'loss/train': '0.67899', 'examples_per_second': '32.47', 'grad_norm': '25.875', 'counters/examples': 148544, 'counters/updates': 4642}
train stats after 148576 examples: {'rewards_train/chosen': '0.20515', 'rewards_train/rejected': '0.055431', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14972', 'logps_train/rejected': '-105.22', 'logps_train/chosen': '-138.32', 'loss/train': '0.63184', 'examples_per_second': '30.426', 'grad_norm': '26', 'counters/examples': 148576, 'counters/updates': 4643}
train stats after 148608 examples: {'rewards_train/chosen': '0.091035', 'rewards_train/rejected': '-0.0099979', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10103', 'logps_train/rejected': '-124.23', 'logps_train/chosen': '-154.01', 'loss/train': '0.65413', 'examples_per_second': '31.565', 'grad_norm': '25.625', 'counters/examples': 148608, 'counters/updates': 4644}
train stats after 148640 examples: {'rewards_train/chosen': '0.081325', 'rewards_train/rejected': '0.021654', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05967', 'logps_train/rejected': '-98.791', 'logps_train/chosen': '-137.71', 'loss/train': '0.67146', 'examples_per_second': '31.716', 'grad_norm': '23.25', 'counters/examples': 148640, 'counters/updates': 4645}
train stats after 148672 examples: {'rewards_train/chosen': '0.19586', 'rewards_train/rejected': '0.058229', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13763', 'logps_train/rejected': '-146.19', 'logps_train/chosen': '-138.28', 'loss/train': '0.63882', 'examples_per_second': '32.372', 'grad_norm': '31.375', 'counters/examples': 148672, 'counters/updates': 4646}
skipping logging after 148704 examples to avoid logging too frequently
train stats after 148736 examples: {'rewards_train/chosen': '0.12907', 'rewards_train/rejected': '0.047323', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08175', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-142.44', 'loss/train': '0.66294', 'examples_per_second': '34.176', 'grad_norm': '25.375', 'counters/examples': 148736, 'counters/updates': 4648}
train stats after 148768 examples: {'rewards_train/chosen': '-0.019986', 'rewards_train/rejected': '0.043085', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.063071', 'logps_train/rejected': '-123.63', 'logps_train/chosen': '-136.25', 'loss/train': '0.73439', 'examples_per_second': '32.65', 'grad_norm': '35.5', 'counters/examples': 148768, 'counters/updates': 4649}
train stats after 148800 examples: {'rewards_train/chosen': '0.10842', 'rewards_train/rejected': '-0.00019627', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10861', 'logps_train/rejected': '-98.143', 'logps_train/chosen': '-104.93', 'loss/train': '0.64935', 'examples_per_second': '32.371', 'grad_norm': '23.5', 'counters/examples': 148800, 'counters/updates': 4650}
train stats after 148832 examples: {'rewards_train/chosen': '0.15486', 'rewards_train/rejected': '0.030494', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12437', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-136.47', 'loss/train': '0.64549', 'examples_per_second': '32.07', 'grad_norm': '28.75', 'counters/examples': 148832, 'counters/updates': 4651}
train stats after 148864 examples: {'rewards_train/chosen': '0.15543', 'rewards_train/rejected': '0.060311', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.095117', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-122.39', 'loss/train': '0.65633', 'examples_per_second': '30.688', 'grad_norm': '26', 'counters/examples': 148864, 'counters/updates': 4652}
train stats after 148896 examples: {'rewards_train/chosen': '0.12394', 'rewards_train/rejected': '0.061904', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062039', 'logps_train/rejected': '-135.72', 'logps_train/chosen': '-128.65', 'loss/train': '0.66908', 'examples_per_second': '31.403', 'grad_norm': '26.5', 'counters/examples': 148896, 'counters/updates': 4653}
skipping logging after 148928 examples to avoid logging too frequently
train stats after 148960 examples: {'rewards_train/chosen': '0.097866', 'rewards_train/rejected': '0.11405', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016185', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-144.76', 'loss/train': '0.71414', 'examples_per_second': '30.502', 'grad_norm': '27.125', 'counters/examples': 148960, 'counters/updates': 4655}
train stats after 148992 examples: {'rewards_train/chosen': '0.07132', 'rewards_train/rejected': '0.048178', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023142', 'logps_train/rejected': '-93.951', 'logps_train/chosen': '-151.75', 'loss/train': '0.69229', 'examples_per_second': '30.799', 'grad_norm': '30.5', 'counters/examples': 148992, 'counters/updates': 4656}
train stats after 149024 examples: {'rewards_train/chosen': '0.14658', 'rewards_train/rejected': '7.5895e-05', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1465', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-153.42', 'loss/train': '0.63682', 'examples_per_second': '33.145', 'grad_norm': '25.75', 'counters/examples': 149024, 'counters/updates': 4657}
train stats after 149056 examples: {'rewards_train/chosen': '0.1042', 'rewards_train/rejected': '-0.019561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12377', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-167.7', 'loss/train': '0.64366', 'examples_per_second': '31.065', 'grad_norm': '27.125', 'counters/examples': 149056, 'counters/updates': 4658}
train stats after 149088 examples: {'rewards_train/chosen': '0.16203', 'rewards_train/rejected': '-0.013533', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17557', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-141.42', 'loss/train': '0.62331', 'examples_per_second': '31.271', 'grad_norm': '26.25', 'counters/examples': 149088, 'counters/updates': 4659}
train stats after 149120 examples: {'rewards_train/chosen': '0.080055', 'rewards_train/rejected': '0.049661', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030395', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-145.3', 'loss/train': '0.68648', 'examples_per_second': '31.529', 'grad_norm': '27.75', 'counters/examples': 149120, 'counters/updates': 4660}
train stats after 149152 examples: {'rewards_train/chosen': '0.10441', 'rewards_train/rejected': '0.028126', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076285', 'logps_train/rejected': '-121.21', 'logps_train/chosen': '-157.6', 'loss/train': '0.66896', 'examples_per_second': '32.383', 'grad_norm': '28.5', 'counters/examples': 149152, 'counters/updates': 4661}
skipping logging after 149184 examples to avoid logging too frequently
train stats after 149216 examples: {'rewards_train/chosen': '0.066188', 'rewards_train/rejected': '0.066175', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '1.3308e-05', 'logps_train/rejected': '-102', 'logps_train/chosen': '-119.73', 'loss/train': '0.70596', 'examples_per_second': '23.572', 'grad_norm': '27.5', 'counters/examples': 149216, 'counters/updates': 4663}
train stats after 149248 examples: {'rewards_train/chosen': '0.17793', 'rewards_train/rejected': '0.027121', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15081', 'logps_train/rejected': '-113.02', 'logps_train/chosen': '-149.92', 'loss/train': '0.63872', 'examples_per_second': '32.178', 'grad_norm': '25.75', 'counters/examples': 149248, 'counters/updates': 4664}
train stats after 149280 examples: {'rewards_train/chosen': '0.06509', 'rewards_train/rejected': '-0.071552', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13664', 'logps_train/rejected': '-87.308', 'logps_train/chosen': '-116.19', 'loss/train': '0.63489', 'examples_per_second': '31.732', 'grad_norm': '27.5', 'counters/examples': 149280, 'counters/updates': 4665}
train stats after 149312 examples: {'rewards_train/chosen': '0.11936', 'rewards_train/rejected': '0.016117', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10325', 'logps_train/rejected': '-104.7', 'logps_train/chosen': '-123.61', 'loss/train': '0.65586', 'examples_per_second': '30.963', 'grad_norm': '36.5', 'counters/examples': 149312, 'counters/updates': 4666}
train stats after 149344 examples: {'rewards_train/chosen': '0.039313', 'rewards_train/rejected': '0.03422', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0050931', 'logps_train/rejected': '-107.91', 'logps_train/chosen': '-121.58', 'loss/train': '0.69336', 'examples_per_second': '32.857', 'grad_norm': '23.875', 'counters/examples': 149344, 'counters/updates': 4667}
skipping logging after 149376 examples to avoid logging too frequently
train stats after 149408 examples: {'rewards_train/chosen': '0.15006', 'rewards_train/rejected': '-0.0043388', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15439', 'logps_train/rejected': '-132.86', 'logps_train/chosen': '-139.52', 'loss/train': '0.62847', 'examples_per_second': '30.595', 'grad_norm': '29', 'counters/examples': 149408, 'counters/updates': 4669}
train stats after 149440 examples: {'rewards_train/chosen': '0.13828', 'rewards_train/rejected': '0.064847', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073435', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-124.17', 'loss/train': '0.66602', 'examples_per_second': '32.159', 'grad_norm': '25.75', 'counters/examples': 149440, 'counters/updates': 4670}
train stats after 149472 examples: {'rewards_train/chosen': '0.054148', 'rewards_train/rejected': '0.068365', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014217', 'logps_train/rejected': '-113.39', 'logps_train/chosen': '-124.76', 'loss/train': '0.70641', 'examples_per_second': '29.762', 'grad_norm': '26.625', 'counters/examples': 149472, 'counters/updates': 4671}
train stats after 149504 examples: {'rewards_train/chosen': '0.10529', 'rewards_train/rejected': '0.10886', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0035774', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-129.21', 'loss/train': '0.7033', 'examples_per_second': '31.465', 'grad_norm': '25.625', 'counters/examples': 149504, 'counters/updates': 4672}
train stats after 149536 examples: {'rewards_train/chosen': '0.073369', 'rewards_train/rejected': '0.050335', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023034', 'logps_train/rejected': '-117.85', 'logps_train/chosen': '-128.7', 'loss/train': '0.69506', 'examples_per_second': '31.1', 'grad_norm': '28.5', 'counters/examples': 149536, 'counters/updates': 4673}
train stats after 149568 examples: {'rewards_train/chosen': '0.19829', 'rewards_train/rejected': '0.046426', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15186', 'logps_train/rejected': '-140.27', 'logps_train/chosen': '-159.58', 'loss/train': '0.62449', 'examples_per_second': '31.321', 'grad_norm': '25.625', 'counters/examples': 149568, 'counters/updates': 4674}
train stats after 149600 examples: {'rewards_train/chosen': '0.15892', 'rewards_train/rejected': '0.061356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097561', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-133.51', 'loss/train': '0.65567', 'examples_per_second': '31.264', 'grad_norm': '27.625', 'counters/examples': 149600, 'counters/updates': 4675}
skipping logging after 149632 examples to avoid logging too frequently
train stats after 149664 examples: {'rewards_train/chosen': '0.093431', 'rewards_train/rejected': '0.071152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02228', 'logps_train/rejected': '-161.45', 'logps_train/chosen': '-133.36', 'loss/train': '0.69737', 'examples_per_second': '37.15', 'grad_norm': '30', 'counters/examples': 149664, 'counters/updates': 4677}
train stats after 149696 examples: {'rewards_train/chosen': '0.087176', 'rewards_train/rejected': '-0.0016987', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.088874', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-130.97', 'loss/train': '0.66351', 'examples_per_second': '32.802', 'grad_norm': '25.375', 'counters/examples': 149696, 'counters/updates': 4678}
train stats after 149728 examples: {'rewards_train/chosen': '0.13289', 'rewards_train/rejected': '0.073675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059216', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-135.3', 'loss/train': '0.67571', 'examples_per_second': '32.055', 'grad_norm': '26.25', 'counters/examples': 149728, 'counters/updates': 4679}
train stats after 149760 examples: {'rewards_train/chosen': '0.095682', 'rewards_train/rejected': '0.050706', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044975', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-150.72', 'loss/train': '0.67673', 'examples_per_second': '30.028', 'grad_norm': '33.75', 'counters/examples': 149760, 'counters/updates': 4680}
train stats after 149792 examples: {'rewards_train/chosen': '0.17871', 'rewards_train/rejected': '0.061272', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11744', 'logps_train/rejected': '-100.59', 'logps_train/chosen': '-129.32', 'loss/train': '0.64873', 'examples_per_second': '30.778', 'grad_norm': '24', 'counters/examples': 149792, 'counters/updates': 4681}
train stats after 149824 examples: {'rewards_train/chosen': '0.14816', 'rewards_train/rejected': '0.080677', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067487', 'logps_train/rejected': '-160.32', 'logps_train/chosen': '-164.12', 'loss/train': '0.67057', 'examples_per_second': '32.032', 'grad_norm': '29.125', 'counters/examples': 149824, 'counters/updates': 4682}
train stats after 149856 examples: {'rewards_train/chosen': '0.17449', 'rewards_train/rejected': '0.098153', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076342', 'logps_train/rejected': '-128.29', 'logps_train/chosen': '-164.33', 'loss/train': '0.66276', 'examples_per_second': '30.994', 'grad_norm': '26.625', 'counters/examples': 149856, 'counters/updates': 4683}
train stats after 149888 examples: {'rewards_train/chosen': '0.089737', 'rewards_train/rejected': '0.051634', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038103', 'logps_train/rejected': '-134.71', 'logps_train/chosen': '-151.05', 'loss/train': '0.68621', 'examples_per_second': '32.038', 'grad_norm': '34.5', 'counters/examples': 149888, 'counters/updates': 4684}
skipping logging after 149920 examples to avoid logging too frequently
train stats after 149952 examples: {'rewards_train/chosen': '0.14457', 'rewards_train/rejected': '0.1015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04307', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-123.06', 'loss/train': '0.68169', 'examples_per_second': '33.65', 'grad_norm': '25.25', 'counters/examples': 149952, 'counters/updates': 4686}
train stats after 149984 examples: {'rewards_train/chosen': '0.14961', 'rewards_train/rejected': '-0.025211', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17482', 'logps_train/rejected': '-79.499', 'logps_train/chosen': '-165.01', 'loss/train': '0.62468', 'examples_per_second': '31.107', 'grad_norm': '24.125', 'counters/examples': 149984, 'counters/updates': 4687}
train stats after 150016 examples: {'rewards_train/chosen': '0.14149', 'rewards_train/rejected': '0.087973', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053518', 'logps_train/rejected': '-141.05', 'logps_train/chosen': '-139.3', 'loss/train': '0.67696', 'examples_per_second': '30.567', 'grad_norm': '25.5', 'counters/examples': 150016, 'counters/updates': 4688}
train stats after 150048 examples: {'rewards_train/chosen': '0.12696', 'rewards_train/rejected': '0.031456', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.095501', 'logps_train/rejected': '-119.88', 'logps_train/chosen': '-136.15', 'loss/train': '0.65314', 'examples_per_second': '31.192', 'grad_norm': '24.125', 'counters/examples': 150048, 'counters/updates': 4689}
skipping logging after 150080 examples to avoid logging too frequently
train stats after 150112 examples: {'rewards_train/chosen': '0.061027', 'rewards_train/rejected': '0.014457', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04657', 'logps_train/rejected': '-89.28', 'logps_train/chosen': '-154.21', 'loss/train': '0.6786', 'examples_per_second': '33.151', 'grad_norm': '28.25', 'counters/examples': 150112, 'counters/updates': 4691}
train stats after 150144 examples: {'rewards_train/chosen': '0.097806', 'rewards_train/rejected': '-0.0029964', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1008', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-141.99', 'loss/train': '0.6585', 'examples_per_second': '32.791', 'grad_norm': '25.5', 'counters/examples': 150144, 'counters/updates': 4692}
train stats after 150176 examples: {'rewards_train/chosen': '0.025651', 'rewards_train/rejected': '0.082458', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.056807', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-120.07', 'loss/train': '0.7343', 'examples_per_second': '32.873', 'grad_norm': '26.5', 'counters/examples': 150176, 'counters/updates': 4693}
skipping logging after 150208 examples to avoid logging too frequently
train stats after 150240 examples: {'rewards_train/chosen': '0.17048', 'rewards_train/rejected': '0.041187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12929', 'logps_train/rejected': '-155.86', 'logps_train/chosen': '-150.52', 'loss/train': '0.6428', 'examples_per_second': '31.667', 'grad_norm': '27.625', 'counters/examples': 150240, 'counters/updates': 4695}
train stats after 150272 examples: {'rewards_train/chosen': '0.15056', 'rewards_train/rejected': '0.14405', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.006511', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-125.14', 'loss/train': '0.70427', 'examples_per_second': '30.48', 'grad_norm': '30', 'counters/examples': 150272, 'counters/updates': 4696}
train stats after 150304 examples: {'rewards_train/chosen': '0.19326', 'rewards_train/rejected': '0.041769', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15149', 'logps_train/rejected': '-104.26', 'logps_train/chosen': '-148.13', 'loss/train': '0.62812', 'examples_per_second': '31.583', 'grad_norm': '25.625', 'counters/examples': 150304, 'counters/updates': 4697}
train stats after 150336 examples: {'rewards_train/chosen': '0.11809', 'rewards_train/rejected': '-0.026006', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14409', 'logps_train/rejected': '-108.32', 'logps_train/chosen': '-114.46', 'loss/train': '0.63032', 'examples_per_second': '31.469', 'grad_norm': '21.75', 'counters/examples': 150336, 'counters/updates': 4698}
skipping logging after 150368 examples to avoid logging too frequently
train stats after 150400 examples: {'rewards_train/chosen': '0.090171', 'rewards_train/rejected': '0.040488', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049684', 'logps_train/rejected': '-109.6', 'logps_train/chosen': '-121.15', 'loss/train': '0.67401', 'examples_per_second': '38.697', 'grad_norm': '24', 'counters/examples': 150400, 'counters/updates': 4700}
train stats after 150432 examples: {'rewards_train/chosen': '0.064866', 'rewards_train/rejected': '-0.012562', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077428', 'logps_train/rejected': '-130', 'logps_train/chosen': '-120.76', 'loss/train': '0.66616', 'examples_per_second': '30.929', 'grad_norm': '26.375', 'counters/examples': 150432, 'counters/updates': 4701}
train stats after 150464 examples: {'rewards_train/chosen': '0.14323', 'rewards_train/rejected': '0.047052', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096181', 'logps_train/rejected': '-97.852', 'logps_train/chosen': '-149.61', 'loss/train': '0.65734', 'examples_per_second': '30.276', 'grad_norm': '26.125', 'counters/examples': 150464, 'counters/updates': 4702}
train stats after 150496 examples: {'rewards_train/chosen': '0.097366', 'rewards_train/rejected': '0.057988', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039378', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-127.9', 'loss/train': '0.68679', 'examples_per_second': '31.527', 'grad_norm': '26.625', 'counters/examples': 150496, 'counters/updates': 4703}
skipping logging after 150528 examples to avoid logging too frequently
train stats after 150560 examples: {'rewards_train/chosen': '0.15286', 'rewards_train/rejected': '0.026685', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12617', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-163.64', 'loss/train': '0.64508', 'examples_per_second': '31.514', 'grad_norm': '31.5', 'counters/examples': 150560, 'counters/updates': 4705}
skipping logging after 150592 examples to avoid logging too frequently
train stats after 150624 examples: {'rewards_train/chosen': '0.078291', 'rewards_train/rejected': '0.033828', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044463', 'logps_train/rejected': '-146.48', 'logps_train/chosen': '-131.56', 'loss/train': '0.68164', 'examples_per_second': '31.547', 'grad_norm': '26.5', 'counters/examples': 150624, 'counters/updates': 4707}
train stats after 150656 examples: {'rewards_train/chosen': '0.16283', 'rewards_train/rejected': '0.11181', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051015', 'logps_train/rejected': '-135.26', 'logps_train/chosen': '-167.42', 'loss/train': '0.68695', 'examples_per_second': '31.32', 'grad_norm': '28.625', 'counters/examples': 150656, 'counters/updates': 4708}
train stats after 150688 examples: {'rewards_train/chosen': '0.13351', 'rewards_train/rejected': '-0.0082421', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14175', 'logps_train/rejected': '-95.717', 'logps_train/chosen': '-138.29', 'loss/train': '0.64164', 'examples_per_second': '30.729', 'grad_norm': '23.875', 'counters/examples': 150688, 'counters/updates': 4709}
train stats after 150720 examples: {'rewards_train/chosen': '0.15509', 'rewards_train/rejected': '0.056434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098655', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-158', 'loss/train': '0.65726', 'examples_per_second': '30.891', 'grad_norm': '29.75', 'counters/examples': 150720, 'counters/updates': 4710}
train stats after 150752 examples: {'rewards_train/chosen': '0.20829', 'rewards_train/rejected': '0.039329', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16896', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-151.14', 'loss/train': '0.62644', 'examples_per_second': '31.265', 'grad_norm': '37.25', 'counters/examples': 150752, 'counters/updates': 4711}
train stats after 150784 examples: {'rewards_train/chosen': '0.12653', 'rewards_train/rejected': '0.078416', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048112', 'logps_train/rejected': '-151.19', 'logps_train/chosen': '-144.14', 'loss/train': '0.68049', 'examples_per_second': '31.576', 'grad_norm': '27.75', 'counters/examples': 150784, 'counters/updates': 4712}
train stats after 150816 examples: {'rewards_train/chosen': '0.082041', 'rewards_train/rejected': '0.032739', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049303', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-150.27', 'loss/train': '0.67545', 'examples_per_second': '32.743', 'grad_norm': '26.25', 'counters/examples': 150816, 'counters/updates': 4713}
train stats after 150848 examples: {'rewards_train/chosen': '0.097966', 'rewards_train/rejected': '0.038091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059875', 'logps_train/rejected': '-87.337', 'logps_train/chosen': '-144.15', 'loss/train': '0.67688', 'examples_per_second': '31.578', 'grad_norm': '22.875', 'counters/examples': 150848, 'counters/updates': 4714}
train stats after 150880 examples: {'rewards_train/chosen': '0.077304', 'rewards_train/rejected': '0.071152', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0061523', 'logps_train/rejected': '-104.01', 'logps_train/chosen': '-139.76', 'loss/train': '0.70124', 'examples_per_second': '32.196', 'grad_norm': '25.875', 'counters/examples': 150880, 'counters/updates': 4715}
train stats after 150912 examples: {'rewards_train/chosen': '0.098689', 'rewards_train/rejected': '0.016338', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082352', 'logps_train/rejected': '-106.33', 'logps_train/chosen': '-132.28', 'loss/train': '0.65923', 'examples_per_second': '30.66', 'grad_norm': '24.375', 'counters/examples': 150912, 'counters/updates': 4716}
train stats after 150944 examples: {'rewards_train/chosen': '0.096332', 'rewards_train/rejected': '0.024197', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072135', 'logps_train/rejected': '-139.96', 'logps_train/chosen': '-135.04', 'loss/train': '0.67252', 'examples_per_second': '32.997', 'grad_norm': '27.25', 'counters/examples': 150944, 'counters/updates': 4717}
train stats after 150976 examples: {'rewards_train/chosen': '0.11875', 'rewards_train/rejected': '0.10227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016476', 'logps_train/rejected': '-158.06', 'logps_train/chosen': '-126.41', 'loss/train': '0.69947', 'examples_per_second': '30.56', 'grad_norm': '31.5', 'counters/examples': 150976, 'counters/updates': 4718}
skipping logging after 151008 examples to avoid logging too frequently
train stats after 151040 examples: {'rewards_train/chosen': '0.039099', 'rewards_train/rejected': '-0.0065165', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045615', 'logps_train/rejected': '-100.13', 'logps_train/chosen': '-148.08', 'loss/train': '0.67648', 'examples_per_second': '30.958', 'grad_norm': '25.375', 'counters/examples': 151040, 'counters/updates': 4720}
train stats after 151072 examples: {'rewards_train/chosen': '0.14326', 'rewards_train/rejected': '0.10141', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041851', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-119.57', 'loss/train': '0.67981', 'examples_per_second': '32.043', 'grad_norm': '27.375', 'counters/examples': 151072, 'counters/updates': 4721}
train stats after 151104 examples: {'rewards_train/chosen': '0.11267', 'rewards_train/rejected': '-0.00688', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11955', 'logps_train/rejected': '-109.31', 'logps_train/chosen': '-116.22', 'loss/train': '0.64304', 'examples_per_second': '31.329', 'grad_norm': '25.5', 'counters/examples': 151104, 'counters/updates': 4722}
train stats after 151136 examples: {'rewards_train/chosen': '0.18412', 'rewards_train/rejected': '0.043833', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14028', 'logps_train/rejected': '-98.306', 'logps_train/chosen': '-130.73', 'loss/train': '0.63559', 'examples_per_second': '30.782', 'grad_norm': '23.5', 'counters/examples': 151136, 'counters/updates': 4723}
train stats after 151168 examples: {'rewards_train/chosen': '0.1785', 'rewards_train/rejected': '0.053374', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12513', 'logps_train/rejected': '-110.87', 'logps_train/chosen': '-150.9', 'loss/train': '0.64106', 'examples_per_second': '31.59', 'grad_norm': '24.5', 'counters/examples': 151168, 'counters/updates': 4724}
train stats after 151200 examples: {'rewards_train/chosen': '0.099242', 'rewards_train/rejected': '0.014746', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084496', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-139.16', 'loss/train': '0.67364', 'examples_per_second': '31.632', 'grad_norm': '25.625', 'counters/examples': 151200, 'counters/updates': 4725}
train stats after 151232 examples: {'rewards_train/chosen': '0.10611', 'rewards_train/rejected': '0.049134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056973', 'logps_train/rejected': '-108.69', 'logps_train/chosen': '-153.41', 'loss/train': '0.68519', 'examples_per_second': '31.906', 'grad_norm': '26.625', 'counters/examples': 151232, 'counters/updates': 4726}
train stats after 151264 examples: {'rewards_train/chosen': '0.10362', 'rewards_train/rejected': '0.034596', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06902', 'logps_train/rejected': '-95.274', 'logps_train/chosen': '-119.55', 'loss/train': '0.66645', 'examples_per_second': '32.022', 'grad_norm': '24', 'counters/examples': 151264, 'counters/updates': 4727}
train stats after 151296 examples: {'rewards_train/chosen': '0.15219', 'rewards_train/rejected': '0.027079', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12511', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-155.68', 'loss/train': '0.64523', 'examples_per_second': '30.776', 'grad_norm': '34.5', 'counters/examples': 151296, 'counters/updates': 4728}
train stats after 151328 examples: {'rewards_train/chosen': '0.11774', 'rewards_train/rejected': '0.060347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057398', 'logps_train/rejected': '-133.12', 'logps_train/chosen': '-139.05', 'loss/train': '0.6788', 'examples_per_second': '32.897', 'grad_norm': '27.125', 'counters/examples': 151328, 'counters/updates': 4729}
train stats after 151360 examples: {'rewards_train/chosen': '0.06534', 'rewards_train/rejected': '0.02759', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03775', 'logps_train/rejected': '-119.52', 'logps_train/chosen': '-127.15', 'loss/train': '0.68627', 'examples_per_second': '31.735', 'grad_norm': '25.625', 'counters/examples': 151360, 'counters/updates': 4730}
train stats after 151392 examples: {'rewards_train/chosen': '0.14961', 'rewards_train/rejected': '0.065088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084519', 'logps_train/rejected': '-109.51', 'logps_train/chosen': '-177.76', 'loss/train': '0.67155', 'examples_per_second': '29.844', 'grad_norm': '28', 'counters/examples': 151392, 'counters/updates': 4731}
train stats after 151424 examples: {'rewards_train/chosen': '0.13572', 'rewards_train/rejected': '0.016501', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11922', 'logps_train/rejected': '-124.68', 'logps_train/chosen': '-166.95', 'loss/train': '0.64132', 'examples_per_second': '31.012', 'grad_norm': '30.125', 'counters/examples': 151424, 'counters/updates': 4732}
train stats after 151456 examples: {'rewards_train/chosen': '0.11246', 'rewards_train/rejected': '0.15846', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.045995', 'logps_train/rejected': '-168.76', 'logps_train/chosen': '-133.69', 'loss/train': '0.73046', 'examples_per_second': '31.57', 'grad_norm': '30.625', 'counters/examples': 151456, 'counters/updates': 4733}
train stats after 151488 examples: {'rewards_train/chosen': '0.16863', 'rewards_train/rejected': '0.027623', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14101', 'logps_train/rejected': '-132.72', 'logps_train/chosen': '-136.11', 'loss/train': '0.63336', 'examples_per_second': '31.523', 'grad_norm': '25', 'counters/examples': 151488, 'counters/updates': 4734}
train stats after 151520 examples: {'rewards_train/chosen': '0.17946', 'rewards_train/rejected': '0.050319', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12914', 'logps_train/rejected': '-102.01', 'logps_train/chosen': '-140.37', 'loss/train': '0.63852', 'examples_per_second': '33.157', 'grad_norm': '24.75', 'counters/examples': 151520, 'counters/updates': 4735}
train stats after 151552 examples: {'rewards_train/chosen': '0.05872', 'rewards_train/rejected': '-0.00086802', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059588', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-142.73', 'loss/train': '0.67598', 'examples_per_second': '31.927', 'grad_norm': '23.75', 'counters/examples': 151552, 'counters/updates': 4736}
train stats after 151584 examples: {'rewards_train/chosen': '0.089973', 'rewards_train/rejected': '0.042678', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047294', 'logps_train/rejected': '-139.92', 'logps_train/chosen': '-129.67', 'loss/train': '0.67703', 'examples_per_second': '30.393', 'grad_norm': '24.875', 'counters/examples': 151584, 'counters/updates': 4737}
train stats after 151616 examples: {'rewards_train/chosen': '0.10983', 'rewards_train/rejected': '0.072121', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037706', 'logps_train/rejected': '-166.47', 'logps_train/chosen': '-131.08', 'loss/train': '0.69166', 'examples_per_second': '30.026', 'grad_norm': '37.75', 'counters/examples': 151616, 'counters/updates': 4738}
train stats after 151648 examples: {'rewards_train/chosen': '0.070856', 'rewards_train/rejected': '0.016375', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054481', 'logps_train/rejected': '-119.37', 'logps_train/chosen': '-141.68', 'loss/train': '0.67531', 'examples_per_second': '32.533', 'grad_norm': '26.75', 'counters/examples': 151648, 'counters/updates': 4739}
train stats after 151680 examples: {'rewards_train/chosen': '0.082699', 'rewards_train/rejected': '-0.00042561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083125', 'logps_train/rejected': '-123.28', 'logps_train/chosen': '-150.12', 'loss/train': '0.66051', 'examples_per_second': '32.305', 'grad_norm': '27.75', 'counters/examples': 151680, 'counters/updates': 4740}
skipping logging after 151712 examples to avoid logging too frequently
train stats after 151744 examples: {'rewards_train/chosen': '0.099866', 'rewards_train/rejected': '-0.016009', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11587', 'logps_train/rejected': '-137.23', 'logps_train/chosen': '-143.06', 'loss/train': '0.64387', 'examples_per_second': '32.04', 'grad_norm': '27.75', 'counters/examples': 151744, 'counters/updates': 4742}
train stats after 151776 examples: {'rewards_train/chosen': '0.069514', 'rewards_train/rejected': '0.013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056514', 'logps_train/rejected': '-121.75', 'logps_train/chosen': '-113.67', 'loss/train': '0.66987', 'examples_per_second': '31.707', 'grad_norm': '24.125', 'counters/examples': 151776, 'counters/updates': 4743}
train stats after 151808 examples: {'rewards_train/chosen': '0.1265', 'rewards_train/rejected': '0.0017009', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1248', 'logps_train/rejected': '-103.97', 'logps_train/chosen': '-140.38', 'loss/train': '0.64127', 'examples_per_second': '31.576', 'grad_norm': '23.5', 'counters/examples': 151808, 'counters/updates': 4744}
train stats after 151840 examples: {'rewards_train/chosen': '0.093125', 'rewards_train/rejected': '-0.0019008', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095026', 'logps_train/rejected': '-169.9', 'logps_train/chosen': '-176.64', 'loss/train': '0.66192', 'examples_per_second': '32.035', 'grad_norm': '29.125', 'counters/examples': 151840, 'counters/updates': 4745}
train stats after 151872 examples: {'rewards_train/chosen': '0.039832', 'rewards_train/rejected': '0.082542', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042709', 'logps_train/rejected': '-132.5', 'logps_train/chosen': '-130.98', 'loss/train': '0.72206', 'examples_per_second': '30.578', 'grad_norm': '28.25', 'counters/examples': 151872, 'counters/updates': 4746}
train stats after 151904 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.087785', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05237', 'logps_train/rejected': '-108.61', 'logps_train/chosen': '-147.87', 'loss/train': '0.67857', 'examples_per_second': '29.996', 'grad_norm': '31', 'counters/examples': 151904, 'counters/updates': 4747}
train stats after 151936 examples: {'rewards_train/chosen': '0.10322', 'rewards_train/rejected': '-0.025001', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12822', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-130.72', 'loss/train': '0.64793', 'examples_per_second': '27.809', 'grad_norm': '24.125', 'counters/examples': 151936, 'counters/updates': 4748}
skipping logging after 151968 examples to avoid logging too frequently
train stats after 152000 examples: {'rewards_train/chosen': '0.17421', 'rewards_train/rejected': '0.057535', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11667', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-172.68', 'loss/train': '0.64507', 'examples_per_second': '31.264', 'grad_norm': '27.125', 'counters/examples': 152000, 'counters/updates': 4750}
train stats after 152032 examples: {'rewards_train/chosen': '0.15294', 'rewards_train/rejected': '0.031453', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12148', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-137.25', 'loss/train': '0.64731', 'examples_per_second': '24.32', 'grad_norm': '30.875', 'counters/examples': 152032, 'counters/updates': 4751}
train stats after 152064 examples: {'rewards_train/chosen': '0.038825', 'rewards_train/rejected': '0.011138', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027687', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-99.633', 'loss/train': '0.68579', 'examples_per_second': '31.199', 'grad_norm': '26.75', 'counters/examples': 152064, 'counters/updates': 4752}
train stats after 152096 examples: {'rewards_train/chosen': '0.17497', 'rewards_train/rejected': '0.054874', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12009', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-142.79', 'loss/train': '0.64812', 'examples_per_second': '30.563', 'grad_norm': '23.25', 'counters/examples': 152096, 'counters/updates': 4753}
train stats after 152128 examples: {'rewards_train/chosen': '0.13175', 'rewards_train/rejected': '0.095147', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036607', 'logps_train/rejected': '-135.37', 'logps_train/chosen': '-171.57', 'loss/train': '0.68279', 'examples_per_second': '31.479', 'grad_norm': '29.875', 'counters/examples': 152128, 'counters/updates': 4754}
skipping logging after 152160 examples to avoid logging too frequently
train stats after 152192 examples: {'rewards_train/chosen': '0.19288', 'rewards_train/rejected': '0.029863', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16302', 'logps_train/rejected': '-150.1', 'logps_train/chosen': '-199', 'loss/train': '0.62693', 'examples_per_second': '31.608', 'grad_norm': '27.375', 'counters/examples': 152192, 'counters/updates': 4756}
train stats after 152224 examples: {'rewards_train/chosen': '0.17034', 'rewards_train/rejected': '0.055379', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11496', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-145.66', 'loss/train': '0.65368', 'examples_per_second': '31.846', 'grad_norm': '29', 'counters/examples': 152224, 'counters/updates': 4757}
train stats after 152256 examples: {'rewards_train/chosen': '0.10611', 'rewards_train/rejected': '0.060118', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045996', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-142.54', 'loss/train': '0.68045', 'examples_per_second': '31.592', 'grad_norm': '24.5', 'counters/examples': 152256, 'counters/updates': 4758}
train stats after 152288 examples: {'rewards_train/chosen': '0.11614', 'rewards_train/rejected': '0.010968', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10518', 'logps_train/rejected': '-127.42', 'logps_train/chosen': '-162.25', 'loss/train': '0.65077', 'examples_per_second': '31.675', 'grad_norm': '27.75', 'counters/examples': 152288, 'counters/updates': 4759}
train stats after 152320 examples: {'rewards_train/chosen': '0.2091', 'rewards_train/rejected': '0.10814', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10096', 'logps_train/rejected': '-142.42', 'logps_train/chosen': '-138.96', 'loss/train': '0.66754', 'examples_per_second': '31.564', 'grad_norm': '41.25', 'counters/examples': 152320, 'counters/updates': 4760}
train stats after 152352 examples: {'rewards_train/chosen': '0.15109', 'rewards_train/rejected': '0.014555', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13653', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-123.73', 'loss/train': '0.63517', 'examples_per_second': '30.705', 'grad_norm': '25.75', 'counters/examples': 152352, 'counters/updates': 4761}
train stats after 152384 examples: {'rewards_train/chosen': '0.14875', 'rewards_train/rejected': '0.048968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099785', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-129.33', 'loss/train': '0.65386', 'examples_per_second': '31.296', 'grad_norm': '25.75', 'counters/examples': 152384, 'counters/updates': 4762}
train stats after 152416 examples: {'rewards_train/chosen': '0.059564', 'rewards_train/rejected': '0.080181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020617', 'logps_train/rejected': '-134.54', 'logps_train/chosen': '-148.95', 'loss/train': '0.71032', 'examples_per_second': '31.575', 'grad_norm': '32.5', 'counters/examples': 152416, 'counters/updates': 4763}
train stats after 152448 examples: {'rewards_train/chosen': '0.0065405', 'rewards_train/rejected': '0.00054317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0059973', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-138.17', 'loss/train': '0.70199', 'examples_per_second': '32.057', 'grad_norm': '26.875', 'counters/examples': 152448, 'counters/updates': 4764}
train stats after 152480 examples: {'rewards_train/chosen': '0.10436', 'rewards_train/rejected': '0.037675', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066681', 'logps_train/rejected': '-139.09', 'logps_train/chosen': '-148.74', 'loss/train': '0.66903', 'examples_per_second': '32.936', 'grad_norm': '29.125', 'counters/examples': 152480, 'counters/updates': 4765}
train stats after 152512 examples: {'rewards_train/chosen': '0.14011', 'rewards_train/rejected': '0.022103', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.118', 'logps_train/rejected': '-97.37', 'logps_train/chosen': '-146.59', 'loss/train': '0.64708', 'examples_per_second': '30.962', 'grad_norm': '23.125', 'counters/examples': 152512, 'counters/updates': 4766}
skipping logging after 152544 examples to avoid logging too frequently
train stats after 152576 examples: {'rewards_train/chosen': '0.047482', 'rewards_train/rejected': '0.062635', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015153', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-143.25', 'loss/train': '0.70752', 'examples_per_second': '33.535', 'grad_norm': '27', 'counters/examples': 152576, 'counters/updates': 4768}
train stats after 152608 examples: {'rewards_train/chosen': '0.1791', 'rewards_train/rejected': '0.082151', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096948', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-166.2', 'loss/train': '0.65873', 'examples_per_second': '31.527', 'grad_norm': '27.25', 'counters/examples': 152608, 'counters/updates': 4769}
train stats after 152640 examples: {'rewards_train/chosen': '0.12838', 'rewards_train/rejected': '0.0066619', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12172', 'logps_train/rejected': '-125.19', 'logps_train/chosen': '-149.67', 'loss/train': '0.64236', 'examples_per_second': '32.869', 'grad_norm': '25.625', 'counters/examples': 152640, 'counters/updates': 4770}
train stats after 152672 examples: {'rewards_train/chosen': '0.10305', 'rewards_train/rejected': '0.053847', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049205', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-161.66', 'loss/train': '0.68165', 'examples_per_second': '32.441', 'grad_norm': '29.25', 'counters/examples': 152672, 'counters/updates': 4771}
skipping logging after 152704 examples to avoid logging too frequently
train stats after 152736 examples: {'rewards_train/chosen': '0.14453', 'rewards_train/rejected': '0.080373', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064161', 'logps_train/rejected': '-116.12', 'logps_train/chosen': '-138.76', 'loss/train': '0.66849', 'examples_per_second': '33.386', 'grad_norm': '24.25', 'counters/examples': 152736, 'counters/updates': 4773}
train stats after 152768 examples: {'rewards_train/chosen': '0.11215', 'rewards_train/rejected': '-0.016845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.129', 'logps_train/rejected': '-101.74', 'logps_train/chosen': '-122.81', 'loss/train': '0.6417', 'examples_per_second': '32.509', 'grad_norm': '25', 'counters/examples': 152768, 'counters/updates': 4774}
train stats after 152800 examples: {'rewards_train/chosen': '0.10721', 'rewards_train/rejected': '0.062828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044385', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-112.33', 'loss/train': '0.67989', 'examples_per_second': '30.226', 'grad_norm': '28.5', 'counters/examples': 152800, 'counters/updates': 4775}
train stats after 152832 examples: {'rewards_train/chosen': '0.14457', 'rewards_train/rejected': '0.031656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11292', 'logps_train/rejected': '-159.98', 'logps_train/chosen': '-146.59', 'loss/train': '0.65592', 'examples_per_second': '30.051', 'grad_norm': '31.375', 'counters/examples': 152832, 'counters/updates': 4776}
train stats after 152864 examples: {'rewards_train/chosen': '0.12281', 'rewards_train/rejected': '0.038272', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08454', 'logps_train/rejected': '-151.16', 'logps_train/chosen': '-153.37', 'loss/train': '0.66578', 'examples_per_second': '32.427', 'grad_norm': '28.875', 'counters/examples': 152864, 'counters/updates': 4777}
train stats after 152896 examples: {'rewards_train/chosen': '0.13205', 'rewards_train/rejected': '0.079226', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052822', 'logps_train/rejected': '-143.63', 'logps_train/chosen': '-158', 'loss/train': '0.67822', 'examples_per_second': '31.502', 'grad_norm': '34.75', 'counters/examples': 152896, 'counters/updates': 4778}
train stats after 152928 examples: {'rewards_train/chosen': '0.13683', 'rewards_train/rejected': '0.044277', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092553', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-123.3', 'loss/train': '0.65639', 'examples_per_second': '31.489', 'grad_norm': '26.5', 'counters/examples': 152928, 'counters/updates': 4779}
skipping logging after 152960 examples to avoid logging too frequently
train stats after 152992 examples: {'rewards_train/chosen': '0.093555', 'rewards_train/rejected': '0.066599', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026956', 'logps_train/rejected': '-118.24', 'logps_train/chosen': '-134.19', 'loss/train': '0.68848', 'examples_per_second': '32.592', 'grad_norm': '25.125', 'counters/examples': 152992, 'counters/updates': 4781}
skipping logging after 153024 examples to avoid logging too frequently
train stats after 153056 examples: {'rewards_train/chosen': '0.1565', 'rewards_train/rejected': '0.048052', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10845', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-143.69', 'loss/train': '0.65365', 'examples_per_second': '30.595', 'grad_norm': '26.375', 'counters/examples': 153056, 'counters/updates': 4783}
skipping logging after 153088 examples to avoid logging too frequently
train stats after 153120 examples: {'rewards_train/chosen': '0.14655', 'rewards_train/rejected': '0.087082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059469', 'logps_train/rejected': '-101.8', 'logps_train/chosen': '-151', 'loss/train': '0.67349', 'examples_per_second': '31.424', 'grad_norm': '31.375', 'counters/examples': 153120, 'counters/updates': 4785}
train stats after 153152 examples: {'rewards_train/chosen': '0.10979', 'rewards_train/rejected': '-0.0044277', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11421', 'logps_train/rejected': '-103.56', 'logps_train/chosen': '-113.25', 'loss/train': '0.64714', 'examples_per_second': '32.674', 'grad_norm': '24', 'counters/examples': 153152, 'counters/updates': 4786}
train stats after 153184 examples: {'rewards_train/chosen': '0.099841', 'rewards_train/rejected': '-0.033867', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13371', 'logps_train/rejected': '-129.28', 'logps_train/chosen': '-120.63', 'loss/train': '0.64347', 'examples_per_second': '33.108', 'grad_norm': '25.25', 'counters/examples': 153184, 'counters/updates': 4787}
skipping logging after 153216 examples to avoid logging too frequently
train stats after 153248 examples: {'rewards_train/chosen': '0.18056', 'rewards_train/rejected': '0.079954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10061', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-167.84', 'loss/train': '0.65545', 'examples_per_second': '31.538', 'grad_norm': '28.625', 'counters/examples': 153248, 'counters/updates': 4789}
train stats after 153280 examples: {'rewards_train/chosen': '0.040225', 'rewards_train/rejected': '0.04348', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0032559', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-93.088', 'loss/train': '0.70323', 'examples_per_second': '31.57', 'grad_norm': '24', 'counters/examples': 153280, 'counters/updates': 4790}
train stats after 153312 examples: {'rewards_train/chosen': '0.15393', 'rewards_train/rejected': '0.10072', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05321', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-141.38', 'loss/train': '0.68027', 'examples_per_second': '31.567', 'grad_norm': '29.25', 'counters/examples': 153312, 'counters/updates': 4791}
train stats after 153344 examples: {'rewards_train/chosen': '0.10811', 'rewards_train/rejected': '0.072474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035638', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-166.52', 'loss/train': '0.68279', 'examples_per_second': '30.742', 'grad_norm': '26.75', 'counters/examples': 153344, 'counters/updates': 4792}
skipping logging after 153376 examples to avoid logging too frequently
train stats after 153408 examples: {'rewards_train/chosen': '0.048928', 'rewards_train/rejected': '0.0087872', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040141', 'logps_train/rejected': '-100.68', 'logps_train/chosen': '-102.96', 'loss/train': '0.6798', 'examples_per_second': '35.936', 'grad_norm': '22.375', 'counters/examples': 153408, 'counters/updates': 4794}
train stats after 153440 examples: {'rewards_train/chosen': '0.14257', 'rewards_train/rejected': '0.039645', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10293', 'logps_train/rejected': '-109.16', 'logps_train/chosen': '-151.78', 'loss/train': '0.65234', 'examples_per_second': '30.947', 'grad_norm': '23.75', 'counters/examples': 153440, 'counters/updates': 4795}
train stats after 153472 examples: {'rewards_train/chosen': '0.10095', 'rewards_train/rejected': '0.10978', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0088299', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-151.19', 'loss/train': '0.71644', 'examples_per_second': '31.593', 'grad_norm': '31.25', 'counters/examples': 153472, 'counters/updates': 4796}
skipping logging after 153504 examples to avoid logging too frequently
train stats after 153536 examples: {'rewards_train/chosen': '0.11743', 'rewards_train/rejected': '-0.011178', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12861', 'logps_train/rejected': '-117.02', 'logps_train/chosen': '-150.82', 'loss/train': '0.64288', 'examples_per_second': '34.405', 'grad_norm': '24.375', 'counters/examples': 153536, 'counters/updates': 4798}
skipping logging after 153568 examples to avoid logging too frequently
train stats after 153600 examples: {'rewards_train/chosen': '0.0262', 'rewards_train/rejected': '0.001741', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024459', 'logps_train/rejected': '-86.291', 'logps_train/chosen': '-90.663', 'loss/train': '0.68756', 'examples_per_second': '33.014', 'grad_norm': '23', 'counters/examples': 153600, 'counters/updates': 4800}
train stats after 153632 examples: {'rewards_train/chosen': '0.12532', 'rewards_train/rejected': '0.033082', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092238', 'logps_train/rejected': '-91.819', 'logps_train/chosen': '-147.46', 'loss/train': '0.65468', 'examples_per_second': '31.184', 'grad_norm': '25.375', 'counters/examples': 153632, 'counters/updates': 4801}
train stats after 153664 examples: {'rewards_train/chosen': '0.12544', 'rewards_train/rejected': '0.022744', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1027', 'logps_train/rejected': '-125.78', 'logps_train/chosen': '-177.45', 'loss/train': '0.65108', 'examples_per_second': '30.081', 'grad_norm': '44.75', 'counters/examples': 153664, 'counters/updates': 4802}
train stats after 153696 examples: {'rewards_train/chosen': '0.15694', 'rewards_train/rejected': '0.087475', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069461', 'logps_train/rejected': '-156.43', 'logps_train/chosen': '-208.45', 'loss/train': '0.66634', 'examples_per_second': '31.494', 'grad_norm': '29.5', 'counters/examples': 153696, 'counters/updates': 4803}
train stats after 153728 examples: {'rewards_train/chosen': '0.18285', 'rewards_train/rejected': '0.026252', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1566', 'logps_train/rejected': '-121.48', 'logps_train/chosen': '-135.82', 'loss/train': '0.62623', 'examples_per_second': '30.557', 'grad_norm': '24.125', 'counters/examples': 153728, 'counters/updates': 4804}
skipping logging after 153760 examples to avoid logging too frequently
train stats after 153792 examples: {'rewards_train/chosen': '0.088869', 'rewards_train/rejected': '0.040954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047916', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-169.78', 'loss/train': '0.67587', 'examples_per_second': '32.548', 'grad_norm': '29.125', 'counters/examples': 153792, 'counters/updates': 4806}
train stats after 153824 examples: {'rewards_train/chosen': '0.15094', 'rewards_train/rejected': '-0.017569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16851', 'logps_train/rejected': '-152.52', 'logps_train/chosen': '-136.47', 'loss/train': '0.62737', 'examples_per_second': '29.844', 'grad_norm': '27.5', 'counters/examples': 153824, 'counters/updates': 4807}
train stats after 153856 examples: {'rewards_train/chosen': '0.12983', 'rewards_train/rejected': '0.083413', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046419', 'logps_train/rejected': '-116.47', 'logps_train/chosen': '-169.32', 'loss/train': '0.68415', 'examples_per_second': '31.542', 'grad_norm': '28.5', 'counters/examples': 153856, 'counters/updates': 4808}
train stats after 153888 examples: {'rewards_train/chosen': '0.13373', 'rewards_train/rejected': '0.095977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037757', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-161.14', 'loss/train': '0.68102', 'examples_per_second': '30.599', 'grad_norm': '26.125', 'counters/examples': 153888, 'counters/updates': 4809}
train stats after 153920 examples: {'rewards_train/chosen': '0.16721', 'rewards_train/rejected': '-0.0014946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1687', 'logps_train/rejected': '-126.43', 'logps_train/chosen': '-140.97', 'loss/train': '0.62045', 'examples_per_second': '29.851', 'grad_norm': '25.875', 'counters/examples': 153920, 'counters/updates': 4810}
train stats after 153952 examples: {'rewards_train/chosen': '0.13893', 'rewards_train/rejected': '-0.012357', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15129', 'logps_train/rejected': '-94.694', 'logps_train/chosen': '-112.32', 'loss/train': '0.63011', 'examples_per_second': '31.876', 'grad_norm': '20.875', 'counters/examples': 153952, 'counters/updates': 4811}
train stats after 153984 examples: {'rewards_train/chosen': '0.11162', 'rewards_train/rejected': '0.045282', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066335', 'logps_train/rejected': '-113.05', 'logps_train/chosen': '-157.82', 'loss/train': '0.67477', 'examples_per_second': '30.982', 'grad_norm': '26.5', 'counters/examples': 153984, 'counters/updates': 4812}
train stats after 154016 examples: {'rewards_train/chosen': '0.049336', 'rewards_train/rejected': '0.056002', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0066657', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-121.47', 'loss/train': '0.70342', 'examples_per_second': '30.112', 'grad_norm': '26.75', 'counters/examples': 154016, 'counters/updates': 4813}
train stats after 154048 examples: {'rewards_train/chosen': '0.099576', 'rewards_train/rejected': '0.095491', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0040849', 'logps_train/rejected': '-150.62', 'logps_train/chosen': '-148.51', 'loss/train': '0.69885', 'examples_per_second': '30.021', 'grad_norm': '27.25', 'counters/examples': 154048, 'counters/updates': 4814}
train stats after 154080 examples: {'rewards_train/chosen': '0.1527', 'rewards_train/rejected': '0.05988', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092822', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-147.02', 'loss/train': '0.65976', 'examples_per_second': '29.649', 'grad_norm': '30.125', 'counters/examples': 154080, 'counters/updates': 4815}
skipping logging after 154112 examples to avoid logging too frequently
train stats after 154144 examples: {'rewards_train/chosen': '0.16936', 'rewards_train/rejected': '0.0026243', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16673', 'logps_train/rejected': '-146.16', 'logps_train/chosen': '-127.36', 'loss/train': '0.62027', 'examples_per_second': '31.458', 'grad_norm': '26.25', 'counters/examples': 154144, 'counters/updates': 4817}
skipping logging after 154176 examples to avoid logging too frequently
train stats after 154208 examples: {'rewards_train/chosen': '0.089176', 'rewards_train/rejected': '0.040248', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048928', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-125.03', 'loss/train': '0.67825', 'examples_per_second': '29.95', 'grad_norm': '25.75', 'counters/examples': 154208, 'counters/updates': 4819}
train stats after 154240 examples: {'rewards_train/chosen': '0.13297', 'rewards_train/rejected': '0.07074', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062227', 'logps_train/rejected': '-139.27', 'logps_train/chosen': '-136.46', 'loss/train': '0.6848', 'examples_per_second': '30.066', 'grad_norm': '27.75', 'counters/examples': 154240, 'counters/updates': 4820}
skipping logging after 154272 examples to avoid logging too frequently
train stats after 154304 examples: {'rewards_train/chosen': '0.070907', 'rewards_train/rejected': '0.0047608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066146', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-109.03', 'loss/train': '0.66845', 'examples_per_second': '34.021', 'grad_norm': '25.375', 'counters/examples': 154304, 'counters/updates': 4822}
train stats after 154336 examples: {'rewards_train/chosen': '0.18506', 'rewards_train/rejected': '0.063284', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12178', 'logps_train/rejected': '-109.49', 'logps_train/chosen': '-172.67', 'loss/train': '0.6426', 'examples_per_second': '30.112', 'grad_norm': '26', 'counters/examples': 154336, 'counters/updates': 4823}
train stats after 154368 examples: {'rewards_train/chosen': '0.15923', 'rewards_train/rejected': '0.11505', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04418', 'logps_train/rejected': '-170.74', 'logps_train/chosen': '-136.75', 'loss/train': '0.68593', 'examples_per_second': '31.521', 'grad_norm': '31.25', 'counters/examples': 154368, 'counters/updates': 4824}
train stats after 154400 examples: {'rewards_train/chosen': '0.12353', 'rewards_train/rejected': '-0.0036683', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1272', 'logps_train/rejected': '-154.42', 'logps_train/chosen': '-162.19', 'loss/train': '0.64217', 'examples_per_second': '33.128', 'grad_norm': '29.125', 'counters/examples': 154400, 'counters/updates': 4825}
train stats after 154432 examples: {'rewards_train/chosen': '0.087583', 'rewards_train/rejected': '0.095216', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0076327', 'logps_train/rejected': '-148.31', 'logps_train/chosen': '-136.52', 'loss/train': '0.70415', 'examples_per_second': '31.731', 'grad_norm': '29.125', 'counters/examples': 154432, 'counters/updates': 4826}
train stats after 154464 examples: {'rewards_train/chosen': '-0.0044131', 'rewards_train/rejected': '-0.068108', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063695', 'logps_train/rejected': '-90.316', 'logps_train/chosen': '-131.5', 'loss/train': '0.6735', 'examples_per_second': '32.022', 'grad_norm': '28.125', 'counters/examples': 154464, 'counters/updates': 4827}
train stats after 154496 examples: {'rewards_train/chosen': '0.069379', 'rewards_train/rejected': '0.053137', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016242', 'logps_train/rejected': '-146.27', 'logps_train/chosen': '-161.12', 'loss/train': '0.69538', 'examples_per_second': '31.577', 'grad_norm': '29.625', 'counters/examples': 154496, 'counters/updates': 4828}
train stats after 154528 examples: {'rewards_train/chosen': '0.16074', 'rewards_train/rejected': '0.047042', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1137', 'logps_train/rejected': '-120.91', 'logps_train/chosen': '-143.13', 'loss/train': '0.6484', 'examples_per_second': '31.527', 'grad_norm': '32.25', 'counters/examples': 154528, 'counters/updates': 4829}
train stats after 154560 examples: {'rewards_train/chosen': '0.1864', 'rewards_train/rejected': '0.074418', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11198', 'logps_train/rejected': '-97.97', 'logps_train/chosen': '-137.59', 'loss/train': '0.64787', 'examples_per_second': '31.739', 'grad_norm': '24.625', 'counters/examples': 154560, 'counters/updates': 4830}
train stats after 154592 examples: {'rewards_train/chosen': '0.16631', 'rewards_train/rejected': '0.0013449', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16497', 'logps_train/rejected': '-108.15', 'logps_train/chosen': '-130.37', 'loss/train': '0.62012', 'examples_per_second': '30.563', 'grad_norm': '21.375', 'counters/examples': 154592, 'counters/updates': 4831}
train stats after 154624 examples: {'rewards_train/chosen': '0.057746', 'rewards_train/rejected': '0.043962', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013783', 'logps_train/rejected': '-139.81', 'logps_train/chosen': '-143.71', 'loss/train': '0.69454', 'examples_per_second': '31.532', 'grad_norm': '31.625', 'counters/examples': 154624, 'counters/updates': 4832}
skipping logging after 154656 examples to avoid logging too frequently
train stats after 154688 examples: {'rewards_train/chosen': '0.17851', 'rewards_train/rejected': '0.1233', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.055204', 'logps_train/rejected': '-148.07', 'logps_train/chosen': '-163.52', 'loss/train': '0.67988', 'examples_per_second': '30.537', 'grad_norm': '29.5', 'counters/examples': 154688, 'counters/updates': 4834}
train stats after 154720 examples: {'rewards_train/chosen': '0.085943', 'rewards_train/rejected': '0.0087898', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077153', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-128.86', 'loss/train': '0.66757', 'examples_per_second': '32.628', 'grad_norm': '32.5', 'counters/examples': 154720, 'counters/updates': 4835}
train stats after 154752 examples: {'rewards_train/chosen': '0.13764', 'rewards_train/rejected': '0.039468', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098172', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-134.74', 'loss/train': '0.65513', 'examples_per_second': '31.567', 'grad_norm': '26.5', 'counters/examples': 154752, 'counters/updates': 4836}
train stats after 154784 examples: {'rewards_train/chosen': '0.065878', 'rewards_train/rejected': '-0.011948', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077825', 'logps_train/rejected': '-136.36', 'logps_train/chosen': '-163.6', 'loss/train': '0.66403', 'examples_per_second': '23.701', 'grad_norm': '26.875', 'counters/examples': 154784, 'counters/updates': 4837}
train stats after 154816 examples: {'rewards_train/chosen': '0.1788', 'rewards_train/rejected': '0.0093389', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16946', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-154.83', 'loss/train': '0.62129', 'examples_per_second': '32.529', 'grad_norm': '27.75', 'counters/examples': 154816, 'counters/updates': 4838}
train stats after 154848 examples: {'rewards_train/chosen': '0.15267', 'rewards_train/rejected': '0.14518', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0074875', 'logps_train/rejected': '-158.06', 'logps_train/chosen': '-141.98', 'loss/train': '0.69662', 'examples_per_second': '31.269', 'grad_norm': '28.25', 'counters/examples': 154848, 'counters/updates': 4839}
train stats after 154880 examples: {'rewards_train/chosen': '0.083598', 'rewards_train/rejected': '-0.039809', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12341', 'logps_train/rejected': '-125.94', 'logps_train/chosen': '-165', 'loss/train': '0.64715', 'examples_per_second': '31.424', 'grad_norm': '23.875', 'counters/examples': 154880, 'counters/updates': 4840}
train stats after 154912 examples: {'rewards_train/chosen': '0.047572', 'rewards_train/rejected': '0.078707', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031134', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-135.96', 'loss/train': '0.71825', 'examples_per_second': '32.167', 'grad_norm': '28', 'counters/examples': 154912, 'counters/updates': 4841}
train stats after 154944 examples: {'rewards_train/chosen': '0.11084', 'rewards_train/rejected': '0.10056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010277', 'logps_train/rejected': '-110.4', 'logps_train/chosen': '-165.23', 'loss/train': '0.69587', 'examples_per_second': '31.502', 'grad_norm': '28.75', 'counters/examples': 154944, 'counters/updates': 4842}
train stats after 154976 examples: {'rewards_train/chosen': '0.1307', 'rewards_train/rejected': '0.015643', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11505', 'logps_train/rejected': '-109.97', 'logps_train/chosen': '-118.33', 'loss/train': '0.64452', 'examples_per_second': '33.013', 'grad_norm': '25.25', 'counters/examples': 154976, 'counters/updates': 4843}
train stats after 155008 examples: {'rewards_train/chosen': '0.098799', 'rewards_train/rejected': '-0.077148', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17595', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-137.7', 'loss/train': '0.61985', 'examples_per_second': '30.705', 'grad_norm': '25.375', 'counters/examples': 155008, 'counters/updates': 4844}
skipping logging after 155040 examples to avoid logging too frequently
train stats after 155072 examples: {'rewards_train/chosen': '0.15925', 'rewards_train/rejected': '0.1044', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.054854', 'logps_train/rejected': '-156.07', 'logps_train/chosen': '-154.91', 'loss/train': '0.67679', 'examples_per_second': '32.277', 'grad_norm': '28.875', 'counters/examples': 155072, 'counters/updates': 4846}
skipping logging after 155104 examples to avoid logging too frequently
train stats after 155136 examples: {'rewards_train/chosen': '0.077644', 'rewards_train/rejected': '0.017459', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060185', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-127.89', 'loss/train': '0.67286', 'examples_per_second': '32.169', 'grad_norm': '26.25', 'counters/examples': 155136, 'counters/updates': 4848}
train stats after 155168 examples: {'rewards_train/chosen': '0.16379', 'rewards_train/rejected': '0.026958', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13683', 'logps_train/rejected': '-136.84', 'logps_train/chosen': '-174.61', 'loss/train': '0.6399', 'examples_per_second': '31.442', 'grad_norm': '29.25', 'counters/examples': 155168, 'counters/updates': 4849}
train stats after 155200 examples: {'rewards_train/chosen': '0.16473', 'rewards_train/rejected': '-0.01005', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17478', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-146.99', 'loss/train': '0.62226', 'examples_per_second': '30.124', 'grad_norm': '30.5', 'counters/examples': 155200, 'counters/updates': 4850}
skipping logging after 155232 examples to avoid logging too frequently
train stats after 155264 examples: {'rewards_train/chosen': '0.13204', 'rewards_train/rejected': '0.041769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090269', 'logps_train/rejected': '-104.07', 'logps_train/chosen': '-124.31', 'loss/train': '0.65717', 'examples_per_second': '31.572', 'grad_norm': '23.125', 'counters/examples': 155264, 'counters/updates': 4852}
skipping logging after 155296 examples to avoid logging too frequently
train stats after 155328 examples: {'rewards_train/chosen': '0.037342', 'rewards_train/rejected': '-0.0015281', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038871', 'logps_train/rejected': '-81.019', 'logps_train/chosen': '-123.69', 'loss/train': '0.68228', 'examples_per_second': '31.254', 'grad_norm': '23', 'counters/examples': 155328, 'counters/updates': 4854}
skipping logging after 155360 examples to avoid logging too frequently
train stats after 155392 examples: {'rewards_train/chosen': '0.14471', 'rewards_train/rejected': '0.032446', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11227', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-177.45', 'loss/train': '0.6547', 'examples_per_second': '31.512', 'grad_norm': '28.125', 'counters/examples': 155392, 'counters/updates': 4856}
train stats after 155424 examples: {'rewards_train/chosen': '0.13282', 'rewards_train/rejected': '0.055273', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077544', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-134.93', 'loss/train': '0.66551', 'examples_per_second': '30.446', 'grad_norm': '25.25', 'counters/examples': 155424, 'counters/updates': 4857}
train stats after 155456 examples: {'rewards_train/chosen': '0.075303', 'rewards_train/rejected': '0.077444', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0021411', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-102.09', 'loss/train': '0.71154', 'examples_per_second': '30.725', 'grad_norm': '23.5', 'counters/examples': 155456, 'counters/updates': 4858}
train stats after 155488 examples: {'rewards_train/chosen': '0.13881', 'rewards_train/rejected': '0.12139', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017416', 'logps_train/rejected': '-144.16', 'logps_train/chosen': '-148.64', 'loss/train': '0.70626', 'examples_per_second': '30.415', 'grad_norm': '30', 'counters/examples': 155488, 'counters/updates': 4859}
train stats after 155520 examples: {'rewards_train/chosen': '0.15065', 'rewards_train/rejected': '0.045351', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1053', 'logps_train/rejected': '-127.61', 'logps_train/chosen': '-141.97', 'loss/train': '0.65177', 'examples_per_second': '30.202', 'grad_norm': '27.5', 'counters/examples': 155520, 'counters/updates': 4860}
train stats after 155552 examples: {'rewards_train/chosen': '0.14062', 'rewards_train/rejected': '-0.041369', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18199', 'logps_train/rejected': '-101.86', 'logps_train/chosen': '-130.07', 'loss/train': '0.61961', 'examples_per_second': '32.089', 'grad_norm': '24.875', 'counters/examples': 155552, 'counters/updates': 4861}
train stats after 155584 examples: {'rewards_train/chosen': '0.1752', 'rewards_train/rejected': '5.6207e-05', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17515', 'logps_train/rejected': '-106.24', 'logps_train/chosen': '-150.08', 'loss/train': '0.62197', 'examples_per_second': '32.372', 'grad_norm': '25.625', 'counters/examples': 155584, 'counters/updates': 4862}
train stats after 155616 examples: {'rewards_train/chosen': '0.10762', 'rewards_train/rejected': '0.071555', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036061', 'logps_train/rejected': '-147.03', 'logps_train/chosen': '-132.7', 'loss/train': '0.68525', 'examples_per_second': '31.574', 'grad_norm': '31.5', 'counters/examples': 155616, 'counters/updates': 4863}
train stats after 155648 examples: {'rewards_train/chosen': '0.16506', 'rewards_train/rejected': '0.10909', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055965', 'logps_train/rejected': '-163.81', 'logps_train/chosen': '-141.84', 'loss/train': '0.68445', 'examples_per_second': '31.43', 'grad_norm': '32.75', 'counters/examples': 155648, 'counters/updates': 4864}
train stats after 155680 examples: {'rewards_train/chosen': '0.10009', 'rewards_train/rejected': '0.066633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033457', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-158.34', 'loss/train': '0.68263', 'examples_per_second': '32.358', 'grad_norm': '30.375', 'counters/examples': 155680, 'counters/updates': 4865}
skipping logging after 155712 examples to avoid logging too frequently
train stats after 155744 examples: {'rewards_train/chosen': '0.16516', 'rewards_train/rejected': '0.02649', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13867', 'logps_train/rejected': '-111', 'logps_train/chosen': '-143.91', 'loss/train': '0.63545', 'examples_per_second': '33.881', 'grad_norm': '25.625', 'counters/examples': 155744, 'counters/updates': 4867}
train stats after 155776 examples: {'rewards_train/chosen': '0.12809', 'rewards_train/rejected': '0.03949', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.088604', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-162.46', 'loss/train': '0.66955', 'examples_per_second': '32.308', 'grad_norm': '30.25', 'counters/examples': 155776, 'counters/updates': 4868}
train stats after 155808 examples: {'rewards_train/chosen': '0.13934', 'rewards_train/rejected': '0.02629', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11305', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-142.22', 'loss/train': '0.64888', 'examples_per_second': '31.105', 'grad_norm': '25.875', 'counters/examples': 155808, 'counters/updates': 4869}
train stats after 155840 examples: {'rewards_train/chosen': '0.10843', 'rewards_train/rejected': '0.063572', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044859', 'logps_train/rejected': '-117.74', 'logps_train/chosen': '-128.56', 'loss/train': '0.67693', 'examples_per_second': '32.326', 'grad_norm': '26.5', 'counters/examples': 155840, 'counters/updates': 4870}
train stats after 155872 examples: {'rewards_train/chosen': '0.15378', 'rewards_train/rejected': '0.017257', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13653', 'logps_train/rejected': '-127.47', 'logps_train/chosen': '-148.6', 'loss/train': '0.63412', 'examples_per_second': '31.522', 'grad_norm': '26.5', 'counters/examples': 155872, 'counters/updates': 4871}
train stats after 155904 examples: {'rewards_train/chosen': '0.022567', 'rewards_train/rejected': '-0.0206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043167', 'logps_train/rejected': '-83.919', 'logps_train/chosen': '-107.09', 'loss/train': '0.67992', 'examples_per_second': '33.351', 'grad_norm': '23.125', 'counters/examples': 155904, 'counters/updates': 4872}
train stats after 155936 examples: {'rewards_train/chosen': '0.10118', 'rewards_train/rejected': '0.014348', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086834', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-140.52', 'loss/train': '0.66098', 'examples_per_second': '30.935', 'grad_norm': '24.75', 'counters/examples': 155936, 'counters/updates': 4873}
train stats after 155968 examples: {'rewards_train/chosen': '0.11645', 'rewards_train/rejected': '0.003937', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11252', 'logps_train/rejected': '-129.62', 'logps_train/chosen': '-183.17', 'loss/train': '0.65244', 'examples_per_second': '31.558', 'grad_norm': '49.25', 'counters/examples': 155968, 'counters/updates': 4874}
train stats after 156000 examples: {'rewards_train/chosen': '0.17669', 'rewards_train/rejected': '0.096708', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07998', 'logps_train/rejected': '-180.16', 'logps_train/chosen': '-162.68', 'loss/train': '0.66599', 'examples_per_second': '30.364', 'grad_norm': '28.125', 'counters/examples': 156000, 'counters/updates': 4875}
Running evaluation after 156000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.88it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 156000: {'rewards_eval/chosen': '0.12932', 'rewards_eval/rejected': '0.041306', 'rewards_eval/accuracies': '0.61719', 'rewards_eval/margins': '0.088009', 'logps_eval/rejected': '-118.2', 'logps_eval/chosen': '-138.14', 'loss/eval': '0.66243'}
skipping save for non epoch
train stats after 156032 examples: {'rewards_train/chosen': '0.064713', 'rewards_train/rejected': '0.04036', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024353', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-131.13', 'loss/train': '0.68821', 'examples_per_second': '33.757', 'grad_norm': '24.375', 'counters/examples': 156032, 'counters/updates': 4876}
train stats after 156064 examples: {'rewards_train/chosen': '0.11393', 'rewards_train/rejected': '0.13093', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016993', 'logps_train/rejected': '-111.92', 'logps_train/chosen': '-127.59', 'loss/train': '0.71811', 'examples_per_second': '30.931', 'grad_norm': '26.75', 'counters/examples': 156064, 'counters/updates': 4877}
skipping logging after 156096 examples to avoid logging too frequently
train stats after 156128 examples: {'rewards_train/chosen': '0.076277', 'rewards_train/rejected': '0.015512', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060764', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-123.17', 'loss/train': '0.67158', 'examples_per_second': '32.578', 'grad_norm': '24.25', 'counters/examples': 156128, 'counters/updates': 4879}
skipping logging after 156160 examples to avoid logging too frequently
train stats after 156192 examples: {'rewards_train/chosen': '0.11205', 'rewards_train/rejected': '0.048191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063863', 'logps_train/rejected': '-148.3', 'logps_train/chosen': '-115.34', 'loss/train': '0.67134', 'examples_per_second': '31.967', 'grad_norm': '27.375', 'counters/examples': 156192, 'counters/updates': 4881}
train stats after 156224 examples: {'rewards_train/chosen': '0.15948', 'rewards_train/rejected': '0.10495', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.054525', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-117.6', 'loss/train': '0.68588', 'examples_per_second': '31.284', 'grad_norm': '24.125', 'counters/examples': 156224, 'counters/updates': 4882}
skipping logging after 156256 examples to avoid logging too frequently
train stats after 156288 examples: {'rewards_train/chosen': '0.051929', 'rewards_train/rejected': '-0.0015778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053507', 'logps_train/rejected': '-101.32', 'logps_train/chosen': '-105.69', 'loss/train': '0.6768', 'examples_per_second': '40.183', 'grad_norm': '32.75', 'counters/examples': 156288, 'counters/updates': 4884}
train stats after 156320 examples: {'rewards_train/chosen': '0.12277', 'rewards_train/rejected': '0.018835', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10394', 'logps_train/rejected': '-130.84', 'logps_train/chosen': '-150.53', 'loss/train': '0.65469', 'examples_per_second': '31.394', 'grad_norm': '39.5', 'counters/examples': 156320, 'counters/updates': 4885}
skipping logging after 156352 examples to avoid logging too frequently
train stats after 156384 examples: {'rewards_train/chosen': '0.14271', 'rewards_train/rejected': '0.041889', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10082', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-140.58', 'loss/train': '0.65344', 'examples_per_second': '31.555', 'grad_norm': '25.125', 'counters/examples': 156384, 'counters/updates': 4887}
train stats after 156416 examples: {'rewards_train/chosen': '0.1196', 'rewards_train/rejected': '-0.043268', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16286', 'logps_train/rejected': '-84.422', 'logps_train/chosen': '-141.95', 'loss/train': '0.62572', 'examples_per_second': '32.341', 'grad_norm': '26.625', 'counters/examples': 156416, 'counters/updates': 4888}
train stats after 156448 examples: {'rewards_train/chosen': '0.11722', 'rewards_train/rejected': '0.016786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10044', 'logps_train/rejected': '-150.25', 'logps_train/chosen': '-160.27', 'loss/train': '0.66028', 'examples_per_second': '30.119', 'grad_norm': '28.25', 'counters/examples': 156448, 'counters/updates': 4889}
skipping logging after 156480 examples to avoid logging too frequently
train stats after 156512 examples: {'rewards_train/chosen': '0.16424', 'rewards_train/rejected': '0.090692', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073544', 'logps_train/rejected': '-126.7', 'logps_train/chosen': '-166.5', 'loss/train': '0.67068', 'examples_per_second': '31.515', 'grad_norm': '29.125', 'counters/examples': 156512, 'counters/updates': 4891}
train stats after 156544 examples: {'rewards_train/chosen': '0.14937', 'rewards_train/rejected': '0.012571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13679', 'logps_train/rejected': '-90.526', 'logps_train/chosen': '-178.41', 'loss/train': '0.64355', 'examples_per_second': '31.859', 'grad_norm': '26.75', 'counters/examples': 156544, 'counters/updates': 4892}
train stats after 156576 examples: {'rewards_train/chosen': '0.044826', 'rewards_train/rejected': '0.0080046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036822', 'logps_train/rejected': '-130.05', 'logps_train/chosen': '-169.41', 'loss/train': '0.6877', 'examples_per_second': '30.222', 'grad_norm': '28.25', 'counters/examples': 156576, 'counters/updates': 4893}
train stats after 156608 examples: {'rewards_train/chosen': '0.13836', 'rewards_train/rejected': '-0.076059', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21442', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-159.85', 'loss/train': '0.5999', 'examples_per_second': '31.383', 'grad_norm': '24.625', 'counters/examples': 156608, 'counters/updates': 4894}
train stats after 156640 examples: {'rewards_train/chosen': '0.065937', 'rewards_train/rejected': '-0.064119', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13006', 'logps_train/rejected': '-118.37', 'logps_train/chosen': '-138.67', 'loss/train': '0.63984', 'examples_per_second': '30.55', 'grad_norm': '24.125', 'counters/examples': 156640, 'counters/updates': 4895}
skipping logging after 156672 examples to avoid logging too frequently
train stats after 156704 examples: {'rewards_train/chosen': '0.11478', 'rewards_train/rejected': '-0.027365', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14214', 'logps_train/rejected': '-146.66', 'logps_train/chosen': '-174.5', 'loss/train': '0.63867', 'examples_per_second': '30.033', 'grad_norm': '27.875', 'counters/examples': 156704, 'counters/updates': 4897}
train stats after 156736 examples: {'rewards_train/chosen': '0.15069', 'rewards_train/rejected': '0.071552', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079137', 'logps_train/rejected': '-144.01', 'logps_train/chosen': '-144.64', 'loss/train': '0.67337', 'examples_per_second': '30.128', 'grad_norm': '43.25', 'counters/examples': 156736, 'counters/updates': 4898}
train stats after 156768 examples: {'rewards_train/chosen': '0.099323', 'rewards_train/rejected': '0.00032356', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-124.13', 'loss/train': '0.65374', 'examples_per_second': '31.479', 'grad_norm': '22.875', 'counters/examples': 156768, 'counters/updates': 4899}
train stats after 156800 examples: {'rewards_train/chosen': '0.19368', 'rewards_train/rejected': '0.082725', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11096', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-189.04', 'loss/train': '0.64761', 'examples_per_second': '30.58', 'grad_norm': '25.625', 'counters/examples': 156800, 'counters/updates': 4900}
skipping logging after 156832 examples to avoid logging too frequently
train stats after 156864 examples: {'rewards_train/chosen': '0.12226', 'rewards_train/rejected': '0.0085841', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11368', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-136.52', 'loss/train': '0.64875', 'examples_per_second': '32.094', 'grad_norm': '23.5', 'counters/examples': 156864, 'counters/updates': 4902}
train stats after 156896 examples: {'rewards_train/chosen': '0.09947', 'rewards_train/rejected': '0.088865', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010605', 'logps_train/rejected': '-125.51', 'logps_train/chosen': '-148.66', 'loss/train': '0.69624', 'examples_per_second': '31.549', 'grad_norm': '38', 'counters/examples': 156896, 'counters/updates': 4903}
train stats after 156928 examples: {'rewards_train/chosen': '0.14687', 'rewards_train/rejected': '0.030821', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11605', 'logps_train/rejected': '-117.01', 'logps_train/chosen': '-117.27', 'loss/train': '0.64877', 'examples_per_second': '32.572', 'grad_norm': '26.75', 'counters/examples': 156928, 'counters/updates': 4904}
skipping logging after 156960 examples to avoid logging too frequently
train stats after 156992 examples: {'rewards_train/chosen': '0.068445', 'rewards_train/rejected': '-0.0010988', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069544', 'logps_train/rejected': '-78.369', 'logps_train/chosen': '-120.52', 'loss/train': '0.66951', 'examples_per_second': '34.156', 'grad_norm': '21.75', 'counters/examples': 156992, 'counters/updates': 4906}
skipping logging after 157024 examples to avoid logging too frequently
train stats after 157056 examples: {'rewards_train/chosen': '0.13223', 'rewards_train/rejected': '0.023518', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10871', 'logps_train/rejected': '-122.63', 'logps_train/chosen': '-141.72', 'loss/train': '0.65341', 'examples_per_second': '31.539', 'grad_norm': '28.875', 'counters/examples': 157056, 'counters/updates': 4908}
train stats after 157088 examples: {'rewards_train/chosen': '0.15931', 'rewards_train/rejected': '0.098919', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060395', 'logps_train/rejected': '-171.56', 'logps_train/chosen': '-181.69', 'loss/train': '0.67505', 'examples_per_second': '30.604', 'grad_norm': '34.25', 'counters/examples': 157088, 'counters/updates': 4909}
train stats after 157120 examples: {'rewards_train/chosen': '0.1536', 'rewards_train/rejected': '0.066229', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087372', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-117.12', 'loss/train': '0.65522', 'examples_per_second': '31.563', 'grad_norm': '23', 'counters/examples': 157120, 'counters/updates': 4910}
train stats after 157152 examples: {'rewards_train/chosen': '0.1899', 'rewards_train/rejected': '0.077971', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11193', 'logps_train/rejected': '-135.25', 'logps_train/chosen': '-147.55', 'loss/train': '0.64901', 'examples_per_second': '31.502', 'grad_norm': '27.75', 'counters/examples': 157152, 'counters/updates': 4911}
train stats after 157184 examples: {'rewards_train/chosen': '0.20103', 'rewards_train/rejected': '0.093559', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10747', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-141.06', 'loss/train': '0.65519', 'examples_per_second': '31.733', 'grad_norm': '24.875', 'counters/examples': 157184, 'counters/updates': 4912}
train stats after 157216 examples: {'rewards_train/chosen': '0.10003', 'rewards_train/rejected': '0.0081728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09186', 'logps_train/rejected': '-148.18', 'logps_train/chosen': '-159.71', 'loss/train': '0.66491', 'examples_per_second': '30.167', 'grad_norm': '28.375', 'counters/examples': 157216, 'counters/updates': 4913}
train stats after 157248 examples: {'rewards_train/chosen': '0.074999', 'rewards_train/rejected': '0.05211', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022888', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-117.48', 'loss/train': '0.6886', 'examples_per_second': '31.058', 'grad_norm': '23.375', 'counters/examples': 157248, 'counters/updates': 4914}
train stats after 157280 examples: {'rewards_train/chosen': '0.15426', 'rewards_train/rejected': '0.047853', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10641', 'logps_train/rejected': '-101.39', 'logps_train/chosen': '-117.49', 'loss/train': '0.6592', 'examples_per_second': '30.753', 'grad_norm': '22.875', 'counters/examples': 157280, 'counters/updates': 4915}
train stats after 157312 examples: {'rewards_train/chosen': '0.095876', 'rewards_train/rejected': '0.027861', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068015', 'logps_train/rejected': '-128.84', 'logps_train/chosen': '-136.63', 'loss/train': '0.67147', 'examples_per_second': '31.537', 'grad_norm': '26.875', 'counters/examples': 157312, 'counters/updates': 4916}
train stats after 157344 examples: {'rewards_train/chosen': '0.15647', 'rewards_train/rejected': '0.02977', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1267', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-149.52', 'loss/train': '0.6456', 'examples_per_second': '31.469', 'grad_norm': '27.75', 'counters/examples': 157344, 'counters/updates': 4917}
skipping logging after 157376 examples to avoid logging too frequently
train stats after 157408 examples: {'rewards_train/chosen': '0.054587', 'rewards_train/rejected': '0.06483', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010243', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-137.7', 'loss/train': '0.71226', 'examples_per_second': '24.367', 'grad_norm': '27.5', 'counters/examples': 157408, 'counters/updates': 4919}
train stats after 157440 examples: {'rewards_train/chosen': '0.079988', 'rewards_train/rejected': '-0.012704', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092692', 'logps_train/rejected': '-83.423', 'logps_train/chosen': '-124.13', 'loss/train': '0.65866', 'examples_per_second': '32.159', 'grad_norm': '23.5', 'counters/examples': 157440, 'counters/updates': 4920}
train stats after 157472 examples: {'rewards_train/chosen': '0.044519', 'rewards_train/rejected': '-0.024945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069464', 'logps_train/rejected': '-89.598', 'logps_train/chosen': '-117.06', 'loss/train': '0.66739', 'examples_per_second': '33.032', 'grad_norm': '24.5', 'counters/examples': 157472, 'counters/updates': 4921}
train stats after 157504 examples: {'rewards_train/chosen': '0.15507', 'rewards_train/rejected': '0.012703', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14237', 'logps_train/rejected': '-91.437', 'logps_train/chosen': '-164.95', 'loss/train': '0.63161', 'examples_per_second': '23.698', 'grad_norm': '26.5', 'counters/examples': 157504, 'counters/updates': 4922}
train stats after 157536 examples: {'rewards_train/chosen': '0.14957', 'rewards_train/rejected': '0.039246', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11033', 'logps_train/rejected': '-88.367', 'logps_train/chosen': '-132.82', 'loss/train': '0.64769', 'examples_per_second': '31.098', 'grad_norm': '21.625', 'counters/examples': 157536, 'counters/updates': 4923}
train stats after 157568 examples: {'rewards_train/chosen': '0.089889', 'rewards_train/rejected': '-0.008655', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098544', 'logps_train/rejected': '-134.18', 'logps_train/chosen': '-142.05', 'loss/train': '0.65238', 'examples_per_second': '31.513', 'grad_norm': '25.5', 'counters/examples': 157568, 'counters/updates': 4924}
train stats after 157600 examples: {'rewards_train/chosen': '0.15586', 'rewards_train/rejected': '0.013122', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14274', 'logps_train/rejected': '-140.93', 'logps_train/chosen': '-171.4', 'loss/train': '0.64124', 'examples_per_second': '31.542', 'grad_norm': '26.5', 'counters/examples': 157600, 'counters/updates': 4925}
train stats after 157632 examples: {'rewards_train/chosen': '0.13067', 'rewards_train/rejected': '0.049918', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080751', 'logps_train/rejected': '-109.56', 'logps_train/chosen': '-109.42', 'loss/train': '0.66639', 'examples_per_second': '30.049', 'grad_norm': '21.25', 'counters/examples': 157632, 'counters/updates': 4926}
train stats after 157664 examples: {'rewards_train/chosen': '0.081131', 'rewards_train/rejected': '0.052185', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028946', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-132.68', 'loss/train': '0.68695', 'examples_per_second': '30.572', 'grad_norm': '26.25', 'counters/examples': 157664, 'counters/updates': 4927}
train stats after 157696 examples: {'rewards_train/chosen': '0.066607', 'rewards_train/rejected': '0.068822', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0022149', 'logps_train/rejected': '-155.28', 'logps_train/chosen': '-142.99', 'loss/train': '0.70573', 'examples_per_second': '32.282', 'grad_norm': '30.125', 'counters/examples': 157696, 'counters/updates': 4928}
train stats after 157728 examples: {'rewards_train/chosen': '0.11373', 'rewards_train/rejected': '0.052538', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061195', 'logps_train/rejected': '-101.82', 'logps_train/chosen': '-151.48', 'loss/train': '0.67126', 'examples_per_second': '30.453', 'grad_norm': '30.5', 'counters/examples': 157728, 'counters/updates': 4929}
train stats after 157760 examples: {'rewards_train/chosen': '0.091786', 'rewards_train/rejected': '-0.03396', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12575', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-160', 'loss/train': '0.63737', 'examples_per_second': '31.443', 'grad_norm': '25.125', 'counters/examples': 157760, 'counters/updates': 4930}
train stats after 157792 examples: {'rewards_train/chosen': '0.12822', 'rewards_train/rejected': '-0.0060942', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13431', 'logps_train/rejected': '-105.38', 'logps_train/chosen': '-157.67', 'loss/train': '0.64022', 'examples_per_second': '30.486', 'grad_norm': '31.375', 'counters/examples': 157792, 'counters/updates': 4931}
train stats after 157824 examples: {'rewards_train/chosen': '0.18091', 'rewards_train/rejected': '0.10111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0798', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-164.92', 'loss/train': '0.66559', 'examples_per_second': '31.561', 'grad_norm': '26', 'counters/examples': 157824, 'counters/updates': 4932}
train stats after 157856 examples: {'rewards_train/chosen': '0.025584', 'rewards_train/rejected': '-0.026184', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051768', 'logps_train/rejected': '-109.76', 'logps_train/chosen': '-114.57', 'loss/train': '0.67674', 'examples_per_second': '32.111', 'grad_norm': '27.375', 'counters/examples': 157856, 'counters/updates': 4933}
train stats after 157888 examples: {'rewards_train/chosen': '0.076773', 'rewards_train/rejected': '0.05416', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022613', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-144.15', 'loss/train': '0.69589', 'examples_per_second': '31.796', 'grad_norm': '27.25', 'counters/examples': 157888, 'counters/updates': 4934}
skipping logging after 157920 examples to avoid logging too frequently
train stats after 157952 examples: {'rewards_train/chosen': '0.05744', 'rewards_train/rejected': '-0.043244', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10068', 'logps_train/rejected': '-108.16', 'logps_train/chosen': '-136.76', 'loss/train': '0.65742', 'examples_per_second': '32.044', 'grad_norm': '24.875', 'counters/examples': 157952, 'counters/updates': 4936}
train stats after 157984 examples: {'rewards_train/chosen': '0.13657', 'rewards_train/rejected': '0.047674', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088893', 'logps_train/rejected': '-126.15', 'logps_train/chosen': '-140.66', 'loss/train': '0.66253', 'examples_per_second': '30.711', 'grad_norm': '25.875', 'counters/examples': 157984, 'counters/updates': 4937}
train stats after 158016 examples: {'rewards_train/chosen': '0.1502', 'rewards_train/rejected': '0.035639', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11456', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-184.06', 'loss/train': '0.64886', 'examples_per_second': '30.101', 'grad_norm': '29.25', 'counters/examples': 158016, 'counters/updates': 4938}
skipping logging after 158048 examples to avoid logging too frequently
train stats after 158080 examples: {'rewards_train/chosen': '0.057041', 'rewards_train/rejected': '0.012752', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044289', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-144.27', 'loss/train': '0.68052', 'examples_per_second': '33.589', 'grad_norm': '30.125', 'counters/examples': 158080, 'counters/updates': 4940}
train stats after 158112 examples: {'rewards_train/chosen': '0.11671', 'rewards_train/rejected': '-0.027844', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14455', 'logps_train/rejected': '-121.11', 'logps_train/chosen': '-123.57', 'loss/train': '0.63511', 'examples_per_second': '31.063', 'grad_norm': '23.125', 'counters/examples': 158112, 'counters/updates': 4941}
skipping logging after 158144 examples to avoid logging too frequently
train stats after 158176 examples: {'rewards_train/chosen': '0.09147', 'rewards_train/rejected': '0.07306', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01841', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-118.79', 'loss/train': '0.69031', 'examples_per_second': '33.215', 'grad_norm': '26.625', 'counters/examples': 158176, 'counters/updates': 4943}
train stats after 158208 examples: {'rewards_train/chosen': '0.18464', 'rewards_train/rejected': '0.12349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061145', 'logps_train/rejected': '-162.03', 'logps_train/chosen': '-170.92', 'loss/train': '0.67582', 'examples_per_second': '31.1', 'grad_norm': '31.5', 'counters/examples': 158208, 'counters/updates': 4944}
train stats after 158240 examples: {'rewards_train/chosen': '0.16648', 'rewards_train/rejected': '0.0106', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.15588', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-181.17', 'loss/train': '0.62762', 'examples_per_second': '30.74', 'grad_norm': '26.625', 'counters/examples': 158240, 'counters/updates': 4945}
skipping logging after 158272 examples to avoid logging too frequently
train stats after 158304 examples: {'rewards_train/chosen': '0.09893', 'rewards_train/rejected': '0.035013', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.063917', 'logps_train/rejected': '-104.83', 'logps_train/chosen': '-129.89', 'loss/train': '0.67498', 'examples_per_second': '30.543', 'grad_norm': '23', 'counters/examples': 158304, 'counters/updates': 4947}
train stats after 158336 examples: {'rewards_train/chosen': '0.081813', 'rewards_train/rejected': '0.028947', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052866', 'logps_train/rejected': '-93.62', 'logps_train/chosen': '-131.74', 'loss/train': '0.67428', 'examples_per_second': '32.325', 'grad_norm': '26', 'counters/examples': 158336, 'counters/updates': 4948}
train stats after 158368 examples: {'rewards_train/chosen': '0.21685', 'rewards_train/rejected': '0.098069', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11878', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-158.18', 'loss/train': '0.6442', 'examples_per_second': '30.739', 'grad_norm': '29.625', 'counters/examples': 158368, 'counters/updates': 4949}
train stats after 158400 examples: {'rewards_train/chosen': '0.1951', 'rewards_train/rejected': '0.10234', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092753', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-149.85', 'loss/train': '0.65889', 'examples_per_second': '30.592', 'grad_norm': '25', 'counters/examples': 158400, 'counters/updates': 4950}
train stats after 158432 examples: {'rewards_train/chosen': '0.11799', 'rewards_train/rejected': '0.012569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10542', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-148.69', 'loss/train': '0.64931', 'examples_per_second': '31.455', 'grad_norm': '24.75', 'counters/examples': 158432, 'counters/updates': 4951}
train stats after 158464 examples: {'rewards_train/chosen': '0.15271', 'rewards_train/rejected': '0.071213', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081494', 'logps_train/rejected': '-152.85', 'logps_train/chosen': '-170.77', 'loss/train': '0.66267', 'examples_per_second': '30.965', 'grad_norm': '27', 'counters/examples': 158464, 'counters/updates': 4952}
train stats after 158496 examples: {'rewards_train/chosen': '0.11781', 'rewards_train/rejected': '0.0012966', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11652', 'logps_train/rejected': '-145.8', 'logps_train/chosen': '-159.57', 'loss/train': '0.65778', 'examples_per_second': '30.55', 'grad_norm': '31.125', 'counters/examples': 158496, 'counters/updates': 4953}
train stats after 158528 examples: {'rewards_train/chosen': '0.10396', 'rewards_train/rejected': '0.054423', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049537', 'logps_train/rejected': '-110.14', 'logps_train/chosen': '-167.88', 'loss/train': '0.68327', 'examples_per_second': '31.539', 'grad_norm': '27.125', 'counters/examples': 158528, 'counters/updates': 4954}
train stats after 158560 examples: {'rewards_train/chosen': '0.15169', 'rewards_train/rejected': '0.063723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087969', 'logps_train/rejected': '-94.897', 'logps_train/chosen': '-132.63', 'loss/train': '0.66201', 'examples_per_second': '31.545', 'grad_norm': '22.625', 'counters/examples': 158560, 'counters/updates': 4955}
skipping logging after 158592 examples to avoid logging too frequently
train stats after 158624 examples: {'rewards_train/chosen': '0.097038', 'rewards_train/rejected': '0.054099', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.042939', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-135.73', 'loss/train': '0.688', 'examples_per_second': '31.32', 'grad_norm': '30.75', 'counters/examples': 158624, 'counters/updates': 4957}
train stats after 158656 examples: {'rewards_train/chosen': '0.077739', 'rewards_train/rejected': '0.027821', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049919', 'logps_train/rejected': '-106.5', 'logps_train/chosen': '-105.92', 'loss/train': '0.68142', 'examples_per_second': '30.736', 'grad_norm': '22.5', 'counters/examples': 158656, 'counters/updates': 4958}
train stats after 158688 examples: {'rewards_train/chosen': '0.069956', 'rewards_train/rejected': '-0.012696', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082652', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-128.75', 'loss/train': '0.66069', 'examples_per_second': '30.93', 'grad_norm': '26.125', 'counters/examples': 158688, 'counters/updates': 4959}
train stats after 158720 examples: {'rewards_train/chosen': '0.19868', 'rewards_train/rejected': '0.057732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14094', 'logps_train/rejected': '-152.48', 'logps_train/chosen': '-182.63', 'loss/train': '0.64258', 'examples_per_second': '30.605', 'grad_norm': '32.75', 'counters/examples': 158720, 'counters/updates': 4960}
train stats after 158752 examples: {'rewards_train/chosen': '0.14319', 'rewards_train/rejected': '0.049374', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093813', 'logps_train/rejected': '-131.78', 'logps_train/chosen': '-169.05', 'loss/train': '0.6608', 'examples_per_second': '31.446', 'grad_norm': '34.25', 'counters/examples': 158752, 'counters/updates': 4961}
train stats after 158784 examples: {'rewards_train/chosen': '0.12643', 'rewards_train/rejected': '0.087753', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03868', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-139.67', 'loss/train': '0.68511', 'examples_per_second': '30.237', 'grad_norm': '25.75', 'counters/examples': 158784, 'counters/updates': 4962}
train stats after 158816 examples: {'rewards_train/chosen': '0.12056', 'rewards_train/rejected': '0.074365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04619', 'logps_train/rejected': '-119.33', 'logps_train/chosen': '-152.29', 'loss/train': '0.67892', 'examples_per_second': '31.468', 'grad_norm': '29.875', 'counters/examples': 158816, 'counters/updates': 4963}
skipping logging after 158848 examples to avoid logging too frequently
train stats after 158880 examples: {'rewards_train/chosen': '0.0054051', 'rewards_train/rejected': '0.021002', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015597', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-142.73', 'loss/train': '0.71445', 'examples_per_second': '31.494', 'grad_norm': '27.625', 'counters/examples': 158880, 'counters/updates': 4965}
train stats after 158912 examples: {'rewards_train/chosen': '0.12784', 'rewards_train/rejected': '0.02354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1043', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-139.98', 'loss/train': '0.65663', 'examples_per_second': '30.847', 'grad_norm': '24.125', 'counters/examples': 158912, 'counters/updates': 4966}
train stats after 158944 examples: {'rewards_train/chosen': '0.15079', 'rewards_train/rejected': '0.056471', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094324', 'logps_train/rejected': '-135.59', 'logps_train/chosen': '-142.79', 'loss/train': '0.66055', 'examples_per_second': '31.524', 'grad_norm': '38', 'counters/examples': 158944, 'counters/updates': 4967}
train stats after 158976 examples: {'rewards_train/chosen': '0.08004', 'rewards_train/rejected': '-0.00020539', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.080245', 'logps_train/rejected': '-145.97', 'logps_train/chosen': '-131.91', 'loss/train': '0.66867', 'examples_per_second': '31.846', 'grad_norm': '27.5', 'counters/examples': 158976, 'counters/updates': 4968}
train stats after 159008 examples: {'rewards_train/chosen': '0.13171', 'rewards_train/rejected': '0.11254', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019167', 'logps_train/rejected': '-139.35', 'logps_train/chosen': '-172.61', 'loss/train': '0.69289', 'examples_per_second': '30.562', 'grad_norm': '31.25', 'counters/examples': 159008, 'counters/updates': 4969}
skipping logging after 159040 examples to avoid logging too frequently
train stats after 159072 examples: {'rewards_train/chosen': '0.10801', 'rewards_train/rejected': '0.079619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028391', 'logps_train/rejected': '-100.35', 'logps_train/chosen': '-122.99', 'loss/train': '0.69726', 'examples_per_second': '31.57', 'grad_norm': '24.5', 'counters/examples': 159072, 'counters/updates': 4971}
train stats after 159104 examples: {'rewards_train/chosen': '0.085415', 'rewards_train/rejected': '0.058203', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027211', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-142.03', 'loss/train': '0.69188', 'examples_per_second': '31.519', 'grad_norm': '28.375', 'counters/examples': 159104, 'counters/updates': 4972}
train stats after 159136 examples: {'rewards_train/chosen': '0.092556', 'rewards_train/rejected': '0.061137', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03142', 'logps_train/rejected': '-133.54', 'logps_train/chosen': '-151.43', 'loss/train': '0.6829', 'examples_per_second': '29.839', 'grad_norm': '29.5', 'counters/examples': 159136, 'counters/updates': 4973}
skipping logging after 159168 examples to avoid logging too frequently
train stats after 159200 examples: {'rewards_train/chosen': '0.14922', 'rewards_train/rejected': '0.13231', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016911', 'logps_train/rejected': '-146.62', 'logps_train/chosen': '-157.04', 'loss/train': '0.69085', 'examples_per_second': '31.372', 'grad_norm': '31.75', 'counters/examples': 159200, 'counters/updates': 4975}
skipping logging after 159232 examples to avoid logging too frequently
train stats after 159264 examples: {'rewards_train/chosen': '0.11514', 'rewards_train/rejected': '-0.025548', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14069', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-157.22', 'loss/train': '0.63486', 'examples_per_second': '36.122', 'grad_norm': '24.25', 'counters/examples': 159264, 'counters/updates': 4977}
train stats after 159296 examples: {'rewards_train/chosen': '0.20883', 'rewards_train/rejected': '0.087845', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12098', 'logps_train/rejected': '-101.61', 'logps_train/chosen': '-144.9', 'loss/train': '0.65118', 'examples_per_second': '31.759', 'grad_norm': '23.5', 'counters/examples': 159296, 'counters/updates': 4978}
skipping logging after 159328 examples to avoid logging too frequently
train stats after 159360 examples: {'rewards_train/chosen': '0.039319', 'rewards_train/rejected': '0.088007', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.048688', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-135.16', 'loss/train': '0.73113', 'examples_per_second': '33.313', 'grad_norm': '37.25', 'counters/examples': 159360, 'counters/updates': 4980}
train stats after 159392 examples: {'rewards_train/chosen': '0.10979', 'rewards_train/rejected': '0.02111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088678', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-121.42', 'loss/train': '0.65636', 'examples_per_second': '31.594', 'grad_norm': '24.5', 'counters/examples': 159392, 'counters/updates': 4981}
train stats after 159424 examples: {'rewards_train/chosen': '0.1099', 'rewards_train/rejected': '0.02059', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089306', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-129.83', 'loss/train': '0.66386', 'examples_per_second': '30.986', 'grad_norm': '32.75', 'counters/examples': 159424, 'counters/updates': 4982}
train stats after 159456 examples: {'rewards_train/chosen': '0.13117', 'rewards_train/rejected': '-0.0018713', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13304', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-144.14', 'loss/train': '0.63484', 'examples_per_second': '30.369', 'grad_norm': '24.625', 'counters/examples': 159456, 'counters/updates': 4983}
train stats after 159488 examples: {'rewards_train/chosen': '0.079773', 'rewards_train/rejected': '0.10467', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.024894', 'logps_train/rejected': '-117.55', 'logps_train/chosen': '-102.16', 'loss/train': '0.71517', 'examples_per_second': '31.915', 'grad_norm': '27.5', 'counters/examples': 159488, 'counters/updates': 4984}
skipping logging after 159520 examples to avoid logging too frequently
train stats after 159552 examples: {'rewards_train/chosen': '0.23474', 'rewards_train/rejected': '0.040114', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19463', 'logps_train/rejected': '-147.44', 'logps_train/chosen': '-171.71', 'loss/train': '0.61554', 'examples_per_second': '30.55', 'grad_norm': '25.25', 'counters/examples': 159552, 'counters/updates': 4986}
train stats after 159584 examples: {'rewards_train/chosen': '0.071415', 'rewards_train/rejected': '0.033664', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037751', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-135.34', 'loss/train': '0.67915', 'examples_per_second': '31.28', 'grad_norm': '25.625', 'counters/examples': 159584, 'counters/updates': 4987}
train stats after 159616 examples: {'rewards_train/chosen': '0.097299', 'rewards_train/rejected': '-0.018724', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11602', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-147.82', 'loss/train': '0.64748', 'examples_per_second': '30.373', 'grad_norm': '27.375', 'counters/examples': 159616, 'counters/updates': 4988}
train stats after 159648 examples: {'rewards_train/chosen': '0.19611', 'rewards_train/rejected': '0.095461', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10065', 'logps_train/rejected': '-139.37', 'logps_train/chosen': '-129.71', 'loss/train': '0.65916', 'examples_per_second': '30.096', 'grad_norm': '30.875', 'counters/examples': 159648, 'counters/updates': 4989}
train stats after 159680 examples: {'rewards_train/chosen': '0.02432', 'rewards_train/rejected': '0.025912', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0015925', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-129.21', 'loss/train': '0.70092', 'examples_per_second': '31.583', 'grad_norm': '31.125', 'counters/examples': 159680, 'counters/updates': 4990}
skipping logging after 159712 examples to avoid logging too frequently
train stats after 159744 examples: {'rewards_train/chosen': '0.089773', 'rewards_train/rejected': '0.12596', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036188', 'logps_train/rejected': '-106.4', 'logps_train/chosen': '-136.99', 'loss/train': '0.71915', 'examples_per_second': '31.937', 'grad_norm': '26.625', 'counters/examples': 159744, 'counters/updates': 4992}
train stats after 159776 examples: {'rewards_train/chosen': '0.14177', 'rewards_train/rejected': '0.068134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073635', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-151.04', 'loss/train': '0.6683', 'examples_per_second': '32.448', 'grad_norm': '25.25', 'counters/examples': 159776, 'counters/updates': 4993}
train stats after 159808 examples: {'rewards_train/chosen': '0.099541', 'rewards_train/rejected': '-0.037571', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13711', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-132.32', 'loss/train': '0.63847', 'examples_per_second': '31.567', 'grad_norm': '27.875', 'counters/examples': 159808, 'counters/updates': 4994}
train stats after 159840 examples: {'rewards_train/chosen': '0.0812', 'rewards_train/rejected': '-0.042743', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12394', 'logps_train/rejected': '-114.89', 'logps_train/chosen': '-162.8', 'loss/train': '0.64358', 'examples_per_second': '31.404', 'grad_norm': '24.75', 'counters/examples': 159840, 'counters/updates': 4995}
train stats after 159872 examples: {'rewards_train/chosen': '0.08363', 'rewards_train/rejected': '0.046998', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.036632', 'logps_train/rejected': '-147.14', 'logps_train/chosen': '-144.33', 'loss/train': '0.6924', 'examples_per_second': '31.568', 'grad_norm': '27', 'counters/examples': 159872, 'counters/updates': 4996}
train stats after 159904 examples: {'rewards_train/chosen': '0.072111', 'rewards_train/rejected': '-0.0096561', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081767', 'logps_train/rejected': '-135.3', 'logps_train/chosen': '-103.65', 'loss/train': '0.66286', 'examples_per_second': '32.584', 'grad_norm': '26.75', 'counters/examples': 159904, 'counters/updates': 4997}
train stats after 159936 examples: {'rewards_train/chosen': '0.059074', 'rewards_train/rejected': '0.048737', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010338', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-132.77', 'loss/train': '0.70061', 'examples_per_second': '29.994', 'grad_norm': '26.875', 'counters/examples': 159936, 'counters/updates': 4998}
train stats after 159968 examples: {'rewards_train/chosen': '0.1058', 'rewards_train/rejected': '0.040545', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065259', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-115.59', 'loss/train': '0.66847', 'examples_per_second': '30.905', 'grad_norm': '24.125', 'counters/examples': 159968, 'counters/updates': 4999}
train stats after 160000 examples: {'rewards_train/chosen': '0.12377', 'rewards_train/rejected': '0.0016038', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12216', 'logps_train/rejected': '-162.18', 'logps_train/chosen': '-162.35', 'loss/train': '0.64555', 'examples_per_second': '32.863', 'grad_norm': '28.75', 'counters/examples': 160000, 'counters/updates': 5000}
train stats after 160032 examples: {'rewards_train/chosen': '0.074789', 'rewards_train/rejected': '-0.01041', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.085199', 'logps_train/rejected': '-138.99', 'logps_train/chosen': '-143.34', 'loss/train': '0.65934', 'examples_per_second': '31.513', 'grad_norm': '27.25', 'counters/examples': 160032, 'counters/updates': 5001}
train stats after 160064 examples: {'rewards_train/chosen': '0.14776', 'rewards_train/rejected': '0.083391', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064374', 'logps_train/rejected': '-137.03', 'logps_train/chosen': '-166.61', 'loss/train': '0.67687', 'examples_per_second': '31.512', 'grad_norm': '30.375', 'counters/examples': 160064, 'counters/updates': 5002}
skipping logging after 160096 examples to avoid logging too frequently
train stats after 160128 examples: {'rewards_train/chosen': '0.07857', 'rewards_train/rejected': '0.041206', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037364', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-113.32', 'loss/train': '0.68534', 'examples_per_second': '30.061', 'grad_norm': '28.375', 'counters/examples': 160128, 'counters/updates': 5004}
train stats after 160160 examples: {'rewards_train/chosen': '0.031404', 'rewards_train/rejected': '-0.0095326', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040936', 'logps_train/rejected': '-118.96', 'logps_train/chosen': '-186.86', 'loss/train': '0.683', 'examples_per_second': '31.569', 'grad_norm': '31.25', 'counters/examples': 160160, 'counters/updates': 5005}
train stats after 160192 examples: {'rewards_train/chosen': '0.23942', 'rewards_train/rejected': '0.11103', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12839', 'logps_train/rejected': '-135.2', 'logps_train/chosen': '-156.79', 'loss/train': '0.64701', 'examples_per_second': '31.551', 'grad_norm': '28.25', 'counters/examples': 160192, 'counters/updates': 5006}
train stats after 160224 examples: {'rewards_train/chosen': '0.1587', 'rewards_train/rejected': '-0.017247', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17594', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-158.14', 'loss/train': '0.6295', 'examples_per_second': '32.293', 'grad_norm': '27.375', 'counters/examples': 160224, 'counters/updates': 5007}
train stats after 160256 examples: {'rewards_train/chosen': '0.16463', 'rewards_train/rejected': '0.027691', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13694', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-191.82', 'loss/train': '0.638', 'examples_per_second': '32.084', 'grad_norm': '28.75', 'counters/examples': 160256, 'counters/updates': 5008}
train stats after 160288 examples: {'rewards_train/chosen': '0.14112', 'rewards_train/rejected': '0.00019214', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14093', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-141.1', 'loss/train': '0.63744', 'examples_per_second': '32.488', 'grad_norm': '25.375', 'counters/examples': 160288, 'counters/updates': 5009}
train stats after 160320 examples: {'rewards_train/chosen': '0.13575', 'rewards_train/rejected': '0.030429', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10532', 'logps_train/rejected': '-113.45', 'logps_train/chosen': '-153.82', 'loss/train': '0.6609', 'examples_per_second': '31.133', 'grad_norm': '27', 'counters/examples': 160320, 'counters/updates': 5010}
train stats after 160352 examples: {'rewards_train/chosen': '0.25072', 'rewards_train/rejected': '0.10114', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14958', 'logps_train/rejected': '-152.24', 'logps_train/chosen': '-159.24', 'loss/train': '0.64737', 'examples_per_second': '22.976', 'grad_norm': '27', 'counters/examples': 160352, 'counters/updates': 5011}
train stats after 160384 examples: {'rewards_train/chosen': '0.16458', 'rewards_train/rejected': '0.017893', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14669', 'logps_train/rejected': '-97.83', 'logps_train/chosen': '-150.52', 'loss/train': '0.63354', 'examples_per_second': '30.204', 'grad_norm': '24.75', 'counters/examples': 160384, 'counters/updates': 5012}
train stats after 160416 examples: {'rewards_train/chosen': '0.11328', 'rewards_train/rejected': '0.022188', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091088', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-142.11', 'loss/train': '0.65564', 'examples_per_second': '32.287', 'grad_norm': '26.75', 'counters/examples': 160416, 'counters/updates': 5013}
skipping logging after 160448 examples to avoid logging too frequently
train stats after 160480 examples: {'rewards_train/chosen': '0.10413', 'rewards_train/rejected': '0.040548', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063579', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-152.83', 'loss/train': '0.68194', 'examples_per_second': '31.02', 'grad_norm': '45.25', 'counters/examples': 160480, 'counters/updates': 5015}
train stats after 160512 examples: {'rewards_train/chosen': '0.096218', 'rewards_train/rejected': '0.059714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036504', 'logps_train/rejected': '-85.159', 'logps_train/chosen': '-125.55', 'loss/train': '0.68047', 'examples_per_second': '31.478', 'grad_norm': '24', 'counters/examples': 160512, 'counters/updates': 5016}
train stats after 160544 examples: {'rewards_train/chosen': '0.20207', 'rewards_train/rejected': '0.10651', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09556', 'logps_train/rejected': '-124.15', 'logps_train/chosen': '-139.18', 'loss/train': '0.65526', 'examples_per_second': '31.468', 'grad_norm': '26.875', 'counters/examples': 160544, 'counters/updates': 5017}
skipping logging after 160576 examples to avoid logging too frequently
train stats after 160608 examples: {'rewards_train/chosen': '0.19693', 'rewards_train/rejected': '0.12298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073946', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-120.16', 'loss/train': '0.67079', 'examples_per_second': '31.532', 'grad_norm': '25.125', 'counters/examples': 160608, 'counters/updates': 5019}
train stats after 160640 examples: {'rewards_train/chosen': '0.086886', 'rewards_train/rejected': '0.07341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013476', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-144.28', 'loss/train': '0.69227', 'examples_per_second': '31.059', 'grad_norm': '27.5', 'counters/examples': 160640, 'counters/updates': 5020}
train stats after 160672 examples: {'rewards_train/chosen': '0.12486', 'rewards_train/rejected': '0.073275', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051584', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-117.56', 'loss/train': '0.67775', 'examples_per_second': '31.587', 'grad_norm': '27.125', 'counters/examples': 160672, 'counters/updates': 5021}
train stats after 160704 examples: {'rewards_train/chosen': '0.11812', 'rewards_train/rejected': '0.077971', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040153', 'logps_train/rejected': '-153.52', 'logps_train/chosen': '-145.88', 'loss/train': '0.69219', 'examples_per_second': '31.184', 'grad_norm': '28.375', 'counters/examples': 160704, 'counters/updates': 5022}
train stats after 160736 examples: {'rewards_train/chosen': '0.13543', 'rewards_train/rejected': '0.053091', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082338', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-113.94', 'loss/train': '0.66668', 'examples_per_second': '30.88', 'grad_norm': '25.75', 'counters/examples': 160736, 'counters/updates': 5023}
train stats after 160768 examples: {'rewards_train/chosen': '0.054439', 'rewards_train/rejected': '-0.073519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12796', 'logps_train/rejected': '-104.72', 'logps_train/chosen': '-115.21', 'loss/train': '0.63888', 'examples_per_second': '30.062', 'grad_norm': '21.75', 'counters/examples': 160768, 'counters/updates': 5024}
train stats after 160800 examples: {'rewards_train/chosen': '0.14994', 'rewards_train/rejected': '0.034217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11572', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-112.01', 'loss/train': '0.64927', 'examples_per_second': '32.917', 'grad_norm': '23', 'counters/examples': 160800, 'counters/updates': 5025}
skipping logging after 160832 examples to avoid logging too frequently
train stats after 160864 examples: {'rewards_train/chosen': '0.15647', 'rewards_train/rejected': '0.014801', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14167', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-172.77', 'loss/train': '0.63121', 'examples_per_second': '34.214', 'grad_norm': '26.75', 'counters/examples': 160864, 'counters/updates': 5027}
train stats after 160896 examples: {'rewards_train/chosen': '0.074227', 'rewards_train/rejected': '0.027627', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0466', 'logps_train/rejected': '-143.02', 'logps_train/chosen': '-148.01', 'loss/train': '0.67991', 'examples_per_second': '31.274', 'grad_norm': '29.25', 'counters/examples': 160896, 'counters/updates': 5028}
train stats after 160928 examples: {'rewards_train/chosen': '0.030275', 'rewards_train/rejected': '-0.028584', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058859', 'logps_train/rejected': '-124.89', 'logps_train/chosen': '-116.42', 'loss/train': '0.67044', 'examples_per_second': '31.746', 'grad_norm': '27.5', 'counters/examples': 160928, 'counters/updates': 5029}
skipping logging after 160960 examples to avoid logging too frequently
train stats after 160992 examples: {'rewards_train/chosen': '0.12014', 'rewards_train/rejected': '-0.033342', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15348', 'logps_train/rejected': '-145.99', 'logps_train/chosen': '-165.71', 'loss/train': '0.63796', 'examples_per_second': '32.442', 'grad_norm': '26.375', 'counters/examples': 160992, 'counters/updates': 5031}
train stats after 161024 examples: {'rewards_train/chosen': '0.13235', 'rewards_train/rejected': '0.052465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079889', 'logps_train/rejected': '-137.98', 'logps_train/chosen': '-167.82', 'loss/train': '0.67', 'examples_per_second': '31.517', 'grad_norm': '29.125', 'counters/examples': 161024, 'counters/updates': 5032}
train stats after 161056 examples: {'rewards_train/chosen': '0.16649', 'rewards_train/rejected': '0.0027049', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16378', 'logps_train/rejected': '-103.95', 'logps_train/chosen': '-136.69', 'loss/train': '0.62395', 'examples_per_second': '30.739', 'grad_norm': '22.25', 'counters/examples': 161056, 'counters/updates': 5033}
train stats after 161088 examples: {'rewards_train/chosen': '0.11498', 'rewards_train/rejected': '-0.010837', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12581', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-138.88', 'loss/train': '0.64459', 'examples_per_second': '32.457', 'grad_norm': '25.875', 'counters/examples': 161088, 'counters/updates': 5034}
train stats after 161120 examples: {'rewards_train/chosen': '0.1083', 'rewards_train/rejected': '0.085539', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022762', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-150.56', 'loss/train': '0.69704', 'examples_per_second': '30.291', 'grad_norm': '29.625', 'counters/examples': 161120, 'counters/updates': 5035}
train stats after 161152 examples: {'rewards_train/chosen': '0.13616', 'rewards_train/rejected': '0.042517', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09364', 'logps_train/rejected': '-128.51', 'logps_train/chosen': '-161.97', 'loss/train': '0.65866', 'examples_per_second': '31.675', 'grad_norm': '26.125', 'counters/examples': 161152, 'counters/updates': 5036}
skipping logging after 161184 examples to avoid logging too frequently
train stats after 161216 examples: {'rewards_train/chosen': '0.043518', 'rewards_train/rejected': '0.050475', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0069574', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-120.8', 'loss/train': '0.70617', 'examples_per_second': '32.508', 'grad_norm': '28.25', 'counters/examples': 161216, 'counters/updates': 5038}
train stats after 161248 examples: {'rewards_train/chosen': '0.14696', 'rewards_train/rejected': '0.092144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05482', 'logps_train/rejected': '-125.54', 'logps_train/chosen': '-123.95', 'loss/train': '0.67432', 'examples_per_second': '31.554', 'grad_norm': '25.875', 'counters/examples': 161248, 'counters/updates': 5039}
train stats after 161280 examples: {'rewards_train/chosen': '0.1042', 'rewards_train/rejected': '-0.0098144', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11401', 'logps_train/rejected': '-109.47', 'logps_train/chosen': '-102.04', 'loss/train': '0.64684', 'examples_per_second': '31.753', 'grad_norm': '22.125', 'counters/examples': 161280, 'counters/updates': 5040}
train stats after 161312 examples: {'rewards_train/chosen': '0.10633', 'rewards_train/rejected': '0.060191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046138', 'logps_train/rejected': '-102.43', 'logps_train/chosen': '-117.2', 'loss/train': '0.67639', 'examples_per_second': '30.535', 'grad_norm': '26.875', 'counters/examples': 161312, 'counters/updates': 5041}
train stats after 161344 examples: {'rewards_train/chosen': '0.078344', 'rewards_train/rejected': '-0.03521', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11355', 'logps_train/rejected': '-100.64', 'logps_train/chosen': '-141.47', 'loss/train': '0.64343', 'examples_per_second': '30.801', 'grad_norm': '22.75', 'counters/examples': 161344, 'counters/updates': 5042}
train stats after 161376 examples: {'rewards_train/chosen': '0.10477', 'rewards_train/rejected': '-0.031259', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13603', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-152.42', 'loss/train': '0.6383', 'examples_per_second': '32.786', 'grad_norm': '28.5', 'counters/examples': 161376, 'counters/updates': 5043}
train stats after 161408 examples: {'rewards_train/chosen': '0.024843', 'rewards_train/rejected': '-0.088545', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11339', 'logps_train/rejected': '-106.57', 'logps_train/chosen': '-123.61', 'loss/train': '0.64747', 'examples_per_second': '32.357', 'grad_norm': '28.625', 'counters/examples': 161408, 'counters/updates': 5044}
train stats after 161440 examples: {'rewards_train/chosen': '0.16917', 'rewards_train/rejected': '0.13194', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03723', 'logps_train/rejected': '-139.28', 'logps_train/chosen': '-165.31', 'loss/train': '0.68936', 'examples_per_second': '31.359', 'grad_norm': '28.125', 'counters/examples': 161440, 'counters/updates': 5045}
train stats after 161472 examples: {'rewards_train/chosen': '0.19017', 'rewards_train/rejected': '-0.027046', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21722', 'logps_train/rejected': '-148.01', 'logps_train/chosen': '-156.36', 'loss/train': '0.61131', 'examples_per_second': '31.474', 'grad_norm': '23.875', 'counters/examples': 161472, 'counters/updates': 5046}
train stats after 161504 examples: {'rewards_train/chosen': '0.11042', 'rewards_train/rejected': '0.092393', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018031', 'logps_train/rejected': '-134.23', 'logps_train/chosen': '-135.91', 'loss/train': '0.69599', 'examples_per_second': '33.006', 'grad_norm': '33', 'counters/examples': 161504, 'counters/updates': 5047}
train stats after 161536 examples: {'rewards_train/chosen': '0.13682', 'rewards_train/rejected': '0.084797', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05202', 'logps_train/rejected': '-113.73', 'logps_train/chosen': '-128.7', 'loss/train': '0.68214', 'examples_per_second': '31.95', 'grad_norm': '29.875', 'counters/examples': 161536, 'counters/updates': 5048}
train stats after 161568 examples: {'rewards_train/chosen': '0.1191', 'rewards_train/rejected': '0.021044', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098059', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-147.55', 'loss/train': '0.65257', 'examples_per_second': '31.558', 'grad_norm': '29.125', 'counters/examples': 161568, 'counters/updates': 5049}
train stats after 161600 examples: {'rewards_train/chosen': '0.1638', 'rewards_train/rejected': '0.041133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12266', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-111.62', 'loss/train': '0.64728', 'examples_per_second': '30.561', 'grad_norm': '24.25', 'counters/examples': 161600, 'counters/updates': 5050}
train stats after 161632 examples: {'rewards_train/chosen': '0.17896', 'rewards_train/rejected': '0.082473', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096486', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-151.83', 'loss/train': '0.65729', 'examples_per_second': '31.089', 'grad_norm': '25.5', 'counters/examples': 161632, 'counters/updates': 5051}
train stats after 161664 examples: {'rewards_train/chosen': '0.21174', 'rewards_train/rejected': '0.057037', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15471', 'logps_train/rejected': '-107.47', 'logps_train/chosen': '-140.68', 'loss/train': '0.63567', 'examples_per_second': '31.069', 'grad_norm': '24.5', 'counters/examples': 161664, 'counters/updates': 5052}
train stats after 161696 examples: {'rewards_train/chosen': '0.23303', 'rewards_train/rejected': '0.10013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13289', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-156.17', 'loss/train': '0.6437', 'examples_per_second': '30.037', 'grad_norm': '26.5', 'counters/examples': 161696, 'counters/updates': 5053}
train stats after 161728 examples: {'rewards_train/chosen': '0.11458', 'rewards_train/rejected': '0.075447', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039131', 'logps_train/rejected': '-105.08', 'logps_train/chosen': '-156.08', 'loss/train': '0.68123', 'examples_per_second': '31.381', 'grad_norm': '24.625', 'counters/examples': 161728, 'counters/updates': 5054}
train stats after 161760 examples: {'rewards_train/chosen': '0.15093', 'rewards_train/rejected': '0.012255', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13868', 'logps_train/rejected': '-150.55', 'logps_train/chosen': '-169.07', 'loss/train': '0.63548', 'examples_per_second': '31.576', 'grad_norm': '29', 'counters/examples': 161760, 'counters/updates': 5055}
skipping logging after 161792 examples to avoid logging too frequently
train stats after 161824 examples: {'rewards_train/chosen': '0.20475', 'rewards_train/rejected': '0.042803', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16194', 'logps_train/rejected': '-132.26', 'logps_train/chosen': '-135.42', 'loss/train': '0.62459', 'examples_per_second': '35.726', 'grad_norm': '25.625', 'counters/examples': 161824, 'counters/updates': 5057}
train stats after 161856 examples: {'rewards_train/chosen': '0.11752', 'rewards_train/rejected': '0.075339', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042185', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-127.75', 'loss/train': '0.68296', 'examples_per_second': '30.062', 'grad_norm': '29.5', 'counters/examples': 161856, 'counters/updates': 5058}
train stats after 161888 examples: {'rewards_train/chosen': '-0.062941', 'rewards_train/rejected': '-0.026054', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036888', 'logps_train/rejected': '-113.92', 'logps_train/chosen': '-146.9', 'loss/train': '0.72479', 'examples_per_second': '31.666', 'grad_norm': '41.75', 'counters/examples': 161888, 'counters/updates': 5059}
skipping logging after 161920 examples to avoid logging too frequently
train stats after 161952 examples: {'rewards_train/chosen': '0.15016', 'rewards_train/rejected': '0.10752', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042643', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-123.99', 'loss/train': '0.68254', 'examples_per_second': '31.586', 'grad_norm': '24.875', 'counters/examples': 161952, 'counters/updates': 5061}
train stats after 161984 examples: {'rewards_train/chosen': '0.16359', 'rewards_train/rejected': '0.074048', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089545', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-163.37', 'loss/train': '0.6601', 'examples_per_second': '32.271', 'grad_norm': '28', 'counters/examples': 161984, 'counters/updates': 5062}
train stats after 162016 examples: {'rewards_train/chosen': '0.082853', 'rewards_train/rejected': '0.0082605', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074592', 'logps_train/rejected': '-115.04', 'logps_train/chosen': '-131.49', 'loss/train': '0.66755', 'examples_per_second': '31.474', 'grad_norm': '27.25', 'counters/examples': 162016, 'counters/updates': 5063}
train stats after 162048 examples: {'rewards_train/chosen': '0.097332', 'rewards_train/rejected': '-0.043646', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14098', 'logps_train/rejected': '-116.99', 'logps_train/chosen': '-169.12', 'loss/train': '0.63407', 'examples_per_second': '31.557', 'grad_norm': '23', 'counters/examples': 162048, 'counters/updates': 5064}
train stats after 162080 examples: {'rewards_train/chosen': '0.14361', 'rewards_train/rejected': '0.025036', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11857', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-168.44', 'loss/train': '0.64591', 'examples_per_second': '31.551', 'grad_norm': '29.75', 'counters/examples': 162080, 'counters/updates': 5065}
train stats after 162112 examples: {'rewards_train/chosen': '0.19169', 'rewards_train/rejected': '0.051611', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14008', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-149.34', 'loss/train': '0.63486', 'examples_per_second': '30.524', 'grad_norm': '24.125', 'counters/examples': 162112, 'counters/updates': 5066}
train stats after 162144 examples: {'rewards_train/chosen': '0.12312', 'rewards_train/rejected': '-0.043918', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16704', 'logps_train/rejected': '-104.5', 'logps_train/chosen': '-124.55', 'loss/train': '0.62005', 'examples_per_second': '32.458', 'grad_norm': '22.25', 'counters/examples': 162144, 'counters/updates': 5067}
train stats after 162176 examples: {'rewards_train/chosen': '0.15379', 'rewards_train/rejected': '-0.00611', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1599', 'logps_train/rejected': '-105.57', 'logps_train/chosen': '-130.9', 'loss/train': '0.62355', 'examples_per_second': '31.594', 'grad_norm': '23.75', 'counters/examples': 162176, 'counters/updates': 5068}
train stats after 162208 examples: {'rewards_train/chosen': '0.047173', 'rewards_train/rejected': '0.034364', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012809', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-103.61', 'loss/train': '0.69855', 'examples_per_second': '30.113', 'grad_norm': '28.125', 'counters/examples': 162208, 'counters/updates': 5069}
skipping logging after 162240 examples to avoid logging too frequently
train stats after 162272 examples: {'rewards_train/chosen': '0.088531', 'rewards_train/rejected': '0.0092277', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079303', 'logps_train/rejected': '-104.19', 'logps_train/chosen': '-153.02', 'loss/train': '0.66333', 'examples_per_second': '32.871', 'grad_norm': '25.125', 'counters/examples': 162272, 'counters/updates': 5071}
train stats after 162304 examples: {'rewards_train/chosen': '0.11104', 'rewards_train/rejected': '0.0034428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1076', 'logps_train/rejected': '-90.807', 'logps_train/chosen': '-125.16', 'loss/train': '0.64799', 'examples_per_second': '30.446', 'grad_norm': '24', 'counters/examples': 162304, 'counters/updates': 5072}
train stats after 162336 examples: {'rewards_train/chosen': '0.11985', 'rewards_train/rejected': '-0.0071009', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12695', 'logps_train/rejected': '-98.33', 'logps_train/chosen': '-147.03', 'loss/train': '0.64864', 'examples_per_second': '32.693', 'grad_norm': '29.625', 'counters/examples': 162336, 'counters/updates': 5073}
skipping logging after 162368 examples to avoid logging too frequently
train stats after 162400 examples: {'rewards_train/chosen': '0.075104', 'rewards_train/rejected': '-0.0010625', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076166', 'logps_train/rejected': '-94.283', 'logps_train/chosen': '-127.89', 'loss/train': '0.66601', 'examples_per_second': '34.595', 'grad_norm': '23.25', 'counters/examples': 162400, 'counters/updates': 5075}
train stats after 162432 examples: {'rewards_train/chosen': '0.10764', 'rewards_train/rejected': '0.071525', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036118', 'logps_train/rejected': '-119.12', 'logps_train/chosen': '-120.24', 'loss/train': '0.68352', 'examples_per_second': '32.122', 'grad_norm': '26', 'counters/examples': 162432, 'counters/updates': 5076}
skipping logging after 162464 examples to avoid logging too frequently
train stats after 162496 examples: {'rewards_train/chosen': '0.11871', 'rewards_train/rejected': '-0.081771', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20048', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-136.38', 'loss/train': '0.6049', 'examples_per_second': '31.702', 'grad_norm': '22.625', 'counters/examples': 162496, 'counters/updates': 5078}
train stats after 162528 examples: {'rewards_train/chosen': '0.16929', 'rewards_train/rejected': '-0.027515', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19681', 'logps_train/rejected': '-130.09', 'logps_train/chosen': '-135.7', 'loss/train': '0.61127', 'examples_per_second': '31.545', 'grad_norm': '25.875', 'counters/examples': 162528, 'counters/updates': 5079}
train stats after 162560 examples: {'rewards_train/chosen': '0.12193', 'rewards_train/rejected': '0.054808', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067127', 'logps_train/rejected': '-142.23', 'logps_train/chosen': '-142.34', 'loss/train': '0.6753', 'examples_per_second': '31.63', 'grad_norm': '27.875', 'counters/examples': 162560, 'counters/updates': 5080}
train stats after 162592 examples: {'rewards_train/chosen': '0.1341', 'rewards_train/rejected': '0.080991', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053112', 'logps_train/rejected': '-137.62', 'logps_train/chosen': '-155.11', 'loss/train': '0.67521', 'examples_per_second': '31.785', 'grad_norm': '28.25', 'counters/examples': 162592, 'counters/updates': 5081}
skipping logging after 162624 examples to avoid logging too frequently
train stats after 162656 examples: {'rewards_train/chosen': '0.12942', 'rewards_train/rejected': '0.010078', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11934', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-178.39', 'loss/train': '0.64404', 'examples_per_second': '31.281', 'grad_norm': '28.5', 'counters/examples': 162656, 'counters/updates': 5083}
train stats after 162688 examples: {'rewards_train/chosen': '0.18634', 'rewards_train/rejected': '0.06186', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12448', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-119.27', 'loss/train': '0.64266', 'examples_per_second': '30.147', 'grad_norm': '33.5', 'counters/examples': 162688, 'counters/updates': 5084}
train stats after 162720 examples: {'rewards_train/chosen': '0.1438', 'rewards_train/rejected': '0.098934', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044862', 'logps_train/rejected': '-142.92', 'logps_train/chosen': '-147.88', 'loss/train': '0.68135', 'examples_per_second': '31.51', 'grad_norm': '27.25', 'counters/examples': 162720, 'counters/updates': 5085}
train stats after 162752 examples: {'rewards_train/chosen': '0.091239', 'rewards_train/rejected': '0.042166', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049073', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-102.76', 'loss/train': '0.67647', 'examples_per_second': '32.417', 'grad_norm': '23.625', 'counters/examples': 162752, 'counters/updates': 5086}
train stats after 162784 examples: {'rewards_train/chosen': '0.13078', 'rewards_train/rejected': '0.0055419', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12523', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-195.07', 'loss/train': '0.65434', 'examples_per_second': '31.229', 'grad_norm': '28.5', 'counters/examples': 162784, 'counters/updates': 5087}
train stats after 162816 examples: {'rewards_train/chosen': '0.17344', 'rewards_train/rejected': '0.060422', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11302', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-125.3', 'loss/train': '0.65416', 'examples_per_second': '31.629', 'grad_norm': '23.75', 'counters/examples': 162816, 'counters/updates': 5088}
train stats after 162848 examples: {'rewards_train/chosen': '0.10623', 'rewards_train/rejected': '0.04943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056797', 'logps_train/rejected': '-121.61', 'logps_train/chosen': '-120.87', 'loss/train': '0.67542', 'examples_per_second': '30.562', 'grad_norm': '27.5', 'counters/examples': 162848, 'counters/updates': 5089}
train stats after 162880 examples: {'rewards_train/chosen': '0.15587', 'rewards_train/rejected': '0.032986', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12288', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-147.22', 'loss/train': '0.64627', 'examples_per_second': '23.529', 'grad_norm': '28', 'counters/examples': 162880, 'counters/updates': 5090}
train stats after 162912 examples: {'rewards_train/chosen': '0.13732', 'rewards_train/rejected': '0.039338', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097978', 'logps_train/rejected': '-132.24', 'logps_train/chosen': '-139.69', 'loss/train': '0.66054', 'examples_per_second': '31.632', 'grad_norm': '26', 'counters/examples': 162912, 'counters/updates': 5091}
skipping logging after 162944 examples to avoid logging too frequently
train stats after 162976 examples: {'rewards_train/chosen': '0.12824', 'rewards_train/rejected': '0.056678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071557', 'logps_train/rejected': '-113.34', 'logps_train/chosen': '-147.87', 'loss/train': '0.66984', 'examples_per_second': '23.399', 'grad_norm': '24.125', 'counters/examples': 162976, 'counters/updates': 5093}
train stats after 163008 examples: {'rewards_train/chosen': '0.13715', 'rewards_train/rejected': '0.050008', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087137', 'logps_train/rejected': '-112.49', 'logps_train/chosen': '-152.3', 'loss/train': '0.65964', 'examples_per_second': '32.429', 'grad_norm': '29', 'counters/examples': 163008, 'counters/updates': 5094}
train stats after 163040 examples: {'rewards_train/chosen': '0.1237', 'rewards_train/rejected': '0.025791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.097909', 'logps_train/rejected': '-101.23', 'logps_train/chosen': '-142.7', 'loss/train': '0.65485', 'examples_per_second': '31.168', 'grad_norm': '25.25', 'counters/examples': 163040, 'counters/updates': 5095}
train stats after 163072 examples: {'rewards_train/chosen': '0.092032', 'rewards_train/rejected': '-0.013924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10596', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-127.12', 'loss/train': '0.65553', 'examples_per_second': '31.039', 'grad_norm': '24.5', 'counters/examples': 163072, 'counters/updates': 5096}
skipping logging after 163104 examples to avoid logging too frequently
train stats after 163136 examples: {'rewards_train/chosen': '0.13839', 'rewards_train/rejected': '0.13519', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0031956', 'logps_train/rejected': '-157.15', 'logps_train/chosen': '-166.26', 'loss/train': '0.70628', 'examples_per_second': '31.415', 'grad_norm': '42', 'counters/examples': 163136, 'counters/updates': 5098}
skipping logging after 163168 examples to avoid logging too frequently
train stats after 163200 examples: {'rewards_train/chosen': '0.17934', 'rewards_train/rejected': '0.042173', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13717', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-143.47', 'loss/train': '0.64252', 'examples_per_second': '30.158', 'grad_norm': '25', 'counters/examples': 163200, 'counters/updates': 5100}
train stats after 163232 examples: {'rewards_train/chosen': '0.10927', 'rewards_train/rejected': '0.047426', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061843', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-164.2', 'loss/train': '0.67565', 'examples_per_second': '30.351', 'grad_norm': '29.875', 'counters/examples': 163232, 'counters/updates': 5101}
skipping logging after 163264 examples to avoid logging too frequently
train stats after 163296 examples: {'rewards_train/chosen': '0.07094', 'rewards_train/rejected': '0.045445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025495', 'logps_train/rejected': '-96.125', 'logps_train/chosen': '-136.81', 'loss/train': '0.69116', 'examples_per_second': '31.673', 'grad_norm': '22.875', 'counters/examples': 163296, 'counters/updates': 5103}
train stats after 163328 examples: {'rewards_train/chosen': '0.13003', 'rewards_train/rejected': '0.040891', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08914', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-130.58', 'loss/train': '0.65897', 'examples_per_second': '31.939', 'grad_norm': '29', 'counters/examples': 163328, 'counters/updates': 5104}
train stats after 163360 examples: {'rewards_train/chosen': '0.17073', 'rewards_train/rejected': '0.044251', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12648', 'logps_train/rejected': '-121.84', 'logps_train/chosen': '-131.03', 'loss/train': '0.64059', 'examples_per_second': '31.684', 'grad_norm': '23.75', 'counters/examples': 163360, 'counters/updates': 5105}
train stats after 163392 examples: {'rewards_train/chosen': '0.072617', 'rewards_train/rejected': '0.042211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030406', 'logps_train/rejected': '-120.82', 'logps_train/chosen': '-134.51', 'loss/train': '0.68611', 'examples_per_second': '31.708', 'grad_norm': '26.875', 'counters/examples': 163392, 'counters/updates': 5106}
train stats after 163424 examples: {'rewards_train/chosen': '0.25069', 'rewards_train/rejected': '0.048703', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20199', 'logps_train/rejected': '-118.91', 'logps_train/chosen': '-160.16', 'loss/train': '0.61225', 'examples_per_second': '31.553', 'grad_norm': '23.875', 'counters/examples': 163424, 'counters/updates': 5107}
train stats after 163456 examples: {'rewards_train/chosen': '0.038496', 'rewards_train/rejected': '-0.023332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061828', 'logps_train/rejected': '-122.08', 'logps_train/chosen': '-142.93', 'loss/train': '0.67102', 'examples_per_second': '31.614', 'grad_norm': '27', 'counters/examples': 163456, 'counters/updates': 5108}
train stats after 163488 examples: {'rewards_train/chosen': '0.14295', 'rewards_train/rejected': '0.088355', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054593', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-130.43', 'loss/train': '0.6746', 'examples_per_second': '31.129', 'grad_norm': '37', 'counters/examples': 163488, 'counters/updates': 5109}
skipping logging after 163520 examples to avoid logging too frequently
train stats after 163552 examples: {'rewards_train/chosen': '0.15176', 'rewards_train/rejected': '0.07749', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074269', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-160.09', 'loss/train': '0.6701', 'examples_per_second': '32.182', 'grad_norm': '28.125', 'counters/examples': 163552, 'counters/updates': 5111}
train stats after 163584 examples: {'rewards_train/chosen': '0.10886', 'rewards_train/rejected': '0.025279', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083579', 'logps_train/rejected': '-120.18', 'logps_train/chosen': '-126.38', 'loss/train': '0.65865', 'examples_per_second': '30.359', 'grad_norm': '27.25', 'counters/examples': 163584, 'counters/updates': 5112}
train stats after 163616 examples: {'rewards_train/chosen': '0.090496', 'rewards_train/rejected': '0.0138', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.076696', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-132.3', 'loss/train': '0.66976', 'examples_per_second': '32.749', 'grad_norm': '25.125', 'counters/examples': 163616, 'counters/updates': 5113}
train stats after 163648 examples: {'rewards_train/chosen': '0.089256', 'rewards_train/rejected': '0.021494', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067761', 'logps_train/rejected': '-101.16', 'logps_train/chosen': '-150.14', 'loss/train': '0.66878', 'examples_per_second': '31.849', 'grad_norm': '25.625', 'counters/examples': 163648, 'counters/updates': 5114}
train stats after 163680 examples: {'rewards_train/chosen': '0.079959', 'rewards_train/rejected': '0.034519', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04544', 'logps_train/rejected': '-117.7', 'logps_train/chosen': '-124.18', 'loss/train': '0.68001', 'examples_per_second': '31.627', 'grad_norm': '27.125', 'counters/examples': 163680, 'counters/updates': 5115}
train stats after 163712 examples: {'rewards_train/chosen': '0.098378', 'rewards_train/rejected': '0.063162', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035216', 'logps_train/rejected': '-131.61', 'logps_train/chosen': '-107.8', 'loss/train': '0.68503', 'examples_per_second': '30.484', 'grad_norm': '28', 'counters/examples': 163712, 'counters/updates': 5116}
train stats after 163744 examples: {'rewards_train/chosen': '0.10804', 'rewards_train/rejected': '0.064151', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043887', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-129.52', 'loss/train': '0.67979', 'examples_per_second': '30.161', 'grad_norm': '24.125', 'counters/examples': 163744, 'counters/updates': 5117}
skipping logging after 163776 examples to avoid logging too frequently
train stats after 163808 examples: {'rewards_train/chosen': '0.15648', 'rewards_train/rejected': '0.015018', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14146', 'logps_train/rejected': '-111.8', 'logps_train/chosen': '-144.88', 'loss/train': '0.63314', 'examples_per_second': '34.521', 'grad_norm': '24.5', 'counters/examples': 163808, 'counters/updates': 5119}
train stats after 163840 examples: {'rewards_train/chosen': '0.072719', 'rewards_train/rejected': '-0.011388', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084107', 'logps_train/rejected': '-121.78', 'logps_train/chosen': '-132.9', 'loss/train': '0.68287', 'examples_per_second': '31.681', 'grad_norm': '43', 'counters/examples': 163840, 'counters/updates': 5120}
train stats after 163872 examples: {'rewards_train/chosen': '0.12703', 'rewards_train/rejected': '0.039896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087137', 'logps_train/rejected': '-143.32', 'logps_train/chosen': '-144.05', 'loss/train': '0.65994', 'examples_per_second': '31.621', 'grad_norm': '28.625', 'counters/examples': 163872, 'counters/updates': 5121}
train stats after 163904 examples: {'rewards_train/chosen': '0.24999', 'rewards_train/rejected': '-0.00068075', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25067', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-203.06', 'loss/train': '0.59081', 'examples_per_second': '30.121', 'grad_norm': '25.375', 'counters/examples': 163904, 'counters/updates': 5122}
train stats after 163936 examples: {'rewards_train/chosen': '0.1062', 'rewards_train/rejected': '0.058428', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047768', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-113.85', 'loss/train': '0.68116', 'examples_per_second': '31.564', 'grad_norm': '27.125', 'counters/examples': 163936, 'counters/updates': 5123}
train stats after 163968 examples: {'rewards_train/chosen': '0.16074', 'rewards_train/rejected': '0.033221', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12752', 'logps_train/rejected': '-159.53', 'logps_train/chosen': '-160.64', 'loss/train': '0.64352', 'examples_per_second': '30.246', 'grad_norm': '27.25', 'counters/examples': 163968, 'counters/updates': 5124}
skipping logging after 164000 examples to avoid logging too frequently
train stats after 164032 examples: {'rewards_train/chosen': '0.10146', 'rewards_train/rejected': '0.072352', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029107', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-110.18', 'loss/train': '0.69443', 'examples_per_second': '31.455', 'grad_norm': '24.875', 'counters/examples': 164032, 'counters/updates': 5126}
skipping logging after 164064 examples to avoid logging too frequently
train stats after 164096 examples: {'rewards_train/chosen': '0.079368', 'rewards_train/rejected': '0.028917', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050451', 'logps_train/rejected': '-132.06', 'logps_train/chosen': '-188.09', 'loss/train': '0.68628', 'examples_per_second': '29.924', 'grad_norm': '29.375', 'counters/examples': 164096, 'counters/updates': 5128}
train stats after 164128 examples: {'rewards_train/chosen': '0.14692', 'rewards_train/rejected': '-0.014751', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16167', 'logps_train/rejected': '-121.54', 'logps_train/chosen': '-149.37', 'loss/train': '0.6217', 'examples_per_second': '32.416', 'grad_norm': '25.375', 'counters/examples': 164128, 'counters/updates': 5129}
skipping logging after 164160 examples to avoid logging too frequently
train stats after 164192 examples: {'rewards_train/chosen': '0.17064', 'rewards_train/rejected': '0.019902', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15073', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-178.97', 'loss/train': '0.62952', 'examples_per_second': '29.486', 'grad_norm': '30', 'counters/examples': 164192, 'counters/updates': 5131}
train stats after 164224 examples: {'rewards_train/chosen': '0.078894', 'rewards_train/rejected': '-0.060273', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13917', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-153.52', 'loss/train': '0.6355', 'examples_per_second': '31.494', 'grad_norm': '28', 'counters/examples': 164224, 'counters/updates': 5132}
train stats after 164256 examples: {'rewards_train/chosen': '0.15902', 'rewards_train/rejected': '0.048441', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11058', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-174.54', 'loss/train': '0.65663', 'examples_per_second': '31.746', 'grad_norm': '41.5', 'counters/examples': 164256, 'counters/updates': 5133}
skipping logging after 164288 examples to avoid logging too frequently
train stats after 164320 examples: {'rewards_train/chosen': '0.10095', 'rewards_train/rejected': '0.022549', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078404', 'logps_train/rejected': '-102.45', 'logps_train/chosen': '-114.25', 'loss/train': '0.66162', 'examples_per_second': '31.835', 'grad_norm': '23.375', 'counters/examples': 164320, 'counters/updates': 5135}
skipping logging after 164352 examples to avoid logging too frequently
train stats after 164384 examples: {'rewards_train/chosen': '0.064606', 'rewards_train/rejected': '-0.0084918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073098', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-124.2', 'loss/train': '0.66592', 'examples_per_second': '29.794', 'grad_norm': '24.75', 'counters/examples': 164384, 'counters/updates': 5137}
train stats after 164416 examples: {'rewards_train/chosen': '0.11993', 'rewards_train/rejected': '-0.028871', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1488', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-136.85', 'loss/train': '0.63007', 'examples_per_second': '31.244', 'grad_norm': '26.875', 'counters/examples': 164416, 'counters/updates': 5138}
train stats after 164448 examples: {'rewards_train/chosen': '0.079881', 'rewards_train/rejected': '0.049697', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030184', 'logps_train/rejected': '-139.06', 'logps_train/chosen': '-154.61', 'loss/train': '0.68542', 'examples_per_second': '29.405', 'grad_norm': '28.75', 'counters/examples': 164448, 'counters/updates': 5139}
train stats after 164480 examples: {'rewards_train/chosen': '0.10815', 'rewards_train/rejected': '-0.025162', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13332', 'logps_train/rejected': '-161.06', 'logps_train/chosen': '-161.54', 'loss/train': '0.64008', 'examples_per_second': '31.929', 'grad_norm': '29', 'counters/examples': 164480, 'counters/updates': 5140}
train stats after 164512 examples: {'rewards_train/chosen': '0.11259', 'rewards_train/rejected': '0.082769', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029817', 'logps_train/rejected': '-183.6', 'logps_train/chosen': '-190.03', 'loss/train': '0.6974', 'examples_per_second': '30.057', 'grad_norm': '37.75', 'counters/examples': 164512, 'counters/updates': 5141}
train stats after 164544 examples: {'rewards_train/chosen': '0.095142', 'rewards_train/rejected': '-0.010414', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10556', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-142.57', 'loss/train': '0.64953', 'examples_per_second': '30.894', 'grad_norm': '24.875', 'counters/examples': 164544, 'counters/updates': 5142}
skipping logging after 164576 examples to avoid logging too frequently
train stats after 164608 examples: {'rewards_train/chosen': '0.10716', 'rewards_train/rejected': '-0.0064428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11361', 'logps_train/rejected': '-129.79', 'logps_train/chosen': '-132.14', 'loss/train': '0.64954', 'examples_per_second': '30.252', 'grad_norm': '24.375', 'counters/examples': 164608, 'counters/updates': 5144}
train stats after 164640 examples: {'rewards_train/chosen': '0.087994', 'rewards_train/rejected': '-0.01435', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10234', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-120.88', 'loss/train': '0.64807', 'examples_per_second': '30.943', 'grad_norm': '25.625', 'counters/examples': 164640, 'counters/updates': 5145}
skipping logging after 164672 examples to avoid logging too frequently
train stats after 164704 examples: {'rewards_train/chosen': '0.074236', 'rewards_train/rejected': '-0.011983', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.086219', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-125.58', 'loss/train': '0.66009', 'examples_per_second': '31.353', 'grad_norm': '28.75', 'counters/examples': 164704, 'counters/updates': 5147}
train stats after 164736 examples: {'rewards_train/chosen': '0.1064', 'rewards_train/rejected': '-0.010773', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11717', 'logps_train/rejected': '-108.67', 'logps_train/chosen': '-132.56', 'loss/train': '0.64475', 'examples_per_second': '31.629', 'grad_norm': '23.625', 'counters/examples': 164736, 'counters/updates': 5148}
train stats after 164768 examples: {'rewards_train/chosen': '0.096509', 'rewards_train/rejected': '0.01976', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076748', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-144.27', 'loss/train': '0.66127', 'examples_per_second': '31.02', 'grad_norm': '24.25', 'counters/examples': 164768, 'counters/updates': 5149}
train stats after 164800 examples: {'rewards_train/chosen': '0.14965', 'rewards_train/rejected': '0.068704', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080949', 'logps_train/rejected': '-128.3', 'logps_train/chosen': '-141.23', 'loss/train': '0.66415', 'examples_per_second': '31.977', 'grad_norm': '26.75', 'counters/examples': 164800, 'counters/updates': 5150}
skipping logging after 164832 examples to avoid logging too frequently
train stats after 164864 examples: {'rewards_train/chosen': '0.2346', 'rewards_train/rejected': '0.20139', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033212', 'logps_train/rejected': '-152.73', 'logps_train/chosen': '-160.43', 'loss/train': '0.69105', 'examples_per_second': '31.608', 'grad_norm': '29.75', 'counters/examples': 164864, 'counters/updates': 5152}
train stats after 164896 examples: {'rewards_train/chosen': '0.10293', 'rewards_train/rejected': '-0.0064396', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10937', 'logps_train/rejected': '-100.3', 'logps_train/chosen': '-131.68', 'loss/train': '0.65042', 'examples_per_second': '30.771', 'grad_norm': '22.75', 'counters/examples': 164896, 'counters/updates': 5153}
skipping logging after 164928 examples to avoid logging too frequently
train stats after 164960 examples: {'rewards_train/chosen': '0.12503', 'rewards_train/rejected': '0.082676', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042356', 'logps_train/rejected': '-117.92', 'logps_train/chosen': '-120.48', 'loss/train': '0.68471', 'examples_per_second': '30.578', 'grad_norm': '24.625', 'counters/examples': 164960, 'counters/updates': 5155}
train stats after 164992 examples: {'rewards_train/chosen': '0.0817', 'rewards_train/rejected': '0.039679', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042021', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-145.64', 'loss/train': '0.67957', 'examples_per_second': '31.269', 'grad_norm': '35.5', 'counters/examples': 164992, 'counters/updates': 5156}
train stats after 165024 examples: {'rewards_train/chosen': '0.062059', 'rewards_train/rejected': '-0.00398', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066039', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-115.2', 'loss/train': '0.66552', 'examples_per_second': '29.989', 'grad_norm': '23.25', 'counters/examples': 165024, 'counters/updates': 5157}
train stats after 165056 examples: {'rewards_train/chosen': '0.09539', 'rewards_train/rejected': '0.0033399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09205', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-125.28', 'loss/train': '0.65839', 'examples_per_second': '32.463', 'grad_norm': '25.875', 'counters/examples': 165056, 'counters/updates': 5158}
train stats after 165088 examples: {'rewards_train/chosen': '0.09687', 'rewards_train/rejected': '-0.01802', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11489', 'logps_train/rejected': '-132.62', 'logps_train/chosen': '-158.93', 'loss/train': '0.64802', 'examples_per_second': '31.529', 'grad_norm': '25.625', 'counters/examples': 165088, 'counters/updates': 5159}
train stats after 165120 examples: {'rewards_train/chosen': '0.081634', 'rewards_train/rejected': '0.00030462', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081329', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-126.25', 'loss/train': '0.66491', 'examples_per_second': '32.86', 'grad_norm': '24.75', 'counters/examples': 165120, 'counters/updates': 5160}
train stats after 165152 examples: {'rewards_train/chosen': '0.15843', 'rewards_train/rejected': '-0.011127', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16956', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-135.29', 'loss/train': '0.62682', 'examples_per_second': '30.263', 'grad_norm': '30', 'counters/examples': 165152, 'counters/updates': 5161}
train stats after 165184 examples: {'rewards_train/chosen': '0.14164', 'rewards_train/rejected': '-0.018327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15997', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-168.35', 'loss/train': '0.62717', 'examples_per_second': '31.609', 'grad_norm': '28.5', 'counters/examples': 165184, 'counters/updates': 5162}
train stats after 165216 examples: {'rewards_train/chosen': '0.11173', 'rewards_train/rejected': '0.060083', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051651', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-138.63', 'loss/train': '0.68132', 'examples_per_second': '31.569', 'grad_norm': '35.75', 'counters/examples': 165216, 'counters/updates': 5163}
skipping logging after 165248 examples to avoid logging too frequently
train stats after 165280 examples: {'rewards_train/chosen': '0.089117', 'rewards_train/rejected': '0.043965', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.045152', 'logps_train/rejected': '-113', 'logps_train/chosen': '-122.89', 'loss/train': '0.68005', 'examples_per_second': '31.61', 'grad_norm': '25.5', 'counters/examples': 165280, 'counters/updates': 5165}
train stats after 165312 examples: {'rewards_train/chosen': '0.10876', 'rewards_train/rejected': '-0.013383', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12214', 'logps_train/rejected': '-154.19', 'logps_train/chosen': '-145.2', 'loss/train': '0.64508', 'examples_per_second': '31.196', 'grad_norm': '28.875', 'counters/examples': 165312, 'counters/updates': 5166}
train stats after 165344 examples: {'rewards_train/chosen': '0.13802', 'rewards_train/rejected': '-0.036509', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17453', 'logps_train/rejected': '-111.43', 'logps_train/chosen': '-138.85', 'loss/train': '0.61687', 'examples_per_second': '30.222', 'grad_norm': '25.125', 'counters/examples': 165344, 'counters/updates': 5167}
skipping logging after 165376 examples to avoid logging too frequently
train stats after 165408 examples: {'rewards_train/chosen': '0.17874', 'rewards_train/rejected': '-0.007201', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18594', 'logps_train/rejected': '-119.18', 'logps_train/chosen': '-131.22', 'loss/train': '0.61381', 'examples_per_second': '30.781', 'grad_norm': '21.875', 'counters/examples': 165408, 'counters/updates': 5169}
train stats after 165440 examples: {'rewards_train/chosen': '0.14191', 'rewards_train/rejected': '0.0090346', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13287', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-141.48', 'loss/train': '0.63872', 'examples_per_second': '31.566', 'grad_norm': '24.5', 'counters/examples': 165440, 'counters/updates': 5170}
train stats after 165472 examples: {'rewards_train/chosen': '0.067957', 'rewards_train/rejected': '-0.0049652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072922', 'logps_train/rejected': '-96.645', 'logps_train/chosen': '-156.95', 'loss/train': '0.6645', 'examples_per_second': '31.316', 'grad_norm': '25.625', 'counters/examples': 165472, 'counters/updates': 5171}
train stats after 165504 examples: {'rewards_train/chosen': '0.12167', 'rewards_train/rejected': '-0.015334', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.137', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-119.18', 'loss/train': '0.64657', 'examples_per_second': '31.005', 'grad_norm': '24.5', 'counters/examples': 165504, 'counters/updates': 5172}
train stats after 165536 examples: {'rewards_train/chosen': '0.19709', 'rewards_train/rejected': '0.054128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14296', 'logps_train/rejected': '-149.39', 'logps_train/chosen': '-138.18', 'loss/train': '0.64046', 'examples_per_second': '30.513', 'grad_norm': '26.625', 'counters/examples': 165536, 'counters/updates': 5173}
train stats after 165568 examples: {'rewards_train/chosen': '0.016956', 'rewards_train/rejected': '-0.061222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078178', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-115.2', 'loss/train': '0.66771', 'examples_per_second': '31.895', 'grad_norm': '27.375', 'counters/examples': 165568, 'counters/updates': 5174}
train stats after 165600 examples: {'rewards_train/chosen': '0.084558', 'rewards_train/rejected': '0.033416', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051143', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-142', 'loss/train': '0.67919', 'examples_per_second': '30.257', 'grad_norm': '25.5', 'counters/examples': 165600, 'counters/updates': 5175}
skipping logging after 165632 examples to avoid logging too frequently
train stats after 165664 examples: {'rewards_train/chosen': '0.11364', 'rewards_train/rejected': '0.044347', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069293', 'logps_train/rejected': '-118.7', 'logps_train/chosen': '-133.92', 'loss/train': '0.67524', 'examples_per_second': '37.114', 'grad_norm': '30.875', 'counters/examples': 165664, 'counters/updates': 5177}
train stats after 165696 examples: {'rewards_train/chosen': '0.19225', 'rewards_train/rejected': '0.041998', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15025', 'logps_train/rejected': '-105.39', 'logps_train/chosen': '-131.02', 'loss/train': '0.63471', 'examples_per_second': '30.544', 'grad_norm': '23.625', 'counters/examples': 165696, 'counters/updates': 5178}
skipping logging after 165728 examples to avoid logging too frequently
train stats after 165760 examples: {'rewards_train/chosen': '0.07705', 'rewards_train/rejected': '0.052095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.024955', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-109.09', 'loss/train': '0.68925', 'examples_per_second': '31.827', 'grad_norm': '24.125', 'counters/examples': 165760, 'counters/updates': 5180}
train stats after 165792 examples: {'rewards_train/chosen': '0.10322', 'rewards_train/rejected': '0.030452', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072772', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-139.52', 'loss/train': '0.66948', 'examples_per_second': '31.668', 'grad_norm': '25.875', 'counters/examples': 165792, 'counters/updates': 5181}
train stats after 165824 examples: {'rewards_train/chosen': '0.090009', 'rewards_train/rejected': '0.038688', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051321', 'logps_train/rejected': '-137.82', 'logps_train/chosen': '-132.74', 'loss/train': '0.67585', 'examples_per_second': '30.792', 'grad_norm': '26', 'counters/examples': 165824, 'counters/updates': 5182}
train stats after 165856 examples: {'rewards_train/chosen': '0.09415', 'rewards_train/rejected': '0.038924', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055227', 'logps_train/rejected': '-123', 'logps_train/chosen': '-165.86', 'loss/train': '0.6759', 'examples_per_second': '30.19', 'grad_norm': '27.75', 'counters/examples': 165856, 'counters/updates': 5183}
train stats after 165888 examples: {'rewards_train/chosen': '0.14768', 'rewards_train/rejected': '-0.022704', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17038', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-166.27', 'loss/train': '0.62587', 'examples_per_second': '32.48', 'grad_norm': '71', 'counters/examples': 165888, 'counters/updates': 5184}
train stats after 165920 examples: {'rewards_train/chosen': '0.066084', 'rewards_train/rejected': '0.031391', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034693', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-133.65', 'loss/train': '0.69015', 'examples_per_second': '27.695', 'grad_norm': '28.625', 'counters/examples': 165920, 'counters/updates': 5185}
train stats after 165952 examples: {'rewards_train/chosen': '0.15115', 'rewards_train/rejected': '0.072894', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078252', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-113.88', 'loss/train': '0.66285', 'examples_per_second': '31.364', 'grad_norm': '26.875', 'counters/examples': 165952, 'counters/updates': 5186}
skipping logging after 165984 examples to avoid logging too frequently
train stats after 166016 examples: {'rewards_train/chosen': '0.17172', 'rewards_train/rejected': '0.084537', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.087185', 'logps_train/rejected': '-163.09', 'logps_train/chosen': '-141.02', 'loss/train': '0.66287', 'examples_per_second': '30.364', 'grad_norm': '26.25', 'counters/examples': 166016, 'counters/updates': 5188}
train stats after 166048 examples: {'rewards_train/chosen': '0.13192', 'rewards_train/rejected': '-0.01875', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15067', 'logps_train/rejected': '-156.75', 'logps_train/chosen': '-144.09', 'loss/train': '0.64611', 'examples_per_second': '31.57', 'grad_norm': '27.125', 'counters/examples': 166048, 'counters/updates': 5189}
skipping logging after 166080 examples to avoid logging too frequently
train stats after 166112 examples: {'rewards_train/chosen': '0.15266', 'rewards_train/rejected': '-0.019188', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17185', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-145.31', 'loss/train': '0.62075', 'examples_per_second': '33.394', 'grad_norm': '23.625', 'counters/examples': 166112, 'counters/updates': 5191}
train stats after 166144 examples: {'rewards_train/chosen': '0.19111', 'rewards_train/rejected': '0.051172', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13994', 'logps_train/rejected': '-147.24', 'logps_train/chosen': '-146.62', 'loss/train': '0.64311', 'examples_per_second': '31.581', 'grad_norm': '28.875', 'counters/examples': 166144, 'counters/updates': 5192}
train stats after 166176 examples: {'rewards_train/chosen': '0.063951', 'rewards_train/rejected': '0.044719', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019232', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-107.6', 'loss/train': '0.69126', 'examples_per_second': '31.864', 'grad_norm': '25.5', 'counters/examples': 166176, 'counters/updates': 5193}
train stats after 166208 examples: {'rewards_train/chosen': '0.093184', 'rewards_train/rejected': '0.020589', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.072595', 'logps_train/rejected': '-120.81', 'logps_train/chosen': '-185.02', 'loss/train': '0.66749', 'examples_per_second': '32.777', 'grad_norm': '27.875', 'counters/examples': 166208, 'counters/updates': 5194}
train stats after 166240 examples: {'rewards_train/chosen': '0.034523', 'rewards_train/rejected': '0.01497', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019553', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-133.82', 'loss/train': '0.69226', 'examples_per_second': '30.497', 'grad_norm': '27', 'counters/examples': 166240, 'counters/updates': 5195}
train stats after 166272 examples: {'rewards_train/chosen': '0.11763', 'rewards_train/rejected': '0.059408', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05822', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-155.78', 'loss/train': '0.6723', 'examples_per_second': '30.562', 'grad_norm': '27.625', 'counters/examples': 166272, 'counters/updates': 5196}
train stats after 166304 examples: {'rewards_train/chosen': '0.041767', 'rewards_train/rejected': '0.012488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02928', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-107.17', 'loss/train': '0.68892', 'examples_per_second': '31.674', 'grad_norm': '24.375', 'counters/examples': 166304, 'counters/updates': 5197}
train stats after 166336 examples: {'rewards_train/chosen': '0.138', 'rewards_train/rejected': '0.097417', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040584', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-127.84', 'loss/train': '0.68376', 'examples_per_second': '31.615', 'grad_norm': '26.375', 'counters/examples': 166336, 'counters/updates': 5198}
train stats after 166368 examples: {'rewards_train/chosen': '0.064783', 'rewards_train/rejected': '0.034987', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029797', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-122.85', 'loss/train': '0.6848', 'examples_per_second': '30.613', 'grad_norm': '26.375', 'counters/examples': 166368, 'counters/updates': 5199}
train stats after 166400 examples: {'rewards_train/chosen': '0.14027', 'rewards_train/rejected': '-0.020173', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16044', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-135.29', 'loss/train': '0.63391', 'examples_per_second': '31.63', 'grad_norm': '24', 'counters/examples': 166400, 'counters/updates': 5200}
train stats after 166432 examples: {'rewards_train/chosen': '0.16481', 'rewards_train/rejected': '0.043368', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12144', 'logps_train/rejected': '-152.09', 'logps_train/chosen': '-151.01', 'loss/train': '0.64201', 'examples_per_second': '31.624', 'grad_norm': '28', 'counters/examples': 166432, 'counters/updates': 5201}
train stats after 166464 examples: {'rewards_train/chosen': '0.046296', 'rewards_train/rejected': '-0.027778', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074074', 'logps_train/rejected': '-93.323', 'logps_train/chosen': '-116.43', 'loss/train': '0.66734', 'examples_per_second': '32.764', 'grad_norm': '24.75', 'counters/examples': 166464, 'counters/updates': 5202}
train stats after 166496 examples: {'rewards_train/chosen': '0.14422', 'rewards_train/rejected': '0.017511', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12671', 'logps_train/rejected': '-115.38', 'logps_train/chosen': '-158.28', 'loss/train': '0.64256', 'examples_per_second': '31.49', 'grad_norm': '25.875', 'counters/examples': 166496, 'counters/updates': 5203}
train stats after 166528 examples: {'rewards_train/chosen': '0.092445', 'rewards_train/rejected': '0.067037', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025408', 'logps_train/rejected': '-99.362', 'logps_train/chosen': '-127.3', 'loss/train': '0.693', 'examples_per_second': '31.584', 'grad_norm': '23.625', 'counters/examples': 166528, 'counters/updates': 5204}
train stats after 166560 examples: {'rewards_train/chosen': '0.05239', 'rewards_train/rejected': '-0.012855', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.065246', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-137.74', 'loss/train': '0.67296', 'examples_per_second': '31.614', 'grad_norm': '26', 'counters/examples': 166560, 'counters/updates': 5205}
train stats after 166592 examples: {'rewards_train/chosen': '0.12968', 'rewards_train/rejected': '-0.0021097', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13179', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-140.27', 'loss/train': '0.64834', 'examples_per_second': '30.643', 'grad_norm': '27', 'counters/examples': 166592, 'counters/updates': 5206}
train stats after 166624 examples: {'rewards_train/chosen': '0.1016', 'rewards_train/rejected': '0.026009', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075595', 'logps_train/rejected': '-90.905', 'logps_train/chosen': '-106.82', 'loss/train': '0.6638', 'examples_per_second': '31.279', 'grad_norm': '21.125', 'counters/examples': 166624, 'counters/updates': 5207}
train stats after 166656 examples: {'rewards_train/chosen': '0.11352', 'rewards_train/rejected': '0.0496', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063915', 'logps_train/rejected': '-139.64', 'logps_train/chosen': '-133.31', 'loss/train': '0.67197', 'examples_per_second': '31.603', 'grad_norm': '27.375', 'counters/examples': 166656, 'counters/updates': 5208}
train stats after 166688 examples: {'rewards_train/chosen': '0.12885', 'rewards_train/rejected': '-0.025703', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15455', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-161.08', 'loss/train': '0.63721', 'examples_per_second': '30.182', 'grad_norm': '29', 'counters/examples': 166688, 'counters/updates': 5209}
skipping logging after 166720 examples to avoid logging too frequently
train stats after 166752 examples: {'rewards_train/chosen': '0.15618', 'rewards_train/rejected': '0.058706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097474', 'logps_train/rejected': '-141.2', 'logps_train/chosen': '-165.39', 'loss/train': '0.65891', 'examples_per_second': '31.493', 'grad_norm': '31.25', 'counters/examples': 166752, 'counters/updates': 5211}
skipping logging after 166784 examples to avoid logging too frequently
train stats after 166816 examples: {'rewards_train/chosen': '0.099432', 'rewards_train/rejected': '0.026888', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072544', 'logps_train/rejected': '-149.38', 'logps_train/chosen': '-151.39', 'loss/train': '0.66983', 'examples_per_second': '30.367', 'grad_norm': '28.875', 'counters/examples': 166816, 'counters/updates': 5213}
train stats after 166848 examples: {'rewards_train/chosen': '0.011646', 'rewards_train/rejected': '0.0047523', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0068936', 'logps_train/rejected': '-105.89', 'logps_train/chosen': '-92.78', 'loss/train': '0.70241', 'examples_per_second': '31.618', 'grad_norm': '26.375', 'counters/examples': 166848, 'counters/updates': 5214}
train stats after 166880 examples: {'rewards_train/chosen': '0.092819', 'rewards_train/rejected': '0.0048824', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087937', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-170.53', 'loss/train': '0.66035', 'examples_per_second': '31.623', 'grad_norm': '31.25', 'counters/examples': 166880, 'counters/updates': 5215}
skipping logging after 166912 examples to avoid logging too frequently
train stats after 166944 examples: {'rewards_train/chosen': '0.081398', 'rewards_train/rejected': '0.029753', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051645', 'logps_train/rejected': '-108.86', 'logps_train/chosen': '-133.59', 'loss/train': '0.6742', 'examples_per_second': '31.099', 'grad_norm': '25.75', 'counters/examples': 166944, 'counters/updates': 5217}
train stats after 166976 examples: {'rewards_train/chosen': '0.11387', 'rewards_train/rejected': '-0.037049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15092', 'logps_train/rejected': '-119.14', 'logps_train/chosen': '-177', 'loss/train': '0.62952', 'examples_per_second': '31.489', 'grad_norm': '25.875', 'counters/examples': 166976, 'counters/updates': 5218}
train stats after 167008 examples: {'rewards_train/chosen': '0.14881', 'rewards_train/rejected': '0.01063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13818', 'logps_train/rejected': '-133.25', 'logps_train/chosen': '-136.71', 'loss/train': '0.63726', 'examples_per_second': '31.357', 'grad_norm': '24.5', 'counters/examples': 167008, 'counters/updates': 5219}
skipping logging after 167040 examples to avoid logging too frequently
train stats after 167072 examples: {'rewards_train/chosen': '0.077594', 'rewards_train/rejected': '0.015821', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061773', 'logps_train/rejected': '-131.88', 'logps_train/chosen': '-123.12', 'loss/train': '0.67326', 'examples_per_second': '30.648', 'grad_norm': '27.125', 'counters/examples': 167072, 'counters/updates': 5221}
skipping logging after 167104 examples to avoid logging too frequently
train stats after 167136 examples: {'rewards_train/chosen': '0.08431', 'rewards_train/rejected': '-0.012623', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096934', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-138.17', 'loss/train': '0.65998', 'examples_per_second': '30.228', 'grad_norm': '26.875', 'counters/examples': 167136, 'counters/updates': 5223}
skipping logging after 167168 examples to avoid logging too frequently
train stats after 167200 examples: {'rewards_train/chosen': '0.10705', 'rewards_train/rejected': '-0.069432', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17648', 'logps_train/rejected': '-158.43', 'logps_train/chosen': '-122.16', 'loss/train': '0.62109', 'examples_per_second': '37.379', 'grad_norm': '27.125', 'counters/examples': 167200, 'counters/updates': 5225}
skipping logging after 167232 examples to avoid logging too frequently
train stats after 167264 examples: {'rewards_train/chosen': '0.16192', 'rewards_train/rejected': '0.08976', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.072162', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-151.12', 'loss/train': '0.66733', 'examples_per_second': '31.371', 'grad_norm': '31.5', 'counters/examples': 167264, 'counters/updates': 5227}
train stats after 167296 examples: {'rewards_train/chosen': '0.078962', 'rewards_train/rejected': '0.091237', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012275', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-167.88', 'loss/train': '0.70927', 'examples_per_second': '31.47', 'grad_norm': '34.5', 'counters/examples': 167296, 'counters/updates': 5228}
train stats after 167328 examples: {'rewards_train/chosen': '0.10228', 'rewards_train/rejected': '0.015002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087282', 'logps_train/rejected': '-108.51', 'logps_train/chosen': '-140.46', 'loss/train': '0.65716', 'examples_per_second': '31.452', 'grad_norm': '23.125', 'counters/examples': 167328, 'counters/updates': 5229}
train stats after 167360 examples: {'rewards_train/chosen': '0.21751', 'rewards_train/rejected': '0.070611', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1469', 'logps_train/rejected': '-142.49', 'logps_train/chosen': '-169.7', 'loss/train': '0.63535', 'examples_per_second': '31.326', 'grad_norm': '27.5', 'counters/examples': 167360, 'counters/updates': 5230}
skipping logging after 167392 examples to avoid logging too frequently
train stats after 167424 examples: {'rewards_train/chosen': '0.15177', 'rewards_train/rejected': '0.10356', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048204', 'logps_train/rejected': '-143.85', 'logps_train/chosen': '-149.41', 'loss/train': '0.68725', 'examples_per_second': '35.447', 'grad_norm': '30.25', 'counters/examples': 167424, 'counters/updates': 5232}
train stats after 167456 examples: {'rewards_train/chosen': '0.10312', 'rewards_train/rejected': '-0.092644', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19576', 'logps_train/rejected': '-147.65', 'logps_train/chosen': '-161.49', 'loss/train': '0.60816', 'examples_per_second': '30.946', 'grad_norm': '29.875', 'counters/examples': 167456, 'counters/updates': 5233}
skipping logging after 167488 examples to avoid logging too frequently
train stats after 167520 examples: {'rewards_train/chosen': '0.10659', 'rewards_train/rejected': '0.054879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051713', 'logps_train/rejected': '-109.54', 'logps_train/chosen': '-141.06', 'loss/train': '0.67768', 'examples_per_second': '31.549', 'grad_norm': '25.125', 'counters/examples': 167520, 'counters/updates': 5235}
skipping logging after 167552 examples to avoid logging too frequently
train stats after 167584 examples: {'rewards_train/chosen': '0.11264', 'rewards_train/rejected': '0.0044937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10815', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-158.67', 'loss/train': '0.65494', 'examples_per_second': '32.469', 'grad_norm': '26.625', 'counters/examples': 167584, 'counters/updates': 5237}
train stats after 167616 examples: {'rewards_train/chosen': '0.13237', 'rewards_train/rejected': '0.014477', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11789', 'logps_train/rejected': '-101.64', 'logps_train/chosen': '-148.2', 'loss/train': '0.64463', 'examples_per_second': '32.023', 'grad_norm': '24.375', 'counters/examples': 167616, 'counters/updates': 5238}
skipping logging after 167648 examples to avoid logging too frequently
train stats after 167680 examples: {'rewards_train/chosen': '0.079951', 'rewards_train/rejected': '0.032935', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047015', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-137.83', 'loss/train': '0.67761', 'examples_per_second': '38.108', 'grad_norm': '27.375', 'counters/examples': 167680, 'counters/updates': 5240}
train stats after 167712 examples: {'rewards_train/chosen': '0.085626', 'rewards_train/rejected': '0.030441', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055185', 'logps_train/rejected': '-97.319', 'logps_train/chosen': '-106.36', 'loss/train': '0.67972', 'examples_per_second': '31.319', 'grad_norm': '24.125', 'counters/examples': 167712, 'counters/updates': 5241}
train stats after 167744 examples: {'rewards_train/chosen': '0.17795', 'rewards_train/rejected': '0.062856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11509', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-144.29', 'loss/train': '0.6465', 'examples_per_second': '30.331', 'grad_norm': '24.625', 'counters/examples': 167744, 'counters/updates': 5242}
train stats after 167776 examples: {'rewards_train/chosen': '0.13344', 'rewards_train/rejected': '0.09661', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.036828', 'logps_train/rejected': '-90.688', 'logps_train/chosen': '-83.275', 'loss/train': '0.68398', 'examples_per_second': '30.142', 'grad_norm': '22', 'counters/examples': 167776, 'counters/updates': 5243}
train stats after 167808 examples: {'rewards_train/chosen': '0.099735', 'rewards_train/rejected': '0.11205', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012313', 'logps_train/rejected': '-147.82', 'logps_train/chosen': '-151.37', 'loss/train': '0.71509', 'examples_per_second': '30.849', 'grad_norm': '32', 'counters/examples': 167808, 'counters/updates': 5244}
skipping logging after 167840 examples to avoid logging too frequently
train stats after 167872 examples: {'rewards_train/chosen': '0.10809', 'rewards_train/rejected': '-0.0049229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11301', 'logps_train/rejected': '-112.16', 'logps_train/chosen': '-137.7', 'loss/train': '0.64939', 'examples_per_second': '31.524', 'grad_norm': '25.125', 'counters/examples': 167872, 'counters/updates': 5246}
train stats after 167904 examples: {'rewards_train/chosen': '0.13815', 'rewards_train/rejected': '-0.012232', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15038', 'logps_train/rejected': '-102.33', 'logps_train/chosen': '-131.11', 'loss/train': '0.62762', 'examples_per_second': '31.63', 'grad_norm': '24.25', 'counters/examples': 167904, 'counters/updates': 5247}
train stats after 167936 examples: {'rewards_train/chosen': '0.084089', 'rewards_train/rejected': '0.069158', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014931', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-130.83', 'loss/train': '0.69759', 'examples_per_second': '32.866', 'grad_norm': '25.875', 'counters/examples': 167936, 'counters/updates': 5248}
train stats after 167968 examples: {'rewards_train/chosen': '0.08466', 'rewards_train/rejected': '0.024326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060335', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-132.22', 'loss/train': '0.67312', 'examples_per_second': '32.498', 'grad_norm': '26.375', 'counters/examples': 167968, 'counters/updates': 5249}
train stats after 168000 examples: {'rewards_train/chosen': '0.11843', 'rewards_train/rejected': '-0.023855', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14229', 'logps_train/rejected': '-112.26', 'logps_train/chosen': '-94.306', 'loss/train': '0.63426', 'examples_per_second': '31.77', 'grad_norm': '21.75', 'counters/examples': 168000, 'counters/updates': 5250}
Running evaluation after 168000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 168000: {'rewards_eval/chosen': '0.11153', 'rewards_eval/rejected': '0.034608', 'rewards_eval/accuracies': '0.60156', 'rewards_eval/margins': '0.07692', 'logps_eval/rejected': '-118.27', 'logps_eval/chosen': '-138.32', 'loss/eval': '0.66806'}
skipping save for non epoch
train stats after 168032 examples: {'rewards_train/chosen': '0.059838', 'rewards_train/rejected': '0.025406', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034432', 'logps_train/rejected': '-98.219', 'logps_train/chosen': '-118.57', 'loss/train': '0.68696', 'examples_per_second': '32.421', 'grad_norm': '22.875', 'counters/examples': 168032, 'counters/updates': 5251}
train stats after 168064 examples: {'rewards_train/chosen': '0.065282', 'rewards_train/rejected': '0.088629', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023347', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-152.23', 'loss/train': '0.71737', 'examples_per_second': '32.327', 'grad_norm': '28.5', 'counters/examples': 168064, 'counters/updates': 5252}
train stats after 168096 examples: {'rewards_train/chosen': '0.07141', 'rewards_train/rejected': '-0.0014759', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072886', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-116.19', 'loss/train': '0.67344', 'examples_per_second': '31.613', 'grad_norm': '23.5', 'counters/examples': 168096, 'counters/updates': 5253}
train stats after 168128 examples: {'rewards_train/chosen': '0.20003', 'rewards_train/rejected': '0.032151', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.16788', 'logps_train/rejected': '-100.43', 'logps_train/chosen': '-132.38', 'loss/train': '0.6174', 'examples_per_second': '31.157', 'grad_norm': '21.625', 'counters/examples': 168128, 'counters/updates': 5254}
train stats after 168160 examples: {'rewards_train/chosen': '0.10222', 'rewards_train/rejected': '0.014116', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088105', 'logps_train/rejected': '-118.27', 'logps_train/chosen': '-149.32', 'loss/train': '0.65867', 'examples_per_second': '31.64', 'grad_norm': '24.25', 'counters/examples': 168160, 'counters/updates': 5255}
train stats after 168192 examples: {'rewards_train/chosen': '0.10843', 'rewards_train/rejected': '0.035753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072681', 'logps_train/rejected': '-100.68', 'logps_train/chosen': '-144.61', 'loss/train': '0.67171', 'examples_per_second': '31.628', 'grad_norm': '27', 'counters/examples': 168192, 'counters/updates': 5256}
train stats after 168224 examples: {'rewards_train/chosen': '0.15311', 'rewards_train/rejected': '0.0017293', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15138', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-164', 'loss/train': '0.63058', 'examples_per_second': '31.617', 'grad_norm': '31', 'counters/examples': 168224, 'counters/updates': 5257}
train stats after 168256 examples: {'rewards_train/chosen': '0.16227', 'rewards_train/rejected': '-0.0042029', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16648', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-149.27', 'loss/train': '0.63159', 'examples_per_second': '31.545', 'grad_norm': '26.75', 'counters/examples': 168256, 'counters/updates': 5258}
train stats after 168288 examples: {'rewards_train/chosen': '0.1436', 'rewards_train/rejected': '-0.0074325', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15104', 'logps_train/rejected': '-101.72', 'logps_train/chosen': '-141.9', 'loss/train': '0.63144', 'examples_per_second': '31.495', 'grad_norm': '26', 'counters/examples': 168288, 'counters/updates': 5259}
train stats after 168320 examples: {'rewards_train/chosen': '0.042951', 'rewards_train/rejected': '0.028489', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014462', 'logps_train/rejected': '-122', 'logps_train/chosen': '-108.26', 'loss/train': '0.69898', 'examples_per_second': '33.134', 'grad_norm': '23.75', 'counters/examples': 168320, 'counters/updates': 5260}
train stats after 168352 examples: {'rewards_train/chosen': '0.096465', 'rewards_train/rejected': '-0.0050353', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1015', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-108.04', 'loss/train': '0.65345', 'examples_per_second': '24.515', 'grad_norm': '21.25', 'counters/examples': 168352, 'counters/updates': 5261}
train stats after 168384 examples: {'rewards_train/chosen': '0.15404', 'rewards_train/rejected': '0.063396', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090649', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-134.06', 'loss/train': '0.66237', 'examples_per_second': '32.981', 'grad_norm': '27.25', 'counters/examples': 168384, 'counters/updates': 5262}
train stats after 168416 examples: {'rewards_train/chosen': '0.020394', 'rewards_train/rejected': '-0.04769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068085', 'logps_train/rejected': '-110.09', 'logps_train/chosen': '-157.41', 'loss/train': '0.66549', 'examples_per_second': '30.256', 'grad_norm': '26.25', 'counters/examples': 168416, 'counters/updates': 5263}
train stats after 168448 examples: {'rewards_train/chosen': '0.16602', 'rewards_train/rejected': '0.064704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10131', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-147.96', 'loss/train': '0.65378', 'examples_per_second': '24.264', 'grad_norm': '26.875', 'counters/examples': 168448, 'counters/updates': 5264}
train stats after 168480 examples: {'rewards_train/chosen': '0.10304', 'rewards_train/rejected': '0.011826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091213', 'logps_train/rejected': '-81.889', 'logps_train/chosen': '-105.87', 'loss/train': '0.65769', 'examples_per_second': '33.116', 'grad_norm': '21.25', 'counters/examples': 168480, 'counters/updates': 5265}
train stats after 168512 examples: {'rewards_train/chosen': '0.090618', 'rewards_train/rejected': '0.0075075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083111', 'logps_train/rejected': '-118.36', 'logps_train/chosen': '-157.33', 'loss/train': '0.66653', 'examples_per_second': '30.178', 'grad_norm': '31.875', 'counters/examples': 168512, 'counters/updates': 5266}
train stats after 168544 examples: {'rewards_train/chosen': '0.11621', 'rewards_train/rejected': '0.039886', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076327', 'logps_train/rejected': '-162.67', 'logps_train/chosen': '-126.51', 'loss/train': '0.67237', 'examples_per_second': '31.366', 'grad_norm': '27.75', 'counters/examples': 168544, 'counters/updates': 5267}
train stats after 168576 examples: {'rewards_train/chosen': '0.15359', 'rewards_train/rejected': '-0.0063736', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15996', 'logps_train/rejected': '-105.68', 'logps_train/chosen': '-141.26', 'loss/train': '0.63132', 'examples_per_second': '31.421', 'grad_norm': '22.75', 'counters/examples': 168576, 'counters/updates': 5268}
train stats after 168608 examples: {'rewards_train/chosen': '0.10798', 'rewards_train/rejected': '0.04885', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059126', 'logps_train/rejected': '-101.14', 'logps_train/chosen': '-127.42', 'loss/train': '0.67362', 'examples_per_second': '30.869', 'grad_norm': '26', 'counters/examples': 168608, 'counters/updates': 5269}
train stats after 168640 examples: {'rewards_train/chosen': '0.07677', 'rewards_train/rejected': '0.037722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039048', 'logps_train/rejected': '-97.315', 'logps_train/chosen': '-109.34', 'loss/train': '0.68591', 'examples_per_second': '30.756', 'grad_norm': '23.125', 'counters/examples': 168640, 'counters/updates': 5270}
train stats after 168672 examples: {'rewards_train/chosen': '0.091897', 'rewards_train/rejected': '0.059643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032254', 'logps_train/rejected': '-126.41', 'logps_train/chosen': '-153.84', 'loss/train': '0.68399', 'examples_per_second': '31.576', 'grad_norm': '28', 'counters/examples': 168672, 'counters/updates': 5271}
train stats after 168704 examples: {'rewards_train/chosen': '0.1441', 'rewards_train/rejected': '0.024497', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1196', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-134.18', 'loss/train': '0.64706', 'examples_per_second': '31.533', 'grad_norm': '25.375', 'counters/examples': 168704, 'counters/updates': 5272}
train stats after 168736 examples: {'rewards_train/chosen': '0.25709', 'rewards_train/rejected': '0.095989', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1611', 'logps_train/rejected': '-162.27', 'logps_train/chosen': '-196.27', 'loss/train': '0.63245', 'examples_per_second': '31.367', 'grad_norm': '28.25', 'counters/examples': 168736, 'counters/updates': 5273}
train stats after 168768 examples: {'rewards_train/chosen': '0.1153', 'rewards_train/rejected': '0.074173', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041123', 'logps_train/rejected': '-108.27', 'logps_train/chosen': '-130.36', 'loss/train': '0.68182', 'examples_per_second': '31.946', 'grad_norm': '27.625', 'counters/examples': 168768, 'counters/updates': 5274}
skipping logging after 168800 examples to avoid logging too frequently
train stats after 168832 examples: {'rewards_train/chosen': '0.16209', 'rewards_train/rejected': '0.052484', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10961', 'logps_train/rejected': '-180.38', 'logps_train/chosen': '-152.11', 'loss/train': '0.65141', 'examples_per_second': '30.633', 'grad_norm': '30', 'counters/examples': 168832, 'counters/updates': 5276}
train stats after 168864 examples: {'rewards_train/chosen': '0.094865', 'rewards_train/rejected': '0.041753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053111', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-122.87', 'loss/train': '0.68208', 'examples_per_second': '32.192', 'grad_norm': '27.375', 'counters/examples': 168864, 'counters/updates': 5277}
train stats after 168896 examples: {'rewards_train/chosen': '0.076381', 'rewards_train/rejected': '0.11768', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041296', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-125.88', 'loss/train': '0.7226', 'examples_per_second': '31.304', 'grad_norm': '26.125', 'counters/examples': 168896, 'counters/updates': 5278}
train stats after 168928 examples: {'rewards_train/chosen': '0.043749', 'rewards_train/rejected': '0.011484', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032265', 'logps_train/rejected': '-76.284', 'logps_train/chosen': '-96.547', 'loss/train': '0.6816', 'examples_per_second': '31.597', 'grad_norm': '23.875', 'counters/examples': 168928, 'counters/updates': 5279}
train stats after 168960 examples: {'rewards_train/chosen': '0.096609', 'rewards_train/rejected': '0.017115', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079494', 'logps_train/rejected': '-119.47', 'logps_train/chosen': '-151.62', 'loss/train': '0.66706', 'examples_per_second': '31.531', 'grad_norm': '31.125', 'counters/examples': 168960, 'counters/updates': 5280}
train stats after 168992 examples: {'rewards_train/chosen': '0.10796', 'rewards_train/rejected': '0.028761', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079197', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-121.19', 'loss/train': '0.66504', 'examples_per_second': '31.962', 'grad_norm': '24.875', 'counters/examples': 168992, 'counters/updates': 5281}
train stats after 169024 examples: {'rewards_train/chosen': '0.14764', 'rewards_train/rejected': '0.022831', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12481', 'logps_train/rejected': '-106.9', 'logps_train/chosen': '-145.99', 'loss/train': '0.64315', 'examples_per_second': '31.997', 'grad_norm': '22.875', 'counters/examples': 169024, 'counters/updates': 5282}
train stats after 169056 examples: {'rewards_train/chosen': '0.14935', 'rewards_train/rejected': '0.071644', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.077703', 'logps_train/rejected': '-146.47', 'logps_train/chosen': '-181.53', 'loss/train': '0.66564', 'examples_per_second': '32.285', 'grad_norm': '28.25', 'counters/examples': 169056, 'counters/updates': 5283}
skipping logging after 169088 examples to avoid logging too frequently
train stats after 169120 examples: {'rewards_train/chosen': '0.10144', 'rewards_train/rejected': '-0.0321', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13354', 'logps_train/rejected': '-127.23', 'logps_train/chosen': '-139.68', 'loss/train': '0.6441', 'examples_per_second': '30.664', 'grad_norm': '26.75', 'counters/examples': 169120, 'counters/updates': 5285}
train stats after 169152 examples: {'rewards_train/chosen': '0.071896', 'rewards_train/rejected': '0.0053144', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066582', 'logps_train/rejected': '-128.24', 'logps_train/chosen': '-137.87', 'loss/train': '0.67375', 'examples_per_second': '32.26', 'grad_norm': '27', 'counters/examples': 169152, 'counters/updates': 5286}
train stats after 169184 examples: {'rewards_train/chosen': '0.14083', 'rewards_train/rejected': '0.048714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092118', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-162.75', 'loss/train': '0.6573', 'examples_per_second': '32.346', 'grad_norm': '25', 'counters/examples': 169184, 'counters/updates': 5287}
train stats after 169216 examples: {'rewards_train/chosen': '0.095882', 'rewards_train/rejected': '-0.0035928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099475', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-117.88', 'loss/train': '0.65352', 'examples_per_second': '31.616', 'grad_norm': '24', 'counters/examples': 169216, 'counters/updates': 5288}
train stats after 169248 examples: {'rewards_train/chosen': '0.17324', 'rewards_train/rejected': '0.071498', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10174', 'logps_train/rejected': '-173.5', 'logps_train/chosen': '-154.25', 'loss/train': '0.66226', 'examples_per_second': '31.805', 'grad_norm': '29.5', 'counters/examples': 169248, 'counters/updates': 5289}
skipping logging after 169280 examples to avoid logging too frequently
train stats after 169312 examples: {'rewards_train/chosen': '0.13347', 'rewards_train/rejected': '0.0256', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10787', 'logps_train/rejected': '-103.23', 'logps_train/chosen': '-139.2', 'loss/train': '0.65479', 'examples_per_second': '31.574', 'grad_norm': '25.125', 'counters/examples': 169312, 'counters/updates': 5291}
train stats after 169344 examples: {'rewards_train/chosen': '0.19067', 'rewards_train/rejected': '0.025845', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16483', 'logps_train/rejected': '-153.07', 'logps_train/chosen': '-174.21', 'loss/train': '0.63119', 'examples_per_second': '32.271', 'grad_norm': '31', 'counters/examples': 169344, 'counters/updates': 5292}
train stats after 169376 examples: {'rewards_train/chosen': '0.10705', 'rewards_train/rejected': '0.015073', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091981', 'logps_train/rejected': '-117.66', 'logps_train/chosen': '-125.29', 'loss/train': '0.65339', 'examples_per_second': '30.921', 'grad_norm': '26.125', 'counters/examples': 169376, 'counters/updates': 5293}
train stats after 169408 examples: {'rewards_train/chosen': '0.029361', 'rewards_train/rejected': '0.080504', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.051143', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-114.22', 'loss/train': '0.73368', 'examples_per_second': '30.667', 'grad_norm': '24.625', 'counters/examples': 169408, 'counters/updates': 5294}
train stats after 169440 examples: {'rewards_train/chosen': '0.11691', 'rewards_train/rejected': '0.0080913', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10882', 'logps_train/rejected': '-145.08', 'logps_train/chosen': '-148.15', 'loss/train': '0.64954', 'examples_per_second': '31.685', 'grad_norm': '27.375', 'counters/examples': 169440, 'counters/updates': 5295}
skipping logging after 169472 examples to avoid logging too frequently
train stats after 169504 examples: {'rewards_train/chosen': '0.10499', 'rewards_train/rejected': '0.12331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.018318', 'logps_train/rejected': '-122.56', 'logps_train/chosen': '-142', 'loss/train': '0.71865', 'examples_per_second': '30.878', 'grad_norm': '27.375', 'counters/examples': 169504, 'counters/updates': 5297}
train stats after 169536 examples: {'rewards_train/chosen': '0.11847', 'rewards_train/rejected': '0.074034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044434', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-108.64', 'loss/train': '0.68322', 'examples_per_second': '31.534', 'grad_norm': '28.125', 'counters/examples': 169536, 'counters/updates': 5298}
train stats after 169568 examples: {'rewards_train/chosen': '0.10618', 'rewards_train/rejected': '0.052737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053444', 'logps_train/rejected': '-135.95', 'logps_train/chosen': '-153.68', 'loss/train': '0.68296', 'examples_per_second': '31.582', 'grad_norm': '29.5', 'counters/examples': 169568, 'counters/updates': 5299}
train stats after 169600 examples: {'rewards_train/chosen': '0.061061', 'rewards_train/rejected': '0.044193', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016868', 'logps_train/rejected': '-150.1', 'logps_train/chosen': '-113.3', 'loss/train': '0.69308', 'examples_per_second': '32.732', 'grad_norm': '27.75', 'counters/examples': 169600, 'counters/updates': 5300}
skipping logging after 169632 examples to avoid logging too frequently
train stats after 169664 examples: {'rewards_train/chosen': '0.091305', 'rewards_train/rejected': '0.01588', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075424', 'logps_train/rejected': '-120.35', 'logps_train/chosen': '-144.39', 'loss/train': '0.6704', 'examples_per_second': '37.23', 'grad_norm': '29.875', 'counters/examples': 169664, 'counters/updates': 5302}
train stats after 169696 examples: {'rewards_train/chosen': '0.11537', 'rewards_train/rejected': '0.014914', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10046', 'logps_train/rejected': '-135.43', 'logps_train/chosen': '-139.37', 'loss/train': '0.65627', 'examples_per_second': '32.445', 'grad_norm': '24.875', 'counters/examples': 169696, 'counters/updates': 5303}
train stats after 169728 examples: {'rewards_train/chosen': '0.15087', 'rewards_train/rejected': '-0.036804', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18768', 'logps_train/rejected': '-108.04', 'logps_train/chosen': '-147', 'loss/train': '0.61336', 'examples_per_second': '31.657', 'grad_norm': '24.125', 'counters/examples': 169728, 'counters/updates': 5304}
train stats after 169760 examples: {'rewards_train/chosen': '0.1106', 'rewards_train/rejected': '0.060573', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050027', 'logps_train/rejected': '-107.25', 'logps_train/chosen': '-124.9', 'loss/train': '0.67675', 'examples_per_second': '30.951', 'grad_norm': '27', 'counters/examples': 169760, 'counters/updates': 5305}
train stats after 169792 examples: {'rewards_train/chosen': '0.046568', 'rewards_train/rejected': '0.049256', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0026881', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-136.53', 'loss/train': '0.7034', 'examples_per_second': '33.047', 'grad_norm': '26.125', 'counters/examples': 169792, 'counters/updates': 5306}
skipping logging after 169824 examples to avoid logging too frequently
train stats after 169856 examples: {'rewards_train/chosen': '0.14393', 'rewards_train/rejected': '0.079705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064225', 'logps_train/rejected': '-114.18', 'logps_train/chosen': '-137.79', 'loss/train': '0.67195', 'examples_per_second': '31.471', 'grad_norm': '26.625', 'counters/examples': 169856, 'counters/updates': 5308}
skipping logging after 169888 examples to avoid logging too frequently
train stats after 169920 examples: {'rewards_train/chosen': '0.16438', 'rewards_train/rejected': '0.031334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13305', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-142.93', 'loss/train': '0.64583', 'examples_per_second': '30.854', 'grad_norm': '28.875', 'counters/examples': 169920, 'counters/updates': 5310}
skipping logging after 169952 examples to avoid logging too frequently
train stats after 169984 examples: {'rewards_train/chosen': '0.12705', 'rewards_train/rejected': '0.035846', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.091204', 'logps_train/rejected': '-117.6', 'logps_train/chosen': '-133.83', 'loss/train': '0.66083', 'examples_per_second': '32.91', 'grad_norm': '25.75', 'counters/examples': 169984, 'counters/updates': 5312}
train stats after 170016 examples: {'rewards_train/chosen': '0.10528', 'rewards_train/rejected': '0.039239', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066039', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-155.35', 'loss/train': '0.67168', 'examples_per_second': '30.494', 'grad_norm': '26.75', 'counters/examples': 170016, 'counters/updates': 5313}
skipping logging after 170048 examples to avoid logging too frequently
train stats after 170080 examples: {'rewards_train/chosen': '0.033121', 'rewards_train/rejected': '0.038824', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0057024', 'logps_train/rejected': '-133.2', 'logps_train/chosen': '-79.932', 'loss/train': '0.70224', 'examples_per_second': '35.032', 'grad_norm': '26.5', 'counters/examples': 170080, 'counters/updates': 5315}
train stats after 170112 examples: {'rewards_train/chosen': '0.11944', 'rewards_train/rejected': '-0.065713', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18515', 'logps_train/rejected': '-140.88', 'logps_train/chosen': '-138.01', 'loss/train': '0.61651', 'examples_per_second': '30.246', 'grad_norm': '26', 'counters/examples': 170112, 'counters/updates': 5316}
train stats after 170144 examples: {'rewards_train/chosen': '0.11705', 'rewards_train/rejected': '-0.0098238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12688', 'logps_train/rejected': '-115.1', 'logps_train/chosen': '-146.14', 'loss/train': '0.64355', 'examples_per_second': '30.81', 'grad_norm': '25', 'counters/examples': 170144, 'counters/updates': 5317}
train stats after 170176 examples: {'rewards_train/chosen': '0.18522', 'rewards_train/rejected': '0.095265', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089956', 'logps_train/rejected': '-104.38', 'logps_train/chosen': '-159.17', 'loss/train': '0.66399', 'examples_per_second': '31.471', 'grad_norm': '28.875', 'counters/examples': 170176, 'counters/updates': 5318}
train stats after 170208 examples: {'rewards_train/chosen': '0.16521', 'rewards_train/rejected': '0.070031', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095175', 'logps_train/rejected': '-118.01', 'logps_train/chosen': '-131.1', 'loss/train': '0.65539', 'examples_per_second': '31.78', 'grad_norm': '31.75', 'counters/examples': 170208, 'counters/updates': 5319}
skipping logging after 170240 examples to avoid logging too frequently
train stats after 170272 examples: {'rewards_train/chosen': '0.078375', 'rewards_train/rejected': '-0.046633', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12501', 'logps_train/rejected': '-94.302', 'logps_train/chosen': '-140.41', 'loss/train': '0.64361', 'examples_per_second': '34.184', 'grad_norm': '22.125', 'counters/examples': 170272, 'counters/updates': 5321}
train stats after 170304 examples: {'rewards_train/chosen': '0.12924', 'rewards_train/rejected': '0.12265', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0065958', 'logps_train/rejected': '-110.22', 'logps_train/chosen': '-148.34', 'loss/train': '0.70469', 'examples_per_second': '31.818', 'grad_norm': '25.75', 'counters/examples': 170304, 'counters/updates': 5322}
skipping logging after 170336 examples to avoid logging too frequently
train stats after 170368 examples: {'rewards_train/chosen': '0.12497', 'rewards_train/rejected': '0.0012905', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12367', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-131.85', 'loss/train': '0.64609', 'examples_per_second': '31.735', 'grad_norm': '23.625', 'counters/examples': 170368, 'counters/updates': 5324}
train stats after 170400 examples: {'rewards_train/chosen': '0.040848', 'rewards_train/rejected': '-0.080903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12175', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-139.18', 'loss/train': '0.65396', 'examples_per_second': '31.363', 'grad_norm': '26.125', 'counters/examples': 170400, 'counters/updates': 5325}
train stats after 170432 examples: {'rewards_train/chosen': '0.14135', 'rewards_train/rejected': '0.1339', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0074432', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-132.79', 'loss/train': '0.69978', 'examples_per_second': '31.507', 'grad_norm': '30.25', 'counters/examples': 170432, 'counters/updates': 5326}
train stats after 170464 examples: {'rewards_train/chosen': '0.108', 'rewards_train/rejected': '-0.011311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11932', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-147.81', 'loss/train': '0.65777', 'examples_per_second': '30.627', 'grad_norm': '27.75', 'counters/examples': 170464, 'counters/updates': 5327}
train stats after 170496 examples: {'rewards_train/chosen': '0.083866', 'rewards_train/rejected': '-0.00018756', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084053', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-118.02', 'loss/train': '0.66199', 'examples_per_second': '31.397', 'grad_norm': '27.5', 'counters/examples': 170496, 'counters/updates': 5328}
train stats after 170528 examples: {'rewards_train/chosen': '0.02954', 'rewards_train/rejected': '-0.041113', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070653', 'logps_train/rejected': '-100.85', 'logps_train/chosen': '-140.53', 'loss/train': '0.66937', 'examples_per_second': '31.49', 'grad_norm': '26.625', 'counters/examples': 170528, 'counters/updates': 5329}
train stats after 170560 examples: {'rewards_train/chosen': '0.14175', 'rewards_train/rejected': '0.023413', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11834', 'logps_train/rejected': '-93.011', 'logps_train/chosen': '-144.11', 'loss/train': '0.64956', 'examples_per_second': '31.744', 'grad_norm': '24.25', 'counters/examples': 170560, 'counters/updates': 5330}
train stats after 170592 examples: {'rewards_train/chosen': '0.044975', 'rewards_train/rejected': '0.049929', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0049535', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-138.4', 'loss/train': '0.70234', 'examples_per_second': '31.473', 'grad_norm': '28.125', 'counters/examples': 170592, 'counters/updates': 5331}
train stats after 170624 examples: {'rewards_train/chosen': '0.031608', 'rewards_train/rejected': '-0.034528', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066136', 'logps_train/rejected': '-109.65', 'logps_train/chosen': '-118.51', 'loss/train': '0.67293', 'examples_per_second': '31.756', 'grad_norm': '24.375', 'counters/examples': 170624, 'counters/updates': 5332}
train stats after 170656 examples: {'rewards_train/chosen': '0.13123', 'rewards_train/rejected': '0.068175', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063051', 'logps_train/rejected': '-148.48', 'logps_train/chosen': '-127.82', 'loss/train': '0.6792', 'examples_per_second': '32.032', 'grad_norm': '26.125', 'counters/examples': 170656, 'counters/updates': 5333}
train stats after 170688 examples: {'rewards_train/chosen': '0.09239', 'rewards_train/rejected': '0.037385', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055005', 'logps_train/rejected': '-98.448', 'logps_train/chosen': '-129.34', 'loss/train': '0.67714', 'examples_per_second': '32.532', 'grad_norm': '27.125', 'counters/examples': 170688, 'counters/updates': 5334}
train stats after 170720 examples: {'rewards_train/chosen': '0.1089', 'rewards_train/rejected': '0.074217', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034682', 'logps_train/rejected': '-146.06', 'logps_train/chosen': '-146.84', 'loss/train': '0.71756', 'examples_per_second': '31.513', 'grad_norm': '32', 'counters/examples': 170720, 'counters/updates': 5335}
train stats after 170752 examples: {'rewards_train/chosen': '0.090882', 'rewards_train/rejected': '0.10923', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018345', 'logps_train/rejected': '-115.1', 'logps_train/chosen': '-137.86', 'loss/train': '0.71117', 'examples_per_second': '31.63', 'grad_norm': '26.75', 'counters/examples': 170752, 'counters/updates': 5336}
train stats after 170784 examples: {'rewards_train/chosen': '0.13964', 'rewards_train/rejected': '0.041616', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098026', 'logps_train/rejected': '-159.68', 'logps_train/chosen': '-138.38', 'loss/train': '0.6604', 'examples_per_second': '31.311', 'grad_norm': '26.5', 'counters/examples': 170784, 'counters/updates': 5337}
train stats after 170816 examples: {'rewards_train/chosen': '0.10354', 'rewards_train/rejected': '0.0066191', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096925', 'logps_train/rejected': '-129.6', 'logps_train/chosen': '-114.79', 'loss/train': '0.65551', 'examples_per_second': '31.453', 'grad_norm': '27.25', 'counters/examples': 170816, 'counters/updates': 5338}
skipping logging after 170848 examples to avoid logging too frequently
train stats after 170880 examples: {'rewards_train/chosen': '0.17185', 'rewards_train/rejected': '-0.05154', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.22339', 'logps_train/rejected': '-97.746', 'logps_train/chosen': '-117.53', 'loss/train': '0.59368', 'examples_per_second': '43.435', 'grad_norm': '20.375', 'counters/examples': 170880, 'counters/updates': 5340}
train stats after 170912 examples: {'rewards_train/chosen': '0.11292', 'rewards_train/rejected': '-0.061544', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17446', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-131.24', 'loss/train': '0.6269', 'examples_per_second': '32.492', 'grad_norm': '25.5', 'counters/examples': 170912, 'counters/updates': 5341}
train stats after 170944 examples: {'rewards_train/chosen': '0.072237', 'rewards_train/rejected': '0.046775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025461', 'logps_train/rejected': '-103.24', 'logps_train/chosen': '-112', 'loss/train': '0.69029', 'examples_per_second': '32.383', 'grad_norm': '23.625', 'counters/examples': 170944, 'counters/updates': 5342}
train stats after 170976 examples: {'rewards_train/chosen': '0.10601', 'rewards_train/rejected': '-0.0083643', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11437', 'logps_train/rejected': '-101.26', 'logps_train/chosen': '-121.7', 'loss/train': '0.64676', 'examples_per_second': '31.154', 'grad_norm': '23', 'counters/examples': 170976, 'counters/updates': 5343}
train stats after 171008 examples: {'rewards_train/chosen': '0.079445', 'rewards_train/rejected': '-0.025093', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10454', 'logps_train/rejected': '-118.08', 'logps_train/chosen': '-124.25', 'loss/train': '0.65217', 'examples_per_second': '31.779', 'grad_norm': '25.875', 'counters/examples': 171008, 'counters/updates': 5344}
train stats after 171040 examples: {'rewards_train/chosen': '0.19987', 'rewards_train/rejected': '0.029094', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17078', 'logps_train/rejected': '-189.42', 'logps_train/chosen': '-174.31', 'loss/train': '0.62675', 'examples_per_second': '31.53', 'grad_norm': '29.875', 'counters/examples': 171040, 'counters/updates': 5345}
train stats after 171072 examples: {'rewards_train/chosen': '0.14755', 'rewards_train/rejected': '0.035341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11221', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-159.2', 'loss/train': '0.64636', 'examples_per_second': '31.693', 'grad_norm': '25.25', 'counters/examples': 171072, 'counters/updates': 5346}
skipping logging after 171104 examples to avoid logging too frequently
train stats after 171136 examples: {'rewards_train/chosen': '0.084593', 'rewards_train/rejected': '0.068233', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01636', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-143.94', 'loss/train': '0.69352', 'examples_per_second': '31.58', 'grad_norm': '26.875', 'counters/examples': 171136, 'counters/updates': 5348}
train stats after 171168 examples: {'rewards_train/chosen': '0.067498', 'rewards_train/rejected': '-0.010882', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07838', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-102.96', 'loss/train': '0.66396', 'examples_per_second': '33.182', 'grad_norm': '23.625', 'counters/examples': 171168, 'counters/updates': 5349}
train stats after 171200 examples: {'rewards_train/chosen': '0.11889', 'rewards_train/rejected': '0.003137', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11575', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-153.72', 'loss/train': '0.64779', 'examples_per_second': '31.595', 'grad_norm': '26.875', 'counters/examples': 171200, 'counters/updates': 5350}
train stats after 171232 examples: {'rewards_train/chosen': '0.17864', 'rewards_train/rejected': '0.0060633', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17257', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-154', 'loss/train': '0.62088', 'examples_per_second': '31.457', 'grad_norm': '25.625', 'counters/examples': 171232, 'counters/updates': 5351}
train stats after 171264 examples: {'rewards_train/chosen': '0.14405', 'rewards_train/rejected': '0.10568', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038368', 'logps_train/rejected': '-166.19', 'logps_train/chosen': '-160.85', 'loss/train': '0.70902', 'examples_per_second': '32.128', 'grad_norm': '34.75', 'counters/examples': 171264, 'counters/updates': 5352}
train stats after 171296 examples: {'rewards_train/chosen': '0.18037', 'rewards_train/rejected': '0.0033081', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17706', 'logps_train/rejected': '-99.12', 'logps_train/chosen': '-115.15', 'loss/train': '0.62803', 'examples_per_second': '32.064', 'grad_norm': '21.5', 'counters/examples': 171296, 'counters/updates': 5353}
train stats after 171328 examples: {'rewards_train/chosen': '0.089323', 'rewards_train/rejected': '0.077815', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011508', 'logps_train/rejected': '-134.14', 'logps_train/chosen': '-131.6', 'loss/train': '0.69669', 'examples_per_second': '30.598', 'grad_norm': '28', 'counters/examples': 171328, 'counters/updates': 5354}
train stats after 171360 examples: {'rewards_train/chosen': '0.16088', 'rewards_train/rejected': '-0.024162', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18504', 'logps_train/rejected': '-136.26', 'logps_train/chosen': '-157.77', 'loss/train': '0.61677', 'examples_per_second': '31.745', 'grad_norm': '30.125', 'counters/examples': 171360, 'counters/updates': 5355}
skipping logging after 171392 examples to avoid logging too frequently
train stats after 171424 examples: {'rewards_train/chosen': '0.16946', 'rewards_train/rejected': '0.042512', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12695', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-164.65', 'loss/train': '0.65363', 'examples_per_second': '31.502', 'grad_norm': '31.875', 'counters/examples': 171424, 'counters/updates': 5357}
skipping logging after 171456 examples to avoid logging too frequently
train stats after 171488 examples: {'rewards_train/chosen': '0.27818', 'rewards_train/rejected': '0.093379', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1848', 'logps_train/rejected': '-189.44', 'logps_train/chosen': '-195.44', 'loss/train': '0.61794', 'examples_per_second': '24.476', 'grad_norm': '31.375', 'counters/examples': 171488, 'counters/updates': 5359}
train stats after 171520 examples: {'rewards_train/chosen': '0.18069', 'rewards_train/rejected': '0.064263', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11643', 'logps_train/rejected': '-106.97', 'logps_train/chosen': '-133.38', 'loss/train': '0.65178', 'examples_per_second': '30.135', 'grad_norm': '23.25', 'counters/examples': 171520, 'counters/updates': 5360}
train stats after 171552 examples: {'rewards_train/chosen': '0.080359', 'rewards_train/rejected': '0.13904', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.058676', 'logps_train/rejected': '-147.34', 'logps_train/chosen': '-135.09', 'loss/train': '0.73498', 'examples_per_second': '30.767', 'grad_norm': '30.875', 'counters/examples': 171552, 'counters/updates': 5361}
train stats after 171584 examples: {'rewards_train/chosen': '0.065714', 'rewards_train/rejected': '0.058298', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0074155', 'logps_train/rejected': '-106.09', 'logps_train/chosen': '-166.98', 'loss/train': '0.70218', 'examples_per_second': '30.276', 'grad_norm': '27.125', 'counters/examples': 171584, 'counters/updates': 5362}
train stats after 171616 examples: {'rewards_train/chosen': '0.1243', 'rewards_train/rejected': '0.065058', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059243', 'logps_train/rejected': '-159.46', 'logps_train/chosen': '-122.11', 'loss/train': '0.67615', 'examples_per_second': '30.662', 'grad_norm': '26.375', 'counters/examples': 171616, 'counters/updates': 5363}
train stats after 171648 examples: {'rewards_train/chosen': '0.094871', 'rewards_train/rejected': '0.040771', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0541', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-165.26', 'loss/train': '0.67973', 'examples_per_second': '31.883', 'grad_norm': '30.75', 'counters/examples': 171648, 'counters/updates': 5364}
train stats after 171680 examples: {'rewards_train/chosen': '0.19604', 'rewards_train/rejected': '0.04809', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.14795', 'logps_train/rejected': '-135.81', 'logps_train/chosen': '-145.63', 'loss/train': '0.63249', 'examples_per_second': '31.328', 'grad_norm': '28.625', 'counters/examples': 171680, 'counters/updates': 5365}
train stats after 171712 examples: {'rewards_train/chosen': '0.092038', 'rewards_train/rejected': '-0.0069919', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.09903', 'logps_train/rejected': '-103.69', 'logps_train/chosen': '-132.23', 'loss/train': '0.65331', 'examples_per_second': '32.704', 'grad_norm': '23.125', 'counters/examples': 171712, 'counters/updates': 5366}
skipping logging after 171744 examples to avoid logging too frequently
train stats after 171776 examples: {'rewards_train/chosen': '0.17938', 'rewards_train/rejected': '0.076583', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1028', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-131.33', 'loss/train': '0.65044', 'examples_per_second': '31.418', 'grad_norm': '26.375', 'counters/examples': 171776, 'counters/updates': 5368}
train stats after 171808 examples: {'rewards_train/chosen': '0.10924', 'rewards_train/rejected': '0.11738', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0081322', 'logps_train/rejected': '-154.36', 'logps_train/chosen': '-156.27', 'loss/train': '0.70931', 'examples_per_second': '32.339', 'grad_norm': '31.875', 'counters/examples': 171808, 'counters/updates': 5369}
train stats after 171840 examples: {'rewards_train/chosen': '0.099347', 'rewards_train/rejected': '0.062654', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.036693', 'logps_train/rejected': '-165.97', 'logps_train/chosen': '-189.94', 'loss/train': '0.68761', 'examples_per_second': '32.367', 'grad_norm': '34.25', 'counters/examples': 171840, 'counters/updates': 5370}
train stats after 171872 examples: {'rewards_train/chosen': '0.11446', 'rewards_train/rejected': '-0.086146', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2006', 'logps_train/rejected': '-135.14', 'logps_train/chosen': '-165.24', 'loss/train': '0.61585', 'examples_per_second': '30.608', 'grad_norm': '26.125', 'counters/examples': 171872, 'counters/updates': 5371}
train stats after 171904 examples: {'rewards_train/chosen': '0.16421', 'rewards_train/rejected': '0.03161', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1326', 'logps_train/rejected': '-123.88', 'logps_train/chosen': '-150.79', 'loss/train': '0.64066', 'examples_per_second': '31.917', 'grad_norm': '27', 'counters/examples': 171904, 'counters/updates': 5372}
train stats after 171936 examples: {'rewards_train/chosen': '0.096431', 'rewards_train/rejected': '0.022351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07408', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-187.9', 'loss/train': '0.67512', 'examples_per_second': '31.447', 'grad_norm': '28.875', 'counters/examples': 171936, 'counters/updates': 5373}
train stats after 171968 examples: {'rewards_train/chosen': '0.10634', 'rewards_train/rejected': '-0.083879', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19022', 'logps_train/rejected': '-92.63', 'logps_train/chosen': '-124.03', 'loss/train': '0.6221', 'examples_per_second': '31.575', 'grad_norm': '24', 'counters/examples': 171968, 'counters/updates': 5374}
train stats after 172000 examples: {'rewards_train/chosen': '0.17378', 'rewards_train/rejected': '0.067524', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10626', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-139.97', 'loss/train': '0.65782', 'examples_per_second': '31.441', 'grad_norm': '23.5', 'counters/examples': 172000, 'counters/updates': 5375}
train stats after 172032 examples: {'rewards_train/chosen': '0.17437', 'rewards_train/rejected': '0.013215', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16116', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-169.06', 'loss/train': '0.62298', 'examples_per_second': '32.457', 'grad_norm': '31', 'counters/examples': 172032, 'counters/updates': 5376}
train stats after 172064 examples: {'rewards_train/chosen': '0.14355', 'rewards_train/rejected': '0.0064728', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13707', 'logps_train/rejected': '-151.15', 'logps_train/chosen': '-139.91', 'loss/train': '0.63878', 'examples_per_second': '31.562', 'grad_norm': '29.5', 'counters/examples': 172064, 'counters/updates': 5377}
train stats after 172096 examples: {'rewards_train/chosen': '0.084984', 'rewards_train/rejected': '-0.010068', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095053', 'logps_train/rejected': '-121.33', 'logps_train/chosen': '-134.46', 'loss/train': '0.66351', 'examples_per_second': '31.408', 'grad_norm': '24.875', 'counters/examples': 172096, 'counters/updates': 5378}
train stats after 172128 examples: {'rewards_train/chosen': '0.13697', 'rewards_train/rejected': '0.048149', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08882', 'logps_train/rejected': '-138.92', 'logps_train/chosen': '-169.1', 'loss/train': '0.66054', 'examples_per_second': '32.498', 'grad_norm': '30.5', 'counters/examples': 172128, 'counters/updates': 5379}
train stats after 172160 examples: {'rewards_train/chosen': '0.18572', 'rewards_train/rejected': '0.015159', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17056', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-134.68', 'loss/train': '0.62311', 'examples_per_second': '30.033', 'grad_norm': '24.75', 'counters/examples': 172160, 'counters/updates': 5380}
train stats after 172192 examples: {'rewards_train/chosen': '0.10497', 'rewards_train/rejected': '0.067037', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037936', 'logps_train/rejected': '-115.16', 'logps_train/chosen': '-147.28', 'loss/train': '0.68379', 'examples_per_second': '31.469', 'grad_norm': '26.375', 'counters/examples': 172192, 'counters/updates': 5381}
train stats after 172224 examples: {'rewards_train/chosen': '-0.013764', 'rewards_train/rejected': '-0.018929', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.005165', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-141.54', 'loss/train': '0.69595', 'examples_per_second': '32.891', 'grad_norm': '30.875', 'counters/examples': 172224, 'counters/updates': 5382}
train stats after 172256 examples: {'rewards_train/chosen': '0.10906', 'rewards_train/rejected': '-0.045122', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15418', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-114.16', 'loss/train': '0.63318', 'examples_per_second': '31.318', 'grad_norm': '23.25', 'counters/examples': 172256, 'counters/updates': 5383}
train stats after 172288 examples: {'rewards_train/chosen': '0.15658', 'rewards_train/rejected': '0.062535', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094048', 'logps_train/rejected': '-143.96', 'logps_train/chosen': '-141.78', 'loss/train': '0.65853', 'examples_per_second': '31.595', 'grad_norm': '32.75', 'counters/examples': 172288, 'counters/updates': 5384}
train stats after 172320 examples: {'rewards_train/chosen': '0.17684', 'rewards_train/rejected': '0.050189', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12665', 'logps_train/rejected': '-137.03', 'logps_train/chosen': '-160.99', 'loss/train': '0.6369', 'examples_per_second': '32.376', 'grad_norm': '27.75', 'counters/examples': 172320, 'counters/updates': 5385}
skipping logging after 172352 examples to avoid logging too frequently
train stats after 172384 examples: {'rewards_train/chosen': '0.048279', 'rewards_train/rejected': '0.068581', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020303', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-116.33', 'loss/train': '0.71011', 'examples_per_second': '36.53', 'grad_norm': '25.375', 'counters/examples': 172384, 'counters/updates': 5387}
train stats after 172416 examples: {'rewards_train/chosen': '0.089688', 'rewards_train/rejected': '-0.011383', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10107', 'logps_train/rejected': '-143.84', 'logps_train/chosen': '-145.08', 'loss/train': '0.66292', 'examples_per_second': '30.836', 'grad_norm': '30.375', 'counters/examples': 172416, 'counters/updates': 5388}
skipping logging after 172448 examples to avoid logging too frequently
train stats after 172480 examples: {'rewards_train/chosen': '0.15106', 'rewards_train/rejected': '0.009741', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14132', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-145', 'loss/train': '0.64342', 'examples_per_second': '31.088', 'grad_norm': '26.125', 'counters/examples': 172480, 'counters/updates': 5390}
train stats after 172512 examples: {'rewards_train/chosen': '0.1784', 'rewards_train/rejected': '0.061437', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11696', 'logps_train/rejected': '-101', 'logps_train/chosen': '-181.15', 'loss/train': '0.65001', 'examples_per_second': '30.656', 'grad_norm': '27.25', 'counters/examples': 172512, 'counters/updates': 5391}
train stats after 172544 examples: {'rewards_train/chosen': '0.20462', 'rewards_train/rejected': '0.027873', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17675', 'logps_train/rejected': '-99.28', 'logps_train/chosen': '-124.33', 'loss/train': '0.61648', 'examples_per_second': '31.252', 'grad_norm': '22.375', 'counters/examples': 172544, 'counters/updates': 5392}
train stats after 172576 examples: {'rewards_train/chosen': '0.16072', 'rewards_train/rejected': '0.0090808', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15164', 'logps_train/rejected': '-167.53', 'logps_train/chosen': '-151.9', 'loss/train': '0.64225', 'examples_per_second': '30.003', 'grad_norm': '32.25', 'counters/examples': 172576, 'counters/updates': 5393}
train stats after 172608 examples: {'rewards_train/chosen': '0.1084', 'rewards_train/rejected': '0.0020332', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10636', 'logps_train/rejected': '-92.719', 'logps_train/chosen': '-135.83', 'loss/train': '0.65511', 'examples_per_second': '30.043', 'grad_norm': '22.75', 'counters/examples': 172608, 'counters/updates': 5394}
skipping logging after 172640 examples to avoid logging too frequently
train stats after 172672 examples: {'rewards_train/chosen': '0.12757', 'rewards_train/rejected': '-0.041667', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16924', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-144.81', 'loss/train': '0.62413', 'examples_per_second': '32.869', 'grad_norm': '24', 'counters/examples': 172672, 'counters/updates': 5396}
train stats after 172704 examples: {'rewards_train/chosen': '0.1859', 'rewards_train/rejected': '0.073', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1129', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-165.81', 'loss/train': '0.64737', 'examples_per_second': '31.584', 'grad_norm': '25.5', 'counters/examples': 172704, 'counters/updates': 5397}
train stats after 172736 examples: {'rewards_train/chosen': '0.074486', 'rewards_train/rejected': '0.033594', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040892', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-151.84', 'loss/train': '0.68432', 'examples_per_second': '31.637', 'grad_norm': '27.875', 'counters/examples': 172736, 'counters/updates': 5398}
train stats after 172768 examples: {'rewards_train/chosen': '0.14238', 'rewards_train/rejected': '0.0056133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13676', 'logps_train/rejected': '-101.22', 'logps_train/chosen': '-129.82', 'loss/train': '0.64013', 'examples_per_second': '31.579', 'grad_norm': '24.875', 'counters/examples': 172768, 'counters/updates': 5399}
train stats after 172800 examples: {'rewards_train/chosen': '0.10254', 'rewards_train/rejected': '0.022483', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080057', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-126.14', 'loss/train': '0.66594', 'examples_per_second': '30.362', 'grad_norm': '25.625', 'counters/examples': 172800, 'counters/updates': 5400}
train stats after 172832 examples: {'rewards_train/chosen': '0.21275', 'rewards_train/rejected': '0.14533', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067426', 'logps_train/rejected': '-155.38', 'logps_train/chosen': '-149.26', 'loss/train': '0.6693', 'examples_per_second': '30.701', 'grad_norm': '27.875', 'counters/examples': 172832, 'counters/updates': 5401}
train stats after 172864 examples: {'rewards_train/chosen': '0.14031', 'rewards_train/rejected': '0.071156', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069157', 'logps_train/rejected': '-139.35', 'logps_train/chosen': '-119.53', 'loss/train': '0.67011', 'examples_per_second': '32.612', 'grad_norm': '27.75', 'counters/examples': 172864, 'counters/updates': 5402}
train stats after 172896 examples: {'rewards_train/chosen': '0.19936', 'rewards_train/rejected': '0.13015', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069212', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-145.96', 'loss/train': '0.66723', 'examples_per_second': '31.436', 'grad_norm': '25.125', 'counters/examples': 172896, 'counters/updates': 5403}
train stats after 172928 examples: {'rewards_train/chosen': '0.083403', 'rewards_train/rejected': '-0.021183', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10459', 'logps_train/rejected': '-138.69', 'logps_train/chosen': '-128.6', 'loss/train': '0.65601', 'examples_per_second': '31.321', 'grad_norm': '23.625', 'counters/examples': 172928, 'counters/updates': 5404}
train stats after 172960 examples: {'rewards_train/chosen': '0.082036', 'rewards_train/rejected': '-0.071907', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15394', 'logps_train/rejected': '-119.2', 'logps_train/chosen': '-136.17', 'loss/train': '0.62595', 'examples_per_second': '29.965', 'grad_norm': '25.375', 'counters/examples': 172960, 'counters/updates': 5405}
skipping logging after 172992 examples to avoid logging too frequently
train stats after 173024 examples: {'rewards_train/chosen': '0.19082', 'rewards_train/rejected': '-0.040389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23121', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-183.17', 'loss/train': '0.60344', 'examples_per_second': '31.482', 'grad_norm': '24.5', 'counters/examples': 173024, 'counters/updates': 5407}
train stats after 173056 examples: {'rewards_train/chosen': '0.12627', 'rewards_train/rejected': '0.059825', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066442', 'logps_train/rejected': '-148.32', 'logps_train/chosen': '-140.31', 'loss/train': '0.67541', 'examples_per_second': '30.118', 'grad_norm': '29.375', 'counters/examples': 173056, 'counters/updates': 5408}
skipping logging after 173088 examples to avoid logging too frequently
train stats after 173120 examples: {'rewards_train/chosen': '0.11788', 'rewards_train/rejected': '0.084708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033169', 'logps_train/rejected': '-141.16', 'logps_train/chosen': '-137.5', 'loss/train': '0.69462', 'examples_per_second': '33.351', 'grad_norm': '33.25', 'counters/examples': 173120, 'counters/updates': 5410}
train stats after 173152 examples: {'rewards_train/chosen': '0.08023', 'rewards_train/rejected': '0.01902', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06121', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-126.43', 'loss/train': '0.67319', 'examples_per_second': '32.312', 'grad_norm': '24.75', 'counters/examples': 173152, 'counters/updates': 5411}
train stats after 173184 examples: {'rewards_train/chosen': '0.071747', 'rewards_train/rejected': '0.061226', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01052', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-134.65', 'loss/train': '0.69385', 'examples_per_second': '31.04', 'grad_norm': '28.375', 'counters/examples': 173184, 'counters/updates': 5412}
train stats after 173216 examples: {'rewards_train/chosen': '0.14331', 'rewards_train/rejected': '0.048652', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094653', 'logps_train/rejected': '-95.889', 'logps_train/chosen': '-114.99', 'loss/train': '0.65466', 'examples_per_second': '30.083', 'grad_norm': '23.75', 'counters/examples': 173216, 'counters/updates': 5413}
train stats after 173248 examples: {'rewards_train/chosen': '0.054132', 'rewards_train/rejected': '0.03017', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023962', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-137.05', 'loss/train': '0.68953', 'examples_per_second': '32.195', 'grad_norm': '29.5', 'counters/examples': 173248, 'counters/updates': 5414}
train stats after 173280 examples: {'rewards_train/chosen': '0.11471', 'rewards_train/rejected': '0.036329', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078383', 'logps_train/rejected': '-133.3', 'logps_train/chosen': '-161.65', 'loss/train': '0.67055', 'examples_per_second': '30.094', 'grad_norm': '27.625', 'counters/examples': 173280, 'counters/updates': 5415}
train stats after 173312 examples: {'rewards_train/chosen': '0.18019', 'rewards_train/rejected': '0.022671', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15752', 'logps_train/rejected': '-155.64', 'logps_train/chosen': '-174.22', 'loss/train': '0.63892', 'examples_per_second': '31.037', 'grad_norm': '27', 'counters/examples': 173312, 'counters/updates': 5416}
train stats after 173344 examples: {'rewards_train/chosen': '0.11329', 'rewards_train/rejected': '-0.039103', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15239', 'logps_train/rejected': '-116.13', 'logps_train/chosen': '-155.66', 'loss/train': '0.6347', 'examples_per_second': '33.209', 'grad_norm': '35.75', 'counters/examples': 173344, 'counters/updates': 5417}
train stats after 173376 examples: {'rewards_train/chosen': '0.278', 'rewards_train/rejected': '0.16223', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11577', 'logps_train/rejected': '-124.73', 'logps_train/chosen': '-180.91', 'loss/train': '0.64936', 'examples_per_second': '31.604', 'grad_norm': '29.625', 'counters/examples': 173376, 'counters/updates': 5418}
train stats after 173408 examples: {'rewards_train/chosen': '0.17385', 'rewards_train/rejected': '0.038257', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1356', 'logps_train/rejected': '-136.22', 'logps_train/chosen': '-157.66', 'loss/train': '0.63368', 'examples_per_second': '32.39', 'grad_norm': '27.75', 'counters/examples': 173408, 'counters/updates': 5419}
skipping logging after 173440 examples to avoid logging too frequently
train stats after 173472 examples: {'rewards_train/chosen': '0.17056', 'rewards_train/rejected': '0.047267', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12329', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-156.33', 'loss/train': '0.64074', 'examples_per_second': '31.546', 'grad_norm': '36.75', 'counters/examples': 173472, 'counters/updates': 5421}
train stats after 173504 examples: {'rewards_train/chosen': '0.065311', 'rewards_train/rejected': '0.0601', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0052112', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-160.5', 'loss/train': '0.69957', 'examples_per_second': '31.936', 'grad_norm': '30.625', 'counters/examples': 173504, 'counters/updates': 5422}
skipping logging after 173536 examples to avoid logging too frequently
train stats after 173568 examples: {'rewards_train/chosen': '0.090064', 'rewards_train/rejected': '0.042011', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048054', 'logps_train/rejected': '-136.07', 'logps_train/chosen': '-138.81', 'loss/train': '0.67708', 'examples_per_second': '31.791', 'grad_norm': '28.375', 'counters/examples': 173568, 'counters/updates': 5424}
train stats after 173600 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '0.03405', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.088958', 'logps_train/rejected': '-138.87', 'logps_train/chosen': '-152.18', 'loss/train': '0.65957', 'examples_per_second': '32.602', 'grad_norm': '26.625', 'counters/examples': 173600, 'counters/updates': 5425}
skipping logging after 173632 examples to avoid logging too frequently
train stats after 173664 examples: {'rewards_train/chosen': '0.20318', 'rewards_train/rejected': '0.11262', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090561', 'logps_train/rejected': '-156.85', 'logps_train/chosen': '-170.93', 'loss/train': '0.6594', 'examples_per_second': '30.02', 'grad_norm': '28.625', 'counters/examples': 173664, 'counters/updates': 5427}
train stats after 173696 examples: {'rewards_train/chosen': '0.084638', 'rewards_train/rejected': '0.07946', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0051781', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-136.25', 'loss/train': '0.70475', 'examples_per_second': '31.028', 'grad_norm': '26.375', 'counters/examples': 173696, 'counters/updates': 5428}
train stats after 173728 examples: {'rewards_train/chosen': '0.16201', 'rewards_train/rejected': '0.070221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091787', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-128.21', 'loss/train': '0.66028', 'examples_per_second': '31.601', 'grad_norm': '35', 'counters/examples': 173728, 'counters/updates': 5429}
train stats after 173760 examples: {'rewards_train/chosen': '0.14046', 'rewards_train/rejected': '0.066347', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074108', 'logps_train/rejected': '-142.04', 'logps_train/chosen': '-125.09', 'loss/train': '0.67359', 'examples_per_second': '31.102', 'grad_norm': '26.5', 'counters/examples': 173760, 'counters/updates': 5430}
train stats after 173792 examples: {'rewards_train/chosen': '0.087884', 'rewards_train/rejected': '0.031986', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055898', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-112.38', 'loss/train': '0.67114', 'examples_per_second': '31.592', 'grad_norm': '34.75', 'counters/examples': 173792, 'counters/updates': 5431}
train stats after 173824 examples: {'rewards_train/chosen': '0.095096', 'rewards_train/rejected': '0.068042', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027054', 'logps_train/rejected': '-120.43', 'logps_train/chosen': '-149.98', 'loss/train': '0.69406', 'examples_per_second': '24.962', 'grad_norm': '30.375', 'counters/examples': 173824, 'counters/updates': 5432}
train stats after 173856 examples: {'rewards_train/chosen': '0.18631', 'rewards_train/rejected': '0.086459', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099848', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-133.63', 'loss/train': '0.65666', 'examples_per_second': '31.57', 'grad_norm': '27.5', 'counters/examples': 173856, 'counters/updates': 5433}
train stats after 173888 examples: {'rewards_train/chosen': '0.12392', 'rewards_train/rejected': '0.10044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023485', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-151.43', 'loss/train': '0.69242', 'examples_per_second': '31.288', 'grad_norm': '28.5', 'counters/examples': 173888, 'counters/updates': 5434}
train stats after 173920 examples: {'rewards_train/chosen': '0.14661', 'rewards_train/rejected': '0.00045409', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14616', 'logps_train/rejected': '-152.58', 'logps_train/chosen': '-192.42', 'loss/train': '0.63655', 'examples_per_second': '24.95', 'grad_norm': '33.25', 'counters/examples': 173920, 'counters/updates': 5435}
train stats after 173952 examples: {'rewards_train/chosen': '0.064255', 'rewards_train/rejected': '0.020756', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043499', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-150.05', 'loss/train': '0.6896', 'examples_per_second': '32.047', 'grad_norm': '26.625', 'counters/examples': 173952, 'counters/updates': 5436}
train stats after 173984 examples: {'rewards_train/chosen': '0.077933', 'rewards_train/rejected': '0.067678', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010255', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-143.56', 'loss/train': '0.69554', 'examples_per_second': '30.574', 'grad_norm': '25.375', 'counters/examples': 173984, 'counters/updates': 5437}
train stats after 174016 examples: {'rewards_train/chosen': '0.07658', 'rewards_train/rejected': '0.14449', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.067909', 'logps_train/rejected': '-159.44', 'logps_train/chosen': '-180.14', 'loss/train': '0.74502', 'examples_per_second': '32.533', 'grad_norm': '35', 'counters/examples': 174016, 'counters/updates': 5438}
train stats after 174048 examples: {'rewards_train/chosen': '0.081758', 'rewards_train/rejected': '0.057054', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024704', 'logps_train/rejected': '-120.32', 'logps_train/chosen': '-124.44', 'loss/train': '0.70006', 'examples_per_second': '31.574', 'grad_norm': '28.875', 'counters/examples': 174048, 'counters/updates': 5439}
train stats after 174080 examples: {'rewards_train/chosen': '0.14135', 'rewards_train/rejected': '0.15375', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012398', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-155.2', 'loss/train': '0.71248', 'examples_per_second': '31.403', 'grad_norm': '31', 'counters/examples': 174080, 'counters/updates': 5440}
train stats after 174112 examples: {'rewards_train/chosen': '0.10524', 'rewards_train/rejected': '-0.014913', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12015', 'logps_train/rejected': '-142.11', 'logps_train/chosen': '-114.79', 'loss/train': '0.64668', 'examples_per_second': '31.632', 'grad_norm': '26.75', 'counters/examples': 174112, 'counters/updates': 5441}
train stats after 174144 examples: {'rewards_train/chosen': '0.12909', 'rewards_train/rejected': '0.034096', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09499', 'logps_train/rejected': '-111.56', 'logps_train/chosen': '-128.22', 'loss/train': '0.6553', 'examples_per_second': '30.098', 'grad_norm': '31', 'counters/examples': 174144, 'counters/updates': 5442}
train stats after 174176 examples: {'rewards_train/chosen': '0.054862', 'rewards_train/rejected': '0.039458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015404', 'logps_train/rejected': '-113.54', 'logps_train/chosen': '-129.61', 'loss/train': '0.702', 'examples_per_second': '32.103', 'grad_norm': '31.25', 'counters/examples': 174176, 'counters/updates': 5443}
train stats after 174208 examples: {'rewards_train/chosen': '0.091018', 'rewards_train/rejected': '0.0031543', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.087864', 'logps_train/rejected': '-157.61', 'logps_train/chosen': '-132.75', 'loss/train': '0.66824', 'examples_per_second': '32.894', 'grad_norm': '27.25', 'counters/examples': 174208, 'counters/updates': 5444}
train stats after 174240 examples: {'rewards_train/chosen': '0.094738', 'rewards_train/rejected': '0.01937', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075368', 'logps_train/rejected': '-102.5', 'logps_train/chosen': '-151.41', 'loss/train': '0.66742', 'examples_per_second': '31.783', 'grad_norm': '27.25', 'counters/examples': 174240, 'counters/updates': 5445}
train stats after 174272 examples: {'rewards_train/chosen': '0.18083', 'rewards_train/rejected': '0.069982', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11085', 'logps_train/rejected': '-102.13', 'logps_train/chosen': '-131.68', 'loss/train': '0.65173', 'examples_per_second': '31.5', 'grad_norm': '26.25', 'counters/examples': 174272, 'counters/updates': 5446}
train stats after 174304 examples: {'rewards_train/chosen': '0.22466', 'rewards_train/rejected': '0.10175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12291', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-190.41', 'loss/train': '0.65071', 'examples_per_second': '31.077', 'grad_norm': '26', 'counters/examples': 174304, 'counters/updates': 5447}
train stats after 174336 examples: {'rewards_train/chosen': '0.12239', 'rewards_train/rejected': '0.078222', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044166', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-138.18', 'loss/train': '0.68971', 'examples_per_second': '32.513', 'grad_norm': '26.875', 'counters/examples': 174336, 'counters/updates': 5448}
train stats after 174368 examples: {'rewards_train/chosen': '0.15051', 'rewards_train/rejected': '0.047832', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10267', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-169.96', 'loss/train': '0.6521', 'examples_per_second': '31.455', 'grad_norm': '26.125', 'counters/examples': 174368, 'counters/updates': 5449}
train stats after 174400 examples: {'rewards_train/chosen': '0.12204', 'rewards_train/rejected': '0.017887', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10415', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-127.91', 'loss/train': '0.65922', 'examples_per_second': '31.946', 'grad_norm': '27.625', 'counters/examples': 174400, 'counters/updates': 5450}
train stats after 174432 examples: {'rewards_train/chosen': '0.19418', 'rewards_train/rejected': '-0.011413', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2056', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-147.12', 'loss/train': '0.61739', 'examples_per_second': '30.862', 'grad_norm': '26.75', 'counters/examples': 174432, 'counters/updates': 5451}
train stats after 174464 examples: {'rewards_train/chosen': '0.092239', 'rewards_train/rejected': '0.059604', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032635', 'logps_train/rejected': '-107.39', 'logps_train/chosen': '-129.59', 'loss/train': '0.69081', 'examples_per_second': '31.332', 'grad_norm': '24.875', 'counters/examples': 174464, 'counters/updates': 5452}
train stats after 174496 examples: {'rewards_train/chosen': '0.18338', 'rewards_train/rejected': '0.01832', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16506', 'logps_train/rejected': '-87.6', 'logps_train/chosen': '-119.49', 'loss/train': '0.65028', 'examples_per_second': '32.494', 'grad_norm': '21.625', 'counters/examples': 174496, 'counters/updates': 5453}
skipping logging after 174528 examples to avoid logging too frequently
train stats after 174560 examples: {'rewards_train/chosen': '0.11775', 'rewards_train/rejected': '0.049142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068604', 'logps_train/rejected': '-123.23', 'logps_train/chosen': '-139.31', 'loss/train': '0.66848', 'examples_per_second': '33.975', 'grad_norm': '25.375', 'counters/examples': 174560, 'counters/updates': 5455}
train stats after 174592 examples: {'rewards_train/chosen': '0.13779', 'rewards_train/rejected': '-0.023542', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16133', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-135.33', 'loss/train': '0.62501', 'examples_per_second': '31.561', 'grad_norm': '24.75', 'counters/examples': 174592, 'counters/updates': 5456}
train stats after 174624 examples: {'rewards_train/chosen': '0.051925', 'rewards_train/rejected': '3.0796e-05', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051894', 'logps_train/rejected': '-111.71', 'logps_train/chosen': '-111.55', 'loss/train': '0.67815', 'examples_per_second': '30.901', 'grad_norm': '23.5', 'counters/examples': 174624, 'counters/updates': 5457}
train stats after 174656 examples: {'rewards_train/chosen': '0.1742', 'rewards_train/rejected': '0.080431', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093767', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-177.93', 'loss/train': '0.66034', 'examples_per_second': '30.141', 'grad_norm': '27.25', 'counters/examples': 174656, 'counters/updates': 5458}
train stats after 174688 examples: {'rewards_train/chosen': '0.13836', 'rewards_train/rejected': '0.042586', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09577', 'logps_train/rejected': '-86.201', 'logps_train/chosen': '-132', 'loss/train': '0.65397', 'examples_per_second': '31.41', 'grad_norm': '24.5', 'counters/examples': 174688, 'counters/updates': 5459}
train stats after 174720 examples: {'rewards_train/chosen': '0.12739', 'rewards_train/rejected': '0.086659', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040734', 'logps_train/rejected': '-138.8', 'logps_train/chosen': '-115.11', 'loss/train': '0.68518', 'examples_per_second': '31.553', 'grad_norm': '26.25', 'counters/examples': 174720, 'counters/updates': 5460}
train stats after 174752 examples: {'rewards_train/chosen': '0.051578', 'rewards_train/rejected': '0.036444', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.015134', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-111.58', 'loss/train': '0.6988', 'examples_per_second': '32.476', 'grad_norm': '29', 'counters/examples': 174752, 'counters/updates': 5461}
train stats after 174784 examples: {'rewards_train/chosen': '0.17097', 'rewards_train/rejected': '0.080777', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090197', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-146.92', 'loss/train': '0.66246', 'examples_per_second': '30.988', 'grad_norm': '23.75', 'counters/examples': 174784, 'counters/updates': 5462}
train stats after 174816 examples: {'rewards_train/chosen': '0.12217', 'rewards_train/rejected': '0.1298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0076353', 'logps_train/rejected': '-123.95', 'logps_train/chosen': '-138.22', 'loss/train': '0.71486', 'examples_per_second': '31.251', 'grad_norm': '28.625', 'counters/examples': 174816, 'counters/updates': 5463}
train stats after 174848 examples: {'rewards_train/chosen': '0.08815', 'rewards_train/rejected': '0.031432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056717', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-118.86', 'loss/train': '0.67692', 'examples_per_second': '31.166', 'grad_norm': '23.375', 'counters/examples': 174848, 'counters/updates': 5464}
train stats after 174880 examples: {'rewards_train/chosen': '0.11279', 'rewards_train/rejected': '0.076597', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036192', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-152.42', 'loss/train': '0.6848', 'examples_per_second': '31.27', 'grad_norm': '25.5', 'counters/examples': 174880, 'counters/updates': 5465}
train stats after 174912 examples: {'rewards_train/chosen': '0.05109', 'rewards_train/rejected': '-0.012029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063119', 'logps_train/rejected': '-102.5', 'logps_train/chosen': '-107.24', 'loss/train': '0.67415', 'examples_per_second': '30.336', 'grad_norm': '22.125', 'counters/examples': 174912, 'counters/updates': 5466}
train stats after 174944 examples: {'rewards_train/chosen': '0.086445', 'rewards_train/rejected': '0.074376', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01207', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-157.54', 'loss/train': '0.70047', 'examples_per_second': '30.652', 'grad_norm': '29.375', 'counters/examples': 174944, 'counters/updates': 5467}
train stats after 174976 examples: {'rewards_train/chosen': '0.1359', 'rewards_train/rejected': '0.076856', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059047', 'logps_train/rejected': '-119.86', 'logps_train/chosen': '-149.69', 'loss/train': '0.67526', 'examples_per_second': '31.781', 'grad_norm': '27', 'counters/examples': 174976, 'counters/updates': 5468}
train stats after 175008 examples: {'rewards_train/chosen': '0.02827', 'rewards_train/rejected': '0.02336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0049105', 'logps_train/rejected': '-155.31', 'logps_train/chosen': '-167.53', 'loss/train': '0.7027', 'examples_per_second': '30.655', 'grad_norm': '30.25', 'counters/examples': 175008, 'counters/updates': 5469}
train stats after 175040 examples: {'rewards_train/chosen': '0.094787', 'rewards_train/rejected': '0.047971', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046816', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-148.41', 'loss/train': '0.68629', 'examples_per_second': '32.501', 'grad_norm': '25.125', 'counters/examples': 175040, 'counters/updates': 5470}
train stats after 175072 examples: {'rewards_train/chosen': '0.086931', 'rewards_train/rejected': '-0.0011136', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088045', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-130.68', 'loss/train': '0.66465', 'examples_per_second': '30.215', 'grad_norm': '24.875', 'counters/examples': 175072, 'counters/updates': 5471}
train stats after 175104 examples: {'rewards_train/chosen': '0.093216', 'rewards_train/rejected': '0.07031', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022906', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-161.62', 'loss/train': '0.68927', 'examples_per_second': '30.901', 'grad_norm': '29.25', 'counters/examples': 175104, 'counters/updates': 5472}
train stats after 175136 examples: {'rewards_train/chosen': '0.15526', 'rewards_train/rejected': '-0.0087342', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.164', 'logps_train/rejected': '-149.29', 'logps_train/chosen': '-151.21', 'loss/train': '0.62289', 'examples_per_second': '31.529', 'grad_norm': '32', 'counters/examples': 175136, 'counters/updates': 5473}
train stats after 175168 examples: {'rewards_train/chosen': '0.13466', 'rewards_train/rejected': '0.0078254', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12683', 'logps_train/rejected': '-125.36', 'logps_train/chosen': '-142.35', 'loss/train': '0.64955', 'examples_per_second': '31.157', 'grad_norm': '25.125', 'counters/examples': 175168, 'counters/updates': 5474}
train stats after 175200 examples: {'rewards_train/chosen': '0.098239', 'rewards_train/rejected': '0.0049111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093328', 'logps_train/rejected': '-149.59', 'logps_train/chosen': '-136.58', 'loss/train': '0.65782', 'examples_per_second': '31.578', 'grad_norm': '28.25', 'counters/examples': 175200, 'counters/updates': 5475}
train stats after 175232 examples: {'rewards_train/chosen': '0.099052', 'rewards_train/rejected': '0.058541', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040511', 'logps_train/rejected': '-137.11', 'logps_train/chosen': '-139.45', 'loss/train': '0.68333', 'examples_per_second': '31.724', 'grad_norm': '28.375', 'counters/examples': 175232, 'counters/updates': 5476}
skipping logging after 175264 examples to avoid logging too frequently
train stats after 175296 examples: {'rewards_train/chosen': '0.075311', 'rewards_train/rejected': '0.021268', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054044', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-117.3', 'loss/train': '0.67817', 'examples_per_second': '31.551', 'grad_norm': '25.5', 'counters/examples': 175296, 'counters/updates': 5478}
train stats after 175328 examples: {'rewards_train/chosen': '0.20513', 'rewards_train/rejected': '0.087224', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11791', 'logps_train/rejected': '-160.12', 'logps_train/chosen': '-137.76', 'loss/train': '0.65193', 'examples_per_second': '31.59', 'grad_norm': '26.875', 'counters/examples': 175328, 'counters/updates': 5479}
train stats after 175360 examples: {'rewards_train/chosen': '0.0656', 'rewards_train/rejected': '0.026574', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039026', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-178.1', 'loss/train': '0.68142', 'examples_per_second': '30.682', 'grad_norm': '31.5', 'counters/examples': 175360, 'counters/updates': 5480}
train stats after 175392 examples: {'rewards_train/chosen': '0.069617', 'rewards_train/rejected': '0.082666', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013048', 'logps_train/rejected': '-129.06', 'logps_train/chosen': '-132.25', 'loss/train': '0.71166', 'examples_per_second': '30.768', 'grad_norm': '32.5', 'counters/examples': 175392, 'counters/updates': 5481}
train stats after 175424 examples: {'rewards_train/chosen': '0.11141', 'rewards_train/rejected': '0.085562', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02585', 'logps_train/rejected': '-142.07', 'logps_train/chosen': '-123.68', 'loss/train': '0.69328', 'examples_per_second': '31.57', 'grad_norm': '27.375', 'counters/examples': 175424, 'counters/updates': 5482}
train stats after 175456 examples: {'rewards_train/chosen': '0.13348', 'rewards_train/rejected': '0.025012', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10847', 'logps_train/rejected': '-100.94', 'logps_train/chosen': '-137.15', 'loss/train': '0.64856', 'examples_per_second': '31.408', 'grad_norm': '26.5', 'counters/examples': 175456, 'counters/updates': 5483}
train stats after 175488 examples: {'rewards_train/chosen': '0.090711', 'rewards_train/rejected': '0.049077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041634', 'logps_train/rejected': '-101.32', 'logps_train/chosen': '-125.87', 'loss/train': '0.6842', 'examples_per_second': '30.062', 'grad_norm': '25.25', 'counters/examples': 175488, 'counters/updates': 5484}
train stats after 175520 examples: {'rewards_train/chosen': '0.14395', 'rewards_train/rejected': '0.066651', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077299', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-138.99', 'loss/train': '0.66632', 'examples_per_second': '30.746', 'grad_norm': '26.75', 'counters/examples': 175520, 'counters/updates': 5485}
train stats after 175552 examples: {'rewards_train/chosen': '0.089613', 'rewards_train/rejected': '0.016817', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072796', 'logps_train/rejected': '-132.82', 'logps_train/chosen': '-114.17', 'loss/train': '0.67958', 'examples_per_second': '31.058', 'grad_norm': '28.75', 'counters/examples': 175552, 'counters/updates': 5486}
train stats after 175584 examples: {'rewards_train/chosen': '0.051984', 'rewards_train/rejected': '0.011591', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040394', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-137.69', 'loss/train': '0.68491', 'examples_per_second': '30.051', 'grad_norm': '26.375', 'counters/examples': 175584, 'counters/updates': 5487}
skipping logging after 175616 examples to avoid logging too frequently
train stats after 175648 examples: {'rewards_train/chosen': '0.14002', 'rewards_train/rejected': '0.018753', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12126', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-124.68', 'loss/train': '0.65814', 'examples_per_second': '30.633', 'grad_norm': '22.25', 'counters/examples': 175648, 'counters/updates': 5489}
train stats after 175680 examples: {'rewards_train/chosen': '0.12682', 'rewards_train/rejected': '0.062756', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064063', 'logps_train/rejected': '-158.17', 'logps_train/chosen': '-159.96', 'loss/train': '0.67595', 'examples_per_second': '31.407', 'grad_norm': '32.5', 'counters/examples': 175680, 'counters/updates': 5490}
train stats after 175712 examples: {'rewards_train/chosen': '0.16853', 'rewards_train/rejected': '0.14535', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023182', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-156.65', 'loss/train': '0.6901', 'examples_per_second': '32.678', 'grad_norm': '30.125', 'counters/examples': 175712, 'counters/updates': 5491}
train stats after 175744 examples: {'rewards_train/chosen': '0.10784', 'rewards_train/rejected': '0.0024226', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10542', 'logps_train/rejected': '-92.182', 'logps_train/chosen': '-160.15', 'loss/train': '0.65313', 'examples_per_second': '30.246', 'grad_norm': '26.375', 'counters/examples': 175744, 'counters/updates': 5492}
train stats after 175776 examples: {'rewards_train/chosen': '0.13126', 'rewards_train/rejected': '0.0088293', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12243', 'logps_train/rejected': '-107.74', 'logps_train/chosen': '-153.08', 'loss/train': '0.64163', 'examples_per_second': '31.468', 'grad_norm': '25.75', 'counters/examples': 175776, 'counters/updates': 5493}
train stats after 175808 examples: {'rewards_train/chosen': '0.16205', 'rewards_train/rejected': '0.052682', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10937', 'logps_train/rejected': '-155.44', 'logps_train/chosen': '-149.1', 'loss/train': '0.65088', 'examples_per_second': '31.592', 'grad_norm': '26.5', 'counters/examples': 175808, 'counters/updates': 5494}
train stats after 175840 examples: {'rewards_train/chosen': '0.13032', 'rewards_train/rejected': '0.034683', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095635', 'logps_train/rejected': '-108.52', 'logps_train/chosen': '-122.41', 'loss/train': '0.65713', 'examples_per_second': '31.014', 'grad_norm': '22', 'counters/examples': 175840, 'counters/updates': 5495}
train stats after 175872 examples: {'rewards_train/chosen': '0.12847', 'rewards_train/rejected': '0.073402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055067', 'logps_train/rejected': '-150.38', 'logps_train/chosen': '-123.32', 'loss/train': '0.69605', 'examples_per_second': '31.983', 'grad_norm': '28.125', 'counters/examples': 175872, 'counters/updates': 5496}
train stats after 175904 examples: {'rewards_train/chosen': '0.14426', 'rewards_train/rejected': '0.06448', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079778', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-124.53', 'loss/train': '0.66807', 'examples_per_second': '30.734', 'grad_norm': '23.625', 'counters/examples': 175904, 'counters/updates': 5497}
train stats after 175936 examples: {'rewards_train/chosen': '0.090081', 'rewards_train/rejected': '0.044329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045752', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-148.77', 'loss/train': '0.67821', 'examples_per_second': '31.57', 'grad_norm': '27.625', 'counters/examples': 175936, 'counters/updates': 5498}
train stats after 175968 examples: {'rewards_train/chosen': '0.13394', 'rewards_train/rejected': '0.00395', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12999', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-140.33', 'loss/train': '0.63736', 'examples_per_second': '31.782', 'grad_norm': '25.375', 'counters/examples': 175968, 'counters/updates': 5499}
train stats after 176000 examples: {'rewards_train/chosen': '0.088464', 'rewards_train/rejected': '-0.028868', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11733', 'logps_train/rejected': '-152', 'logps_train/chosen': '-124.42', 'loss/train': '0.64959', 'examples_per_second': '31.674', 'grad_norm': '28.625', 'counters/examples': 176000, 'counters/updates': 5500}
skipping logging after 176032 examples to avoid logging too frequently
train stats after 176064 examples: {'rewards_train/chosen': '0.049308', 'rewards_train/rejected': '0.047755', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0015523', 'logps_train/rejected': '-129.68', 'logps_train/chosen': '-149.81', 'loss/train': '0.70492', 'examples_per_second': '34.399', 'grad_norm': '25.625', 'counters/examples': 176064, 'counters/updates': 5502}
train stats after 176096 examples: {'rewards_train/chosen': '0.14015', 'rewards_train/rejected': '-0.017857', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15801', 'logps_train/rejected': '-142.55', 'logps_train/chosen': '-165.44', 'loss/train': '0.63696', 'examples_per_second': '31.575', 'grad_norm': '30.75', 'counters/examples': 176096, 'counters/updates': 5503}
train stats after 176128 examples: {'rewards_train/chosen': '0.1554', 'rewards_train/rejected': '-0.023202', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1786', 'logps_train/rejected': '-96.977', 'logps_train/chosen': '-129.29', 'loss/train': '0.61645', 'examples_per_second': '31.59', 'grad_norm': '20.125', 'counters/examples': 176128, 'counters/updates': 5504}
train stats after 176160 examples: {'rewards_train/chosen': '0.12857', 'rewards_train/rejected': '0.082843', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045732', 'logps_train/rejected': '-141.22', 'logps_train/chosen': '-174.41', 'loss/train': '0.68259', 'examples_per_second': '31.277', 'grad_norm': '30.75', 'counters/examples': 176160, 'counters/updates': 5505}
train stats after 176192 examples: {'rewards_train/chosen': '0.12954', 'rewards_train/rejected': '-0.022256', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15179', 'logps_train/rejected': '-154.25', 'logps_train/chosen': '-150.53', 'loss/train': '0.62533', 'examples_per_second': '30.997', 'grad_norm': '27', 'counters/examples': 176192, 'counters/updates': 5506}
train stats after 176224 examples: {'rewards_train/chosen': '0.038295', 'rewards_train/rejected': '0.0033208', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034974', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-129.17', 'loss/train': '0.68719', 'examples_per_second': '31.482', 'grad_norm': '27.25', 'counters/examples': 176224, 'counters/updates': 5507}
train stats after 176256 examples: {'rewards_train/chosen': '0.1249', 'rewards_train/rejected': '-0.044353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16925', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-124.23', 'loss/train': '0.62755', 'examples_per_second': '31.502', 'grad_norm': '27.5', 'counters/examples': 176256, 'counters/updates': 5508}
train stats after 176288 examples: {'rewards_train/chosen': '0.14429', 'rewards_train/rejected': '0.021487', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1228', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-141.33', 'loss/train': '0.64651', 'examples_per_second': '32.115', 'grad_norm': '23.75', 'counters/examples': 176288, 'counters/updates': 5509}
train stats after 176320 examples: {'rewards_train/chosen': '0.090939', 'rewards_train/rejected': '0.010853', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080086', 'logps_train/rejected': '-106.69', 'logps_train/chosen': '-149.92', 'loss/train': '0.66335', 'examples_per_second': '30.135', 'grad_norm': '30.5', 'counters/examples': 176320, 'counters/updates': 5510}
train stats after 176352 examples: {'rewards_train/chosen': '0.19548', 'rewards_train/rejected': '0.068463', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12702', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-172.62', 'loss/train': '0.65915', 'examples_per_second': '32.151', 'grad_norm': '28.75', 'counters/examples': 176352, 'counters/updates': 5511}
train stats after 176384 examples: {'rewards_train/chosen': '0.1428', 'rewards_train/rejected': '0.059665', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083132', 'logps_train/rejected': '-95.464', 'logps_train/chosen': '-149.2', 'loss/train': '0.6722', 'examples_per_second': '31.545', 'grad_norm': '26', 'counters/examples': 176384, 'counters/updates': 5512}
skipping logging after 176416 examples to avoid logging too frequently
train stats after 176448 examples: {'rewards_train/chosen': '0.1863', 'rewards_train/rejected': '-0.052316', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23861', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-169.15', 'loss/train': '0.59901', 'examples_per_second': '31.558', 'grad_norm': '27.75', 'counters/examples': 176448, 'counters/updates': 5514}
skipping logging after 176480 examples to avoid logging too frequently
train stats after 176512 examples: {'rewards_train/chosen': '0.2413', 'rewards_train/rejected': '0.056634', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18467', 'logps_train/rejected': '-131.83', 'logps_train/chosen': '-134.15', 'loss/train': '0.6463', 'examples_per_second': '32.401', 'grad_norm': '26.125', 'counters/examples': 176512, 'counters/updates': 5516}
train stats after 176544 examples: {'rewards_train/chosen': '0.19977', 'rewards_train/rejected': '0.054111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14565', 'logps_train/rejected': '-134.42', 'logps_train/chosen': '-175.75', 'loss/train': '0.64001', 'examples_per_second': '31.735', 'grad_norm': '32', 'counters/examples': 176544, 'counters/updates': 5517}
skipping logging after 176576 examples to avoid logging too frequently
train stats after 176608 examples: {'rewards_train/chosen': '0.14377', 'rewards_train/rejected': '0.096634', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047141', 'logps_train/rejected': '-118.19', 'logps_train/chosen': '-118.65', 'loss/train': '0.68901', 'examples_per_second': '32.228', 'grad_norm': '25', 'counters/examples': 176608, 'counters/updates': 5519}
train stats after 176640 examples: {'rewards_train/chosen': '0.14487', 'rewards_train/rejected': '-0.014863', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15973', 'logps_train/rejected': '-92.044', 'logps_train/chosen': '-134.66', 'loss/train': '0.62752', 'examples_per_second': '31.618', 'grad_norm': '23.625', 'counters/examples': 176640, 'counters/updates': 5520}
train stats after 176672 examples: {'rewards_train/chosen': '0.10595', 'rewards_train/rejected': '0.046037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059909', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-123', 'loss/train': '0.67861', 'examples_per_second': '30.016', 'grad_norm': '27.125', 'counters/examples': 176672, 'counters/updates': 5521}
train stats after 176704 examples: {'rewards_train/chosen': '0.16458', 'rewards_train/rejected': '0.053721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11086', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-151.06', 'loss/train': '0.6456', 'examples_per_second': '30.474', 'grad_norm': '26.125', 'counters/examples': 176704, 'counters/updates': 5522}
skipping logging after 176736 examples to avoid logging too frequently
train stats after 176768 examples: {'rewards_train/chosen': '0.11583', 'rewards_train/rejected': '-0.028161', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14399', 'logps_train/rejected': '-83.896', 'logps_train/chosen': '-118.73', 'loss/train': '0.63117', 'examples_per_second': '32.638', 'grad_norm': '21.5', 'counters/examples': 176768, 'counters/updates': 5524}
train stats after 176800 examples: {'rewards_train/chosen': '0.032093', 'rewards_train/rejected': '-0.048753', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080845', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-157.72', 'loss/train': '0.67135', 'examples_per_second': '30.884', 'grad_norm': '28.25', 'counters/examples': 176800, 'counters/updates': 5525}
train stats after 176832 examples: {'rewards_train/chosen': '0.21686', 'rewards_train/rejected': '0.071085', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14578', 'logps_train/rejected': '-152.71', 'logps_train/chosen': '-162.42', 'loss/train': '0.63532', 'examples_per_second': '30.109', 'grad_norm': '31.25', 'counters/examples': 176832, 'counters/updates': 5526}
skipping logging after 176864 examples to avoid logging too frequently
train stats after 176896 examples: {'rewards_train/chosen': '0.10621', 'rewards_train/rejected': '0.015922', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090288', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-126.18', 'loss/train': '0.6572', 'examples_per_second': '31.397', 'grad_norm': '24.875', 'counters/examples': 176896, 'counters/updates': 5528}
train stats after 176928 examples: {'rewards_train/chosen': '0.14857', 'rewards_train/rejected': '0.067783', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.080786', 'logps_train/rejected': '-130.21', 'logps_train/chosen': '-145.67', 'loss/train': '0.67039', 'examples_per_second': '31.505', 'grad_norm': '26.875', 'counters/examples': 176928, 'counters/updates': 5529}
train stats after 176960 examples: {'rewards_train/chosen': '0.0086938', 'rewards_train/rejected': '-0.016726', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02542', 'logps_train/rejected': '-97.173', 'logps_train/chosen': '-115.21', 'loss/train': '0.6893', 'examples_per_second': '32.953', 'grad_norm': '24.75', 'counters/examples': 176960, 'counters/updates': 5530}
train stats after 176992 examples: {'rewards_train/chosen': '0.1711', 'rewards_train/rejected': '0.099766', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071335', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-152.97', 'loss/train': '0.67052', 'examples_per_second': '31.137', 'grad_norm': '28.375', 'counters/examples': 176992, 'counters/updates': 5531}
skipping logging after 177024 examples to avoid logging too frequently
train stats after 177056 examples: {'rewards_train/chosen': '0.03173', 'rewards_train/rejected': '0.039213', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0074829', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-157.7', 'loss/train': '0.70819', 'examples_per_second': '24.77', 'grad_norm': '30.875', 'counters/examples': 177056, 'counters/updates': 5533}
train stats after 177088 examples: {'rewards_train/chosen': '0.17818', 'rewards_train/rejected': '0.10042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077765', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-118.56', 'loss/train': '0.66753', 'examples_per_second': '30.028', 'grad_norm': '26.125', 'counters/examples': 177088, 'counters/updates': 5534}
train stats after 177120 examples: {'rewards_train/chosen': '0.11237', 'rewards_train/rejected': '0.045399', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066973', 'logps_train/rejected': '-153.73', 'logps_train/chosen': '-150.38', 'loss/train': '0.67123', 'examples_per_second': '31.362', 'grad_norm': '28.5', 'counters/examples': 177120, 'counters/updates': 5535}
train stats after 177152 examples: {'rewards_train/chosen': '0.052221', 'rewards_train/rejected': '0.020521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0317', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-137.13', 'loss/train': '0.68676', 'examples_per_second': '30.251', 'grad_norm': '27.125', 'counters/examples': 177152, 'counters/updates': 5536}
train stats after 177184 examples: {'rewards_train/chosen': '0.15599', 'rewards_train/rejected': '-0.0020982', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15809', 'logps_train/rejected': '-155.65', 'logps_train/chosen': '-188.77', 'loss/train': '0.6263', 'examples_per_second': '30.679', 'grad_norm': '30.25', 'counters/examples': 177184, 'counters/updates': 5537}
train stats after 177216 examples: {'rewards_train/chosen': '0.1444', 'rewards_train/rejected': '0.028087', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11631', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-149.73', 'loss/train': '0.64436', 'examples_per_second': '31.372', 'grad_norm': '24.25', 'counters/examples': 177216, 'counters/updates': 5538}
skipping logging after 177248 examples to avoid logging too frequently
train stats after 177280 examples: {'rewards_train/chosen': '0.17314', 'rewards_train/rejected': '0.052559', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12058', 'logps_train/rejected': '-114.57', 'logps_train/chosen': '-133.3', 'loss/train': '0.64624', 'examples_per_second': '31.684', 'grad_norm': '24', 'counters/examples': 177280, 'counters/updates': 5540}
train stats after 177312 examples: {'rewards_train/chosen': '0.026476', 'rewards_train/rejected': '0.036709', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010234', 'logps_train/rejected': '-91.797', 'logps_train/chosen': '-136.55', 'loss/train': '0.70685', 'examples_per_second': '32.805', 'grad_norm': '23.875', 'counters/examples': 177312, 'counters/updates': 5541}
train stats after 177344 examples: {'rewards_train/chosen': '0.10648', 'rewards_train/rejected': '0.081685', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.024798', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-148.48', 'loss/train': '0.69717', 'examples_per_second': '30.583', 'grad_norm': '26.75', 'counters/examples': 177344, 'counters/updates': 5542}
train stats after 177376 examples: {'rewards_train/chosen': '0.27379', 'rewards_train/rejected': '0.027868', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24592', 'logps_train/rejected': '-146.76', 'logps_train/chosen': '-201.89', 'loss/train': '0.59405', 'examples_per_second': '30.298', 'grad_norm': '27.5', 'counters/examples': 177376, 'counters/updates': 5543}
skipping logging after 177408 examples to avoid logging too frequently
train stats after 177440 examples: {'rewards_train/chosen': '0.16613', 'rewards_train/rejected': '-0.011951', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17808', 'logps_train/rejected': '-90.59', 'logps_train/chosen': '-129.58', 'loss/train': '0.62135', 'examples_per_second': '30.558', 'grad_norm': '24.75', 'counters/examples': 177440, 'counters/updates': 5545}
train stats after 177472 examples: {'rewards_train/chosen': '0.085224', 'rewards_train/rejected': '0.0055331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079691', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-158.92', 'loss/train': '0.66155', 'examples_per_second': '31.562', 'grad_norm': '24.25', 'counters/examples': 177472, 'counters/updates': 5546}
train stats after 177504 examples: {'rewards_train/chosen': '0.20155', 'rewards_train/rejected': '0.02759', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17396', 'logps_train/rejected': '-105.19', 'logps_train/chosen': '-145.9', 'loss/train': '0.63023', 'examples_per_second': '31.423', 'grad_norm': '25.375', 'counters/examples': 177504, 'counters/updates': 5547}
skipping logging after 177536 examples to avoid logging too frequently
train stats after 177568 examples: {'rewards_train/chosen': '0.073086', 'rewards_train/rejected': '0.029914', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043172', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-107.94', 'loss/train': '0.67955', 'examples_per_second': '33.604', 'grad_norm': '28', 'counters/examples': 177568, 'counters/updates': 5549}
train stats after 177600 examples: {'rewards_train/chosen': '0.031887', 'rewards_train/rejected': '-0.049677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081564', 'logps_train/rejected': '-131.21', 'logps_train/chosen': '-137.85', 'loss/train': '0.66608', 'examples_per_second': '30.482', 'grad_norm': '26.625', 'counters/examples': 177600, 'counters/updates': 5550}
train stats after 177632 examples: {'rewards_train/chosen': '0.0808', 'rewards_train/rejected': '-0.0056738', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086473', 'logps_train/rejected': '-139.87', 'logps_train/chosen': '-132.54', 'loss/train': '0.65905', 'examples_per_second': '32.198', 'grad_norm': '26.25', 'counters/examples': 177632, 'counters/updates': 5551}
train stats after 177664 examples: {'rewards_train/chosen': '0.050269', 'rewards_train/rejected': '-0.010315', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060585', 'logps_train/rejected': '-108.26', 'logps_train/chosen': '-131.94', 'loss/train': '0.67023', 'examples_per_second': '31.887', 'grad_norm': '24.625', 'counters/examples': 177664, 'counters/updates': 5552}
train stats after 177696 examples: {'rewards_train/chosen': '0.11459', 'rewards_train/rejected': '0.083249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031338', 'logps_train/rejected': '-105.2', 'logps_train/chosen': '-164.88', 'loss/train': '0.69295', 'examples_per_second': '30.467', 'grad_norm': '28.125', 'counters/examples': 177696, 'counters/updates': 5553}
train stats after 177728 examples: {'rewards_train/chosen': '0.17937', 'rewards_train/rejected': '0.090493', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088878', 'logps_train/rejected': '-141.06', 'logps_train/chosen': '-157.15', 'loss/train': '0.66237', 'examples_per_second': '30.407', 'grad_norm': '32.25', 'counters/examples': 177728, 'counters/updates': 5554}
train stats after 177760 examples: {'rewards_train/chosen': '0.12606', 'rewards_train/rejected': '0.033732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092324', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-142.91', 'loss/train': '0.66001', 'examples_per_second': '32.204', 'grad_norm': '27', 'counters/examples': 177760, 'counters/updates': 5555}
train stats after 177792 examples: {'rewards_train/chosen': '0.17704', 'rewards_train/rejected': '0.10334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073699', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-160.86', 'loss/train': '0.67906', 'examples_per_second': '31.554', 'grad_norm': '46.5', 'counters/examples': 177792, 'counters/updates': 5556}
train stats after 177824 examples: {'rewards_train/chosen': '0.052452', 'rewards_train/rejected': '0.052654', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0002015', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-132.17', 'loss/train': '0.70877', 'examples_per_second': '31.549', 'grad_norm': '29.125', 'counters/examples': 177824, 'counters/updates': 5557}
train stats after 177856 examples: {'rewards_train/chosen': '0.1395', 'rewards_train/rejected': '0.008768', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13073', 'logps_train/rejected': '-166.64', 'logps_train/chosen': '-148.77', 'loss/train': '0.64856', 'examples_per_second': '31.449', 'grad_norm': '28.5', 'counters/examples': 177856, 'counters/updates': 5558}
train stats after 177888 examples: {'rewards_train/chosen': '0.12913', 'rewards_train/rejected': '0.0885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040625', 'logps_train/rejected': '-164.21', 'logps_train/chosen': '-156.09', 'loss/train': '0.68128', 'examples_per_second': '30.921', 'grad_norm': '29.25', 'counters/examples': 177888, 'counters/updates': 5559}
train stats after 177920 examples: {'rewards_train/chosen': '0.11364', 'rewards_train/rejected': '0.019269', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09437', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-132.8', 'loss/train': '0.6566', 'examples_per_second': '31.531', 'grad_norm': '24.25', 'counters/examples': 177920, 'counters/updates': 5560}
train stats after 177952 examples: {'rewards_train/chosen': '0.14261', 'rewards_train/rejected': '0.023764', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11884', 'logps_train/rejected': '-88.861', 'logps_train/chosen': '-126.46', 'loss/train': '0.64248', 'examples_per_second': '31.571', 'grad_norm': '22.375', 'counters/examples': 177952, 'counters/updates': 5561}
train stats after 177984 examples: {'rewards_train/chosen': '0.16383', 'rewards_train/rejected': '0.053614', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11021', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-150.83', 'loss/train': '0.66191', 'examples_per_second': '33.027', 'grad_norm': '35.5', 'counters/examples': 177984, 'counters/updates': 5562}
skipping logging after 178016 examples to avoid logging too frequently
train stats after 178048 examples: {'rewards_train/chosen': '0.23862', 'rewards_train/rejected': '0.019935', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21869', 'logps_train/rejected': '-105.37', 'logps_train/chosen': '-170.78', 'loss/train': '0.60687', 'examples_per_second': '30.484', 'grad_norm': '23.25', 'counters/examples': 178048, 'counters/updates': 5564}
train stats after 178080 examples: {'rewards_train/chosen': '0.12201', 'rewards_train/rejected': '0.027551', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.094455', 'logps_train/rejected': '-106.28', 'logps_train/chosen': '-130.62', 'loss/train': '0.66733', 'examples_per_second': '31.222', 'grad_norm': '23.25', 'counters/examples': 178080, 'counters/updates': 5565}
train stats after 178112 examples: {'rewards_train/chosen': '0.11769', 'rewards_train/rejected': '0.12086', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0031615', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-133.65', 'loss/train': '0.7033', 'examples_per_second': '30.552', 'grad_norm': '27.125', 'counters/examples': 178112, 'counters/updates': 5566}
train stats after 178144 examples: {'rewards_train/chosen': '0.088279', 'rewards_train/rejected': '0.117', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028717', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-140.87', 'loss/train': '0.71429', 'examples_per_second': '33.166', 'grad_norm': '27.25', 'counters/examples': 178144, 'counters/updates': 5567}
train stats after 178176 examples: {'rewards_train/chosen': '0.038524', 'rewards_train/rejected': '-0.069725', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10825', 'logps_train/rejected': '-101.75', 'logps_train/chosen': '-106.09', 'loss/train': '0.65569', 'examples_per_second': '32.126', 'grad_norm': '27.125', 'counters/examples': 178176, 'counters/updates': 5568}
train stats after 178208 examples: {'rewards_train/chosen': '0.11148', 'rewards_train/rejected': '0.032287', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079195', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-151.51', 'loss/train': '0.66373', 'examples_per_second': '31.416', 'grad_norm': '24.875', 'counters/examples': 178208, 'counters/updates': 5569}
train stats after 178240 examples: {'rewards_train/chosen': '0.10892', 'rewards_train/rejected': '0.0059426', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10298', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-147.87', 'loss/train': '0.65533', 'examples_per_second': '30.141', 'grad_norm': '27', 'counters/examples': 178240, 'counters/updates': 5570}
train stats after 178272 examples: {'rewards_train/chosen': '0.063381', 'rewards_train/rejected': '0.016432', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046949', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-123.53', 'loss/train': '0.67699', 'examples_per_second': '33.128', 'grad_norm': '23.75', 'counters/examples': 178272, 'counters/updates': 5571}
train stats after 178304 examples: {'rewards_train/chosen': '0.12084', 'rewards_train/rejected': '0.016392', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10445', 'logps_train/rejected': '-99.562', 'logps_train/chosen': '-126', 'loss/train': '0.65588', 'examples_per_second': '31.413', 'grad_norm': '24.125', 'counters/examples': 178304, 'counters/updates': 5572}
skipping logging after 178336 examples to avoid logging too frequently
train stats after 178368 examples: {'rewards_train/chosen': '0.11532', 'rewards_train/rejected': '0.0075256', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1078', 'logps_train/rejected': '-85.376', 'logps_train/chosen': '-135.82', 'loss/train': '0.64918', 'examples_per_second': '30.964', 'grad_norm': '21.625', 'counters/examples': 178368, 'counters/updates': 5574}
train stats after 178400 examples: {'rewards_train/chosen': '0.081656', 'rewards_train/rejected': '0.045159', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036497', 'logps_train/rejected': '-112.76', 'logps_train/chosen': '-117.28', 'loss/train': '0.69117', 'examples_per_second': '33.029', 'grad_norm': '25.75', 'counters/examples': 178400, 'counters/updates': 5575}
skipping logging after 178432 examples to avoid logging too frequently
train stats after 178464 examples: {'rewards_train/chosen': '0.17646', 'rewards_train/rejected': '-0.0039863', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18045', 'logps_train/rejected': '-86.493', 'logps_train/chosen': '-144.15', 'loss/train': '0.61708', 'examples_per_second': '30.521', 'grad_norm': '24.75', 'counters/examples': 178464, 'counters/updates': 5577}
train stats after 178496 examples: {'rewards_train/chosen': '0.15911', 'rewards_train/rejected': '0.10957', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049539', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-151.16', 'loss/train': '0.67926', 'examples_per_second': '31.185', 'grad_norm': '25.875', 'counters/examples': 178496, 'counters/updates': 5578}
train stats after 178528 examples: {'rewards_train/chosen': '0.14107', 'rewards_train/rejected': '0.058807', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082261', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-133.66', 'loss/train': '0.675', 'examples_per_second': '31.279', 'grad_norm': '25.875', 'counters/examples': 178528, 'counters/updates': 5579}
skipping logging after 178560 examples to avoid logging too frequently
train stats after 178592 examples: {'rewards_train/chosen': '0.11137', 'rewards_train/rejected': '0.00033859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11104', 'logps_train/rejected': '-82.535', 'logps_train/chosen': '-123.28', 'loss/train': '0.65376', 'examples_per_second': '30.257', 'grad_norm': '20.375', 'counters/examples': 178592, 'counters/updates': 5581}
train stats after 178624 examples: {'rewards_train/chosen': '0.081687', 'rewards_train/rejected': '0.014662', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067025', 'logps_train/rejected': '-92.165', 'logps_train/chosen': '-96.273', 'loss/train': '0.66649', 'examples_per_second': '33.321', 'grad_norm': '24.5', 'counters/examples': 178624, 'counters/updates': 5582}
train stats after 178656 examples: {'rewards_train/chosen': '0.18364', 'rewards_train/rejected': '0.083399', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10025', 'logps_train/rejected': '-139.37', 'logps_train/chosen': '-133.83', 'loss/train': '0.66151', 'examples_per_second': '30.785', 'grad_norm': '26', 'counters/examples': 178656, 'counters/updates': 5583}
skipping logging after 178688 examples to avoid logging too frequently
train stats after 178720 examples: {'rewards_train/chosen': '0.075549', 'rewards_train/rejected': '0.013887', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.061662', 'logps_train/rejected': '-88.589', 'logps_train/chosen': '-121.77', 'loss/train': '0.67032', 'examples_per_second': '32.995', 'grad_norm': '22.5', 'counters/examples': 178720, 'counters/updates': 5585}
skipping logging after 178752 examples to avoid logging too frequently
train stats after 178784 examples: {'rewards_train/chosen': '0.14391', 'rewards_train/rejected': '0.0198', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12411', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-190.95', 'loss/train': '0.65166', 'examples_per_second': '31.53', 'grad_norm': '27.5', 'counters/examples': 178784, 'counters/updates': 5587}
skipping logging after 178816 examples to avoid logging too frequently
train stats after 178848 examples: {'rewards_train/chosen': '0.07637', 'rewards_train/rejected': '-0.020555', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.096925', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-129.66', 'loss/train': '0.66494', 'examples_per_second': '31.555', 'grad_norm': '25.375', 'counters/examples': 178848, 'counters/updates': 5589}
train stats after 178880 examples: {'rewards_train/chosen': '0.14737', 'rewards_train/rejected': '0.11144', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03593', 'logps_train/rejected': '-169.6', 'logps_train/chosen': '-130.52', 'loss/train': '0.69127', 'examples_per_second': '31.214', 'grad_norm': '32', 'counters/examples': 178880, 'counters/updates': 5590}
train stats after 178912 examples: {'rewards_train/chosen': '0.17251', 'rewards_train/rejected': '0.096643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075864', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-147.49', 'loss/train': '0.6719', 'examples_per_second': '31.323', 'grad_norm': '27.375', 'counters/examples': 178912, 'counters/updates': 5591}
train stats after 178944 examples: {'rewards_train/chosen': '0.094431', 'rewards_train/rejected': '0.1279', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033466', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-138.62', 'loss/train': '0.72661', 'examples_per_second': '30.946', 'grad_norm': '28.375', 'counters/examples': 178944, 'counters/updates': 5592}
train stats after 178976 examples: {'rewards_train/chosen': '0.18768', 'rewards_train/rejected': '0.0058922', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18178', 'logps_train/rejected': '-110.76', 'logps_train/chosen': '-128.77', 'loss/train': '0.61525', 'examples_per_second': '30.809', 'grad_norm': '23.75', 'counters/examples': 178976, 'counters/updates': 5593}
train stats after 179008 examples: {'rewards_train/chosen': '0.022516', 'rewards_train/rejected': '-0.021983', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044498', 'logps_train/rejected': '-139.93', 'logps_train/chosen': '-125.43', 'loss/train': '0.68089', 'examples_per_second': '30.186', 'grad_norm': '27.875', 'counters/examples': 179008, 'counters/updates': 5594}
train stats after 179040 examples: {'rewards_train/chosen': '0.12954', 'rewards_train/rejected': '0.038801', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090744', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-119.97', 'loss/train': '0.66341', 'examples_per_second': '31.154', 'grad_norm': '21.625', 'counters/examples': 179040, 'counters/updates': 5595}
train stats after 179072 examples: {'rewards_train/chosen': '0.14025', 'rewards_train/rejected': '0.032778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10747', 'logps_train/rejected': '-139.81', 'logps_train/chosen': '-136.39', 'loss/train': '0.64938', 'examples_per_second': '32.201', 'grad_norm': '29.625', 'counters/examples': 179072, 'counters/updates': 5596}
train stats after 179104 examples: {'rewards_train/chosen': '0.11462', 'rewards_train/rejected': '0.021562', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.09306', 'logps_train/rejected': '-97.647', 'logps_train/chosen': '-151.48', 'loss/train': '0.66731', 'examples_per_second': '33.033', 'grad_norm': '25.25', 'counters/examples': 179104, 'counters/updates': 5597}
skipping logging after 179136 examples to avoid logging too frequently
train stats after 179168 examples: {'rewards_train/chosen': '0.035045', 'rewards_train/rejected': '-0.024472', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059516', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-133.89', 'loss/train': '0.67201', 'examples_per_second': '33.02', 'grad_norm': '24.25', 'counters/examples': 179168, 'counters/updates': 5599}
train stats after 179200 examples: {'rewards_train/chosen': '0.091483', 'rewards_train/rejected': '0.042268', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049215', 'logps_train/rejected': '-141.2', 'logps_train/chosen': '-155.54', 'loss/train': '0.68363', 'examples_per_second': '31.19', 'grad_norm': '27.5', 'counters/examples': 179200, 'counters/updates': 5600}
skipping logging after 179232 examples to avoid logging too frequently
train stats after 179264 examples: {'rewards_train/chosen': '0.10368', 'rewards_train/rejected': '0.052652', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05103', 'logps_train/rejected': '-160.52', 'logps_train/chosen': '-160.19', 'loss/train': '0.6794', 'examples_per_second': '30.544', 'grad_norm': '29.875', 'counters/examples': 179264, 'counters/updates': 5602}
train stats after 179296 examples: {'rewards_train/chosen': '0.15409', 'rewards_train/rejected': '-0.013881', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16797', 'logps_train/rejected': '-111.08', 'logps_train/chosen': '-152.44', 'loss/train': '0.62946', 'examples_per_second': '23.996', 'grad_norm': '27', 'counters/examples': 179296, 'counters/updates': 5603}
train stats after 179328 examples: {'rewards_train/chosen': '0.1608', 'rewards_train/rejected': '0.046829', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11398', 'logps_train/rejected': '-117', 'logps_train/chosen': '-135.28', 'loss/train': '0.64933', 'examples_per_second': '30.492', 'grad_norm': '45.5', 'counters/examples': 179328, 'counters/updates': 5604}
train stats after 179360 examples: {'rewards_train/chosen': '0.1662', 'rewards_train/rejected': '0.048788', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11741', 'logps_train/rejected': '-140.86', 'logps_train/chosen': '-117.9', 'loss/train': '0.64436', 'examples_per_second': '32.409', 'grad_norm': '24.25', 'counters/examples': 179360, 'counters/updates': 5605}
train stats after 179392 examples: {'rewards_train/chosen': '0.086068', 'rewards_train/rejected': '0.030582', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055487', 'logps_train/rejected': '-101.18', 'logps_train/chosen': '-133.89', 'loss/train': '0.67696', 'examples_per_second': '25.666', 'grad_norm': '23.375', 'counters/examples': 179392, 'counters/updates': 5606}
skipping logging after 179424 examples to avoid logging too frequently
train stats after 179456 examples: {'rewards_train/chosen': '-0.0062233', 'rewards_train/rejected': '-0.014896', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0086731', 'logps_train/rejected': '-103.36', 'logps_train/chosen': '-114.87', 'loss/train': '0.69658', 'examples_per_second': '41.678', 'grad_norm': '23', 'counters/examples': 179456, 'counters/updates': 5608}
train stats after 179488 examples: {'rewards_train/chosen': '0.19761', 'rewards_train/rejected': '0.050317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1473', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-127.85', 'loss/train': '0.63196', 'examples_per_second': '31.547', 'grad_norm': '24.25', 'counters/examples': 179488, 'counters/updates': 5609}
train stats after 179520 examples: {'rewards_train/chosen': '0.13016', 'rewards_train/rejected': '-0.0071654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13733', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-156.73', 'loss/train': '0.63516', 'examples_per_second': '31.146', 'grad_norm': '24.25', 'counters/examples': 179520, 'counters/updates': 5610}
train stats after 179552 examples: {'rewards_train/chosen': '0.074881', 'rewards_train/rejected': '0.07248', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0024012', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-167.39', 'loss/train': '0.70572', 'examples_per_second': '30.038', 'grad_norm': '30.5', 'counters/examples': 179552, 'counters/updates': 5611}
train stats after 179584 examples: {'rewards_train/chosen': '0.16232', 'rewards_train/rejected': '-0.015048', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17737', 'logps_train/rejected': '-104.91', 'logps_train/chosen': '-150.38', 'loss/train': '0.62074', 'examples_per_second': '31.805', 'grad_norm': '24.625', 'counters/examples': 179584, 'counters/updates': 5612}
skipping logging after 179616 examples to avoid logging too frequently
train stats after 179648 examples: {'rewards_train/chosen': '0.084571', 'rewards_train/rejected': '0.064646', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019926', 'logps_train/rejected': '-163.57', 'logps_train/chosen': '-179', 'loss/train': '0.69757', 'examples_per_second': '33.383', 'grad_norm': '31.25', 'counters/examples': 179648, 'counters/updates': 5614}
train stats after 179680 examples: {'rewards_train/chosen': '0.13702', 'rewards_train/rejected': '0.019882', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11714', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-111.03', 'loss/train': '0.6448', 'examples_per_second': '30.623', 'grad_norm': '26.5', 'counters/examples': 179680, 'counters/updates': 5615}
train stats after 179712 examples: {'rewards_train/chosen': '0.16244', 'rewards_train/rejected': '0.031207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13123', 'logps_train/rejected': '-126.17', 'logps_train/chosen': '-148.9', 'loss/train': '0.64884', 'examples_per_second': '30.171', 'grad_norm': '28.5', 'counters/examples': 179712, 'counters/updates': 5616}
skipping logging after 179744 examples to avoid logging too frequently
train stats after 179776 examples: {'rewards_train/chosen': '0.13402', 'rewards_train/rejected': '0.079797', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054219', 'logps_train/rejected': '-140.07', 'logps_train/chosen': '-172.54', 'loss/train': '0.6789', 'examples_per_second': '31.433', 'grad_norm': '28.125', 'counters/examples': 179776, 'counters/updates': 5618}
train stats after 179808 examples: {'rewards_train/chosen': '0.11974', 'rewards_train/rejected': '0.088327', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031412', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-134.05', 'loss/train': '0.6977', 'examples_per_second': '31.553', 'grad_norm': '27.75', 'counters/examples': 179808, 'counters/updates': 5619}
train stats after 179840 examples: {'rewards_train/chosen': '0.06536', 'rewards_train/rejected': '-0.024102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089462', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-129.36', 'loss/train': '0.66494', 'examples_per_second': '31.541', 'grad_norm': '25.25', 'counters/examples': 179840, 'counters/updates': 5620}
skipping logging after 179872 examples to avoid logging too frequently
train stats after 179904 examples: {'rewards_train/chosen': '0.11237', 'rewards_train/rejected': '-0.019866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13223', 'logps_train/rejected': '-99.444', 'logps_train/chosen': '-108.79', 'loss/train': '0.63976', 'examples_per_second': '33.168', 'grad_norm': '22', 'counters/examples': 179904, 'counters/updates': 5622}
train stats after 179936 examples: {'rewards_train/chosen': '0.12411', 'rewards_train/rejected': '-0.013723', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13783', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-137.69', 'loss/train': '0.63904', 'examples_per_second': '31.552', 'grad_norm': '23.875', 'counters/examples': 179936, 'counters/updates': 5623}
skipping logging after 179968 examples to avoid logging too frequently
train stats after 180000 examples: {'rewards_train/chosen': '0.13208', 'rewards_train/rejected': '0.066553', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065532', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-137.3', 'loss/train': '0.68416', 'examples_per_second': '31.4', 'grad_norm': '26.75', 'counters/examples': 180000, 'counters/updates': 5625}
Running evaluation after 180000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 180000: {'rewards_eval/chosen': '0.13231', 'rewards_eval/rejected': '0.042587', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.089721', 'logps_eval/rejected': '-118.19', 'logps_eval/chosen': '-138.11', 'loss/eval': '0.66354'}
skipping save for non epoch
train stats after 180032 examples: {'rewards_train/chosen': '0.084149', 'rewards_train/rejected': '0.059945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024204', 'logps_train/rejected': '-149.17', 'logps_train/chosen': '-147.54', 'loss/train': '0.69471', 'examples_per_second': '32.055', 'grad_norm': '30', 'counters/examples': 180032, 'counters/updates': 5626}
skipping logging after 180064 examples to avoid logging too frequently
train stats after 180096 examples: {'rewards_train/chosen': '0.078177', 'rewards_train/rejected': '9.8765e-05', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.078078', 'logps_train/rejected': '-118.61', 'logps_train/chosen': '-155.29', 'loss/train': '0.66129', 'examples_per_second': '33.232', 'grad_norm': '25.75', 'counters/examples': 180096, 'counters/updates': 5628}
train stats after 180128 examples: {'rewards_train/chosen': '0.1093', 'rewards_train/rejected': '0.093283', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016017', 'logps_train/rejected': '-100.36', 'logps_train/chosen': '-122.16', 'loss/train': '0.69934', 'examples_per_second': '29.819', 'grad_norm': '27.125', 'counters/examples': 180128, 'counters/updates': 5629}
skipping logging after 180160 examples to avoid logging too frequently
train stats after 180192 examples: {'rewards_train/chosen': '0.15534', 'rewards_train/rejected': '0.047158', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10818', 'logps_train/rejected': '-123.63', 'logps_train/chosen': '-169.85', 'loss/train': '0.65225', 'examples_per_second': '31.562', 'grad_norm': '26.5', 'counters/examples': 180192, 'counters/updates': 5631}
train stats after 180224 examples: {'rewards_train/chosen': '0.095036', 'rewards_train/rejected': '0.082277', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01276', 'logps_train/rejected': '-171.99', 'logps_train/chosen': '-137.17', 'loss/train': '0.69841', 'examples_per_second': '30.841', 'grad_norm': '29.875', 'counters/examples': 180224, 'counters/updates': 5632}
train stats after 180256 examples: {'rewards_train/chosen': '0.11827', 'rewards_train/rejected': '0.051167', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.067108', 'logps_train/rejected': '-98.167', 'logps_train/chosen': '-180.54', 'loss/train': '0.66901', 'examples_per_second': '30.211', 'grad_norm': '26.375', 'counters/examples': 180256, 'counters/updates': 5633}
train stats after 180288 examples: {'rewards_train/chosen': '0.080345', 'rewards_train/rejected': '0.001337', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079008', 'logps_train/rejected': '-102.75', 'logps_train/chosen': '-135.83', 'loss/train': '0.66325', 'examples_per_second': '31.083', 'grad_norm': '23.625', 'counters/examples': 180288, 'counters/updates': 5634}
train stats after 180320 examples: {'rewards_train/chosen': '0.12096', 'rewards_train/rejected': '-0.0081746', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12913', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-162.02', 'loss/train': '0.64409', 'examples_per_second': '32.019', 'grad_norm': '24.75', 'counters/examples': 180320, 'counters/updates': 5635}
train stats after 180352 examples: {'rewards_train/chosen': '0.074406', 'rewards_train/rejected': '0.14234', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.067938', 'logps_train/rejected': '-151.06', 'logps_train/chosen': '-142.89', 'loss/train': '0.74418', 'examples_per_second': '31.951', 'grad_norm': '29.75', 'counters/examples': 180352, 'counters/updates': 5636}
skipping logging after 180384 examples to avoid logging too frequently
train stats after 180416 examples: {'rewards_train/chosen': '0.0641', 'rewards_train/rejected': '-0.073744', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13784', 'logps_train/rejected': '-96.916', 'logps_train/chosen': '-112.05', 'loss/train': '0.63824', 'examples_per_second': '31.717', 'grad_norm': '26.875', 'counters/examples': 180416, 'counters/updates': 5638}
train stats after 180448 examples: {'rewards_train/chosen': '0.07761', 'rewards_train/rejected': '0.060668', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016942', 'logps_train/rejected': '-122.6', 'logps_train/chosen': '-106.65', 'loss/train': '0.70033', 'examples_per_second': '31.897', 'grad_norm': '26.25', 'counters/examples': 180448, 'counters/updates': 5639}
train stats after 180480 examples: {'rewards_train/chosen': '0.15543', 'rewards_train/rejected': '0.039233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1162', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-117.81', 'loss/train': '0.64639', 'examples_per_second': '31.134', 'grad_norm': '29.25', 'counters/examples': 180480, 'counters/updates': 5640}
skipping logging after 180512 examples to avoid logging too frequently
train stats after 180544 examples: {'rewards_train/chosen': '0.085977', 'rewards_train/rejected': '-0.0061572', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.092134', 'logps_train/rejected': '-94.844', 'logps_train/chosen': '-103.6', 'loss/train': '0.65448', 'examples_per_second': '34.95', 'grad_norm': '22.125', 'counters/examples': 180544, 'counters/updates': 5642}
train stats after 180576 examples: {'rewards_train/chosen': '0.050829', 'rewards_train/rejected': '0.044453', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0063762', 'logps_train/rejected': '-100.07', 'logps_train/chosen': '-161.4', 'loss/train': '0.70221', 'examples_per_second': '30.543', 'grad_norm': '33', 'counters/examples': 180576, 'counters/updates': 5643}
train stats after 180608 examples: {'rewards_train/chosen': '0.15391', 'rewards_train/rejected': '0.024105', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12981', 'logps_train/rejected': '-128.22', 'logps_train/chosen': '-173.77', 'loss/train': '0.64986', 'examples_per_second': '30.143', 'grad_norm': '34.5', 'counters/examples': 180608, 'counters/updates': 5644}
train stats after 180640 examples: {'rewards_train/chosen': '0.10911', 'rewards_train/rejected': '-0.017686', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12679', 'logps_train/rejected': '-104.22', 'logps_train/chosen': '-121.9', 'loss/train': '0.64494', 'examples_per_second': '31.387', 'grad_norm': '23.375', 'counters/examples': 180640, 'counters/updates': 5645}
train stats after 180672 examples: {'rewards_train/chosen': '0.17149', 'rewards_train/rejected': '0.039686', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1318', 'logps_train/rejected': '-92.981', 'logps_train/chosen': '-136.43', 'loss/train': '0.63708', 'examples_per_second': '32.755', 'grad_norm': '22.375', 'counters/examples': 180672, 'counters/updates': 5646}
train stats after 180704 examples: {'rewards_train/chosen': '0.080237', 'rewards_train/rejected': '0.033158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047078', 'logps_train/rejected': '-113.82', 'logps_train/chosen': '-124.82', 'loss/train': '0.67659', 'examples_per_second': '31.419', 'grad_norm': '25', 'counters/examples': 180704, 'counters/updates': 5647}
train stats after 180736 examples: {'rewards_train/chosen': '0.10492', 'rewards_train/rejected': '0.11401', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0090904', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-130.73', 'loss/train': '0.71172', 'examples_per_second': '31.319', 'grad_norm': '29.25', 'counters/examples': 180736, 'counters/updates': 5648}
train stats after 180768 examples: {'rewards_train/chosen': '0.14935', 'rewards_train/rejected': '0.077191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072159', 'logps_train/rejected': '-106.44', 'logps_train/chosen': '-134.24', 'loss/train': '0.66485', 'examples_per_second': '30.944', 'grad_norm': '25.5', 'counters/examples': 180768, 'counters/updates': 5649}
skipping logging after 180800 examples to avoid logging too frequently
train stats after 180832 examples: {'rewards_train/chosen': '0.14317', 'rewards_train/rejected': '0.026723', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11645', 'logps_train/rejected': '-140.13', 'logps_train/chosen': '-159.53', 'loss/train': '0.6479', 'examples_per_second': '31.93', 'grad_norm': '29.875', 'counters/examples': 180832, 'counters/updates': 5651}
train stats after 180864 examples: {'rewards_train/chosen': '0.090131', 'rewards_train/rejected': '0.06361', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026521', 'logps_train/rejected': '-113.22', 'logps_train/chosen': '-170.8', 'loss/train': '0.70319', 'examples_per_second': '31.319', 'grad_norm': '26.625', 'counters/examples': 180864, 'counters/updates': 5652}
train stats after 180896 examples: {'rewards_train/chosen': '0.18576', 'rewards_train/rejected': '0.10919', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076574', 'logps_train/rejected': '-149.35', 'logps_train/chosen': '-158.27', 'loss/train': '0.66702', 'examples_per_second': '30.729', 'grad_norm': '28.5', 'counters/examples': 180896, 'counters/updates': 5653}
train stats after 180928 examples: {'rewards_train/chosen': '0.18577', 'rewards_train/rejected': '-0.029226', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21499', 'logps_train/rejected': '-147.36', 'logps_train/chosen': '-151.55', 'loss/train': '0.61584', 'examples_per_second': '30.579', 'grad_norm': '25', 'counters/examples': 180928, 'counters/updates': 5654}
train stats after 180960 examples: {'rewards_train/chosen': '0.19553', 'rewards_train/rejected': '0.028896', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16664', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-120.42', 'loss/train': '0.6309', 'examples_per_second': '32.718', 'grad_norm': '31.25', 'counters/examples': 180960, 'counters/updates': 5655}
train stats after 180992 examples: {'rewards_train/chosen': '0.13469', 'rewards_train/rejected': '0.035768', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098924', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-165.08', 'loss/train': '0.65463', 'examples_per_second': '31.641', 'grad_norm': '28.875', 'counters/examples': 180992, 'counters/updates': 5656}
train stats after 181024 examples: {'rewards_train/chosen': '0.11662', 'rewards_train/rejected': '0.075132', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041485', 'logps_train/rejected': '-104.81', 'logps_train/chosen': '-140.63', 'loss/train': '0.68634', 'examples_per_second': '30.29', 'grad_norm': '24.25', 'counters/examples': 181024, 'counters/updates': 5657}
train stats after 181056 examples: {'rewards_train/chosen': '0.10708', 'rewards_train/rejected': '0.016009', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091074', 'logps_train/rejected': '-118.52', 'logps_train/chosen': '-153.79', 'loss/train': '0.6592', 'examples_per_second': '32.951', 'grad_norm': '27.375', 'counters/examples': 181056, 'counters/updates': 5658}
train stats after 181088 examples: {'rewards_train/chosen': '0.12968', 'rewards_train/rejected': '0.087994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041689', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-176.78', 'loss/train': '0.68524', 'examples_per_second': '30.815', 'grad_norm': '27.875', 'counters/examples': 181088, 'counters/updates': 5659}
train stats after 181120 examples: {'rewards_train/chosen': '0.17637', 'rewards_train/rejected': '0.028952', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14742', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-155.46', 'loss/train': '0.64118', 'examples_per_second': '30.88', 'grad_norm': '27.875', 'counters/examples': 181120, 'counters/updates': 5660}
train stats after 181152 examples: {'rewards_train/chosen': '0.048259', 'rewards_train/rejected': '0.052614', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0043552', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-167.85', 'loss/train': '0.70393', 'examples_per_second': '31.642', 'grad_norm': '32.25', 'counters/examples': 181152, 'counters/updates': 5661}
train stats after 181184 examples: {'rewards_train/chosen': '0.05243', 'rewards_train/rejected': '0.0024478', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049982', 'logps_train/rejected': '-125.12', 'logps_train/chosen': '-136.64', 'loss/train': '0.68059', 'examples_per_second': '30.173', 'grad_norm': '27.75', 'counters/examples': 181184, 'counters/updates': 5662}
train stats after 181216 examples: {'rewards_train/chosen': '0.17184', 'rewards_train/rejected': '0.029149', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14269', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-171.72', 'loss/train': '0.63254', 'examples_per_second': '30.762', 'grad_norm': '24.125', 'counters/examples': 181216, 'counters/updates': 5663}
train stats after 181248 examples: {'rewards_train/chosen': '0.16487', 'rewards_train/rejected': '0.086077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078795', 'logps_train/rejected': '-142.2', 'logps_train/chosen': '-165.43', 'loss/train': '0.67196', 'examples_per_second': '30.151', 'grad_norm': '28.75', 'counters/examples': 181248, 'counters/updates': 5664}
train stats after 181280 examples: {'rewards_train/chosen': '0.099779', 'rewards_train/rejected': '0.04468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055099', 'logps_train/rejected': '-137.97', 'logps_train/chosen': '-199.76', 'loss/train': '0.68351', 'examples_per_second': '32.707', 'grad_norm': '32', 'counters/examples': 181280, 'counters/updates': 5665}
train stats after 181312 examples: {'rewards_train/chosen': '0.041898', 'rewards_train/rejected': '0.034148', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0077505', 'logps_train/rejected': '-113.79', 'logps_train/chosen': '-122.85', 'loss/train': '0.69764', 'examples_per_second': '32.895', 'grad_norm': '27', 'counters/examples': 181312, 'counters/updates': 5666}
skipping logging after 181344 examples to avoid logging too frequently
train stats after 181376 examples: {'rewards_train/chosen': '0.073095', 'rewards_train/rejected': '-0.033791', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10689', 'logps_train/rejected': '-152.31', 'logps_train/chosen': '-117.48', 'loss/train': '0.65653', 'examples_per_second': '30.435', 'grad_norm': '30.5', 'counters/examples': 181376, 'counters/updates': 5668}
skipping logging after 181408 examples to avoid logging too frequently
train stats after 181440 examples: {'rewards_train/chosen': '0.25927', 'rewards_train/rejected': '0.030979', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22829', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-165.42', 'loss/train': '0.62518', 'examples_per_second': '34.626', 'grad_norm': '26.875', 'counters/examples': 181440, 'counters/updates': 5670}
train stats after 181472 examples: {'rewards_train/chosen': '0.18021', 'rewards_train/rejected': '0.11958', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060626', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-166.04', 'loss/train': '0.67161', 'examples_per_second': '30.675', 'grad_norm': '25.75', 'counters/examples': 181472, 'counters/updates': 5671}
train stats after 181504 examples: {'rewards_train/chosen': '0.11668', 'rewards_train/rejected': '0.047539', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069143', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-129.79', 'loss/train': '0.669', 'examples_per_second': '32.353', 'grad_norm': '25.5', 'counters/examples': 181504, 'counters/updates': 5672}
train stats after 181536 examples: {'rewards_train/chosen': '0.24447', 'rewards_train/rejected': '0.036299', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20817', 'logps_train/rejected': '-102.47', 'logps_train/chosen': '-169.23', 'loss/train': '0.60536', 'examples_per_second': '31.753', 'grad_norm': '29', 'counters/examples': 181536, 'counters/updates': 5673}
train stats after 181568 examples: {'rewards_train/chosen': '0.099203', 'rewards_train/rejected': '-0.037804', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13701', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-152.82', 'loss/train': '0.64163', 'examples_per_second': '31.235', 'grad_norm': '30', 'counters/examples': 181568, 'counters/updates': 5674}
train stats after 181600 examples: {'rewards_train/chosen': '0.098006', 'rewards_train/rejected': '0.033363', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064643', 'logps_train/rejected': '-154.18', 'logps_train/chosen': '-133.09', 'loss/train': '0.6761', 'examples_per_second': '30.51', 'grad_norm': '26.25', 'counters/examples': 181600, 'counters/updates': 5675}
train stats after 181632 examples: {'rewards_train/chosen': '0.13441', 'rewards_train/rejected': '0.038496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095918', 'logps_train/rejected': '-94.686', 'logps_train/chosen': '-113.34', 'loss/train': '0.66058', 'examples_per_second': '32.766', 'grad_norm': '23.75', 'counters/examples': 181632, 'counters/updates': 5676}
train stats after 181664 examples: {'rewards_train/chosen': '0.21918', 'rewards_train/rejected': '-0.057842', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27703', 'logps_train/rejected': '-106.09', 'logps_train/chosen': '-150.54', 'loss/train': '0.59057', 'examples_per_second': '31.634', 'grad_norm': '22.875', 'counters/examples': 181664, 'counters/updates': 5677}
train stats after 181696 examples: {'rewards_train/chosen': '0.14772', 'rewards_train/rejected': '-0.01783', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16555', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-146.49', 'loss/train': '0.62654', 'examples_per_second': '31.118', 'grad_norm': '26.5', 'counters/examples': 181696, 'counters/updates': 5678}
train stats after 181728 examples: {'rewards_train/chosen': '0.1168', 'rewards_train/rejected': '0.07302', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043776', 'logps_train/rejected': '-166.86', 'logps_train/chosen': '-178', 'loss/train': '0.68383', 'examples_per_second': '31.588', 'grad_norm': '31.625', 'counters/examples': 181728, 'counters/updates': 5679}
train stats after 181760 examples: {'rewards_train/chosen': '0.054305', 'rewards_train/rejected': '0.010537', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043768', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-147.06', 'loss/train': '0.68273', 'examples_per_second': '31.601', 'grad_norm': '27.125', 'counters/examples': 181760, 'counters/updates': 5680}
skipping logging after 181792 examples to avoid logging too frequently
train stats after 181824 examples: {'rewards_train/chosen': '0.097549', 'rewards_train/rejected': '-0.003038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10059', 'logps_train/rejected': '-87.59', 'logps_train/chosen': '-108.16', 'loss/train': '0.65308', 'examples_per_second': '31.481', 'grad_norm': '22.5', 'counters/examples': 181824, 'counters/updates': 5682}
train stats after 181856 examples: {'rewards_train/chosen': '0.17283', 'rewards_train/rejected': '0.052601', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12023', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-150.41', 'loss/train': '0.64184', 'examples_per_second': '32.602', 'grad_norm': '26.5', 'counters/examples': 181856, 'counters/updates': 5683}
train stats after 181888 examples: {'rewards_train/chosen': '0.063047', 'rewards_train/rejected': '-0.0437', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10675', 'logps_train/rejected': '-122.17', 'logps_train/chosen': '-161.57', 'loss/train': '0.6585', 'examples_per_second': '30.671', 'grad_norm': '29.125', 'counters/examples': 181888, 'counters/updates': 5684}
skipping logging after 181920 examples to avoid logging too frequently
train stats after 181952 examples: {'rewards_train/chosen': '0.084696', 'rewards_train/rejected': '-0.0044128', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089109', 'logps_train/rejected': '-108.76', 'logps_train/chosen': '-102.38', 'loss/train': '0.66082', 'examples_per_second': '35.458', 'grad_norm': '22.375', 'counters/examples': 181952, 'counters/updates': 5686}
train stats after 181984 examples: {'rewards_train/chosen': '0.162', 'rewards_train/rejected': '0.058394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10361', 'logps_train/rejected': '-131.27', 'logps_train/chosen': '-132.41', 'loss/train': '0.66634', 'examples_per_second': '31.612', 'grad_norm': '25.875', 'counters/examples': 181984, 'counters/updates': 5687}
skipping logging after 182016 examples to avoid logging too frequently
train stats after 182048 examples: {'rewards_train/chosen': '0.086076', 'rewards_train/rejected': '0.035559', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050517', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-131.63', 'loss/train': '0.67604', 'examples_per_second': '33.933', 'grad_norm': '23.75', 'counters/examples': 182048, 'counters/updates': 5689}
train stats after 182080 examples: {'rewards_train/chosen': '0.098992', 'rewards_train/rejected': '0.01269', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086302', 'logps_train/rejected': '-162.91', 'logps_train/chosen': '-158.01', 'loss/train': '0.66233', 'examples_per_second': '31.641', 'grad_norm': '26.375', 'counters/examples': 182080, 'counters/updates': 5690}
skipping logging after 182112 examples to avoid logging too frequently
train stats after 182144 examples: {'rewards_train/chosen': '0.12982', 'rewards_train/rejected': '-0.050213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18004', 'logps_train/rejected': '-102.78', 'logps_train/chosen': '-137.37', 'loss/train': '0.62064', 'examples_per_second': '33.1', 'grad_norm': '24.5', 'counters/examples': 182144, 'counters/updates': 5692}
skipping logging after 182176 examples to avoid logging too frequently
train stats after 182208 examples: {'rewards_train/chosen': '0.11651', 'rewards_train/rejected': '0.10462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011891', 'logps_train/rejected': '-149.95', 'logps_train/chosen': '-153.67', 'loss/train': '0.70118', 'examples_per_second': '31.253', 'grad_norm': '27.5', 'counters/examples': 182208, 'counters/updates': 5694}
skipping logging after 182240 examples to avoid logging too frequently
train stats after 182272 examples: {'rewards_train/chosen': '0.21124', 'rewards_train/rejected': '0.091739', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1195', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-153.65', 'loss/train': '0.65552', 'examples_per_second': '31.078', 'grad_norm': '28.375', 'counters/examples': 182272, 'counters/updates': 5696}
train stats after 182304 examples: {'rewards_train/chosen': '0.12827', 'rewards_train/rejected': '0.086809', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041464', 'logps_train/rejected': '-95.564', 'logps_train/chosen': '-132.08', 'loss/train': '0.68558', 'examples_per_second': '31.718', 'grad_norm': '23.25', 'counters/examples': 182304, 'counters/updates': 5697}
train stats after 182336 examples: {'rewards_train/chosen': '0.092055', 'rewards_train/rejected': '0.056917', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035138', 'logps_train/rejected': '-121.88', 'logps_train/chosen': '-155.75', 'loss/train': '0.68624', 'examples_per_second': '31.804', 'grad_norm': '29', 'counters/examples': 182336, 'counters/updates': 5698}
train stats after 182368 examples: {'rewards_train/chosen': '0.12078', 'rewards_train/rejected': '0.043131', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07765', 'logps_train/rejected': '-132.31', 'logps_train/chosen': '-133.48', 'loss/train': '0.66273', 'examples_per_second': '30.633', 'grad_norm': '26.5', 'counters/examples': 182368, 'counters/updates': 5699}
train stats after 182400 examples: {'rewards_train/chosen': '0.1866', 'rewards_train/rejected': '-0.024706', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21131', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-164.85', 'loss/train': '0.60914', 'examples_per_second': '31.649', 'grad_norm': '25.625', 'counters/examples': 182400, 'counters/updates': 5700}
train stats after 182432 examples: {'rewards_train/chosen': '0.16567', 'rewards_train/rejected': '0.066879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098793', 'logps_train/rejected': '-102.11', 'logps_train/chosen': '-107.65', 'loss/train': '0.65503', 'examples_per_second': '32.832', 'grad_norm': '22.5', 'counters/examples': 182432, 'counters/updates': 5701}
train stats after 182464 examples: {'rewards_train/chosen': '0.14763', 'rewards_train/rejected': '0.036954', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11068', 'logps_train/rejected': '-106.99', 'logps_train/chosen': '-145.7', 'loss/train': '0.65042', 'examples_per_second': '31.345', 'grad_norm': '25.5', 'counters/examples': 182464, 'counters/updates': 5702}
train stats after 182496 examples: {'rewards_train/chosen': '0.21376', 'rewards_train/rejected': '0.035246', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17851', 'logps_train/rejected': '-119.99', 'logps_train/chosen': '-120.55', 'loss/train': '0.63289', 'examples_per_second': '31.672', 'grad_norm': '23.875', 'counters/examples': 182496, 'counters/updates': 5703}
train stats after 182528 examples: {'rewards_train/chosen': '0.094894', 'rewards_train/rejected': '0.034947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059947', 'logps_train/rejected': '-138.57', 'logps_train/chosen': '-113.77', 'loss/train': '0.67197', 'examples_per_second': '31.495', 'grad_norm': '25.25', 'counters/examples': 182528, 'counters/updates': 5704}
skipping logging after 182560 examples to avoid logging too frequently
train stats after 182592 examples: {'rewards_train/chosen': '0.13277', 'rewards_train/rejected': '0.073736', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059034', 'logps_train/rejected': '-121.54', 'logps_train/chosen': '-125.69', 'loss/train': '0.67094', 'examples_per_second': '33.134', 'grad_norm': '26', 'counters/examples': 182592, 'counters/updates': 5706}
train stats after 182624 examples: {'rewards_train/chosen': '0.17269', 'rewards_train/rejected': '0.12565', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.047043', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-122.73', 'loss/train': '0.68116', 'examples_per_second': '24.709', 'grad_norm': '25.5', 'counters/examples': 182624, 'counters/updates': 5707}
skipping logging after 182656 examples to avoid logging too frequently
train stats after 182688 examples: {'rewards_train/chosen': '0.11813', 'rewards_train/rejected': '0.002877', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11525', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-130.02', 'loss/train': '0.65053', 'examples_per_second': '31.208', 'grad_norm': '23.25', 'counters/examples': 182688, 'counters/updates': 5709}
train stats after 182720 examples: {'rewards_train/chosen': '0.16358', 'rewards_train/rejected': '0.088061', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075515', 'logps_train/rejected': '-133.01', 'logps_train/chosen': '-168.72', 'loss/train': '0.6656', 'examples_per_second': '30.679', 'grad_norm': '34.75', 'counters/examples': 182720, 'counters/updates': 5710}
train stats after 182752 examples: {'rewards_train/chosen': '0.1012', 'rewards_train/rejected': '0.011089', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090109', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-123.25', 'loss/train': '0.66094', 'examples_per_second': '31.751', 'grad_norm': '25.5', 'counters/examples': 182752, 'counters/updates': 5711}
train stats after 182784 examples: {'rewards_train/chosen': '0.092491', 'rewards_train/rejected': '0.054718', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037773', 'logps_train/rejected': '-88.132', 'logps_train/chosen': '-137.4', 'loss/train': '0.68788', 'examples_per_second': '31.656', 'grad_norm': '38.5', 'counters/examples': 182784, 'counters/updates': 5712}
train stats after 182816 examples: {'rewards_train/chosen': '0.16568', 'rewards_train/rejected': '0.064339', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10134', 'logps_train/rejected': '-122.22', 'logps_train/chosen': '-144.72', 'loss/train': '0.66057', 'examples_per_second': '30.515', 'grad_norm': '28.5', 'counters/examples': 182816, 'counters/updates': 5713}
train stats after 182848 examples: {'rewards_train/chosen': '0.097508', 'rewards_train/rejected': '0.069435', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028073', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-115.47', 'loss/train': '0.69708', 'examples_per_second': '31.635', 'grad_norm': '29.875', 'counters/examples': 182848, 'counters/updates': 5714}
train stats after 182880 examples: {'rewards_train/chosen': '0.084057', 'rewards_train/rejected': '0.023748', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060309', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-128.56', 'loss/train': '0.68199', 'examples_per_second': '30.129', 'grad_norm': '26.375', 'counters/examples': 182880, 'counters/updates': 5715}
train stats after 182912 examples: {'rewards_train/chosen': '0.072368', 'rewards_train/rejected': '-0.050048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12242', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-147.02', 'loss/train': '0.64534', 'examples_per_second': '32.924', 'grad_norm': '26.25', 'counters/examples': 182912, 'counters/updates': 5716}
train stats after 182944 examples: {'rewards_train/chosen': '0.13529', 'rewards_train/rejected': '0.044631', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090662', 'logps_train/rejected': '-133.75', 'logps_train/chosen': '-164.72', 'loss/train': '0.66407', 'examples_per_second': '30.216', 'grad_norm': '29.75', 'counters/examples': 182944, 'counters/updates': 5717}
train stats after 182976 examples: {'rewards_train/chosen': '0.060908', 'rewards_train/rejected': '0.029205', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031703', 'logps_train/rejected': '-123.93', 'logps_train/chosen': '-140', 'loss/train': '0.68651', 'examples_per_second': '31.653', 'grad_norm': '29', 'counters/examples': 182976, 'counters/updates': 5718}
skipping logging after 183008 examples to avoid logging too frequently
train stats after 183040 examples: {'rewards_train/chosen': '0.18789', 'rewards_train/rejected': '0.053966', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13393', 'logps_train/rejected': '-113.17', 'logps_train/chosen': '-168.16', 'loss/train': '0.63814', 'examples_per_second': '34.447', 'grad_norm': '24', 'counters/examples': 183040, 'counters/updates': 5720}
skipping logging after 183072 examples to avoid logging too frequently
train stats after 183104 examples: {'rewards_train/chosen': '0.11936', 'rewards_train/rejected': '0.075259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044101', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-127.57', 'loss/train': '0.68252', 'examples_per_second': '32.5', 'grad_norm': '24.25', 'counters/examples': 183104, 'counters/updates': 5722}
train stats after 183136 examples: {'rewards_train/chosen': '0.14457', 'rewards_train/rejected': '0.013415', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13116', 'logps_train/rejected': '-89.492', 'logps_train/chosen': '-118.61', 'loss/train': '0.63591', 'examples_per_second': '29.912', 'grad_norm': '21.125', 'counters/examples': 183136, 'counters/updates': 5723}
train stats after 183168 examples: {'rewards_train/chosen': '0.085059', 'rewards_train/rejected': '0.075622', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0094366', 'logps_train/rejected': '-121.29', 'logps_train/chosen': '-169.98', 'loss/train': '0.69927', 'examples_per_second': '30.248', 'grad_norm': '33.25', 'counters/examples': 183168, 'counters/updates': 5724}
train stats after 183200 examples: {'rewards_train/chosen': '0.22289', 'rewards_train/rejected': '0.07919', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1437', 'logps_train/rejected': '-133.73', 'logps_train/chosen': '-152.73', 'loss/train': '0.64683', 'examples_per_second': '31.122', 'grad_norm': '27.375', 'counters/examples': 183200, 'counters/updates': 5725}
train stats after 183232 examples: {'rewards_train/chosen': '0.075344', 'rewards_train/rejected': '0.091832', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.016488', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-154.89', 'loss/train': '0.71624', 'examples_per_second': '31.645', 'grad_norm': '31.875', 'counters/examples': 183232, 'counters/updates': 5726}
train stats after 183264 examples: {'rewards_train/chosen': '0.19203', 'rewards_train/rejected': '0.047561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14447', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-137.47', 'loss/train': '0.63472', 'examples_per_second': '31.575', 'grad_norm': '27.5', 'counters/examples': 183264, 'counters/updates': 5727}
train stats after 183296 examples: {'rewards_train/chosen': '0.17186', 'rewards_train/rejected': '0.022083', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14978', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-145.62', 'loss/train': '0.62956', 'examples_per_second': '31.811', 'grad_norm': '27.625', 'counters/examples': 183296, 'counters/updates': 5728}
skipping logging after 183328 examples to avoid logging too frequently
train stats after 183360 examples: {'rewards_train/chosen': '0.086848', 'rewards_train/rejected': '0.069022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017826', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-151.58', 'loss/train': '0.6986', 'examples_per_second': '31.987', 'grad_norm': '29.625', 'counters/examples': 183360, 'counters/updates': 5730}
train stats after 183392 examples: {'rewards_train/chosen': '0.13546', 'rewards_train/rejected': '0.015867', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11959', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-129.11', 'loss/train': '0.65685', 'examples_per_second': '31.776', 'grad_norm': '26.625', 'counters/examples': 183392, 'counters/updates': 5731}
train stats after 183424 examples: {'rewards_train/chosen': '0.16938', 'rewards_train/rejected': '-0.071125', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24051', 'logps_train/rejected': '-118.76', 'logps_train/chosen': '-172.68', 'loss/train': '0.59552', 'examples_per_second': '31.43', 'grad_norm': '23.5', 'counters/examples': 183424, 'counters/updates': 5732}
train stats after 183456 examples: {'rewards_train/chosen': '0.19018', 'rewards_train/rejected': '0.030542', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15964', 'logps_train/rejected': '-111.46', 'logps_train/chosen': '-165.88', 'loss/train': '0.62988', 'examples_per_second': '32.702', 'grad_norm': '30', 'counters/examples': 183456, 'counters/updates': 5733}
train stats after 183488 examples: {'rewards_train/chosen': '0.12278', 'rewards_train/rejected': '0.035033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087743', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-163.07', 'loss/train': '0.66701', 'examples_per_second': '31.931', 'grad_norm': '28.375', 'counters/examples': 183488, 'counters/updates': 5734}
train stats after 183520 examples: {'rewards_train/chosen': '0.25101', 'rewards_train/rejected': '0.099075', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15194', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-173.89', 'loss/train': '0.62807', 'examples_per_second': '31.275', 'grad_norm': '25.5', 'counters/examples': 183520, 'counters/updates': 5735}
skipping logging after 183552 examples to avoid logging too frequently
train stats after 183584 examples: {'rewards_train/chosen': '0.12032', 'rewards_train/rejected': '0.043256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077064', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-157.4', 'loss/train': '0.67411', 'examples_per_second': '31.604', 'grad_norm': '29.125', 'counters/examples': 183584, 'counters/updates': 5737}
train stats after 183616 examples: {'rewards_train/chosen': '0.20292', 'rewards_train/rejected': '0.0073413', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.19557', 'logps_train/rejected': '-119.35', 'logps_train/chosen': '-153.68', 'loss/train': '0.61533', 'examples_per_second': '31.645', 'grad_norm': '26', 'counters/examples': 183616, 'counters/updates': 5738}
train stats after 183648 examples: {'rewards_train/chosen': '0.15046', 'rewards_train/rejected': '0.016311', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13415', 'logps_train/rejected': '-105.14', 'logps_train/chosen': '-140.66', 'loss/train': '0.64049', 'examples_per_second': '31.645', 'grad_norm': '23', 'counters/examples': 183648, 'counters/updates': 5739}
train stats after 183680 examples: {'rewards_train/chosen': '0.17004', 'rewards_train/rejected': '-0.0089547', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.179', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-138.85', 'loss/train': '0.6185', 'examples_per_second': '31.533', 'grad_norm': '23.125', 'counters/examples': 183680, 'counters/updates': 5740}
train stats after 183712 examples: {'rewards_train/chosen': '0.11443', 'rewards_train/rejected': '0.012457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10197', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-115.71', 'loss/train': '0.68256', 'examples_per_second': '30.501', 'grad_norm': '24.875', 'counters/examples': 183712, 'counters/updates': 5741}
train stats after 183744 examples: {'rewards_train/chosen': '0.16255', 'rewards_train/rejected': '0.07613', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086418', 'logps_train/rejected': '-141.92', 'logps_train/chosen': '-116.76', 'loss/train': '0.67363', 'examples_per_second': '31.629', 'grad_norm': '27.75', 'counters/examples': 183744, 'counters/updates': 5742}
skipping logging after 183776 examples to avoid logging too frequently
train stats after 183808 examples: {'rewards_train/chosen': '0.091645', 'rewards_train/rejected': '0.059221', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032424', 'logps_train/rejected': '-105.95', 'logps_train/chosen': '-110.94', 'loss/train': '0.69302', 'examples_per_second': '32.72', 'grad_norm': '26.75', 'counters/examples': 183808, 'counters/updates': 5744}
train stats after 183840 examples: {'rewards_train/chosen': '0.16691', 'rewards_train/rejected': '0.046473', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12044', 'logps_train/rejected': '-154.47', 'logps_train/chosen': '-162.21', 'loss/train': '0.64939', 'examples_per_second': '31.4', 'grad_norm': '28.375', 'counters/examples': 183840, 'counters/updates': 5745}
train stats after 183872 examples: {'rewards_train/chosen': '0.091043', 'rewards_train/rejected': '-0.02821', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11925', 'logps_train/rejected': '-119.72', 'logps_train/chosen': '-114.5', 'loss/train': '0.64702', 'examples_per_second': '31.914', 'grad_norm': '24', 'counters/examples': 183872, 'counters/updates': 5746}
train stats after 183904 examples: {'rewards_train/chosen': '0.18084', 'rewards_train/rejected': '-0.013469', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1943', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-158.25', 'loss/train': '0.61389', 'examples_per_second': '30.558', 'grad_norm': '26.5', 'counters/examples': 183904, 'counters/updates': 5747}
train stats after 183936 examples: {'rewards_train/chosen': '0.16142', 'rewards_train/rejected': '0.12988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031548', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-160.24', 'loss/train': '0.68593', 'examples_per_second': '32.155', 'grad_norm': '26.625', 'counters/examples': 183936, 'counters/updates': 5748}
skipping logging after 183968 examples to avoid logging too frequently
train stats after 184000 examples: {'rewards_train/chosen': '0.18287', 'rewards_train/rejected': '0.094126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088745', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-117.77', 'loss/train': '0.66245', 'examples_per_second': '30.276', 'grad_norm': '28.75', 'counters/examples': 184000, 'counters/updates': 5750}
train stats after 184032 examples: {'rewards_train/chosen': '0.2093', 'rewards_train/rejected': '0.062503', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1468', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-175.73', 'loss/train': '0.63553', 'examples_per_second': '30.819', 'grad_norm': '27.75', 'counters/examples': 184032, 'counters/updates': 5751}
train stats after 184064 examples: {'rewards_train/chosen': '0.16195', 'rewards_train/rejected': '0.053659', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10829', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-142.87', 'loss/train': '0.6613', 'examples_per_second': '30.165', 'grad_norm': '42.25', 'counters/examples': 184064, 'counters/updates': 5752}
train stats after 184096 examples: {'rewards_train/chosen': '0.21554', 'rewards_train/rejected': '0.032761', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18278', 'logps_train/rejected': '-150.44', 'logps_train/chosen': '-191.28', 'loss/train': '0.62578', 'examples_per_second': '31.625', 'grad_norm': '30.375', 'counters/examples': 184096, 'counters/updates': 5753}
train stats after 184128 examples: {'rewards_train/chosen': '0.15696', 'rewards_train/rejected': '0.070485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086479', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-127.85', 'loss/train': '0.6613', 'examples_per_second': '31.243', 'grad_norm': '23.25', 'counters/examples': 184128, 'counters/updates': 5754}
train stats after 184160 examples: {'rewards_train/chosen': '0.15118', 'rewards_train/rejected': '0.084273', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06691', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-119.6', 'loss/train': '0.66986', 'examples_per_second': '31.567', 'grad_norm': '27.625', 'counters/examples': 184160, 'counters/updates': 5755}
train stats after 184192 examples: {'rewards_train/chosen': '0.20472', 'rewards_train/rejected': '0.07892', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1258', 'logps_train/rejected': '-139.84', 'logps_train/chosen': '-162.21', 'loss/train': '0.6416', 'examples_per_second': '31.811', 'grad_norm': '29', 'counters/examples': 184192, 'counters/updates': 5756}
skipping logging after 184224 examples to avoid logging too frequently
train stats after 184256 examples: {'rewards_train/chosen': '0.16949', 'rewards_train/rejected': '0.15946', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010036', 'logps_train/rejected': '-166.53', 'logps_train/chosen': '-142.95', 'loss/train': '0.70406', 'examples_per_second': '31.062', 'grad_norm': '34.25', 'counters/examples': 184256, 'counters/updates': 5758}
train stats after 184288 examples: {'rewards_train/chosen': '0.12504', 'rewards_train/rejected': '0.12433', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00071174', 'logps_train/rejected': '-169.18', 'logps_train/chosen': '-129.94', 'loss/train': '0.70575', 'examples_per_second': '32.197', 'grad_norm': '33', 'counters/examples': 184288, 'counters/updates': 5759}
skipping logging after 184320 examples to avoid logging too frequently
train stats after 184352 examples: {'rewards_train/chosen': '0.18277', 'rewards_train/rejected': '0.023974', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15879', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-121.21', 'loss/train': '0.63081', 'examples_per_second': '30.112', 'grad_norm': '25.75', 'counters/examples': 184352, 'counters/updates': 5761}
skipping logging after 184384 examples to avoid logging too frequently
train stats after 184416 examples: {'rewards_train/chosen': '0.11324', 'rewards_train/rejected': '0.011234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.102', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-121.45', 'loss/train': '0.65278', 'examples_per_second': '36.305', 'grad_norm': '25.75', 'counters/examples': 184416, 'counters/updates': 5763}
train stats after 184448 examples: {'rewards_train/chosen': '0.077291', 'rewards_train/rejected': '0.019272', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05802', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-132.94', 'loss/train': '0.67572', 'examples_per_second': '31.985', 'grad_norm': '26', 'counters/examples': 184448, 'counters/updates': 5764}
skipping logging after 184480 examples to avoid logging too frequently
train stats after 184512 examples: {'rewards_train/chosen': '0.15616', 'rewards_train/rejected': '0.0029335', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15322', 'logps_train/rejected': '-98.912', 'logps_train/chosen': '-142.93', 'loss/train': '0.63601', 'examples_per_second': '30.706', 'grad_norm': '25.875', 'counters/examples': 184512, 'counters/updates': 5766}
train stats after 184544 examples: {'rewards_train/chosen': '0.14985', 'rewards_train/rejected': '-0.015961', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16581', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-123.37', 'loss/train': '0.62335', 'examples_per_second': '31.598', 'grad_norm': '23.625', 'counters/examples': 184544, 'counters/updates': 5767}
train stats after 184576 examples: {'rewards_train/chosen': '0.1831', 'rewards_train/rejected': '-0.027962', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21106', 'logps_train/rejected': '-106.8', 'logps_train/chosen': '-147.02', 'loss/train': '0.60905', 'examples_per_second': '30.727', 'grad_norm': '23', 'counters/examples': 184576, 'counters/updates': 5768}
train stats after 184608 examples: {'rewards_train/chosen': '0.16012', 'rewards_train/rejected': '0.11739', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042733', 'logps_train/rejected': '-134.17', 'logps_train/chosen': '-152.84', 'loss/train': '0.6874', 'examples_per_second': '31.634', 'grad_norm': '31.625', 'counters/examples': 184608, 'counters/updates': 5769}
skipping logging after 184640 examples to avoid logging too frequently
train stats after 184672 examples: {'rewards_train/chosen': '0.22255', 'rewards_train/rejected': '0.12157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10098', 'logps_train/rejected': '-157.99', 'logps_train/chosen': '-147.34', 'loss/train': '0.66176', 'examples_per_second': '30.239', 'grad_norm': '30.75', 'counters/examples': 184672, 'counters/updates': 5771}
train stats after 184704 examples: {'rewards_train/chosen': '0.14636', 'rewards_train/rejected': '-0.02664', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.173', 'logps_train/rejected': '-118.83', 'logps_train/chosen': '-111.03', 'loss/train': '0.61901', 'examples_per_second': '30.326', 'grad_norm': '21.875', 'counters/examples': 184704, 'counters/updates': 5772}
skipping logging after 184736 examples to avoid logging too frequently
train stats after 184768 examples: {'rewards_train/chosen': '0.17063', 'rewards_train/rejected': '-0.056691', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22732', 'logps_train/rejected': '-116.2', 'logps_train/chosen': '-133.39', 'loss/train': '0.59882', 'examples_per_second': '25.558', 'grad_norm': '29.125', 'counters/examples': 184768, 'counters/updates': 5774}
train stats after 184800 examples: {'rewards_train/chosen': '0.08183', 'rewards_train/rejected': '0.0067467', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075083', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-128.68', 'loss/train': '0.66825', 'examples_per_second': '32.734', 'grad_norm': '31.5', 'counters/examples': 184800, 'counters/updates': 5775}
train stats after 184832 examples: {'rewards_train/chosen': '0.081548', 'rewards_train/rejected': '0.049204', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032343', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-141.61', 'loss/train': '0.69105', 'examples_per_second': '30.12', 'grad_norm': '27.75', 'counters/examples': 184832, 'counters/updates': 5776}
train stats after 184864 examples: {'rewards_train/chosen': '0.13556', 'rewards_train/rejected': '0.039112', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096452', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-142.97', 'loss/train': '0.655', 'examples_per_second': '26.051', 'grad_norm': '30.75', 'counters/examples': 184864, 'counters/updates': 5777}
train stats after 184896 examples: {'rewards_train/chosen': '0.13704', 'rewards_train/rejected': '0.046765', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090279', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-164.01', 'loss/train': '0.66354', 'examples_per_second': '31.014', 'grad_norm': '29.625', 'counters/examples': 184896, 'counters/updates': 5778}
train stats after 184928 examples: {'rewards_train/chosen': '0.098973', 'rewards_train/rejected': '0.063097', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.035876', 'logps_train/rejected': '-109.3', 'logps_train/chosen': '-122.14', 'loss/train': '0.68736', 'examples_per_second': '31.244', 'grad_norm': '28.125', 'counters/examples': 184928, 'counters/updates': 5779}
skipping logging after 184960 examples to avoid logging too frequently
train stats after 184992 examples: {'rewards_train/chosen': '0.22761', 'rewards_train/rejected': '0.0876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14001', 'logps_train/rejected': '-135.54', 'logps_train/chosen': '-164.54', 'loss/train': '0.64433', 'examples_per_second': '32.018', 'grad_norm': '31', 'counters/examples': 184992, 'counters/updates': 5781}
skipping logging after 185024 examples to avoid logging too frequently
train stats after 185056 examples: {'rewards_train/chosen': '0.18896', 'rewards_train/rejected': '0.038534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15043', 'logps_train/rejected': '-140.67', 'logps_train/chosen': '-128.31', 'loss/train': '0.63158', 'examples_per_second': '31.57', 'grad_norm': '27.125', 'counters/examples': 185056, 'counters/updates': 5783}
skipping logging after 185088 examples to avoid logging too frequently
train stats after 185120 examples: {'rewards_train/chosen': '0.14845', 'rewards_train/rejected': '-0.0058066', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15426', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-156.74', 'loss/train': '0.63244', 'examples_per_second': '30.444', 'grad_norm': '26.125', 'counters/examples': 185120, 'counters/updates': 5785}
skipping logging after 185152 examples to avoid logging too frequently
train stats after 185184 examples: {'rewards_train/chosen': '0.22351', 'rewards_train/rejected': '0.06773', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15578', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-122.4', 'loss/train': '0.63692', 'examples_per_second': '30.709', 'grad_norm': '24.75', 'counters/examples': 185184, 'counters/updates': 5787}
train stats after 185216 examples: {'rewards_train/chosen': '0.093836', 'rewards_train/rejected': '0.090437', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0033991', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-140.43', 'loss/train': '0.70384', 'examples_per_second': '31.324', 'grad_norm': '28.625', 'counters/examples': 185216, 'counters/updates': 5788}
train stats after 185248 examples: {'rewards_train/chosen': '0.15137', 'rewards_train/rejected': '0.028504', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12286', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-132.66', 'loss/train': '0.65019', 'examples_per_second': '31.086', 'grad_norm': '22.125', 'counters/examples': 185248, 'counters/updates': 5789}
skipping logging after 185280 examples to avoid logging too frequently
train stats after 185312 examples: {'rewards_train/chosen': '0.14855', 'rewards_train/rejected': '0.082849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0657', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-143.88', 'loss/train': '0.67081', 'examples_per_second': '32.218', 'grad_norm': '29.25', 'counters/examples': 185312, 'counters/updates': 5791}
train stats after 185344 examples: {'rewards_train/chosen': '0.15936', 'rewards_train/rejected': '0.0078753', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15148', 'logps_train/rejected': '-148.62', 'logps_train/chosen': '-128.74', 'loss/train': '0.62611', 'examples_per_second': '30.154', 'grad_norm': '25.125', 'counters/examples': 185344, 'counters/updates': 5792}
skipping logging after 185376 examples to avoid logging too frequently
train stats after 185408 examples: {'rewards_train/chosen': '0.18181', 'rewards_train/rejected': '0.055584', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12623', 'logps_train/rejected': '-103.77', 'logps_train/chosen': '-130.09', 'loss/train': '0.64579', 'examples_per_second': '31.296', 'grad_norm': '22.875', 'counters/examples': 185408, 'counters/updates': 5794}
train stats after 185440 examples: {'rewards_train/chosen': '0.16805', 'rewards_train/rejected': '0.023493', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14456', 'logps_train/rejected': '-116.21', 'logps_train/chosen': '-161.19', 'loss/train': '0.63991', 'examples_per_second': '30.053', 'grad_norm': '25.875', 'counters/examples': 185440, 'counters/updates': 5795}
skipping logging after 185472 examples to avoid logging too frequently
train stats after 185504 examples: {'rewards_train/chosen': '0.15586', 'rewards_train/rejected': '0.049758', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1061', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-162.93', 'loss/train': '0.65763', 'examples_per_second': '31.812', 'grad_norm': '27.5', 'counters/examples': 185504, 'counters/updates': 5797}
train stats after 185536 examples: {'rewards_train/chosen': '0.13853', 'rewards_train/rejected': '0.033493', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10504', 'logps_train/rejected': '-145.44', 'logps_train/chosen': '-150.63', 'loss/train': '0.65905', 'examples_per_second': '32.079', 'grad_norm': '27.125', 'counters/examples': 185536, 'counters/updates': 5798}
train stats after 185568 examples: {'rewards_train/chosen': '0.054974', 'rewards_train/rejected': '-0.027159', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082133', 'logps_train/rejected': '-119.03', 'logps_train/chosen': '-140.66', 'loss/train': '0.66556', 'examples_per_second': '30.829', 'grad_norm': '25.625', 'counters/examples': 185568, 'counters/updates': 5799}
skipping logging after 185600 examples to avoid logging too frequently
train stats after 185632 examples: {'rewards_train/chosen': '0.074565', 'rewards_train/rejected': '-0.0038093', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078375', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-143.12', 'loss/train': '0.66143', 'examples_per_second': '33.551', 'grad_norm': '24.375', 'counters/examples': 185632, 'counters/updates': 5801}
skipping logging after 185664 examples to avoid logging too frequently
train stats after 185696 examples: {'rewards_train/chosen': '0.13081', 'rewards_train/rejected': '0.0054084', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1254', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-164.05', 'loss/train': '0.64458', 'examples_per_second': '34.43', 'grad_norm': '24.625', 'counters/examples': 185696, 'counters/updates': 5803}
train stats after 185728 examples: {'rewards_train/chosen': '0.13362', 'rewards_train/rejected': '0.05828', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075344', 'logps_train/rejected': '-103.5', 'logps_train/chosen': '-140.5', 'loss/train': '0.67289', 'examples_per_second': '31.603', 'grad_norm': '26.75', 'counters/examples': 185728, 'counters/updates': 5804}
train stats after 185760 examples: {'rewards_train/chosen': '0.082122', 'rewards_train/rejected': '-0.0096279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09175', 'logps_train/rejected': '-137.82', 'logps_train/chosen': '-164.04', 'loss/train': '0.65911', 'examples_per_second': '31.694', 'grad_norm': '28.25', 'counters/examples': 185760, 'counters/updates': 5805}
train stats after 185792 examples: {'rewards_train/chosen': '0.14371', 'rewards_train/rejected': '0.091895', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051815', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-136.63', 'loss/train': '0.67428', 'examples_per_second': '31.587', 'grad_norm': '30.625', 'counters/examples': 185792, 'counters/updates': 5806}
train stats after 185824 examples: {'rewards_train/chosen': '0.11392', 'rewards_train/rejected': '0.030901', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083024', 'logps_train/rejected': '-109.3', 'logps_train/chosen': '-112.07', 'loss/train': '0.65886', 'examples_per_second': '30.616', 'grad_norm': '23.125', 'counters/examples': 185824, 'counters/updates': 5807}
train stats after 185856 examples: {'rewards_train/chosen': '0.14835', 'rewards_train/rejected': '-0.012854', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16121', 'logps_train/rejected': '-127.85', 'logps_train/chosen': '-171.44', 'loss/train': '0.62538', 'examples_per_second': '30.06', 'grad_norm': '31.875', 'counters/examples': 185856, 'counters/updates': 5808}
train stats after 185888 examples: {'rewards_train/chosen': '0.11542', 'rewards_train/rejected': '0.059082', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056342', 'logps_train/rejected': '-103.42', 'logps_train/chosen': '-146.79', 'loss/train': '0.67446', 'examples_per_second': '31.449', 'grad_norm': '27.125', 'counters/examples': 185888, 'counters/updates': 5809}
train stats after 185920 examples: {'rewards_train/chosen': '0.11616', 'rewards_train/rejected': '0.090242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025915', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-111.08', 'loss/train': '0.69639', 'examples_per_second': '31.606', 'grad_norm': '26.25', 'counters/examples': 185920, 'counters/updates': 5810}
train stats after 185952 examples: {'rewards_train/chosen': '0.1786', 'rewards_train/rejected': '-0.049502', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22811', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-158.06', 'loss/train': '0.5973', 'examples_per_second': '30.31', 'grad_norm': '23.125', 'counters/examples': 185952, 'counters/updates': 5811}
train stats after 185984 examples: {'rewards_train/chosen': '0.13509', 'rewards_train/rejected': '-0.028702', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16379', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-153.98', 'loss/train': '0.62356', 'examples_per_second': '32.934', 'grad_norm': '27.625', 'counters/examples': 185984, 'counters/updates': 5812}
train stats after 186016 examples: {'rewards_train/chosen': '0.21963', 'rewards_train/rejected': '-0.052256', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27189', 'logps_train/rejected': '-108.32', 'logps_train/chosen': '-198.89', 'loss/train': '0.58698', 'examples_per_second': '31.634', 'grad_norm': '26.625', 'counters/examples': 186016, 'counters/updates': 5813}
train stats after 186048 examples: {'rewards_train/chosen': '0.12215', 'rewards_train/rejected': '0.041033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081121', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-156.38', 'loss/train': '0.66248', 'examples_per_second': '31.568', 'grad_norm': '26.375', 'counters/examples': 186048, 'counters/updates': 5814}
train stats after 186080 examples: {'rewards_train/chosen': '0.080161', 'rewards_train/rejected': '-0.016659', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.096819', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-164.26', 'loss/train': '0.66328', 'examples_per_second': '32.445', 'grad_norm': '26', 'counters/examples': 186080, 'counters/updates': 5815}
skipping logging after 186112 examples to avoid logging too frequently
train stats after 186144 examples: {'rewards_train/chosen': '0.12934', 'rewards_train/rejected': '-0.066459', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1958', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-99.558', 'loss/train': '0.61262', 'examples_per_second': '38.459', 'grad_norm': '22.125', 'counters/examples': 186144, 'counters/updates': 5817}
train stats after 186176 examples: {'rewards_train/chosen': '0.094238', 'rewards_train/rejected': '0.042694', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051544', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-119.5', 'loss/train': '0.67472', 'examples_per_second': '31.541', 'grad_norm': '25.625', 'counters/examples': 186176, 'counters/updates': 5818}
train stats after 186208 examples: {'rewards_train/chosen': '0.12519', 'rewards_train/rejected': '0.073053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052135', 'logps_train/rejected': '-116.4', 'logps_train/chosen': '-136.03', 'loss/train': '0.67634', 'examples_per_second': '32.115', 'grad_norm': '25', 'counters/examples': 186208, 'counters/updates': 5819}
train stats after 186240 examples: {'rewards_train/chosen': '0.15189', 'rewards_train/rejected': '0.042612', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10928', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-118.36', 'loss/train': '0.64437', 'examples_per_second': '31.45', 'grad_norm': '23.625', 'counters/examples': 186240, 'counters/updates': 5820}
train stats after 186272 examples: {'rewards_train/chosen': '0.058667', 'rewards_train/rejected': '0.060646', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0019787', 'logps_train/rejected': '-137.92', 'logps_train/chosen': '-153.21', 'loss/train': '0.70502', 'examples_per_second': '32.17', 'grad_norm': '30.5', 'counters/examples': 186272, 'counters/updates': 5821}
skipping logging after 186304 examples to avoid logging too frequently
train stats after 186336 examples: {'rewards_train/chosen': '0.13244', 'rewards_train/rejected': '0.049014', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083425', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-162.19', 'loss/train': '0.66722', 'examples_per_second': '32.918', 'grad_norm': '25.125', 'counters/examples': 186336, 'counters/updates': 5823}
train stats after 186368 examples: {'rewards_train/chosen': '0.10237', 'rewards_train/rejected': '0.025988', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076382', 'logps_train/rejected': '-99.452', 'logps_train/chosen': '-163.12', 'loss/train': '0.66648', 'examples_per_second': '31.618', 'grad_norm': '28.5', 'counters/examples': 186368, 'counters/updates': 5824}
train stats after 186400 examples: {'rewards_train/chosen': '0.089867', 'rewards_train/rejected': '-0.021189', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11106', 'logps_train/rejected': '-84.561', 'logps_train/chosen': '-97.045', 'loss/train': '0.64631', 'examples_per_second': '32.195', 'grad_norm': '19.625', 'counters/examples': 186400, 'counters/updates': 5825}
train stats after 186432 examples: {'rewards_train/chosen': '0.075896', 'rewards_train/rejected': '0.021051', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054845', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-156.9', 'loss/train': '0.68334', 'examples_per_second': '31.861', 'grad_norm': '28', 'counters/examples': 186432, 'counters/updates': 5826}
train stats after 186464 examples: {'rewards_train/chosen': '0.14422', 'rewards_train/rejected': '-0.019827', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16405', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-138.03', 'loss/train': '0.62189', 'examples_per_second': '31.475', 'grad_norm': '25.5', 'counters/examples': 186464, 'counters/updates': 5827}
train stats after 186496 examples: {'rewards_train/chosen': '0.14596', 'rewards_train/rejected': '0.050767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095194', 'logps_train/rejected': '-104.68', 'logps_train/chosen': '-198.03', 'loss/train': '0.67716', 'examples_per_second': '30.722', 'grad_norm': '26.375', 'counters/examples': 186496, 'counters/updates': 5828}
train stats after 186528 examples: {'rewards_train/chosen': '0.16247', 'rewards_train/rejected': '0.039123', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12335', 'logps_train/rejected': '-146', 'logps_train/chosen': '-148.07', 'loss/train': '0.6534', 'examples_per_second': '31.65', 'grad_norm': '29.625', 'counters/examples': 186528, 'counters/updates': 5829}
train stats after 186560 examples: {'rewards_train/chosen': '0.086777', 'rewards_train/rejected': '0.019819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066958', 'logps_train/rejected': '-101.13', 'logps_train/chosen': '-125.12', 'loss/train': '0.67038', 'examples_per_second': '32.371', 'grad_norm': '23.375', 'counters/examples': 186560, 'counters/updates': 5830}
train stats after 186592 examples: {'rewards_train/chosen': '0.15588', 'rewards_train/rejected': '0.092196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063684', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-150.88', 'loss/train': '0.67642', 'examples_per_second': '30.12', 'grad_norm': '26.25', 'counters/examples': 186592, 'counters/updates': 5831}
train stats after 186624 examples: {'rewards_train/chosen': '0.13414', 'rewards_train/rejected': '0.070606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063538', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-153.14', 'loss/train': '0.67329', 'examples_per_second': '31.616', 'grad_norm': '29.75', 'counters/examples': 186624, 'counters/updates': 5832}
train stats after 186656 examples: {'rewards_train/chosen': '0.14636', 'rewards_train/rejected': '0.04164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10472', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-138.24', 'loss/train': '0.65704', 'examples_per_second': '31.617', 'grad_norm': '26.25', 'counters/examples': 186656, 'counters/updates': 5833}
train stats after 186688 examples: {'rewards_train/chosen': '0.12402', 'rewards_train/rejected': '0.10221', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021806', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-147.35', 'loss/train': '0.68812', 'examples_per_second': '30.368', 'grad_norm': '26.125', 'counters/examples': 186688, 'counters/updates': 5834}
train stats after 186720 examples: {'rewards_train/chosen': '0.12119', 'rewards_train/rejected': '-0.00025538', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12145', 'logps_train/rejected': '-108.94', 'logps_train/chosen': '-114.14', 'loss/train': '0.64352', 'examples_per_second': '31.606', 'grad_norm': '23.625', 'counters/examples': 186720, 'counters/updates': 5835}
train stats after 186752 examples: {'rewards_train/chosen': '0.13399', 'rewards_train/rejected': '0.047368', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086626', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-157.17', 'loss/train': '0.66826', 'examples_per_second': '32.112', 'grad_norm': '26.5', 'counters/examples': 186752, 'counters/updates': 5836}
train stats after 186784 examples: {'rewards_train/chosen': '0.14', 'rewards_train/rejected': '0.050293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089711', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-179.23', 'loss/train': '0.66586', 'examples_per_second': '31.425', 'grad_norm': '28.5', 'counters/examples': 186784, 'counters/updates': 5837}
train stats after 186816 examples: {'rewards_train/chosen': '0.17975', 'rewards_train/rejected': '0.072673', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10708', 'logps_train/rejected': '-131.15', 'logps_train/chosen': '-150.87', 'loss/train': '0.65911', 'examples_per_second': '32.205', 'grad_norm': '30', 'counters/examples': 186816, 'counters/updates': 5838}
train stats after 186848 examples: {'rewards_train/chosen': '0.172', 'rewards_train/rejected': '0.040637', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13136', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-120.18', 'loss/train': '0.63905', 'examples_per_second': '30.716', 'grad_norm': '24.25', 'counters/examples': 186848, 'counters/updates': 5839}
train stats after 186880 examples: {'rewards_train/chosen': '0.10662', 'rewards_train/rejected': '-0.014664', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12128', 'logps_train/rejected': '-133.65', 'logps_train/chosen': '-117.7', 'loss/train': '0.64372', 'examples_per_second': '33.082', 'grad_norm': '29.375', 'counters/examples': 186880, 'counters/updates': 5840}
train stats after 186912 examples: {'rewards_train/chosen': '0.10565', 'rewards_train/rejected': '0.03236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073292', 'logps_train/rejected': '-133.15', 'logps_train/chosen': '-161.82', 'loss/train': '0.66792', 'examples_per_second': '30.864', 'grad_norm': '34.25', 'counters/examples': 186912, 'counters/updates': 5841}
skipping logging after 186944 examples to avoid logging too frequently
train stats after 186976 examples: {'rewards_train/chosen': '0.17025', 'rewards_train/rejected': '0.032623', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13763', 'logps_train/rejected': '-107.44', 'logps_train/chosen': '-125.51', 'loss/train': '0.6413', 'examples_per_second': '34.329', 'grad_norm': '24.375', 'counters/examples': 186976, 'counters/updates': 5843}
train stats after 187008 examples: {'rewards_train/chosen': '0.086819', 'rewards_train/rejected': '0.082272', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0045468', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-123.02', 'loss/train': '0.70206', 'examples_per_second': '30.496', 'grad_norm': '25.875', 'counters/examples': 187008, 'counters/updates': 5844}
train stats after 187040 examples: {'rewards_train/chosen': '0.12373', 'rewards_train/rejected': '0.043374', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080361', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-139.98', 'loss/train': '0.66511', 'examples_per_second': '31.692', 'grad_norm': '26.375', 'counters/examples': 187040, 'counters/updates': 5845}
train stats after 187072 examples: {'rewards_train/chosen': '0.05974', 'rewards_train/rejected': '0.039329', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020411', 'logps_train/rejected': '-86.005', 'logps_train/chosen': '-124.44', 'loss/train': '0.69028', 'examples_per_second': '32.734', 'grad_norm': '22.5', 'counters/examples': 187072, 'counters/updates': 5846}
train stats after 187104 examples: {'rewards_train/chosen': '0.095501', 'rewards_train/rejected': '0.0043716', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09113', 'logps_train/rejected': '-93.207', 'logps_train/chosen': '-139', 'loss/train': '0.66154', 'examples_per_second': '30.136', 'grad_norm': '23.125', 'counters/examples': 187104, 'counters/updates': 5847}
train stats after 187136 examples: {'rewards_train/chosen': '0.1568', 'rewards_train/rejected': '0.033027', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12378', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-135.93', 'loss/train': '0.65229', 'examples_per_second': '30.791', 'grad_norm': '23.625', 'counters/examples': 187136, 'counters/updates': 5848}
train stats after 187168 examples: {'rewards_train/chosen': '0.17498', 'rewards_train/rejected': '-0.021293', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19627', 'logps_train/rejected': '-120.29', 'logps_train/chosen': '-128.54', 'loss/train': '0.61124', 'examples_per_second': '31.627', 'grad_norm': '26.25', 'counters/examples': 187168, 'counters/updates': 5849}
train stats after 187200 examples: {'rewards_train/chosen': '0.14729', 'rewards_train/rejected': '0.029071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11822', 'logps_train/rejected': '-122.79', 'logps_train/chosen': '-144.02', 'loss/train': '0.64424', 'examples_per_second': '31.848', 'grad_norm': '23.375', 'counters/examples': 187200, 'counters/updates': 5850}
train stats after 187232 examples: {'rewards_train/chosen': '0.069472', 'rewards_train/rejected': '0.0076633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061809', 'logps_train/rejected': '-99.185', 'logps_train/chosen': '-125.52', 'loss/train': '0.6765', 'examples_per_second': '31.599', 'grad_norm': '28.125', 'counters/examples': 187232, 'counters/updates': 5851}
skipping logging after 187264 examples to avoid logging too frequently
train stats after 187296 examples: {'rewards_train/chosen': '0.11423', 'rewards_train/rejected': '0.042715', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07152', 'logps_train/rejected': '-106.82', 'logps_train/chosen': '-143.87', 'loss/train': '0.6667', 'examples_per_second': '35.773', 'grad_norm': '26.875', 'counters/examples': 187296, 'counters/updates': 5853}
train stats after 187328 examples: {'rewards_train/chosen': '0.14106', 'rewards_train/rejected': '-0.053586', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19465', 'logps_train/rejected': '-109.85', 'logps_train/chosen': '-134.48', 'loss/train': '0.60846', 'examples_per_second': '31.587', 'grad_norm': '22.75', 'counters/examples': 187328, 'counters/updates': 5854}
train stats after 187360 examples: {'rewards_train/chosen': '0.063706', 'rewards_train/rejected': '-0.016793', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080499', 'logps_train/rejected': '-111.2', 'logps_train/chosen': '-138.93', 'loss/train': '0.67173', 'examples_per_second': '31.593', 'grad_norm': '27.75', 'counters/examples': 187360, 'counters/updates': 5855}
skipping logging after 187392 examples to avoid logging too frequently
train stats after 187424 examples: {'rewards_train/chosen': '0.19498', 'rewards_train/rejected': '0.042927', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15205', 'logps_train/rejected': '-138.93', 'logps_train/chosen': '-148.94', 'loss/train': '0.63262', 'examples_per_second': '32.311', 'grad_norm': '27.625', 'counters/examples': 187424, 'counters/updates': 5857}
train stats after 187456 examples: {'rewards_train/chosen': '0.19793', 'rewards_train/rejected': '0.098312', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099618', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-135.34', 'loss/train': '0.66371', 'examples_per_second': '30.101', 'grad_norm': '21.5', 'counters/examples': 187456, 'counters/updates': 5858}
train stats after 187488 examples: {'rewards_train/chosen': '0.13307', 'rewards_train/rejected': '0.082321', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05075', 'logps_train/rejected': '-101.93', 'logps_train/chosen': '-131.28', 'loss/train': '0.67937', 'examples_per_second': '30.477', 'grad_norm': '23.875', 'counters/examples': 187488, 'counters/updates': 5859}
train stats after 187520 examples: {'rewards_train/chosen': '0.20574', 'rewards_train/rejected': '0.14166', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064084', 'logps_train/rejected': '-195.58', 'logps_train/chosen': '-162.25', 'loss/train': '0.6863', 'examples_per_second': '31.419', 'grad_norm': '41', 'counters/examples': 187520, 'counters/updates': 5860}
train stats after 187552 examples: {'rewards_train/chosen': '0.18223', 'rewards_train/rejected': '0.098002', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084227', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-138.97', 'loss/train': '0.6668', 'examples_per_second': '30.143', 'grad_norm': '29.625', 'counters/examples': 187552, 'counters/updates': 5861}
train stats after 187584 examples: {'rewards_train/chosen': '0.1218', 'rewards_train/rejected': '0.071568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050232', 'logps_train/rejected': '-128.84', 'logps_train/chosen': '-125.74', 'loss/train': '0.68288', 'examples_per_second': '32.495', 'grad_norm': '25.75', 'counters/examples': 187584, 'counters/updates': 5862}
train stats after 187616 examples: {'rewards_train/chosen': '0.086341', 'rewards_train/rejected': '0.030615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055725', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-136.98', 'loss/train': '0.677', 'examples_per_second': '31.473', 'grad_norm': '31.625', 'counters/examples': 187616, 'counters/updates': 5863}
skipping logging after 187648 examples to avoid logging too frequently
train stats after 187680 examples: {'rewards_train/chosen': '0.036902', 'rewards_train/rejected': '-0.036928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073831', 'logps_train/rejected': '-123.6', 'logps_train/chosen': '-152.66', 'loss/train': '0.66933', 'examples_per_second': '32.482', 'grad_norm': '28.625', 'counters/examples': 187680, 'counters/updates': 5865}
train stats after 187712 examples: {'rewards_train/chosen': '0.051558', 'rewards_train/rejected': '0.10651', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.05495', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-124.1', 'loss/train': '0.73367', 'examples_per_second': '31.547', 'grad_norm': '46', 'counters/examples': 187712, 'counters/updates': 5866}
train stats after 187744 examples: {'rewards_train/chosen': '0.21414', 'rewards_train/rejected': '0.023172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19097', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-135.58', 'loss/train': '0.61342', 'examples_per_second': '30.733', 'grad_norm': '24', 'counters/examples': 187744, 'counters/updates': 5867}
train stats after 187776 examples: {'rewards_train/chosen': '0.22537', 'rewards_train/rejected': '0.10637', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.119', 'logps_train/rejected': '-138.16', 'logps_train/chosen': '-138.61', 'loss/train': '0.6547', 'examples_per_second': '30.417', 'grad_norm': '34.5', 'counters/examples': 187776, 'counters/updates': 5868}
train stats after 187808 examples: {'rewards_train/chosen': '0.12437', 'rewards_train/rejected': '0.021931', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10244', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-131.67', 'loss/train': '0.65561', 'examples_per_second': '31.586', 'grad_norm': '26.125', 'counters/examples': 187808, 'counters/updates': 5869}
train stats after 187840 examples: {'rewards_train/chosen': '0.12225', 'rewards_train/rejected': '-0.0058668', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12812', 'logps_train/rejected': '-99.087', 'logps_train/chosen': '-130.89', 'loss/train': '0.63775', 'examples_per_second': '30.237', 'grad_norm': '22.125', 'counters/examples': 187840, 'counters/updates': 5870}
train stats after 187872 examples: {'rewards_train/chosen': '0.18624', 'rewards_train/rejected': '0.072093', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11415', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-171.27', 'loss/train': '0.64806', 'examples_per_second': '31.503', 'grad_norm': '26.375', 'counters/examples': 187872, 'counters/updates': 5871}
train stats after 187904 examples: {'rewards_train/chosen': '0.092967', 'rewards_train/rejected': '0.017943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075024', 'logps_train/rejected': '-149.33', 'logps_train/chosen': '-143.33', 'loss/train': '0.67093', 'examples_per_second': '30.886', 'grad_norm': '26.375', 'counters/examples': 187904, 'counters/updates': 5872}
train stats after 187936 examples: {'rewards_train/chosen': '0.17061', 'rewards_train/rejected': '0.019498', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15112', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-137.11', 'loss/train': '0.63879', 'examples_per_second': '30.027', 'grad_norm': '24.75', 'counters/examples': 187936, 'counters/updates': 5873}
train stats after 187968 examples: {'rewards_train/chosen': '0.10771', 'rewards_train/rejected': '-0.014142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12185', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-136.79', 'loss/train': '0.6529', 'examples_per_second': '32.619', 'grad_norm': '26.125', 'counters/examples': 187968, 'counters/updates': 5874}
train stats after 188000 examples: {'rewards_train/chosen': '0.26484', 'rewards_train/rejected': '0.0352', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22964', 'logps_train/rejected': '-120.94', 'logps_train/chosen': '-134.47', 'loss/train': '0.59881', 'examples_per_second': '31.395', 'grad_norm': '25.75', 'counters/examples': 188000, 'counters/updates': 5875}
train stats after 188032 examples: {'rewards_train/chosen': '0.20666', 'rewards_train/rejected': '0.070069', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13659', 'logps_train/rejected': '-152.61', 'logps_train/chosen': '-174.58', 'loss/train': '0.64212', 'examples_per_second': '30.586', 'grad_norm': '29.25', 'counters/examples': 188032, 'counters/updates': 5876}
skipping logging after 188064 examples to avoid logging too frequently
train stats after 188096 examples: {'rewards_train/chosen': '0.11604', 'rewards_train/rejected': '0.044966', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071072', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-111.39', 'loss/train': '0.68724', 'examples_per_second': '38.245', 'grad_norm': '26.875', 'counters/examples': 188096, 'counters/updates': 5878}
train stats after 188128 examples: {'rewards_train/chosen': '0.16712', 'rewards_train/rejected': '0.066775', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10035', 'logps_train/rejected': '-152.85', 'logps_train/chosen': '-171.8', 'loss/train': '0.66698', 'examples_per_second': '31.533', 'grad_norm': '37.25', 'counters/examples': 188128, 'counters/updates': 5879}
train stats after 188160 examples: {'rewards_train/chosen': '0.054658', 'rewards_train/rejected': '-0.045519', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10018', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-132.98', 'loss/train': '0.65619', 'examples_per_second': '30.106', 'grad_norm': '25.875', 'counters/examples': 188160, 'counters/updates': 5880}
train stats after 188192 examples: {'rewards_train/chosen': '0.18062', 'rewards_train/rejected': '0.035516', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14511', 'logps_train/rejected': '-103.55', 'logps_train/chosen': '-138.57', 'loss/train': '0.62928', 'examples_per_second': '24.548', 'grad_norm': '23.625', 'counters/examples': 188192, 'counters/updates': 5881}
skipping logging after 188224 examples to avoid logging too frequently
train stats after 188256 examples: {'rewards_train/chosen': '0.15717', 'rewards_train/rejected': '0.019286', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13788', 'logps_train/rejected': '-112.31', 'logps_train/chosen': '-116.05', 'loss/train': '0.64098', 'examples_per_second': '31.326', 'grad_norm': '28.125', 'counters/examples': 188256, 'counters/updates': 5883}
skipping logging after 188288 examples to avoid logging too frequently
train stats after 188320 examples: {'rewards_train/chosen': '0.10405', 'rewards_train/rejected': '0.060835', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043213', 'logps_train/rejected': '-137.14', 'logps_train/chosen': '-151.22', 'loss/train': '0.68895', 'examples_per_second': '31.499', 'grad_norm': '28.375', 'counters/examples': 188320, 'counters/updates': 5885}
skipping logging after 188352 examples to avoid logging too frequently
train stats after 188384 examples: {'rewards_train/chosen': '0.11931', 'rewards_train/rejected': '0.10769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01162', 'logps_train/rejected': '-131.1', 'logps_train/chosen': '-122.01', 'loss/train': '0.70138', 'examples_per_second': '31.337', 'grad_norm': '24.75', 'counters/examples': 188384, 'counters/updates': 5887}
train stats after 188416 examples: {'rewards_train/chosen': '0.078151', 'rewards_train/rejected': '0.016103', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062048', 'logps_train/rejected': '-116.16', 'logps_train/chosen': '-103.49', 'loss/train': '0.67279', 'examples_per_second': '31.601', 'grad_norm': '21.75', 'counters/examples': 188416, 'counters/updates': 5888}
train stats after 188448 examples: {'rewards_train/chosen': '0.098661', 'rewards_train/rejected': '0.047211', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05145', 'logps_train/rejected': '-109.06', 'logps_train/chosen': '-101.61', 'loss/train': '0.67583', 'examples_per_second': '31.544', 'grad_norm': '23.5', 'counters/examples': 188448, 'counters/updates': 5889}
train stats after 188480 examples: {'rewards_train/chosen': '0.12931', 'rewards_train/rejected': '-0.010455', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13977', 'logps_train/rejected': '-126.25', 'logps_train/chosen': '-146.44', 'loss/train': '0.64901', 'examples_per_second': '30.636', 'grad_norm': '27.375', 'counters/examples': 188480, 'counters/updates': 5890}
train stats after 188512 examples: {'rewards_train/chosen': '0.17264', 'rewards_train/rejected': '0.095509', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077136', 'logps_train/rejected': '-151.41', 'logps_train/chosen': '-168.55', 'loss/train': '0.67356', 'examples_per_second': '31.594', 'grad_norm': '27.125', 'counters/examples': 188512, 'counters/updates': 5891}
train stats after 188544 examples: {'rewards_train/chosen': '0.069052', 'rewards_train/rejected': '0.055968', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013085', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-154.52', 'loss/train': '0.69776', 'examples_per_second': '31.438', 'grad_norm': '38.25', 'counters/examples': 188544, 'counters/updates': 5892}
skipping logging after 188576 examples to avoid logging too frequently
train stats after 188608 examples: {'rewards_train/chosen': '0.2281', 'rewards_train/rejected': '0.04818', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17992', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-139.12', 'loss/train': '0.62358', 'examples_per_second': '31.137', 'grad_norm': '31.375', 'counters/examples': 188608, 'counters/updates': 5894}
train stats after 188640 examples: {'rewards_train/chosen': '0.11914', 'rewards_train/rejected': '0.061016', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058127', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-129.1', 'loss/train': '0.67045', 'examples_per_second': '31.52', 'grad_norm': '24.875', 'counters/examples': 188640, 'counters/updates': 5895}
train stats after 188672 examples: {'rewards_train/chosen': '0.1767', 'rewards_train/rejected': '-0.0018999', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1786', 'logps_train/rejected': '-103.87', 'logps_train/chosen': '-145.74', 'loss/train': '0.6189', 'examples_per_second': '32.216', 'grad_norm': '22.625', 'counters/examples': 188672, 'counters/updates': 5896}
train stats after 188704 examples: {'rewards_train/chosen': '0.2147', 'rewards_train/rejected': '0.079757', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13495', 'logps_train/rejected': '-106.49', 'logps_train/chosen': '-137.91', 'loss/train': '0.64413', 'examples_per_second': '30.216', 'grad_norm': '22.75', 'counters/examples': 188704, 'counters/updates': 5897}
train stats after 188736 examples: {'rewards_train/chosen': '0.1317', 'rewards_train/rejected': '0.0093034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12239', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-113.31', 'loss/train': '0.64459', 'examples_per_second': '30.904', 'grad_norm': '23.625', 'counters/examples': 188736, 'counters/updates': 5898}
skipping logging after 188768 examples to avoid logging too frequently
train stats after 188800 examples: {'rewards_train/chosen': '0.2124', 'rewards_train/rejected': '0.089023', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12338', 'logps_train/rejected': '-97.372', 'logps_train/chosen': '-141.72', 'loss/train': '0.64875', 'examples_per_second': '34.485', 'grad_norm': '26.25', 'counters/examples': 188800, 'counters/updates': 5900}
skipping logging after 188832 examples to avoid logging too frequently
train stats after 188864 examples: {'rewards_train/chosen': '0.17373', 'rewards_train/rejected': '-0.033523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20725', 'logps_train/rejected': '-89.29', 'logps_train/chosen': '-144.39', 'loss/train': '0.60873', 'examples_per_second': '32.264', 'grad_norm': '25.125', 'counters/examples': 188864, 'counters/updates': 5902}
train stats after 188896 examples: {'rewards_train/chosen': '0.079949', 'rewards_train/rejected': '0.028499', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05145', 'logps_train/rejected': '-89.965', 'logps_train/chosen': '-117.81', 'loss/train': '0.67512', 'examples_per_second': '31.847', 'grad_norm': '22.875', 'counters/examples': 188896, 'counters/updates': 5903}
train stats after 188928 examples: {'rewards_train/chosen': '0.17898', 'rewards_train/rejected': '0.033641', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14534', 'logps_train/rejected': '-142.12', 'logps_train/chosen': '-170.45', 'loss/train': '0.64133', 'examples_per_second': '32.444', 'grad_norm': '29', 'counters/examples': 188928, 'counters/updates': 5904}
train stats after 188960 examples: {'rewards_train/chosen': '0.14514', 'rewards_train/rejected': '-0.01604', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16118', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-123.44', 'loss/train': '0.62544', 'examples_per_second': '30.463', 'grad_norm': '22.625', 'counters/examples': 188960, 'counters/updates': 5905}
train stats after 188992 examples: {'rewards_train/chosen': '0.13166', 'rewards_train/rejected': '0.039705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091955', 'logps_train/rejected': '-132.12', 'logps_train/chosen': '-180.69', 'loss/train': '0.65934', 'examples_per_second': '30.018', 'grad_norm': '28.875', 'counters/examples': 188992, 'counters/updates': 5906}
train stats after 189024 examples: {'rewards_train/chosen': '0.1526', 'rewards_train/rejected': '0.026892', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12571', 'logps_train/rejected': '-151.2', 'logps_train/chosen': '-159.25', 'loss/train': '0.64703', 'examples_per_second': '30.241', 'grad_norm': '33.75', 'counters/examples': 189024, 'counters/updates': 5907}
train stats after 189056 examples: {'rewards_train/chosen': '0.16485', 'rewards_train/rejected': '0.10775', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0571', 'logps_train/rejected': '-125.18', 'logps_train/chosen': '-129.03', 'loss/train': '0.67768', 'examples_per_second': '30.069', 'grad_norm': '25.875', 'counters/examples': 189056, 'counters/updates': 5908}
train stats after 189088 examples: {'rewards_train/chosen': '0.079352', 'rewards_train/rejected': '0.055438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023914', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-149.56', 'loss/train': '0.69165', 'examples_per_second': '32.622', 'grad_norm': '29', 'counters/examples': 189088, 'counters/updates': 5909}
train stats after 189120 examples: {'rewards_train/chosen': '0.076824', 'rewards_train/rejected': '0.032235', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044589', 'logps_train/rejected': '-90.661', 'logps_train/chosen': '-86.795', 'loss/train': '0.67923', 'examples_per_second': '32.081', 'grad_norm': '19.875', 'counters/examples': 189120, 'counters/updates': 5910}
train stats after 189152 examples: {'rewards_train/chosen': '0.16281', 'rewards_train/rejected': '0.060511', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1023', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-132.9', 'loss/train': '0.65586', 'examples_per_second': '31.541', 'grad_norm': '24.5', 'counters/examples': 189152, 'counters/updates': 5911}
train stats after 189184 examples: {'rewards_train/chosen': '0.11184', 'rewards_train/rejected': '0.056596', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05524', 'logps_train/rejected': '-112.67', 'logps_train/chosen': '-117.85', 'loss/train': '0.67967', 'examples_per_second': '31.533', 'grad_norm': '35', 'counters/examples': 189184, 'counters/updates': 5912}
train stats after 189216 examples: {'rewards_train/chosen': '0.14772', 'rewards_train/rejected': '0.023199', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12452', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-158.89', 'loss/train': '0.64854', 'examples_per_second': '31.551', 'grad_norm': '24.875', 'counters/examples': 189216, 'counters/updates': 5913}
train stats after 189248 examples: {'rewards_train/chosen': '0.063243', 'rewards_train/rejected': '0.032255', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030987', 'logps_train/rejected': '-134.84', 'logps_train/chosen': '-132.09', 'loss/train': '0.69169', 'examples_per_second': '32.19', 'grad_norm': '27.25', 'counters/examples': 189248, 'counters/updates': 5914}
train stats after 189280 examples: {'rewards_train/chosen': '0.17772', 'rewards_train/rejected': '0.077732', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099991', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-140.23', 'loss/train': '0.66291', 'examples_per_second': '31.523', 'grad_norm': '27.5', 'counters/examples': 189280, 'counters/updates': 5915}
train stats after 189312 examples: {'rewards_train/chosen': '0.21357', 'rewards_train/rejected': '0.076448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13712', 'logps_train/rejected': '-178.39', 'logps_train/chosen': '-123.93', 'loss/train': '0.64706', 'examples_per_second': '31.49', 'grad_norm': '28.25', 'counters/examples': 189312, 'counters/updates': 5916}
skipping logging after 189344 examples to avoid logging too frequently
train stats after 189376 examples: {'rewards_train/chosen': '0.17142', 'rewards_train/rejected': '0.025506', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14591', 'logps_train/rejected': '-103.26', 'logps_train/chosen': '-131.65', 'loss/train': '0.63903', 'examples_per_second': '35.827', 'grad_norm': '22.25', 'counters/examples': 189376, 'counters/updates': 5918}
skipping logging after 189408 examples to avoid logging too frequently
train stats after 189440 examples: {'rewards_train/chosen': '0.096718', 'rewards_train/rejected': '-0.044303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14102', 'logps_train/rejected': '-97.661', 'logps_train/chosen': '-128.38', 'loss/train': '0.63791', 'examples_per_second': '36.321', 'grad_norm': '21.375', 'counters/examples': 189440, 'counters/updates': 5920}
train stats after 189472 examples: {'rewards_train/chosen': '0.15518', 'rewards_train/rejected': '0.037022', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11815', 'logps_train/rejected': '-127.81', 'logps_train/chosen': '-166.44', 'loss/train': '0.64331', 'examples_per_second': '31.532', 'grad_norm': '25.625', 'counters/examples': 189472, 'counters/updates': 5921}
train stats after 189504 examples: {'rewards_train/chosen': '0.10217', 'rewards_train/rejected': '0.064464', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03771', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-129.53', 'loss/train': '0.68649', 'examples_per_second': '31.46', 'grad_norm': '27.75', 'counters/examples': 189504, 'counters/updates': 5922}
skipping logging after 189536 examples to avoid logging too frequently
train stats after 189568 examples: {'rewards_train/chosen': '0.17503', 'rewards_train/rejected': '0.058511', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11652', 'logps_train/rejected': '-130.97', 'logps_train/chosen': '-157.26', 'loss/train': '0.64684', 'examples_per_second': '33.314', 'grad_norm': '27.125', 'counters/examples': 189568, 'counters/updates': 5924}
train stats after 189600 examples: {'rewards_train/chosen': '0.26607', 'rewards_train/rejected': '0.082389', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18368', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-199.66', 'loss/train': '0.62089', 'examples_per_second': '30.174', 'grad_norm': '31.25', 'counters/examples': 189600, 'counters/updates': 5925}
train stats after 189632 examples: {'rewards_train/chosen': '0.22256', 'rewards_train/rejected': '-0.044394', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26695', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-150.15', 'loss/train': '0.58648', 'examples_per_second': '32.065', 'grad_norm': '24.125', 'counters/examples': 189632, 'counters/updates': 5926}
train stats after 189664 examples: {'rewards_train/chosen': '0.14765', 'rewards_train/rejected': '0.066941', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080713', 'logps_train/rejected': '-127.02', 'logps_train/chosen': '-135.76', 'loss/train': '0.66385', 'examples_per_second': '31.109', 'grad_norm': '25.75', 'counters/examples': 189664, 'counters/updates': 5927}
train stats after 189696 examples: {'rewards_train/chosen': '0.12986', 'rewards_train/rejected': '0.046013', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083845', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-155.72', 'loss/train': '0.66257', 'examples_per_second': '31.057', 'grad_norm': '28.25', 'counters/examples': 189696, 'counters/updates': 5928}
train stats after 189728 examples: {'rewards_train/chosen': '0.082688', 'rewards_train/rejected': '-0.049529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13222', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-125.13', 'loss/train': '0.6446', 'examples_per_second': '29.996', 'grad_norm': '24.875', 'counters/examples': 189728, 'counters/updates': 5929}
train stats after 189760 examples: {'rewards_train/chosen': '0.12582', 'rewards_train/rejected': '-0.010665', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13649', 'logps_train/rejected': '-109.96', 'logps_train/chosen': '-115.79', 'loss/train': '0.64027', 'examples_per_second': '30.615', 'grad_norm': '25.25', 'counters/examples': 189760, 'counters/updates': 5930}
train stats after 189792 examples: {'rewards_train/chosen': '0.101', 'rewards_train/rejected': '0.073298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027706', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-107.4', 'loss/train': '0.68868', 'examples_per_second': '31.023', 'grad_norm': '25.875', 'counters/examples': 189792, 'counters/updates': 5931}
train stats after 189824 examples: {'rewards_train/chosen': '0.10228', 'rewards_train/rejected': '0.021178', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081102', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-160.33', 'loss/train': '0.67121', 'examples_per_second': '31.992', 'grad_norm': '30.375', 'counters/examples': 189824, 'counters/updates': 5932}
train stats after 189856 examples: {'rewards_train/chosen': '0.15871', 'rewards_train/rejected': '0.0096321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14908', 'logps_train/rejected': '-148.31', 'logps_train/chosen': '-112.44', 'loss/train': '0.63325', 'examples_per_second': '30.441', 'grad_norm': '28', 'counters/examples': 189856, 'counters/updates': 5933}
train stats after 189888 examples: {'rewards_train/chosen': '0.1404', 'rewards_train/rejected': '-0.014642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15505', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-135.99', 'loss/train': '0.63204', 'examples_per_second': '31.499', 'grad_norm': '21.75', 'counters/examples': 189888, 'counters/updates': 5934}
train stats after 189920 examples: {'rewards_train/chosen': '0.14145', 'rewards_train/rejected': '0.053228', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088223', 'logps_train/rejected': '-159.88', 'logps_train/chosen': '-148.81', 'loss/train': '0.6644', 'examples_per_second': '32.046', 'grad_norm': '27.75', 'counters/examples': 189920, 'counters/updates': 5935}
train stats after 189952 examples: {'rewards_train/chosen': '0.14854', 'rewards_train/rejected': '0.055873', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092668', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-164.4', 'loss/train': '0.66011', 'examples_per_second': '31.25', 'grad_norm': '31.5', 'counters/examples': 189952, 'counters/updates': 5936}
train stats after 189984 examples: {'rewards_train/chosen': '0.090369', 'rewards_train/rejected': '0.022928', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067441', 'logps_train/rejected': '-128.29', 'logps_train/chosen': '-147.17', 'loss/train': '0.67406', 'examples_per_second': '31.333', 'grad_norm': '25.25', 'counters/examples': 189984, 'counters/updates': 5937}
train stats after 190016 examples: {'rewards_train/chosen': '0.10305', 'rewards_train/rejected': '-0.0069733', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11002', 'logps_train/rejected': '-105.61', 'logps_train/chosen': '-163.45', 'loss/train': '0.64889', 'examples_per_second': '31.505', 'grad_norm': '29.125', 'counters/examples': 190016, 'counters/updates': 5938}
train stats after 190048 examples: {'rewards_train/chosen': '0.16643', 'rewards_train/rejected': '0.063826', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1026', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-115.65', 'loss/train': '0.65216', 'examples_per_second': '31.284', 'grad_norm': '23.125', 'counters/examples': 190048, 'counters/updates': 5939}
train stats after 190080 examples: {'rewards_train/chosen': '0.20316', 'rewards_train/rejected': '0.1326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070568', 'logps_train/rejected': '-166.31', 'logps_train/chosen': '-176.11', 'loss/train': '0.66891', 'examples_per_second': '31.542', 'grad_norm': '30.625', 'counters/examples': 190080, 'counters/updates': 5940}
train stats after 190112 examples: {'rewards_train/chosen': '0.098648', 'rewards_train/rejected': '0.041368', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057279', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-123.23', 'loss/train': '0.67626', 'examples_per_second': '30.559', 'grad_norm': '26.125', 'counters/examples': 190112, 'counters/updates': 5941}
train stats after 190144 examples: {'rewards_train/chosen': '0.14639', 'rewards_train/rejected': '0.062312', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084074', 'logps_train/rejected': '-103.62', 'logps_train/chosen': '-131.46', 'loss/train': '0.66532', 'examples_per_second': '33.014', 'grad_norm': '25', 'counters/examples': 190144, 'counters/updates': 5942}
skipping logging after 190176 examples to avoid logging too frequently
train stats after 190208 examples: {'rewards_train/chosen': '0.22701', 'rewards_train/rejected': '0.037988', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18902', 'logps_train/rejected': '-127.6', 'logps_train/chosen': '-130.5', 'loss/train': '0.61444', 'examples_per_second': '29.947', 'grad_norm': '22.875', 'counters/examples': 190208, 'counters/updates': 5944}
train stats after 190240 examples: {'rewards_train/chosen': '0.13631', 'rewards_train/rejected': '0.10855', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027763', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-138.44', 'loss/train': '0.7039', 'examples_per_second': '24.231', 'grad_norm': '28.625', 'counters/examples': 190240, 'counters/updates': 5945}
train stats after 190272 examples: {'rewards_train/chosen': '0.097733', 'rewards_train/rejected': '-0.038954', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13669', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-118.58', 'loss/train': '0.63762', 'examples_per_second': '31.048', 'grad_norm': '23.25', 'counters/examples': 190272, 'counters/updates': 5946}
train stats after 190304 examples: {'rewards_train/chosen': '0.094515', 'rewards_train/rejected': '0.021434', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073081', 'logps_train/rejected': '-152.41', 'logps_train/chosen': '-153.4', 'loss/train': '0.6717', 'examples_per_second': '30.945', 'grad_norm': '28.75', 'counters/examples': 190304, 'counters/updates': 5947}
train stats after 190336 examples: {'rewards_train/chosen': '0.10529', 'rewards_train/rejected': '0.027728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077558', 'logps_train/rejected': '-136.65', 'logps_train/chosen': '-139.16', 'loss/train': '0.67493', 'examples_per_second': '23.845', 'grad_norm': '27', 'counters/examples': 190336, 'counters/updates': 5948}
train stats after 190368 examples: {'rewards_train/chosen': '0.14712', 'rewards_train/rejected': '0.046692', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10043', 'logps_train/rejected': '-104.53', 'logps_train/chosen': '-126.32', 'loss/train': '0.65591', 'examples_per_second': '32.325', 'grad_norm': '23.75', 'counters/examples': 190368, 'counters/updates': 5949}
train stats after 190400 examples: {'rewards_train/chosen': '0.15048', 'rewards_train/rejected': '0.078291', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072188', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-171.89', 'loss/train': '0.66691', 'examples_per_second': '31.086', 'grad_norm': '29', 'counters/examples': 190400, 'counters/updates': 5950}
train stats after 190432 examples: {'rewards_train/chosen': '0.098832', 'rewards_train/rejected': '0.024871', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073961', 'logps_train/rejected': '-107.97', 'logps_train/chosen': '-119.28', 'loss/train': '0.6622', 'examples_per_second': '32.967', 'grad_norm': '24.25', 'counters/examples': 190432, 'counters/updates': 5951}
train stats after 190464 examples: {'rewards_train/chosen': '0.20689', 'rewards_train/rejected': '-0.015949', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22284', 'logps_train/rejected': '-98.24', 'logps_train/chosen': '-170.58', 'loss/train': '0.60894', 'examples_per_second': '30.431', 'grad_norm': '24.375', 'counters/examples': 190464, 'counters/updates': 5952}
train stats after 190496 examples: {'rewards_train/chosen': '0.15109', 'rewards_train/rejected': '0.024969', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12612', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-125.84', 'loss/train': '0.64236', 'examples_per_second': '30.03', 'grad_norm': '25.5', 'counters/examples': 190496, 'counters/updates': 5953}
train stats after 190528 examples: {'rewards_train/chosen': '0.10035', 'rewards_train/rejected': '0.031048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0693', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-110.71', 'loss/train': '0.66508', 'examples_per_second': '32.572', 'grad_norm': '23.5', 'counters/examples': 190528, 'counters/updates': 5954}
train stats after 190560 examples: {'rewards_train/chosen': '0.0040999', 'rewards_train/rejected': '-0.083836', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087936', 'logps_train/rejected': '-175.13', 'logps_train/chosen': '-148.8', 'loss/train': '0.66478', 'examples_per_second': '30.596', 'grad_norm': '28', 'counters/examples': 190560, 'counters/updates': 5955}
train stats after 190592 examples: {'rewards_train/chosen': '0.12694', 'rewards_train/rejected': '-0.055725', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18266', 'logps_train/rejected': '-95.895', 'logps_train/chosen': '-132.37', 'loss/train': '0.61737', 'examples_per_second': '31.768', 'grad_norm': '23.875', 'counters/examples': 190592, 'counters/updates': 5956}
train stats after 190624 examples: {'rewards_train/chosen': '0.12987', 'rewards_train/rejected': '0.048786', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081086', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-170.71', 'loss/train': '0.67932', 'examples_per_second': '31.777', 'grad_norm': '31', 'counters/examples': 190624, 'counters/updates': 5957}
train stats after 190656 examples: {'rewards_train/chosen': '0.081967', 'rewards_train/rejected': '-0.0067029', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08867', 'logps_train/rejected': '-116.5', 'logps_train/chosen': '-164.1', 'loss/train': '0.65677', 'examples_per_second': '32.613', 'grad_norm': '28.75', 'counters/examples': 190656, 'counters/updates': 5958}
train stats after 190688 examples: {'rewards_train/chosen': '0.17771', 'rewards_train/rejected': '0.052534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12517', 'logps_train/rejected': '-132.43', 'logps_train/chosen': '-132.93', 'loss/train': '0.65075', 'examples_per_second': '31.359', 'grad_norm': '25.25', 'counters/examples': 190688, 'counters/updates': 5959}
train stats after 190720 examples: {'rewards_train/chosen': '0.14805', 'rewards_train/rejected': '0.010461', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13758', 'logps_train/rejected': '-96.759', 'logps_train/chosen': '-152.6', 'loss/train': '0.63764', 'examples_per_second': '30.579', 'grad_norm': '24.625', 'counters/examples': 190720, 'counters/updates': 5960}
train stats after 190752 examples: {'rewards_train/chosen': '0.13992', 'rewards_train/rejected': '0.0073592', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13256', 'logps_train/rejected': '-147.54', 'logps_train/chosen': '-182.76', 'loss/train': '0.64513', 'examples_per_second': '31.483', 'grad_norm': '28', 'counters/examples': 190752, 'counters/updates': 5961}
train stats after 190784 examples: {'rewards_train/chosen': '0.14126', 'rewards_train/rejected': '0.082888', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058369', 'logps_train/rejected': '-138.53', 'logps_train/chosen': '-179.97', 'loss/train': '0.68365', 'examples_per_second': '30.951', 'grad_norm': '33', 'counters/examples': 190784, 'counters/updates': 5962}
train stats after 190816 examples: {'rewards_train/chosen': '0.040021', 'rewards_train/rejected': '0.030448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.009573', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-153.47', 'loss/train': '0.69653', 'examples_per_second': '31.428', 'grad_norm': '28.75', 'counters/examples': 190816, 'counters/updates': 5963}
train stats after 190848 examples: {'rewards_train/chosen': '0.12174', 'rewards_train/rejected': '0.025476', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096268', 'logps_train/rejected': '-135.31', 'logps_train/chosen': '-144.64', 'loss/train': '0.66816', 'examples_per_second': '32.038', 'grad_norm': '30.5', 'counters/examples': 190848, 'counters/updates': 5964}
train stats after 190880 examples: {'rewards_train/chosen': '0.17308', 'rewards_train/rejected': '0.011274', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16181', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-166.85', 'loss/train': '0.63396', 'examples_per_second': '32.989', 'grad_norm': '25.875', 'counters/examples': 190880, 'counters/updates': 5965}
train stats after 190912 examples: {'rewards_train/chosen': '0.13724', 'rewards_train/rejected': '0.071362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065874', 'logps_train/rejected': '-116.57', 'logps_train/chosen': '-118.94', 'loss/train': '0.66962', 'examples_per_second': '30.417', 'grad_norm': '24.75', 'counters/examples': 190912, 'counters/updates': 5966}
skipping logging after 190944 examples to avoid logging too frequently
train stats after 190976 examples: {'rewards_train/chosen': '0.09396', 'rewards_train/rejected': '0.070662', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023298', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-130.82', 'loss/train': '0.69109', 'examples_per_second': '31.493', 'grad_norm': '24.25', 'counters/examples': 190976, 'counters/updates': 5968}
train stats after 191008 examples: {'rewards_train/chosen': '0.20585', 'rewards_train/rejected': '0.069096', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13675', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-192.3', 'loss/train': '0.64537', 'examples_per_second': '30.742', 'grad_norm': '27.75', 'counters/examples': 191008, 'counters/updates': 5969}
train stats after 191040 examples: {'rewards_train/chosen': '0.12372', 'rewards_train/rejected': '0.041985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081738', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-155.72', 'loss/train': '0.6694', 'examples_per_second': '31.387', 'grad_norm': '27.125', 'counters/examples': 191040, 'counters/updates': 5970}
train stats after 191072 examples: {'rewards_train/chosen': '0.042634', 'rewards_train/rejected': '0.044659', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0020248', 'logps_train/rejected': '-117.18', 'logps_train/chosen': '-93.063', 'loss/train': '0.70718', 'examples_per_second': '31.658', 'grad_norm': '26', 'counters/examples': 191072, 'counters/updates': 5971}
train stats after 191104 examples: {'rewards_train/chosen': '0.096446', 'rewards_train/rejected': '-0.00055403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097', 'logps_train/rejected': '-130.73', 'logps_train/chosen': '-146.4', 'loss/train': '0.66244', 'examples_per_second': '30.186', 'grad_norm': '27.125', 'counters/examples': 191104, 'counters/updates': 5972}
train stats after 191136 examples: {'rewards_train/chosen': '0.16692', 'rewards_train/rejected': '0.015057', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15186', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-142.77', 'loss/train': '0.63961', 'examples_per_second': '31.408', 'grad_norm': '31.25', 'counters/examples': 191136, 'counters/updates': 5973}
skipping logging after 191168 examples to avoid logging too frequently
train stats after 191200 examples: {'rewards_train/chosen': '0.12159', 'rewards_train/rejected': '0.066148', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055446', 'logps_train/rejected': '-147.45', 'logps_train/chosen': '-139.9', 'loss/train': '0.6765', 'examples_per_second': '31.527', 'grad_norm': '26', 'counters/examples': 191200, 'counters/updates': 5975}
train stats after 191232 examples: {'rewards_train/chosen': '0.19539', 'rewards_train/rejected': '0.062687', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1327', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-173.73', 'loss/train': '0.6387', 'examples_per_second': '30.15', 'grad_norm': '47.5', 'counters/examples': 191232, 'counters/updates': 5976}
train stats after 191264 examples: {'rewards_train/chosen': '0.16355', 'rewards_train/rejected': '0.040301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12324', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-154.41', 'loss/train': '0.64268', 'examples_per_second': '30.294', 'grad_norm': '25.625', 'counters/examples': 191264, 'counters/updates': 5977}
train stats after 191296 examples: {'rewards_train/chosen': '0.14653', 'rewards_train/rejected': '0.024259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12227', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-140.11', 'loss/train': '0.65058', 'examples_per_second': '31.276', 'grad_norm': '23.25', 'counters/examples': 191296, 'counters/updates': 5978}
train stats after 191328 examples: {'rewards_train/chosen': '0.12908', 'rewards_train/rejected': '0.039346', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089733', 'logps_train/rejected': '-146.74', 'logps_train/chosen': '-134.48', 'loss/train': '0.67141', 'examples_per_second': '30.573', 'grad_norm': '30.25', 'counters/examples': 191328, 'counters/updates': 5979}
train stats after 191360 examples: {'rewards_train/chosen': '0.023312', 'rewards_train/rejected': '-0.011791', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035103', 'logps_train/rejected': '-126.36', 'logps_train/chosen': '-158.61', 'loss/train': '0.68266', 'examples_per_second': '31.386', 'grad_norm': '37.25', 'counters/examples': 191360, 'counters/updates': 5980}
train stats after 191392 examples: {'rewards_train/chosen': '0.23209', 'rewards_train/rejected': '0.074701', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15739', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-146.44', 'loss/train': '0.63587', 'examples_per_second': '31.619', 'grad_norm': '26.5', 'counters/examples': 191392, 'counters/updates': 5981}
train stats after 191424 examples: {'rewards_train/chosen': '0.18693', 'rewards_train/rejected': '0.045199', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14173', 'logps_train/rejected': '-120.94', 'logps_train/chosen': '-139.73', 'loss/train': '0.63854', 'examples_per_second': '30.616', 'grad_norm': '23.25', 'counters/examples': 191424, 'counters/updates': 5982}
train stats after 191456 examples: {'rewards_train/chosen': '0.13224', 'rewards_train/rejected': '-0.045914', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17816', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-140.81', 'loss/train': '0.61587', 'examples_per_second': '30.13', 'grad_norm': '23.625', 'counters/examples': 191456, 'counters/updates': 5983}
train stats after 191488 examples: {'rewards_train/chosen': '0.059586', 'rewards_train/rejected': '-0.0027264', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062312', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-128.05', 'loss/train': '0.67354', 'examples_per_second': '30.079', 'grad_norm': '27.875', 'counters/examples': 191488, 'counters/updates': 5984}
train stats after 191520 examples: {'rewards_train/chosen': '0.13367', 'rewards_train/rejected': '-0.0083903', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14206', 'logps_train/rejected': '-146.44', 'logps_train/chosen': '-159.57', 'loss/train': '0.63942', 'examples_per_second': '31.559', 'grad_norm': '26.375', 'counters/examples': 191520, 'counters/updates': 5985}
train stats after 191552 examples: {'rewards_train/chosen': '0.12719', 'rewards_train/rejected': '0.09305', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.034144', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-132.98', 'loss/train': '0.68935', 'examples_per_second': '31.573', 'grad_norm': '31.75', 'counters/examples': 191552, 'counters/updates': 5986}
train stats after 191584 examples: {'rewards_train/chosen': '0.15056', 'rewards_train/rejected': '0.072009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07855', 'logps_train/rejected': '-148.87', 'logps_train/chosen': '-154.15', 'loss/train': '0.67491', 'examples_per_second': '31.324', 'grad_norm': '26.25', 'counters/examples': 191584, 'counters/updates': 5987}
train stats after 191616 examples: {'rewards_train/chosen': '0.26966', 'rewards_train/rejected': '0.074051', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19561', 'logps_train/rejected': '-144.51', 'logps_train/chosen': '-153.93', 'loss/train': '0.63034', 'examples_per_second': '30.708', 'grad_norm': '26.625', 'counters/examples': 191616, 'counters/updates': 5988}
train stats after 191648 examples: {'rewards_train/chosen': '0.135', 'rewards_train/rejected': '0.020399', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1146', 'logps_train/rejected': '-107.96', 'logps_train/chosen': '-124.53', 'loss/train': '0.64591', 'examples_per_second': '30.997', 'grad_norm': '23.625', 'counters/examples': 191648, 'counters/updates': 5989}
skipping logging after 191680 examples to avoid logging too frequently
train stats after 191712 examples: {'rewards_train/chosen': '0.20079', 'rewards_train/rejected': '0.13507', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065715', 'logps_train/rejected': '-159.32', 'logps_train/chosen': '-174.04', 'loss/train': '0.67707', 'examples_per_second': '31.536', 'grad_norm': '28', 'counters/examples': 191712, 'counters/updates': 5991}
train stats after 191744 examples: {'rewards_train/chosen': '0.16834', 'rewards_train/rejected': '0.052511', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11583', 'logps_train/rejected': '-133.02', 'logps_train/chosen': '-156.07', 'loss/train': '0.66039', 'examples_per_second': '30.753', 'grad_norm': '26.875', 'counters/examples': 191744, 'counters/updates': 5992}
skipping logging after 191776 examples to avoid logging too frequently
train stats after 191808 examples: {'rewards_train/chosen': '0.098627', 'rewards_train/rejected': '0.020134', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078493', 'logps_train/rejected': '-80.175', 'logps_train/chosen': '-109.93', 'loss/train': '0.67143', 'examples_per_second': '31.533', 'grad_norm': '21.75', 'counters/examples': 191808, 'counters/updates': 5994}
skipping logging after 191840 examples to avoid logging too frequently
train stats after 191872 examples: {'rewards_train/chosen': '0.1292', 'rewards_train/rejected': '0.088607', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040596', 'logps_train/rejected': '-156.8', 'logps_train/chosen': '-158.61', 'loss/train': '0.69882', 'examples_per_second': '30.195', 'grad_norm': '29.75', 'counters/examples': 191872, 'counters/updates': 5996}
skipping logging after 191904 examples to avoid logging too frequently
train stats after 191936 examples: {'rewards_train/chosen': '0.26511', 'rewards_train/rejected': '0.036641', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22847', 'logps_train/rejected': '-133.45', 'logps_train/chosen': '-165.65', 'loss/train': '0.60835', 'examples_per_second': '31.707', 'grad_norm': '23.625', 'counters/examples': 191936, 'counters/updates': 5998}
train stats after 191968 examples: {'rewards_train/chosen': '0.17183', 'rewards_train/rejected': '0.075682', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096149', 'logps_train/rejected': '-135.42', 'logps_train/chosen': '-141.05', 'loss/train': '0.66299', 'examples_per_second': '30.454', 'grad_norm': '26.875', 'counters/examples': 191968, 'counters/updates': 5999}
train stats after 192000 examples: {'rewards_train/chosen': '0.1978', 'rewards_train/rejected': '0.0020872', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19571', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-147.89', 'loss/train': '0.61331', 'examples_per_second': '31.408', 'grad_norm': '23.75', 'counters/examples': 192000, 'counters/updates': 6000}
Running evaluation after 192000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 192000: {'rewards_eval/chosen': '0.12652', 'rewards_eval/rejected': '0.043379', 'rewards_eval/accuracies': '0.58984', 'rewards_eval/margins': '0.083138', 'logps_eval/rejected': '-118.18', 'logps_eval/chosen': '-138.17', 'loss/eval': '0.66641'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880/step-192000...
writing checkpoint to .cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880/step-192000/policy.pt...
train stats after 192032 examples: {'rewards_train/chosen': '0.12184', 'rewards_train/rejected': '0.04101', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080827', 'logps_train/rejected': '-109.52', 'logps_train/chosen': '-125.35', 'loss/train': '0.66749', 'examples_per_second': '24.41', 'grad_norm': '24.25', 'counters/examples': 192032, 'counters/updates': 6001}
train stats after 192064 examples: {'rewards_train/chosen': '0.25731', 'rewards_train/rejected': '0.0028303', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.25448', 'logps_train/rejected': '-129.47', 'logps_train/chosen': '-160.68', 'loss/train': '0.59502', 'examples_per_second': '30.092', 'grad_norm': '23.625', 'counters/examples': 192064, 'counters/updates': 6002}
train stats after 192096 examples: {'rewards_train/chosen': '0.21336', 'rewards_train/rejected': '0.11138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10198', 'logps_train/rejected': '-141.29', 'logps_train/chosen': '-184.48', 'loss/train': '0.66792', 'examples_per_second': '32.073', 'grad_norm': '32.75', 'counters/examples': 192096, 'counters/updates': 6003}
skipping logging after 192128 examples to avoid logging too frequently
train stats after 192160 examples: {'rewards_train/chosen': '0.174', 'rewards_train/rejected': '-0.019612', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19361', 'logps_train/rejected': '-152.81', 'logps_train/chosen': '-149.08', 'loss/train': '0.62018', 'examples_per_second': '31.501', 'grad_norm': '25.875', 'counters/examples': 192160, 'counters/updates': 6005}
train stats after 192192 examples: {'rewards_train/chosen': '0.1991', 'rewards_train/rejected': '0.040316', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15878', 'logps_train/rejected': '-98.276', 'logps_train/chosen': '-170.66', 'loss/train': '0.63102', 'examples_per_second': '31.558', 'grad_norm': '24.25', 'counters/examples': 192192, 'counters/updates': 6006}
skipping logging after 192224 examples to avoid logging too frequently
train stats after 192256 examples: {'rewards_train/chosen': '0.094113', 'rewards_train/rejected': '0.032517', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061595', 'logps_train/rejected': '-113.48', 'logps_train/chosen': '-129.93', 'loss/train': '0.67673', 'examples_per_second': '31.633', 'grad_norm': '22.5', 'counters/examples': 192256, 'counters/updates': 6008}
train stats after 192288 examples: {'rewards_train/chosen': '0.16623', 'rewards_train/rejected': '0.01945', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14678', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-151.01', 'loss/train': '0.63914', 'examples_per_second': '31.27', 'grad_norm': '24.125', 'counters/examples': 192288, 'counters/updates': 6009}
train stats after 192320 examples: {'rewards_train/chosen': '0.080321', 'rewards_train/rejected': '-0.00086543', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.081186', 'logps_train/rejected': '-150.78', 'logps_train/chosen': '-115.94', 'loss/train': '0.66323', 'examples_per_second': '31.647', 'grad_norm': '25', 'counters/examples': 192320, 'counters/updates': 6010}
skipping logging after 192352 examples to avoid logging too frequently
train stats after 192384 examples: {'rewards_train/chosen': '0.19688', 'rewards_train/rejected': '0.082269', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11461', 'logps_train/rejected': '-94.536', 'logps_train/chosen': '-123.71', 'loss/train': '0.65194', 'examples_per_second': '30.528', 'grad_norm': '45', 'counters/examples': 192384, 'counters/updates': 6012}
train stats after 192416 examples: {'rewards_train/chosen': '0.12789', 'rewards_train/rejected': '0.001969', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12592', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-138.25', 'loss/train': '0.64609', 'examples_per_second': '31.046', 'grad_norm': '31.875', 'counters/examples': 192416, 'counters/updates': 6013}
train stats after 192448 examples: {'rewards_train/chosen': '0.15534', 'rewards_train/rejected': '0.070506', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084838', 'logps_train/rejected': '-119', 'logps_train/chosen': '-124.96', 'loss/train': '0.6642', 'examples_per_second': '32.598', 'grad_norm': '25.375', 'counters/examples': 192448, 'counters/updates': 6014}
train stats after 192480 examples: {'rewards_train/chosen': '0.11334', 'rewards_train/rejected': '0.1446', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031269', 'logps_train/rejected': '-156.63', 'logps_train/chosen': '-161.12', 'loss/train': '0.72601', 'examples_per_second': '30.228', 'grad_norm': '33', 'counters/examples': 192480, 'counters/updates': 6015}
train stats after 192512 examples: {'rewards_train/chosen': '0.099563', 'rewards_train/rejected': '-0.057493', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15706', 'logps_train/rejected': '-160.96', 'logps_train/chosen': '-128.07', 'loss/train': '0.63033', 'examples_per_second': '30.795', 'grad_norm': '28.5', 'counters/examples': 192512, 'counters/updates': 6016}
train stats after 192544 examples: {'rewards_train/chosen': '0.085044', 'rewards_train/rejected': '0.010623', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074421', 'logps_train/rejected': '-144.31', 'logps_train/chosen': '-166.69', 'loss/train': '0.67412', 'examples_per_second': '32.176', 'grad_norm': '28.875', 'counters/examples': 192544, 'counters/updates': 6017}
train stats after 192576 examples: {'rewards_train/chosen': '0.13456', 'rewards_train/rejected': '0.058541', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076021', 'logps_train/rejected': '-98.013', 'logps_train/chosen': '-129.09', 'loss/train': '0.66617', 'examples_per_second': '30.826', 'grad_norm': '23.125', 'counters/examples': 192576, 'counters/updates': 6018}
train stats after 192608 examples: {'rewards_train/chosen': '0.18515', 'rewards_train/rejected': '-0.0095334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19469', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-157.31', 'loss/train': '0.6215', 'examples_per_second': '31.416', 'grad_norm': '31.75', 'counters/examples': 192608, 'counters/updates': 6019}
train stats after 192640 examples: {'rewards_train/chosen': '0.11784', 'rewards_train/rejected': '-0.013155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.131', 'logps_train/rejected': '-168.1', 'logps_train/chosen': '-180.67', 'loss/train': '0.64458', 'examples_per_second': '30.264', 'grad_norm': '29.375', 'counters/examples': 192640, 'counters/updates': 6020}
train stats after 192672 examples: {'rewards_train/chosen': '0.19638', 'rewards_train/rejected': '0.067439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12894', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-158.76', 'loss/train': '0.64245', 'examples_per_second': '30.207', 'grad_norm': '24.5', 'counters/examples': 192672, 'counters/updates': 6021}
train stats after 192704 examples: {'rewards_train/chosen': '0.079025', 'rewards_train/rejected': '-0.034309', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11333', 'logps_train/rejected': '-90.16', 'logps_train/chosen': '-145.96', 'loss/train': '0.64884', 'examples_per_second': '32.543', 'grad_norm': '23.625', 'counters/examples': 192704, 'counters/updates': 6022}
train stats after 192736 examples: {'rewards_train/chosen': '0.15425', 'rewards_train/rejected': '-0.045114', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19936', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-133.53', 'loss/train': '0.61605', 'examples_per_second': '31.389', 'grad_norm': '22.25', 'counters/examples': 192736, 'counters/updates': 6023}
train stats after 192768 examples: {'rewards_train/chosen': '0.22699', 'rewards_train/rejected': '0.0097414', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21725', 'logps_train/rejected': '-104.57', 'logps_train/chosen': '-154.11', 'loss/train': '0.60559', 'examples_per_second': '31.63', 'grad_norm': '23.75', 'counters/examples': 192768, 'counters/updates': 6024}
train stats after 192800 examples: {'rewards_train/chosen': '0.246', 'rewards_train/rejected': '0.079701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1663', 'logps_train/rejected': '-100.39', 'logps_train/chosen': '-135.28', 'loss/train': '0.62996', 'examples_per_second': '31.648', 'grad_norm': '21.75', 'counters/examples': 192800, 'counters/updates': 6025}
train stats after 192832 examples: {'rewards_train/chosen': '0.10339', 'rewards_train/rejected': '-0.021877', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12527', 'logps_train/rejected': '-160.14', 'logps_train/chosen': '-154.55', 'loss/train': '0.63818', 'examples_per_second': '31.365', 'grad_norm': '26.5', 'counters/examples': 192832, 'counters/updates': 6026}
train stats after 192864 examples: {'rewards_train/chosen': '0.18093', 'rewards_train/rejected': '-0.05774', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.23867', 'logps_train/rejected': '-87.865', 'logps_train/chosen': '-146.21', 'loss/train': '0.5886', 'examples_per_second': '32.174', 'grad_norm': '22.125', 'counters/examples': 192864, 'counters/updates': 6027}
train stats after 192896 examples: {'rewards_train/chosen': '0.23524', 'rewards_train/rejected': '0.014365', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22088', 'logps_train/rejected': '-153.07', 'logps_train/chosen': '-194.53', 'loss/train': '0.60391', 'examples_per_second': '30.644', 'grad_norm': '26.75', 'counters/examples': 192896, 'counters/updates': 6028}
train stats after 192928 examples: {'rewards_train/chosen': '0.049962', 'rewards_train/rejected': '0.00053007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049432', 'logps_train/rejected': '-103.68', 'logps_train/chosen': '-110.79', 'loss/train': '0.68372', 'examples_per_second': '32.032', 'grad_norm': '25.25', 'counters/examples': 192928, 'counters/updates': 6029}
train stats after 192960 examples: {'rewards_train/chosen': '0.10524', 'rewards_train/rejected': '-0.069689', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17493', 'logps_train/rejected': '-82.066', 'logps_train/chosen': '-105.92', 'loss/train': '0.61793', 'examples_per_second': '30.808', 'grad_norm': '19.75', 'counters/examples': 192960, 'counters/updates': 6030}
train stats after 192992 examples: {'rewards_train/chosen': '0.097908', 'rewards_train/rejected': '0.063294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034615', 'logps_train/rejected': '-162.99', 'logps_train/chosen': '-155.53', 'loss/train': '0.69819', 'examples_per_second': '31.635', 'grad_norm': '43', 'counters/examples': 192992, 'counters/updates': 6031}
train stats after 193024 examples: {'rewards_train/chosen': '0.09967', 'rewards_train/rejected': '-0.024791', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12446', 'logps_train/rejected': '-141.58', 'logps_train/chosen': '-129.08', 'loss/train': '0.64081', 'examples_per_second': '31.668', 'grad_norm': '26.75', 'counters/examples': 193024, 'counters/updates': 6032}
skipping logging after 193056 examples to avoid logging too frequently
train stats after 193088 examples: {'rewards_train/chosen': '0.20937', 'rewards_train/rejected': '0.044392', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16498', 'logps_train/rejected': '-100.46', 'logps_train/chosen': '-131.97', 'loss/train': '0.63066', 'examples_per_second': '31.547', 'grad_norm': '26.625', 'counters/examples': 193088, 'counters/updates': 6034}
train stats after 193120 examples: {'rewards_train/chosen': '0.11572', 'rewards_train/rejected': '0.080882', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034833', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-108.78', 'loss/train': '0.68279', 'examples_per_second': '31.66', 'grad_norm': '46.75', 'counters/examples': 193120, 'counters/updates': 6035}
skipping logging after 193152 examples to avoid logging too frequently
train stats after 193184 examples: {'rewards_train/chosen': '0.13231', 'rewards_train/rejected': '0.028823', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10349', 'logps_train/rejected': '-96.304', 'logps_train/chosen': '-140.84', 'loss/train': '0.64996', 'examples_per_second': '32.423', 'grad_norm': '23.5', 'counters/examples': 193184, 'counters/updates': 6037}
train stats after 193216 examples: {'rewards_train/chosen': '0.093019', 'rewards_train/rejected': '-0.0038057', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096824', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-122.65', 'loss/train': '0.65959', 'examples_per_second': '32.822', 'grad_norm': '27.375', 'counters/examples': 193216, 'counters/updates': 6038}
train stats after 193248 examples: {'rewards_train/chosen': '0.10192', 'rewards_train/rejected': '-0.016293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11822', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-124.19', 'loss/train': '0.64423', 'examples_per_second': '31.611', 'grad_norm': '27.625', 'counters/examples': 193248, 'counters/updates': 6039}
train stats after 193280 examples: {'rewards_train/chosen': '0.11896', 'rewards_train/rejected': '0.071559', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047401', 'logps_train/rejected': '-168.82', 'logps_train/chosen': '-144.2', 'loss/train': '0.68755', 'examples_per_second': '30.733', 'grad_norm': '32', 'counters/examples': 193280, 'counters/updates': 6040}
train stats after 193312 examples: {'rewards_train/chosen': '0.13801', 'rewards_train/rejected': '0.10134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036675', 'logps_train/rejected': '-156.57', 'logps_train/chosen': '-133.79', 'loss/train': '0.69', 'examples_per_second': '31.66', 'grad_norm': '26.125', 'counters/examples': 193312, 'counters/updates': 6041}
train stats after 193344 examples: {'rewards_train/chosen': '0.16574', 'rewards_train/rejected': '0.057783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10796', 'logps_train/rejected': '-155.37', 'logps_train/chosen': '-146.9', 'loss/train': '0.65425', 'examples_per_second': '30.06', 'grad_norm': '28.5', 'counters/examples': 193344, 'counters/updates': 6042}
train stats after 193376 examples: {'rewards_train/chosen': '0.22503', 'rewards_train/rejected': '0.06742', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15761', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-140.32', 'loss/train': '0.63366', 'examples_per_second': '31.525', 'grad_norm': '28.125', 'counters/examples': 193376, 'counters/updates': 6043}
skipping logging after 193408 examples to avoid logging too frequently
train stats after 193440 examples: {'rewards_train/chosen': '0.13659', 'rewards_train/rejected': '0.10924', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027353', 'logps_train/rejected': '-134.8', 'logps_train/chosen': '-147.29', 'loss/train': '0.69688', 'examples_per_second': '31.322', 'grad_norm': '27.5', 'counters/examples': 193440, 'counters/updates': 6045}
train stats after 193472 examples: {'rewards_train/chosen': '0.09873', 'rewards_train/rejected': '0.063439', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035291', 'logps_train/rejected': '-149.93', 'logps_train/chosen': '-142.04', 'loss/train': '0.6942', 'examples_per_second': '31.643', 'grad_norm': '30.25', 'counters/examples': 193472, 'counters/updates': 6046}
train stats after 193504 examples: {'rewards_train/chosen': '0.139', 'rewards_train/rejected': '0.027979', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11102', 'logps_train/rejected': '-94.703', 'logps_train/chosen': '-103.89', 'loss/train': '0.65116', 'examples_per_second': '31.65', 'grad_norm': '23.25', 'counters/examples': 193504, 'counters/updates': 6047}
skipping logging after 193536 examples to avoid logging too frequently
train stats after 193568 examples: {'rewards_train/chosen': '0.19473', 'rewards_train/rejected': '-0.0015757', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19631', 'logps_train/rejected': '-122.27', 'logps_train/chosen': '-116.31', 'loss/train': '0.61104', 'examples_per_second': '31.636', 'grad_norm': '22.25', 'counters/examples': 193568, 'counters/updates': 6049}
train stats after 193600 examples: {'rewards_train/chosen': '0.13904', 'rewards_train/rejected': '0.028444', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1106', 'logps_train/rejected': '-138.04', 'logps_train/chosen': '-145.75', 'loss/train': '0.66262', 'examples_per_second': '30.706', 'grad_norm': '26.375', 'counters/examples': 193600, 'counters/updates': 6050}
train stats after 193632 examples: {'rewards_train/chosen': '0.12064', 'rewards_train/rejected': '-0.021501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14214', 'logps_train/rejected': '-138.67', 'logps_train/chosen': '-153.89', 'loss/train': '0.63884', 'examples_per_second': '32.739', 'grad_norm': '27', 'counters/examples': 193632, 'counters/updates': 6051}
train stats after 193664 examples: {'rewards_train/chosen': '0.13632', 'rewards_train/rejected': '0.02932', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.107', 'logps_train/rejected': '-96.939', 'logps_train/chosen': '-100.61', 'loss/train': '0.65266', 'examples_per_second': '30.598', 'grad_norm': '22', 'counters/examples': 193664, 'counters/updates': 6052}
train stats after 193696 examples: {'rewards_train/chosen': '0.10219', 'rewards_train/rejected': '-0.01293', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11512', 'logps_train/rejected': '-100.31', 'logps_train/chosen': '-102.76', 'loss/train': '0.64828', 'examples_per_second': '32.465', 'grad_norm': '22.5', 'counters/examples': 193696, 'counters/updates': 6053}
train stats after 193728 examples: {'rewards_train/chosen': '0.17286', 'rewards_train/rejected': '0.0031787', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16968', 'logps_train/rejected': '-118.22', 'logps_train/chosen': '-181.13', 'loss/train': '0.63843', 'examples_per_second': '31.634', 'grad_norm': '30.75', 'counters/examples': 193728, 'counters/updates': 6054}
train stats after 193760 examples: {'rewards_train/chosen': '0.16495', 'rewards_train/rejected': '0.007873', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15708', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-134.79', 'loss/train': '0.63412', 'examples_per_second': '31.619', 'grad_norm': '25.375', 'counters/examples': 193760, 'counters/updates': 6055}
train stats after 193792 examples: {'rewards_train/chosen': '0.088641', 'rewards_train/rejected': '0.023902', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064738', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-149.56', 'loss/train': '0.66822', 'examples_per_second': '32.369', 'grad_norm': '24.375', 'counters/examples': 193792, 'counters/updates': 6056}
train stats after 193824 examples: {'rewards_train/chosen': '0.15703', 'rewards_train/rejected': '-0.013322', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17036', 'logps_train/rejected': '-121.14', 'logps_train/chosen': '-139.54', 'loss/train': '0.62176', 'examples_per_second': '30.084', 'grad_norm': '24.875', 'counters/examples': 193824, 'counters/updates': 6057}
train stats after 193856 examples: {'rewards_train/chosen': '0.19866', 'rewards_train/rejected': '0.090148', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10851', 'logps_train/rejected': '-147.95', 'logps_train/chosen': '-164.39', 'loss/train': '0.64984', 'examples_per_second': '31.602', 'grad_norm': '28.25', 'counters/examples': 193856, 'counters/updates': 6058}
train stats after 193888 examples: {'rewards_train/chosen': '0.14707', 'rewards_train/rejected': '0.023514', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12356', 'logps_train/rejected': '-99.144', 'logps_train/chosen': '-141.32', 'loss/train': '0.64494', 'examples_per_second': '27.128', 'grad_norm': '22.75', 'counters/examples': 193888, 'counters/updates': 6059}
skipping logging after 193920 examples to avoid logging too frequently
train stats after 193952 examples: {'rewards_train/chosen': '0.10328', 'rewards_train/rejected': '0.10272', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00055334', 'logps_train/rejected': '-146.23', 'logps_train/chosen': '-123.74', 'loss/train': '0.71081', 'examples_per_second': '31.663', 'grad_norm': '30.875', 'counters/examples': 193952, 'counters/updates': 6061}
train stats after 193984 examples: {'rewards_train/chosen': '0.13452', 'rewards_train/rejected': '0.042683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091835', 'logps_train/rejected': '-103.69', 'logps_train/chosen': '-144.72', 'loss/train': '0.66181', 'examples_per_second': '32.728', 'grad_norm': '25', 'counters/examples': 193984, 'counters/updates': 6062}
skipping logging after 194016 examples to avoid logging too frequently
train stats after 194048 examples: {'rewards_train/chosen': '0.17595', 'rewards_train/rejected': '0.059646', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11631', 'logps_train/rejected': '-102.22', 'logps_train/chosen': '-152.62', 'loss/train': '0.6519', 'examples_per_second': '33.997', 'grad_norm': '25.5', 'counters/examples': 194048, 'counters/updates': 6064}
train stats after 194080 examples: {'rewards_train/chosen': '0.20621', 'rewards_train/rejected': '0.053359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15285', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-125.85', 'loss/train': '0.63568', 'examples_per_second': '30.344', 'grad_norm': '23.75', 'counters/examples': 194080, 'counters/updates': 6065}
train stats after 194112 examples: {'rewards_train/chosen': '0.17413', 'rewards_train/rejected': '0.082179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091955', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-141.07', 'loss/train': '0.66008', 'examples_per_second': '30.912', 'grad_norm': '25.375', 'counters/examples': 194112, 'counters/updates': 6066}
train stats after 194144 examples: {'rewards_train/chosen': '0.16896', 'rewards_train/rejected': '-0.033461', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20242', 'logps_train/rejected': '-93.396', 'logps_train/chosen': '-123.12', 'loss/train': '0.60995', 'examples_per_second': '31.677', 'grad_norm': '19.5', 'counters/examples': 194144, 'counters/updates': 6067}
train stats after 194176 examples: {'rewards_train/chosen': '0.16641', 'rewards_train/rejected': '0.040543', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12586', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-166.67', 'loss/train': '0.65121', 'examples_per_second': '31.667', 'grad_norm': '26.125', 'counters/examples': 194176, 'counters/updates': 6068}
skipping logging after 194208 examples to avoid logging too frequently
train stats after 194240 examples: {'rewards_train/chosen': '0.027789', 'rewards_train/rejected': '-0.021093', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.048882', 'logps_train/rejected': '-96.98', 'logps_train/chosen': '-130.39', 'loss/train': '0.6736', 'examples_per_second': '30.443', 'grad_norm': '23.125', 'counters/examples': 194240, 'counters/updates': 6070}
train stats after 194272 examples: {'rewards_train/chosen': '0.14619', 'rewards_train/rejected': '-0.02314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16933', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-160.45', 'loss/train': '0.62953', 'examples_per_second': '32.707', 'grad_norm': '26', 'counters/examples': 194272, 'counters/updates': 6071}
skipping logging after 194304 examples to avoid logging too frequently
train stats after 194336 examples: {'rewards_train/chosen': '0.076967', 'rewards_train/rejected': '0.028968', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047999', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-134.83', 'loss/train': '0.67656', 'examples_per_second': '30.181', 'grad_norm': '26.625', 'counters/examples': 194336, 'counters/updates': 6073}
skipping logging after 194368 examples to avoid logging too frequently
train stats after 194400 examples: {'rewards_train/chosen': '0.12575', 'rewards_train/rejected': '0.1157', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01005', 'logps_train/rejected': '-150', 'logps_train/chosen': '-172.01', 'loss/train': '0.69386', 'examples_per_second': '30.204', 'grad_norm': '29.125', 'counters/examples': 194400, 'counters/updates': 6075}
train stats after 194432 examples: {'rewards_train/chosen': '0.22317', 'rewards_train/rejected': '-0.0008384', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22401', 'logps_train/rejected': '-128.83', 'logps_train/chosen': '-157.38', 'loss/train': '0.61111', 'examples_per_second': '31.627', 'grad_norm': '26.125', 'counters/examples': 194432, 'counters/updates': 6076}
skipping logging after 194464 examples to avoid logging too frequently
train stats after 194496 examples: {'rewards_train/chosen': '0.17205', 'rewards_train/rejected': '0.023068', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14898', 'logps_train/rejected': '-89.23', 'logps_train/chosen': '-109.68', 'loss/train': '0.63078', 'examples_per_second': '33.117', 'grad_norm': '25.625', 'counters/examples': 194496, 'counters/updates': 6078}
train stats after 194528 examples: {'rewards_train/chosen': '0.11399', 'rewards_train/rejected': '0.03169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082297', 'logps_train/rejected': '-134.38', 'logps_train/chosen': '-133.58', 'loss/train': '0.66333', 'examples_per_second': '31.876', 'grad_norm': '26.625', 'counters/examples': 194528, 'counters/updates': 6079}
train stats after 194560 examples: {'rewards_train/chosen': '0.19915', 'rewards_train/rejected': '0.028482', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17066', 'logps_train/rejected': '-136.52', 'logps_train/chosen': '-157.36', 'loss/train': '0.62889', 'examples_per_second': '31.302', 'grad_norm': '25', 'counters/examples': 194560, 'counters/updates': 6080}
train stats after 194592 examples: {'rewards_train/chosen': '0.13656', 'rewards_train/rejected': '0.063397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073162', 'logps_train/rejected': '-103.73', 'logps_train/chosen': '-156.31', 'loss/train': '0.67641', 'examples_per_second': '31.644', 'grad_norm': '29.125', 'counters/examples': 194592, 'counters/updates': 6081}
train stats after 194624 examples: {'rewards_train/chosen': '0.14505', 'rewards_train/rejected': '0.030789', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11426', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-129.06', 'loss/train': '0.64921', 'examples_per_second': '30.797', 'grad_norm': '24.625', 'counters/examples': 194624, 'counters/updates': 6082}
train stats after 194656 examples: {'rewards_train/chosen': '0.09775', 'rewards_train/rejected': '0.022544', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075206', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-107.46', 'loss/train': '0.66411', 'examples_per_second': '30.854', 'grad_norm': '21.375', 'counters/examples': 194656, 'counters/updates': 6083}
train stats after 194688 examples: {'rewards_train/chosen': '0.16141', 'rewards_train/rejected': '0.090881', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07053', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-172.64', 'loss/train': '0.6727', 'examples_per_second': '30.296', 'grad_norm': '28', 'counters/examples': 194688, 'counters/updates': 6084}
train stats after 194720 examples: {'rewards_train/chosen': '0.088451', 'rewards_train/rejected': '0.073113', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015338', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-142.31', 'loss/train': '0.7043', 'examples_per_second': '31.611', 'grad_norm': '35.5', 'counters/examples': 194720, 'counters/updates': 6085}
train stats after 194752 examples: {'rewards_train/chosen': '0.15011', 'rewards_train/rejected': '0.063077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087029', 'logps_train/rejected': '-150.84', 'logps_train/chosen': '-153.92', 'loss/train': '0.663', 'examples_per_second': '30.677', 'grad_norm': '28.625', 'counters/examples': 194752, 'counters/updates': 6086}
train stats after 194784 examples: {'rewards_train/chosen': '0.11247', 'rewards_train/rejected': '0.04476', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06771', 'logps_train/rejected': '-138.52', 'logps_train/chosen': '-116.59', 'loss/train': '0.69004', 'examples_per_second': '31.34', 'grad_norm': '41.25', 'counters/examples': 194784, 'counters/updates': 6087}
train stats after 194816 examples: {'rewards_train/chosen': '0.072015', 'rewards_train/rejected': '0.0020215', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069994', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-160.4', 'loss/train': '0.67348', 'examples_per_second': '31.63', 'grad_norm': '29.375', 'counters/examples': 194816, 'counters/updates': 6088}
train stats after 194848 examples: {'rewards_train/chosen': '0.11476', 'rewards_train/rejected': '-0.033564', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14833', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-182.94', 'loss/train': '0.64466', 'examples_per_second': '31.409', 'grad_norm': '29.625', 'counters/examples': 194848, 'counters/updates': 6089}
train stats after 194880 examples: {'rewards_train/chosen': '0.16509', 'rewards_train/rejected': '0.062311', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10278', 'logps_train/rejected': '-128.54', 'logps_train/chosen': '-161.59', 'loss/train': '0.65452', 'examples_per_second': '30.282', 'grad_norm': '28.125', 'counters/examples': 194880, 'counters/updates': 6090}
train stats after 194912 examples: {'rewards_train/chosen': '0.10816', 'rewards_train/rejected': '0.067645', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040515', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-146.8', 'loss/train': '0.68483', 'examples_per_second': '31.27', 'grad_norm': '26.125', 'counters/examples': 194912, 'counters/updates': 6091}
skipping logging after 194944 examples to avoid logging too frequently
train stats after 194976 examples: {'rewards_train/chosen': '0.10321', 'rewards_train/rejected': '-0.070488', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.1737', 'logps_train/rejected': '-94.841', 'logps_train/chosen': '-134.14', 'loss/train': '0.62009', 'examples_per_second': '35.847', 'grad_norm': '22.75', 'counters/examples': 194976, 'counters/updates': 6093}
train stats after 195008 examples: {'rewards_train/chosen': '0.088505', 'rewards_train/rejected': '-0.036516', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12502', 'logps_train/rejected': '-102.6', 'logps_train/chosen': '-126.41', 'loss/train': '0.64662', 'examples_per_second': '30.657', 'grad_norm': '22.375', 'counters/examples': 195008, 'counters/updates': 6094}
skipping logging after 195040 examples to avoid logging too frequently
train stats after 195072 examples: {'rewards_train/chosen': '0.20704', 'rewards_train/rejected': '0.070525', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13651', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-149.02', 'loss/train': '0.64619', 'examples_per_second': '32.386', 'grad_norm': '29.75', 'counters/examples': 195072, 'counters/updates': 6096}
train stats after 195104 examples: {'rewards_train/chosen': '0.12609', 'rewards_train/rejected': '0.1415', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015413', 'logps_train/rejected': '-135.69', 'logps_train/chosen': '-173.39', 'loss/train': '0.72241', 'examples_per_second': '31.574', 'grad_norm': '32.5', 'counters/examples': 195104, 'counters/updates': 6097}
train stats after 195136 examples: {'rewards_train/chosen': '0.18758', 'rewards_train/rejected': '0.074089', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11349', 'logps_train/rejected': '-111.8', 'logps_train/chosen': '-116.87', 'loss/train': '0.6507', 'examples_per_second': '31.169', 'grad_norm': '24.625', 'counters/examples': 195136, 'counters/updates': 6098}
skipping logging after 195168 examples to avoid logging too frequently
train stats after 195200 examples: {'rewards_train/chosen': '0.15926', 'rewards_train/rejected': '0.022389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13687', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-147.89', 'loss/train': '0.64085', 'examples_per_second': '31.538', 'grad_norm': '24.875', 'counters/examples': 195200, 'counters/updates': 6100}
train stats after 195232 examples: {'rewards_train/chosen': '0.10562', 'rewards_train/rejected': '0.035596', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070026', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-119.74', 'loss/train': '0.66706', 'examples_per_second': '31.6', 'grad_norm': '24', 'counters/examples': 195232, 'counters/updates': 6101}
train stats after 195264 examples: {'rewards_train/chosen': '0.18374', 'rewards_train/rejected': '0.060162', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12358', 'logps_train/rejected': '-135.95', 'logps_train/chosen': '-148.19', 'loss/train': '0.64926', 'examples_per_second': '30.791', 'grad_norm': '26.75', 'counters/examples': 195264, 'counters/updates': 6102}
train stats after 195296 examples: {'rewards_train/chosen': '0.059179', 'rewards_train/rejected': '-0.019973', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079152', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-130.32', 'loss/train': '0.66481', 'examples_per_second': '30.697', 'grad_norm': '25.375', 'counters/examples': 195296, 'counters/updates': 6103}
train stats after 195328 examples: {'rewards_train/chosen': '0.18978', 'rewards_train/rejected': '0.011614', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17816', 'logps_train/rejected': '-104.64', 'logps_train/chosen': '-113.13', 'loss/train': '0.62043', 'examples_per_second': '30.306', 'grad_norm': '26.125', 'counters/examples': 195328, 'counters/updates': 6104}
train stats after 195360 examples: {'rewards_train/chosen': '0.22099', 'rewards_train/rejected': '0.12734', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093649', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-138.79', 'loss/train': '0.65897', 'examples_per_second': '32.546', 'grad_norm': '24.625', 'counters/examples': 195360, 'counters/updates': 6105}
train stats after 195392 examples: {'rewards_train/chosen': '0.12586', 'rewards_train/rejected': '0.053608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072254', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-113.56', 'loss/train': '0.6679', 'examples_per_second': '31.558', 'grad_norm': '26', 'counters/examples': 195392, 'counters/updates': 6106}
train stats after 195424 examples: {'rewards_train/chosen': '0.10092', 'rewards_train/rejected': '-0.022271', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12319', 'logps_train/rejected': '-98.582', 'logps_train/chosen': '-128.32', 'loss/train': '0.6487', 'examples_per_second': '31.838', 'grad_norm': '21.75', 'counters/examples': 195424, 'counters/updates': 6107}
train stats after 195456 examples: {'rewards_train/chosen': '0.06721', 'rewards_train/rejected': '0.013294', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053916', 'logps_train/rejected': '-111.8', 'logps_train/chosen': '-157.6', 'loss/train': '0.67241', 'examples_per_second': '30.536', 'grad_norm': '28.125', 'counters/examples': 195456, 'counters/updates': 6108}
train stats after 195488 examples: {'rewards_train/chosen': '0.21914', 'rewards_train/rejected': '-0.007374', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22652', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-181.9', 'loss/train': '0.60071', 'examples_per_second': '30.133', 'grad_norm': '27.125', 'counters/examples': 195488, 'counters/updates': 6109}
train stats after 195520 examples: {'rewards_train/chosen': '0.1268', 'rewards_train/rejected': '0.098198', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028601', 'logps_train/rejected': '-155.9', 'logps_train/chosen': '-143.67', 'loss/train': '0.69402', 'examples_per_second': '31.558', 'grad_norm': '27', 'counters/examples': 195520, 'counters/updates': 6110}
train stats after 195552 examples: {'rewards_train/chosen': '0.17333', 'rewards_train/rejected': '0.02697', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14636', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-150.85', 'loss/train': '0.6323', 'examples_per_second': '31.356', 'grad_norm': '24.125', 'counters/examples': 195552, 'counters/updates': 6111}
train stats after 195584 examples: {'rewards_train/chosen': '0.17887', 'rewards_train/rejected': '0.046229', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13264', 'logps_train/rejected': '-157.56', 'logps_train/chosen': '-160.91', 'loss/train': '0.64086', 'examples_per_second': '32.691', 'grad_norm': '39.5', 'counters/examples': 195584, 'counters/updates': 6112}
train stats after 195616 examples: {'rewards_train/chosen': '0.23511', 'rewards_train/rejected': '0.035492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19962', 'logps_train/rejected': '-138.59', 'logps_train/chosen': '-146.89', 'loss/train': '0.61419', 'examples_per_second': '32.891', 'grad_norm': '24.75', 'counters/examples': 195616, 'counters/updates': 6113}
train stats after 195648 examples: {'rewards_train/chosen': '0.18941', 'rewards_train/rejected': '0.064431', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12498', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-139.11', 'loss/train': '0.64522', 'examples_per_second': '30.816', 'grad_norm': '24.375', 'counters/examples': 195648, 'counters/updates': 6114}
train stats after 195680 examples: {'rewards_train/chosen': '0.14415', 'rewards_train/rejected': '0.18167', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.037518', 'logps_train/rejected': '-151.34', 'logps_train/chosen': '-146.3', 'loss/train': '0.72259', 'examples_per_second': '31.605', 'grad_norm': '29', 'counters/examples': 195680, 'counters/updates': 6115}
train stats after 195712 examples: {'rewards_train/chosen': '0.18564', 'rewards_train/rejected': '0.036734', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1489', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-172.93', 'loss/train': '0.64492', 'examples_per_second': '30.149', 'grad_norm': '26.75', 'counters/examples': 195712, 'counters/updates': 6116}
train stats after 195744 examples: {'rewards_train/chosen': '0.1263', 'rewards_train/rejected': '-0.020584', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14688', 'logps_train/rejected': '-99.549', 'logps_train/chosen': '-131.03', 'loss/train': '0.63478', 'examples_per_second': '30.572', 'grad_norm': '24', 'counters/examples': 195744, 'counters/updates': 6117}
train stats after 195776 examples: {'rewards_train/chosen': '0.16173', 'rewards_train/rejected': '-0.0094177', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17115', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-139.04', 'loss/train': '0.62418', 'examples_per_second': '30.529', 'grad_norm': '23.875', 'counters/examples': 195776, 'counters/updates': 6118}
train stats after 195808 examples: {'rewards_train/chosen': '0.13019', 'rewards_train/rejected': '0.10846', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021725', 'logps_train/rejected': '-145.59', 'logps_train/chosen': '-132.9', 'loss/train': '0.69953', 'examples_per_second': '30.274', 'grad_norm': '29.75', 'counters/examples': 195808, 'counters/updates': 6119}
train stats after 195840 examples: {'rewards_train/chosen': '0.17367', 'rewards_train/rejected': '0.055532', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11813', 'logps_train/rejected': '-93.858', 'logps_train/chosen': '-108.11', 'loss/train': '0.64504', 'examples_per_second': '31.168', 'grad_norm': '20.875', 'counters/examples': 195840, 'counters/updates': 6120}
train stats after 195872 examples: {'rewards_train/chosen': '0.25519', 'rewards_train/rejected': '0.092583', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16261', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-167.48', 'loss/train': '0.62417', 'examples_per_second': '31.604', 'grad_norm': '25.875', 'counters/examples': 195872, 'counters/updates': 6121}
train stats after 195904 examples: {'rewards_train/chosen': '0.057672', 'rewards_train/rejected': '0.10006', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.042384', 'logps_train/rejected': '-122.85', 'logps_train/chosen': '-152.19', 'loss/train': '0.73133', 'examples_per_second': '31.633', 'grad_norm': '30.25', 'counters/examples': 195904, 'counters/updates': 6122}
train stats after 195936 examples: {'rewards_train/chosen': '0.044965', 'rewards_train/rejected': '-0.078016', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12298', 'logps_train/rejected': '-130.95', 'logps_train/chosen': '-147.78', 'loss/train': '0.65458', 'examples_per_second': '31.957', 'grad_norm': '24.875', 'counters/examples': 195936, 'counters/updates': 6123}
skipping logging after 195968 examples to avoid logging too frequently
train stats after 196000 examples: {'rewards_train/chosen': '0.14549', 'rewards_train/rejected': '0.020038', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12545', 'logps_train/rejected': '-92.973', 'logps_train/chosen': '-90.382', 'loss/train': '0.64653', 'examples_per_second': '34.377', 'grad_norm': '22.5', 'counters/examples': 196000, 'counters/updates': 6125}
skipping logging after 196032 examples to avoid logging too frequently
train stats after 196064 examples: {'rewards_train/chosen': '0.17125', 'rewards_train/rejected': '0.10822', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063024', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-124.7', 'loss/train': '0.67654', 'examples_per_second': '30.893', 'grad_norm': '24.375', 'counters/examples': 196064, 'counters/updates': 6127}
train stats after 196096 examples: {'rewards_train/chosen': '0.091189', 'rewards_train/rejected': '-0.008366', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099555', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-141.44', 'loss/train': '0.65232', 'examples_per_second': '31.025', 'grad_norm': '24.25', 'counters/examples': 196096, 'counters/updates': 6128}
skipping logging after 196128 examples to avoid logging too frequently
train stats after 196160 examples: {'rewards_train/chosen': '0.23847', 'rewards_train/rejected': '0.019697', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21877', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-168.01', 'loss/train': '0.60389', 'examples_per_second': '33.554', 'grad_norm': '24.5', 'counters/examples': 196160, 'counters/updates': 6130}
train stats after 196192 examples: {'rewards_train/chosen': '0.1179', 'rewards_train/rejected': '0.086179', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031725', 'logps_train/rejected': '-146.77', 'logps_train/chosen': '-160.33', 'loss/train': '0.69411', 'examples_per_second': '31.598', 'grad_norm': '30.25', 'counters/examples': 196192, 'counters/updates': 6131}
train stats after 196224 examples: {'rewards_train/chosen': '0.086298', 'rewards_train/rejected': '0.02836', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057938', 'logps_train/rejected': '-148.7', 'logps_train/chosen': '-128.96', 'loss/train': '0.68134', 'examples_per_second': '32.342', 'grad_norm': '26.75', 'counters/examples': 196224, 'counters/updates': 6132}
train stats after 196256 examples: {'rewards_train/chosen': '0.13806', 'rewards_train/rejected': '0.029833', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10823', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-138.49', 'loss/train': '0.65363', 'examples_per_second': '32.88', 'grad_norm': '26.875', 'counters/examples': 196256, 'counters/updates': 6133}
train stats after 196288 examples: {'rewards_train/chosen': '0.13483', 'rewards_train/rejected': '0.00033337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1345', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-143.81', 'loss/train': '0.63972', 'examples_per_second': '30.401', 'grad_norm': '29.375', 'counters/examples': 196288, 'counters/updates': 6134}
train stats after 196320 examples: {'rewards_train/chosen': '0.10982', 'rewards_train/rejected': '0.10812', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0016997', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-96.639', 'loss/train': '0.70523', 'examples_per_second': '32.329', 'grad_norm': '25', 'counters/examples': 196320, 'counters/updates': 6135}
train stats after 196352 examples: {'rewards_train/chosen': '0.10587', 'rewards_train/rejected': '-0.014273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12014', 'logps_train/rejected': '-73.792', 'logps_train/chosen': '-129.16', 'loss/train': '0.64485', 'examples_per_second': '32.438', 'grad_norm': '22.75', 'counters/examples': 196352, 'counters/updates': 6136}
skipping logging after 196384 examples to avoid logging too frequently
train stats after 196416 examples: {'rewards_train/chosen': '0.10953', 'rewards_train/rejected': '0.052078', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057448', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-119.12', 'loss/train': '0.67739', 'examples_per_second': '31.647', 'grad_norm': '25', 'counters/examples': 196416, 'counters/updates': 6138}
train stats after 196448 examples: {'rewards_train/chosen': '0.1741', 'rewards_train/rejected': '0.11661', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057498', 'logps_train/rejected': '-156.11', 'logps_train/chosen': '-141.89', 'loss/train': '0.673', 'examples_per_second': '30.914', 'grad_norm': '30.25', 'counters/examples': 196448, 'counters/updates': 6139}
train stats after 196480 examples: {'rewards_train/chosen': '0.14823', 'rewards_train/rejected': '0.063106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085121', 'logps_train/rejected': '-133.86', 'logps_train/chosen': '-124.66', 'loss/train': '0.66149', 'examples_per_second': '30.857', 'grad_norm': '24', 'counters/examples': 196480, 'counters/updates': 6140}
train stats after 196512 examples: {'rewards_train/chosen': '0.10558', 'rewards_train/rejected': '0.007439', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098138', 'logps_train/rejected': '-157.7', 'logps_train/chosen': '-138.06', 'loss/train': '0.66258', 'examples_per_second': '32.769', 'grad_norm': '30.125', 'counters/examples': 196512, 'counters/updates': 6141}
train stats after 196544 examples: {'rewards_train/chosen': '0.079446', 'rewards_train/rejected': '0.046371', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033075', 'logps_train/rejected': '-158.17', 'logps_train/chosen': '-129.91', 'loss/train': '0.68731', 'examples_per_second': '31.531', 'grad_norm': '26.25', 'counters/examples': 196544, 'counters/updates': 6142}
train stats after 196576 examples: {'rewards_train/chosen': '0.10622', 'rewards_train/rejected': '0.064226', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041991', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-152.04', 'loss/train': '0.68516', 'examples_per_second': '31.585', 'grad_norm': '32.75', 'counters/examples': 196576, 'counters/updates': 6143}
train stats after 196608 examples: {'rewards_train/chosen': '0.18563', 'rewards_train/rejected': '0.038824', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1468', 'logps_train/rejected': '-136.65', 'logps_train/chosen': '-154.66', 'loss/train': '0.63437', 'examples_per_second': '31.532', 'grad_norm': '27.875', 'counters/examples': 196608, 'counters/updates': 6144}
train stats after 196640 examples: {'rewards_train/chosen': '0.14795', 'rewards_train/rejected': '0.095887', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052062', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-160.79', 'loss/train': '0.68686', 'examples_per_second': '31.582', 'grad_norm': '29.625', 'counters/examples': 196640, 'counters/updates': 6145}
train stats after 196672 examples: {'rewards_train/chosen': '0.090985', 'rewards_train/rejected': '0.053539', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037446', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-157.46', 'loss/train': '0.68568', 'examples_per_second': '31.726', 'grad_norm': '27.375', 'counters/examples': 196672, 'counters/updates': 6146}
train stats after 196704 examples: {'rewards_train/chosen': '0.074181', 'rewards_train/rejected': '0.032765', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.041416', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-129.67', 'loss/train': '0.68127', 'examples_per_second': '32.712', 'grad_norm': '26.625', 'counters/examples': 196704, 'counters/updates': 6147}
train stats after 196736 examples: {'rewards_train/chosen': '0.15663', 'rewards_train/rejected': '0.17296', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016331', 'logps_train/rejected': '-151.76', 'logps_train/chosen': '-162.55', 'loss/train': '0.71693', 'examples_per_second': '24.373', 'grad_norm': '32', 'counters/examples': 196736, 'counters/updates': 6148}
train stats after 196768 examples: {'rewards_train/chosen': '0.094673', 'rewards_train/rejected': '0.020566', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.074106', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-134.05', 'loss/train': '0.67514', 'examples_per_second': '30.098', 'grad_norm': '25.75', 'counters/examples': 196768, 'counters/updates': 6149}
skipping logging after 196800 examples to avoid logging too frequently
train stats after 196832 examples: {'rewards_train/chosen': '0.095092', 'rewards_train/rejected': '0.095898', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00080552', 'logps_train/rejected': '-125.47', 'logps_train/chosen': '-135.62', 'loss/train': '0.70328', 'examples_per_second': '23.976', 'grad_norm': '26.875', 'counters/examples': 196832, 'counters/updates': 6151}
train stats after 196864 examples: {'rewards_train/chosen': '0.10832', 'rewards_train/rejected': '0.026387', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081936', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-155.12', 'loss/train': '0.66382', 'examples_per_second': '30.629', 'grad_norm': '27.75', 'counters/examples': 196864, 'counters/updates': 6152}
skipping logging after 196896 examples to avoid logging too frequently
train stats after 196928 examples: {'rewards_train/chosen': '0.086305', 'rewards_train/rejected': '-0.0010363', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087341', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-134.92', 'loss/train': '0.6572', 'examples_per_second': '31.538', 'grad_norm': '25', 'counters/examples': 196928, 'counters/updates': 6154}
train stats after 196960 examples: {'rewards_train/chosen': '0.15118', 'rewards_train/rejected': '0.021509', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12967', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-160.99', 'loss/train': '0.64632', 'examples_per_second': '31.561', 'grad_norm': '27', 'counters/examples': 196960, 'counters/updates': 6155}
train stats after 196992 examples: {'rewards_train/chosen': '0.16023', 'rewards_train/rejected': '0.056325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10391', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-137.98', 'loss/train': '0.66314', 'examples_per_second': '31.977', 'grad_norm': '27.25', 'counters/examples': 196992, 'counters/updates': 6156}
train stats after 197024 examples: {'rewards_train/chosen': '0.11181', 'rewards_train/rejected': '0.036046', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075768', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-156.99', 'loss/train': '0.6702', 'examples_per_second': '30.659', 'grad_norm': '28.625', 'counters/examples': 197024, 'counters/updates': 6157}
train stats after 197056 examples: {'rewards_train/chosen': '0.30484', 'rewards_train/rejected': '0.062681', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24216', 'logps_train/rejected': '-116.57', 'logps_train/chosen': '-156', 'loss/train': '0.59636', 'examples_per_second': '30.516', 'grad_norm': '25.625', 'counters/examples': 197056, 'counters/updates': 6158}
train stats after 197088 examples: {'rewards_train/chosen': '0.082984', 'rewards_train/rejected': '-0.065565', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14855', 'logps_train/rejected': '-75.637', 'logps_train/chosen': '-139.35', 'loss/train': '0.63604', 'examples_per_second': '31.588', 'grad_norm': '21.625', 'counters/examples': 197088, 'counters/updates': 6159}
train stats after 197120 examples: {'rewards_train/chosen': '0.07903', 'rewards_train/rejected': '0.080521', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0014908', 'logps_train/rejected': '-160.25', 'logps_train/chosen': '-121.82', 'loss/train': '0.70777', 'examples_per_second': '31.321', 'grad_norm': '32.5', 'counters/examples': 197120, 'counters/updates': 6160}
train stats after 197152 examples: {'rewards_train/chosen': '0.14233', 'rewards_train/rejected': '0.034702', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10763', 'logps_train/rejected': '-95.964', 'logps_train/chosen': '-103.5', 'loss/train': '0.65398', 'examples_per_second': '31.28', 'grad_norm': '20.875', 'counters/examples': 197152, 'counters/updates': 6161}
train stats after 197184 examples: {'rewards_train/chosen': '0.10385', 'rewards_train/rejected': '0.062955', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040892', 'logps_train/rejected': '-113.74', 'logps_train/chosen': '-104.05', 'loss/train': '0.68616', 'examples_per_second': '30.329', 'grad_norm': '23.625', 'counters/examples': 197184, 'counters/updates': 6162}
train stats after 197216 examples: {'rewards_train/chosen': '0.12184', 'rewards_train/rejected': '0.076456', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045385', 'logps_train/rejected': '-127.22', 'logps_train/chosen': '-139.34', 'loss/train': '0.67891', 'examples_per_second': '31.619', 'grad_norm': '30.375', 'counters/examples': 197216, 'counters/updates': 6163}
train stats after 197248 examples: {'rewards_train/chosen': '0.077741', 'rewards_train/rejected': '-0.01291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09065', 'logps_train/rejected': '-113.83', 'logps_train/chosen': '-164.54', 'loss/train': '0.65412', 'examples_per_second': '30.09', 'grad_norm': '32.5', 'counters/examples': 197248, 'counters/updates': 6164}
train stats after 197280 examples: {'rewards_train/chosen': '0.15323', 'rewards_train/rejected': '0.050166', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10306', 'logps_train/rejected': '-105.19', 'logps_train/chosen': '-141.77', 'loss/train': '0.65194', 'examples_per_second': '31.012', 'grad_norm': '24.25', 'counters/examples': 197280, 'counters/updates': 6165}
train stats after 197312 examples: {'rewards_train/chosen': '0.099323', 'rewards_train/rejected': '0.038806', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060517', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-148.4', 'loss/train': '0.67266', 'examples_per_second': '30.031', 'grad_norm': '25.25', 'counters/examples': 197312, 'counters/updates': 6166}
train stats after 197344 examples: {'rewards_train/chosen': '0.073102', 'rewards_train/rejected': '-0.013782', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086884', 'logps_train/rejected': '-108.84', 'logps_train/chosen': '-129.31', 'loss/train': '0.66316', 'examples_per_second': '31.354', 'grad_norm': '23.875', 'counters/examples': 197344, 'counters/updates': 6167}
train stats after 197376 examples: {'rewards_train/chosen': '0.053691', 'rewards_train/rejected': '-0.0084769', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062168', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-136.38', 'loss/train': '0.67264', 'examples_per_second': '31.639', 'grad_norm': '25.75', 'counters/examples': 197376, 'counters/updates': 6168}
train stats after 197408 examples: {'rewards_train/chosen': '0.095434', 'rewards_train/rejected': '0.038392', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057042', 'logps_train/rejected': '-123.44', 'logps_train/chosen': '-136.03', 'loss/train': '0.68115', 'examples_per_second': '30.167', 'grad_norm': '26.375', 'counters/examples': 197408, 'counters/updates': 6169}
train stats after 197440 examples: {'rewards_train/chosen': '0.14197', 'rewards_train/rejected': '0.1188', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023165', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-129.66', 'loss/train': '0.69245', 'examples_per_second': '31.323', 'grad_norm': '26.5', 'counters/examples': 197440, 'counters/updates': 6170}
skipping logging after 197472 examples to avoid logging too frequently
train stats after 197504 examples: {'rewards_train/chosen': '0.11264', 'rewards_train/rejected': '-0.037891', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15053', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-128.45', 'loss/train': '0.63069', 'examples_per_second': '34.237', 'grad_norm': '25.25', 'counters/examples': 197504, 'counters/updates': 6172}
skipping logging after 197536 examples to avoid logging too frequently
train stats after 197568 examples: {'rewards_train/chosen': '0.21102', 'rewards_train/rejected': '0.0092068', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20181', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-161.58', 'loss/train': '0.61486', 'examples_per_second': '31.54', 'grad_norm': '28', 'counters/examples': 197568, 'counters/updates': 6174}
train stats after 197600 examples: {'rewards_train/chosen': '0.12834', 'rewards_train/rejected': '0.1246', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0037409', 'logps_train/rejected': '-125.47', 'logps_train/chosen': '-148.16', 'loss/train': '0.70521', 'examples_per_second': '32.366', 'grad_norm': '26.75', 'counters/examples': 197600, 'counters/updates': 6175}
train stats after 197632 examples: {'rewards_train/chosen': '0.15838', 'rewards_train/rejected': '0.071186', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08719', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-139.29', 'loss/train': '0.66872', 'examples_per_second': '30.115', 'grad_norm': '29.5', 'counters/examples': 197632, 'counters/updates': 6176}
train stats after 197664 examples: {'rewards_train/chosen': '0.076844', 'rewards_train/rejected': '-0.021111', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097955', 'logps_train/rejected': '-118.68', 'logps_train/chosen': '-129.28', 'loss/train': '0.65299', 'examples_per_second': '30.707', 'grad_norm': '23.625', 'counters/examples': 197664, 'counters/updates': 6177}
train stats after 197696 examples: {'rewards_train/chosen': '0.20522', 'rewards_train/rejected': '0.01346', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19176', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-135.78', 'loss/train': '0.61805', 'examples_per_second': '30.756', 'grad_norm': '25.125', 'counters/examples': 197696, 'counters/updates': 6178}
train stats after 197728 examples: {'rewards_train/chosen': '0.11127', 'rewards_train/rejected': '0.10089', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.010382', 'logps_train/rejected': '-144.72', 'logps_train/chosen': '-120.79', 'loss/train': '0.70137', 'examples_per_second': '31.508', 'grad_norm': '31.625', 'counters/examples': 197728, 'counters/updates': 6179}
train stats after 197760 examples: {'rewards_train/chosen': '0.072279', 'rewards_train/rejected': '0.036817', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035462', 'logps_train/rejected': '-134.31', 'logps_train/chosen': '-145.08', 'loss/train': '0.68525', 'examples_per_second': '31.606', 'grad_norm': '30.125', 'counters/examples': 197760, 'counters/updates': 6180}
train stats after 197792 examples: {'rewards_train/chosen': '0.11937', 'rewards_train/rejected': '-0.051043', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17041', 'logps_train/rejected': '-142.97', 'logps_train/chosen': '-128.75', 'loss/train': '0.62288', 'examples_per_second': '31.532', 'grad_norm': '22.875', 'counters/examples': 197792, 'counters/updates': 6181}
train stats after 197824 examples: {'rewards_train/chosen': '0.15654', 'rewards_train/rejected': '0.074657', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08188', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-109.44', 'loss/train': '0.67066', 'examples_per_second': '31.322', 'grad_norm': '23.625', 'counters/examples': 197824, 'counters/updates': 6182}
train stats after 197856 examples: {'rewards_train/chosen': '0.12818', 'rewards_train/rejected': '-0.058328', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18651', 'logps_train/rejected': '-145.83', 'logps_train/chosen': '-148.54', 'loss/train': '0.61823', 'examples_per_second': '32.334', 'grad_norm': '30.375', 'counters/examples': 197856, 'counters/updates': 6183}
train stats after 197888 examples: {'rewards_train/chosen': '0.10777', 'rewards_train/rejected': '0.10526', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0025063', 'logps_train/rejected': '-150.06', 'logps_train/chosen': '-145.23', 'loss/train': '0.70336', 'examples_per_second': '30.641', 'grad_norm': '30.5', 'counters/examples': 197888, 'counters/updates': 6184}
skipping logging after 197920 examples to avoid logging too frequently
train stats after 197952 examples: {'rewards_train/chosen': '0.17305', 'rewards_train/rejected': '0.099341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073708', 'logps_train/rejected': '-134.13', 'logps_train/chosen': '-165.39', 'loss/train': '0.6682', 'examples_per_second': '33.592', 'grad_norm': '28.625', 'counters/examples': 197952, 'counters/updates': 6186}
train stats after 197984 examples: {'rewards_train/chosen': '0.11744', 'rewards_train/rejected': '-0.013395', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13084', 'logps_train/rejected': '-98.304', 'logps_train/chosen': '-122.73', 'loss/train': '0.64188', 'examples_per_second': '30.012', 'grad_norm': '22.875', 'counters/examples': 197984, 'counters/updates': 6187}
train stats after 198016 examples: {'rewards_train/chosen': '0.082063', 'rewards_train/rejected': '-0.073795', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15586', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-178.04', 'loss/train': '0.64229', 'examples_per_second': '31.575', 'grad_norm': '25.875', 'counters/examples': 198016, 'counters/updates': 6188}
skipping logging after 198048 examples to avoid logging too frequently
train stats after 198080 examples: {'rewards_train/chosen': '0.069238', 'rewards_train/rejected': '0.045438', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023799', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-124.44', 'loss/train': '0.69257', 'examples_per_second': '31.125', 'grad_norm': '43.75', 'counters/examples': 198080, 'counters/updates': 6190}
train stats after 198112 examples: {'rewards_train/chosen': '0.10005', 'rewards_train/rejected': '0.043133', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056913', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-159.11', 'loss/train': '0.68243', 'examples_per_second': '31.435', 'grad_norm': '27.625', 'counters/examples': 198112, 'counters/updates': 6191}
train stats after 198144 examples: {'rewards_train/chosen': '0.15641', 'rewards_train/rejected': '0.058788', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097626', 'logps_train/rejected': '-90.71', 'logps_train/chosen': '-132.51', 'loss/train': '0.65466', 'examples_per_second': '31.343', 'grad_norm': '24.25', 'counters/examples': 198144, 'counters/updates': 6192}
train stats after 198176 examples: {'rewards_train/chosen': '0.19866', 'rewards_train/rejected': '0.06401', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13465', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-177.29', 'loss/train': '0.64272', 'examples_per_second': '31.558', 'grad_norm': '25.25', 'counters/examples': 198176, 'counters/updates': 6193}
train stats after 198208 examples: {'rewards_train/chosen': '0.094368', 'rewards_train/rejected': '0.014687', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079681', 'logps_train/rejected': '-90.707', 'logps_train/chosen': '-161.63', 'loss/train': '0.66601', 'examples_per_second': '32.348', 'grad_norm': '24.125', 'counters/examples': 198208, 'counters/updates': 6194}
skipping logging after 198240 examples to avoid logging too frequently
train stats after 198272 examples: {'rewards_train/chosen': '0.054943', 'rewards_train/rejected': '-0.013534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068477', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-147.62', 'loss/train': '0.67026', 'examples_per_second': '33.219', 'grad_norm': '26.75', 'counters/examples': 198272, 'counters/updates': 6196}
train stats after 198304 examples: {'rewards_train/chosen': '0.10713', 'rewards_train/rejected': '-0.0020282', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10916', 'logps_train/rejected': '-99.345', 'logps_train/chosen': '-112.4', 'loss/train': '0.65489', 'examples_per_second': '31.52', 'grad_norm': '25.5', 'counters/examples': 198304, 'counters/updates': 6197}
train stats after 198336 examples: {'rewards_train/chosen': '0.13095', 'rewards_train/rejected': '0.028091', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10286', 'logps_train/rejected': '-119.66', 'logps_train/chosen': '-120.95', 'loss/train': '0.65081', 'examples_per_second': '30.592', 'grad_norm': '26.625', 'counters/examples': 198336, 'counters/updates': 6198}
train stats after 198368 examples: {'rewards_train/chosen': '0.14363', 'rewards_train/rejected': '0.053654', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089972', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-140.01', 'loss/train': '0.65939', 'examples_per_second': '31.589', 'grad_norm': '25.25', 'counters/examples': 198368, 'counters/updates': 6199}
train stats after 198400 examples: {'rewards_train/chosen': '0.11433', 'rewards_train/rejected': '0.0094021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10493', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-154.67', 'loss/train': '0.65808', 'examples_per_second': '30.65', 'grad_norm': '25', 'counters/examples': 198400, 'counters/updates': 6200}
train stats after 198432 examples: {'rewards_train/chosen': '0.20125', 'rewards_train/rejected': '0.03054', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17071', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-144.03', 'loss/train': '0.62372', 'examples_per_second': '31.549', 'grad_norm': '25.5', 'counters/examples': 198432, 'counters/updates': 6201}
train stats after 198464 examples: {'rewards_train/chosen': '0.14001', 'rewards_train/rejected': '-0.080619', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22063', 'logps_train/rejected': '-96.747', 'logps_train/chosen': '-125.33', 'loss/train': '0.60046', 'examples_per_second': '32.629', 'grad_norm': '22.5', 'counters/examples': 198464, 'counters/updates': 6202}
train stats after 198496 examples: {'rewards_train/chosen': '0.15874', 'rewards_train/rejected': '0.022844', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1359', 'logps_train/rejected': '-146.03', 'logps_train/chosen': '-139.96', 'loss/train': '0.64149', 'examples_per_second': '30.826', 'grad_norm': '25.625', 'counters/examples': 198496, 'counters/updates': 6203}
train stats after 198528 examples: {'rewards_train/chosen': '0.081496', 'rewards_train/rejected': '-0.016441', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097937', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-113.05', 'loss/train': '0.65538', 'examples_per_second': '31.44', 'grad_norm': '23.125', 'counters/examples': 198528, 'counters/updates': 6204}
train stats after 198560 examples: {'rewards_train/chosen': '0.11504', 'rewards_train/rejected': '0.030038', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085004', 'logps_train/rejected': '-103.6', 'logps_train/chosen': '-121.25', 'loss/train': '0.66715', 'examples_per_second': '31.657', 'grad_norm': '26.5', 'counters/examples': 198560, 'counters/updates': 6205}
train stats after 198592 examples: {'rewards_train/chosen': '0.21496', 'rewards_train/rejected': '0.11294', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10202', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-162.97', 'loss/train': '0.65089', 'examples_per_second': '30.482', 'grad_norm': '26.375', 'counters/examples': 198592, 'counters/updates': 6206}
train stats after 198624 examples: {'rewards_train/chosen': '0.14277', 'rewards_train/rejected': '-0.067235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21001', 'logps_train/rejected': '-121.38', 'logps_train/chosen': '-126.54', 'loss/train': '0.61149', 'examples_per_second': '32.027', 'grad_norm': '24.625', 'counters/examples': 198624, 'counters/updates': 6207}
train stats after 198656 examples: {'rewards_train/chosen': '0.10134', 'rewards_train/rejected': '-0.0025742', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10392', 'logps_train/rejected': '-103.99', 'logps_train/chosen': '-150.78', 'loss/train': '0.65553', 'examples_per_second': '31.531', 'grad_norm': '25.625', 'counters/examples': 198656, 'counters/updates': 6208}
train stats after 198688 examples: {'rewards_train/chosen': '0.17454', 'rewards_train/rejected': '0.032389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14215', 'logps_train/rejected': '-138.88', 'logps_train/chosen': '-136', 'loss/train': '0.64204', 'examples_per_second': '30.394', 'grad_norm': '24.125', 'counters/examples': 198688, 'counters/updates': 6209}
train stats after 198720 examples: {'rewards_train/chosen': '0.071199', 'rewards_train/rejected': '0.047953', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023246', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-169.77', 'loss/train': '0.69896', 'examples_per_second': '31.397', 'grad_norm': '28.875', 'counters/examples': 198720, 'counters/updates': 6210}
train stats after 198752 examples: {'rewards_train/chosen': '0.15313', 'rewards_train/rejected': '0.19265', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.039522', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-139.27', 'loss/train': '0.72419', 'examples_per_second': '31.552', 'grad_norm': '29.875', 'counters/examples': 198752, 'counters/updates': 6211}
skipping logging after 198784 examples to avoid logging too frequently
train stats after 198816 examples: {'rewards_train/chosen': '0.11012', 'rewards_train/rejected': '-0.0022283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11235', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-171.14', 'loss/train': '0.6472', 'examples_per_second': '32.859', 'grad_norm': '28.75', 'counters/examples': 198816, 'counters/updates': 6213}
train stats after 198848 examples: {'rewards_train/chosen': '0.18623', 'rewards_train/rejected': '0.050066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13616', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-131.06', 'loss/train': '0.63705', 'examples_per_second': '31.512', 'grad_norm': '24.25', 'counters/examples': 198848, 'counters/updates': 6214}
train stats after 198880 examples: {'rewards_train/chosen': '0.14314', 'rewards_train/rejected': '0.00032063', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14282', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-137.46', 'loss/train': '0.65364', 'examples_per_second': '31.031', 'grad_norm': '27', 'counters/examples': 198880, 'counters/updates': 6215}
train stats after 198912 examples: {'rewards_train/chosen': '0.15234', 'rewards_train/rejected': '0.049558', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10278', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-134.97', 'loss/train': '0.65982', 'examples_per_second': '31.853', 'grad_norm': '25', 'counters/examples': 198912, 'counters/updates': 6216}
skipping logging after 198944 examples to avoid logging too frequently
train stats after 198976 examples: {'rewards_train/chosen': '0.15855', 'rewards_train/rejected': '-0.0021766', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16073', 'logps_train/rejected': '-106.25', 'logps_train/chosen': '-113.81', 'loss/train': '0.62844', 'examples_per_second': '31.346', 'grad_norm': '24.5', 'counters/examples': 198976, 'counters/updates': 6218}
train stats after 199008 examples: {'rewards_train/chosen': '0.083692', 'rewards_train/rejected': '0.043534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040158', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-137.61', 'loss/train': '0.68278', 'examples_per_second': '32.652', 'grad_norm': '26', 'counters/examples': 199008, 'counters/updates': 6219}
train stats after 199040 examples: {'rewards_train/chosen': '0.14827', 'rewards_train/rejected': '0.022079', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12619', 'logps_train/rejected': '-134.74', 'logps_train/chosen': '-184.93', 'loss/train': '0.64581', 'examples_per_second': '31.56', 'grad_norm': '29.25', 'counters/examples': 199040, 'counters/updates': 6220}
train stats after 199072 examples: {'rewards_train/chosen': '0.12751', 'rewards_train/rejected': '-0.01063', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13814', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-190.39', 'loss/train': '0.63901', 'examples_per_second': '31.558', 'grad_norm': '26.375', 'counters/examples': 199072, 'counters/updates': 6221}
train stats after 199104 examples: {'rewards_train/chosen': '0.15802', 'rewards_train/rejected': '0.038351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11967', 'logps_train/rejected': '-126.1', 'logps_train/chosen': '-175.48', 'loss/train': '0.64764', 'examples_per_second': '31.314', 'grad_norm': '27.5', 'counters/examples': 199104, 'counters/updates': 6222}
skipping logging after 199136 examples to avoid logging too frequently
train stats after 199168 examples: {'rewards_train/chosen': '0.14352', 'rewards_train/rejected': '-0.047399', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19092', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-137.32', 'loss/train': '0.61898', 'examples_per_second': '35.014', 'grad_norm': '23.875', 'counters/examples': 199168, 'counters/updates': 6224}
train stats after 199200 examples: {'rewards_train/chosen': '0.10046', 'rewards_train/rejected': '-0.018819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11928', 'logps_train/rejected': '-101.32', 'logps_train/chosen': '-115.09', 'loss/train': '0.64836', 'examples_per_second': '31.553', 'grad_norm': '24.625', 'counters/examples': 199200, 'counters/updates': 6225}
skipping logging after 199232 examples to avoid logging too frequently
train stats after 199264 examples: {'rewards_train/chosen': '0.11129', 'rewards_train/rejected': '-0.00038401', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11167', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-147.36', 'loss/train': '0.65272', 'examples_per_second': '31.06', 'grad_norm': '29', 'counters/examples': 199264, 'counters/updates': 6227}
skipping logging after 199296 examples to avoid logging too frequently
train stats after 199328 examples: {'rewards_train/chosen': '0.12813', 'rewards_train/rejected': '-0.0015284', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12966', 'logps_train/rejected': '-166.49', 'logps_train/chosen': '-172.36', 'loss/train': '0.64291', 'examples_per_second': '31.646', 'grad_norm': '29', 'counters/examples': 199328, 'counters/updates': 6229}
train stats after 199360 examples: {'rewards_train/chosen': '0.15179', 'rewards_train/rejected': '0.063542', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088243', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-133.4', 'loss/train': '0.66145', 'examples_per_second': '30.686', 'grad_norm': '22.25', 'counters/examples': 199360, 'counters/updates': 6230}
train stats after 199392 examples: {'rewards_train/chosen': '0.15108', 'rewards_train/rejected': '0.0077223', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14336', 'logps_train/rejected': '-109.9', 'logps_train/chosen': '-120.54', 'loss/train': '0.63657', 'examples_per_second': '30.127', 'grad_norm': '22.5', 'counters/examples': 199392, 'counters/updates': 6231}
skipping logging after 199424 examples to avoid logging too frequently
train stats after 199456 examples: {'rewards_train/chosen': '0.093588', 'rewards_train/rejected': '0.027262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066325', 'logps_train/rejected': '-89.408', 'logps_train/chosen': '-110.21', 'loss/train': '0.66955', 'examples_per_second': '34.327', 'grad_norm': '23.375', 'counters/examples': 199456, 'counters/updates': 6233}
train stats after 199488 examples: {'rewards_train/chosen': '0.10754', 'rewards_train/rejected': '0.060377', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047164', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-121.83', 'loss/train': '0.68613', 'examples_per_second': '32.952', 'grad_norm': '34.75', 'counters/examples': 199488, 'counters/updates': 6234}
train stats after 199520 examples: {'rewards_train/chosen': '0.14672', 'rewards_train/rejected': '0.03713', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10959', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-156.79', 'loss/train': '0.65643', 'examples_per_second': '31.294', 'grad_norm': '25.875', 'counters/examples': 199520, 'counters/updates': 6235}
train stats after 199552 examples: {'rewards_train/chosen': '0.21009', 'rewards_train/rejected': '0.065707', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14439', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-158.93', 'loss/train': '0.63729', 'examples_per_second': '30.727', 'grad_norm': '28.25', 'counters/examples': 199552, 'counters/updates': 6236}
train stats after 199584 examples: {'rewards_train/chosen': '0.11269', 'rewards_train/rejected': '0.11186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00083181', 'logps_train/rejected': '-125.24', 'logps_train/chosen': '-126.31', 'loss/train': '0.70648', 'examples_per_second': '31.411', 'grad_norm': '28.25', 'counters/examples': 199584, 'counters/updates': 6237}
train stats after 199616 examples: {'rewards_train/chosen': '0.12472', 'rewards_train/rejected': '0.028397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096327', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-138.51', 'loss/train': '0.66028', 'examples_per_second': '32.754', 'grad_norm': '24.375', 'counters/examples': 199616, 'counters/updates': 6238}
skipping logging after 199648 examples to avoid logging too frequently
train stats after 199680 examples: {'rewards_train/chosen': '0.13804', 'rewards_train/rejected': '-0.0035694', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14161', 'logps_train/rejected': '-107.84', 'logps_train/chosen': '-131.48', 'loss/train': '0.63319', 'examples_per_second': '31.901', 'grad_norm': '21.5', 'counters/examples': 199680, 'counters/updates': 6240}
train stats after 199712 examples: {'rewards_train/chosen': '0.093409', 'rewards_train/rejected': '-0.026029', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11944', 'logps_train/rejected': '-121.39', 'logps_train/chosen': '-151.03', 'loss/train': '0.6473', 'examples_per_second': '31.548', 'grad_norm': '25.625', 'counters/examples': 199712, 'counters/updates': 6241}
train stats after 199744 examples: {'rewards_train/chosen': '0.22623', 'rewards_train/rejected': '0.086053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14018', 'logps_train/rejected': '-132.38', 'logps_train/chosen': '-144.2', 'loss/train': '0.63694', 'examples_per_second': '31.453', 'grad_norm': '26.875', 'counters/examples': 199744, 'counters/updates': 6242}
train stats after 199776 examples: {'rewards_train/chosen': '0.20508', 'rewards_train/rejected': '0.068066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13701', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-160.52', 'loss/train': '0.64043', 'examples_per_second': '31.548', 'grad_norm': '26.75', 'counters/examples': 199776, 'counters/updates': 6243}
train stats after 199808 examples: {'rewards_train/chosen': '0.13011', 'rewards_train/rejected': '-0.004232', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.13435', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-137.23', 'loss/train': '0.63771', 'examples_per_second': '31.59', 'grad_norm': '25', 'counters/examples': 199808, 'counters/updates': 6244}
train stats after 199840 examples: {'rewards_train/chosen': '0.11393', 'rewards_train/rejected': '0.021957', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091975', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-120.77', 'loss/train': '0.6556', 'examples_per_second': '31.264', 'grad_norm': '25.5', 'counters/examples': 199840, 'counters/updates': 6245}
skipping logging after 199872 examples to avoid logging too frequently
train stats after 199904 examples: {'rewards_train/chosen': '0.15468', 'rewards_train/rejected': '0.0032623', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15142', 'logps_train/rejected': '-110.64', 'logps_train/chosen': '-138.82', 'loss/train': '0.63585', 'examples_per_second': '29.871', 'grad_norm': '24.25', 'counters/examples': 199904, 'counters/updates': 6247}
train stats after 199936 examples: {'rewards_train/chosen': '0.16244', 'rewards_train/rejected': '0.041855', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12059', 'logps_train/rejected': '-129.32', 'logps_train/chosen': '-143.82', 'loss/train': '0.65035', 'examples_per_second': '31.558', 'grad_norm': '25.75', 'counters/examples': 199936, 'counters/updates': 6248}
train stats after 199968 examples: {'rewards_train/chosen': '0.11631', 'rewards_train/rejected': '0.082455', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033857', 'logps_train/rejected': '-119.2', 'logps_train/chosen': '-126.8', 'loss/train': '0.68149', 'examples_per_second': '30.575', 'grad_norm': '24.125', 'counters/examples': 199968, 'counters/updates': 6249}
train stats after 200000 examples: {'rewards_train/chosen': '0.036162', 'rewards_train/rejected': '-0.089738', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1259', 'logps_train/rejected': '-102.84', 'logps_train/chosen': '-144.31', 'loss/train': '0.6434', 'examples_per_second': '30.012', 'grad_norm': '23.125', 'counters/examples': 200000, 'counters/updates': 6250}
train stats after 200032 examples: {'rewards_train/chosen': '0.12502', 'rewards_train/rejected': '-0.029202', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15422', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-130.67', 'loss/train': '0.62915', 'examples_per_second': '31.775', 'grad_norm': '25', 'counters/examples': 200032, 'counters/updates': 6251}
train stats after 200064 examples: {'rewards_train/chosen': '0.12966', 'rewards_train/rejected': '-0.032073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16173', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-136.86', 'loss/train': '0.62574', 'examples_per_second': '33.127', 'grad_norm': '31.125', 'counters/examples': 200064, 'counters/updates': 6252}
train stats after 200096 examples: {'rewards_train/chosen': '0.16038', 'rewards_train/rejected': '0.020034', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14034', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-146.84', 'loss/train': '0.63425', 'examples_per_second': '31.566', 'grad_norm': '23.625', 'counters/examples': 200096, 'counters/updates': 6253}
train stats after 200128 examples: {'rewards_train/chosen': '0.17342', 'rewards_train/rejected': '0.02253', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15089', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-139.16', 'loss/train': '0.63368', 'examples_per_second': '31.547', 'grad_norm': '24.625', 'counters/examples': 200128, 'counters/updates': 6254}
train stats after 200160 examples: {'rewards_train/chosen': '0.19433', 'rewards_train/rejected': '-0.011547', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20588', 'logps_train/rejected': '-148.95', 'logps_train/chosen': '-185.2', 'loss/train': '0.6045', 'examples_per_second': '31.558', 'grad_norm': '27.125', 'counters/examples': 200160, 'counters/updates': 6255}
train stats after 200192 examples: {'rewards_train/chosen': '0.073446', 'rewards_train/rejected': '0.057018', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016428', 'logps_train/rejected': '-132.4', 'logps_train/chosen': '-165.78', 'loss/train': '0.70322', 'examples_per_second': '29.908', 'grad_norm': '30.75', 'counters/examples': 200192, 'counters/updates': 6256}
train stats after 200224 examples: {'rewards_train/chosen': '0.15023', 'rewards_train/rejected': '0.10954', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040687', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-137.5', 'loss/train': '0.6923', 'examples_per_second': '30.951', 'grad_norm': '27.875', 'counters/examples': 200224, 'counters/updates': 6257}
train stats after 200256 examples: {'rewards_train/chosen': '0.13557', 'rewards_train/rejected': '0.089686', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.045888', 'logps_train/rejected': '-166.88', 'logps_train/chosen': '-231.03', 'loss/train': '0.68055', 'examples_per_second': '30.061', 'grad_norm': '32.25', 'counters/examples': 200256, 'counters/updates': 6258}
train stats after 200288 examples: {'rewards_train/chosen': '0.15627', 'rewards_train/rejected': '0.00039235', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15588', 'logps_train/rejected': '-102.12', 'logps_train/chosen': '-142.41', 'loss/train': '0.63428', 'examples_per_second': '30.187', 'grad_norm': '22.75', 'counters/examples': 200288, 'counters/updates': 6259}
train stats after 200320 examples: {'rewards_train/chosen': '0.22142', 'rewards_train/rejected': '0.02057', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20085', 'logps_train/rejected': '-132.11', 'logps_train/chosen': '-135.62', 'loss/train': '0.61668', 'examples_per_second': '30.946', 'grad_norm': '30.25', 'counters/examples': 200320, 'counters/updates': 6260}
skipping logging after 200352 examples to avoid logging too frequently
train stats after 200384 examples: {'rewards_train/chosen': '0.16197', 'rewards_train/rejected': '-0.011878', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17385', 'logps_train/rejected': '-102.22', 'logps_train/chosen': '-113.94', 'loss/train': '0.62', 'examples_per_second': '31.592', 'grad_norm': '21.5', 'counters/examples': 200384, 'counters/updates': 6262}
train stats after 200416 examples: {'rewards_train/chosen': '0.12249', 'rewards_train/rejected': '0.0078382', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11465', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-133.73', 'loss/train': '0.64789', 'examples_per_second': '31.589', 'grad_norm': '24.25', 'counters/examples': 200416, 'counters/updates': 6263}
train stats after 200448 examples: {'rewards_train/chosen': '0.080107', 'rewards_train/rejected': '-0.021448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10155', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-126.81', 'loss/train': '0.65802', 'examples_per_second': '30.078', 'grad_norm': '27.875', 'counters/examples': 200448, 'counters/updates': 6264}
skipping logging after 200480 examples to avoid logging too frequently
train stats after 200512 examples: {'rewards_train/chosen': '0.1808', 'rewards_train/rejected': '0.024666', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15613', 'logps_train/rejected': '-141.18', 'logps_train/chosen': '-157.48', 'loss/train': '0.62702', 'examples_per_second': '30.067', 'grad_norm': '26.125', 'counters/examples': 200512, 'counters/updates': 6266}
train stats after 200544 examples: {'rewards_train/chosen': '0.24775', 'rewards_train/rejected': '0.10345', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1443', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-146.19', 'loss/train': '0.64958', 'examples_per_second': '30.591', 'grad_norm': '26.5', 'counters/examples': 200544, 'counters/updates': 6267}
train stats after 200576 examples: {'rewards_train/chosen': '0.081243', 'rewards_train/rejected': '-0.022445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10369', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-141.18', 'loss/train': '0.65046', 'examples_per_second': '32.466', 'grad_norm': '27.25', 'counters/examples': 200576, 'counters/updates': 6268}
skipping logging after 200608 examples to avoid logging too frequently
train stats after 200640 examples: {'rewards_train/chosen': '0.066844', 'rewards_train/rejected': '-0.029921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096766', 'logps_train/rejected': '-94.599', 'logps_train/chosen': '-114.5', 'loss/train': '0.6573', 'examples_per_second': '32.225', 'grad_norm': '31.625', 'counters/examples': 200640, 'counters/updates': 6270}
train stats after 200672 examples: {'rewards_train/chosen': '0.072912', 'rewards_train/rejected': '-0.03378', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10669', 'logps_train/rejected': '-151.55', 'logps_train/chosen': '-140.08', 'loss/train': '0.66783', 'examples_per_second': '31.495', 'grad_norm': '31.75', 'counters/examples': 200672, 'counters/updates': 6271}
train stats after 200704 examples: {'rewards_train/chosen': '0.17454', 'rewards_train/rejected': '0.2036', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029058', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-130.15', 'loss/train': '0.718', 'examples_per_second': '30.606', 'grad_norm': '26.875', 'counters/examples': 200704, 'counters/updates': 6272}
train stats after 200736 examples: {'rewards_train/chosen': '0.072541', 'rewards_train/rejected': '0.029215', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043326', 'logps_train/rejected': '-135.11', 'logps_train/chosen': '-139.6', 'loss/train': '0.6808', 'examples_per_second': '32.826', 'grad_norm': '28', 'counters/examples': 200736, 'counters/updates': 6273}
train stats after 200768 examples: {'rewards_train/chosen': '0.14449', 'rewards_train/rejected': '0.093648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050841', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-162.9', 'loss/train': '0.6836', 'examples_per_second': '32.051', 'grad_norm': '29.125', 'counters/examples': 200768, 'counters/updates': 6274}
train stats after 200800 examples: {'rewards_train/chosen': '0.151', 'rewards_train/rejected': '0.11153', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039462', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-157.63', 'loss/train': '0.68599', 'examples_per_second': '30.392', 'grad_norm': '33.75', 'counters/examples': 200800, 'counters/updates': 6275}
train stats after 200832 examples: {'rewards_train/chosen': '0.23272', 'rewards_train/rejected': '0.0973', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13542', 'logps_train/rejected': '-119.88', 'logps_train/chosen': '-120.13', 'loss/train': '0.63705', 'examples_per_second': '32.446', 'grad_norm': '22.25', 'counters/examples': 200832, 'counters/updates': 6276}
train stats after 200864 examples: {'rewards_train/chosen': '0.11753', 'rewards_train/rejected': '-0.040728', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15826', 'logps_train/rejected': '-110.39', 'logps_train/chosen': '-131.12', 'loss/train': '0.63472', 'examples_per_second': '30.407', 'grad_norm': '24.25', 'counters/examples': 200864, 'counters/updates': 6277}
train stats after 200896 examples: {'rewards_train/chosen': '0.12964', 'rewards_train/rejected': '-0.032978', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16262', 'logps_train/rejected': '-92.018', 'logps_train/chosen': '-154.54', 'loss/train': '0.62605', 'examples_per_second': '24.761', 'grad_norm': '26.25', 'counters/examples': 200896, 'counters/updates': 6278}
train stats after 200928 examples: {'rewards_train/chosen': '0.12134', 'rewards_train/rejected': '0.023034', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.098308', 'logps_train/rejected': '-129.64', 'logps_train/chosen': '-153.73', 'loss/train': '0.66172', 'examples_per_second': '31.344', 'grad_norm': '27.875', 'counters/examples': 200928, 'counters/updates': 6279}
train stats after 200960 examples: {'rewards_train/chosen': '0.059269', 'rewards_train/rejected': '0.054276', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.004993', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-144.16', 'loss/train': '0.70892', 'examples_per_second': '31.511', 'grad_norm': '103.5', 'counters/examples': 200960, 'counters/updates': 6280}
skipping logging after 200992 examples to avoid logging too frequently
train stats after 201024 examples: {'rewards_train/chosen': '0.13332', 'rewards_train/rejected': '-0.014694', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14801', 'logps_train/rejected': '-94.619', 'logps_train/chosen': '-130.32', 'loss/train': '0.63614', 'examples_per_second': '31.546', 'grad_norm': '23.5', 'counters/examples': 201024, 'counters/updates': 6282}
skipping logging after 201056 examples to avoid logging too frequently
train stats after 201088 examples: {'rewards_train/chosen': '0.065879', 'rewards_train/rejected': '0.022017', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043862', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-164.1', 'loss/train': '0.6875', 'examples_per_second': '31.638', 'grad_norm': '26.25', 'counters/examples': 201088, 'counters/updates': 6284}
train stats after 201120 examples: {'rewards_train/chosen': '0.12516', 'rewards_train/rejected': '-0.0032759', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12844', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-126.77', 'loss/train': '0.63858', 'examples_per_second': '30.005', 'grad_norm': '25.375', 'counters/examples': 201120, 'counters/updates': 6285}
train stats after 201152 examples: {'rewards_train/chosen': '0.14852', 'rewards_train/rejected': '-0.052165', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20069', 'logps_train/rejected': '-88.351', 'logps_train/chosen': '-117.03', 'loss/train': '0.61104', 'examples_per_second': '32.08', 'grad_norm': '21.125', 'counters/examples': 201152, 'counters/updates': 6286}
train stats after 201184 examples: {'rewards_train/chosen': '0.084742', 'rewards_train/rejected': '0.014516', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070226', 'logps_train/rejected': '-126.15', 'logps_train/chosen': '-134.36', 'loss/train': '0.66719', 'examples_per_second': '31.319', 'grad_norm': '27.5', 'counters/examples': 201184, 'counters/updates': 6287}
train stats after 201216 examples: {'rewards_train/chosen': '0.16437', 'rewards_train/rejected': '0.011489', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15288', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-191.12', 'loss/train': '0.63443', 'examples_per_second': '30.574', 'grad_norm': '26.375', 'counters/examples': 201216, 'counters/updates': 6288}
train stats after 201248 examples: {'rewards_train/chosen': '0.053992', 'rewards_train/rejected': '0.10305', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.049058', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-135.41', 'loss/train': '0.72433', 'examples_per_second': '31.831', 'grad_norm': '26.5', 'counters/examples': 201248, 'counters/updates': 6289}
train stats after 201280 examples: {'rewards_train/chosen': '0.20058', 'rewards_train/rejected': '0.0097657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19082', 'logps_train/rejected': '-110.96', 'logps_train/chosen': '-119.18', 'loss/train': '0.61445', 'examples_per_second': '31.648', 'grad_norm': '20.5', 'counters/examples': 201280, 'counters/updates': 6290}
train stats after 201312 examples: {'rewards_train/chosen': '0.061293', 'rewards_train/rejected': '0.10362', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.042327', 'logps_train/rejected': '-140.89', 'logps_train/chosen': '-129.38', 'loss/train': '0.72309', 'examples_per_second': '30.384', 'grad_norm': '28.625', 'counters/examples': 201312, 'counters/updates': 6291}
train stats after 201344 examples: {'rewards_train/chosen': '0.17286', 'rewards_train/rejected': '-0.036749', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20961', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-150.97', 'loss/train': '0.6029', 'examples_per_second': '33.058', 'grad_norm': '26.875', 'counters/examples': 201344, 'counters/updates': 6292}
train stats after 201376 examples: {'rewards_train/chosen': '0.1642', 'rewards_train/rejected': '0.063357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10085', 'logps_train/rejected': '-154.21', 'logps_train/chosen': '-154.56', 'loss/train': '0.66357', 'examples_per_second': '30.054', 'grad_norm': '29.75', 'counters/examples': 201376, 'counters/updates': 6293}
train stats after 201408 examples: {'rewards_train/chosen': '0.081696', 'rewards_train/rejected': '0.034609', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047088', 'logps_train/rejected': '-125.2', 'logps_train/chosen': '-160.06', 'loss/train': '0.68171', 'examples_per_second': '31.381', 'grad_norm': '27.125', 'counters/examples': 201408, 'counters/updates': 6294}
train stats after 201440 examples: {'rewards_train/chosen': '0.15926', 'rewards_train/rejected': '0.026274', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13298', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-151.23', 'loss/train': '0.64382', 'examples_per_second': '30.909', 'grad_norm': '25.125', 'counters/examples': 201440, 'counters/updates': 6295}
train stats after 201472 examples: {'rewards_train/chosen': '0.08372', 'rewards_train/rejected': '0.019595', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064125', 'logps_train/rejected': '-126.11', 'logps_train/chosen': '-158.73', 'loss/train': '0.66881', 'examples_per_second': '31.225', 'grad_norm': '26.25', 'counters/examples': 201472, 'counters/updates': 6296}
train stats after 201504 examples: {'rewards_train/chosen': '0.12937', 'rewards_train/rejected': '0.074339', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05503', 'logps_train/rejected': '-95.696', 'logps_train/chosen': '-121.98', 'loss/train': '0.6763', 'examples_per_second': '30.124', 'grad_norm': '22.25', 'counters/examples': 201504, 'counters/updates': 6297}
train stats after 201536 examples: {'rewards_train/chosen': '0.20015', 'rewards_train/rejected': '0.035558', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1646', 'logps_train/rejected': '-102.44', 'logps_train/chosen': '-151.26', 'loss/train': '0.62646', 'examples_per_second': '30.15', 'grad_norm': '22.75', 'counters/examples': 201536, 'counters/updates': 6298}
train stats after 201568 examples: {'rewards_train/chosen': '0.12597', 'rewards_train/rejected': '0.069528', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056443', 'logps_train/rejected': '-153.95', 'logps_train/chosen': '-153.08', 'loss/train': '0.67877', 'examples_per_second': '31.416', 'grad_norm': '31.25', 'counters/examples': 201568, 'counters/updates': 6299}
train stats after 201600 examples: {'rewards_train/chosen': '0.16914', 'rewards_train/rejected': '0.037578', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13156', 'logps_train/rejected': '-144.53', 'logps_train/chosen': '-123.54', 'loss/train': '0.64043', 'examples_per_second': '31.614', 'grad_norm': '24', 'counters/examples': 201600, 'counters/updates': 6300}
train stats after 201632 examples: {'rewards_train/chosen': '0.19252', 'rewards_train/rejected': '0.015838', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17668', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-162.18', 'loss/train': '0.62822', 'examples_per_second': '30.976', 'grad_norm': '39.25', 'counters/examples': 201632, 'counters/updates': 6301}
train stats after 201664 examples: {'rewards_train/chosen': '0.092969', 'rewards_train/rejected': '-0.026967', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11994', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-136.32', 'loss/train': '0.64287', 'examples_per_second': '31.618', 'grad_norm': '28', 'counters/examples': 201664, 'counters/updates': 6302}
train stats after 201696 examples: {'rewards_train/chosen': '0.15908', 'rewards_train/rejected': '0.077158', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081925', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-106.58', 'loss/train': '0.66524', 'examples_per_second': '31.606', 'grad_norm': '27.375', 'counters/examples': 201696, 'counters/updates': 6303}
train stats after 201728 examples: {'rewards_train/chosen': '0.16154', 'rewards_train/rejected': '0.0093263', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15221', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-136.25', 'loss/train': '0.62647', 'examples_per_second': '32.134', 'grad_norm': '24', 'counters/examples': 201728, 'counters/updates': 6304}
train stats after 201760 examples: {'rewards_train/chosen': '0.12831', 'rewards_train/rejected': '0.044555', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083753', 'logps_train/rejected': '-96.078', 'logps_train/chosen': '-115.66', 'loss/train': '0.66671', 'examples_per_second': '31.545', 'grad_norm': '21.625', 'counters/examples': 201760, 'counters/updates': 6305}
train stats after 201792 examples: {'rewards_train/chosen': '0.085793', 'rewards_train/rejected': '0.025427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060366', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-109.46', 'loss/train': '0.67738', 'examples_per_second': '32.974', 'grad_norm': '24.5', 'counters/examples': 201792, 'counters/updates': 6306}
train stats after 201824 examples: {'rewards_train/chosen': '0.040797', 'rewards_train/rejected': '0.066975', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026178', 'logps_train/rejected': '-115.15', 'logps_train/chosen': '-151.18', 'loss/train': '0.71386', 'examples_per_second': '32.526', 'grad_norm': '28', 'counters/examples': 201824, 'counters/updates': 6307}
train stats after 201856 examples: {'rewards_train/chosen': '0.12479', 'rewards_train/rejected': '-0.020444', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14523', 'logps_train/rejected': '-105.13', 'logps_train/chosen': '-133.86', 'loss/train': '0.63581', 'examples_per_second': '30.1', 'grad_norm': '21.875', 'counters/examples': 201856, 'counters/updates': 6308}
train stats after 201888 examples: {'rewards_train/chosen': '0.096806', 'rewards_train/rejected': '0.057905', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038901', 'logps_train/rejected': '-95.085', 'logps_train/chosen': '-135.65', 'loss/train': '0.68234', 'examples_per_second': '30.703', 'grad_norm': '23.25', 'counters/examples': 201888, 'counters/updates': 6309}
train stats after 201920 examples: {'rewards_train/chosen': '0.20396', 'rewards_train/rejected': '0.034811', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16914', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-153', 'loss/train': '0.62372', 'examples_per_second': '32.532', 'grad_norm': '27.375', 'counters/examples': 201920, 'counters/updates': 6310}
train stats after 201952 examples: {'rewards_train/chosen': '0.17137', 'rewards_train/rejected': '-0.021404', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19278', 'logps_train/rejected': '-98.654', 'logps_train/chosen': '-148.2', 'loss/train': '0.61845', 'examples_per_second': '32.398', 'grad_norm': '24.5', 'counters/examples': 201952, 'counters/updates': 6311}
train stats after 201984 examples: {'rewards_train/chosen': '0.12411', 'rewards_train/rejected': '0.057633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066481', 'logps_train/rejected': '-118.77', 'logps_train/chosen': '-130.1', 'loss/train': '0.67007', 'examples_per_second': '32.142', 'grad_norm': '25.5', 'counters/examples': 201984, 'counters/updates': 6312}
skipping logging after 202016 examples to avoid logging too frequently
train stats after 202048 examples: {'rewards_train/chosen': '0.10871', 'rewards_train/rejected': '-0.0079821', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11669', 'logps_train/rejected': '-99.047', 'logps_train/chosen': '-102.35', 'loss/train': '0.64706', 'examples_per_second': '32.406', 'grad_norm': '20.875', 'counters/examples': 202048, 'counters/updates': 6314}
train stats after 202080 examples: {'rewards_train/chosen': '0.070769', 'rewards_train/rejected': '0.057803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012966', 'logps_train/rejected': '-175.24', 'logps_train/chosen': '-143.58', 'loss/train': '0.69945', 'examples_per_second': '31.623', 'grad_norm': '35.75', 'counters/examples': 202080, 'counters/updates': 6315}
train stats after 202112 examples: {'rewards_train/chosen': '0.084035', 'rewards_train/rejected': '0.10734', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023308', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-126.09', 'loss/train': '0.71736', 'examples_per_second': '31.476', 'grad_norm': '41.75', 'counters/examples': 202112, 'counters/updates': 6316}
skipping logging after 202144 examples to avoid logging too frequently
train stats after 202176 examples: {'rewards_train/chosen': '0.083734', 'rewards_train/rejected': '-0.021451', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10518', 'logps_train/rejected': '-97.727', 'logps_train/chosen': '-105.42', 'loss/train': '0.65708', 'examples_per_second': '31.65', 'grad_norm': '24.875', 'counters/examples': 202176, 'counters/updates': 6318}
train stats after 202208 examples: {'rewards_train/chosen': '0.13995', 'rewards_train/rejected': '-0.033031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17298', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-131.65', 'loss/train': '0.61908', 'examples_per_second': '30.261', 'grad_norm': '28.875', 'counters/examples': 202208, 'counters/updates': 6319}
train stats after 202240 examples: {'rewards_train/chosen': '0.097603', 'rewards_train/rejected': '0.026726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070877', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-127.38', 'loss/train': '0.6701', 'examples_per_second': '31.646', 'grad_norm': '27', 'counters/examples': 202240, 'counters/updates': 6320}
train stats after 202272 examples: {'rewards_train/chosen': '0.1945', 'rewards_train/rejected': '0.04292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15158', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-153.09', 'loss/train': '0.63232', 'examples_per_second': '32.192', 'grad_norm': '24.625', 'counters/examples': 202272, 'counters/updates': 6321}
train stats after 202304 examples: {'rewards_train/chosen': '0.23163', 'rewards_train/rejected': '0.067771', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16385', 'logps_train/rejected': '-176.81', 'logps_train/chosen': '-184.42', 'loss/train': '0.62612', 'examples_per_second': '31.584', 'grad_norm': '27.625', 'counters/examples': 202304, 'counters/updates': 6322}
train stats after 202336 examples: {'rewards_train/chosen': '0.12458', 'rewards_train/rejected': '0.047685', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076893', 'logps_train/rejected': '-141.44', 'logps_train/chosen': '-166.16', 'loss/train': '0.66499', 'examples_per_second': '24.441', 'grad_norm': '27.125', 'counters/examples': 202336, 'counters/updates': 6323}
train stats after 202368 examples: {'rewards_train/chosen': '0.13766', 'rewards_train/rejected': '0.13432', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0033399', 'logps_train/rejected': '-130.09', 'logps_train/chosen': '-145.8', 'loss/train': '0.70514', 'examples_per_second': '32.728', 'grad_norm': '29.125', 'counters/examples': 202368, 'counters/updates': 6324}
skipping logging after 202400 examples to avoid logging too frequently
train stats after 202432 examples: {'rewards_train/chosen': '0.18054', 'rewards_train/rejected': '-0.039955', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2205', 'logps_train/rejected': '-98.496', 'logps_train/chosen': '-139.28', 'loss/train': '0.61054', 'examples_per_second': '23.998', 'grad_norm': '23.625', 'counters/examples': 202432, 'counters/updates': 6326}
skipping logging after 202464 examples to avoid logging too frequently
train stats after 202496 examples: {'rewards_train/chosen': '0.044556', 'rewards_train/rejected': '-0.052619', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097175', 'logps_train/rejected': '-138.84', 'logps_train/chosen': '-126.38', 'loss/train': '0.65739', 'examples_per_second': '32.916', 'grad_norm': '25', 'counters/examples': 202496, 'counters/updates': 6328}
train stats after 202528 examples: {'rewards_train/chosen': '0.11463', 'rewards_train/rejected': '0.014035', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10059', 'logps_train/rejected': '-101.77', 'logps_train/chosen': '-129.96', 'loss/train': '0.65942', 'examples_per_second': '32.38', 'grad_norm': '25.25', 'counters/examples': 202528, 'counters/updates': 6329}
train stats after 202560 examples: {'rewards_train/chosen': '0.1069', 'rewards_train/rejected': '0.042895', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064006', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-135.68', 'loss/train': '0.68405', 'examples_per_second': '31.569', 'grad_norm': '28', 'counters/examples': 202560, 'counters/updates': 6330}
skipping logging after 202592 examples to avoid logging too frequently
train stats after 202624 examples: {'rewards_train/chosen': '0.11642', 'rewards_train/rejected': '0.053687', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062733', 'logps_train/rejected': '-161.21', 'logps_train/chosen': '-127.91', 'loss/train': '0.67504', 'examples_per_second': '32.15', 'grad_norm': '26.875', 'counters/examples': 202624, 'counters/updates': 6332}
train stats after 202656 examples: {'rewards_train/chosen': '0.12123', 'rewards_train/rejected': '-0.001723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12295', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-106.92', 'loss/train': '0.64344', 'examples_per_second': '32.566', 'grad_norm': '21.25', 'counters/examples': 202656, 'counters/updates': 6333}
train stats after 202688 examples: {'rewards_train/chosen': '0.10184', 'rewards_train/rejected': '-0.013146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11499', 'logps_train/rejected': '-119.62', 'logps_train/chosen': '-138.34', 'loss/train': '0.65093', 'examples_per_second': '31.639', 'grad_norm': '25.375', 'counters/examples': 202688, 'counters/updates': 6334}
train stats after 202720 examples: {'rewards_train/chosen': '0.11451', 'rewards_train/rejected': '0.03899', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075515', 'logps_train/rejected': '-99.204', 'logps_train/chosen': '-123.66', 'loss/train': '0.66221', 'examples_per_second': '32.054', 'grad_norm': '23.25', 'counters/examples': 202720, 'counters/updates': 6335}
train stats after 202752 examples: {'rewards_train/chosen': '0.11293', 'rewards_train/rejected': '0.056923', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05601', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-141.66', 'loss/train': '0.67779', 'examples_per_second': '31.643', 'grad_norm': '26.5', 'counters/examples': 202752, 'counters/updates': 6336}
train stats after 202784 examples: {'rewards_train/chosen': '0.13381', 'rewards_train/rejected': '0.04231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091499', 'logps_train/rejected': '-155.22', 'logps_train/chosen': '-140.81', 'loss/train': '0.65531', 'examples_per_second': '31.534', 'grad_norm': '30.75', 'counters/examples': 202784, 'counters/updates': 6337}
train stats after 202816 examples: {'rewards_train/chosen': '0.12442', 'rewards_train/rejected': '-0.052276', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17669', 'logps_train/rejected': '-123.79', 'logps_train/chosen': '-155.09', 'loss/train': '0.6272', 'examples_per_second': '32.761', 'grad_norm': '23.375', 'counters/examples': 202816, 'counters/updates': 6338}
skipping logging after 202848 examples to avoid logging too frequently
train stats after 202880 examples: {'rewards_train/chosen': '0.12536', 'rewards_train/rejected': '0.080658', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044702', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-130.2', 'loss/train': '0.68008', 'examples_per_second': '33.968', 'grad_norm': '27.25', 'counters/examples': 202880, 'counters/updates': 6340}
skipping logging after 202912 examples to avoid logging too frequently
train stats after 202944 examples: {'rewards_train/chosen': '0.079425', 'rewards_train/rejected': '-0.0047763', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084201', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-146.12', 'loss/train': '0.67548', 'examples_per_second': '30.915', 'grad_norm': '30.125', 'counters/examples': 202944, 'counters/updates': 6342}
train stats after 202976 examples: {'rewards_train/chosen': '0.11305', 'rewards_train/rejected': '0.03081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082242', 'logps_train/rejected': '-109.27', 'logps_train/chosen': '-139.37', 'loss/train': '0.66467', 'examples_per_second': '31.503', 'grad_norm': '26.75', 'counters/examples': 202976, 'counters/updates': 6343}
train stats after 203008 examples: {'rewards_train/chosen': '0.075286', 'rewards_train/rejected': '-0.035484', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11077', 'logps_train/rejected': '-113.21', 'logps_train/chosen': '-136.69', 'loss/train': '0.66621', 'examples_per_second': '32.732', 'grad_norm': '29.125', 'counters/examples': 203008, 'counters/updates': 6344}
train stats after 203040 examples: {'rewards_train/chosen': '0.22099', 'rewards_train/rejected': '0.016013', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20498', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-163.18', 'loss/train': '0.6099', 'examples_per_second': '30.152', 'grad_norm': '26.625', 'counters/examples': 203040, 'counters/updates': 6345}
train stats after 203072 examples: {'rewards_train/chosen': '0.011951', 'rewards_train/rejected': '0.03896', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02701', 'logps_train/rejected': '-102.58', 'logps_train/chosen': '-131.78', 'loss/train': '0.71618', 'examples_per_second': '30.698', 'grad_norm': '27.625', 'counters/examples': 203072, 'counters/updates': 6346}
train stats after 203104 examples: {'rewards_train/chosen': '0.097171', 'rewards_train/rejected': '0.074924', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022247', 'logps_train/rejected': '-142.53', 'logps_train/chosen': '-118.22', 'loss/train': '0.69424', 'examples_per_second': '31.001', 'grad_norm': '27.75', 'counters/examples': 203104, 'counters/updates': 6347}
train stats after 203136 examples: {'rewards_train/chosen': '0.14497', 'rewards_train/rejected': '-0.016604', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16158', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-166.57', 'loss/train': '0.63068', 'examples_per_second': '32.752', 'grad_norm': '25.375', 'counters/examples': 203136, 'counters/updates': 6348}
train stats after 203168 examples: {'rewards_train/chosen': '0.090441', 'rewards_train/rejected': '-0.080158', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1706', 'logps_train/rejected': '-77.877', 'logps_train/chosen': '-104.11', 'loss/train': '0.61888', 'examples_per_second': '31.082', 'grad_norm': '19.625', 'counters/examples': 203168, 'counters/updates': 6349}
train stats after 203200 examples: {'rewards_train/chosen': '0.133', 'rewards_train/rejected': '-0.013055', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14605', 'logps_train/rejected': '-132.36', 'logps_train/chosen': '-130.53', 'loss/train': '0.63476', 'examples_per_second': '31.621', 'grad_norm': '26.375', 'counters/examples': 203200, 'counters/updates': 6350}
train stats after 203232 examples: {'rewards_train/chosen': '0.20095', 'rewards_train/rejected': '0.093623', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10733', 'logps_train/rejected': '-178.83', 'logps_train/chosen': '-166.02', 'loss/train': '0.65179', 'examples_per_second': '31.63', 'grad_norm': '30.25', 'counters/examples': 203232, 'counters/updates': 6351}
train stats after 203264 examples: {'rewards_train/chosen': '0.11779', 'rewards_train/rejected': '0.061543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056249', 'logps_train/rejected': '-123.02', 'logps_train/chosen': '-150.82', 'loss/train': '0.6733', 'examples_per_second': '30.695', 'grad_norm': '25.375', 'counters/examples': 203264, 'counters/updates': 6352}
skipping logging after 203296 examples to avoid logging too frequently
train stats after 203328 examples: {'rewards_train/chosen': '0.042234', 'rewards_train/rejected': '-0.034885', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077119', 'logps_train/rejected': '-110.39', 'logps_train/chosen': '-127.9', 'loss/train': '0.66552', 'examples_per_second': '30.749', 'grad_norm': '22.25', 'counters/examples': 203328, 'counters/updates': 6354}
train stats after 203360 examples: {'rewards_train/chosen': '0.091675', 'rewards_train/rejected': '0.019816', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07186', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-126.46', 'loss/train': '0.66266', 'examples_per_second': '31.63', 'grad_norm': '26.125', 'counters/examples': 203360, 'counters/updates': 6355}
train stats after 203392 examples: {'rewards_train/chosen': '0.14488', 'rewards_train/rejected': '0.014637', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13025', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-117.68', 'loss/train': '0.64373', 'examples_per_second': '31.232', 'grad_norm': '21.875', 'counters/examples': 203392, 'counters/updates': 6356}
train stats after 203424 examples: {'rewards_train/chosen': '0.21345', 'rewards_train/rejected': '0.081194', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13226', 'logps_train/rejected': '-144.79', 'logps_train/chosen': '-158.17', 'loss/train': '0.63928', 'examples_per_second': '31.997', 'grad_norm': '30.5', 'counters/examples': 203424, 'counters/updates': 6357}
skipping logging after 203456 examples to avoid logging too frequently
train stats after 203488 examples: {'rewards_train/chosen': '0.14507', 'rewards_train/rejected': '0.048777', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096293', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-167.17', 'loss/train': '0.6671', 'examples_per_second': '31.365', 'grad_norm': '28', 'counters/examples': 203488, 'counters/updates': 6359}
train stats after 203520 examples: {'rewards_train/chosen': '0.080716', 'rewards_train/rejected': '-0.055696', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13641', 'logps_train/rejected': '-105.06', 'logps_train/chosen': '-118.68', 'loss/train': '0.64067', 'examples_per_second': '30.148', 'grad_norm': '24', 'counters/examples': 203520, 'counters/updates': 6360}
train stats after 203552 examples: {'rewards_train/chosen': '0.22142', 'rewards_train/rejected': '-0.023078', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2445', 'logps_train/rejected': '-108.84', 'logps_train/chosen': '-135.31', 'loss/train': '0.59637', 'examples_per_second': '30.094', 'grad_norm': '22.75', 'counters/examples': 203552, 'counters/updates': 6361}
train stats after 203584 examples: {'rewards_train/chosen': '0.1417', 'rewards_train/rejected': '0.16971', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.028002', 'logps_train/rejected': '-146.45', 'logps_train/chosen': '-178.73', 'loss/train': '0.71947', 'examples_per_second': '30.627', 'grad_norm': '29.75', 'counters/examples': 203584, 'counters/updates': 6362}
train stats after 203616 examples: {'rewards_train/chosen': '0.065969', 'rewards_train/rejected': '0.021603', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044366', 'logps_train/rejected': '-158.02', 'logps_train/chosen': '-152.89', 'loss/train': '0.68805', 'examples_per_second': '31.67', 'grad_norm': '27', 'counters/examples': 203616, 'counters/updates': 6363}
train stats after 203648 examples: {'rewards_train/chosen': '0.11414', 'rewards_train/rejected': '0.0062571', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10788', 'logps_train/rejected': '-112.08', 'logps_train/chosen': '-140.68', 'loss/train': '0.64643', 'examples_per_second': '31.566', 'grad_norm': '23.875', 'counters/examples': 203648, 'counters/updates': 6364}
train stats after 203680 examples: {'rewards_train/chosen': '0.11998', 'rewards_train/rejected': '-0.0001135', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1201', 'logps_train/rejected': '-162.39', 'logps_train/chosen': '-126.61', 'loss/train': '0.65091', 'examples_per_second': '29.984', 'grad_norm': '28', 'counters/examples': 203680, 'counters/updates': 6365}
skipping logging after 203712 examples to avoid logging too frequently
train stats after 203744 examples: {'rewards_train/chosen': '0.010926', 'rewards_train/rejected': '0.024356', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01343', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-123.96', 'loss/train': '0.71972', 'examples_per_second': '34.303', 'grad_norm': '28.5', 'counters/examples': 203744, 'counters/updates': 6367}
skipping logging after 203776 examples to avoid logging too frequently
train stats after 203808 examples: {'rewards_train/chosen': '0.13034', 'rewards_train/rejected': '-0.065018', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19535', 'logps_train/rejected': '-112.16', 'logps_train/chosen': '-133.81', 'loss/train': '0.61061', 'examples_per_second': '39.254', 'grad_norm': '21.375', 'counters/examples': 203808, 'counters/updates': 6369}
train stats after 203840 examples: {'rewards_train/chosen': '0.19711', 'rewards_train/rejected': '0.062176', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13493', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-125.19', 'loss/train': '0.63355', 'examples_per_second': '30.959', 'grad_norm': '26.25', 'counters/examples': 203840, 'counters/updates': 6370}
train stats after 203872 examples: {'rewards_train/chosen': '0.20799', 'rewards_train/rejected': '0.018035', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18996', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-150.7', 'loss/train': '0.62411', 'examples_per_second': '32.604', 'grad_norm': '26.375', 'counters/examples': 203872, 'counters/updates': 6371}
train stats after 203904 examples: {'rewards_train/chosen': '0.11905', 'rewards_train/rejected': '0.10095', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018106', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-155.02', 'loss/train': '0.69704', 'examples_per_second': '31.653', 'grad_norm': '27.875', 'counters/examples': 203904, 'counters/updates': 6372}
train stats after 203936 examples: {'rewards_train/chosen': '0.042066', 'rewards_train/rejected': '0.0038406', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038225', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-96.425', 'loss/train': '0.68625', 'examples_per_second': '32.646', 'grad_norm': '25', 'counters/examples': 203936, 'counters/updates': 6373}
skipping logging after 203968 examples to avoid logging too frequently
train stats after 204000 examples: {'rewards_train/chosen': '0.099779', 'rewards_train/rejected': '0.082432', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017347', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-124.6', 'loss/train': '0.70035', 'examples_per_second': '31.644', 'grad_norm': '26', 'counters/examples': 204000, 'counters/updates': 6375}
Running evaluation after 204000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 204000: {'rewards_eval/chosen': '0.15354', 'rewards_eval/rejected': '0.036232', 'rewards_eval/accuracies': '0.62109', 'rewards_eval/margins': '0.1173', 'logps_eval/rejected': '-118.25', 'logps_eval/chosen': '-137.9', 'loss/eval': '0.65066'}
skipping save for non epoch
train stats after 204032 examples: {'rewards_train/chosen': '0.11931', 'rewards_train/rejected': '0.043259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076055', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-165.47', 'loss/train': '0.66803', 'examples_per_second': '34.385', 'grad_norm': '25.5', 'counters/examples': 204032, 'counters/updates': 6376}
train stats after 204064 examples: {'rewards_train/chosen': '0.074152', 'rewards_train/rejected': '0.05023', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.023922', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-123.12', 'loss/train': '0.70015', 'examples_per_second': '31.724', 'grad_norm': '29.125', 'counters/examples': 204064, 'counters/updates': 6377}
train stats after 204096 examples: {'rewards_train/chosen': '0.094235', 'rewards_train/rejected': '0.057724', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036512', 'logps_train/rejected': '-121.93', 'logps_train/chosen': '-133.76', 'loss/train': '0.69004', 'examples_per_second': '32.155', 'grad_norm': '25.25', 'counters/examples': 204096, 'counters/updates': 6378}
train stats after 204128 examples: {'rewards_train/chosen': '0.11242', 'rewards_train/rejected': '-0.0089704', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12139', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-125.47', 'loss/train': '0.64341', 'examples_per_second': '33.122', 'grad_norm': '28.125', 'counters/examples': 204128, 'counters/updates': 6379}
skipping logging after 204160 examples to avoid logging too frequently
train stats after 204192 examples: {'rewards_train/chosen': '0.033256', 'rewards_train/rejected': '-0.025547', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058802', 'logps_train/rejected': '-111.8', 'logps_train/chosen': '-116.91', 'loss/train': '0.67583', 'examples_per_second': '31.583', 'grad_norm': '22.5', 'counters/examples': 204192, 'counters/updates': 6381}
train stats after 204224 examples: {'rewards_train/chosen': '0.14548', 'rewards_train/rejected': '0.0059209', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13956', 'logps_train/rejected': '-103.08', 'logps_train/chosen': '-125.12', 'loss/train': '0.63654', 'examples_per_second': '30.629', 'grad_norm': '22.375', 'counters/examples': 204224, 'counters/updates': 6382}
train stats after 204256 examples: {'rewards_train/chosen': '0.13882', 'rewards_train/rejected': '0.098115', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04071', 'logps_train/rejected': '-147.24', 'logps_train/chosen': '-164.35', 'loss/train': '0.69995', 'examples_per_second': '32.867', 'grad_norm': '29.625', 'counters/examples': 204256, 'counters/updates': 6383}
train stats after 204288 examples: {'rewards_train/chosen': '0.12907', 'rewards_train/rejected': '0.0011858', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12788', 'logps_train/rejected': '-152.56', 'logps_train/chosen': '-141.74', 'loss/train': '0.65084', 'examples_per_second': '32.631', 'grad_norm': '25.875', 'counters/examples': 204288, 'counters/updates': 6384}
train stats after 204320 examples: {'rewards_train/chosen': '0.21972', 'rewards_train/rejected': '0.050887', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16883', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-154.97', 'loss/train': '0.63079', 'examples_per_second': '31.472', 'grad_norm': '28.875', 'counters/examples': 204320, 'counters/updates': 6385}
train stats after 204352 examples: {'rewards_train/chosen': '0.1784', 'rewards_train/rejected': '0.057968', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12043', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-163.35', 'loss/train': '0.64294', 'examples_per_second': '31.675', 'grad_norm': '25.625', 'counters/examples': 204352, 'counters/updates': 6386}
skipping logging after 204384 examples to avoid logging too frequently
train stats after 204416 examples: {'rewards_train/chosen': '0.12758', 'rewards_train/rejected': '0.061307', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066276', 'logps_train/rejected': '-147.8', 'logps_train/chosen': '-158.99', 'loss/train': '0.67517', 'examples_per_second': '30.143', 'grad_norm': '29.625', 'counters/examples': 204416, 'counters/updates': 6388}
train stats after 204448 examples: {'rewards_train/chosen': '0.092522', 'rewards_train/rejected': '0.032622', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059899', 'logps_train/rejected': '-174.46', 'logps_train/chosen': '-155.65', 'loss/train': '0.69216', 'examples_per_second': '30.699', 'grad_norm': '29.25', 'counters/examples': 204448, 'counters/updates': 6389}
train stats after 204480 examples: {'rewards_train/chosen': '0.15208', 'rewards_train/rejected': '0.069688', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082388', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-156.35', 'loss/train': '0.66714', 'examples_per_second': '31.97', 'grad_norm': '26.875', 'counters/examples': 204480, 'counters/updates': 6390}
train stats after 204512 examples: {'rewards_train/chosen': '0.11473', 'rewards_train/rejected': '0.074982', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039751', 'logps_train/rejected': '-154.44', 'logps_train/chosen': '-123.76', 'loss/train': '0.68316', 'examples_per_second': '31.157', 'grad_norm': '25.5', 'counters/examples': 204512, 'counters/updates': 6391}
train stats after 204544 examples: {'rewards_train/chosen': '0.18705', 'rewards_train/rejected': '0.062399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12466', 'logps_train/rejected': '-112.69', 'logps_train/chosen': '-114.46', 'loss/train': '0.64582', 'examples_per_second': '30.478', 'grad_norm': '23.75', 'counters/examples': 204544, 'counters/updates': 6392}
skipping logging after 204576 examples to avoid logging too frequently
train stats after 204608 examples: {'rewards_train/chosen': '0.14366', 'rewards_train/rejected': '0.060849', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082807', 'logps_train/rejected': '-133.68', 'logps_train/chosen': '-153.74', 'loss/train': '0.6667', 'examples_per_second': '31.488', 'grad_norm': '27', 'counters/examples': 204608, 'counters/updates': 6394}
train stats after 204640 examples: {'rewards_train/chosen': '0.10889', 'rewards_train/rejected': '0.059757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04913', 'logps_train/rejected': '-122.32', 'logps_train/chosen': '-130.3', 'loss/train': '0.68051', 'examples_per_second': '31.207', 'grad_norm': '27.125', 'counters/examples': 204640, 'counters/updates': 6395}
train stats after 204672 examples: {'rewards_train/chosen': '0.11125', 'rewards_train/rejected': '0.046526', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064719', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-106.69', 'loss/train': '0.6702', 'examples_per_second': '31.629', 'grad_norm': '24', 'counters/examples': 204672, 'counters/updates': 6396}
train stats after 204704 examples: {'rewards_train/chosen': '0.11201', 'rewards_train/rejected': '0.012983', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099027', 'logps_train/rejected': '-145.19', 'logps_train/chosen': '-165.01', 'loss/train': '0.65554', 'examples_per_second': '31.606', 'grad_norm': '27.125', 'counters/examples': 204704, 'counters/updates': 6397}
train stats after 204736 examples: {'rewards_train/chosen': '0.1663', 'rewards_train/rejected': '-0.041823', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20812', 'logps_train/rejected': '-132.03', 'logps_train/chosen': '-144.22', 'loss/train': '0.6076', 'examples_per_second': '30.398', 'grad_norm': '30.75', 'counters/examples': 204736, 'counters/updates': 6398}
skipping logging after 204768 examples to avoid logging too frequently
train stats after 204800 examples: {'rewards_train/chosen': '0.05442', 'rewards_train/rejected': '0.06533', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.01091', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-131.98', 'loss/train': '0.70778', 'examples_per_second': '31.856', 'grad_norm': '29.5', 'counters/examples': 204800, 'counters/updates': 6400}
train stats after 204832 examples: {'rewards_train/chosen': '0.16412', 'rewards_train/rejected': '0.12904', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035079', 'logps_train/rejected': '-159.49', 'logps_train/chosen': '-167.05', 'loss/train': '0.68583', 'examples_per_second': '31.432', 'grad_norm': '31', 'counters/examples': 204832, 'counters/updates': 6401}
train stats after 204864 examples: {'rewards_train/chosen': '0.097897', 'rewards_train/rejected': '0.13141', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033513', 'logps_train/rejected': '-123.65', 'logps_train/chosen': '-151.73', 'loss/train': '0.71975', 'examples_per_second': '31.494', 'grad_norm': '30.75', 'counters/examples': 204864, 'counters/updates': 6402}
train stats after 204896 examples: {'rewards_train/chosen': '0.067102', 'rewards_train/rejected': '0.14203', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.07493', 'logps_train/rejected': '-99.522', 'logps_train/chosen': '-151.44', 'loss/train': '0.74554', 'examples_per_second': '30.561', 'grad_norm': '27.125', 'counters/examples': 204896, 'counters/updates': 6403}
train stats after 204928 examples: {'rewards_train/chosen': '0.16647', 'rewards_train/rejected': '-0.0077173', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17419', 'logps_train/rejected': '-100.9', 'logps_train/chosen': '-117.55', 'loss/train': '0.62105', 'examples_per_second': '31.041', 'grad_norm': '22.375', 'counters/examples': 204928, 'counters/updates': 6404}
train stats after 204960 examples: {'rewards_train/chosen': '0.098261', 'rewards_train/rejected': '-0.0031686', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10143', 'logps_train/rejected': '-119.12', 'logps_train/chosen': '-177.64', 'loss/train': '0.65606', 'examples_per_second': '31.786', 'grad_norm': '29.625', 'counters/examples': 204960, 'counters/updates': 6405}
train stats after 204992 examples: {'rewards_train/chosen': '0.16', 'rewards_train/rejected': '0.04887', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11113', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-151.18', 'loss/train': '0.65273', 'examples_per_second': '31.535', 'grad_norm': '27.75', 'counters/examples': 204992, 'counters/updates': 6406}
train stats after 205024 examples: {'rewards_train/chosen': '0.14815', 'rewards_train/rejected': '0.017647', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13051', 'logps_train/rejected': '-158.33', 'logps_train/chosen': '-139.28', 'loss/train': '0.6444', 'examples_per_second': '30.2', 'grad_norm': '26.875', 'counters/examples': 205024, 'counters/updates': 6407}
train stats after 205056 examples: {'rewards_train/chosen': '0.12797', 'rewards_train/rejected': '0.055932', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07204', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-138.5', 'loss/train': '0.66809', 'examples_per_second': '32.15', 'grad_norm': '34.75', 'counters/examples': 205056, 'counters/updates': 6408}
train stats after 205088 examples: {'rewards_train/chosen': '0.15005', 'rewards_train/rejected': '0.024028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12602', 'logps_train/rejected': '-125.44', 'logps_train/chosen': '-140.96', 'loss/train': '0.64302', 'examples_per_second': '32.733', 'grad_norm': '26.875', 'counters/examples': 205088, 'counters/updates': 6409}
train stats after 205120 examples: {'rewards_train/chosen': '0.13279', 'rewards_train/rejected': '0.023129', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10966', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-131.34', 'loss/train': '0.65034', 'examples_per_second': '31.118', 'grad_norm': '23.125', 'counters/examples': 205120, 'counters/updates': 6410}
skipping logging after 205152 examples to avoid logging too frequently
train stats after 205184 examples: {'rewards_train/chosen': '0.16374', 'rewards_train/rejected': '-0.011507', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17525', 'logps_train/rejected': '-130.3', 'logps_train/chosen': '-145.64', 'loss/train': '0.61979', 'examples_per_second': '31.645', 'grad_norm': '30', 'counters/examples': 205184, 'counters/updates': 6412}
train stats after 205216 examples: {'rewards_train/chosen': '0.15999', 'rewards_train/rejected': '0.021867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13812', 'logps_train/rejected': '-124.05', 'logps_train/chosen': '-127.37', 'loss/train': '0.63626', 'examples_per_second': '30.647', 'grad_norm': '25', 'counters/examples': 205216, 'counters/updates': 6413}
train stats after 205248 examples: {'rewards_train/chosen': '0.11778', 'rewards_train/rejected': '0.0087322', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10905', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-152.3', 'loss/train': '0.65647', 'examples_per_second': '31.62', 'grad_norm': '29.25', 'counters/examples': 205248, 'counters/updates': 6414}
train stats after 205280 examples: {'rewards_train/chosen': '0.16524', 'rewards_train/rejected': '0.12831', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036931', 'logps_train/rejected': '-152.47', 'logps_train/chosen': '-149.13', 'loss/train': '0.6862', 'examples_per_second': '32.21', 'grad_norm': '28.375', 'counters/examples': 205280, 'counters/updates': 6415}
train stats after 205312 examples: {'rewards_train/chosen': '0.1309', 'rewards_train/rejected': '0.04393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086972', 'logps_train/rejected': '-119.95', 'logps_train/chosen': '-152.73', 'loss/train': '0.65902', 'examples_per_second': '31.532', 'grad_norm': '24', 'counters/examples': 205312, 'counters/updates': 6416}
train stats after 205344 examples: {'rewards_train/chosen': '0.18362', 'rewards_train/rejected': '-0.019146', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20277', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-169.82', 'loss/train': '0.61323', 'examples_per_second': '31.412', 'grad_norm': '24.375', 'counters/examples': 205344, 'counters/updates': 6417}
train stats after 205376 examples: {'rewards_train/chosen': '0.14689', 'rewards_train/rejected': '0.002783', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14411', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-154.33', 'loss/train': '0.64532', 'examples_per_second': '31.974', 'grad_norm': '25.125', 'counters/examples': 205376, 'counters/updates': 6418}
train stats after 205408 examples: {'rewards_train/chosen': '0.063496', 'rewards_train/rejected': '0.055187', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0083088', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-145.59', 'loss/train': '0.70681', 'examples_per_second': '30.287', 'grad_norm': '27.375', 'counters/examples': 205408, 'counters/updates': 6419}
train stats after 205440 examples: {'rewards_train/chosen': '0.12837', 'rewards_train/rejected': '0.04603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082344', 'logps_train/rejected': '-96.376', 'logps_train/chosen': '-135.94', 'loss/train': '0.66758', 'examples_per_second': '31.861', 'grad_norm': '27.75', 'counters/examples': 205440, 'counters/updates': 6420}
train stats after 205472 examples: {'rewards_train/chosen': '0.11346', 'rewards_train/rejected': '0.052889', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060566', 'logps_train/rejected': '-107.2', 'logps_train/chosen': '-113.1', 'loss/train': '0.67582', 'examples_per_second': '31.453', 'grad_norm': '23', 'counters/examples': 205472, 'counters/updates': 6421}
train stats after 205504 examples: {'rewards_train/chosen': '0.18676', 'rewards_train/rejected': '0.079611', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10715', 'logps_train/rejected': '-154.26', 'logps_train/chosen': '-172.03', 'loss/train': '0.65293', 'examples_per_second': '30.522', 'grad_norm': '26.875', 'counters/examples': 205504, 'counters/updates': 6422}
skipping logging after 205536 examples to avoid logging too frequently
train stats after 205568 examples: {'rewards_train/chosen': '0.1044', 'rewards_train/rejected': '0.11954', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015142', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-143.45', 'loss/train': '0.71046', 'examples_per_second': '31.667', 'grad_norm': '25.875', 'counters/examples': 205568, 'counters/updates': 6424}
train stats after 205600 examples: {'rewards_train/chosen': '0.2276', 'rewards_train/rejected': '0.11916', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10844', 'logps_train/rejected': '-136.06', 'logps_train/chosen': '-155.23', 'loss/train': '0.65424', 'examples_per_second': '30.262', 'grad_norm': '27.5', 'counters/examples': 205600, 'counters/updates': 6425}
train stats after 205632 examples: {'rewards_train/chosen': '0.11491', 'rewards_train/rejected': '0.033033', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081877', 'logps_train/rejected': '-147.53', 'logps_train/chosen': '-133.56', 'loss/train': '0.66129', 'examples_per_second': '30.626', 'grad_norm': '27', 'counters/examples': 205632, 'counters/updates': 6426}
train stats after 205664 examples: {'rewards_train/chosen': '0.1367', 'rewards_train/rejected': '-0.020095', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1568', 'logps_train/rejected': '-140.39', 'logps_train/chosen': '-152.46', 'loss/train': '0.64616', 'examples_per_second': '31.648', 'grad_norm': '26', 'counters/examples': 205664, 'counters/updates': 6427}
train stats after 205696 examples: {'rewards_train/chosen': '0.1329', 'rewards_train/rejected': '0.019851', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11305', 'logps_train/rejected': '-86.054', 'logps_train/chosen': '-114.94', 'loss/train': '0.65112', 'examples_per_second': '30.175', 'grad_norm': '25.375', 'counters/examples': 205696, 'counters/updates': 6428}
train stats after 205728 examples: {'rewards_train/chosen': '0.15709', 'rewards_train/rejected': '0.042039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11506', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-134.9', 'loss/train': '0.6438', 'examples_per_second': '30.212', 'grad_norm': '28.25', 'counters/examples': 205728, 'counters/updates': 6429}
train stats after 205760 examples: {'rewards_train/chosen': '0.17085', 'rewards_train/rejected': '0.014576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15628', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-145.59', 'loss/train': '0.63329', 'examples_per_second': '31.655', 'grad_norm': '26.125', 'counters/examples': 205760, 'counters/updates': 6430}
skipping logging after 205792 examples to avoid logging too frequently
train stats after 205824 examples: {'rewards_train/chosen': '0.19436', 'rewards_train/rejected': '0.081364', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.113', 'logps_train/rejected': '-148.28', 'logps_train/chosen': '-158.89', 'loss/train': '0.65413', 'examples_per_second': '30.799', 'grad_norm': '28.25', 'counters/examples': 205824, 'counters/updates': 6432}
train stats after 205856 examples: {'rewards_train/chosen': '0.096411', 'rewards_train/rejected': '0.028955', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067456', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-130.7', 'loss/train': '0.66773', 'examples_per_second': '30.923', 'grad_norm': '25.25', 'counters/examples': 205856, 'counters/updates': 6433}
train stats after 205888 examples: {'rewards_train/chosen': '0.1109', 'rewards_train/rejected': '0.084211', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026687', 'logps_train/rejected': '-114.46', 'logps_train/chosen': '-140.84', 'loss/train': '0.69722', 'examples_per_second': '31.236', 'grad_norm': '26', 'counters/examples': 205888, 'counters/updates': 6434}
train stats after 205920 examples: {'rewards_train/chosen': '0.12283', 'rewards_train/rejected': '0.072972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049858', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-118.33', 'loss/train': '0.68203', 'examples_per_second': '32.197', 'grad_norm': '27.125', 'counters/examples': 205920, 'counters/updates': 6435}
train stats after 205952 examples: {'rewards_train/chosen': '0.13871', 'rewards_train/rejected': '-0.060854', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.19956', 'logps_train/rejected': '-108.25', 'logps_train/chosen': '-158.24', 'loss/train': '0.60774', 'examples_per_second': '32.753', 'grad_norm': '25.125', 'counters/examples': 205952, 'counters/updates': 6436}
train stats after 205984 examples: {'rewards_train/chosen': '0.093897', 'rewards_train/rejected': '-0.016477', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11037', 'logps_train/rejected': '-123.24', 'logps_train/chosen': '-165.46', 'loss/train': '0.65145', 'examples_per_second': '31.478', 'grad_norm': '26', 'counters/examples': 205984, 'counters/updates': 6437}
skipping logging after 206016 examples to avoid logging too frequently
train stats after 206048 examples: {'rewards_train/chosen': '0.13416', 'rewards_train/rejected': '0.066286', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067876', 'logps_train/rejected': '-155.38', 'logps_train/chosen': '-140.18', 'loss/train': '0.6677', 'examples_per_second': '31.66', 'grad_norm': '26.5', 'counters/examples': 206048, 'counters/updates': 6439}
train stats after 206080 examples: {'rewards_train/chosen': '0.15799', 'rewards_train/rejected': '0.036139', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12185', 'logps_train/rejected': '-142.59', 'logps_train/chosen': '-154.28', 'loss/train': '0.64899', 'examples_per_second': '31.649', 'grad_norm': '26.25', 'counters/examples': 206080, 'counters/updates': 6440}
train stats after 206112 examples: {'rewards_train/chosen': '0.095169', 'rewards_train/rejected': '0.048515', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046653', 'logps_train/rejected': '-119.2', 'logps_train/chosen': '-132.18', 'loss/train': '0.68217', 'examples_per_second': '32.556', 'grad_norm': '26.375', 'counters/examples': 206112, 'counters/updates': 6441}
train stats after 206144 examples: {'rewards_train/chosen': '0.20466', 'rewards_train/rejected': '0.14077', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063891', 'logps_train/rejected': '-123.9', 'logps_train/chosen': '-153.87', 'loss/train': '0.67445', 'examples_per_second': '30.154', 'grad_norm': '26.5', 'counters/examples': 206144, 'counters/updates': 6442}
train stats after 206176 examples: {'rewards_train/chosen': '0.11822', 'rewards_train/rejected': '0.017583', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10064', 'logps_train/rejected': '-97.225', 'logps_train/chosen': '-165.28', 'loss/train': '0.66297', 'examples_per_second': '31.88', 'grad_norm': '23.625', 'counters/examples': 206176, 'counters/updates': 6443}
train stats after 206208 examples: {'rewards_train/chosen': '0.16852', 'rewards_train/rejected': '0.082905', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085615', 'logps_train/rejected': '-116.77', 'logps_train/chosen': '-126.41', 'loss/train': '0.66591', 'examples_per_second': '30.6', 'grad_norm': '22.75', 'counters/examples': 206208, 'counters/updates': 6444}
train stats after 206240 examples: {'rewards_train/chosen': '0.094845', 'rewards_train/rejected': '-0.057965', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15281', 'logps_train/rejected': '-93.54', 'logps_train/chosen': '-95.373', 'loss/train': '0.63228', 'examples_per_second': '31.589', 'grad_norm': '21.875', 'counters/examples': 206240, 'counters/updates': 6445}
train stats after 206272 examples: {'rewards_train/chosen': '0.14885', 'rewards_train/rejected': '0.04345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1054', 'logps_train/rejected': '-94.178', 'logps_train/chosen': '-148.89', 'loss/train': '0.65686', 'examples_per_second': '30.15', 'grad_norm': '21.875', 'counters/examples': 206272, 'counters/updates': 6446}
skipping logging after 206304 examples to avoid logging too frequently
train stats after 206336 examples: {'rewards_train/chosen': '0.16269', 'rewards_train/rejected': '-0.039352', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20204', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-147.74', 'loss/train': '0.62891', 'examples_per_second': '31.561', 'grad_norm': '26.75', 'counters/examples': 206336, 'counters/updates': 6448}
train stats after 206368 examples: {'rewards_train/chosen': '0.1469', 'rewards_train/rejected': '-0.0078912', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15479', 'logps_train/rejected': '-101.83', 'logps_train/chosen': '-144.4', 'loss/train': '0.63285', 'examples_per_second': '31.884', 'grad_norm': '26.875', 'counters/examples': 206368, 'counters/updates': 6449}
train stats after 206400 examples: {'rewards_train/chosen': '0.18046', 'rewards_train/rejected': '0.0067529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17371', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-142.7', 'loss/train': '0.63071', 'examples_per_second': '31.629', 'grad_norm': '25.625', 'counters/examples': 206400, 'counters/updates': 6450}
train stats after 206432 examples: {'rewards_train/chosen': '0.16211', 'rewards_train/rejected': '0.037796', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12432', 'logps_train/rejected': '-182.68', 'logps_train/chosen': '-157.43', 'loss/train': '0.64561', 'examples_per_second': '31.015', 'grad_norm': '29', 'counters/examples': 206432, 'counters/updates': 6451}
skipping logging after 206464 examples to avoid logging too frequently
train stats after 206496 examples: {'rewards_train/chosen': '0.1103', 'rewards_train/rejected': '-0.01609', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12639', 'logps_train/rejected': '-136.25', 'logps_train/chosen': '-138.42', 'loss/train': '0.64118', 'examples_per_second': '25.851', 'grad_norm': '26.125', 'counters/examples': 206496, 'counters/updates': 6453}
train stats after 206528 examples: {'rewards_train/chosen': '0.12459', 'rewards_train/rejected': '-0.0072224', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13181', 'logps_train/rejected': '-106.99', 'logps_train/chosen': '-106.5', 'loss/train': '0.63796', 'examples_per_second': '32.212', 'grad_norm': '22.875', 'counters/examples': 206528, 'counters/updates': 6454}
train stats after 206560 examples: {'rewards_train/chosen': '0.17773', 'rewards_train/rejected': '-0.019943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19768', 'logps_train/rejected': '-144.05', 'logps_train/chosen': '-146.94', 'loss/train': '0.62443', 'examples_per_second': '33.073', 'grad_norm': '25.5', 'counters/examples': 206560, 'counters/updates': 6455}
train stats after 206592 examples: {'rewards_train/chosen': '0.11817', 'rewards_train/rejected': '-0.015159', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13333', 'logps_train/rejected': '-152.73', 'logps_train/chosen': '-174.65', 'loss/train': '0.64709', 'examples_per_second': '31.608', 'grad_norm': '31.375', 'counters/examples': 206592, 'counters/updates': 6456}
skipping logging after 206624 examples to avoid logging too frequently
train stats after 206656 examples: {'rewards_train/chosen': '0.10333', 'rewards_train/rejected': '-0.083676', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18701', 'logps_train/rejected': '-104.11', 'logps_train/chosen': '-116.52', 'loss/train': '0.61869', 'examples_per_second': '39.491', 'grad_norm': '22.125', 'counters/examples': 206656, 'counters/updates': 6458}
train stats after 206688 examples: {'rewards_train/chosen': '0.22098', 'rewards_train/rejected': '0.025414', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19556', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-135.66', 'loss/train': '0.61132', 'examples_per_second': '30.599', 'grad_norm': '23.25', 'counters/examples': 206688, 'counters/updates': 6459}
train stats after 206720 examples: {'rewards_train/chosen': '0.12658', 'rewards_train/rejected': '0.011195', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11539', 'logps_train/rejected': '-92.303', 'logps_train/chosen': '-130.52', 'loss/train': '0.64965', 'examples_per_second': '32.598', 'grad_norm': '22.625', 'counters/examples': 206720, 'counters/updates': 6460}
train stats after 206752 examples: {'rewards_train/chosen': '0.17325', 'rewards_train/rejected': '0.047381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12587', 'logps_train/rejected': '-134.01', 'logps_train/chosen': '-164.88', 'loss/train': '0.64509', 'examples_per_second': '32.235', 'grad_norm': '28.375', 'counters/examples': 206752, 'counters/updates': 6461}
skipping logging after 206784 examples to avoid logging too frequently
train stats after 206816 examples: {'rewards_train/chosen': '0.13483', 'rewards_train/rejected': '0.075944', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058882', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-105.35', 'loss/train': '0.68291', 'examples_per_second': '34.341', 'grad_norm': '25.75', 'counters/examples': 206816, 'counters/updates': 6463}
train stats after 206848 examples: {'rewards_train/chosen': '0.1425', 'rewards_train/rejected': '-0.0014967', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.144', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-147.6', 'loss/train': '0.63449', 'examples_per_second': '31.415', 'grad_norm': '30.25', 'counters/examples': 206848, 'counters/updates': 6464}
train stats after 206880 examples: {'rewards_train/chosen': '0.20655', 'rewards_train/rejected': '0.072368', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13418', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-137.73', 'loss/train': '0.64347', 'examples_per_second': '31.06', 'grad_norm': '24.875', 'counters/examples': 206880, 'counters/updates': 6465}
train stats after 206912 examples: {'rewards_train/chosen': '0.17709', 'rewards_train/rejected': '0.093039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084055', 'logps_train/rejected': '-142.18', 'logps_train/chosen': '-139.26', 'loss/train': '0.66487', 'examples_per_second': '32.14', 'grad_norm': '27.375', 'counters/examples': 206912, 'counters/updates': 6466}
train stats after 206944 examples: {'rewards_train/chosen': '0.19036', 'rewards_train/rejected': '0.0067418', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18362', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-131.17', 'loss/train': '0.61913', 'examples_per_second': '31.924', 'grad_norm': '36.25', 'counters/examples': 206944, 'counters/updates': 6467}
train stats after 206976 examples: {'rewards_train/chosen': '0.12404', 'rewards_train/rejected': '-0.0058572', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1299', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-145.17', 'loss/train': '0.64236', 'examples_per_second': '32.105', 'grad_norm': '26.5', 'counters/examples': 206976, 'counters/updates': 6468}
train stats after 207008 examples: {'rewards_train/chosen': '0.17337', 'rewards_train/rejected': '0.025241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14812', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-161.05', 'loss/train': '0.64616', 'examples_per_second': '31.412', 'grad_norm': '26.25', 'counters/examples': 207008, 'counters/updates': 6469}
train stats after 207040 examples: {'rewards_train/chosen': '0.19421', 'rewards_train/rejected': '0.10723', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086981', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-125.46', 'loss/train': '0.66001', 'examples_per_second': '31.515', 'grad_norm': '24.375', 'counters/examples': 207040, 'counters/updates': 6470}
train stats after 207072 examples: {'rewards_train/chosen': '0.074229', 'rewards_train/rejected': '-0.020544', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094773', 'logps_train/rejected': '-107.04', 'logps_train/chosen': '-135.79', 'loss/train': '0.65867', 'examples_per_second': '32.093', 'grad_norm': '24.375', 'counters/examples': 207072, 'counters/updates': 6471}
train stats after 207104 examples: {'rewards_train/chosen': '0.18328', 'rewards_train/rejected': '0.080588', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10269', 'logps_train/rejected': '-168.16', 'logps_train/chosen': '-164.21', 'loss/train': '0.66113', 'examples_per_second': '31.612', 'grad_norm': '28.5', 'counters/examples': 207104, 'counters/updates': 6472}
train stats after 207136 examples: {'rewards_train/chosen': '0.089718', 'rewards_train/rejected': '0.0095712', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080146', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-110.35', 'loss/train': '0.6632', 'examples_per_second': '32.55', 'grad_norm': '21.5', 'counters/examples': 207136, 'counters/updates': 6473}
train stats after 207168 examples: {'rewards_train/chosen': '0.075696', 'rewards_train/rejected': '0.074333', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.001363', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-140.13', 'loss/train': '0.7048', 'examples_per_second': '31.457', 'grad_norm': '29.875', 'counters/examples': 207168, 'counters/updates': 6474}
train stats after 207200 examples: {'rewards_train/chosen': '0.13599', 'rewards_train/rejected': '-0.00015284', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13614', 'logps_train/rejected': '-107.68', 'logps_train/chosen': '-149.95', 'loss/train': '0.6337', 'examples_per_second': '32.202', 'grad_norm': '24.375', 'counters/examples': 207200, 'counters/updates': 6475}
train stats after 207232 examples: {'rewards_train/chosen': '0.15456', 'rewards_train/rejected': '-0.015947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17051', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-127.96', 'loss/train': '0.62225', 'examples_per_second': '32.345', 'grad_norm': '26', 'counters/examples': 207232, 'counters/updates': 6476}
train stats after 207264 examples: {'rewards_train/chosen': '0.14222', 'rewards_train/rejected': '0.12114', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021078', 'logps_train/rejected': '-124.73', 'logps_train/chosen': '-139.72', 'loss/train': '0.69237', 'examples_per_second': '30.615', 'grad_norm': '29.5', 'counters/examples': 207264, 'counters/updates': 6477}
train stats after 207296 examples: {'rewards_train/chosen': '0.15169', 'rewards_train/rejected': '0.065793', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085898', 'logps_train/rejected': '-134.41', 'logps_train/chosen': '-129.57', 'loss/train': '0.66086', 'examples_per_second': '33.054', 'grad_norm': '27.875', 'counters/examples': 207296, 'counters/updates': 6478}
skipping logging after 207328 examples to avoid logging too frequently
train stats after 207360 examples: {'rewards_train/chosen': '0.14281', 'rewards_train/rejected': '0.076495', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066313', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-163.94', 'loss/train': '0.67317', 'examples_per_second': '31.625', 'grad_norm': '25.375', 'counters/examples': 207360, 'counters/updates': 6480}
train stats after 207392 examples: {'rewards_train/chosen': '0.18321', 'rewards_train/rejected': '0.106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077217', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-182.32', 'loss/train': '0.67824', 'examples_per_second': '30.513', 'grad_norm': '32.5', 'counters/examples': 207392, 'counters/updates': 6481}
train stats after 207424 examples: {'rewards_train/chosen': '0.18005', 'rewards_train/rejected': '0.04361', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13645', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-135.15', 'loss/train': '0.64067', 'examples_per_second': '33.021', 'grad_norm': '24.625', 'counters/examples': 207424, 'counters/updates': 6482}
train stats after 207456 examples: {'rewards_train/chosen': '0.12632', 'rewards_train/rejected': '0.085115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041205', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-135.41', 'loss/train': '0.68548', 'examples_per_second': '30.758', 'grad_norm': '35.5', 'counters/examples': 207456, 'counters/updates': 6483}
train stats after 207488 examples: {'rewards_train/chosen': '0.13046', 'rewards_train/rejected': '0.050015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080443', 'logps_train/rejected': '-110.34', 'logps_train/chosen': '-98.565', 'loss/train': '0.65843', 'examples_per_second': '32.952', 'grad_norm': '22.375', 'counters/examples': 207488, 'counters/updates': 6484}
train stats after 207520 examples: {'rewards_train/chosen': '0.13424', 'rewards_train/rejected': '0.073809', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060436', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-142.91', 'loss/train': '0.67972', 'examples_per_second': '31.582', 'grad_norm': '27.125', 'counters/examples': 207520, 'counters/updates': 6485}
train stats after 207552 examples: {'rewards_train/chosen': '0.090324', 'rewards_train/rejected': '0.027967', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062356', 'logps_train/rejected': '-106.27', 'logps_train/chosen': '-144.6', 'loss/train': '0.66973', 'examples_per_second': '30.486', 'grad_norm': '25.5', 'counters/examples': 207552, 'counters/updates': 6486}
train stats after 207584 examples: {'rewards_train/chosen': '0.076782', 'rewards_train/rejected': '-0.059476', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13626', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-136.89', 'loss/train': '0.63819', 'examples_per_second': '32.513', 'grad_norm': '24.5', 'counters/examples': 207584, 'counters/updates': 6487}
train stats after 207616 examples: {'rewards_train/chosen': '0.0996', 'rewards_train/rejected': '0.0088066', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090793', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-129.31', 'loss/train': '0.66219', 'examples_per_second': '30.579', 'grad_norm': '24.375', 'counters/examples': 207616, 'counters/updates': 6488}
train stats after 207648 examples: {'rewards_train/chosen': '0.082541', 'rewards_train/rejected': '0.013272', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069269', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-164.55', 'loss/train': '0.66884', 'examples_per_second': '32.543', 'grad_norm': '28', 'counters/examples': 207648, 'counters/updates': 6489}
train stats after 207680 examples: {'rewards_train/chosen': '0.071797', 'rewards_train/rejected': '0.10408', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032287', 'logps_train/rejected': '-105.28', 'logps_train/chosen': '-111.59', 'loss/train': '0.71977', 'examples_per_second': '31.49', 'grad_norm': '24.25', 'counters/examples': 207680, 'counters/updates': 6490}
train stats after 207712 examples: {'rewards_train/chosen': '0.15274', 'rewards_train/rejected': '-0.02925', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18199', 'logps_train/rejected': '-109.49', 'logps_train/chosen': '-141.51', 'loss/train': '0.62565', 'examples_per_second': '30.371', 'grad_norm': '25.5', 'counters/examples': 207712, 'counters/updates': 6491}
train stats after 207744 examples: {'rewards_train/chosen': '0.17828', 'rewards_train/rejected': '0.099957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078324', 'logps_train/rejected': '-154.15', 'logps_train/chosen': '-168.05', 'loss/train': '0.66759', 'examples_per_second': '31.434', 'grad_norm': '28.125', 'counters/examples': 207744, 'counters/updates': 6492}
train stats after 207776 examples: {'rewards_train/chosen': '0.18614', 'rewards_train/rejected': '0.028935', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15721', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-162.17', 'loss/train': '0.6304', 'examples_per_second': '30.475', 'grad_norm': '26.25', 'counters/examples': 207776, 'counters/updates': 6493}
train stats after 207808 examples: {'rewards_train/chosen': '0.15401', 'rewards_train/rejected': '0.047195', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10682', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-142.4', 'loss/train': '0.65138', 'examples_per_second': '24.195', 'grad_norm': '24.25', 'counters/examples': 207808, 'counters/updates': 6494}
skipping logging after 207840 examples to avoid logging too frequently
train stats after 207872 examples: {'rewards_train/chosen': '0.24455', 'rewards_train/rejected': '0.12485', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1197', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-148.06', 'loss/train': '0.64942', 'examples_per_second': '31.325', 'grad_norm': '26.625', 'counters/examples': 207872, 'counters/updates': 6496}
train stats after 207904 examples: {'rewards_train/chosen': '0.13034', 'rewards_train/rejected': '0.0036866', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12665', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-160.56', 'loss/train': '0.65164', 'examples_per_second': '24.269', 'grad_norm': '28.125', 'counters/examples': 207904, 'counters/updates': 6497}
train stats after 207936 examples: {'rewards_train/chosen': '0.24227', 'rewards_train/rejected': '0.097491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14478', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-215.65', 'loss/train': '0.63757', 'examples_per_second': '31.667', 'grad_norm': '32.5', 'counters/examples': 207936, 'counters/updates': 6498}
train stats after 207968 examples: {'rewards_train/chosen': '0.12524', 'rewards_train/rejected': '0.082905', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042335', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-149.48', 'loss/train': '0.68912', 'examples_per_second': '31.668', 'grad_norm': '31', 'counters/examples': 207968, 'counters/updates': 6499}
skipping logging after 208000 examples to avoid logging too frequently
train stats after 208032 examples: {'rewards_train/chosen': '0.15563', 'rewards_train/rejected': '0.055306', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.10033', 'logps_train/rejected': '-158.08', 'logps_train/chosen': '-134.33', 'loss/train': '0.66082', 'examples_per_second': '32.083', 'grad_norm': '26.75', 'counters/examples': 208032, 'counters/updates': 6501}
train stats after 208064 examples: {'rewards_train/chosen': '0.066884', 'rewards_train/rejected': '-0.056622', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12351', 'logps_train/rejected': '-146', 'logps_train/chosen': '-124.76', 'loss/train': '0.65152', 'examples_per_second': '31.68', 'grad_norm': '35.25', 'counters/examples': 208064, 'counters/updates': 6502}
train stats after 208096 examples: {'rewards_train/chosen': '0.18203', 'rewards_train/rejected': '0.017016', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16502', 'logps_train/rejected': '-150.05', 'logps_train/chosen': '-140.16', 'loss/train': '0.62702', 'examples_per_second': '31.597', 'grad_norm': '25.75', 'counters/examples': 208096, 'counters/updates': 6503}
train stats after 208128 examples: {'rewards_train/chosen': '0.12261', 'rewards_train/rejected': '0.017681', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10493', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-172.48', 'loss/train': '0.65574', 'examples_per_second': '31.249', 'grad_norm': '30.5', 'counters/examples': 208128, 'counters/updates': 6504}
train stats after 208160 examples: {'rewards_train/chosen': '0.22797', 'rewards_train/rejected': '-0.010143', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23811', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-139.6', 'loss/train': '0.59558', 'examples_per_second': '31.152', 'grad_norm': '25.875', 'counters/examples': 208160, 'counters/updates': 6505}
skipping logging after 208192 examples to avoid logging too frequently
train stats after 208224 examples: {'rewards_train/chosen': '0.18603', 'rewards_train/rejected': '0.040793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14524', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-173.21', 'loss/train': '0.64126', 'examples_per_second': '34.695', 'grad_norm': '26.875', 'counters/examples': 208224, 'counters/updates': 6507}
skipping logging after 208256 examples to avoid logging too frequently
train stats after 208288 examples: {'rewards_train/chosen': '0.092003', 'rewards_train/rejected': '0.035349', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056654', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-117.66', 'loss/train': '0.67501', 'examples_per_second': '30.464', 'grad_norm': '29', 'counters/examples': 208288, 'counters/updates': 6509}
skipping logging after 208320 examples to avoid logging too frequently
train stats after 208352 examples: {'rewards_train/chosen': '0.068684', 'rewards_train/rejected': '0.081755', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013072', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-85.117', 'loss/train': '0.71552', 'examples_per_second': '31.351', 'grad_norm': '27.625', 'counters/examples': 208352, 'counters/updates': 6511}
train stats after 208384 examples: {'rewards_train/chosen': '0.112', 'rewards_train/rejected': '0.058132', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053868', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-112.42', 'loss/train': '0.68506', 'examples_per_second': '31.662', 'grad_norm': '24.375', 'counters/examples': 208384, 'counters/updates': 6512}
train stats after 208416 examples: {'rewards_train/chosen': '0.092532', 'rewards_train/rejected': '-0.058767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1513', 'logps_train/rejected': '-144.51', 'logps_train/chosen': '-129.62', 'loss/train': '0.6346', 'examples_per_second': '31.721', 'grad_norm': '28', 'counters/examples': 208416, 'counters/updates': 6513}
train stats after 208448 examples: {'rewards_train/chosen': '0.16899', 'rewards_train/rejected': '0.068735', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10026', 'logps_train/rejected': '-96.58', 'logps_train/chosen': '-171.23', 'loss/train': '0.66036', 'examples_per_second': '31.177', 'grad_norm': '24.75', 'counters/examples': 208448, 'counters/updates': 6514}
train stats after 208480 examples: {'rewards_train/chosen': '0.12387', 'rewards_train/rejected': '-0.0021786', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12605', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-146.53', 'loss/train': '0.64519', 'examples_per_second': '30.306', 'grad_norm': '26.125', 'counters/examples': 208480, 'counters/updates': 6515}
train stats after 208512 examples: {'rewards_train/chosen': '0.076003', 'rewards_train/rejected': '-0.0084814', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084484', 'logps_train/rejected': '-105.54', 'logps_train/chosen': '-140.53', 'loss/train': '0.6686', 'examples_per_second': '30.397', 'grad_norm': '25.75', 'counters/examples': 208512, 'counters/updates': 6516}
skipping logging after 208544 examples to avoid logging too frequently
train stats after 208576 examples: {'rewards_train/chosen': '0.081763', 'rewards_train/rejected': '0.0025502', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079213', 'logps_train/rejected': '-100.78', 'logps_train/chosen': '-131.38', 'loss/train': '0.6712', 'examples_per_second': '31.308', 'grad_norm': '27', 'counters/examples': 208576, 'counters/updates': 6518}
train stats after 208608 examples: {'rewards_train/chosen': '0.15933', 'rewards_train/rejected': '0.1242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035132', 'logps_train/rejected': '-160.42', 'logps_train/chosen': '-158.56', 'loss/train': '0.68925', 'examples_per_second': '32.339', 'grad_norm': '36.75', 'counters/examples': 208608, 'counters/updates': 6519}
skipping logging after 208640 examples to avoid logging too frequently
train stats after 208672 examples: {'rewards_train/chosen': '0.1702', 'rewards_train/rejected': '0.21787', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047669', 'logps_train/rejected': '-122.62', 'logps_train/chosen': '-162.34', 'loss/train': '0.73583', 'examples_per_second': '32.685', 'grad_norm': '31.5', 'counters/examples': 208672, 'counters/updates': 6521}
skipping logging after 208704 examples to avoid logging too frequently
train stats after 208736 examples: {'rewards_train/chosen': '0.13693', 'rewards_train/rejected': '-0.060002', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19693', 'logps_train/rejected': '-139.57', 'logps_train/chosen': '-135.16', 'loss/train': '0.6097', 'examples_per_second': '34.398', 'grad_norm': '24.875', 'counters/examples': 208736, 'counters/updates': 6523}
train stats after 208768 examples: {'rewards_train/chosen': '0.10949', 'rewards_train/rejected': '0.08382', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02567', 'logps_train/rejected': '-113.04', 'logps_train/chosen': '-114.48', 'loss/train': '0.68893', 'examples_per_second': '30.706', 'grad_norm': '24.25', 'counters/examples': 208768, 'counters/updates': 6524}
skipping logging after 208800 examples to avoid logging too frequently
train stats after 208832 examples: {'rewards_train/chosen': '0.1644', 'rewards_train/rejected': '-0.064759', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22916', 'logps_train/rejected': '-99.1', 'logps_train/chosen': '-115.67', 'loss/train': '0.5981', 'examples_per_second': '36.294', 'grad_norm': '21.875', 'counters/examples': 208832, 'counters/updates': 6526}
train stats after 208864 examples: {'rewards_train/chosen': '0.094146', 'rewards_train/rejected': '-0.060235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15438', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-144.81', 'loss/train': '0.63696', 'examples_per_second': '31.415', 'grad_norm': '28.625', 'counters/examples': 208864, 'counters/updates': 6527}
train stats after 208896 examples: {'rewards_train/chosen': '0.20645', 'rewards_train/rejected': '0.089409', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11704', 'logps_train/rejected': '-112.87', 'logps_train/chosen': '-122.75', 'loss/train': '0.65419', 'examples_per_second': '31.628', 'grad_norm': '53.5', 'counters/examples': 208896, 'counters/updates': 6528}
skipping logging after 208928 examples to avoid logging too frequently
train stats after 208960 examples: {'rewards_train/chosen': '0.17635', 'rewards_train/rejected': '0.10016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076186', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-127.33', 'loss/train': '0.66337', 'examples_per_second': '31.583', 'grad_norm': '23.125', 'counters/examples': 208960, 'counters/updates': 6530}
train stats after 208992 examples: {'rewards_train/chosen': '0.1233', 'rewards_train/rejected': '0.084423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03888', 'logps_train/rejected': '-138.95', 'logps_train/chosen': '-170.89', 'loss/train': '0.69005', 'examples_per_second': '30.108', 'grad_norm': '28.125', 'counters/examples': 208992, 'counters/updates': 6531}
train stats after 209024 examples: {'rewards_train/chosen': '0.10702', 'rewards_train/rejected': '-0.025429', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13244', 'logps_train/rejected': '-111.26', 'logps_train/chosen': '-124.31', 'loss/train': '0.64407', 'examples_per_second': '31.61', 'grad_norm': '23.75', 'counters/examples': 209024, 'counters/updates': 6532}
train stats after 209056 examples: {'rewards_train/chosen': '0.16832', 'rewards_train/rejected': '-0.0045888', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17291', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-168.68', 'loss/train': '0.62467', 'examples_per_second': '32.643', 'grad_norm': '26.375', 'counters/examples': 209056, 'counters/updates': 6533}
skipping logging after 209088 examples to avoid logging too frequently
train stats after 209120 examples: {'rewards_train/chosen': '0.12613', 'rewards_train/rejected': '0.029443', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096692', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-140.5', 'loss/train': '0.65775', 'examples_per_second': '32.929', 'grad_norm': '31.375', 'counters/examples': 209120, 'counters/updates': 6535}
train stats after 209152 examples: {'rewards_train/chosen': '0.18123', 'rewards_train/rejected': '0.047966', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13327', 'logps_train/rejected': '-127.71', 'logps_train/chosen': '-164.97', 'loss/train': '0.63976', 'examples_per_second': '31.653', 'grad_norm': '28', 'counters/examples': 209152, 'counters/updates': 6536}
train stats after 209184 examples: {'rewards_train/chosen': '0.095262', 'rewards_train/rejected': '0.039083', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056179', 'logps_train/rejected': '-141.58', 'logps_train/chosen': '-140.19', 'loss/train': '0.68121', 'examples_per_second': '30.999', 'grad_norm': '29.25', 'counters/examples': 209184, 'counters/updates': 6537}
train stats after 209216 examples: {'rewards_train/chosen': '0.14298', 'rewards_train/rejected': '0.077715', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065265', 'logps_train/rejected': '-147.68', 'logps_train/chosen': '-120.28', 'loss/train': '0.67952', 'examples_per_second': '31.555', 'grad_norm': '24.625', 'counters/examples': 209216, 'counters/updates': 6538}
train stats after 209248 examples: {'rewards_train/chosen': '0.017953', 'rewards_train/rejected': '0.0068975', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011056', 'logps_train/rejected': '-151.97', 'logps_train/chosen': '-152.47', 'loss/train': '0.6949', 'examples_per_second': '31.488', 'grad_norm': '28.25', 'counters/examples': 209248, 'counters/updates': 6539}
train stats after 209280 examples: {'rewards_train/chosen': '0.18034', 'rewards_train/rejected': '0.079656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10069', 'logps_train/rejected': '-111.87', 'logps_train/chosen': '-161.83', 'loss/train': '0.6581', 'examples_per_second': '31.667', 'grad_norm': '25.25', 'counters/examples': 209280, 'counters/updates': 6540}
skipping logging after 209312 examples to avoid logging too frequently
train stats after 209344 examples: {'rewards_train/chosen': '0.12098', 'rewards_train/rejected': '-0.058425', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1794', 'logps_train/rejected': '-96.442', 'logps_train/chosen': '-165.65', 'loss/train': '0.61764', 'examples_per_second': '33.597', 'grad_norm': '24.25', 'counters/examples': 209344, 'counters/updates': 6542}
skipping logging after 209376 examples to avoid logging too frequently
train stats after 209408 examples: {'rewards_train/chosen': '0.097667', 'rewards_train/rejected': '0.015196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082471', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-117.94', 'loss/train': '0.66273', 'examples_per_second': '31.804', 'grad_norm': '24.125', 'counters/examples': 209408, 'counters/updates': 6544}
train stats after 209440 examples: {'rewards_train/chosen': '0.12119', 'rewards_train/rejected': '-0.0093877', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13058', 'logps_train/rejected': '-91.97', 'logps_train/chosen': '-110.63', 'loss/train': '0.64135', 'examples_per_second': '32.218', 'grad_norm': '23', 'counters/examples': 209440, 'counters/updates': 6545}
train stats after 209472 examples: {'rewards_train/chosen': '0.13956', 'rewards_train/rejected': '0.080463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059095', 'logps_train/rejected': '-103.91', 'logps_train/chosen': '-93.994', 'loss/train': '0.67947', 'examples_per_second': '30.012', 'grad_norm': '22.125', 'counters/examples': 209472, 'counters/updates': 6546}
skipping logging after 209504 examples to avoid logging too frequently
train stats after 209536 examples: {'rewards_train/chosen': '0.16669', 'rewards_train/rejected': '-0.0024996', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16919', 'logps_train/rejected': '-106.23', 'logps_train/chosen': '-141.69', 'loss/train': '0.6273', 'examples_per_second': '34.356', 'grad_norm': '22.875', 'counters/examples': 209536, 'counters/updates': 6548}
train stats after 209568 examples: {'rewards_train/chosen': '0.17084', 'rewards_train/rejected': '0.053129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11771', 'logps_train/rejected': '-151.79', 'logps_train/chosen': '-187.11', 'loss/train': '0.65103', 'examples_per_second': '31.645', 'grad_norm': '31.375', 'counters/examples': 209568, 'counters/updates': 6549}
train stats after 209600 examples: {'rewards_train/chosen': '0.075478', 'rewards_train/rejected': '0.015568', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05991', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-133.64', 'loss/train': '0.67591', 'examples_per_second': '31.525', 'grad_norm': '25.875', 'counters/examples': 209600, 'counters/updates': 6550}
train stats after 209632 examples: {'rewards_train/chosen': '0.1205', 'rewards_train/rejected': '0.05118', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069317', 'logps_train/rejected': '-97.487', 'logps_train/chosen': '-117.97', 'loss/train': '0.66221', 'examples_per_second': '30.005', 'grad_norm': '23.75', 'counters/examples': 209632, 'counters/updates': 6551}
skipping logging after 209664 examples to avoid logging too frequently
train stats after 209696 examples: {'rewards_train/chosen': '0.13618', 'rewards_train/rejected': '0.01069', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12549', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-158.87', 'loss/train': '0.64607', 'examples_per_second': '30.143', 'grad_norm': '27', 'counters/examples': 209696, 'counters/updates': 6553}
train stats after 209728 examples: {'rewards_train/chosen': '0.097301', 'rewards_train/rejected': '-0.025846', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12315', 'logps_train/rejected': '-142.35', 'logps_train/chosen': '-114', 'loss/train': '0.64295', 'examples_per_second': '31.548', 'grad_norm': '26', 'counters/examples': 209728, 'counters/updates': 6554}
skipping logging after 209760 examples to avoid logging too frequently
train stats after 209792 examples: {'rewards_train/chosen': '0.15901', 'rewards_train/rejected': '-0.014893', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1739', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-140.45', 'loss/train': '0.62373', 'examples_per_second': '31.647', 'grad_norm': '26.375', 'counters/examples': 209792, 'counters/updates': 6556}
train stats after 209824 examples: {'rewards_train/chosen': '0.056017', 'rewards_train/rejected': '-0.043634', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09965', 'logps_train/rejected': '-109.3', 'logps_train/chosen': '-141.93', 'loss/train': '0.65697', 'examples_per_second': '30.8', 'grad_norm': '22.875', 'counters/examples': 209824, 'counters/updates': 6557}
train stats after 209856 examples: {'rewards_train/chosen': '0.10873', 'rewards_train/rejected': '0.012008', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.096719', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-138.55', 'loss/train': '0.66611', 'examples_per_second': '31.798', 'grad_norm': '27.375', 'counters/examples': 209856, 'counters/updates': 6558}
train stats after 209888 examples: {'rewards_train/chosen': '0.19444', 'rewards_train/rejected': '0.012929', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18151', 'logps_train/rejected': '-108.68', 'logps_train/chosen': '-170.94', 'loss/train': '0.6226', 'examples_per_second': '31.665', 'grad_norm': '27', 'counters/examples': 209888, 'counters/updates': 6559}
train stats after 209920 examples: {'rewards_train/chosen': '0.14235', 'rewards_train/rejected': '0.024041', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11831', 'logps_train/rejected': '-150.71', 'logps_train/chosen': '-118.69', 'loss/train': '0.65301', 'examples_per_second': '32.369', 'grad_norm': '26.75', 'counters/examples': 209920, 'counters/updates': 6560}
train stats after 209952 examples: {'rewards_train/chosen': '0.097342', 'rewards_train/rejected': '0.10561', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0082687', 'logps_train/rejected': '-143.66', 'logps_train/chosen': '-130.58', 'loss/train': '0.70863', 'examples_per_second': '32.226', 'grad_norm': '31.375', 'counters/examples': 209952, 'counters/updates': 6561}
train stats after 209984 examples: {'rewards_train/chosen': '0.10572', 'rewards_train/rejected': '-0.0021748', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10789', 'logps_train/rejected': '-155.11', 'logps_train/chosen': '-150.18', 'loss/train': '0.65582', 'examples_per_second': '32.082', 'grad_norm': '28', 'counters/examples': 209984, 'counters/updates': 6562}
skipping logging after 210016 examples to avoid logging too frequently
train stats after 210048 examples: {'rewards_train/chosen': '0.14873', 'rewards_train/rejected': '-0.020632', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16936', 'logps_train/rejected': '-115.99', 'logps_train/chosen': '-177.3', 'loss/train': '0.63051', 'examples_per_second': '31.647', 'grad_norm': '25.25', 'counters/examples': 210048, 'counters/updates': 6564}
train stats after 210080 examples: {'rewards_train/chosen': '0.11534', 'rewards_train/rejected': '0.052244', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063096', 'logps_train/rejected': '-112.37', 'logps_train/chosen': '-117.16', 'loss/train': '0.67321', 'examples_per_second': '31.892', 'grad_norm': '28.5', 'counters/examples': 210080, 'counters/updates': 6565}
train stats after 210112 examples: {'rewards_train/chosen': '0.20611', 'rewards_train/rejected': '0.045286', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16083', 'logps_train/rejected': '-150.47', 'logps_train/chosen': '-169.95', 'loss/train': '0.65255', 'examples_per_second': '31.329', 'grad_norm': '26.875', 'counters/examples': 210112, 'counters/updates': 6566}
train stats after 210144 examples: {'rewards_train/chosen': '0.19991', 'rewards_train/rejected': '0.046096', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15382', 'logps_train/rejected': '-109.92', 'logps_train/chosen': '-126.69', 'loss/train': '0.63103', 'examples_per_second': '31.97', 'grad_norm': '22.75', 'counters/examples': 210144, 'counters/updates': 6567}
train stats after 210176 examples: {'rewards_train/chosen': '0.15666', 'rewards_train/rejected': '-0.047892', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20455', 'logps_train/rejected': '-144.76', 'logps_train/chosen': '-152.72', 'loss/train': '0.61595', 'examples_per_second': '30.655', 'grad_norm': '25.25', 'counters/examples': 210176, 'counters/updates': 6568}
train stats after 210208 examples: {'rewards_train/chosen': '0.12134', 'rewards_train/rejected': '0.033931', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087405', 'logps_train/rejected': '-141.11', 'logps_train/chosen': '-176.46', 'loss/train': '0.65773', 'examples_per_second': '30.219', 'grad_norm': '28', 'counters/examples': 210208, 'counters/updates': 6569}
train stats after 210240 examples: {'rewards_train/chosen': '0.18439', 'rewards_train/rejected': '0.049312', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13508', 'logps_train/rejected': '-97.105', 'logps_train/chosen': '-138.37', 'loss/train': '0.64193', 'examples_per_second': '30.601', 'grad_norm': '27.25', 'counters/examples': 210240, 'counters/updates': 6570}
train stats after 210272 examples: {'rewards_train/chosen': '0.13392', 'rewards_train/rejected': '-0.00053393', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13445', 'logps_train/rejected': '-98.308', 'logps_train/chosen': '-120.96', 'loss/train': '0.64296', 'examples_per_second': '32.132', 'grad_norm': '23.25', 'counters/examples': 210272, 'counters/updates': 6571}
train stats after 210304 examples: {'rewards_train/chosen': '0.098603', 'rewards_train/rejected': '-0.043868', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14247', 'logps_train/rejected': '-90.281', 'logps_train/chosen': '-105.58', 'loss/train': '0.63162', 'examples_per_second': '30.315', 'grad_norm': '20.625', 'counters/examples': 210304, 'counters/updates': 6572}
train stats after 210336 examples: {'rewards_train/chosen': '0.085897', 'rewards_train/rejected': '-0.027029', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11293', 'logps_train/rejected': '-118.77', 'logps_train/chosen': '-125.56', 'loss/train': '0.64583', 'examples_per_second': '32.395', 'grad_norm': '26.875', 'counters/examples': 210336, 'counters/updates': 6573}
train stats after 210368 examples: {'rewards_train/chosen': '0.23802', 'rewards_train/rejected': '0.12198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11605', 'logps_train/rejected': '-146.99', 'logps_train/chosen': '-176.63', 'loss/train': '0.66463', 'examples_per_second': '30.131', 'grad_norm': '30.375', 'counters/examples': 210368, 'counters/updates': 6574}
train stats after 210400 examples: {'rewards_train/chosen': '0.072053', 'rewards_train/rejected': '0.050219', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021834', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-115.28', 'loss/train': '0.69642', 'examples_per_second': '31.631', 'grad_norm': '26.625', 'counters/examples': 210400, 'counters/updates': 6575}
train stats after 210432 examples: {'rewards_train/chosen': '0.093069', 'rewards_train/rejected': '0.057039', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03603', 'logps_train/rejected': '-106.26', 'logps_train/chosen': '-129.11', 'loss/train': '0.69061', 'examples_per_second': '30.866', 'grad_norm': '27.875', 'counters/examples': 210432, 'counters/updates': 6576}
skipping logging after 210464 examples to avoid logging too frequently
train stats after 210496 examples: {'rewards_train/chosen': '0.14999', 'rewards_train/rejected': '0.0053532', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14464', 'logps_train/rejected': '-98.255', 'logps_train/chosen': '-163.24', 'loss/train': '0.63961', 'examples_per_second': '35.782', 'grad_norm': '23.375', 'counters/examples': 210496, 'counters/updates': 6578}
train stats after 210528 examples: {'rewards_train/chosen': '0.18035', 'rewards_train/rejected': '0.087486', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092861', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-134.03', 'loss/train': '0.66463', 'examples_per_second': '31.653', 'grad_norm': '36', 'counters/examples': 210528, 'counters/updates': 6579}
train stats after 210560 examples: {'rewards_train/chosen': '0.12336', 'rewards_train/rejected': '0.11841', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0049438', 'logps_train/rejected': '-159.07', 'logps_train/chosen': '-137.94', 'loss/train': '0.70468', 'examples_per_second': '31.835', 'grad_norm': '41.5', 'counters/examples': 210560, 'counters/updates': 6580}
train stats after 210592 examples: {'rewards_train/chosen': '0.23493', 'rewards_train/rejected': '-0.02381', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25874', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-143.1', 'loss/train': '0.58997', 'examples_per_second': '30.575', 'grad_norm': '21.75', 'counters/examples': 210592, 'counters/updates': 6581}
train stats after 210624 examples: {'rewards_train/chosen': '0.057365', 'rewards_train/rejected': '-0.01647', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073836', 'logps_train/rejected': '-139.72', 'logps_train/chosen': '-135.97', 'loss/train': '0.66678', 'examples_per_second': '31.56', 'grad_norm': '33', 'counters/examples': 210624, 'counters/updates': 6582}
train stats after 210656 examples: {'rewards_train/chosen': '-0.032563', 'rewards_train/rejected': '-0.041103', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0085396', 'logps_train/rejected': '-115', 'logps_train/chosen': '-121.71', 'loss/train': '0.69841', 'examples_per_second': '32.378', 'grad_norm': '25', 'counters/examples': 210656, 'counters/updates': 6583}
train stats after 210688 examples: {'rewards_train/chosen': '0.14247', 'rewards_train/rejected': '-0.034614', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17709', 'logps_train/rejected': '-104.47', 'logps_train/chosen': '-146.81', 'loss/train': '0.62198', 'examples_per_second': '31.04', 'grad_norm': '28.375', 'counters/examples': 210688, 'counters/updates': 6584}
train stats after 210720 examples: {'rewards_train/chosen': '0.16001', 'rewards_train/rejected': '-0.0075527', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16756', 'logps_train/rejected': '-116.15', 'logps_train/chosen': '-139.95', 'loss/train': '0.62736', 'examples_per_second': '31.585', 'grad_norm': '29.75', 'counters/examples': 210720, 'counters/updates': 6585}
train stats after 210752 examples: {'rewards_train/chosen': '0.1365', 'rewards_train/rejected': '0.056492', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080011', 'logps_train/rejected': '-113.79', 'logps_train/chosen': '-152.77', 'loss/train': '0.66585', 'examples_per_second': '31.924', 'grad_norm': '24.625', 'counters/examples': 210752, 'counters/updates': 6586}
skipping logging after 210784 examples to avoid logging too frequently
train stats after 210816 examples: {'rewards_train/chosen': '0.10336', 'rewards_train/rejected': '0.018695', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08466', 'logps_train/rejected': '-156.76', 'logps_train/chosen': '-118.72', 'loss/train': '0.66479', 'examples_per_second': '31.63', 'grad_norm': '26.625', 'counters/examples': 210816, 'counters/updates': 6588}
skipping logging after 210848 examples to avoid logging too frequently
train stats after 210880 examples: {'rewards_train/chosen': '0.13135', 'rewards_train/rejected': '0.060151', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071202', 'logps_train/rejected': '-161.21', 'logps_train/chosen': '-144.75', 'loss/train': '0.68095', 'examples_per_second': '33.119', 'grad_norm': '27.375', 'counters/examples': 210880, 'counters/updates': 6590}
train stats after 210912 examples: {'rewards_train/chosen': '0.078674', 'rewards_train/rejected': '0.025454', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05322', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-127.77', 'loss/train': '0.68027', 'examples_per_second': '31.048', 'grad_norm': '26.25', 'counters/examples': 210912, 'counters/updates': 6591}
train stats after 210944 examples: {'rewards_train/chosen': '0.16007', 'rewards_train/rejected': '-0.023471', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18354', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-133.36', 'loss/train': '0.62534', 'examples_per_second': '31.536', 'grad_norm': '31.875', 'counters/examples': 210944, 'counters/updates': 6592}
train stats after 210976 examples: {'rewards_train/chosen': '0.17343', 'rewards_train/rejected': '0.058389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11504', 'logps_train/rejected': '-158.93', 'logps_train/chosen': '-151.97', 'loss/train': '0.65182', 'examples_per_second': '31.532', 'grad_norm': '27', 'counters/examples': 210976, 'counters/updates': 6593}
train stats after 211008 examples: {'rewards_train/chosen': '0.058875', 'rewards_train/rejected': '-0.01924', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078114', 'logps_train/rejected': '-111.35', 'logps_train/chosen': '-147.78', 'loss/train': '0.67288', 'examples_per_second': '32.335', 'grad_norm': '26.25', 'counters/examples': 211008, 'counters/updates': 6594}
train stats after 211040 examples: {'rewards_train/chosen': '0.20099', 'rewards_train/rejected': '0.062359', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13863', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-150.35', 'loss/train': '0.64164', 'examples_per_second': '31.634', 'grad_norm': '28', 'counters/examples': 211040, 'counters/updates': 6595}
train stats after 211072 examples: {'rewards_train/chosen': '0.12924', 'rewards_train/rejected': '0.093868', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035367', 'logps_train/rejected': '-148.7', 'logps_train/chosen': '-135.63', 'loss/train': '0.68509', 'examples_per_second': '30.697', 'grad_norm': '26.75', 'counters/examples': 211072, 'counters/updates': 6596}
train stats after 211104 examples: {'rewards_train/chosen': '0.13391', 'rewards_train/rejected': '0.058323', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075584', 'logps_train/rejected': '-156.26', 'logps_train/chosen': '-132.62', 'loss/train': '0.67358', 'examples_per_second': '31.667', 'grad_norm': '27.125', 'counters/examples': 211104, 'counters/updates': 6597}
train stats after 211136 examples: {'rewards_train/chosen': '0.058429', 'rewards_train/rejected': '-0.027437', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085867', 'logps_train/rejected': '-94.45', 'logps_train/chosen': '-131.86', 'loss/train': '0.66618', 'examples_per_second': '30.982', 'grad_norm': '24', 'counters/examples': 211136, 'counters/updates': 6598}
train stats after 211168 examples: {'rewards_train/chosen': '0.18176', 'rewards_train/rejected': '0.0030333', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17872', 'logps_train/rejected': '-100.78', 'logps_train/chosen': '-141.57', 'loss/train': '0.62399', 'examples_per_second': '30.543', 'grad_norm': '23', 'counters/examples': 211168, 'counters/updates': 6599}
skipping logging after 211200 examples to avoid logging too frequently
train stats after 211232 examples: {'rewards_train/chosen': '0.11889', 'rewards_train/rejected': '0.040424', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078462', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-128', 'loss/train': '0.66875', 'examples_per_second': '31.623', 'grad_norm': '23.375', 'counters/examples': 211232, 'counters/updates': 6601}
skipping logging after 211264 examples to avoid logging too frequently
train stats after 211296 examples: {'rewards_train/chosen': '0.12666', 'rewards_train/rejected': '0.061948', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064707', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-105.32', 'loss/train': '0.68025', 'examples_per_second': '31.647', 'grad_norm': '27.125', 'counters/examples': 211296, 'counters/updates': 6603}
skipping logging after 211328 examples to avoid logging too frequently
train stats after 211360 examples: {'rewards_train/chosen': '0.25946', 'rewards_train/rejected': '0.082529', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17694', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-157.13', 'loss/train': '0.62191', 'examples_per_second': '31.967', 'grad_norm': '28.25', 'counters/examples': 211360, 'counters/updates': 6605}
train stats after 211392 examples: {'rewards_train/chosen': '0.11391', 'rewards_train/rejected': '-0.0031632', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11707', 'logps_train/rejected': '-117.94', 'logps_train/chosen': '-156.83', 'loss/train': '0.64756', 'examples_per_second': '31.623', 'grad_norm': '26.125', 'counters/examples': 211392, 'counters/updates': 6606}
train stats after 211424 examples: {'rewards_train/chosen': '0.16804', 'rewards_train/rejected': '0.060932', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1071', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-150.57', 'loss/train': '0.65496', 'examples_per_second': '30.211', 'grad_norm': '28.125', 'counters/examples': 211424, 'counters/updates': 6607}
train stats after 211456 examples: {'rewards_train/chosen': '0.16175', 'rewards_train/rejected': '0.054072', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10768', 'logps_train/rejected': '-133.82', 'logps_train/chosen': '-149.42', 'loss/train': '0.65058', 'examples_per_second': '31.734', 'grad_norm': '31.875', 'counters/examples': 211456, 'counters/updates': 6608}
train stats after 211488 examples: {'rewards_train/chosen': '0.15215', 'rewards_train/rejected': '0.018169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13398', 'logps_train/rejected': '-109.28', 'logps_train/chosen': '-137.05', 'loss/train': '0.64501', 'examples_per_second': '31.795', 'grad_norm': '22.875', 'counters/examples': 211488, 'counters/updates': 6609}
train stats after 211520 examples: {'rewards_train/chosen': '0.19045', 'rewards_train/rejected': '0.12536', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065092', 'logps_train/rejected': '-147.83', 'logps_train/chosen': '-182.62', 'loss/train': '0.67836', 'examples_per_second': '30.409', 'grad_norm': '30.5', 'counters/examples': 211520, 'counters/updates': 6610}
train stats after 211552 examples: {'rewards_train/chosen': '0.17248', 'rewards_train/rejected': '0.05957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11291', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-156.63', 'loss/train': '0.6506', 'examples_per_second': '30.073', 'grad_norm': '25', 'counters/examples': 211552, 'counters/updates': 6611}
train stats after 211584 examples: {'rewards_train/chosen': '0.19924', 'rewards_train/rejected': '0.038067', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16117', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-154.45', 'loss/train': '0.63336', 'examples_per_second': '30.338', 'grad_norm': '26.125', 'counters/examples': 211584, 'counters/updates': 6612}
train stats after 211616 examples: {'rewards_train/chosen': '0.076049', 'rewards_train/rejected': '0.060564', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015485', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-139', 'loss/train': '0.70066', 'examples_per_second': '31.61', 'grad_norm': '32.25', 'counters/examples': 211616, 'counters/updates': 6613}
train stats after 211648 examples: {'rewards_train/chosen': '0.16182', 'rewards_train/rejected': '0.13555', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02627', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-150.44', 'loss/train': '0.68705', 'examples_per_second': '31.512', 'grad_norm': '26.25', 'counters/examples': 211648, 'counters/updates': 6614}
train stats after 211680 examples: {'rewards_train/chosen': '0.12335', 'rewards_train/rejected': '-0.070302', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19365', 'logps_train/rejected': '-121.37', 'logps_train/chosen': '-141.47', 'loss/train': '0.61387', 'examples_per_second': '30.679', 'grad_norm': '22.875', 'counters/examples': 211680, 'counters/updates': 6615}
train stats after 211712 examples: {'rewards_train/chosen': '0.15309', 'rewards_train/rejected': '0.031239', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12185', 'logps_train/rejected': '-130', 'logps_train/chosen': '-195.73', 'loss/train': '0.64666', 'examples_per_second': '30.123', 'grad_norm': '28.625', 'counters/examples': 211712, 'counters/updates': 6616}
train stats after 211744 examples: {'rewards_train/chosen': '0.13866', 'rewards_train/rejected': '0.020651', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11801', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-122.92', 'loss/train': '0.65164', 'examples_per_second': '31.036', 'grad_norm': '24.875', 'counters/examples': 211744, 'counters/updates': 6617}
train stats after 211776 examples: {'rewards_train/chosen': '0.1224', 'rewards_train/rejected': '0.12214', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.00026249', 'logps_train/rejected': '-127.11', 'logps_train/chosen': '-148.62', 'loss/train': '0.70186', 'examples_per_second': '31.486', 'grad_norm': '27.875', 'counters/examples': 211776, 'counters/updates': 6618}
train stats after 211808 examples: {'rewards_train/chosen': '0.14024', 'rewards_train/rejected': '-0.0145', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15474', 'logps_train/rejected': '-91.963', 'logps_train/chosen': '-127.87', 'loss/train': '0.63478', 'examples_per_second': '31.419', 'grad_norm': '22.5', 'counters/examples': 211808, 'counters/updates': 6619}
skipping logging after 211840 examples to avoid logging too frequently
train stats after 211872 examples: {'rewards_train/chosen': '0.16994', 'rewards_train/rejected': '0.058823', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11111', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-138.06', 'loss/train': '0.65148', 'examples_per_second': '33.879', 'grad_norm': '25.375', 'counters/examples': 211872, 'counters/updates': 6621}
train stats after 211904 examples: {'rewards_train/chosen': '0.10799', 'rewards_train/rejected': '0.028056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079932', 'logps_train/rejected': '-98.625', 'logps_train/chosen': '-102.45', 'loss/train': '0.66127', 'examples_per_second': '31.579', 'grad_norm': '21.625', 'counters/examples': 211904, 'counters/updates': 6622}
train stats after 211936 examples: {'rewards_train/chosen': '0.17392', 'rewards_train/rejected': '0.0034691', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17046', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-144.9', 'loss/train': '0.62284', 'examples_per_second': '29.927', 'grad_norm': '26.5', 'counters/examples': 211936, 'counters/updates': 6623}
train stats after 211968 examples: {'rewards_train/chosen': '0.077696', 'rewards_train/rejected': '-0.026655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10435', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-135.9', 'loss/train': '0.65236', 'examples_per_second': '32.894', 'grad_norm': '27.375', 'counters/examples': 211968, 'counters/updates': 6624}
train stats after 212000 examples: {'rewards_train/chosen': '0.18526', 'rewards_train/rejected': '0.033419', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15185', 'logps_train/rejected': '-131.61', 'logps_train/chosen': '-171.68', 'loss/train': '0.63166', 'examples_per_second': '30.259', 'grad_norm': '27', 'counters/examples': 212000, 'counters/updates': 6625}
train stats after 212032 examples: {'rewards_train/chosen': '0.13043', 'rewards_train/rejected': '-0.042614', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17304', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-166.22', 'loss/train': '0.62043', 'examples_per_second': '31.595', 'grad_norm': '25.375', 'counters/examples': 212032, 'counters/updates': 6626}
train stats after 212064 examples: {'rewards_train/chosen': '0.080843', 'rewards_train/rejected': '-0.0089745', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089817', 'logps_train/rejected': '-88.325', 'logps_train/chosen': '-119.21', 'loss/train': '0.65797', 'examples_per_second': '27.236', 'grad_norm': '20.75', 'counters/examples': 212064, 'counters/updates': 6627}
skipping logging after 212096 examples to avoid logging too frequently
train stats after 212128 examples: {'rewards_train/chosen': '0.15101', 'rewards_train/rejected': '-0.0055784', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15659', 'logps_train/rejected': '-88.421', 'logps_train/chosen': '-117.16', 'loss/train': '0.62986', 'examples_per_second': '36.01', 'grad_norm': '20.25', 'counters/examples': 212128, 'counters/updates': 6629}
train stats after 212160 examples: {'rewards_train/chosen': '0.14023', 'rewards_train/rejected': '0.017971', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12225', 'logps_train/rejected': '-136.3', 'logps_train/chosen': '-176.72', 'loss/train': '0.65101', 'examples_per_second': '31.434', 'grad_norm': '27.375', 'counters/examples': 212160, 'counters/updates': 6630}
train stats after 212192 examples: {'rewards_train/chosen': '0.18767', 'rewards_train/rejected': '0.06581', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12186', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-180.14', 'loss/train': '0.65377', 'examples_per_second': '31.613', 'grad_norm': '29.5', 'counters/examples': 212192, 'counters/updates': 6631}
train stats after 212224 examples: {'rewards_train/chosen': '0.14728', 'rewards_train/rejected': '-0.0015456', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14883', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-131.36', 'loss/train': '0.63047', 'examples_per_second': '30.255', 'grad_norm': '24.5', 'counters/examples': 212224, 'counters/updates': 6632}
train stats after 212256 examples: {'rewards_train/chosen': '0.18566', 'rewards_train/rejected': '0.045968', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13969', 'logps_train/rejected': '-122.77', 'logps_train/chosen': '-138.16', 'loss/train': '0.64494', 'examples_per_second': '30.957', 'grad_norm': '25.5', 'counters/examples': 212256, 'counters/updates': 6633}
train stats after 212288 examples: {'rewards_train/chosen': '0.18862', 'rewards_train/rejected': '-0.034734', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22336', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-179.92', 'loss/train': '0.60317', 'examples_per_second': '32.222', 'grad_norm': '31.125', 'counters/examples': 212288, 'counters/updates': 6634}
train stats after 212320 examples: {'rewards_train/chosen': '0.2044', 'rewards_train/rejected': '0.048568', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15583', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-114.04', 'loss/train': '0.62706', 'examples_per_second': '30.985', 'grad_norm': '25.5', 'counters/examples': 212320, 'counters/updates': 6635}
train stats after 212352 examples: {'rewards_train/chosen': '0.091707', 'rewards_train/rejected': '-0.013879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10559', 'logps_train/rejected': '-103.29', 'logps_train/chosen': '-124.54', 'loss/train': '0.65681', 'examples_per_second': '31.699', 'grad_norm': '22.625', 'counters/examples': 212352, 'counters/updates': 6636}
train stats after 212384 examples: {'rewards_train/chosen': '0.094496', 'rewards_train/rejected': '-0.031713', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12621', 'logps_train/rejected': '-121.09', 'logps_train/chosen': '-129.41', 'loss/train': '0.64182', 'examples_per_second': '31.221', 'grad_norm': '23.75', 'counters/examples': 212384, 'counters/updates': 6637}
train stats after 212416 examples: {'rewards_train/chosen': '0.19875', 'rewards_train/rejected': '0.035', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16375', 'logps_train/rejected': '-118.81', 'logps_train/chosen': '-152.5', 'loss/train': '0.63188', 'examples_per_second': '30.788', 'grad_norm': '26.75', 'counters/examples': 212416, 'counters/updates': 6638}
train stats after 212448 examples: {'rewards_train/chosen': '0.14448', 'rewards_train/rejected': '0.11329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03119', 'logps_train/rejected': '-134.95', 'logps_train/chosen': '-156.87', 'loss/train': '0.70229', 'examples_per_second': '31.584', 'grad_norm': '30.75', 'counters/examples': 212448, 'counters/updates': 6639}
train stats after 212480 examples: {'rewards_train/chosen': '0.12993', 'rewards_train/rejected': '-0.025499', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15543', 'logps_train/rejected': '-107.95', 'logps_train/chosen': '-123.65', 'loss/train': '0.62854', 'examples_per_second': '32.109', 'grad_norm': '23.5', 'counters/examples': 212480, 'counters/updates': 6640}
train stats after 212512 examples: {'rewards_train/chosen': '0.12104', 'rewards_train/rejected': '0.014495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10654', 'logps_train/rejected': '-108.86', 'logps_train/chosen': '-149.58', 'loss/train': '0.65675', 'examples_per_second': '31.6', 'grad_norm': '25.25', 'counters/examples': 212512, 'counters/updates': 6641}
train stats after 212544 examples: {'rewards_train/chosen': '0.2028', 'rewards_train/rejected': '0.067751', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13505', 'logps_train/rejected': '-107.88', 'logps_train/chosen': '-129.5', 'loss/train': '0.63865', 'examples_per_second': '29.878', 'grad_norm': '26.125', 'counters/examples': 212544, 'counters/updates': 6642}
train stats after 212576 examples: {'rewards_train/chosen': '0.21965', 'rewards_train/rejected': '0.065265', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15438', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-123.67', 'loss/train': '0.62676', 'examples_per_second': '30.49', 'grad_norm': '25.75', 'counters/examples': 212576, 'counters/updates': 6643}
train stats after 212608 examples: {'rewards_train/chosen': '0.034815', 'rewards_train/rejected': '0.014471', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020344', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-117.89', 'loss/train': '0.69865', 'examples_per_second': '31.35', 'grad_norm': '26.625', 'counters/examples': 212608, 'counters/updates': 6644}
train stats after 212640 examples: {'rewards_train/chosen': '0.15798', 'rewards_train/rejected': '0.0063124', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15166', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-127.77', 'loss/train': '0.63125', 'examples_per_second': '30.826', 'grad_norm': '25.25', 'counters/examples': 212640, 'counters/updates': 6645}
train stats after 212672 examples: {'rewards_train/chosen': '0.1855', 'rewards_train/rejected': '0.030658', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15484', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-151.45', 'loss/train': '0.63501', 'examples_per_second': '32.676', 'grad_norm': '26.125', 'counters/examples': 212672, 'counters/updates': 6646}
train stats after 212704 examples: {'rewards_train/chosen': '0.056853', 'rewards_train/rejected': '-0.009528', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066381', 'logps_train/rejected': '-121.5', 'logps_train/chosen': '-161.03', 'loss/train': '0.66948', 'examples_per_second': '32.677', 'grad_norm': '26.125', 'counters/examples': 212704, 'counters/updates': 6647}
train stats after 212736 examples: {'rewards_train/chosen': '0.11661', 'rewards_train/rejected': '0.0044605', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11215', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-148', 'loss/train': '0.64998', 'examples_per_second': '32.153', 'grad_norm': '29.75', 'counters/examples': 212736, 'counters/updates': 6648}
skipping logging after 212768 examples to avoid logging too frequently
train stats after 212800 examples: {'rewards_train/chosen': '0.13837', 'rewards_train/rejected': '0.1067', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.031666', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-159.5', 'loss/train': '0.69722', 'examples_per_second': '31.356', 'grad_norm': '33.25', 'counters/examples': 212800, 'counters/updates': 6650}
skipping logging after 212832 examples to avoid logging too frequently
train stats after 212864 examples: {'rewards_train/chosen': '0.10393', 'rewards_train/rejected': '0.070093', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033841', 'logps_train/rejected': '-118.68', 'logps_train/chosen': '-133.81', 'loss/train': '0.68534', 'examples_per_second': '30.773', 'grad_norm': '25.25', 'counters/examples': 212864, 'counters/updates': 6652}
skipping logging after 212896 examples to avoid logging too frequently
train stats after 212928 examples: {'rewards_train/chosen': '0.13531', 'rewards_train/rejected': '0.0019274', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-123.31', 'logps_train/chosen': '-127.57', 'loss/train': '0.64306', 'examples_per_second': '30.595', 'grad_norm': '23.375', 'counters/examples': 212928, 'counters/updates': 6654}
train stats after 212960 examples: {'rewards_train/chosen': '0.1254', 'rewards_train/rejected': '-0.0075522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13296', 'logps_train/rejected': '-121.43', 'logps_train/chosen': '-135.79', 'loss/train': '0.64371', 'examples_per_second': '31.047', 'grad_norm': '23.5', 'counters/examples': 212960, 'counters/updates': 6655}
train stats after 212992 examples: {'rewards_train/chosen': '0.07541', 'rewards_train/rejected': '-0.079191', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1546', 'logps_train/rejected': '-76.138', 'logps_train/chosen': '-155.07', 'loss/train': '0.63164', 'examples_per_second': '31.304', 'grad_norm': '24.75', 'counters/examples': 212992, 'counters/updates': 6656}
skipping logging after 213024 examples to avoid logging too frequently
train stats after 213056 examples: {'rewards_train/chosen': '0.11525', 'rewards_train/rejected': '-0.0001048', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11535', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-121.41', 'loss/train': '0.64907', 'examples_per_second': '31.612', 'grad_norm': '26', 'counters/examples': 213056, 'counters/updates': 6658}
train stats after 213088 examples: {'rewards_train/chosen': '0.13537', 'rewards_train/rejected': '0.022266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1131', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-136.22', 'loss/train': '0.64798', 'examples_per_second': '31.451', 'grad_norm': '22.875', 'counters/examples': 213088, 'counters/updates': 6659}
train stats after 213120 examples: {'rewards_train/chosen': '0.13222', 'rewards_train/rejected': '-0.014037', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14626', 'logps_train/rejected': '-105.96', 'logps_train/chosen': '-141.31', 'loss/train': '0.63941', 'examples_per_second': '30.212', 'grad_norm': '24', 'counters/examples': 213120, 'counters/updates': 6660}
train stats after 213152 examples: {'rewards_train/chosen': '0.078626', 'rewards_train/rejected': '-0.01674', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095366', 'logps_train/rejected': '-97.012', 'logps_train/chosen': '-136.33', 'loss/train': '0.65625', 'examples_per_second': '31.06', 'grad_norm': '24.875', 'counters/examples': 213152, 'counters/updates': 6661}
train stats after 213184 examples: {'rewards_train/chosen': '0.10983', 'rewards_train/rejected': '0.019656', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090169', 'logps_train/rejected': '-137.92', 'logps_train/chosen': '-144.4', 'loss/train': '0.6644', 'examples_per_second': '30.613', 'grad_norm': '25.625', 'counters/examples': 213184, 'counters/updates': 6662}
train stats after 213216 examples: {'rewards_train/chosen': '0.16244', 'rewards_train/rejected': '0.027441', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.135', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-152.53', 'loss/train': '0.64193', 'examples_per_second': '32.623', 'grad_norm': '28.75', 'counters/examples': 213216, 'counters/updates': 6663}
train stats after 213248 examples: {'rewards_train/chosen': '0.21627', 'rewards_train/rejected': '0.007741', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20853', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-145.67', 'loss/train': '0.61131', 'examples_per_second': '31.584', 'grad_norm': '24', 'counters/examples': 213248, 'counters/updates': 6664}
train stats after 213280 examples: {'rewards_train/chosen': '0.15302', 'rewards_train/rejected': '0.065963', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087059', 'logps_train/rejected': '-117.72', 'logps_train/chosen': '-124.97', 'loss/train': '0.6663', 'examples_per_second': '25.454', 'grad_norm': '24.375', 'counters/examples': 213280, 'counters/updates': 6665}
train stats after 213312 examples: {'rewards_train/chosen': '0.12472', 'rewards_train/rejected': '0.054635', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070085', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-135.68', 'loss/train': '0.66732', 'examples_per_second': '30.618', 'grad_norm': '25.25', 'counters/examples': 213312, 'counters/updates': 6666}
train stats after 213344 examples: {'rewards_train/chosen': '0.11551', 'rewards_train/rejected': '0.071614', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0439', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-124.43', 'loss/train': '0.68755', 'examples_per_second': '31.32', 'grad_norm': '27.75', 'counters/examples': 213344, 'counters/updates': 6667}
train stats after 213376 examples: {'rewards_train/chosen': '0.087925', 'rewards_train/rejected': '0.037503', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050422', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-167.88', 'loss/train': '0.67738', 'examples_per_second': '24.588', 'grad_norm': '28.375', 'counters/examples': 213376, 'counters/updates': 6668}
skipping logging after 213408 examples to avoid logging too frequently
train stats after 213440 examples: {'rewards_train/chosen': '0.19963', 'rewards_train/rejected': '0.063647', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13598', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-146.9', 'loss/train': '0.64025', 'examples_per_second': '30.529', 'grad_norm': '26.75', 'counters/examples': 213440, 'counters/updates': 6670}
skipping logging after 213472 examples to avoid logging too frequently
train stats after 213504 examples: {'rewards_train/chosen': '0.16754', 'rewards_train/rejected': '0.067049', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10049', 'logps_train/rejected': '-113.17', 'logps_train/chosen': '-145.37', 'loss/train': '0.65386', 'examples_per_second': '34.162', 'grad_norm': '26.875', 'counters/examples': 213504, 'counters/updates': 6672}
skipping logging after 213536 examples to avoid logging too frequently
train stats after 213568 examples: {'rewards_train/chosen': '0.19024', 'rewards_train/rejected': '0.027944', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.1623', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-129.16', 'loss/train': '0.63008', 'examples_per_second': '33.947', 'grad_norm': '23.875', 'counters/examples': 213568, 'counters/updates': 6674}
skipping logging after 213600 examples to avoid logging too frequently
train stats after 213632 examples: {'rewards_train/chosen': '0.16873', 'rewards_train/rejected': '0.040704', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12803', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-152.8', 'loss/train': '0.64326', 'examples_per_second': '30.957', 'grad_norm': '35', 'counters/examples': 213632, 'counters/updates': 6676}
train stats after 213664 examples: {'rewards_train/chosen': '0.13301', 'rewards_train/rejected': '-0.033011', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16602', 'logps_train/rejected': '-92.34', 'logps_train/chosen': '-182.12', 'loss/train': '0.62732', 'examples_per_second': '33.115', 'grad_norm': '26.75', 'counters/examples': 213664, 'counters/updates': 6677}
train stats after 213696 examples: {'rewards_train/chosen': '0.19208', 'rewards_train/rejected': '0.11857', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073515', 'logps_train/rejected': '-153.16', 'logps_train/chosen': '-143.96', 'loss/train': '0.66853', 'examples_per_second': '30.121', 'grad_norm': '31.5', 'counters/examples': 213696, 'counters/updates': 6678}
train stats after 213728 examples: {'rewards_train/chosen': '0.22369', 'rewards_train/rejected': '0.0072413', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21645', 'logps_train/rejected': '-107.47', 'logps_train/chosen': '-139.87', 'loss/train': '0.60909', 'examples_per_second': '31.554', 'grad_norm': '20.625', 'counters/examples': 213728, 'counters/updates': 6679}
train stats after 213760 examples: {'rewards_train/chosen': '0.15953', 'rewards_train/rejected': '0.040434', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1191', 'logps_train/rejected': '-154.42', 'logps_train/chosen': '-177.84', 'loss/train': '0.65185', 'examples_per_second': '29.735', 'grad_norm': '28.625', 'counters/examples': 213760, 'counters/updates': 6680}
train stats after 213792 examples: {'rewards_train/chosen': '0.041798', 'rewards_train/rejected': '0.02951', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012288', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-122.25', 'loss/train': '0.69927', 'examples_per_second': '30.12', 'grad_norm': '24.875', 'counters/examples': 213792, 'counters/updates': 6681}
train stats after 213824 examples: {'rewards_train/chosen': '0.24035', 'rewards_train/rejected': '0.00055548', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2398', 'logps_train/rejected': '-124.38', 'logps_train/chosen': '-168.5', 'loss/train': '0.59798', 'examples_per_second': '30.762', 'grad_norm': '23.125', 'counters/examples': 213824, 'counters/updates': 6682}
train stats after 213856 examples: {'rewards_train/chosen': '0.10329', 'rewards_train/rejected': '0.033825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069468', 'logps_train/rejected': '-105.39', 'logps_train/chosen': '-120.95', 'loss/train': '0.66726', 'examples_per_second': '32.404', 'grad_norm': '25', 'counters/examples': 213856, 'counters/updates': 6683}
train stats after 213888 examples: {'rewards_train/chosen': '0.13815', 'rewards_train/rejected': '0.018672', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11948', 'logps_train/rejected': '-119.98', 'logps_train/chosen': '-150.03', 'loss/train': '0.64928', 'examples_per_second': '31.587', 'grad_norm': '28.125', 'counters/examples': 213888, 'counters/updates': 6684}
train stats after 213920 examples: {'rewards_train/chosen': '0.11712', 'rewards_train/rejected': '-0.014075', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1312', 'logps_train/rejected': '-109.48', 'logps_train/chosen': '-165.07', 'loss/train': '0.64492', 'examples_per_second': '31.79', 'grad_norm': '26.625', 'counters/examples': 213920, 'counters/updates': 6685}
train stats after 213952 examples: {'rewards_train/chosen': '0.017', 'rewards_train/rejected': '-0.03096', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.047961', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-136.12', 'loss/train': '0.68284', 'examples_per_second': '31.582', 'grad_norm': '27.375', 'counters/examples': 213952, 'counters/updates': 6686}
train stats after 213984 examples: {'rewards_train/chosen': '0.091477', 'rewards_train/rejected': '0.023682', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067795', 'logps_train/rejected': '-110.76', 'logps_train/chosen': '-136.52', 'loss/train': '0.66866', 'examples_per_second': '31.626', 'grad_norm': '24.125', 'counters/examples': 213984, 'counters/updates': 6687}
train stats after 214016 examples: {'rewards_train/chosen': '0.14228', 'rewards_train/rejected': '-0.022285', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16456', 'logps_train/rejected': '-115.55', 'logps_train/chosen': '-131.85', 'loss/train': '0.6261', 'examples_per_second': '31.649', 'grad_norm': '23.25', 'counters/examples': 214016, 'counters/updates': 6688}
train stats after 214048 examples: {'rewards_train/chosen': '0.14353', 'rewards_train/rejected': '0.039786', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10375', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-138.72', 'loss/train': '0.65873', 'examples_per_second': '31.031', 'grad_norm': '28.25', 'counters/examples': 214048, 'counters/updates': 6689}
train stats after 214080 examples: {'rewards_train/chosen': '0.15531', 'rewards_train/rejected': '0.0098854', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14543', 'logps_train/rejected': '-149.59', 'logps_train/chosen': '-141.24', 'loss/train': '0.63853', 'examples_per_second': '31.466', 'grad_norm': '24.75', 'counters/examples': 214080, 'counters/updates': 6690}
train stats after 214112 examples: {'rewards_train/chosen': '0.072622', 'rewards_train/rejected': '-0.00025095', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.072873', 'logps_train/rejected': '-104.7', 'logps_train/chosen': '-124.66', 'loss/train': '0.66937', 'examples_per_second': '32.161', 'grad_norm': '28.25', 'counters/examples': 214112, 'counters/updates': 6691}
train stats after 214144 examples: {'rewards_train/chosen': '0.040254', 'rewards_train/rejected': '0.026233', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014021', 'logps_train/rejected': '-86.869', 'logps_train/chosen': '-90.407', 'loss/train': '0.69449', 'examples_per_second': '31.52', 'grad_norm': '22.5', 'counters/examples': 214144, 'counters/updates': 6692}
train stats after 214176 examples: {'rewards_train/chosen': '0.091123', 'rewards_train/rejected': '0.0095853', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081538', 'logps_train/rejected': '-115.59', 'logps_train/chosen': '-138.65', 'loss/train': '0.66307', 'examples_per_second': '31.09', 'grad_norm': '26', 'counters/examples': 214176, 'counters/updates': 6693}
train stats after 214208 examples: {'rewards_train/chosen': '0.18723', 'rewards_train/rejected': '-0.020313', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20754', 'logps_train/rejected': '-85.602', 'logps_train/chosen': '-125.36', 'loss/train': '0.6046', 'examples_per_second': '32.401', 'grad_norm': '20', 'counters/examples': 214208, 'counters/updates': 6694}
train stats after 214240 examples: {'rewards_train/chosen': '-0.0080059', 'rewards_train/rejected': '0.034673', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.042679', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-137.09', 'loss/train': '0.72783', 'examples_per_second': '31.607', 'grad_norm': '30.375', 'counters/examples': 214240, 'counters/updates': 6695}
train stats after 214272 examples: {'rewards_train/chosen': '0.12289', 'rewards_train/rejected': '0.0028393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12005', 'logps_train/rejected': '-133.78', 'logps_train/chosen': '-178.3', 'loss/train': '0.64823', 'examples_per_second': '30.138', 'grad_norm': '26.25', 'counters/examples': 214272, 'counters/updates': 6696}
train stats after 214304 examples: {'rewards_train/chosen': '0.13293', 'rewards_train/rejected': '0.020754', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11218', 'logps_train/rejected': '-104.85', 'logps_train/chosen': '-160.88', 'loss/train': '0.6496', 'examples_per_second': '31.942', 'grad_norm': '25.5', 'counters/examples': 214304, 'counters/updates': 6697}
train stats after 214336 examples: {'rewards_train/chosen': '0.21388', 'rewards_train/rejected': '0.056658', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15722', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-122.23', 'loss/train': '0.63534', 'examples_per_second': '32.331', 'grad_norm': '22.5', 'counters/examples': 214336, 'counters/updates': 6698}
train stats after 214368 examples: {'rewards_train/chosen': '0.077555', 'rewards_train/rejected': '-0.046773', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12433', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-134.61', 'loss/train': '0.64518', 'examples_per_second': '31.152', 'grad_norm': '23.375', 'counters/examples': 214368, 'counters/updates': 6699}
train stats after 214400 examples: {'rewards_train/chosen': '0.084022', 'rewards_train/rejected': '-0.011212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095234', 'logps_train/rejected': '-110.81', 'logps_train/chosen': '-143.04', 'loss/train': '0.66215', 'examples_per_second': '31.998', 'grad_norm': '25', 'counters/examples': 214400, 'counters/updates': 6700}
train stats after 214432 examples: {'rewards_train/chosen': '0.10144', 'rewards_train/rejected': '0.027354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074087', 'logps_train/rejected': '-103.21', 'logps_train/chosen': '-148.2', 'loss/train': '0.67499', 'examples_per_second': '30.424', 'grad_norm': '28.125', 'counters/examples': 214432, 'counters/updates': 6701}
train stats after 214464 examples: {'rewards_train/chosen': '0.21102', 'rewards_train/rejected': '-0.010105', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22113', 'logps_train/rejected': '-139.39', 'logps_train/chosen': '-139.58', 'loss/train': '0.60883', 'examples_per_second': '31.608', 'grad_norm': '22.75', 'counters/examples': 214464, 'counters/updates': 6702}
skipping logging after 214496 examples to avoid logging too frequently
train stats after 214528 examples: {'rewards_train/chosen': '0.17197', 'rewards_train/rejected': '0.053793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11818', 'logps_train/rejected': '-107.42', 'logps_train/chosen': '-148.09', 'loss/train': '0.64904', 'examples_per_second': '31.564', 'grad_norm': '26.25', 'counters/examples': 214528, 'counters/updates': 6704}
train stats after 214560 examples: {'rewards_train/chosen': '0.15977', 'rewards_train/rejected': '0.088688', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071081', 'logps_train/rejected': '-110.45', 'logps_train/chosen': '-119.52', 'loss/train': '0.66573', 'examples_per_second': '31.485', 'grad_norm': '23', 'counters/examples': 214560, 'counters/updates': 6705}
train stats after 214592 examples: {'rewards_train/chosen': '0.020166', 'rewards_train/rejected': '0.031695', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011529', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-178.81', 'loss/train': '0.71541', 'examples_per_second': '31.676', 'grad_norm': '33', 'counters/examples': 214592, 'counters/updates': 6706}
skipping logging after 214624 examples to avoid logging too frequently
train stats after 214656 examples: {'rewards_train/chosen': '0.11457', 'rewards_train/rejected': '-0.012233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12681', 'logps_train/rejected': '-103.06', 'logps_train/chosen': '-111.7', 'loss/train': '0.64293', 'examples_per_second': '35.938', 'grad_norm': '21.625', 'counters/examples': 214656, 'counters/updates': 6708}
train stats after 214688 examples: {'rewards_train/chosen': '0.067145', 'rewards_train/rejected': '-0.032856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1', 'logps_train/rejected': '-80.817', 'logps_train/chosen': '-131.02', 'loss/train': '0.66007', 'examples_per_second': '32.321', 'grad_norm': '21.5', 'counters/examples': 214688, 'counters/updates': 6709}
train stats after 214720 examples: {'rewards_train/chosen': '0.15731', 'rewards_train/rejected': '0.036641', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12067', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-149.99', 'loss/train': '0.65057', 'examples_per_second': '30.096', 'grad_norm': '23', 'counters/examples': 214720, 'counters/updates': 6710}
train stats after 214752 examples: {'rewards_train/chosen': '0.15478', 'rewards_train/rejected': '0.014458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14032', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-192.35', 'loss/train': '0.65178', 'examples_per_second': '31.639', 'grad_norm': '26.625', 'counters/examples': 214752, 'counters/updates': 6711}
skipping logging after 214784 examples to avoid logging too frequently
train stats after 214816 examples: {'rewards_train/chosen': '0.076762', 'rewards_train/rejected': '0.0481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028662', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-96.536', 'loss/train': '0.69745', 'examples_per_second': '31.717', 'grad_norm': '24.875', 'counters/examples': 214816, 'counters/updates': 6713}
train stats after 214848 examples: {'rewards_train/chosen': '0.11187', 'rewards_train/rejected': '-0.0098701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12174', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-123.87', 'loss/train': '0.65225', 'examples_per_second': '31.241', 'grad_norm': '24.375', 'counters/examples': 214848, 'counters/updates': 6714}
train stats after 214880 examples: {'rewards_train/chosen': '0.12774', 'rewards_train/rejected': '0.0045501', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12319', 'logps_train/rejected': '-121.28', 'logps_train/chosen': '-138.91', 'loss/train': '0.64022', 'examples_per_second': '33.095', 'grad_norm': '23.5', 'counters/examples': 214880, 'counters/updates': 6715}
train stats after 214912 examples: {'rewards_train/chosen': '0.061478', 'rewards_train/rejected': '0.0032862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058192', 'logps_train/rejected': '-128.9', 'logps_train/chosen': '-136.15', 'loss/train': '0.67164', 'examples_per_second': '32.877', 'grad_norm': '25.875', 'counters/examples': 214912, 'counters/updates': 6716}
train stats after 214944 examples: {'rewards_train/chosen': '0.14829', 'rewards_train/rejected': '0.095529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052758', 'logps_train/rejected': '-106.45', 'logps_train/chosen': '-142.06', 'loss/train': '0.67715', 'examples_per_second': '30.646', 'grad_norm': '25.25', 'counters/examples': 214944, 'counters/updates': 6717}
train stats after 214976 examples: {'rewards_train/chosen': '0.10622', 'rewards_train/rejected': '0.046837', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059384', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-137.95', 'loss/train': '0.68054', 'examples_per_second': '31.623', 'grad_norm': '27.625', 'counters/examples': 214976, 'counters/updates': 6718}
skipping logging after 215008 examples to avoid logging too frequently
train stats after 215040 examples: {'rewards_train/chosen': '0.13488', 'rewards_train/rejected': '0.073749', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061131', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-147.55', 'loss/train': '0.6815', 'examples_per_second': '30.259', 'grad_norm': '71', 'counters/examples': 215040, 'counters/updates': 6720}
skipping logging after 215072 examples to avoid logging too frequently
train stats after 215104 examples: {'rewards_train/chosen': '0.19423', 'rewards_train/rejected': '0.014597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17963', 'logps_train/rejected': '-105.06', 'logps_train/chosen': '-182.46', 'loss/train': '0.62915', 'examples_per_second': '33.369', 'grad_norm': '24.625', 'counters/examples': 215104, 'counters/updates': 6722}
train stats after 215136 examples: {'rewards_train/chosen': '0.2405', 'rewards_train/rejected': '-0.066625', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.30713', 'logps_train/rejected': '-120.26', 'logps_train/chosen': '-169.98', 'loss/train': '0.56076', 'examples_per_second': '31.571', 'grad_norm': '24.5', 'counters/examples': 215136, 'counters/updates': 6723}
train stats after 215168 examples: {'rewards_train/chosen': '0.13516', 'rewards_train/rejected': '0.049209', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085955', 'logps_train/rejected': '-147.79', 'logps_train/chosen': '-113.98', 'loss/train': '0.6678', 'examples_per_second': '30.175', 'grad_norm': '28.75', 'counters/examples': 215168, 'counters/updates': 6724}
train stats after 215200 examples: {'rewards_train/chosen': '0.11149', 'rewards_train/rejected': '0.020703', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090782', 'logps_train/rejected': '-117.35', 'logps_train/chosen': '-120.14', 'loss/train': '0.66315', 'examples_per_second': '31.537', 'grad_norm': '24.125', 'counters/examples': 215200, 'counters/updates': 6725}
train stats after 215232 examples: {'rewards_train/chosen': '0.072886', 'rewards_train/rejected': '0.0055461', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06734', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-128', 'loss/train': '0.6677', 'examples_per_second': '31.618', 'grad_norm': '22.625', 'counters/examples': 215232, 'counters/updates': 6726}
train stats after 215264 examples: {'rewards_train/chosen': '0.075628', 'rewards_train/rejected': '-0.013253', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088881', 'logps_train/rejected': '-189.07', 'logps_train/chosen': '-164.3', 'loss/train': '0.6688', 'examples_per_second': '30.645', 'grad_norm': '31.25', 'counters/examples': 215264, 'counters/updates': 6727}
train stats after 215296 examples: {'rewards_train/chosen': '0.28251', 'rewards_train/rejected': '-0.010571', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.29308', 'logps_train/rejected': '-106.16', 'logps_train/chosen': '-149.53', 'loss/train': '0.57807', 'examples_per_second': '30.74', 'grad_norm': '20.875', 'counters/examples': 215296, 'counters/updates': 6728}
train stats after 215328 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '-0.047093', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18287', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-140.95', 'loss/train': '0.6164', 'examples_per_second': '31.085', 'grad_norm': '26', 'counters/examples': 215328, 'counters/updates': 6729}
skipping logging after 215360 examples to avoid logging too frequently
train stats after 215392 examples: {'rewards_train/chosen': '0.088218', 'rewards_train/rejected': '0.02101', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067208', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-102.19', 'loss/train': '0.66733', 'examples_per_second': '33.946', 'grad_norm': '23.5', 'counters/examples': 215392, 'counters/updates': 6731}
train stats after 215424 examples: {'rewards_train/chosen': '0.19151', 'rewards_train/rejected': '0.025104', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1664', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-151.05', 'loss/train': '0.63063', 'examples_per_second': '32.058', 'grad_norm': '26.125', 'counters/examples': 215424, 'counters/updates': 6732}
train stats after 215456 examples: {'rewards_train/chosen': '0.1411', 'rewards_train/rejected': '-0.034171', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17527', 'logps_train/rejected': '-120.8', 'logps_train/chosen': '-128.18', 'loss/train': '0.61914', 'examples_per_second': '31.117', 'grad_norm': '23.5', 'counters/examples': 215456, 'counters/updates': 6733}
train stats after 215488 examples: {'rewards_train/chosen': '0.089741', 'rewards_train/rejected': '0.06053', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029211', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-133', 'loss/train': '0.69122', 'examples_per_second': '31.854', 'grad_norm': '28', 'counters/examples': 215488, 'counters/updates': 6734}
skipping logging after 215520 examples to avoid logging too frequently
train stats after 215552 examples: {'rewards_train/chosen': '0.091415', 'rewards_train/rejected': '0.074115', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0173', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-119.63', 'loss/train': '0.69772', 'examples_per_second': '33.39', 'grad_norm': '32.5', 'counters/examples': 215552, 'counters/updates': 6736}
train stats after 215584 examples: {'rewards_train/chosen': '0.15927', 'rewards_train/rejected': '0.088564', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070704', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-144.25', 'loss/train': '0.67042', 'examples_per_second': '31.346', 'grad_norm': '26.625', 'counters/examples': 215584, 'counters/updates': 6737}
train stats after 215616 examples: {'rewards_train/chosen': '0.13057', 'rewards_train/rejected': '0.10784', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022727', 'logps_train/rejected': '-141.99', 'logps_train/chosen': '-132.43', 'loss/train': '0.69313', 'examples_per_second': '30.653', 'grad_norm': '29.25', 'counters/examples': 215616, 'counters/updates': 6738}
train stats after 215648 examples: {'rewards_train/chosen': '0.15087', 'rewards_train/rejected': '-0.013534', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1644', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-151.97', 'loss/train': '0.62452', 'examples_per_second': '31.007', 'grad_norm': '22.625', 'counters/examples': 215648, 'counters/updates': 6739}
skipping logging after 215680 examples to avoid logging too frequently
train stats after 215712 examples: {'rewards_train/chosen': '0.16977', 'rewards_train/rejected': '-0.027635', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1974', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-166.75', 'loss/train': '0.613', 'examples_per_second': '33.504', 'grad_norm': '26.625', 'counters/examples': 215712, 'counters/updates': 6741}
train stats after 215744 examples: {'rewards_train/chosen': '0.2004', 'rewards_train/rejected': '0.061427', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13897', 'logps_train/rejected': '-106.32', 'logps_train/chosen': '-140.91', 'loss/train': '0.63694', 'examples_per_second': '30.571', 'grad_norm': '24.125', 'counters/examples': 215744, 'counters/updates': 6742}
skipping logging after 215776 examples to avoid logging too frequently
train stats after 215808 examples: {'rewards_train/chosen': '0.10568', 'rewards_train/rejected': '0.046608', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059071', 'logps_train/rejected': '-138.14', 'logps_train/chosen': '-156.48', 'loss/train': '0.67642', 'examples_per_second': '31.959', 'grad_norm': '29.5', 'counters/examples': 215808, 'counters/updates': 6744}
train stats after 215840 examples: {'rewards_train/chosen': '0.080998', 'rewards_train/rejected': '-0.017839', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098837', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-139.13', 'loss/train': '0.6615', 'examples_per_second': '30.501', 'grad_norm': '26.875', 'counters/examples': 215840, 'counters/updates': 6745}
train stats after 215872 examples: {'rewards_train/chosen': '0.12576', 'rewards_train/rejected': '0.030371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095392', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-142.24', 'loss/train': '0.65345', 'examples_per_second': '31.65', 'grad_norm': '25.5', 'counters/examples': 215872, 'counters/updates': 6746}
train stats after 215904 examples: {'rewards_train/chosen': '0.15232', 'rewards_train/rejected': '0.051832', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10049', 'logps_train/rejected': '-154.32', 'logps_train/chosen': '-141.92', 'loss/train': '0.66', 'examples_per_second': '31.627', 'grad_norm': '29.25', 'counters/examples': 215904, 'counters/updates': 6747}
skipping logging after 215936 examples to avoid logging too frequently
train stats after 215968 examples: {'rewards_train/chosen': '0.077761', 'rewards_train/rejected': '-0.030395', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10816', 'logps_train/rejected': '-102.34', 'logps_train/chosen': '-152.12', 'loss/train': '0.65879', 'examples_per_second': '36.024', 'grad_norm': '35.25', 'counters/examples': 215968, 'counters/updates': 6749}
train stats after 216000 examples: {'rewards_train/chosen': '0.23718', 'rewards_train/rejected': '0.072437', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16475', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-140.78', 'loss/train': '0.63275', 'examples_per_second': '30.248', 'grad_norm': '31', 'counters/examples': 216000, 'counters/updates': 6750}
Running evaluation after 216000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.27it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.95it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.92it/s]
eval after 216000: {'rewards_eval/chosen': '0.1437', 'rewards_eval/rejected': '0.03507', 'rewards_eval/accuracies': '0.61719', 'rewards_eval/margins': '0.10863', 'logps_eval/rejected': '-118.26', 'logps_eval/chosen': '-138', 'loss/eval': '0.65503'}
skipping save for non epoch
train stats after 216032 examples: {'rewards_train/chosen': '0.20165', 'rewards_train/rejected': '0.074332', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12732', 'logps_train/rejected': '-127.39', 'logps_train/chosen': '-144.07', 'loss/train': '0.6516', 'examples_per_second': '34.35', 'grad_norm': '25.625', 'counters/examples': 216032, 'counters/updates': 6751}
train stats after 216064 examples: {'rewards_train/chosen': '0.19424', 'rewards_train/rejected': '0.042673', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15157', 'logps_train/rejected': '-135.2', 'logps_train/chosen': '-136.53', 'loss/train': '0.63877', 'examples_per_second': '32.45', 'grad_norm': '32', 'counters/examples': 216064, 'counters/updates': 6752}
skipping logging after 216096 examples to avoid logging too frequently
train stats after 216128 examples: {'rewards_train/chosen': '0.18385', 'rewards_train/rejected': '-0.045331', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22918', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-202.4', 'loss/train': '0.59475', 'examples_per_second': '30.283', 'grad_norm': '26.5', 'counters/examples': 216128, 'counters/updates': 6754}
train stats after 216160 examples: {'rewards_train/chosen': '0.069753', 'rewards_train/rejected': '0.025638', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044115', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-133.32', 'loss/train': '0.68575', 'examples_per_second': '32.17', 'grad_norm': '29', 'counters/examples': 216160, 'counters/updates': 6755}
train stats after 216192 examples: {'rewards_train/chosen': '0.1868', 'rewards_train/rejected': '0.089485', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097315', 'logps_train/rejected': '-169.74', 'logps_train/chosen': '-154.85', 'loss/train': '0.66517', 'examples_per_second': '31.276', 'grad_norm': '29', 'counters/examples': 216192, 'counters/updates': 6756}
train stats after 216224 examples: {'rewards_train/chosen': '0.15793', 'rewards_train/rejected': '0.032676', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12526', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-117.78', 'loss/train': '0.64781', 'examples_per_second': '31.008', 'grad_norm': '24.375', 'counters/examples': 216224, 'counters/updates': 6757}
train stats after 216256 examples: {'rewards_train/chosen': '0.18864', 'rewards_train/rejected': '0.038758', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14988', 'logps_train/rejected': '-131.21', 'logps_train/chosen': '-177.63', 'loss/train': '0.64339', 'examples_per_second': '32.389', 'grad_norm': '26.125', 'counters/examples': 216256, 'counters/updates': 6758}
train stats after 216288 examples: {'rewards_train/chosen': '0.12856', 'rewards_train/rejected': '0.06218', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066376', 'logps_train/rejected': '-159.14', 'logps_train/chosen': '-121.82', 'loss/train': '0.67778', 'examples_per_second': '31.321', 'grad_norm': '26.875', 'counters/examples': 216288, 'counters/updates': 6759}
train stats after 216320 examples: {'rewards_train/chosen': '0.19711', 'rewards_train/rejected': '0.056679', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14043', 'logps_train/rejected': '-146.37', 'logps_train/chosen': '-186.59', 'loss/train': '0.63862', 'examples_per_second': '31.646', 'grad_norm': '27.5', 'counters/examples': 216320, 'counters/updates': 6760}
train stats after 216352 examples: {'rewards_train/chosen': '0.089717', 'rewards_train/rejected': '0.01301', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076707', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-160.06', 'loss/train': '0.67371', 'examples_per_second': '33.138', 'grad_norm': '31.25', 'counters/examples': 216352, 'counters/updates': 6761}
train stats after 216384 examples: {'rewards_train/chosen': '0.16154', 'rewards_train/rejected': '-0.018995', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18054', 'logps_train/rejected': '-98.538', 'logps_train/chosen': '-156.62', 'loss/train': '0.61729', 'examples_per_second': '31.629', 'grad_norm': '25.125', 'counters/examples': 216384, 'counters/updates': 6762}
train stats after 216416 examples: {'rewards_train/chosen': '0.14314', 'rewards_train/rejected': '0.04942', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093715', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-131.44', 'loss/train': '0.65578', 'examples_per_second': '30.135', 'grad_norm': '26.25', 'counters/examples': 216416, 'counters/updates': 6763}
skipping logging after 216448 examples to avoid logging too frequently
train stats after 216480 examples: {'rewards_train/chosen': '0.17713', 'rewards_train/rejected': '-0.0065864', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18371', 'logps_train/rejected': '-128.81', 'logps_train/chosen': '-145.14', 'loss/train': '0.61497', 'examples_per_second': '33.61', 'grad_norm': '24', 'counters/examples': 216480, 'counters/updates': 6765}
train stats after 216512 examples: {'rewards_train/chosen': '0.24617', 'rewards_train/rejected': '-0.0094953', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25566', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-135.15', 'loss/train': '0.59003', 'examples_per_second': '31.784', 'grad_norm': '23.25', 'counters/examples': 216512, 'counters/updates': 6766}
train stats after 216544 examples: {'rewards_train/chosen': '0.17711', 'rewards_train/rejected': '0.033075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14404', 'logps_train/rejected': '-161.26', 'logps_train/chosen': '-173.55', 'loss/train': '0.63779', 'examples_per_second': '31.647', 'grad_norm': '27.625', 'counters/examples': 216544, 'counters/updates': 6767}
train stats after 216576 examples: {'rewards_train/chosen': '0.15869', 'rewards_train/rejected': '0.044657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11403', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-137.81', 'loss/train': '0.64995', 'examples_per_second': '30.528', 'grad_norm': '26.375', 'counters/examples': 216576, 'counters/updates': 6768}
train stats after 216608 examples: {'rewards_train/chosen': '0.13494', 'rewards_train/rejected': '-0.00081077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13575', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-116.56', 'loss/train': '0.63471', 'examples_per_second': '31.876', 'grad_norm': '23.25', 'counters/examples': 216608, 'counters/updates': 6769}
train stats after 216640 examples: {'rewards_train/chosen': '0.075074', 'rewards_train/rejected': '0.029329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045745', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-124.77', 'loss/train': '0.68162', 'examples_per_second': '30.241', 'grad_norm': '26.25', 'counters/examples': 216640, 'counters/updates': 6770}
skipping logging after 216672 examples to avoid logging too frequently
train stats after 216704 examples: {'rewards_train/chosen': '0.15585', 'rewards_train/rejected': '-0.060535', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21639', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-162.89', 'loss/train': '0.60609', 'examples_per_second': '32.117', 'grad_norm': '27.625', 'counters/examples': 216704, 'counters/updates': 6772}
train stats after 216736 examples: {'rewards_train/chosen': '0.1345', 'rewards_train/rejected': '0.0081261', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12637', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-154.1', 'loss/train': '0.63787', 'examples_per_second': '31.016', 'grad_norm': '26.5', 'counters/examples': 216736, 'counters/updates': 6773}
train stats after 216768 examples: {'rewards_train/chosen': '0.16003', 'rewards_train/rejected': '-0.011603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17164', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-116.37', 'loss/train': '0.62897', 'examples_per_second': '31.641', 'grad_norm': '24.75', 'counters/examples': 216768, 'counters/updates': 6774}
skipping logging after 216800 examples to avoid logging too frequently
train stats after 216832 examples: {'rewards_train/chosen': '0.10716', 'rewards_train/rejected': '-0.039566', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14672', 'logps_train/rejected': '-94.882', 'logps_train/chosen': '-130.81', 'loss/train': '0.63758', 'examples_per_second': '30.171', 'grad_norm': '24.125', 'counters/examples': 216832, 'counters/updates': 6776}
skipping logging after 216864 examples to avoid logging too frequently
train stats after 216896 examples: {'rewards_train/chosen': '0.10564', 'rewards_train/rejected': '-0.028621', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.13426', 'logps_train/rejected': '-88.975', 'logps_train/chosen': '-124.3', 'loss/train': '0.63846', 'examples_per_second': '35.638', 'grad_norm': '24.75', 'counters/examples': 216896, 'counters/updates': 6778}
train stats after 216928 examples: {'rewards_train/chosen': '0.12152', 'rewards_train/rejected': '0.044767', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07675', 'logps_train/rejected': '-104.63', 'logps_train/chosen': '-130.38', 'loss/train': '0.66647', 'examples_per_second': '33.135', 'grad_norm': '25.75', 'counters/examples': 216928, 'counters/updates': 6779}
train stats after 216960 examples: {'rewards_train/chosen': '0.17904', 'rewards_train/rejected': '0.078771', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10027', 'logps_train/rejected': '-102.91', 'logps_train/chosen': '-140.66', 'loss/train': '0.65116', 'examples_per_second': '31.661', 'grad_norm': '24.25', 'counters/examples': 216960, 'counters/updates': 6780}
train stats after 216992 examples: {'rewards_train/chosen': '0.08948', 'rewards_train/rejected': '-0.02669', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11617', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-109.75', 'loss/train': '0.65261', 'examples_per_second': '32.733', 'grad_norm': '26.75', 'counters/examples': 216992, 'counters/updates': 6781}
train stats after 217024 examples: {'rewards_train/chosen': '0.19516', 'rewards_train/rejected': '0.028154', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16701', 'logps_train/rejected': '-110.82', 'logps_train/chosen': '-146.02', 'loss/train': '0.62137', 'examples_per_second': '31.01', 'grad_norm': '23.875', 'counters/examples': 217024, 'counters/updates': 6782}
train stats after 217056 examples: {'rewards_train/chosen': '0.15416', 'rewards_train/rejected': '0.015633', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13853', 'logps_train/rejected': '-121.2', 'logps_train/chosen': '-160.67', 'loss/train': '0.64389', 'examples_per_second': '31.711', 'grad_norm': '31', 'counters/examples': 217056, 'counters/updates': 6783}
skipping logging after 217088 examples to avoid logging too frequently
train stats after 217120 examples: {'rewards_train/chosen': '0.10754', 'rewards_train/rejected': '-0.025499', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13304', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-102.31', 'loss/train': '0.64512', 'examples_per_second': '34.316', 'grad_norm': '21.875', 'counters/examples': 217120, 'counters/updates': 6785}
train stats after 217152 examples: {'rewards_train/chosen': '0.062312', 'rewards_train/rejected': '0.043691', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018621', 'logps_train/rejected': '-117.17', 'logps_train/chosen': '-150.22', 'loss/train': '0.69727', 'examples_per_second': '31.142', 'grad_norm': '29.75', 'counters/examples': 217152, 'counters/updates': 6786}
train stats after 217184 examples: {'rewards_train/chosen': '0.15951', 'rewards_train/rejected': '0.086584', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072923', 'logps_train/rejected': '-144.17', 'logps_train/chosen': '-150.09', 'loss/train': '0.66609', 'examples_per_second': '31.65', 'grad_norm': '28.875', 'counters/examples': 217184, 'counters/updates': 6787}
train stats after 217216 examples: {'rewards_train/chosen': '0.11455', 'rewards_train/rejected': '0.006371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10818', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-141.94', 'loss/train': '0.65288', 'examples_per_second': '31.466', 'grad_norm': '29.375', 'counters/examples': 217216, 'counters/updates': 6788}
train stats after 217248 examples: {'rewards_train/chosen': '0.17259', 'rewards_train/rejected': '0.0089488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16364', 'logps_train/rejected': '-172.89', 'logps_train/chosen': '-164.97', 'loss/train': '0.63047', 'examples_per_second': '32.472', 'grad_norm': '29.375', 'counters/examples': 217248, 'counters/updates': 6789}
train stats after 217280 examples: {'rewards_train/chosen': '0.17576', 'rewards_train/rejected': '-0.038488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21425', 'logps_train/rejected': '-175.25', 'logps_train/chosen': '-174.9', 'loss/train': '0.62431', 'examples_per_second': '31.624', 'grad_norm': '27.375', 'counters/examples': 217280, 'counters/updates': 6790}
train stats after 217312 examples: {'rewards_train/chosen': '0.092713', 'rewards_train/rejected': '-0.0054527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098166', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-121.34', 'loss/train': '0.65642', 'examples_per_second': '31.652', 'grad_norm': '26.75', 'counters/examples': 217312, 'counters/updates': 6791}
train stats after 217344 examples: {'rewards_train/chosen': '0.096667', 'rewards_train/rejected': '0.012594', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084072', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-158.77', 'loss/train': '0.66221', 'examples_per_second': '31.208', 'grad_norm': '25.125', 'counters/examples': 217344, 'counters/updates': 6792}
train stats after 217376 examples: {'rewards_train/chosen': '0.12604', 'rewards_train/rejected': '-0.020315', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14635', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-122.04', 'loss/train': '0.63265', 'examples_per_second': '30.576', 'grad_norm': '22.625', 'counters/examples': 217376, 'counters/updates': 6793}
train stats after 217408 examples: {'rewards_train/chosen': '0.1107', 'rewards_train/rejected': '0.032432', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078271', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-153.02', 'loss/train': '0.6635', 'examples_per_second': '31.313', 'grad_norm': '26', 'counters/examples': 217408, 'counters/updates': 6794}
train stats after 217440 examples: {'rewards_train/chosen': '0.2123', 'rewards_train/rejected': '0.10015', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11215', 'logps_train/rejected': '-110.25', 'logps_train/chosen': '-129.89', 'loss/train': '0.65056', 'examples_per_second': '30.678', 'grad_norm': '26.375', 'counters/examples': 217440, 'counters/updates': 6795}
skipping logging after 217472 examples to avoid logging too frequently
train stats after 217504 examples: {'rewards_train/chosen': '0.16065', 'rewards_train/rejected': '0.0022133', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15843', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-167.08', 'loss/train': '0.62668', 'examples_per_second': '33.559', 'grad_norm': '26.625', 'counters/examples': 217504, 'counters/updates': 6797}
train stats after 217536 examples: {'rewards_train/chosen': '0.072884', 'rewards_train/rejected': '0.011654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06123', 'logps_train/rejected': '-112.4', 'logps_train/chosen': '-104.7', 'loss/train': '0.6773', 'examples_per_second': '31.017', 'grad_norm': '28.875', 'counters/examples': 217536, 'counters/updates': 6798}
train stats after 217568 examples: {'rewards_train/chosen': '0.17319', 'rewards_train/rejected': '0.09749', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075705', 'logps_train/rejected': '-130.83', 'logps_train/chosen': '-111.54', 'loss/train': '0.66944', 'examples_per_second': '32.539', 'grad_norm': '30.375', 'counters/examples': 217568, 'counters/updates': 6799}
skipping logging after 217600 examples to avoid logging too frequently
train stats after 217632 examples: {'rewards_train/chosen': '0.036458', 'rewards_train/rejected': '0.019871', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016587', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-118.18', 'loss/train': '0.7009', 'examples_per_second': '26.224', 'grad_norm': '25.75', 'counters/examples': 217632, 'counters/updates': 6801}
train stats after 217664 examples: {'rewards_train/chosen': '0.15518', 'rewards_train/rejected': '0.042014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11316', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-181.25', 'loss/train': '0.65172', 'examples_per_second': '30.164', 'grad_norm': '32.5', 'counters/examples': 217664, 'counters/updates': 6802}
train stats after 217696 examples: {'rewards_train/chosen': '0.064816', 'rewards_train/rejected': '0.017093', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047722', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-176.36', 'loss/train': '0.67917', 'examples_per_second': '31.479', 'grad_norm': '27.75', 'counters/examples': 217696, 'counters/updates': 6803}
train stats after 217728 examples: {'rewards_train/chosen': '0.18151', 'rewards_train/rejected': '-0.039989', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22149', 'logps_train/rejected': '-102.68', 'logps_train/chosen': '-146.86', 'loss/train': '0.60289', 'examples_per_second': '30.237', 'grad_norm': '21.75', 'counters/examples': 217728, 'counters/updates': 6804}
skipping logging after 217760 examples to avoid logging too frequently
train stats after 217792 examples: {'rewards_train/chosen': '0.13154', 'rewards_train/rejected': '0.10331', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028228', 'logps_train/rejected': '-147.11', 'logps_train/chosen': '-157', 'loss/train': '0.69161', 'examples_per_second': '32.494', 'grad_norm': '30.375', 'counters/examples': 217792, 'counters/updates': 6806}
train stats after 217824 examples: {'rewards_train/chosen': '0.1963', 'rewards_train/rejected': '-0.038197', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2345', 'logps_train/rejected': '-113.12', 'logps_train/chosen': '-139.21', 'loss/train': '0.60993', 'examples_per_second': '30.737', 'grad_norm': '24.375', 'counters/examples': 217824, 'counters/updates': 6807}
train stats after 217856 examples: {'rewards_train/chosen': '0.077581', 'rewards_train/rejected': '0.077149', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00043171', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-162.15', 'loss/train': '0.71051', 'examples_per_second': '30.736', 'grad_norm': '29.75', 'counters/examples': 217856, 'counters/updates': 6808}
train stats after 217888 examples: {'rewards_train/chosen': '0.076086', 'rewards_train/rejected': '0.024385', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051701', 'logps_train/rejected': '-97.157', 'logps_train/chosen': '-127.38', 'loss/train': '0.67727', 'examples_per_second': '31.501', 'grad_norm': '24.75', 'counters/examples': 217888, 'counters/updates': 6809}
skipping logging after 217920 examples to avoid logging too frequently
train stats after 217952 examples: {'rewards_train/chosen': '0.055488', 'rewards_train/rejected': '0.049899', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0055887', 'logps_train/rejected': '-109.65', 'logps_train/chosen': '-124.57', 'loss/train': '0.70093', 'examples_per_second': '32.234', 'grad_norm': '26.25', 'counters/examples': 217952, 'counters/updates': 6811}
train stats after 217984 examples: {'rewards_train/chosen': '0.077673', 'rewards_train/rejected': '0.031115', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046558', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-124.71', 'loss/train': '0.68201', 'examples_per_second': '30.56', 'grad_norm': '27.875', 'counters/examples': 217984, 'counters/updates': 6812}
train stats after 218016 examples: {'rewards_train/chosen': '0.1423', 'rewards_train/rejected': '0.10295', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039351', 'logps_train/rejected': '-125.09', 'logps_train/chosen': '-158.92', 'loss/train': '0.69287', 'examples_per_second': '31.666', 'grad_norm': '27.5', 'counters/examples': 218016, 'counters/updates': 6813}
train stats after 218048 examples: {'rewards_train/chosen': '0.23265', 'rewards_train/rejected': '-0.0010292', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.23368', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-140.23', 'loss/train': '0.59757', 'examples_per_second': '31.31', 'grad_norm': '25.625', 'counters/examples': 218048, 'counters/updates': 6814}
train stats after 218080 examples: {'rewards_train/chosen': '0.20566', 'rewards_train/rejected': '0.092168', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11349', 'logps_train/rejected': '-158.04', 'logps_train/chosen': '-208.66', 'loss/train': '0.66377', 'examples_per_second': '31.552', 'grad_norm': '32.5', 'counters/examples': 218080, 'counters/updates': 6815}
train stats after 218112 examples: {'rewards_train/chosen': '0.21128', 'rewards_train/rejected': '0.062795', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14849', 'logps_train/rejected': '-109.11', 'logps_train/chosen': '-126.45', 'loss/train': '0.64064', 'examples_per_second': '31.979', 'grad_norm': '24.125', 'counters/examples': 218112, 'counters/updates': 6816}
train stats after 218144 examples: {'rewards_train/chosen': '0.2162', 'rewards_train/rejected': '0.079493', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13671', 'logps_train/rejected': '-126.64', 'logps_train/chosen': '-127.68', 'loss/train': '0.63664', 'examples_per_second': '31.136', 'grad_norm': '24.625', 'counters/examples': 218144, 'counters/updates': 6817}
train stats after 218176 examples: {'rewards_train/chosen': '0.037528', 'rewards_train/rejected': '0.041712', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0041838', 'logps_train/rejected': '-142.5', 'logps_train/chosen': '-128.17', 'loss/train': '0.70406', 'examples_per_second': '30.12', 'grad_norm': '37.25', 'counters/examples': 218176, 'counters/updates': 6818}
train stats after 218208 examples: {'rewards_train/chosen': '0.16522', 'rewards_train/rejected': '-0.022629', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18785', 'logps_train/rejected': '-144.43', 'logps_train/chosen': '-156.56', 'loss/train': '0.61857', 'examples_per_second': '32.23', 'grad_norm': '25.125', 'counters/examples': 218208, 'counters/updates': 6819}
train stats after 218240 examples: {'rewards_train/chosen': '0.14849', 'rewards_train/rejected': '0.029126', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11936', 'logps_train/rejected': '-154.68', 'logps_train/chosen': '-143.19', 'loss/train': '0.65481', 'examples_per_second': '32.385', 'grad_norm': '31.875', 'counters/examples': 218240, 'counters/updates': 6820}
train stats after 218272 examples: {'rewards_train/chosen': '0.077072', 'rewards_train/rejected': '-0.065794', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14287', 'logps_train/rejected': '-94.484', 'logps_train/chosen': '-106.82', 'loss/train': '0.6383', 'examples_per_second': '31.738', 'grad_norm': '24', 'counters/examples': 218272, 'counters/updates': 6821}
train stats after 218304 examples: {'rewards_train/chosen': '0.14671', 'rewards_train/rejected': '0.042948', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10377', 'logps_train/rejected': '-93.337', 'logps_train/chosen': '-144.95', 'loss/train': '0.65249', 'examples_per_second': '31.452', 'grad_norm': '23.125', 'counters/examples': 218304, 'counters/updates': 6822}
train stats after 218336 examples: {'rewards_train/chosen': '0.20885', 'rewards_train/rejected': '0.0071104', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20174', 'logps_train/rejected': '-111.14', 'logps_train/chosen': '-131.78', 'loss/train': '0.6148', 'examples_per_second': '30.988', 'grad_norm': '23.125', 'counters/examples': 218336, 'counters/updates': 6823}
train stats after 218368 examples: {'rewards_train/chosen': '0.10719', 'rewards_train/rejected': '-0.041193', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14838', 'logps_train/rejected': '-112.83', 'logps_train/chosen': '-116.03', 'loss/train': '0.63198', 'examples_per_second': '31.66', 'grad_norm': '23.5', 'counters/examples': 218368, 'counters/updates': 6824}
train stats after 218400 examples: {'rewards_train/chosen': '0.035957', 'rewards_train/rejected': '0.05631', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020352', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-143.37', 'loss/train': '0.71788', 'examples_per_second': '31.691', 'grad_norm': '29', 'counters/examples': 218400, 'counters/updates': 6825}
skipping logging after 218432 examples to avoid logging too frequently
train stats after 218464 examples: {'rewards_train/chosen': '0.16843', 'rewards_train/rejected': '0.024649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14378', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-147.32', 'loss/train': '0.63761', 'examples_per_second': '31.413', 'grad_norm': '26.375', 'counters/examples': 218464, 'counters/updates': 6827}
train stats after 218496 examples: {'rewards_train/chosen': '0.098853', 'rewards_train/rejected': '-0.017589', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11644', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-153.29', 'loss/train': '0.6543', 'examples_per_second': '31.238', 'grad_norm': '28.625', 'counters/examples': 218496, 'counters/updates': 6828}
train stats after 218528 examples: {'rewards_train/chosen': '0.074928', 'rewards_train/rejected': '-0.050478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12541', 'logps_train/rejected': '-92.566', 'logps_train/chosen': '-154.2', 'loss/train': '0.64583', 'examples_per_second': '32.046', 'grad_norm': '25', 'counters/examples': 218528, 'counters/updates': 6829}
skipping logging after 218560 examples to avoid logging too frequently
train stats after 218592 examples: {'rewards_train/chosen': '0.13944', 'rewards_train/rejected': '0.0045761', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13486', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-138.75', 'loss/train': '0.63872', 'examples_per_second': '30.279', 'grad_norm': '26.5', 'counters/examples': 218592, 'counters/updates': 6831}
skipping logging after 218624 examples to avoid logging too frequently
train stats after 218656 examples: {'rewards_train/chosen': '0.20369', 'rewards_train/rejected': '-0.034406', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.2381', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-147.37', 'loss/train': '0.59358', 'examples_per_second': '34.131', 'grad_norm': '23.625', 'counters/examples': 218656, 'counters/updates': 6833}
train stats after 218688 examples: {'rewards_train/chosen': '0.16762', 'rewards_train/rejected': '0.017795', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14983', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-162.66', 'loss/train': '0.63606', 'examples_per_second': '31.867', 'grad_norm': '25.625', 'counters/examples': 218688, 'counters/updates': 6834}
train stats after 218720 examples: {'rewards_train/chosen': '0.16348', 'rewards_train/rejected': '0.036727', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12675', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-159.24', 'loss/train': '0.64962', 'examples_per_second': '31.668', 'grad_norm': '26.25', 'counters/examples': 218720, 'counters/updates': 6835}
train stats after 218752 examples: {'rewards_train/chosen': '0.116', 'rewards_train/rejected': '0.048372', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067625', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-154.83', 'loss/train': '0.66883', 'examples_per_second': '23.689', 'grad_norm': '28.5', 'counters/examples': 218752, 'counters/updates': 6836}
train stats after 218784 examples: {'rewards_train/chosen': '0.093492', 'rewards_train/rejected': '0.0081732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085319', 'logps_train/rejected': '-102.22', 'logps_train/chosen': '-121.39', 'loss/train': '0.66577', 'examples_per_second': '32.174', 'grad_norm': '23.25', 'counters/examples': 218784, 'counters/updates': 6837}
skipping logging after 218816 examples to avoid logging too frequently
train stats after 218848 examples: {'rewards_train/chosen': '0.13045', 'rewards_train/rejected': '-0.041448', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1719', 'logps_train/rejected': '-103.9', 'logps_train/chosen': '-137.14', 'loss/train': '0.62752', 'examples_per_second': '24.331', 'grad_norm': '23.25', 'counters/examples': 218848, 'counters/updates': 6839}
skipping logging after 218880 examples to avoid logging too frequently
train stats after 218912 examples: {'rewards_train/chosen': '0.053079', 'rewards_train/rejected': '0.016382', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.036698', 'logps_train/rejected': '-91.121', 'logps_train/chosen': '-117.05', 'loss/train': '0.68362', 'examples_per_second': '33.028', 'grad_norm': '22.875', 'counters/examples': 218912, 'counters/updates': 6841}
train stats after 218944 examples: {'rewards_train/chosen': '0.18339', 'rewards_train/rejected': '0.035743', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14765', 'logps_train/rejected': '-114', 'logps_train/chosen': '-141.71', 'loss/train': '0.64117', 'examples_per_second': '31.611', 'grad_norm': '26.25', 'counters/examples': 218944, 'counters/updates': 6842}
train stats after 218976 examples: {'rewards_train/chosen': '0.13443', 'rewards_train/rejected': '0.10172', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032706', 'logps_train/rejected': '-126.6', 'logps_train/chosen': '-111.67', 'loss/train': '0.69051', 'examples_per_second': '30.044', 'grad_norm': '31.375', 'counters/examples': 218976, 'counters/updates': 6843}
train stats after 219008 examples: {'rewards_train/chosen': '0.13492', 'rewards_train/rejected': '0.045855', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089064', 'logps_train/rejected': '-152.9', 'logps_train/chosen': '-152.78', 'loss/train': '0.66354', 'examples_per_second': '31.443', 'grad_norm': '27.375', 'counters/examples': 219008, 'counters/updates': 6844}
train stats after 219040 examples: {'rewards_train/chosen': '0.19859', 'rewards_train/rejected': '0.010945', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18764', 'logps_train/rejected': '-147.08', 'logps_train/chosen': '-181.55', 'loss/train': '0.61914', 'examples_per_second': '33.146', 'grad_norm': '28', 'counters/examples': 219040, 'counters/updates': 6845}
train stats after 219072 examples: {'rewards_train/chosen': '0.18726', 'rewards_train/rejected': '0.017565', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16969', 'logps_train/rejected': '-103.25', 'logps_train/chosen': '-139.09', 'loss/train': '0.63051', 'examples_per_second': '31.643', 'grad_norm': '23.25', 'counters/examples': 219072, 'counters/updates': 6846}
skipping logging after 219104 examples to avoid logging too frequently
train stats after 219136 examples: {'rewards_train/chosen': '0.10024', 'rewards_train/rejected': '0.04441', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055827', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-106.61', 'loss/train': '0.67329', 'examples_per_second': '31.51', 'grad_norm': '25.25', 'counters/examples': 219136, 'counters/updates': 6848}
skipping logging after 219168 examples to avoid logging too frequently
train stats after 219200 examples: {'rewards_train/chosen': '0.22124', 'rewards_train/rejected': '0.079979', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14126', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-157.85', 'loss/train': '0.63547', 'examples_per_second': '31.01', 'grad_norm': '27.375', 'counters/examples': 219200, 'counters/updates': 6850}
train stats after 219232 examples: {'rewards_train/chosen': '0.052866', 'rewards_train/rejected': '0.0024141', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050451', 'logps_train/rejected': '-114.29', 'logps_train/chosen': '-107.8', 'loss/train': '0.67715', 'examples_per_second': '30.544', 'grad_norm': '27.25', 'counters/examples': 219232, 'counters/updates': 6851}
skipping logging after 219264 examples to avoid logging too frequently
train stats after 219296 examples: {'rewards_train/chosen': '0.19116', 'rewards_train/rejected': '-0.092308', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.28346', 'logps_train/rejected': '-120.55', 'logps_train/chosen': '-134.91', 'loss/train': '0.58232', 'examples_per_second': '32.083', 'grad_norm': '23.75', 'counters/examples': 219296, 'counters/updates': 6853}
train stats after 219328 examples: {'rewards_train/chosen': '0.044581', 'rewards_train/rejected': '0.012953', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031628', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-147.49', 'loss/train': '0.6923', 'examples_per_second': '31.682', 'grad_norm': '27', 'counters/examples': 219328, 'counters/updates': 6854}
train stats after 219360 examples: {'rewards_train/chosen': '0.20871', 'rewards_train/rejected': '-0.039671', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24838', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-123.59', 'loss/train': '0.59275', 'examples_per_second': '32.334', 'grad_norm': '22.625', 'counters/examples': 219360, 'counters/updates': 6855}
skipping logging after 219392 examples to avoid logging too frequently
train stats after 219424 examples: {'rewards_train/chosen': '0.095086', 'rewards_train/rejected': '-0.015608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11069', 'logps_train/rejected': '-101.92', 'logps_train/chosen': '-157.11', 'loss/train': '0.66443', 'examples_per_second': '32.524', 'grad_norm': '26.875', 'counters/examples': 219424, 'counters/updates': 6857}
train stats after 219456 examples: {'rewards_train/chosen': '0.13122', 'rewards_train/rejected': '0.010573', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12064', 'logps_train/rejected': '-109.39', 'logps_train/chosen': '-149.45', 'loss/train': '0.64469', 'examples_per_second': '30.85', 'grad_norm': '22.625', 'counters/examples': 219456, 'counters/updates': 6858}
train stats after 219488 examples: {'rewards_train/chosen': '0.11284', 'rewards_train/rejected': '0.10689', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0059482', 'logps_train/rejected': '-149.82', 'logps_train/chosen': '-158.37', 'loss/train': '0.7006', 'examples_per_second': '31.631', 'grad_norm': '27.375', 'counters/examples': 219488, 'counters/updates': 6859}
skipping logging after 219520 examples to avoid logging too frequently
train stats after 219552 examples: {'rewards_train/chosen': '0.12672', 'rewards_train/rejected': '-0.0025431', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12926', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-132.97', 'loss/train': '0.63947', 'examples_per_second': '32.359', 'grad_norm': '25.5', 'counters/examples': 219552, 'counters/updates': 6861}
train stats after 219584 examples: {'rewards_train/chosen': '0.22749', 'rewards_train/rejected': '-0.017089', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24458', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-165.19', 'loss/train': '0.60192', 'examples_per_second': '30.489', 'grad_norm': '24.625', 'counters/examples': 219584, 'counters/updates': 6862}
train stats after 219616 examples: {'rewards_train/chosen': '0.066068', 'rewards_train/rejected': '0.023448', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04262', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-163.59', 'loss/train': '0.68969', 'examples_per_second': '30.876', 'grad_norm': '30', 'counters/examples': 219616, 'counters/updates': 6863}
train stats after 219648 examples: {'rewards_train/chosen': '0.071', 'rewards_train/rejected': '0.015956', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055044', 'logps_train/rejected': '-104.33', 'logps_train/chosen': '-132.26', 'loss/train': '0.68328', 'examples_per_second': '32.075', 'grad_norm': '25.75', 'counters/examples': 219648, 'counters/updates': 6864}
train stats after 219680 examples: {'rewards_train/chosen': '0.086568', 'rewards_train/rejected': '-0.0154', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10197', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-145.33', 'loss/train': '0.6525', 'examples_per_second': '31.651', 'grad_norm': '28.25', 'counters/examples': 219680, 'counters/updates': 6865}
train stats after 219712 examples: {'rewards_train/chosen': '0.15434', 'rewards_train/rejected': '0.057149', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097193', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-160.64', 'loss/train': '0.65354', 'examples_per_second': '30.856', 'grad_norm': '27.25', 'counters/examples': 219712, 'counters/updates': 6866}
train stats after 219744 examples: {'rewards_train/chosen': '0.13558', 'rewards_train/rejected': '0.039024', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096553', 'logps_train/rejected': '-102.75', 'logps_train/chosen': '-131.97', 'loss/train': '0.66188', 'examples_per_second': '31.348', 'grad_norm': '22.625', 'counters/examples': 219744, 'counters/updates': 6867}
train stats after 219776 examples: {'rewards_train/chosen': '0.084126', 'rewards_train/rejected': '-0.023768', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10789', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-143.72', 'loss/train': '0.65302', 'examples_per_second': '31.826', 'grad_norm': '28.5', 'counters/examples': 219776, 'counters/updates': 6868}
train stats after 219808 examples: {'rewards_train/chosen': '0.079605', 'rewards_train/rejected': '0.062502', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017103', 'logps_train/rejected': '-106.43', 'logps_train/chosen': '-144.57', 'loss/train': '0.69801', 'examples_per_second': '31.991', 'grad_norm': '24.375', 'counters/examples': 219808, 'counters/updates': 6869}
train stats after 219840 examples: {'rewards_train/chosen': '0.16958', 'rewards_train/rejected': '0.13111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038471', 'logps_train/rejected': '-111.56', 'logps_train/chosen': '-119.19', 'loss/train': '0.68609', 'examples_per_second': '31.668', 'grad_norm': '23.5', 'counters/examples': 219840, 'counters/updates': 6870}
train stats after 219872 examples: {'rewards_train/chosen': '0.1373', 'rewards_train/rejected': '0.022901', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1144', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-149.04', 'loss/train': '0.649', 'examples_per_second': '31.295', 'grad_norm': '24.875', 'counters/examples': 219872, 'counters/updates': 6871}
skipping logging after 219904 examples to avoid logging too frequently
train stats after 219936 examples: {'rewards_train/chosen': '0.075936', 'rewards_train/rejected': '0.0084694', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067467', 'logps_train/rejected': '-154.64', 'logps_train/chosen': '-154.94', 'loss/train': '0.67375', 'examples_per_second': '30.256', 'grad_norm': '27.5', 'counters/examples': 219936, 'counters/updates': 6873}
skipping logging after 219968 examples to avoid logging too frequently
train stats after 220000 examples: {'rewards_train/chosen': '0.14905', 'rewards_train/rejected': '-0.03039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17944', 'logps_train/rejected': '-146.57', 'logps_train/chosen': '-164.44', 'loss/train': '0.61986', 'examples_per_second': '30.389', 'grad_norm': '73.5', 'counters/examples': 220000, 'counters/updates': 6875}
train stats after 220032 examples: {'rewards_train/chosen': '0.054401', 'rewards_train/rejected': '0.035997', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018404', 'logps_train/rejected': '-139.68', 'logps_train/chosen': '-147.21', 'loss/train': '0.69855', 'examples_per_second': '32.739', 'grad_norm': '27.5', 'counters/examples': 220032, 'counters/updates': 6876}
train stats after 220064 examples: {'rewards_train/chosen': '0.10119', 'rewards_train/rejected': '-0.032423', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13362', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-93.795', 'loss/train': '0.64207', 'examples_per_second': '31.644', 'grad_norm': '22.625', 'counters/examples': 220064, 'counters/updates': 6877}
train stats after 220096 examples: {'rewards_train/chosen': '0.053125', 'rewards_train/rejected': '-0.02252', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075644', 'logps_train/rejected': '-94.71', 'logps_train/chosen': '-105.94', 'loss/train': '0.66695', 'examples_per_second': '31.49', 'grad_norm': '22.25', 'counters/examples': 220096, 'counters/updates': 6878}
train stats after 220128 examples: {'rewards_train/chosen': '0.090538', 'rewards_train/rejected': '0.036129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054409', 'logps_train/rejected': '-137.58', 'logps_train/chosen': '-134.73', 'loss/train': '0.68277', 'examples_per_second': '31.622', 'grad_norm': '26.625', 'counters/examples': 220128, 'counters/updates': 6879}
train stats after 220160 examples: {'rewards_train/chosen': '0.13404', 'rewards_train/rejected': '0.060885', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.07316', 'logps_train/rejected': '-110.04', 'logps_train/chosen': '-129.82', 'loss/train': '0.66323', 'examples_per_second': '31.012', 'grad_norm': '23.75', 'counters/examples': 220160, 'counters/updates': 6880}
skipping logging after 220192 examples to avoid logging too frequently
train stats after 220224 examples: {'rewards_train/chosen': '0.12867', 'rewards_train/rejected': '0.043612', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085062', 'logps_train/rejected': '-131.6', 'logps_train/chosen': '-132.13', 'loss/train': '0.66356', 'examples_per_second': '33.862', 'grad_norm': '27.5', 'counters/examples': 220224, 'counters/updates': 6882}
skipping logging after 220256 examples to avoid logging too frequently
train stats after 220288 examples: {'rewards_train/chosen': '0.065832', 'rewards_train/rejected': '-0.0043318', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070164', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-141.8', 'loss/train': '0.67444', 'examples_per_second': '31.075', 'grad_norm': '27', 'counters/examples': 220288, 'counters/updates': 6884}
skipping logging after 220320 examples to avoid logging too frequently
train stats after 220352 examples: {'rewards_train/chosen': '0.03989', 'rewards_train/rejected': '-0.025753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065643', 'logps_train/rejected': '-122.95', 'logps_train/chosen': '-129.6', 'loss/train': '0.67578', 'examples_per_second': '31.686', 'grad_norm': '25.5', 'counters/examples': 220352, 'counters/updates': 6886}
skipping logging after 220384 examples to avoid logging too frequently
train stats after 220416 examples: {'rewards_train/chosen': '0.15951', 'rewards_train/rejected': '0.014254', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14525', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-150.79', 'loss/train': '0.63148', 'examples_per_second': '30.114', 'grad_norm': '24.25', 'counters/examples': 220416, 'counters/updates': 6888}
train stats after 220448 examples: {'rewards_train/chosen': '0.15174', 'rewards_train/rejected': '0.074896', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.076846', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-133.83', 'loss/train': '0.67991', 'examples_per_second': '31.596', 'grad_norm': '25.875', 'counters/examples': 220448, 'counters/updates': 6889}
train stats after 220480 examples: {'rewards_train/chosen': '0.13483', 'rewards_train/rejected': '0.00019919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13463', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-152', 'loss/train': '0.64029', 'examples_per_second': '31.666', 'grad_norm': '24.375', 'counters/examples': 220480, 'counters/updates': 6890}
train stats after 220512 examples: {'rewards_train/chosen': '0.13579', 'rewards_train/rejected': '0.066626', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069163', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-103.41', 'loss/train': '0.67569', 'examples_per_second': '30.749', 'grad_norm': '26', 'counters/examples': 220512, 'counters/updates': 6891}
train stats after 220544 examples: {'rewards_train/chosen': '0.11057', 'rewards_train/rejected': '0.07501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035562', 'logps_train/rejected': '-138.92', 'logps_train/chosen': '-151.04', 'loss/train': '0.68597', 'examples_per_second': '32.396', 'grad_norm': '28.75', 'counters/examples': 220544, 'counters/updates': 6892}
train stats after 220576 examples: {'rewards_train/chosen': '0.16731', 'rewards_train/rejected': '0.089109', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078199', 'logps_train/rejected': '-161.87', 'logps_train/chosen': '-168.83', 'loss/train': '0.67228', 'examples_per_second': '30.911', 'grad_norm': '28.875', 'counters/examples': 220576, 'counters/updates': 6893}
train stats after 220608 examples: {'rewards_train/chosen': '0.18023', 'rewards_train/rejected': '-0.036689', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21692', 'logps_train/rejected': '-101.92', 'logps_train/chosen': '-122.64', 'loss/train': '0.60079', 'examples_per_second': '31.512', 'grad_norm': '23.375', 'counters/examples': 220608, 'counters/updates': 6894}
train stats after 220640 examples: {'rewards_train/chosen': '0.045025', 'rewards_train/rejected': '-0.029075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074099', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-129.16', 'loss/train': '0.67433', 'examples_per_second': '32.207', 'grad_norm': '26.75', 'counters/examples': 220640, 'counters/updates': 6895}
train stats after 220672 examples: {'rewards_train/chosen': '0.21157', 'rewards_train/rejected': '0.036954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17461', 'logps_train/rejected': '-187.56', 'logps_train/chosen': '-193.63', 'loss/train': '0.62876', 'examples_per_second': '32.547', 'grad_norm': '30', 'counters/examples': 220672, 'counters/updates': 6896}
train stats after 220704 examples: {'rewards_train/chosen': '0.10588', 'rewards_train/rejected': '-0.11058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21646', 'logps_train/rejected': '-112.12', 'logps_train/chosen': '-132.62', 'loss/train': '0.60795', 'examples_per_second': '30.792', 'grad_norm': '22.25', 'counters/examples': 220704, 'counters/updates': 6897}
train stats after 220736 examples: {'rewards_train/chosen': '0.13818', 'rewards_train/rejected': '0.05882', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07936', 'logps_train/rejected': '-118.44', 'logps_train/chosen': '-152.96', 'loss/train': '0.67387', 'examples_per_second': '30.221', 'grad_norm': '26.75', 'counters/examples': 220736, 'counters/updates': 6898}
skipping logging after 220768 examples to avoid logging too frequently
train stats after 220800 examples: {'rewards_train/chosen': '0.087885', 'rewards_train/rejected': '0.0029636', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084922', 'logps_train/rejected': '-96.123', 'logps_train/chosen': '-148.12', 'loss/train': '0.66202', 'examples_per_second': '34.534', 'grad_norm': '22.875', 'counters/examples': 220800, 'counters/updates': 6900}
train stats after 220832 examples: {'rewards_train/chosen': '0.15737', 'rewards_train/rejected': '0.063448', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093919', 'logps_train/rejected': '-96.815', 'logps_train/chosen': '-129.12', 'loss/train': '0.65855', 'examples_per_second': '32.525', 'grad_norm': '23.5', 'counters/examples': 220832, 'counters/updates': 6901}
train stats after 220864 examples: {'rewards_train/chosen': '0.086555', 'rewards_train/rejected': '-0.018027', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10458', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-124.65', 'loss/train': '0.65384', 'examples_per_second': '31.408', 'grad_norm': '23.125', 'counters/examples': 220864, 'counters/updates': 6902}
skipping logging after 220896 examples to avoid logging too frequently
train stats after 220928 examples: {'rewards_train/chosen': '0.07682', 'rewards_train/rejected': '0.07725', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00043021', 'logps_train/rejected': '-138.73', 'logps_train/chosen': '-95.407', 'loss/train': '0.70131', 'examples_per_second': '31.197', 'grad_norm': '23.625', 'counters/examples': 220928, 'counters/updates': 6904}
train stats after 220960 examples: {'rewards_train/chosen': '0.085502', 'rewards_train/rejected': '0.00028476', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085218', 'logps_train/rejected': '-107.64', 'logps_train/chosen': '-129.86', 'loss/train': '0.67067', 'examples_per_second': '32.943', 'grad_norm': '25', 'counters/examples': 220960, 'counters/updates': 6905}
train stats after 220992 examples: {'rewards_train/chosen': '0.11299', 'rewards_train/rejected': '0.0069237', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10606', 'logps_train/rejected': '-154', 'logps_train/chosen': '-148.1', 'loss/train': '0.65428', 'examples_per_second': '31.641', 'grad_norm': '29.5', 'counters/examples': 220992, 'counters/updates': 6906}
train stats after 221024 examples: {'rewards_train/chosen': '0.12111', 'rewards_train/rejected': '0.065704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05541', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-145.22', 'loss/train': '0.6905', 'examples_per_second': '31.343', 'grad_norm': '29.25', 'counters/examples': 221024, 'counters/updates': 6907}
train stats after 221056 examples: {'rewards_train/chosen': '0.13777', 'rewards_train/rejected': '0.049524', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088243', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-133.01', 'loss/train': '0.67147', 'examples_per_second': '31.526', 'grad_norm': '30.5', 'counters/examples': 221056, 'counters/updates': 6908}
train stats after 221088 examples: {'rewards_train/chosen': '0.14687', 'rewards_train/rejected': '0.049129', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097738', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-139.78', 'loss/train': '0.66306', 'examples_per_second': '32.533', 'grad_norm': '27.75', 'counters/examples': 221088, 'counters/updates': 6909}
train stats after 221120 examples: {'rewards_train/chosen': '0.1024', 'rewards_train/rejected': '-0.028721', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13112', 'logps_train/rejected': '-128.8', 'logps_train/chosen': '-133.91', 'loss/train': '0.63967', 'examples_per_second': '31.492', 'grad_norm': '38.75', 'counters/examples': 221120, 'counters/updates': 6910}
train stats after 221152 examples: {'rewards_train/chosen': '0.12957', 'rewards_train/rejected': '0.035861', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.093714', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-91.983', 'loss/train': '0.65901', 'examples_per_second': '30.776', 'grad_norm': '23.5', 'counters/examples': 221152, 'counters/updates': 6911}
train stats after 221184 examples: {'rewards_train/chosen': '0.058433', 'rewards_train/rejected': '-0.014779', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073212', 'logps_train/rejected': '-107.65', 'logps_train/chosen': '-147.48', 'loss/train': '0.66755', 'examples_per_second': '30.507', 'grad_norm': '27.375', 'counters/examples': 221184, 'counters/updates': 6912}
train stats after 221216 examples: {'rewards_train/chosen': '0.15347', 'rewards_train/rejected': '0.022564', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13091', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-131.88', 'loss/train': '0.6427', 'examples_per_second': '31.927', 'grad_norm': '27.125', 'counters/examples': 221216, 'counters/updates': 6913}
train stats after 221248 examples: {'rewards_train/chosen': '0.085802', 'rewards_train/rejected': '0.0099428', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075859', 'logps_train/rejected': '-115.99', 'logps_train/chosen': '-132.59', 'loss/train': '0.66747', 'examples_per_second': '31.68', 'grad_norm': '26.75', 'counters/examples': 221248, 'counters/updates': 6914}
skipping logging after 221280 examples to avoid logging too frequently
train stats after 221312 examples: {'rewards_train/chosen': '0.12701', 'rewards_train/rejected': '0.037687', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089321', 'logps_train/rejected': '-95.285', 'logps_train/chosen': '-132.48', 'loss/train': '0.66244', 'examples_per_second': '35.377', 'grad_norm': '24.625', 'counters/examples': 221312, 'counters/updates': 6916}
train stats after 221344 examples: {'rewards_train/chosen': '0.10413', 'rewards_train/rejected': '0.011452', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092675', 'logps_train/rejected': '-131.17', 'logps_train/chosen': '-113.29', 'loss/train': '0.66004', 'examples_per_second': '31.461', 'grad_norm': '28.375', 'counters/examples': 221344, 'counters/updates': 6917}
skipping logging after 221376 examples to avoid logging too frequently
train stats after 221408 examples: {'rewards_train/chosen': '0.11541', 'rewards_train/rejected': '-0.0019659', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11738', 'logps_train/rejected': '-95.803', 'logps_train/chosen': '-135.91', 'loss/train': '0.65116', 'examples_per_second': '33.654', 'grad_norm': '23', 'counters/examples': 221408, 'counters/updates': 6919}
skipping logging after 221440 examples to avoid logging too frequently
train stats after 221472 examples: {'rewards_train/chosen': '0.17734', 'rewards_train/rejected': '0.017468', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15987', 'logps_train/rejected': '-122.92', 'logps_train/chosen': '-128.35', 'loss/train': '0.62877', 'examples_per_second': '31.502', 'grad_norm': '23.25', 'counters/examples': 221472, 'counters/updates': 6921}
skipping logging after 221504 examples to avoid logging too frequently
train stats after 221536 examples: {'rewards_train/chosen': '0.18048', 'rewards_train/rejected': '0.14232', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038162', 'logps_train/rejected': '-153.36', 'logps_train/chosen': '-150.39', 'loss/train': '0.68906', 'examples_per_second': '30.245', 'grad_norm': '29.375', 'counters/examples': 221536, 'counters/updates': 6923}
skipping logging after 221568 examples to avoid logging too frequently
train stats after 221600 examples: {'rewards_train/chosen': '0.12968', 'rewards_train/rejected': '0.034587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09509', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-134.58', 'loss/train': '0.65619', 'examples_per_second': '34.827', 'grad_norm': '25.625', 'counters/examples': 221600, 'counters/updates': 6925}
train stats after 221632 examples: {'rewards_train/chosen': '0.095378', 'rewards_train/rejected': '0.029112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066266', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-176.57', 'loss/train': '0.67546', 'examples_per_second': '31.012', 'grad_norm': '28.375', 'counters/examples': 221632, 'counters/updates': 6926}
skipping logging after 221664 examples to avoid logging too frequently
train stats after 221696 examples: {'rewards_train/chosen': '0.14459', 'rewards_train/rejected': '0.10265', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041941', 'logps_train/rejected': '-141.23', 'logps_train/chosen': '-109.61', 'loss/train': '0.69496', 'examples_per_second': '32.202', 'grad_norm': '29.5', 'counters/examples': 221696, 'counters/updates': 6928}
train stats after 221728 examples: {'rewards_train/chosen': '0.17272', 'rewards_train/rejected': '0.081957', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090766', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-139.25', 'loss/train': '0.66295', 'examples_per_second': '33.075', 'grad_norm': '26.25', 'counters/examples': 221728, 'counters/updates': 6929}
train stats after 221760 examples: {'rewards_train/chosen': '0.12322', 'rewards_train/rejected': '0.047241', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075974', 'logps_train/rejected': '-98.044', 'logps_train/chosen': '-118.43', 'loss/train': '0.66895', 'examples_per_second': '32.772', 'grad_norm': '22.875', 'counters/examples': 221760, 'counters/updates': 6930}
train stats after 221792 examples: {'rewards_train/chosen': '0.13492', 'rewards_train/rejected': '-0.061416', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19634', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-129.57', 'loss/train': '0.61895', 'examples_per_second': '30.255', 'grad_norm': '22.875', 'counters/examples': 221792, 'counters/updates': 6931}
train stats after 221824 examples: {'rewards_train/chosen': '0.19779', 'rewards_train/rejected': '0.076578', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12121', 'logps_train/rejected': '-156.39', 'logps_train/chosen': '-152.83', 'loss/train': '0.65957', 'examples_per_second': '31.66', 'grad_norm': '29.875', 'counters/examples': 221824, 'counters/updates': 6932}
skipping logging after 221856 examples to avoid logging too frequently
train stats after 221888 examples: {'rewards_train/chosen': '0.19556', 'rewards_train/rejected': '0.11003', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085539', 'logps_train/rejected': '-121.48', 'logps_train/chosen': '-137.48', 'loss/train': '0.66198', 'examples_per_second': '31.13', 'grad_norm': '25.125', 'counters/examples': 221888, 'counters/updates': 6934}
skipping logging after 221920 examples to avoid logging too frequently
train stats after 221952 examples: {'rewards_train/chosen': '0.13796', 'rewards_train/rejected': '0.17052', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.032561', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-169.34', 'loss/train': '0.72474', 'examples_per_second': '30.649', 'grad_norm': '30.75', 'counters/examples': 221952, 'counters/updates': 6936}
train stats after 221984 examples: {'rewards_train/chosen': '0.098469', 'rewards_train/rejected': '-0.053603', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15207', 'logps_train/rejected': '-95.196', 'logps_train/chosen': '-115.6', 'loss/train': '0.6332', 'examples_per_second': '31.733', 'grad_norm': '22.625', 'counters/examples': 221984, 'counters/updates': 6937}
train stats after 222016 examples: {'rewards_train/chosen': '0.12674', 'rewards_train/rejected': '-0.015658', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1424', 'logps_train/rejected': '-120.85', 'logps_train/chosen': '-119.01', 'loss/train': '0.63628', 'examples_per_second': '30.955', 'grad_norm': '24.75', 'counters/examples': 222016, 'counters/updates': 6938}
train stats after 222048 examples: {'rewards_train/chosen': '0.029751', 'rewards_train/rejected': '-0.021193', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050945', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-115.91', 'loss/train': '0.67629', 'examples_per_second': '32.913', 'grad_norm': '25.375', 'counters/examples': 222048, 'counters/updates': 6939}
train stats after 222080 examples: {'rewards_train/chosen': '0.24629', 'rewards_train/rejected': '0.028941', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21735', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-184.07', 'loss/train': '0.6035', 'examples_per_second': '32.385', 'grad_norm': '27.75', 'counters/examples': 222080, 'counters/updates': 6940}
train stats after 222112 examples: {'rewards_train/chosen': '0.16559', 'rewards_train/rejected': '0.055329', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11026', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-139.3', 'loss/train': '0.65351', 'examples_per_second': '31.638', 'grad_norm': '24.625', 'counters/examples': 222112, 'counters/updates': 6941}
train stats after 222144 examples: {'rewards_train/chosen': '0.087267', 'rewards_train/rejected': '0.015579', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071688', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-189.91', 'loss/train': '0.67355', 'examples_per_second': '31.654', 'grad_norm': '29.375', 'counters/examples': 222144, 'counters/updates': 6942}
skipping logging after 222176 examples to avoid logging too frequently
train stats after 222208 examples: {'rewards_train/chosen': '0.14', 'rewards_train/rejected': '0.0071063', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1329', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-151.49', 'loss/train': '0.64706', 'examples_per_second': '30.664', 'grad_norm': '26.75', 'counters/examples': 222208, 'counters/updates': 6944}
train stats after 222240 examples: {'rewards_train/chosen': '0.15693', 'rewards_train/rejected': '0.048285', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10864', 'logps_train/rejected': '-113.13', 'logps_train/chosen': '-130.16', 'loss/train': '0.64897', 'examples_per_second': '32.306', 'grad_norm': '23.5', 'counters/examples': 222240, 'counters/updates': 6945}
train stats after 222272 examples: {'rewards_train/chosen': '0.11403', 'rewards_train/rejected': '0.00091974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11311', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-126.74', 'loss/train': '0.64782', 'examples_per_second': '31.027', 'grad_norm': '24.125', 'counters/examples': 222272, 'counters/updates': 6946}
train stats after 222304 examples: {'rewards_train/chosen': '0.11926', 'rewards_train/rejected': '-0.039453', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15872', 'logps_train/rejected': '-123.79', 'logps_train/chosen': '-116.9', 'loss/train': '0.62814', 'examples_per_second': '30.17', 'grad_norm': '25.625', 'counters/examples': 222304, 'counters/updates': 6947}
train stats after 222336 examples: {'rewards_train/chosen': '0.036182', 'rewards_train/rejected': '0.050411', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01423', 'logps_train/rejected': '-103.11', 'logps_train/chosen': '-138.46', 'loss/train': '0.70703', 'examples_per_second': '31.34', 'grad_norm': '27.625', 'counters/examples': 222336, 'counters/updates': 6948}
train stats after 222368 examples: {'rewards_train/chosen': '0.19186', 'rewards_train/rejected': '0.095929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095932', 'logps_train/rejected': '-102.44', 'logps_train/chosen': '-118.8', 'loss/train': '0.66279', 'examples_per_second': '31.697', 'grad_norm': '23.375', 'counters/examples': 222368, 'counters/updates': 6949}
train stats after 222400 examples: {'rewards_train/chosen': '0.13439', 'rewards_train/rejected': '-0.032522', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16692', 'logps_train/rejected': '-85.865', 'logps_train/chosen': '-163.04', 'loss/train': '0.62323', 'examples_per_second': '31.666', 'grad_norm': '23.5', 'counters/examples': 222400, 'counters/updates': 6950}
train stats after 222432 examples: {'rewards_train/chosen': '0.13188', 'rewards_train/rejected': '0.06009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071789', 'logps_train/rejected': '-116.06', 'logps_train/chosen': '-119.47', 'loss/train': '0.67335', 'examples_per_second': '30.644', 'grad_norm': '25.125', 'counters/examples': 222432, 'counters/updates': 6951}
train stats after 222464 examples: {'rewards_train/chosen': '0.11502', 'rewards_train/rejected': '0.065367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049655', 'logps_train/rejected': '-142.5', 'logps_train/chosen': '-162.31', 'loss/train': '0.68195', 'examples_per_second': '31.41', 'grad_norm': '30.875', 'counters/examples': 222464, 'counters/updates': 6952}
train stats after 222496 examples: {'rewards_train/chosen': '0.1433', 'rewards_train/rejected': '-0.0081281', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15143', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-126.03', 'loss/train': '0.63656', 'examples_per_second': '31.638', 'grad_norm': '25.125', 'counters/examples': 222496, 'counters/updates': 6953}
train stats after 222528 examples: {'rewards_train/chosen': '0.14866', 'rewards_train/rejected': '-0.010237', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15889', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-165.33', 'loss/train': '0.63501', 'examples_per_second': '32.7', 'grad_norm': '36.5', 'counters/examples': 222528, 'counters/updates': 6954}
train stats after 222560 examples: {'rewards_train/chosen': '0.11626', 'rewards_train/rejected': '0.049964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066299', 'logps_train/rejected': '-105.68', 'logps_train/chosen': '-135.72', 'loss/train': '0.66841', 'examples_per_second': '32.502', 'grad_norm': '24.625', 'counters/examples': 222560, 'counters/updates': 6955}
train stats after 222592 examples: {'rewards_train/chosen': '0.13031', 'rewards_train/rejected': '-0.023059', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15336', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-145.11', 'loss/train': '0.62854', 'examples_per_second': '30.623', 'grad_norm': '21.75', 'counters/examples': 222592, 'counters/updates': 6956}
train stats after 222624 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '0.12053', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026308', 'logps_train/rejected': '-172.82', 'logps_train/chosen': '-152.19', 'loss/train': '0.69609', 'examples_per_second': '31.538', 'grad_norm': '35', 'counters/examples': 222624, 'counters/updates': 6957}
skipping logging after 222656 examples to avoid logging too frequently
train stats after 222688 examples: {'rewards_train/chosen': '0.15924', 'rewards_train/rejected': '-0.016133', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17538', 'logps_train/rejected': '-102.05', 'logps_train/chosen': '-158.91', 'loss/train': '0.62171', 'examples_per_second': '31.045', 'grad_norm': '23.5', 'counters/examples': 222688, 'counters/updates': 6959}
skipping logging after 222720 examples to avoid logging too frequently
train stats after 222752 examples: {'rewards_train/chosen': '0.076798', 'rewards_train/rejected': '-0.036398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1132', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-133.87', 'loss/train': '0.65236', 'examples_per_second': '31.293', 'grad_norm': '23.375', 'counters/examples': 222752, 'counters/updates': 6961}
train stats after 222784 examples: {'rewards_train/chosen': '0.13703', 'rewards_train/rejected': '-0.011142', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14817', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-112.33', 'loss/train': '0.63294', 'examples_per_second': '32.148', 'grad_norm': '25', 'counters/examples': 222784, 'counters/updates': 6962}
train stats after 222816 examples: {'rewards_train/chosen': '0.19324', 'rewards_train/rejected': '0.0070122', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18623', 'logps_train/rejected': '-110.1', 'logps_train/chosen': '-152.35', 'loss/train': '0.61567', 'examples_per_second': '32.081', 'grad_norm': '25.25', 'counters/examples': 222816, 'counters/updates': 6963}
train stats after 222848 examples: {'rewards_train/chosen': '0.050396', 'rewards_train/rejected': '0.098827', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048431', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-143.49', 'loss/train': '0.73191', 'examples_per_second': '31.64', 'grad_norm': '28.5', 'counters/examples': 222848, 'counters/updates': 6964}
train stats after 222880 examples: {'rewards_train/chosen': '0.11089', 'rewards_train/rejected': '0.015789', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095098', 'logps_train/rejected': '-119.66', 'logps_train/chosen': '-188.86', 'loss/train': '0.65617', 'examples_per_second': '32.404', 'grad_norm': '31.125', 'counters/examples': 222880, 'counters/updates': 6965}
train stats after 222912 examples: {'rewards_train/chosen': '0.088899', 'rewards_train/rejected': '0.053022', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035878', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-165.21', 'loss/train': '0.68912', 'examples_per_second': '32.588', 'grad_norm': '28.5', 'counters/examples': 222912, 'counters/updates': 6966}
train stats after 222944 examples: {'rewards_train/chosen': '0.10267', 'rewards_train/rejected': '0.02034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082332', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-146.93', 'loss/train': '0.66542', 'examples_per_second': '31.025', 'grad_norm': '25.5', 'counters/examples': 222944, 'counters/updates': 6967}
train stats after 222976 examples: {'rewards_train/chosen': '0.15462', 'rewards_train/rejected': '0.15312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0014959', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-136.01', 'loss/train': '0.7095', 'examples_per_second': '30.713', 'grad_norm': '27.875', 'counters/examples': 222976, 'counters/updates': 6968}
train stats after 223008 examples: {'rewards_train/chosen': '0.073661', 'rewards_train/rejected': '0.055535', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018126', 'logps_train/rejected': '-153.36', 'logps_train/chosen': '-123.35', 'loss/train': '0.69832', 'examples_per_second': '33.27', 'grad_norm': '31.125', 'counters/examples': 223008, 'counters/updates': 6969}
train stats after 223040 examples: {'rewards_train/chosen': '0.12171', 'rewards_train/rejected': '0.090609', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031104', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-127.38', 'loss/train': '0.6922', 'examples_per_second': '31.564', 'grad_norm': '27', 'counters/examples': 223040, 'counters/updates': 6970}
train stats after 223072 examples: {'rewards_train/chosen': '0.12605', 'rewards_train/rejected': '0.066532', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059518', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-147.97', 'loss/train': '0.6744', 'examples_per_second': '32.713', 'grad_norm': '30', 'counters/examples': 223072, 'counters/updates': 6971}
skipping logging after 223104 examples to avoid logging too frequently
train stats after 223136 examples: {'rewards_train/chosen': '0.12589', 'rewards_train/rejected': '0.026933', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098953', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-140.3', 'loss/train': '0.6622', 'examples_per_second': '31.641', 'grad_norm': '26.875', 'counters/examples': 223136, 'counters/updates': 6973}
train stats after 223168 examples: {'rewards_train/chosen': '0.23517', 'rewards_train/rejected': '0.0044598', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23071', 'logps_train/rejected': '-102.29', 'logps_train/chosen': '-144.75', 'loss/train': '0.60395', 'examples_per_second': '31.535', 'grad_norm': '21.75', 'counters/examples': 223168, 'counters/updates': 6974}
train stats after 223200 examples: {'rewards_train/chosen': '0.15329', 'rewards_train/rejected': '0.067125', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086164', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-138.69', 'loss/train': '0.6656', 'examples_per_second': '22.323', 'grad_norm': '24.125', 'counters/examples': 223200, 'counters/updates': 6975}
train stats after 223232 examples: {'rewards_train/chosen': '0.11179', 'rewards_train/rejected': '-0.049398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16119', 'logps_train/rejected': '-84.307', 'logps_train/chosen': '-118.5', 'loss/train': '0.63278', 'examples_per_second': '31.295', 'grad_norm': '21.5', 'counters/examples': 223232, 'counters/updates': 6976}
train stats after 223264 examples: {'rewards_train/chosen': '0.15886', 'rewards_train/rejected': '0.011923', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14694', 'logps_train/rejected': '-128.9', 'logps_train/chosen': '-115.58', 'loss/train': '0.62845', 'examples_per_second': '32.221', 'grad_norm': '25.75', 'counters/examples': 223264, 'counters/updates': 6977}
skipping logging after 223296 examples to avoid logging too frequently
train stats after 223328 examples: {'rewards_train/chosen': '0.10233', 'rewards_train/rejected': '0.039381', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062953', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-146.86', 'loss/train': '0.67716', 'examples_per_second': '30.313', 'grad_norm': '26.375', 'counters/examples': 223328, 'counters/updates': 6979}
train stats after 223360 examples: {'rewards_train/chosen': '0.14603', 'rewards_train/rejected': '0.01143', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1346', 'logps_train/rejected': '-138.23', 'logps_train/chosen': '-156.23', 'loss/train': '0.64446', 'examples_per_second': '32.572', 'grad_norm': '25.375', 'counters/examples': 223360, 'counters/updates': 6980}
skipping logging after 223392 examples to avoid logging too frequently
train stats after 223424 examples: {'rewards_train/chosen': '0.10193', 'rewards_train/rejected': '-0.020706', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12264', 'logps_train/rejected': '-145.87', 'logps_train/chosen': '-151.66', 'loss/train': '0.65141', 'examples_per_second': '30.133', 'grad_norm': '29.625', 'counters/examples': 223424, 'counters/updates': 6982}
train stats after 223456 examples: {'rewards_train/chosen': '0.19811', 'rewards_train/rejected': '0.12967', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068434', 'logps_train/rejected': '-136.68', 'logps_train/chosen': '-137.81', 'loss/train': '0.68354', 'examples_per_second': '30.428', 'grad_norm': '28.625', 'counters/examples': 223456, 'counters/updates': 6983}
train stats after 223488 examples: {'rewards_train/chosen': '0.13863', 'rewards_train/rejected': '-0.053213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19184', 'logps_train/rejected': '-107.28', 'logps_train/chosen': '-129.93', 'loss/train': '0.62178', 'examples_per_second': '31.314', 'grad_norm': '21.375', 'counters/examples': 223488, 'counters/updates': 6984}
train stats after 223520 examples: {'rewards_train/chosen': '0.22444', 'rewards_train/rejected': '0.032404', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19204', 'logps_train/rejected': '-90.537', 'logps_train/chosen': '-143.9', 'loss/train': '0.61851', 'examples_per_second': '30.302', 'grad_norm': '25.625', 'counters/examples': 223520, 'counters/updates': 6985}
train stats after 223552 examples: {'rewards_train/chosen': '0.17903', 'rewards_train/rejected': '-0.036933', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21596', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-147.2', 'loss/train': '0.60998', 'examples_per_second': '30.761', 'grad_norm': '24.75', 'counters/examples': 223552, 'counters/updates': 6986}
train stats after 223584 examples: {'rewards_train/chosen': '0.081392', 'rewards_train/rejected': '0.055502', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02589', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-185.12', 'loss/train': '0.68873', 'examples_per_second': '31.602', 'grad_norm': '31.875', 'counters/examples': 223584, 'counters/updates': 6987}
train stats after 223616 examples: {'rewards_train/chosen': '0.25743', 'rewards_train/rejected': '0.14469', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11274', 'logps_train/rejected': '-145.09', 'logps_train/chosen': '-170.19', 'loss/train': '0.65262', 'examples_per_second': '31.754', 'grad_norm': '25.625', 'counters/examples': 223616, 'counters/updates': 6988}
skipping logging after 223648 examples to avoid logging too frequently
train stats after 223680 examples: {'rewards_train/chosen': '0.22728', 'rewards_train/rejected': '-0.059914', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28719', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-154.86', 'loss/train': '0.57597', 'examples_per_second': '32.834', 'grad_norm': '23', 'counters/examples': 223680, 'counters/updates': 6990}
train stats after 223712 examples: {'rewards_train/chosen': '0.11288', 'rewards_train/rejected': '0.037621', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075255', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-136.7', 'loss/train': '0.66771', 'examples_per_second': '32.01', 'grad_norm': '26.125', 'counters/examples': 223712, 'counters/updates': 6991}
skipping logging after 223744 examples to avoid logging too frequently
train stats after 223776 examples: {'rewards_train/chosen': '0.13623', 'rewards_train/rejected': '0.014683', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12155', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-146.7', 'loss/train': '0.64292', 'examples_per_second': '30.798', 'grad_norm': '27.25', 'counters/examples': 223776, 'counters/updates': 6993}
skipping logging after 223808 examples to avoid logging too frequently
train stats after 223840 examples: {'rewards_train/chosen': '0.18079', 'rewards_train/rejected': '0.024739', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15605', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-134.51', 'loss/train': '0.63113', 'examples_per_second': '30.865', 'grad_norm': '22', 'counters/examples': 223840, 'counters/updates': 6995}
skipping logging after 223872 examples to avoid logging too frequently
train stats after 223904 examples: {'rewards_train/chosen': '0.091372', 'rewards_train/rejected': '0.068717', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022655', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-118.82', 'loss/train': '0.6892', 'examples_per_second': '34.104', 'grad_norm': '30', 'counters/examples': 223904, 'counters/updates': 6997}
train stats after 223936 examples: {'rewards_train/chosen': '0.13252', 'rewards_train/rejected': '0.014173', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11835', 'logps_train/rejected': '-120.82', 'logps_train/chosen': '-125.84', 'loss/train': '0.64653', 'examples_per_second': '32.408', 'grad_norm': '50.5', 'counters/examples': 223936, 'counters/updates': 6998}
train stats after 223968 examples: {'rewards_train/chosen': '0.061993', 'rewards_train/rejected': '0.092995', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.031002', 'logps_train/rejected': '-144.18', 'logps_train/chosen': '-165.72', 'loss/train': '0.72801', 'examples_per_second': '31.65', 'grad_norm': '30.5', 'counters/examples': 223968, 'counters/updates': 6999}
train stats after 224000 examples: {'rewards_train/chosen': '0.14718', 'rewards_train/rejected': '-0.079971', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22716', 'logps_train/rejected': '-89.908', 'logps_train/chosen': '-132.79', 'loss/train': '0.59983', 'examples_per_second': '31.672', 'grad_norm': '26.875', 'counters/examples': 224000, 'counters/updates': 7000}
skipping logging after 224032 examples to avoid logging too frequently
train stats after 224064 examples: {'rewards_train/chosen': '0.14175', 'rewards_train/rejected': '-0.071016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21276', 'logps_train/rejected': '-130.09', 'logps_train/chosen': '-179.82', 'loss/train': '0.61055', 'examples_per_second': '30.474', 'grad_norm': '31.375', 'counters/examples': 224064, 'counters/updates': 7002}
train stats after 224096 examples: {'rewards_train/chosen': '0.055803', 'rewards_train/rejected': '-0.070171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12597', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-145.23', 'loss/train': '0.64425', 'examples_per_second': '32.257', 'grad_norm': '26.625', 'counters/examples': 224096, 'counters/updates': 7003}
train stats after 224128 examples: {'rewards_train/chosen': '0.15494', 'rewards_train/rejected': '0.047278', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10766', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-145.34', 'loss/train': '0.65184', 'examples_per_second': '30.59', 'grad_norm': '26.25', 'counters/examples': 224128, 'counters/updates': 7004}
train stats after 224160 examples: {'rewards_train/chosen': '0.13473', 'rewards_train/rejected': '0.0026487', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13209', 'logps_train/rejected': '-105.96', 'logps_train/chosen': '-142.84', 'loss/train': '0.6438', 'examples_per_second': '32.001', 'grad_norm': '23.25', 'counters/examples': 224160, 'counters/updates': 7005}
train stats after 224192 examples: {'rewards_train/chosen': '0.13426', 'rewards_train/rejected': '0.045896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088365', 'logps_train/rejected': '-119.98', 'logps_train/chosen': '-127.17', 'loss/train': '0.66049', 'examples_per_second': '32.748', 'grad_norm': '24.375', 'counters/examples': 224192, 'counters/updates': 7006}
train stats after 224224 examples: {'rewards_train/chosen': '0.14467', 'rewards_train/rejected': '0.013594', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13107', 'logps_train/rejected': '-111.98', 'logps_train/chosen': '-143.52', 'loss/train': '0.64235', 'examples_per_second': '24.39', 'grad_norm': '24.375', 'counters/examples': 224224, 'counters/updates': 7007}
train stats after 224256 examples: {'rewards_train/chosen': '0.17564', 'rewards_train/rejected': '0.093411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.082232', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-152.41', 'loss/train': '0.66912', 'examples_per_second': '31.96', 'grad_norm': '32.25', 'counters/examples': 224256, 'counters/updates': 7008}
train stats after 224288 examples: {'rewards_train/chosen': '0.16515', 'rewards_train/rejected': '0.035326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12982', 'logps_train/rejected': '-139.99', 'logps_train/chosen': '-146.23', 'loss/train': '0.64841', 'examples_per_second': '31.523', 'grad_norm': '25.875', 'counters/examples': 224288, 'counters/updates': 7009}
train stats after 224320 examples: {'rewards_train/chosen': '0.090182', 'rewards_train/rejected': '0.034545', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055637', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-135.87', 'loss/train': '0.67808', 'examples_per_second': '24.245', 'grad_norm': '33.25', 'counters/examples': 224320, 'counters/updates': 7010}
train stats after 224352 examples: {'rewards_train/chosen': '0.13384', 'rewards_train/rejected': '0.046134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087708', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-143.45', 'loss/train': '0.6664', 'examples_per_second': '31.664', 'grad_norm': '27.75', 'counters/examples': 224352, 'counters/updates': 7011}
train stats after 224384 examples: {'rewards_train/chosen': '0.074598', 'rewards_train/rejected': '-0.0095405', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084139', 'logps_train/rejected': '-95.786', 'logps_train/chosen': '-124.72', 'loss/train': '0.65856', 'examples_per_second': '31.982', 'grad_norm': '23.125', 'counters/examples': 224384, 'counters/updates': 7012}
train stats after 224416 examples: {'rewards_train/chosen': '0.19807', 'rewards_train/rejected': '0.058138', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13994', 'logps_train/rejected': '-144.14', 'logps_train/chosen': '-135.11', 'loss/train': '0.63981', 'examples_per_second': '30.861', 'grad_norm': '26.375', 'counters/examples': 224416, 'counters/updates': 7013}
train stats after 224448 examples: {'rewards_train/chosen': '0.19998', 'rewards_train/rejected': '0.031379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1686', 'logps_train/rejected': '-131.14', 'logps_train/chosen': '-134.6', 'loss/train': '0.62644', 'examples_per_second': '32.36', 'grad_norm': '26.125', 'counters/examples': 224448, 'counters/updates': 7014}
train stats after 224480 examples: {'rewards_train/chosen': '0.096593', 'rewards_train/rejected': '0.076392', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020201', 'logps_train/rejected': '-145.58', 'logps_train/chosen': '-141.54', 'loss/train': '0.69554', 'examples_per_second': '32.482', 'grad_norm': '31.5', 'counters/examples': 224480, 'counters/updates': 7015}
train stats after 224512 examples: {'rewards_train/chosen': '0.092618', 'rewards_train/rejected': '0.017007', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075612', 'logps_train/rejected': '-150.39', 'logps_train/chosen': '-136.9', 'loss/train': '0.66747', 'examples_per_second': '31.358', 'grad_norm': '27.125', 'counters/examples': 224512, 'counters/updates': 7016}
train stats after 224544 examples: {'rewards_train/chosen': '0.16205', 'rewards_train/rejected': '-0.030175', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19222', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-150.78', 'loss/train': '0.62328', 'examples_per_second': '30.961', 'grad_norm': '25.25', 'counters/examples': 224544, 'counters/updates': 7017}
train stats after 224576 examples: {'rewards_train/chosen': '0.10491', 'rewards_train/rejected': '-0.020145', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12506', 'logps_train/rejected': '-96.236', 'logps_train/chosen': '-177.94', 'loss/train': '0.64274', 'examples_per_second': '31.658', 'grad_norm': '26.875', 'counters/examples': 224576, 'counters/updates': 7018}
skipping logging after 224608 examples to avoid logging too frequently
train stats after 224640 examples: {'rewards_train/chosen': '0.087459', 'rewards_train/rejected': '-0.049947', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13741', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-154.84', 'loss/train': '0.63784', 'examples_per_second': '31.551', 'grad_norm': '26.25', 'counters/examples': 224640, 'counters/updates': 7020}
skipping logging after 224672 examples to avoid logging too frequently
train stats after 224704 examples: {'rewards_train/chosen': '0.071299', 'rewards_train/rejected': '0.038575', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032724', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-141.65', 'loss/train': '0.69257', 'examples_per_second': '31.674', 'grad_norm': '27.25', 'counters/examples': 224704, 'counters/updates': 7022}
train stats after 224736 examples: {'rewards_train/chosen': '0.15159', 'rewards_train/rejected': '0.099693', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051897', 'logps_train/rejected': '-154.62', 'logps_train/chosen': '-138.69', 'loss/train': '0.68244', 'examples_per_second': '31.522', 'grad_norm': '29.625', 'counters/examples': 224736, 'counters/updates': 7023}
skipping logging after 224768 examples to avoid logging too frequently
train stats after 224800 examples: {'rewards_train/chosen': '0.089405', 'rewards_train/rejected': '0.029196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06021', 'logps_train/rejected': '-96.327', 'logps_train/chosen': '-102.07', 'loss/train': '0.67765', 'examples_per_second': '32.704', 'grad_norm': '22.875', 'counters/examples': 224800, 'counters/updates': 7025}
train stats after 224832 examples: {'rewards_train/chosen': '0.16181', 'rewards_train/rejected': '0.021263', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14054', 'logps_train/rejected': '-132.64', 'logps_train/chosen': '-150.4', 'loss/train': '0.63778', 'examples_per_second': '31.12', 'grad_norm': '35.5', 'counters/examples': 224832, 'counters/updates': 7026}
skipping logging after 224864 examples to avoid logging too frequently
train stats after 224896 examples: {'rewards_train/chosen': '0.11739', 'rewards_train/rejected': '0.089056', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028335', 'logps_train/rejected': '-113.09', 'logps_train/chosen': '-123.56', 'loss/train': '0.69377', 'examples_per_second': '36.716', 'grad_norm': '24.25', 'counters/examples': 224896, 'counters/updates': 7028}
train stats after 224928 examples: {'rewards_train/chosen': '0.17757', 'rewards_train/rejected': '0.040448', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13712', 'logps_train/rejected': '-97.652', 'logps_train/chosen': '-114.81', 'loss/train': '0.64732', 'examples_per_second': '31.638', 'grad_norm': '23.75', 'counters/examples': 224928, 'counters/updates': 7029}
skipping logging after 224960 examples to avoid logging too frequently
train stats after 224992 examples: {'rewards_train/chosen': '0.06204', 'rewards_train/rejected': '-0.083337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14538', 'logps_train/rejected': '-90.667', 'logps_train/chosen': '-118.97', 'loss/train': '0.63163', 'examples_per_second': '34.211', 'grad_norm': '23.25', 'counters/examples': 224992, 'counters/updates': 7031}
skipping logging after 225024 examples to avoid logging too frequently
train stats after 225056 examples: {'rewards_train/chosen': '0.12336', 'rewards_train/rejected': '-0.015058', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13842', 'logps_train/rejected': '-91.395', 'logps_train/chosen': '-138.49', 'loss/train': '0.641', 'examples_per_second': '32.697', 'grad_norm': '24.75', 'counters/examples': 225056, 'counters/updates': 7033}
train stats after 225088 examples: {'rewards_train/chosen': '0.1657', 'rewards_train/rejected': '-0.067666', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23336', 'logps_train/rejected': '-79.261', 'logps_train/chosen': '-156.21', 'loss/train': '0.59876', 'examples_per_second': '31.307', 'grad_norm': '28.75', 'counters/examples': 225088, 'counters/updates': 7034}
train stats after 225120 examples: {'rewards_train/chosen': '0.15725', 'rewards_train/rejected': '0.081256', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075993', 'logps_train/rejected': '-135.13', 'logps_train/chosen': '-120.44', 'loss/train': '0.66728', 'examples_per_second': '31.088', 'grad_norm': '25.375', 'counters/examples': 225120, 'counters/updates': 7035}
skipping logging after 225152 examples to avoid logging too frequently
train stats after 225184 examples: {'rewards_train/chosen': '0.14996', 'rewards_train/rejected': '-0.013848', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16381', 'logps_train/rejected': '-104.49', 'logps_train/chosen': '-117.87', 'loss/train': '0.62778', 'examples_per_second': '30.334', 'grad_norm': '23.375', 'counters/examples': 225184, 'counters/updates': 7037}
train stats after 225216 examples: {'rewards_train/chosen': '0.16022', 'rewards_train/rejected': '0.0084044', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15181', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-151.05', 'loss/train': '0.63237', 'examples_per_second': '32.766', 'grad_norm': '27.25', 'counters/examples': 225216, 'counters/updates': 7038}
train stats after 225248 examples: {'rewards_train/chosen': '0.094692', 'rewards_train/rejected': '0.075649', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019042', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-143.4', 'loss/train': '0.70317', 'examples_per_second': '31.491', 'grad_norm': '27.125', 'counters/examples': 225248, 'counters/updates': 7039}
train stats after 225280 examples: {'rewards_train/chosen': '0.1451', 'rewards_train/rejected': '0.062059', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083045', 'logps_train/rejected': '-106.85', 'logps_train/chosen': '-116.53', 'loss/train': '0.66202', 'examples_per_second': '31.649', 'grad_norm': '23.25', 'counters/examples': 225280, 'counters/updates': 7040}
skipping logging after 225312 examples to avoid logging too frequently
train stats after 225344 examples: {'rewards_train/chosen': '0.091936', 'rewards_train/rejected': '0.053015', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038921', 'logps_train/rejected': '-100.57', 'logps_train/chosen': '-93.888', 'loss/train': '0.68145', 'examples_per_second': '30.479', 'grad_norm': '23.875', 'counters/examples': 225344, 'counters/updates': 7042}
train stats after 225376 examples: {'rewards_train/chosen': '0.17567', 'rewards_train/rejected': '-0.044469', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22014', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-139.29', 'loss/train': '0.60406', 'examples_per_second': '31.137', 'grad_norm': '25.375', 'counters/examples': 225376, 'counters/updates': 7043}
train stats after 225408 examples: {'rewards_train/chosen': '0.091368', 'rewards_train/rejected': '0.092356', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00098817', 'logps_train/rejected': '-152.38', 'logps_train/chosen': '-144.88', 'loss/train': '0.7069', 'examples_per_second': '30.111', 'grad_norm': '28.75', 'counters/examples': 225408, 'counters/updates': 7044}
train stats after 225440 examples: {'rewards_train/chosen': '0.19228', 'rewards_train/rejected': '-0.022709', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21499', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-149.95', 'loss/train': '0.61399', 'examples_per_second': '30.006', 'grad_norm': '25', 'counters/examples': 225440, 'counters/updates': 7045}
train stats after 225472 examples: {'rewards_train/chosen': '0.097608', 'rewards_train/rejected': '0.039733', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057875', 'logps_train/rejected': '-93.298', 'logps_train/chosen': '-109.4', 'loss/train': '0.6725', 'examples_per_second': '31.849', 'grad_norm': '21.25', 'counters/examples': 225472, 'counters/updates': 7046}
train stats after 225504 examples: {'rewards_train/chosen': '0.14934', 'rewards_train/rejected': '-0.0070634', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15641', 'logps_train/rejected': '-167.15', 'logps_train/chosen': '-165.39', 'loss/train': '0.63277', 'examples_per_second': '31.053', 'grad_norm': '27.125', 'counters/examples': 225504, 'counters/updates': 7047}
skipping logging after 225536 examples to avoid logging too frequently
train stats after 225568 examples: {'rewards_train/chosen': '0.0034998', 'rewards_train/rejected': '-0.0072445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010744', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-114.86', 'loss/train': '0.7055', 'examples_per_second': '30.955', 'grad_norm': '26.875', 'counters/examples': 225568, 'counters/updates': 7049}
train stats after 225600 examples: {'rewards_train/chosen': '0.061854', 'rewards_train/rejected': '0.030103', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031751', 'logps_train/rejected': '-147.77', 'logps_train/chosen': '-145.9', 'loss/train': '0.70532', 'examples_per_second': '31.617', 'grad_norm': '30.875', 'counters/examples': 225600, 'counters/updates': 7050}
train stats after 225632 examples: {'rewards_train/chosen': '0.17204', 'rewards_train/rejected': '-0.0014695', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17351', 'logps_train/rejected': '-102.04', 'logps_train/chosen': '-119.49', 'loss/train': '0.62336', 'examples_per_second': '31.649', 'grad_norm': '22.375', 'counters/examples': 225632, 'counters/updates': 7051}
train stats after 225664 examples: {'rewards_train/chosen': '0.099389', 'rewards_train/rejected': '-0.021031', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12042', 'logps_train/rejected': '-115.26', 'logps_train/chosen': '-128.73', 'loss/train': '0.64876', 'examples_per_second': '31.493', 'grad_norm': '23.75', 'counters/examples': 225664, 'counters/updates': 7052}
train stats after 225696 examples: {'rewards_train/chosen': '0.17273', 'rewards_train/rejected': '0.065811', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10692', 'logps_train/rejected': '-153.34', 'logps_train/chosen': '-116.92', 'loss/train': '0.64927', 'examples_per_second': '31.343', 'grad_norm': '24.75', 'counters/examples': 225696, 'counters/updates': 7053}
train stats after 225728 examples: {'rewards_train/chosen': '0.23941', 'rewards_train/rejected': '0.029979', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20943', 'logps_train/rejected': '-127.74', 'logps_train/chosen': '-173.66', 'loss/train': '0.61146', 'examples_per_second': '31.617', 'grad_norm': '27.625', 'counters/examples': 225728, 'counters/updates': 7054}
train stats after 225760 examples: {'rewards_train/chosen': '0.13914', 'rewards_train/rejected': '0.046806', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092332', 'logps_train/rejected': '-105.06', 'logps_train/chosen': '-95.386', 'loss/train': '0.66133', 'examples_per_second': '31.86', 'grad_norm': '26.625', 'counters/examples': 225760, 'counters/updates': 7055}
train stats after 225792 examples: {'rewards_train/chosen': '0.098759', 'rewards_train/rejected': '0.021517', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077241', 'logps_train/rejected': '-93.119', 'logps_train/chosen': '-139.28', 'loss/train': '0.66395', 'examples_per_second': '30.443', 'grad_norm': '25', 'counters/examples': 225792, 'counters/updates': 7056}
train stats after 225824 examples: {'rewards_train/chosen': '0.12488', 'rewards_train/rejected': '0.06228', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062601', 'logps_train/rejected': '-135.22', 'logps_train/chosen': '-150.77', 'loss/train': '0.681', 'examples_per_second': '32.47', 'grad_norm': '27.75', 'counters/examples': 225824, 'counters/updates': 7057}
skipping logging after 225856 examples to avoid logging too frequently
train stats after 225888 examples: {'rewards_train/chosen': '0.18369', 'rewards_train/rejected': '0.070126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11356', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-150.07', 'loss/train': '0.65442', 'examples_per_second': '30.598', 'grad_norm': '25.875', 'counters/examples': 225888, 'counters/updates': 7059}
train stats after 225920 examples: {'rewards_train/chosen': '0.093447', 'rewards_train/rejected': '0.036507', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05694', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-141.95', 'loss/train': '0.68264', 'examples_per_second': '30.099', 'grad_norm': '31.5', 'counters/examples': 225920, 'counters/updates': 7060}
train stats after 225952 examples: {'rewards_train/chosen': '0.055086', 'rewards_train/rejected': '0.030354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024732', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-99.254', 'loss/train': '0.69682', 'examples_per_second': '32.311', 'grad_norm': '29.25', 'counters/examples': 225952, 'counters/updates': 7061}
train stats after 225984 examples: {'rewards_train/chosen': '0.12439', 'rewards_train/rejected': '-0.01438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13877', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-145.09', 'loss/train': '0.63731', 'examples_per_second': '31.462', 'grad_norm': '24.25', 'counters/examples': 225984, 'counters/updates': 7062}
train stats after 226016 examples: {'rewards_train/chosen': '0.14256', 'rewards_train/rejected': '0.033801', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10876', 'logps_train/rejected': '-153.21', 'logps_train/chosen': '-161.83', 'loss/train': '0.651', 'examples_per_second': '31.985', 'grad_norm': '30.125', 'counters/examples': 226016, 'counters/updates': 7063}
skipping logging after 226048 examples to avoid logging too frequently
train stats after 226080 examples: {'rewards_train/chosen': '0.093277', 'rewards_train/rejected': '0.0094311', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083846', 'logps_train/rejected': '-96.435', 'logps_train/chosen': '-137.7', 'loss/train': '0.66762', 'examples_per_second': '30.799', 'grad_norm': '23.625', 'counters/examples': 226080, 'counters/updates': 7065}
skipping logging after 226112 examples to avoid logging too frequently
train stats after 226144 examples: {'rewards_train/chosen': '0.086919', 'rewards_train/rejected': '0.0086214', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078297', 'logps_train/rejected': '-122.89', 'logps_train/chosen': '-139.3', 'loss/train': '0.6706', 'examples_per_second': '34.025', 'grad_norm': '29.875', 'counters/examples': 226144, 'counters/updates': 7067}
train stats after 226176 examples: {'rewards_train/chosen': '0.11316', 'rewards_train/rejected': '0.06342', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049735', 'logps_train/rejected': '-153.66', 'logps_train/chosen': '-115.54', 'loss/train': '0.68948', 'examples_per_second': '31.425', 'grad_norm': '27.75', 'counters/examples': 226176, 'counters/updates': 7068}
train stats after 226208 examples: {'rewards_train/chosen': '0.21245', 'rewards_train/rejected': '0.046501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16595', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-106.87', 'loss/train': '0.62574', 'examples_per_second': '32.365', 'grad_norm': '23.75', 'counters/examples': 226208, 'counters/updates': 7069}
train stats after 226240 examples: {'rewards_train/chosen': '0.21842', 'rewards_train/rejected': '0.069182', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14924', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-174.3', 'loss/train': '0.6393', 'examples_per_second': '31.194', 'grad_norm': '26', 'counters/examples': 226240, 'counters/updates': 7070}
train stats after 226272 examples: {'rewards_train/chosen': '0.19278', 'rewards_train/rejected': '0.059803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13298', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-108.05', 'loss/train': '0.64257', 'examples_per_second': '30.171', 'grad_norm': '22.25', 'counters/examples': 226272, 'counters/updates': 7071}
train stats after 226304 examples: {'rewards_train/chosen': '0.092071', 'rewards_train/rejected': '0.05302', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039051', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-118.3', 'loss/train': '0.68318', 'examples_per_second': '32.464', 'grad_norm': '26.625', 'counters/examples': 226304, 'counters/updates': 7072}
train stats after 226336 examples: {'rewards_train/chosen': '0.16271', 'rewards_train/rejected': '-0.0019863', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1647', 'logps_train/rejected': '-133.6', 'logps_train/chosen': '-121.31', 'loss/train': '0.64035', 'examples_per_second': '33.344', 'grad_norm': '26.375', 'counters/examples': 226336, 'counters/updates': 7073}
train stats after 226368 examples: {'rewards_train/chosen': '0.19271', 'rewards_train/rejected': '0.0973', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095408', 'logps_train/rejected': '-132.5', 'logps_train/chosen': '-153.71', 'loss/train': '0.66564', 'examples_per_second': '31.643', 'grad_norm': '28.25', 'counters/examples': 226368, 'counters/updates': 7074}
train stats after 226400 examples: {'rewards_train/chosen': '0.11791', 'rewards_train/rejected': '0.020725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097186', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-123.86', 'loss/train': '0.66122', 'examples_per_second': '30.261', 'grad_norm': '23.5', 'counters/examples': 226400, 'counters/updates': 7075}
train stats after 226432 examples: {'rewards_train/chosen': '0.12519', 'rewards_train/rejected': '0.038817', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086372', 'logps_train/rejected': '-151.55', 'logps_train/chosen': '-151.32', 'loss/train': '0.67139', 'examples_per_second': '31.373', 'grad_norm': '31', 'counters/examples': 226432, 'counters/updates': 7076}
train stats after 226464 examples: {'rewards_train/chosen': '0.10837', 'rewards_train/rejected': '0.042907', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065463', 'logps_train/rejected': '-94.781', 'logps_train/chosen': '-114.04', 'loss/train': '0.67486', 'examples_per_second': '31.728', 'grad_norm': '31.125', 'counters/examples': 226464, 'counters/updates': 7077}
train stats after 226496 examples: {'rewards_train/chosen': '0.1887', 'rewards_train/rejected': '-0.049222', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23793', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-142.49', 'loss/train': '0.60256', 'examples_per_second': '30.943', 'grad_norm': '25.25', 'counters/examples': 226496, 'counters/updates': 7078}
train stats after 226528 examples: {'rewards_train/chosen': '0.11336', 'rewards_train/rejected': '0.13927', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025915', 'logps_train/rejected': '-113.88', 'logps_train/chosen': '-133.96', 'loss/train': '0.72651', 'examples_per_second': '30.569', 'grad_norm': '28', 'counters/examples': 226528, 'counters/updates': 7079}
train stats after 226560 examples: {'rewards_train/chosen': '0.07964', 'rewards_train/rejected': '0.03939', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04025', 'logps_train/rejected': '-125.29', 'logps_train/chosen': '-137.9', 'loss/train': '0.68059', 'examples_per_second': '30.547', 'grad_norm': '23.5', 'counters/examples': 226560, 'counters/updates': 7080}
train stats after 226592 examples: {'rewards_train/chosen': '0.22771', 'rewards_train/rejected': '0.010839', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21687', 'logps_train/rejected': '-160.59', 'logps_train/chosen': '-172.17', 'loss/train': '0.61032', 'examples_per_second': '31.593', 'grad_norm': '29.25', 'counters/examples': 226592, 'counters/updates': 7081}
skipping logging after 226624 examples to avoid logging too frequently
train stats after 226656 examples: {'rewards_train/chosen': '0.12793', 'rewards_train/rejected': '0.073056', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054873', 'logps_train/rejected': '-124.23', 'logps_train/chosen': '-147.96', 'loss/train': '0.68364', 'examples_per_second': '31.638', 'grad_norm': '35.5', 'counters/examples': 226656, 'counters/updates': 7083}
train stats after 226688 examples: {'rewards_train/chosen': '0.10707', 'rewards_train/rejected': '0.049876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057198', 'logps_train/rejected': '-127.58', 'logps_train/chosen': '-143.76', 'loss/train': '0.67822', 'examples_per_second': '32.218', 'grad_norm': '27.875', 'counters/examples': 226688, 'counters/updates': 7084}
skipping logging after 226720 examples to avoid logging too frequently
train stats after 226752 examples: {'rewards_train/chosen': '0.18225', 'rewards_train/rejected': '0.087662', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094587', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-131.92', 'loss/train': '0.66126', 'examples_per_second': '31.315', 'grad_norm': '26.25', 'counters/examples': 226752, 'counters/updates': 7086}
train stats after 226784 examples: {'rewards_train/chosen': '0.16353', 'rewards_train/rejected': '0.013103', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15042', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-173.23', 'loss/train': '0.64219', 'examples_per_second': '32.859', 'grad_norm': '27.25', 'counters/examples': 226784, 'counters/updates': 7087}
train stats after 226816 examples: {'rewards_train/chosen': '0.17326', 'rewards_train/rejected': '0.05628', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11698', 'logps_train/rejected': '-104.18', 'logps_train/chosen': '-139.23', 'loss/train': '0.65827', 'examples_per_second': '30.081', 'grad_norm': '23.5', 'counters/examples': 226816, 'counters/updates': 7088}
train stats after 226848 examples: {'rewards_train/chosen': '0.13567', 'rewards_train/rejected': '-0.077699', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21337', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-142.43', 'loss/train': '0.5997', 'examples_per_second': '32.123', 'grad_norm': '23.75', 'counters/examples': 226848, 'counters/updates': 7089}
train stats after 226880 examples: {'rewards_train/chosen': '0.18275', 'rewards_train/rejected': '0.016326', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16642', 'logps_train/rejected': '-143.17', 'logps_train/chosen': '-176.47', 'loss/train': '0.62318', 'examples_per_second': '30.154', 'grad_norm': '27.625', 'counters/examples': 226880, 'counters/updates': 7090}
train stats after 226912 examples: {'rewards_train/chosen': '0.11496', 'rewards_train/rejected': '-0.00323', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11819', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-137.31', 'loss/train': '0.64893', 'examples_per_second': '32.902', 'grad_norm': '25.75', 'counters/examples': 226912, 'counters/updates': 7091}
train stats after 226944 examples: {'rewards_train/chosen': '0.17427', 'rewards_train/rejected': '0.028396', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14588', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-163.21', 'loss/train': '0.63925', 'examples_per_second': '30.355', 'grad_norm': '26.75', 'counters/examples': 226944, 'counters/updates': 7092}
train stats after 226976 examples: {'rewards_train/chosen': '0.12625', 'rewards_train/rejected': '0.015169', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11108', 'logps_train/rejected': '-124.68', 'logps_train/chosen': '-192.12', 'loss/train': '0.65282', 'examples_per_second': '31.549', 'grad_norm': '30.375', 'counters/examples': 226976, 'counters/updates': 7093}
skipping logging after 227008 examples to avoid logging too frequently
train stats after 227040 examples: {'rewards_train/chosen': '0.22111', 'rewards_train/rejected': '-0.02801', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24912', 'logps_train/rejected': '-139.6', 'logps_train/chosen': '-180.86', 'loss/train': '0.59192', 'examples_per_second': '31.62', 'grad_norm': '24.875', 'counters/examples': 227040, 'counters/updates': 7095}
train stats after 227072 examples: {'rewards_train/chosen': '0.10975', 'rewards_train/rejected': '-0.0028777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11262', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-147.22', 'loss/train': '0.6504', 'examples_per_second': '31.585', 'grad_norm': '26.125', 'counters/examples': 227072, 'counters/updates': 7096}
train stats after 227104 examples: {'rewards_train/chosen': '0.19188', 'rewards_train/rejected': '0.13769', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054185', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-188.73', 'loss/train': '0.68131', 'examples_per_second': '30.163', 'grad_norm': '34.75', 'counters/examples': 227104, 'counters/updates': 7097}
train stats after 227136 examples: {'rewards_train/chosen': '0.14129', 'rewards_train/rejected': '-0.038336', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17962', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-151.05', 'loss/train': '0.63102', 'examples_per_second': '31.657', 'grad_norm': '24.5', 'counters/examples': 227136, 'counters/updates': 7098}
train stats after 227168 examples: {'rewards_train/chosen': '0.11746', 'rewards_train/rejected': '0.069525', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047937', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-149.94', 'loss/train': '0.67628', 'examples_per_second': '32.212', 'grad_norm': '26', 'counters/examples': 227168, 'counters/updates': 7099}
train stats after 227200 examples: {'rewards_train/chosen': '0.14056', 'rewards_train/rejected': '0.10121', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039349', 'logps_train/rejected': '-160.33', 'logps_train/chosen': '-158.84', 'loss/train': '0.68487', 'examples_per_second': '31.635', 'grad_norm': '29.125', 'counters/examples': 227200, 'counters/updates': 7100}
train stats after 227232 examples: {'rewards_train/chosen': '0.15102', 'rewards_train/rejected': '-0.014517', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16553', 'logps_train/rejected': '-137.17', 'logps_train/chosen': '-157.64', 'loss/train': '0.62086', 'examples_per_second': '31.617', 'grad_norm': '28.375', 'counters/examples': 227232, 'counters/updates': 7101}
skipping logging after 227264 examples to avoid logging too frequently
train stats after 227296 examples: {'rewards_train/chosen': '0.16689', 'rewards_train/rejected': '-0.0077547', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17465', 'logps_train/rejected': '-98.878', 'logps_train/chosen': '-165.76', 'loss/train': '0.62235', 'examples_per_second': '31.002', 'grad_norm': '26.125', 'counters/examples': 227296, 'counters/updates': 7103}
skipping logging after 227328 examples to avoid logging too frequently
train stats after 227360 examples: {'rewards_train/chosen': '0.088386', 'rewards_train/rejected': '0.019476', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06891', 'logps_train/rejected': '-138.76', 'logps_train/chosen': '-95.779', 'loss/train': '0.67457', 'examples_per_second': '30.636', 'grad_norm': '29', 'counters/examples': 227360, 'counters/updates': 7105}
skipping logging after 227392 examples to avoid logging too frequently
train stats after 227424 examples: {'rewards_train/chosen': '0.26029', 'rewards_train/rejected': '0.051785', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2085', 'logps_train/rejected': '-138.84', 'logps_train/chosen': '-148.33', 'loss/train': '0.61226', 'examples_per_second': '34.312', 'grad_norm': '24.75', 'counters/examples': 227424, 'counters/updates': 7107}
train stats after 227456 examples: {'rewards_train/chosen': '0.22519', 'rewards_train/rejected': '-0.013885', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23907', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-160.12', 'loss/train': '0.5948', 'examples_per_second': '31.644', 'grad_norm': '26', 'counters/examples': 227456, 'counters/updates': 7108}
train stats after 227488 examples: {'rewards_train/chosen': '0.1438', 'rewards_train/rejected': '-0.017389', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16119', 'logps_train/rejected': '-115.39', 'logps_train/chosen': '-138.18', 'loss/train': '0.63035', 'examples_per_second': '31.626', 'grad_norm': '26.875', 'counters/examples': 227488, 'counters/updates': 7109}
train stats after 227520 examples: {'rewards_train/chosen': '0.18065', 'rewards_train/rejected': '-0.062216', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24286', 'logps_train/rejected': '-101.67', 'logps_train/chosen': '-116.82', 'loss/train': '0.59114', 'examples_per_second': '31.379', 'grad_norm': '21.125', 'counters/examples': 227520, 'counters/updates': 7110}
train stats after 227552 examples: {'rewards_train/chosen': '0.067045', 'rewards_train/rejected': '0.041442', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025603', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-147.93', 'loss/train': '0.69225', 'examples_per_second': '31.456', 'grad_norm': '28.625', 'counters/examples': 227552, 'counters/updates': 7111}
skipping logging after 227584 examples to avoid logging too frequently
train stats after 227616 examples: {'rewards_train/chosen': '0.16394', 'rewards_train/rejected': '0.0058722', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15807', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-134.64', 'loss/train': '0.64345', 'examples_per_second': '36.312', 'grad_norm': '26.75', 'counters/examples': 227616, 'counters/updates': 7113}
train stats after 227648 examples: {'rewards_train/chosen': '0.095788', 'rewards_train/rejected': '-0.071276', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16706', 'logps_train/rejected': '-98.049', 'logps_train/chosen': '-121.61', 'loss/train': '0.62721', 'examples_per_second': '30.808', 'grad_norm': '21.25', 'counters/examples': 227648, 'counters/updates': 7114}
train stats after 227680 examples: {'rewards_train/chosen': '0.06216', 'rewards_train/rejected': '0.080814', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018654', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-145.3', 'loss/train': '0.71715', 'examples_per_second': '32.651', 'grad_norm': '28.25', 'counters/examples': 227680, 'counters/updates': 7115}
train stats after 227712 examples: {'rewards_train/chosen': '0.086085', 'rewards_train/rejected': '0.064444', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021641', 'logps_train/rejected': '-178.01', 'logps_train/chosen': '-115.95', 'loss/train': '0.69379', 'examples_per_second': '30.224', 'grad_norm': '28.25', 'counters/examples': 227712, 'counters/updates': 7116}
train stats after 227744 examples: {'rewards_train/chosen': '0.15756', 'rewards_train/rejected': '0.048897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10866', 'logps_train/rejected': '-145.69', 'logps_train/chosen': '-154.72', 'loss/train': '0.65644', 'examples_per_second': '30.169', 'grad_norm': '26.75', 'counters/examples': 227744, 'counters/updates': 7117}
train stats after 227776 examples: {'rewards_train/chosen': '0.1301', 'rewards_train/rejected': '-0.020161', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15026', 'logps_train/rejected': '-123.79', 'logps_train/chosen': '-143.37', 'loss/train': '0.63646', 'examples_per_second': '30.036', 'grad_norm': '27.75', 'counters/examples': 227776, 'counters/updates': 7118}
train stats after 227808 examples: {'rewards_train/chosen': '0.14794', 'rewards_train/rejected': '0.064791', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.083152', 'logps_train/rejected': '-160.75', 'logps_train/chosen': '-151.48', 'loss/train': '0.67049', 'examples_per_second': '31.5', 'grad_norm': '28.75', 'counters/examples': 227808, 'counters/updates': 7119}
train stats after 227840 examples: {'rewards_train/chosen': '0.18361', 'rewards_train/rejected': '0.10212', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081493', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-158.53', 'loss/train': '0.66102', 'examples_per_second': '31.679', 'grad_norm': '26', 'counters/examples': 227840, 'counters/updates': 7120}
skipping logging after 227872 examples to avoid logging too frequently
train stats after 227904 examples: {'rewards_train/chosen': '0.18064', 'rewards_train/rejected': '0.059156', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12148', 'logps_train/rejected': '-130.85', 'logps_train/chosen': '-157.99', 'loss/train': '0.65064', 'examples_per_second': '30.543', 'grad_norm': '24.875', 'counters/examples': 227904, 'counters/updates': 7122}
skipping logging after 227936 examples to avoid logging too frequently
train stats after 227968 examples: {'rewards_train/chosen': '0.15033', 'rewards_train/rejected': '-0.080847', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23117', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-151.28', 'loss/train': '0.59871', 'examples_per_second': '31.611', 'grad_norm': '25.125', 'counters/examples': 227968, 'counters/updates': 7124}
train stats after 228000 examples: {'rewards_train/chosen': '0.14551', 'rewards_train/rejected': '0.011526', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13398', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-148.96', 'loss/train': '0.64772', 'examples_per_second': '31.551', 'grad_norm': '25.625', 'counters/examples': 228000, 'counters/updates': 7125}
Running evaluation after 228000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.27it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 228000: {'rewards_eval/chosen': '0.13273', 'rewards_eval/rejected': '0.040872', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.091861', 'logps_eval/rejected': '-118.2', 'logps_eval/chosen': '-138.11', 'loss/eval': '0.66502'}
skipping save for non epoch
train stats after 228032 examples: {'rewards_train/chosen': '0.035226', 'rewards_train/rejected': '0.050849', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.015623', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-126.69', 'loss/train': '0.71666', 'examples_per_second': '32.896', 'grad_norm': '26.625', 'counters/examples': 228032, 'counters/updates': 7126}
train stats after 228064 examples: {'rewards_train/chosen': '0.29097', 'rewards_train/rejected': '-0.0062115', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.29718', 'logps_train/rejected': '-109.16', 'logps_train/chosen': '-182.21', 'loss/train': '0.56793', 'examples_per_second': '31.719', 'grad_norm': '23.75', 'counters/examples': 228064, 'counters/updates': 7127}
train stats after 228096 examples: {'rewards_train/chosen': '0.11823', 'rewards_train/rejected': '0.09283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025401', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-154.23', 'loss/train': '0.69306', 'examples_per_second': '30.028', 'grad_norm': '27.75', 'counters/examples': 228096, 'counters/updates': 7128}
train stats after 228128 examples: {'rewards_train/chosen': '0.064388', 'rewards_train/rejected': '0.018853', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045535', 'logps_train/rejected': '-184.75', 'logps_train/chosen': '-161.89', 'loss/train': '0.68211', 'examples_per_second': '29.957', 'grad_norm': '30.5', 'counters/examples': 228128, 'counters/updates': 7129}
skipping logging after 228160 examples to avoid logging too frequently
train stats after 228192 examples: {'rewards_train/chosen': '0.063408', 'rewards_train/rejected': '0.069578', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0061696', 'logps_train/rejected': '-116.26', 'logps_train/chosen': '-122.94', 'loss/train': '0.70741', 'examples_per_second': '33.933', 'grad_norm': '26.25', 'counters/examples': 228192, 'counters/updates': 7131}
skipping logging after 228224 examples to avoid logging too frequently
train stats after 228256 examples: {'rewards_train/chosen': '0.13913', 'rewards_train/rejected': '-8.5998e-05', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13921', 'logps_train/rejected': '-91.946', 'logps_train/chosen': '-134.02', 'loss/train': '0.64161', 'examples_per_second': '31.281', 'grad_norm': '21.875', 'counters/examples': 228256, 'counters/updates': 7133}
train stats after 228288 examples: {'rewards_train/chosen': '0.18349', 'rewards_train/rejected': '0.0021458', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18135', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-157.66', 'loss/train': '0.62198', 'examples_per_second': '32.783', 'grad_norm': '28', 'counters/examples': 228288, 'counters/updates': 7134}
train stats after 228320 examples: {'rewards_train/chosen': '0.17351', 'rewards_train/rejected': '0.05717', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11633', 'logps_train/rejected': '-114.99', 'logps_train/chosen': '-138.54', 'loss/train': '0.65442', 'examples_per_second': '32.92', 'grad_norm': '23', 'counters/examples': 228320, 'counters/updates': 7135}
train stats after 228352 examples: {'rewards_train/chosen': '0.20734', 'rewards_train/rejected': '0.0070024', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20033', 'logps_train/rejected': '-135.91', 'logps_train/chosen': '-164.98', 'loss/train': '0.60994', 'examples_per_second': '30.449', 'grad_norm': '25', 'counters/examples': 228352, 'counters/updates': 7136}
train stats after 228384 examples: {'rewards_train/chosen': '0.20507', 'rewards_train/rejected': '0.029219', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17585', 'logps_train/rejected': '-103.02', 'logps_train/chosen': '-135.87', 'loss/train': '0.62383', 'examples_per_second': '31.57', 'grad_norm': '22.5', 'counters/examples': 228384, 'counters/updates': 7137}
train stats after 228416 examples: {'rewards_train/chosen': '0.19795', 'rewards_train/rejected': '0.057979', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13997', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-196.53', 'loss/train': '0.64566', 'examples_per_second': '31.645', 'grad_norm': '26.75', 'counters/examples': 228416, 'counters/updates': 7138}
train stats after 228448 examples: {'rewards_train/chosen': '0.12474', 'rewards_train/rejected': '0.08345', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041291', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-160.33', 'loss/train': '0.68469', 'examples_per_second': '31.285', 'grad_norm': '28', 'counters/examples': 228448, 'counters/updates': 7139}
skipping logging after 228480 examples to avoid logging too frequently
train stats after 228512 examples: {'rewards_train/chosen': '0.12579', 'rewards_train/rejected': '0.072361', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053433', 'logps_train/rejected': '-142.43', 'logps_train/chosen': '-179.46', 'loss/train': '0.67893', 'examples_per_second': '31.611', 'grad_norm': '27.875', 'counters/examples': 228512, 'counters/updates': 7141}
train stats after 228544 examples: {'rewards_train/chosen': '0.17121', 'rewards_train/rejected': '0.0239', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14731', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-131.55', 'loss/train': '0.63676', 'examples_per_second': '30.131', 'grad_norm': '22.375', 'counters/examples': 228544, 'counters/updates': 7142}
skipping logging after 228576 examples to avoid logging too frequently
train stats after 228608 examples: {'rewards_train/chosen': '0.10897', 'rewards_train/rejected': '-0.0033874', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11235', 'logps_train/rejected': '-100.17', 'logps_train/chosen': '-124.11', 'loss/train': '0.64624', 'examples_per_second': '32.024', 'grad_norm': '23.25', 'counters/examples': 228608, 'counters/updates': 7144}
train stats after 228640 examples: {'rewards_train/chosen': '0.097936', 'rewards_train/rejected': '-0.034545', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13248', 'logps_train/rejected': '-106.91', 'logps_train/chosen': '-137.31', 'loss/train': '0.64397', 'examples_per_second': '31.47', 'grad_norm': '24.625', 'counters/examples': 228640, 'counters/updates': 7145}
skipping logging after 228672 examples to avoid logging too frequently
train stats after 228704 examples: {'rewards_train/chosen': '0.062238', 'rewards_train/rejected': '-0.11747', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17971', 'logps_train/rejected': '-94.052', 'logps_train/chosen': '-139.47', 'loss/train': '0.62575', 'examples_per_second': '31.84', 'grad_norm': '23.375', 'counters/examples': 228704, 'counters/updates': 7147}
train stats after 228736 examples: {'rewards_train/chosen': '0.051171', 'rewards_train/rejected': '-0.061385', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11256', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-132', 'loss/train': '0.65203', 'examples_per_second': '31.765', 'grad_norm': '24.25', 'counters/examples': 228736, 'counters/updates': 7148}
train stats after 228768 examples: {'rewards_train/chosen': '0.13981', 'rewards_train/rejected': '0.050238', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.089569', 'logps_train/rejected': '-126.9', 'logps_train/chosen': '-155.81', 'loss/train': '0.65789', 'examples_per_second': '24.511', 'grad_norm': '28.125', 'counters/examples': 228768, 'counters/updates': 7149}
train stats after 228800 examples: {'rewards_train/chosen': '0.16006', 'rewards_train/rejected': '0.083065', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076994', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-139.08', 'loss/train': '0.68552', 'examples_per_second': '30.837', 'grad_norm': '30', 'counters/examples': 228800, 'counters/updates': 7150}
skipping logging after 228832 examples to avoid logging too frequently
train stats after 228864 examples: {'rewards_train/chosen': '0.1623', 'rewards_train/rejected': '0.06727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095026', 'logps_train/rejected': '-131.98', 'logps_train/chosen': '-150.7', 'loss/train': '0.65627', 'examples_per_second': '33.259', 'grad_norm': '27.125', 'counters/examples': 228864, 'counters/updates': 7152}
skipping logging after 228896 examples to avoid logging too frequently
train stats after 228928 examples: {'rewards_train/chosen': '0.14885', 'rewards_train/rejected': '0.018758', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13009', 'logps_train/rejected': '-112.91', 'logps_train/chosen': '-122.8', 'loss/train': '0.64354', 'examples_per_second': '32.513', 'grad_norm': '24', 'counters/examples': 228928, 'counters/updates': 7154}
skipping logging after 228960 examples to avoid logging too frequently
train stats after 228992 examples: {'rewards_train/chosen': '0.21035', 'rewards_train/rejected': '-0.001464', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21181', 'logps_train/rejected': '-120.1', 'logps_train/chosen': '-139.8', 'loss/train': '0.60931', 'examples_per_second': '31.578', 'grad_norm': '25.625', 'counters/examples': 228992, 'counters/updates': 7156}
train stats after 229024 examples: {'rewards_train/chosen': '0.16004', 'rewards_train/rejected': '0.019371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14067', 'logps_train/rejected': '-101.34', 'logps_train/chosen': '-149.83', 'loss/train': '0.64111', 'examples_per_second': '32.127', 'grad_norm': '23.25', 'counters/examples': 229024, 'counters/updates': 7157}
train stats after 229056 examples: {'rewards_train/chosen': '0.063531', 'rewards_train/rejected': '0.047692', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015839', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-119.86', 'loss/train': '0.69421', 'examples_per_second': '31.64', 'grad_norm': '29.375', 'counters/examples': 229056, 'counters/updates': 7158}
train stats after 229088 examples: {'rewards_train/chosen': '0.091016', 'rewards_train/rejected': '0.049891', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041125', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-134.65', 'loss/train': '0.68171', 'examples_per_second': '31.153', 'grad_norm': '27.125', 'counters/examples': 229088, 'counters/updates': 7159}
train stats after 229120 examples: {'rewards_train/chosen': '0.15488', 'rewards_train/rejected': '-0.032508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18739', 'logps_train/rejected': '-115.26', 'logps_train/chosen': '-168.93', 'loss/train': '0.61352', 'examples_per_second': '30.321', 'grad_norm': '27.25', 'counters/examples': 229120, 'counters/updates': 7160}
train stats after 229152 examples: {'rewards_train/chosen': '0.13297', 'rewards_train/rejected': '0.069943', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063023', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-136.83', 'loss/train': '0.6708', 'examples_per_second': '30.106', 'grad_norm': '32.25', 'counters/examples': 229152, 'counters/updates': 7161}
train stats after 229184 examples: {'rewards_train/chosen': '0.13367', 'rewards_train/rejected': '0.056215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07745', 'logps_train/rejected': '-99.907', 'logps_train/chosen': '-120.9', 'loss/train': '0.67207', 'examples_per_second': '31.312', 'grad_norm': '24.5', 'counters/examples': 229184, 'counters/updates': 7162}
train stats after 229216 examples: {'rewards_train/chosen': '0.14516', 'rewards_train/rejected': '0.076646', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.06851', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-157.32', 'loss/train': '0.68224', 'examples_per_second': '31.305', 'grad_norm': '31.875', 'counters/examples': 229216, 'counters/updates': 7163}
train stats after 229248 examples: {'rewards_train/chosen': '0.16045', 'rewards_train/rejected': '0.02924', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13121', 'logps_train/rejected': '-178.34', 'logps_train/chosen': '-166.58', 'loss/train': '0.64924', 'examples_per_second': '30.322', 'grad_norm': '31.625', 'counters/examples': 229248, 'counters/updates': 7164}
train stats after 229280 examples: {'rewards_train/chosen': '0.2057', 'rewards_train/rejected': '0.057537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14816', 'logps_train/rejected': '-162.81', 'logps_train/chosen': '-154.76', 'loss/train': '0.63538', 'examples_per_second': '33.292', 'grad_norm': '32.5', 'counters/examples': 229280, 'counters/updates': 7165}
train stats after 229312 examples: {'rewards_train/chosen': '0.1508', 'rewards_train/rejected': '0.15762', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0068221', 'logps_train/rejected': '-123.46', 'logps_train/chosen': '-131.16', 'loss/train': '0.7227', 'examples_per_second': '31.362', 'grad_norm': '29.375', 'counters/examples': 229312, 'counters/updates': 7166}
skipping logging after 229344 examples to avoid logging too frequently
train stats after 229376 examples: {'rewards_train/chosen': '0.056231', 'rewards_train/rejected': '-0.010419', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06665', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-157.92', 'loss/train': '0.67276', 'examples_per_second': '30.427', 'grad_norm': '26.5', 'counters/examples': 229376, 'counters/updates': 7168}
train stats after 229408 examples: {'rewards_train/chosen': '0.075598', 'rewards_train/rejected': '-0.055505', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1311', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-164.14', 'loss/train': '0.64536', 'examples_per_second': '32.824', 'grad_norm': '26.5', 'counters/examples': 229408, 'counters/updates': 7169}
train stats after 229440 examples: {'rewards_train/chosen': '0.08811', 'rewards_train/rejected': '-0.020539', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10865', 'logps_train/rejected': '-105.02', 'logps_train/chosen': '-146.46', 'loss/train': '0.65099', 'examples_per_second': '30.088', 'grad_norm': '26.125', 'counters/examples': 229440, 'counters/updates': 7170}
train stats after 229472 examples: {'rewards_train/chosen': '0.033542', 'rewards_train/rejected': '0.060151', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026609', 'logps_train/rejected': '-106.3', 'logps_train/chosen': '-117.85', 'loss/train': '0.72043', 'examples_per_second': '31.78', 'grad_norm': '26', 'counters/examples': 229472, 'counters/updates': 7171}
train stats after 229504 examples: {'rewards_train/chosen': '0.1631', 'rewards_train/rejected': '-0.039913', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20301', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-167.8', 'loss/train': '0.61165', 'examples_per_second': '32.371', 'grad_norm': '25.125', 'counters/examples': 229504, 'counters/updates': 7172}
train stats after 229536 examples: {'rewards_train/chosen': '0.19177', 'rewards_train/rejected': '0.054685', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13709', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-123.83', 'loss/train': '0.6391', 'examples_per_second': '32.351', 'grad_norm': '24', 'counters/examples': 229536, 'counters/updates': 7173}
train stats after 229568 examples: {'rewards_train/chosen': '0.17965', 'rewards_train/rejected': '0.077801', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10185', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-149.17', 'loss/train': '0.65553', 'examples_per_second': '31.346', 'grad_norm': '28.875', 'counters/examples': 229568, 'counters/updates': 7174}
train stats after 229600 examples: {'rewards_train/chosen': '0.10079', 'rewards_train/rejected': '0.037992', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062799', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-115.47', 'loss/train': '0.67422', 'examples_per_second': '32.542', 'grad_norm': '23.875', 'counters/examples': 229600, 'counters/updates': 7175}
train stats after 229632 examples: {'rewards_train/chosen': '0.21673', 'rewards_train/rejected': '0.048096', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16863', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-141.97', 'loss/train': '0.62663', 'examples_per_second': '32.345', 'grad_norm': '25.125', 'counters/examples': 229632, 'counters/updates': 7176}
train stats after 229664 examples: {'rewards_train/chosen': '0.18282', 'rewards_train/rejected': '0.072101', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11072', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-167.81', 'loss/train': '0.65874', 'examples_per_second': '30.426', 'grad_norm': '27.25', 'counters/examples': 229664, 'counters/updates': 7177}
train stats after 229696 examples: {'rewards_train/chosen': '0.20352', 'rewards_train/rejected': '0.12171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081816', 'logps_train/rejected': '-95.709', 'logps_train/chosen': '-167.76', 'loss/train': '0.66925', 'examples_per_second': '24.276', 'grad_norm': '29.125', 'counters/examples': 229696, 'counters/updates': 7178}
train stats after 229728 examples: {'rewards_train/chosen': '0.17078', 'rewards_train/rejected': '-0.025097', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19588', 'logps_train/rejected': '-110.93', 'logps_train/chosen': '-157.19', 'loss/train': '0.62156', 'examples_per_second': '31.297', 'grad_norm': '24.375', 'counters/examples': 229728, 'counters/updates': 7179}
train stats after 229760 examples: {'rewards_train/chosen': '0.094868', 'rewards_train/rejected': '-0.02786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12273', 'logps_train/rejected': '-109.78', 'logps_train/chosen': '-163.32', 'loss/train': '0.64161', 'examples_per_second': '31.47', 'grad_norm': '24.625', 'counters/examples': 229760, 'counters/updates': 7180}
train stats after 229792 examples: {'rewards_train/chosen': '0.12275', 'rewards_train/rejected': '0.0076248', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11513', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-134.63', 'loss/train': '0.6504', 'examples_per_second': '24.266', 'grad_norm': '23.875', 'counters/examples': 229792, 'counters/updates': 7181}
train stats after 229824 examples: {'rewards_train/chosen': '0.10449', 'rewards_train/rejected': '-0.040942', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14543', 'logps_train/rejected': '-90.658', 'logps_train/chosen': '-126.91', 'loss/train': '0.6387', 'examples_per_second': '32.555', 'grad_norm': '22.625', 'counters/examples': 229824, 'counters/updates': 7182}
skipping logging after 229856 examples to avoid logging too frequently
train stats after 229888 examples: {'rewards_train/chosen': '0.24677', 'rewards_train/rejected': '0.027849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21893', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-164.08', 'loss/train': '0.61183', 'examples_per_second': '29.965', 'grad_norm': '26', 'counters/examples': 229888, 'counters/updates': 7184}
train stats after 229920 examples: {'rewards_train/chosen': '0.050037', 'rewards_train/rejected': '-0.036202', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086239', 'logps_train/rejected': '-123.71', 'logps_train/chosen': '-148.5', 'loss/train': '0.66783', 'examples_per_second': '30.483', 'grad_norm': '26.375', 'counters/examples': 229920, 'counters/updates': 7185}
train stats after 229952 examples: {'rewards_train/chosen': '0.16031', 'rewards_train/rejected': '-0.032282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1926', 'logps_train/rejected': '-95.673', 'logps_train/chosen': '-145.85', 'loss/train': '0.61705', 'examples_per_second': '31.75', 'grad_norm': '23.375', 'counters/examples': 229952, 'counters/updates': 7186}
skipping logging after 229984 examples to avoid logging too frequently
train stats after 230016 examples: {'rewards_train/chosen': '0.051255', 'rewards_train/rejected': '-0.044293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.095548', 'logps_train/rejected': '-93.41', 'logps_train/chosen': '-104.67', 'loss/train': '0.66278', 'examples_per_second': '31.65', 'grad_norm': '22.875', 'counters/examples': 230016, 'counters/updates': 7188}
train stats after 230048 examples: {'rewards_train/chosen': '0.11597', 'rewards_train/rejected': '0.10445', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011529', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-160.97', 'loss/train': '0.70425', 'examples_per_second': '31.851', 'grad_norm': '26.625', 'counters/examples': 230048, 'counters/updates': 7189}
train stats after 230080 examples: {'rewards_train/chosen': '0.14019', 'rewards_train/rejected': '0.022588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1176', 'logps_train/rejected': '-99.439', 'logps_train/chosen': '-117.11', 'loss/train': '0.64942', 'examples_per_second': '31.074', 'grad_norm': '24.75', 'counters/examples': 230080, 'counters/updates': 7190}
skipping logging after 230112 examples to avoid logging too frequently
train stats after 230144 examples: {'rewards_train/chosen': '0.18341', 'rewards_train/rejected': '0.16332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020092', 'logps_train/rejected': '-151.73', 'logps_train/chosen': '-161.19', 'loss/train': '0.70728', 'examples_per_second': '33.905', 'grad_norm': '32.25', 'counters/examples': 230144, 'counters/updates': 7192}
train stats after 230176 examples: {'rewards_train/chosen': '0.16114', 'rewards_train/rejected': '-0.00021082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16135', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-153.55', 'loss/train': '0.63345', 'examples_per_second': '32.163', 'grad_norm': '27.125', 'counters/examples': 230176, 'counters/updates': 7193}
train stats after 230208 examples: {'rewards_train/chosen': '0.15319', 'rewards_train/rejected': '-0.027606', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1808', 'logps_train/rejected': '-136.22', 'logps_train/chosen': '-144.15', 'loss/train': '0.61787', 'examples_per_second': '30.807', 'grad_norm': '24.25', 'counters/examples': 230208, 'counters/updates': 7194}
train stats after 230240 examples: {'rewards_train/chosen': '0.11139', 'rewards_train/rejected': '0.043791', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0676', 'logps_train/rejected': '-158.67', 'logps_train/chosen': '-133.15', 'loss/train': '0.67563', 'examples_per_second': '31.593', 'grad_norm': '29.5', 'counters/examples': 230240, 'counters/updates': 7195}
train stats after 230272 examples: {'rewards_train/chosen': '0.16257', 'rewards_train/rejected': '-0.019844', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18241', 'logps_train/rejected': '-121.48', 'logps_train/chosen': '-122.19', 'loss/train': '0.61297', 'examples_per_second': '30.711', 'grad_norm': '30', 'counters/examples': 230272, 'counters/updates': 7196}
train stats after 230304 examples: {'rewards_train/chosen': '0.13154', 'rewards_train/rejected': '0.033601', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09794', 'logps_train/rejected': '-110.28', 'logps_train/chosen': '-111.37', 'loss/train': '0.66081', 'examples_per_second': '32.543', 'grad_norm': '24.25', 'counters/examples': 230304, 'counters/updates': 7197}
train stats after 230336 examples: {'rewards_train/chosen': '0.10181', 'rewards_train/rejected': '0.0021433', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099662', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-152.98', 'loss/train': '0.65584', 'examples_per_second': '31.079', 'grad_norm': '25.125', 'counters/examples': 230336, 'counters/updates': 7198}
train stats after 230368 examples: {'rewards_train/chosen': '0.14212', 'rewards_train/rejected': '0.06296', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.079162', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-124.84', 'loss/train': '0.66827', 'examples_per_second': '30.886', 'grad_norm': '27.125', 'counters/examples': 230368, 'counters/updates': 7199}
train stats after 230400 examples: {'rewards_train/chosen': '0.11736', 'rewards_train/rejected': '0.077712', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039648', 'logps_train/rejected': '-144.3', 'logps_train/chosen': '-122.88', 'loss/train': '0.71048', 'examples_per_second': '31.093', 'grad_norm': '37', 'counters/examples': 230400, 'counters/updates': 7200}
train stats after 230432 examples: {'rewards_train/chosen': '0.1745', 'rewards_train/rejected': '0.061635', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11286', 'logps_train/rejected': '-136.83', 'logps_train/chosen': '-153.23', 'loss/train': '0.65397', 'examples_per_second': '31.598', 'grad_norm': '29.875', 'counters/examples': 230432, 'counters/updates': 7201}
train stats after 230464 examples: {'rewards_train/chosen': '0.052741', 'rewards_train/rejected': '-0.0080215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060763', 'logps_train/rejected': '-173.54', 'logps_train/chosen': '-143.06', 'loss/train': '0.67737', 'examples_per_second': '31.616', 'grad_norm': '31.75', 'counters/examples': 230464, 'counters/updates': 7202}
train stats after 230496 examples: {'rewards_train/chosen': '0.1212', 'rewards_train/rejected': '0.048836', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072359', 'logps_train/rejected': '-97.861', 'logps_train/chosen': '-116.89', 'loss/train': '0.6702', 'examples_per_second': '31.07', 'grad_norm': '22.625', 'counters/examples': 230496, 'counters/updates': 7203}
train stats after 230528 examples: {'rewards_train/chosen': '0.16714', 'rewards_train/rejected': '0.010962', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15618', 'logps_train/rejected': '-110.82', 'logps_train/chosen': '-163.67', 'loss/train': '0.62856', 'examples_per_second': '33.321', 'grad_norm': '28.5', 'counters/examples': 230528, 'counters/updates': 7204}
train stats after 230560 examples: {'rewards_train/chosen': '0.057086', 'rewards_train/rejected': '-0.00034906', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057435', 'logps_train/rejected': '-127.61', 'logps_train/chosen': '-114.15', 'loss/train': '0.673', 'examples_per_second': '31.558', 'grad_norm': '26', 'counters/examples': 230560, 'counters/updates': 7205}
train stats after 230592 examples: {'rewards_train/chosen': '0.14329', 'rewards_train/rejected': '0.011419', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13187', 'logps_train/rejected': '-121.1', 'logps_train/chosen': '-128.91', 'loss/train': '0.64388', 'examples_per_second': '30.982', 'grad_norm': '24.375', 'counters/examples': 230592, 'counters/updates': 7206}
train stats after 230624 examples: {'rewards_train/chosen': '0.051799', 'rewards_train/rejected': '0.0056958', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046103', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-112.46', 'loss/train': '0.6881', 'examples_per_second': '31.806', 'grad_norm': '26.75', 'counters/examples': 230624, 'counters/updates': 7207}
skipping logging after 230656 examples to avoid logging too frequently
train stats after 230688 examples: {'rewards_train/chosen': '0.041775', 'rewards_train/rejected': '0.043879', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0021042', 'logps_train/rejected': '-99.716', 'logps_train/chosen': '-147.83', 'loss/train': '0.70793', 'examples_per_second': '30.921', 'grad_norm': '24.25', 'counters/examples': 230688, 'counters/updates': 7209}
train stats after 230720 examples: {'rewards_train/chosen': '0.17376', 'rewards_train/rejected': '0.076618', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097139', 'logps_train/rejected': '-117.51', 'logps_train/chosen': '-133.98', 'loss/train': '0.65443', 'examples_per_second': '30.822', 'grad_norm': '23.25', 'counters/examples': 230720, 'counters/updates': 7210}
train stats after 230752 examples: {'rewards_train/chosen': '0.14961', 'rewards_train/rejected': '-0.013247', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16286', 'logps_train/rejected': '-150.57', 'logps_train/chosen': '-136.4', 'loss/train': '0.63085', 'examples_per_second': '31.612', 'grad_norm': '24.75', 'counters/examples': 230752, 'counters/updates': 7211}
skipping logging after 230784 examples to avoid logging too frequently
train stats after 230816 examples: {'rewards_train/chosen': '0.11052', 'rewards_train/rejected': '0.091066', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.019453', 'logps_train/rejected': '-119.34', 'logps_train/chosen': '-158.88', 'loss/train': '0.6975', 'examples_per_second': '32.633', 'grad_norm': '26.875', 'counters/examples': 230816, 'counters/updates': 7213}
skipping logging after 230848 examples to avoid logging too frequently
train stats after 230880 examples: {'rewards_train/chosen': '0.10611', 'rewards_train/rejected': '-0.027212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13332', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-142.59', 'loss/train': '0.64154', 'examples_per_second': '30.043', 'grad_norm': '23.75', 'counters/examples': 230880, 'counters/updates': 7215}
train stats after 230912 examples: {'rewards_train/chosen': '0.15389', 'rewards_train/rejected': '0.064589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089297', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-131.51', 'loss/train': '0.66024', 'examples_per_second': '32.251', 'grad_norm': '25.125', 'counters/examples': 230912, 'counters/updates': 7216}
train stats after 230944 examples: {'rewards_train/chosen': '0.18118', 'rewards_train/rejected': '0.035467', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14572', 'logps_train/rejected': '-168.46', 'logps_train/chosen': '-158.4', 'loss/train': '0.64049', 'examples_per_second': '32.282', 'grad_norm': '26.125', 'counters/examples': 230944, 'counters/updates': 7217}
train stats after 230976 examples: {'rewards_train/chosen': '0.11505', 'rewards_train/rejected': '0.091828', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02322', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-158.21', 'loss/train': '0.69741', 'examples_per_second': '31.563', 'grad_norm': '29.5', 'counters/examples': 230976, 'counters/updates': 7218}
train stats after 231008 examples: {'rewards_train/chosen': '0.12669', 'rewards_train/rejected': '0.062139', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064552', 'logps_train/rejected': '-126.4', 'logps_train/chosen': '-134.36', 'loss/train': '0.66738', 'examples_per_second': '31.539', 'grad_norm': '26.625', 'counters/examples': 231008, 'counters/updates': 7219}
train stats after 231040 examples: {'rewards_train/chosen': '0.17925', 'rewards_train/rejected': '0.050148', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1291', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-124.62', 'loss/train': '0.64472', 'examples_per_second': '31.504', 'grad_norm': '26.125', 'counters/examples': 231040, 'counters/updates': 7220}
skipping logging after 231072 examples to avoid logging too frequently
train stats after 231104 examples: {'rewards_train/chosen': '0.086992', 'rewards_train/rejected': '0.038072', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048919', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-125.92', 'loss/train': '0.68113', 'examples_per_second': '31.903', 'grad_norm': '26.625', 'counters/examples': 231104, 'counters/updates': 7222}
skipping logging after 231136 examples to avoid logging too frequently
train stats after 231168 examples: {'rewards_train/chosen': '0.1398', 'rewards_train/rejected': '-0.014565', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15436', 'logps_train/rejected': '-103.48', 'logps_train/chosen': '-148.32', 'loss/train': '0.63554', 'examples_per_second': '30.259', 'grad_norm': '24.75', 'counters/examples': 231168, 'counters/updates': 7224}
skipping logging after 231200 examples to avoid logging too frequently
train stats after 231232 examples: {'rewards_train/chosen': '0.095777', 'rewards_train/rejected': '-0.043307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13908', 'logps_train/rejected': '-106.2', 'logps_train/chosen': '-137.38', 'loss/train': '0.64247', 'examples_per_second': '30.152', 'grad_norm': '26.25', 'counters/examples': 231232, 'counters/updates': 7226}
train stats after 231264 examples: {'rewards_train/chosen': '0.22042', 'rewards_train/rejected': '0.025297', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19512', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-165.97', 'loss/train': '0.61025', 'examples_per_second': '31.358', 'grad_norm': '26.75', 'counters/examples': 231264, 'counters/updates': 7227}
train stats after 231296 examples: {'rewards_train/chosen': '0.10582', 'rewards_train/rejected': '0.020023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085793', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-152.63', 'loss/train': '0.66542', 'examples_per_second': '31.58', 'grad_norm': '30', 'counters/examples': 231296, 'counters/updates': 7228}
skipping logging after 231328 examples to avoid logging too frequently
train stats after 231360 examples: {'rewards_train/chosen': '0.17778', 'rewards_train/rejected': '0.11286', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06492', 'logps_train/rejected': '-107.97', 'logps_train/chosen': '-140.03', 'loss/train': '0.6701', 'examples_per_second': '31.57', 'grad_norm': '33.75', 'counters/examples': 231360, 'counters/updates': 7230}
train stats after 231392 examples: {'rewards_train/chosen': '0.086632', 'rewards_train/rejected': '-0.063929', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15056', 'logps_train/rejected': '-114.61', 'logps_train/chosen': '-124.4', 'loss/train': '0.63682', 'examples_per_second': '31.692', 'grad_norm': '22.5', 'counters/examples': 231392, 'counters/updates': 7231}
train stats after 231424 examples: {'rewards_train/chosen': '0.17514', 'rewards_train/rejected': '0.05566', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11948', 'logps_train/rejected': '-138.29', 'logps_train/chosen': '-115.76', 'loss/train': '0.65185', 'examples_per_second': '30.393', 'grad_norm': '24.125', 'counters/examples': 231424, 'counters/updates': 7232}
skipping logging after 231456 examples to avoid logging too frequently
train stats after 231488 examples: {'rewards_train/chosen': '0.15637', 'rewards_train/rejected': '0.031274', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1251', 'logps_train/rejected': '-115.75', 'logps_train/chosen': '-152.7', 'loss/train': '0.64461', 'examples_per_second': '33.519', 'grad_norm': '29.75', 'counters/examples': 231488, 'counters/updates': 7234}
train stats after 231520 examples: {'rewards_train/chosen': '0.17558', 'rewards_train/rejected': '0.099615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07596', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-139.02', 'loss/train': '0.66904', 'examples_per_second': '31.58', 'grad_norm': '34.25', 'counters/examples': 231520, 'counters/updates': 7235}
skipping logging after 231552 examples to avoid logging too frequently
train stats after 231584 examples: {'rewards_train/chosen': '0.19927', 'rewards_train/rejected': '0.077882', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12138', 'logps_train/rejected': '-149.95', 'logps_train/chosen': '-151.35', 'loss/train': '0.66018', 'examples_per_second': '30.878', 'grad_norm': '28.375', 'counters/examples': 231584, 'counters/updates': 7237}
train stats after 231616 examples: {'rewards_train/chosen': '0.11927', 'rewards_train/rejected': '-0.026424', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14569', 'logps_train/rejected': '-98.453', 'logps_train/chosen': '-130.81', 'loss/train': '0.63502', 'examples_per_second': '32.492', 'grad_norm': '24.5', 'counters/examples': 231616, 'counters/updates': 7238}
train stats after 231648 examples: {'rewards_train/chosen': '0.15133', 'rewards_train/rejected': '0.037449', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11388', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-157.01', 'loss/train': '0.64793', 'examples_per_second': '32.742', 'grad_norm': '28.5', 'counters/examples': 231648, 'counters/updates': 7239}
train stats after 231680 examples: {'rewards_train/chosen': '0.25468', 'rewards_train/rejected': '0.052221', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20246', 'logps_train/rejected': '-118.81', 'logps_train/chosen': '-148.82', 'loss/train': '0.63458', 'examples_per_second': '30.141', 'grad_norm': '42.25', 'counters/examples': 231680, 'counters/updates': 7240}
skipping logging after 231712 examples to avoid logging too frequently
train stats after 231744 examples: {'rewards_train/chosen': '0.20357', 'rewards_train/rejected': '-0.021641', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22521', 'logps_train/rejected': '-96.954', 'logps_train/chosen': '-152.34', 'loss/train': '0.60638', 'examples_per_second': '31.619', 'grad_norm': '22', 'counters/examples': 231744, 'counters/updates': 7242}
train stats after 231776 examples: {'rewards_train/chosen': '0.08883', 'rewards_train/rejected': '0.02297', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065859', 'logps_train/rejected': '-127.62', 'logps_train/chosen': '-126.18', 'loss/train': '0.67478', 'examples_per_second': '31.501', 'grad_norm': '27.75', 'counters/examples': 231776, 'counters/updates': 7243}
train stats after 231808 examples: {'rewards_train/chosen': '0.16319', 'rewards_train/rejected': '-0.00065101', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16384', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-122.95', 'loss/train': '0.6287', 'examples_per_second': '30.169', 'grad_norm': '21.75', 'counters/examples': 231808, 'counters/updates': 7244}
train stats after 231840 examples: {'rewards_train/chosen': '0.011818', 'rewards_train/rejected': '0.021402', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0095841', 'logps_train/rejected': '-158.67', 'logps_train/chosen': '-152.34', 'loss/train': '0.71287', 'examples_per_second': '32.057', 'grad_norm': '33.25', 'counters/examples': 231840, 'counters/updates': 7245}
train stats after 231872 examples: {'rewards_train/chosen': '0.20525', 'rewards_train/rejected': '0.043773', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16148', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-154.33', 'loss/train': '0.63373', 'examples_per_second': '30.563', 'grad_norm': '26.125', 'counters/examples': 231872, 'counters/updates': 7246}
skipping logging after 231904 examples to avoid logging too frequently
train stats after 231936 examples: {'rewards_train/chosen': '0.14195', 'rewards_train/rejected': '-0.032193', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17414', 'logps_train/rejected': '-85.292', 'logps_train/chosen': '-137.44', 'loss/train': '0.62004', 'examples_per_second': '34.484', 'grad_norm': '23.75', 'counters/examples': 231936, 'counters/updates': 7248}
train stats after 231968 examples: {'rewards_train/chosen': '0.21923', 'rewards_train/rejected': '0.017351', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.20188', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-139.93', 'loss/train': '0.61346', 'examples_per_second': '30.661', 'grad_norm': '25.5', 'counters/examples': 231968, 'counters/updates': 7249}
train stats after 232000 examples: {'rewards_train/chosen': '0.1382', 'rewards_train/rejected': '0.076576', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.061629', 'logps_train/rejected': '-157.41', 'logps_train/chosen': '-121.97', 'loss/train': '0.67954', 'examples_per_second': '31.319', 'grad_norm': '28.375', 'counters/examples': 232000, 'counters/updates': 7250}
skipping logging after 232032 examples to avoid logging too frequently
train stats after 232064 examples: {'rewards_train/chosen': '0.10188', 'rewards_train/rejected': '-0.059145', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16103', 'logps_train/rejected': '-104.59', 'logps_train/chosen': '-131', 'loss/train': '0.63536', 'examples_per_second': '31.822', 'grad_norm': '22.75', 'counters/examples': 232064, 'counters/updates': 7252}
train stats after 232096 examples: {'rewards_train/chosen': '0.082913', 'rewards_train/rejected': '0.022086', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060828', 'logps_train/rejected': '-111.96', 'logps_train/chosen': '-155.01', 'loss/train': '0.68168', 'examples_per_second': '32.921', 'grad_norm': '27.75', 'counters/examples': 232096, 'counters/updates': 7253}
train stats after 232128 examples: {'rewards_train/chosen': '0.11518', 'rewards_train/rejected': '0.0042739', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1109', 'logps_train/rejected': '-92.406', 'logps_train/chosen': '-133.04', 'loss/train': '0.64864', 'examples_per_second': '31.746', 'grad_norm': '23.625', 'counters/examples': 232128, 'counters/updates': 7254}
train stats after 232160 examples: {'rewards_train/chosen': '0.11869', 'rewards_train/rejected': '0.042444', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076244', 'logps_train/rejected': '-97.86', 'logps_train/chosen': '-114.21', 'loss/train': '0.67406', 'examples_per_second': '31.243', 'grad_norm': '22.125', 'counters/examples': 232160, 'counters/updates': 7255}
train stats after 232192 examples: {'rewards_train/chosen': '0.029478', 'rewards_train/rejected': '-0.084934', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11441', 'logps_train/rejected': '-118.37', 'logps_train/chosen': '-116.74', 'loss/train': '0.65231', 'examples_per_second': '30.657', 'grad_norm': '22', 'counters/examples': 232192, 'counters/updates': 7256}
skipping logging after 232224 examples to avoid logging too frequently
train stats after 232256 examples: {'rewards_train/chosen': '0.085772', 'rewards_train/rejected': '0.0081427', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077629', 'logps_train/rejected': '-110.34', 'logps_train/chosen': '-135.95', 'loss/train': '0.68122', 'examples_per_second': '32.005', 'grad_norm': '24.5', 'counters/examples': 232256, 'counters/updates': 7258}
train stats after 232288 examples: {'rewards_train/chosen': '0.11775', 'rewards_train/rejected': '0.071962', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045786', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-119.52', 'loss/train': '0.67855', 'examples_per_second': '31.633', 'grad_norm': '25.375', 'counters/examples': 232288, 'counters/updates': 7259}
train stats after 232320 examples: {'rewards_train/chosen': '0.19994', 'rewards_train/rejected': '-0.017632', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21757', 'logps_train/rejected': '-155.7', 'logps_train/chosen': '-161.88', 'loss/train': '0.60919', 'examples_per_second': '30.668', 'grad_norm': '27.75', 'counters/examples': 232320, 'counters/updates': 7260}
skipping logging after 232352 examples to avoid logging too frequently
train stats after 232384 examples: {'rewards_train/chosen': '0.10656', 'rewards_train/rejected': '0.033543', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073013', 'logps_train/rejected': '-102.05', 'logps_train/chosen': '-160.74', 'loss/train': '0.66785', 'examples_per_second': '30.634', 'grad_norm': '26', 'counters/examples': 232384, 'counters/updates': 7262}
train stats after 232416 examples: {'rewards_train/chosen': '0.10877', 'rewards_train/rejected': '0.072463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03631', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-140.79', 'loss/train': '0.6883', 'examples_per_second': '31.671', 'grad_norm': '26.5', 'counters/examples': 232416, 'counters/updates': 7263}
train stats after 232448 examples: {'rewards_train/chosen': '0.11924', 'rewards_train/rejected': '0.022263', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096981', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-147.82', 'loss/train': '0.65482', 'examples_per_second': '32.174', 'grad_norm': '29', 'counters/examples': 232448, 'counters/updates': 7264}
train stats after 232480 examples: {'rewards_train/chosen': '0.25185', 'rewards_train/rejected': '0.091297', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16055', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-142.96', 'loss/train': '0.62618', 'examples_per_second': '32.122', 'grad_norm': '29.625', 'counters/examples': 232480, 'counters/updates': 7265}
train stats after 232512 examples: {'rewards_train/chosen': '0.22351', 'rewards_train/rejected': '-0.032234', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25574', 'logps_train/rejected': '-138.21', 'logps_train/chosen': '-164.82', 'loss/train': '0.58926', 'examples_per_second': '30.399', 'grad_norm': '28.125', 'counters/examples': 232512, 'counters/updates': 7266}
train stats after 232544 examples: {'rewards_train/chosen': '0.067411', 'rewards_train/rejected': '-0.067311', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13472', 'logps_train/rejected': '-142.58', 'logps_train/chosen': '-135.39', 'loss/train': '0.63903', 'examples_per_second': '30.146', 'grad_norm': '29.75', 'counters/examples': 232544, 'counters/updates': 7267}
train stats after 232576 examples: {'rewards_train/chosen': '0.063257', 'rewards_train/rejected': '-0.043469', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10673', 'logps_train/rejected': '-95.74', 'logps_train/chosen': '-130.59', 'loss/train': '0.65702', 'examples_per_second': '32.291', 'grad_norm': '23.375', 'counters/examples': 232576, 'counters/updates': 7268}
train stats after 232608 examples: {'rewards_train/chosen': '0.22347', 'rewards_train/rejected': '-0.033733', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25721', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-135.47', 'loss/train': '0.59316', 'examples_per_second': '31.628', 'grad_norm': '23', 'counters/examples': 232608, 'counters/updates': 7269}
train stats after 232640 examples: {'rewards_train/chosen': '0.17563', 'rewards_train/rejected': '0.028959', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14667', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-134.13', 'loss/train': '0.6273', 'examples_per_second': '32.952', 'grad_norm': '24.625', 'counters/examples': 232640, 'counters/updates': 7270}
train stats after 232672 examples: {'rewards_train/chosen': '0.14909', 'rewards_train/rejected': '-0.046355', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19544', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-159.58', 'loss/train': '0.6149', 'examples_per_second': '30.572', 'grad_norm': '25.875', 'counters/examples': 232672, 'counters/updates': 7271}
train stats after 232704 examples: {'rewards_train/chosen': '0.10441', 'rewards_train/rejected': '-0.018317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12273', 'logps_train/rejected': '-127.22', 'logps_train/chosen': '-156.09', 'loss/train': '0.64604', 'examples_per_second': '32.944', 'grad_norm': '30.125', 'counters/examples': 232704, 'counters/updates': 7272}
train stats after 232736 examples: {'rewards_train/chosen': '0.20935', 'rewards_train/rejected': '0.052401', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15695', 'logps_train/rejected': '-185.53', 'logps_train/chosen': '-165.53', 'loss/train': '0.63074', 'examples_per_second': '31.397', 'grad_norm': '34.75', 'counters/examples': 232736, 'counters/updates': 7273}
train stats after 232768 examples: {'rewards_train/chosen': '0.088905', 'rewards_train/rejected': '0.0089439', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079961', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-125.32', 'loss/train': '0.66957', 'examples_per_second': '32.718', 'grad_norm': '26.125', 'counters/examples': 232768, 'counters/updates': 7274}
train stats after 232800 examples: {'rewards_train/chosen': '0.21979', 'rewards_train/rejected': '-0.03394', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25373', 'logps_train/rejected': '-88.876', 'logps_train/chosen': '-112.23', 'loss/train': '0.58564', 'examples_per_second': '31.618', 'grad_norm': '20', 'counters/examples': 232800, 'counters/updates': 7275}
train stats after 232832 examples: {'rewards_train/chosen': '0.13443', 'rewards_train/rejected': '0.001778', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13266', 'logps_train/rejected': '-141.84', 'logps_train/chosen': '-180.4', 'loss/train': '0.6416', 'examples_per_second': '30.278', 'grad_norm': '27.5', 'counters/examples': 232832, 'counters/updates': 7276}
skipping logging after 232864 examples to avoid logging too frequently
train stats after 232896 examples: {'rewards_train/chosen': '0.15098', 'rewards_train/rejected': '0.027534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12345', 'logps_train/rejected': '-120.47', 'logps_train/chosen': '-136.69', 'loss/train': '0.65327', 'examples_per_second': '31.639', 'grad_norm': '24', 'counters/examples': 232896, 'counters/updates': 7278}
train stats after 232928 examples: {'rewards_train/chosen': '0.15767', 'rewards_train/rejected': '-0.0066792', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16435', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-139.82', 'loss/train': '0.62919', 'examples_per_second': '31.608', 'grad_norm': '23.875', 'counters/examples': 232928, 'counters/updates': 7279}
train stats after 232960 examples: {'rewards_train/chosen': '0.11163', 'rewards_train/rejected': '0.025788', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085839', 'logps_train/rejected': '-110.73', 'logps_train/chosen': '-128.22', 'loss/train': '0.66472', 'examples_per_second': '32.873', 'grad_norm': '23.125', 'counters/examples': 232960, 'counters/updates': 7280}
train stats after 232992 examples: {'rewards_train/chosen': '0.17373', 'rewards_train/rejected': '-0.0037157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17744', 'logps_train/rejected': '-118.46', 'logps_train/chosen': '-138.05', 'loss/train': '0.63017', 'examples_per_second': '31.631', 'grad_norm': '23', 'counters/examples': 232992, 'counters/updates': 7281}
train stats after 233024 examples: {'rewards_train/chosen': '0.24531', 'rewards_train/rejected': '-0.040851', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.28616', 'logps_train/rejected': '-115.36', 'logps_train/chosen': '-138.09', 'loss/train': '0.59245', 'examples_per_second': '31.447', 'grad_norm': '24.375', 'counters/examples': 233024, 'counters/updates': 7282}
train stats after 233056 examples: {'rewards_train/chosen': '0.16896', 'rewards_train/rejected': '0.11048', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05848', 'logps_train/rejected': '-133.27', 'logps_train/chosen': '-149.33', 'loss/train': '0.68119', 'examples_per_second': '31.938', 'grad_norm': '26.125', 'counters/examples': 233056, 'counters/updates': 7283}
train stats after 233088 examples: {'rewards_train/chosen': '0.1509', 'rewards_train/rejected': '0.0077542', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14314', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-138.54', 'loss/train': '0.63644', 'examples_per_second': '31.938', 'grad_norm': '24.625', 'counters/examples': 233088, 'counters/updates': 7284}
train stats after 233120 examples: {'rewards_train/chosen': '0.11277', 'rewards_train/rejected': '0.010241', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10253', 'logps_train/rejected': '-121.32', 'logps_train/chosen': '-102.78', 'loss/train': '0.65098', 'examples_per_second': '31.948', 'grad_norm': '25.875', 'counters/examples': 233120, 'counters/updates': 7285}
train stats after 233152 examples: {'rewards_train/chosen': '0.15442', 'rewards_train/rejected': '-0.031365', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18578', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-140.12', 'loss/train': '0.61224', 'examples_per_second': '30.188', 'grad_norm': '22', 'counters/examples': 233152, 'counters/updates': 7286}
train stats after 233184 examples: {'rewards_train/chosen': '0.13517', 'rewards_train/rejected': '0.04412', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091052', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-135.03', 'loss/train': '0.66486', 'examples_per_second': '32.053', 'grad_norm': '26.875', 'counters/examples': 233184, 'counters/updates': 7287}
train stats after 233216 examples: {'rewards_train/chosen': '0.14357', 'rewards_train/rejected': '-0.04845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19202', 'logps_train/rejected': '-131.98', 'logps_train/chosen': '-173.67', 'loss/train': '0.62428', 'examples_per_second': '32.172', 'grad_norm': '25', 'counters/examples': 233216, 'counters/updates': 7288}
train stats after 233248 examples: {'rewards_train/chosen': '0.089866', 'rewards_train/rejected': '0.015978', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073887', 'logps_train/rejected': '-99.228', 'logps_train/chosen': '-115.49', 'loss/train': '0.66702', 'examples_per_second': '31.425', 'grad_norm': '23.125', 'counters/examples': 233248, 'counters/updates': 7289}
train stats after 233280 examples: {'rewards_train/chosen': '0.19996', 'rewards_train/rejected': '0.062079', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13788', 'logps_train/rejected': '-111.39', 'logps_train/chosen': '-152.99', 'loss/train': '0.6402', 'examples_per_second': '31.705', 'grad_norm': '27.25', 'counters/examples': 233280, 'counters/updates': 7290}
train stats after 233312 examples: {'rewards_train/chosen': '0.12644', 'rewards_train/rejected': '-0.0030245', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12946', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-117.8', 'loss/train': '0.64187', 'examples_per_second': '31.293', 'grad_norm': '24.625', 'counters/examples': 233312, 'counters/updates': 7291}
train stats after 233344 examples: {'rewards_train/chosen': '0.15666', 'rewards_train/rejected': '0.027909', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12875', 'logps_train/rejected': '-156.48', 'logps_train/chosen': '-134.85', 'loss/train': '0.64196', 'examples_per_second': '31.526', 'grad_norm': '29.125', 'counters/examples': 233344, 'counters/updates': 7292}
train stats after 233376 examples: {'rewards_train/chosen': '0.17539', 'rewards_train/rejected': '0.031609', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14379', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-148.37', 'loss/train': '0.64163', 'examples_per_second': '31.693', 'grad_norm': '24.125', 'counters/examples': 233376, 'counters/updates': 7293}
train stats after 233408 examples: {'rewards_train/chosen': '0.1096', 'rewards_train/rejected': '0.057716', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051882', 'logps_train/rejected': '-134.14', 'logps_train/chosen': '-147.44', 'loss/train': '0.67832', 'examples_per_second': '31.642', 'grad_norm': '26.5', 'counters/examples': 233408, 'counters/updates': 7294}
train stats after 233440 examples: {'rewards_train/chosen': '0.21125', 'rewards_train/rejected': '0.092035', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11921', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-154.39', 'loss/train': '0.64553', 'examples_per_second': '30.767', 'grad_norm': '26', 'counters/examples': 233440, 'counters/updates': 7295}
train stats after 233472 examples: {'rewards_train/chosen': '0.12914', 'rewards_train/rejected': '0.00061518', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12853', 'logps_train/rejected': '-81.883', 'logps_train/chosen': '-129.94', 'loss/train': '0.64424', 'examples_per_second': '32.947', 'grad_norm': '23.75', 'counters/examples': 233472, 'counters/updates': 7296}
train stats after 233504 examples: {'rewards_train/chosen': '0.16851', 'rewards_train/rejected': '0.010516', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15799', 'logps_train/rejected': '-152.49', 'logps_train/chosen': '-117.73', 'loss/train': '0.637', 'examples_per_second': '31.628', 'grad_norm': '26.875', 'counters/examples': 233504, 'counters/updates': 7297}
skipping logging after 233536 examples to avoid logging too frequently
train stats after 233568 examples: {'rewards_train/chosen': '0.11073', 'rewards_train/rejected': '0.051819', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058912', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-119.32', 'loss/train': '0.67414', 'examples_per_second': '33.882', 'grad_norm': '25.625', 'counters/examples': 233568, 'counters/updates': 7299}
train stats after 233600 examples: {'rewards_train/chosen': '0.18258', 'rewards_train/rejected': '-0.048775', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23136', 'logps_train/rejected': '-145.88', 'logps_train/chosen': '-138.45', 'loss/train': '0.60071', 'examples_per_second': '32.469', 'grad_norm': '28.125', 'counters/examples': 233600, 'counters/updates': 7300}
train stats after 233632 examples: {'rewards_train/chosen': '0.20365', 'rewards_train/rejected': '0.0048015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19885', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-122.25', 'loss/train': '0.61064', 'examples_per_second': '30.457', 'grad_norm': '21.75', 'counters/examples': 233632, 'counters/updates': 7301}
train stats after 233664 examples: {'rewards_train/chosen': '0.11534', 'rewards_train/rejected': '-0.081226', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19657', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-138.81', 'loss/train': '0.60994', 'examples_per_second': '32.193', 'grad_norm': '25.75', 'counters/examples': 233664, 'counters/updates': 7302}
train stats after 233696 examples: {'rewards_train/chosen': '0.18939', 'rewards_train/rejected': '0.0067185', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18267', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-135.62', 'loss/train': '0.62239', 'examples_per_second': '30.066', 'grad_norm': '26.125', 'counters/examples': 233696, 'counters/updates': 7303}
train stats after 233728 examples: {'rewards_train/chosen': '0.10052', 'rewards_train/rejected': '0.12516', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.024636', 'logps_train/rejected': '-164.27', 'logps_train/chosen': '-165.73', 'loss/train': '0.72259', 'examples_per_second': '31.536', 'grad_norm': '32.75', 'counters/examples': 233728, 'counters/updates': 7304}
skipping logging after 233760 examples to avoid logging too frequently
train stats after 233792 examples: {'rewards_train/chosen': '0.1928', 'rewards_train/rejected': '0.049995', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1428', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-153.33', 'loss/train': '0.64138', 'examples_per_second': '31.314', 'grad_norm': '25.375', 'counters/examples': 233792, 'counters/updates': 7306}
train stats after 233824 examples: {'rewards_train/chosen': '0.18745', 'rewards_train/rejected': '0.035924', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15153', 'logps_train/rejected': '-150.01', 'logps_train/chosen': '-156.22', 'loss/train': '0.63073', 'examples_per_second': '30.472', 'grad_norm': '27.5', 'counters/examples': 233824, 'counters/updates': 7307}
train stats after 233856 examples: {'rewards_train/chosen': '0.22068', 'rewards_train/rejected': '0.092211', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12847', 'logps_train/rejected': '-106.96', 'logps_train/chosen': '-139.38', 'loss/train': '0.63685', 'examples_per_second': '31.142', 'grad_norm': '24.5', 'counters/examples': 233856, 'counters/updates': 7308}
train stats after 233888 examples: {'rewards_train/chosen': '0.17862', 'rewards_train/rejected': '-0.034678', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21329', 'logps_train/rejected': '-109.76', 'logps_train/chosen': '-139.27', 'loss/train': '0.60346', 'examples_per_second': '32.947', 'grad_norm': '27.625', 'counters/examples': 233888, 'counters/updates': 7309}
train stats after 233920 examples: {'rewards_train/chosen': '0.17868', 'rewards_train/rejected': '0.14786', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030818', 'logps_train/rejected': '-162.91', 'logps_train/chosen': '-182.35', 'loss/train': '0.70115', 'examples_per_second': '33.061', 'grad_norm': '31.875', 'counters/examples': 233920, 'counters/updates': 7310}
train stats after 233952 examples: {'rewards_train/chosen': '0.17872', 'rewards_train/rejected': '0.020199', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15853', 'logps_train/rejected': '-142.64', 'logps_train/chosen': '-141.92', 'loss/train': '0.62626', 'examples_per_second': '31.651', 'grad_norm': '29.75', 'counters/examples': 233952, 'counters/updates': 7311}
train stats after 233984 examples: {'rewards_train/chosen': '0.16166', 'rewards_train/rejected': '-0.033028', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19469', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-146.53', 'loss/train': '0.61766', 'examples_per_second': '32.382', 'grad_norm': '27.875', 'counters/examples': 233984, 'counters/updates': 7312}
train stats after 234016 examples: {'rewards_train/chosen': '0.17437', 'rewards_train/rejected': '-0.0077749', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18214', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-176.02', 'loss/train': '0.61941', 'examples_per_second': '31.146', 'grad_norm': '26.375', 'counters/examples': 234016, 'counters/updates': 7313}
train stats after 234048 examples: {'rewards_train/chosen': '0.15908', 'rewards_train/rejected': '-0.0041718', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16325', 'logps_train/rejected': '-158.84', 'logps_train/chosen': '-151.61', 'loss/train': '0.62974', 'examples_per_second': '31.751', 'grad_norm': '26.75', 'counters/examples': 234048, 'counters/updates': 7314}
train stats after 234080 examples: {'rewards_train/chosen': '0.17409', 'rewards_train/rejected': '0.049714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12438', 'logps_train/rejected': '-147.44', 'logps_train/chosen': '-176.82', 'loss/train': '0.64771', 'examples_per_second': '30.077', 'grad_norm': '28.5', 'counters/examples': 234080, 'counters/updates': 7315}
train stats after 234112 examples: {'rewards_train/chosen': '0.16762', 'rewards_train/rejected': '0.029358', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13826', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-168.84', 'loss/train': '0.64716', 'examples_per_second': '30.601', 'grad_norm': '27.75', 'counters/examples': 234112, 'counters/updates': 7316}
train stats after 234144 examples: {'rewards_train/chosen': '0.15076', 'rewards_train/rejected': '-0.060942', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2117', 'logps_train/rejected': '-99.751', 'logps_train/chosen': '-152.39', 'loss/train': '0.60942', 'examples_per_second': '31.731', 'grad_norm': '22.25', 'counters/examples': 234144, 'counters/updates': 7317}
train stats after 234176 examples: {'rewards_train/chosen': '0.17824', 'rewards_train/rejected': '0.056832', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12141', 'logps_train/rejected': '-133.09', 'logps_train/chosen': '-147.74', 'loss/train': '0.64664', 'examples_per_second': '32.064', 'grad_norm': '29', 'counters/examples': 234176, 'counters/updates': 7318}
train stats after 234208 examples: {'rewards_train/chosen': '0.13325', 'rewards_train/rejected': '-0.0020146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13526', 'logps_train/rejected': '-122.15', 'logps_train/chosen': '-121.77', 'loss/train': '0.64549', 'examples_per_second': '30.897', 'grad_norm': '25.75', 'counters/examples': 234208, 'counters/updates': 7319}
skipping logging after 234240 examples to avoid logging too frequently
train stats after 234272 examples: {'rewards_train/chosen': '0.093652', 'rewards_train/rejected': '0.065655', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027997', 'logps_train/rejected': '-117.19', 'logps_train/chosen': '-112.38', 'loss/train': '0.68374', 'examples_per_second': '31.626', 'grad_norm': '24.625', 'counters/examples': 234272, 'counters/updates': 7321}
skipping logging after 234304 examples to avoid logging too frequently
train stats after 234336 examples: {'rewards_train/chosen': '0.14953', 'rewards_train/rejected': '-0.07328', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22281', 'logps_train/rejected': '-101.96', 'logps_train/chosen': '-153.27', 'loss/train': '0.59619', 'examples_per_second': '25.277', 'grad_norm': '22.875', 'counters/examples': 234336, 'counters/updates': 7323}
train stats after 234368 examples: {'rewards_train/chosen': '0.12295', 'rewards_train/rejected': '0.062408', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060539', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-156.09', 'loss/train': '0.67011', 'examples_per_second': '30.672', 'grad_norm': '29.375', 'counters/examples': 234368, 'counters/updates': 7324}
skipping logging after 234400 examples to avoid logging too frequently
train stats after 234432 examples: {'rewards_train/chosen': '0.11285', 'rewards_train/rejected': '0.071122', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041729', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-138.92', 'loss/train': '0.68299', 'examples_per_second': '36.194', 'grad_norm': '29', 'counters/examples': 234432, 'counters/updates': 7326}
train stats after 234464 examples: {'rewards_train/chosen': '0.16394', 'rewards_train/rejected': '-0.018601', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18254', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-129.53', 'loss/train': '0.62443', 'examples_per_second': '31.623', 'grad_norm': '25.25', 'counters/examples': 234464, 'counters/updates': 7327}
train stats after 234496 examples: {'rewards_train/chosen': '0.15867', 'rewards_train/rejected': '-0.053752', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21242', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-166.71', 'loss/train': '0.60643', 'examples_per_second': '31.581', 'grad_norm': '24.25', 'counters/examples': 234496, 'counters/updates': 7328}
train stats after 234528 examples: {'rewards_train/chosen': '0.13665', 'rewards_train/rejected': '0.062929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07372', 'logps_train/rejected': '-167.98', 'logps_train/chosen': '-162.93', 'loss/train': '0.66776', 'examples_per_second': '30.158', 'grad_norm': '28.375', 'counters/examples': 234528, 'counters/updates': 7329}
train stats after 234560 examples: {'rewards_train/chosen': '0.011681', 'rewards_train/rejected': '0.0010047', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010676', 'logps_train/rejected': '-156.04', 'logps_train/chosen': '-141.94', 'loss/train': '0.70527', 'examples_per_second': '31.402', 'grad_norm': '30.125', 'counters/examples': 234560, 'counters/updates': 7330}
train stats after 234592 examples: {'rewards_train/chosen': '0.16144', 'rewards_train/rejected': '0.097265', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064175', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-146.81', 'loss/train': '0.68416', 'examples_per_second': '30.536', 'grad_norm': '31.25', 'counters/examples': 234592, 'counters/updates': 7331}
train stats after 234624 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '0.025126', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097886', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-133.23', 'loss/train': '0.67064', 'examples_per_second': '31.583', 'grad_norm': '26.75', 'counters/examples': 234624, 'counters/updates': 7332}
train stats after 234656 examples: {'rewards_train/chosen': '0.08196', 'rewards_train/rejected': '0.042974', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038986', 'logps_train/rejected': '-135.13', 'logps_train/chosen': '-156.81', 'loss/train': '0.6898', 'examples_per_second': '30.962', 'grad_norm': '27.625', 'counters/examples': 234656, 'counters/updates': 7333}
train stats after 234688 examples: {'rewards_train/chosen': '0.19873', 'rewards_train/rejected': '0.10473', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093999', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-176.16', 'loss/train': '0.66375', 'examples_per_second': '31.36', 'grad_norm': '28.75', 'counters/examples': 234688, 'counters/updates': 7334}
train stats after 234720 examples: {'rewards_train/chosen': '0.0022495', 'rewards_train/rejected': '-0.12507', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12732', 'logps_train/rejected': '-106.78', 'logps_train/chosen': '-135.15', 'loss/train': '0.64596', 'examples_per_second': '30.556', 'grad_norm': '28', 'counters/examples': 234720, 'counters/updates': 7335}
skipping logging after 234752 examples to avoid logging too frequently
train stats after 234784 examples: {'rewards_train/chosen': '0.16814', 'rewards_train/rejected': '0.049488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11865', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-163.56', 'loss/train': '0.64479', 'examples_per_second': '31.001', 'grad_norm': '24.875', 'counters/examples': 234784, 'counters/updates': 7337}
train stats after 234816 examples: {'rewards_train/chosen': '0.14211', 'rewards_train/rejected': '0.085298', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056808', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-143.53', 'loss/train': '0.6858', 'examples_per_second': '30.288', 'grad_norm': '26.625', 'counters/examples': 234816, 'counters/updates': 7338}
train stats after 234848 examples: {'rewards_train/chosen': '0.052157', 'rewards_train/rejected': '-0.077077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12923', 'logps_train/rejected': '-101', 'logps_train/chosen': '-116.87', 'loss/train': '0.64242', 'examples_per_second': '31.654', 'grad_norm': '21.25', 'counters/examples': 234848, 'counters/updates': 7339}
train stats after 234880 examples: {'rewards_train/chosen': '0.14941', 'rewards_train/rejected': '0.049297', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10011', 'logps_train/rejected': '-139.94', 'logps_train/chosen': '-154.46', 'loss/train': '0.66533', 'examples_per_second': '30.636', 'grad_norm': '28', 'counters/examples': 234880, 'counters/updates': 7340}
train stats after 234912 examples: {'rewards_train/chosen': '0.15752', 'rewards_train/rejected': '-0.044655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20218', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-149.03', 'loss/train': '0.61789', 'examples_per_second': '31.355', 'grad_norm': '29.375', 'counters/examples': 234912, 'counters/updates': 7341}
train stats after 234944 examples: {'rewards_train/chosen': '0.13558', 'rewards_train/rejected': '0.079757', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055825', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-152.35', 'loss/train': '0.67644', 'examples_per_second': '31.641', 'grad_norm': '29.25', 'counters/examples': 234944, 'counters/updates': 7342}
train stats after 234976 examples: {'rewards_train/chosen': '0.19961', 'rewards_train/rejected': '0.0036655', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19595', 'logps_train/rejected': '-99.131', 'logps_train/chosen': '-127.31', 'loss/train': '0.61555', 'examples_per_second': '31.617', 'grad_norm': '22', 'counters/examples': 234976, 'counters/updates': 7343}
train stats after 235008 examples: {'rewards_train/chosen': '0.1393', 'rewards_train/rejected': '0.046759', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092541', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-136.94', 'loss/train': '0.66512', 'examples_per_second': '32.418', 'grad_norm': '25.625', 'counters/examples': 235008, 'counters/updates': 7344}
train stats after 235040 examples: {'rewards_train/chosen': '0.12218', 'rewards_train/rejected': '-0.021196', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14338', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-140.99', 'loss/train': '0.64276', 'examples_per_second': '32.306', 'grad_norm': '34.5', 'counters/examples': 235040, 'counters/updates': 7345}
skipping logging after 235072 examples to avoid logging too frequently
train stats after 235104 examples: {'rewards_train/chosen': '0.062061', 'rewards_train/rejected': '-0.085959', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14802', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-136.13', 'loss/train': '0.63088', 'examples_per_second': '31.292', 'grad_norm': '24.375', 'counters/examples': 235104, 'counters/updates': 7347}
skipping logging after 235136 examples to avoid logging too frequently
train stats after 235168 examples: {'rewards_train/chosen': '0.17155', 'rewards_train/rejected': '0.12251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049041', 'logps_train/rejected': '-125.76', 'logps_train/chosen': '-167.2', 'loss/train': '0.6792', 'examples_per_second': '23.504', 'grad_norm': '28.125', 'counters/examples': 235168, 'counters/updates': 7349}
train stats after 235200 examples: {'rewards_train/chosen': '0.13742', 'rewards_train/rejected': '-0.040783', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17821', 'logps_train/rejected': '-122.84', 'logps_train/chosen': '-133.98', 'loss/train': '0.62564', 'examples_per_second': '31.623', 'grad_norm': '26.375', 'counters/examples': 235200, 'counters/updates': 7350}
train stats after 235232 examples: {'rewards_train/chosen': '0.15101', 'rewards_train/rejected': '0.086597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064417', 'logps_train/rejected': '-144.44', 'logps_train/chosen': '-172.79', 'loss/train': '0.69015', 'examples_per_second': '32.16', 'grad_norm': '33.25', 'counters/examples': 235232, 'counters/updates': 7351}
train stats after 235264 examples: {'rewards_train/chosen': '0.16479', 'rewards_train/rejected': '0.059509', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10528', 'logps_train/rejected': '-121.97', 'logps_train/chosen': '-120.29', 'loss/train': '0.6586', 'examples_per_second': '23.023', 'grad_norm': '24.625', 'counters/examples': 235264, 'counters/updates': 7352}
skipping logging after 235296 examples to avoid logging too frequently
train stats after 235328 examples: {'rewards_train/chosen': '0.14627', 'rewards_train/rejected': '-0.086263', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23253', 'logps_train/rejected': '-114.24', 'logps_train/chosen': '-160.65', 'loss/train': '0.59772', 'examples_per_second': '30.66', 'grad_norm': '24.25', 'counters/examples': 235328, 'counters/updates': 7354}
train stats after 235360 examples: {'rewards_train/chosen': '0.14113', 'rewards_train/rejected': '0.036325', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1048', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-104.92', 'loss/train': '0.65251', 'examples_per_second': '32.821', 'grad_norm': '21.125', 'counters/examples': 235360, 'counters/updates': 7355}
train stats after 235392 examples: {'rewards_train/chosen': '0.13814', 'rewards_train/rejected': '-0.016012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15415', 'logps_train/rejected': '-116.54', 'logps_train/chosen': '-104.65', 'loss/train': '0.63434', 'examples_per_second': '31.671', 'grad_norm': '23.375', 'counters/examples': 235392, 'counters/updates': 7356}
train stats after 235424 examples: {'rewards_train/chosen': '0.13861', 'rewards_train/rejected': '0.043268', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095341', 'logps_train/rejected': '-111.22', 'logps_train/chosen': '-144.39', 'loss/train': '0.65808', 'examples_per_second': '32.349', 'grad_norm': '25.375', 'counters/examples': 235424, 'counters/updates': 7357}
train stats after 235456 examples: {'rewards_train/chosen': '0.1594', 'rewards_train/rejected': '0.0033994', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.156', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-146.18', 'loss/train': '0.6313', 'examples_per_second': '31.64', 'grad_norm': '25.125', 'counters/examples': 235456, 'counters/updates': 7358}
train stats after 235488 examples: {'rewards_train/chosen': '0.20142', 'rewards_train/rejected': '0.0086985', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19273', 'logps_train/rejected': '-128.43', 'logps_train/chosen': '-151.51', 'loss/train': '0.61474', 'examples_per_second': '31.312', 'grad_norm': '24.875', 'counters/examples': 235488, 'counters/updates': 7359}
train stats after 235520 examples: {'rewards_train/chosen': '0.090729', 'rewards_train/rejected': '0.099739', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00901', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-140.68', 'loss/train': '0.7104', 'examples_per_second': '31.386', 'grad_norm': '30', 'counters/examples': 235520, 'counters/updates': 7360}
train stats after 235552 examples: {'rewards_train/chosen': '0.13481', 'rewards_train/rejected': '-0.021179', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15599', 'logps_train/rejected': '-88.751', 'logps_train/chosen': '-150.57', 'loss/train': '0.62977', 'examples_per_second': '30.57', 'grad_norm': '26', 'counters/examples': 235552, 'counters/updates': 7361}
skipping logging after 235584 examples to avoid logging too frequently
train stats after 235616 examples: {'rewards_train/chosen': '0.13599', 'rewards_train/rejected': '0.044094', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.091897', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-132.98', 'loss/train': '0.66444', 'examples_per_second': '30.936', 'grad_norm': '25.5', 'counters/examples': 235616, 'counters/updates': 7363}
train stats after 235648 examples: {'rewards_train/chosen': '0.07234', 'rewards_train/rejected': '0.0091234', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063216', 'logps_train/rejected': '-106.59', 'logps_train/chosen': '-116.59', 'loss/train': '0.67179', 'examples_per_second': '31.48', 'grad_norm': '24.125', 'counters/examples': 235648, 'counters/updates': 7364}
skipping logging after 235680 examples to avoid logging too frequently
train stats after 235712 examples: {'rewards_train/chosen': '0.15288', 'rewards_train/rejected': '0.036498', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11638', 'logps_train/rejected': '-153.37', 'logps_train/chosen': '-166.51', 'loss/train': '0.67289', 'examples_per_second': '30.678', 'grad_norm': '27.875', 'counters/examples': 235712, 'counters/updates': 7366}
train stats after 235744 examples: {'rewards_train/chosen': '0.12466', 'rewards_train/rejected': '0.022293', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10237', 'logps_train/rejected': '-119.47', 'logps_train/chosen': '-123.63', 'loss/train': '0.65483', 'examples_per_second': '32.54', 'grad_norm': '24.5', 'counters/examples': 235744, 'counters/updates': 7367}
skipping logging after 235776 examples to avoid logging too frequently
train stats after 235808 examples: {'rewards_train/chosen': '0.054366', 'rewards_train/rejected': '-0.060497', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11486', 'logps_train/rejected': '-94.291', 'logps_train/chosen': '-133.7', 'loss/train': '0.64886', 'examples_per_second': '31.004', 'grad_norm': '21.75', 'counters/examples': 235808, 'counters/updates': 7369}
train stats after 235840 examples: {'rewards_train/chosen': '0.13554', 'rewards_train/rejected': '0.091994', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04355', 'logps_train/rejected': '-119.42', 'logps_train/chosen': '-160.56', 'loss/train': '0.68188', 'examples_per_second': '32.377', 'grad_norm': '25.75', 'counters/examples': 235840, 'counters/updates': 7370}
train stats after 235872 examples: {'rewards_train/chosen': '0.17443', 'rewards_train/rejected': '0.044521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12991', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-154.67', 'loss/train': '0.65454', 'examples_per_second': '31.967', 'grad_norm': '27.75', 'counters/examples': 235872, 'counters/updates': 7371}
skipping logging after 235904 examples to avoid logging too frequently
train stats after 235936 examples: {'rewards_train/chosen': '0.19474', 'rewards_train/rejected': '0.074502', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12024', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-162.73', 'loss/train': '0.65202', 'examples_per_second': '33.247', 'grad_norm': '26.75', 'counters/examples': 235936, 'counters/updates': 7373}
skipping logging after 235968 examples to avoid logging too frequently
train stats after 236000 examples: {'rewards_train/chosen': '0.040414', 'rewards_train/rejected': '0.028366', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012048', 'logps_train/rejected': '-87.572', 'logps_train/chosen': '-109.56', 'loss/train': '0.70462', 'examples_per_second': '30.404', 'grad_norm': '24.625', 'counters/examples': 236000, 'counters/updates': 7375}
skipping logging after 236032 examples to avoid logging too frequently
train stats after 236064 examples: {'rewards_train/chosen': '0.16421', 'rewards_train/rejected': '0.015616', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14859', 'logps_train/rejected': '-98.054', 'logps_train/chosen': '-148.43', 'loss/train': '0.63872', 'examples_per_second': '31.055', 'grad_norm': '25.75', 'counters/examples': 236064, 'counters/updates': 7377}
train stats after 236096 examples: {'rewards_train/chosen': '0.151', 'rewards_train/rejected': '-0.017028', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16802', 'logps_train/rejected': '-111.57', 'logps_train/chosen': '-126.51', 'loss/train': '0.63399', 'examples_per_second': '32.534', 'grad_norm': '24.375', 'counters/examples': 236096, 'counters/updates': 7378}
train stats after 236128 examples: {'rewards_train/chosen': '0.17022', 'rewards_train/rejected': '0.0069669', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16326', 'logps_train/rejected': '-113.34', 'logps_train/chosen': '-193.67', 'loss/train': '0.62629', 'examples_per_second': '31.536', 'grad_norm': '30.5', 'counters/examples': 236128, 'counters/updates': 7379}
skipping logging after 236160 examples to avoid logging too frequently
train stats after 236192 examples: {'rewards_train/chosen': '0.18596', 'rewards_train/rejected': '0.037023', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14894', 'logps_train/rejected': '-112.04', 'logps_train/chosen': '-158.43', 'loss/train': '0.63375', 'examples_per_second': '31.621', 'grad_norm': '25.125', 'counters/examples': 236192, 'counters/updates': 7381}
skipping logging after 236224 examples to avoid logging too frequently
train stats after 236256 examples: {'rewards_train/chosen': '0.15721', 'rewards_train/rejected': '0.13371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023503', 'logps_train/rejected': '-146.03', 'logps_train/chosen': '-128.5', 'loss/train': '0.69874', 'examples_per_second': '30.208', 'grad_norm': '29.375', 'counters/examples': 236256, 'counters/updates': 7383}
train stats after 236288 examples: {'rewards_train/chosen': '0.2042', 'rewards_train/rejected': '-0.074128', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27833', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-126.97', 'loss/train': '0.58213', 'examples_per_second': '30.618', 'grad_norm': '22.125', 'counters/examples': 236288, 'counters/updates': 7384}
train stats after 236320 examples: {'rewards_train/chosen': '0.064767', 'rewards_train/rejected': '0.013435', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051332', 'logps_train/rejected': '-109.31', 'logps_train/chosen': '-143.76', 'loss/train': '0.67625', 'examples_per_second': '30.621', 'grad_norm': '25.625', 'counters/examples': 236320, 'counters/updates': 7385}
train stats after 236352 examples: {'rewards_train/chosen': '0.05215', 'rewards_train/rejected': '0.045184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0069653', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-120.29', 'loss/train': '0.71232', 'examples_per_second': '32.691', 'grad_norm': '28', 'counters/examples': 236352, 'counters/updates': 7386}
train stats after 236384 examples: {'rewards_train/chosen': '0.094226', 'rewards_train/rejected': '-0.0089347', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10316', 'logps_train/rejected': '-148.95', 'logps_train/chosen': '-143.97', 'loss/train': '0.66331', 'examples_per_second': '31.5', 'grad_norm': '28.125', 'counters/examples': 236384, 'counters/updates': 7387}
train stats after 236416 examples: {'rewards_train/chosen': '0.19075', 'rewards_train/rejected': '0.02546', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16529', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-125.59', 'loss/train': '0.62642', 'examples_per_second': '32.041', 'grad_norm': '24', 'counters/examples': 236416, 'counters/updates': 7388}
train stats after 236448 examples: {'rewards_train/chosen': '0.11571', 'rewards_train/rejected': '0.017767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097942', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-160.5', 'loss/train': '0.65552', 'examples_per_second': '31.56', 'grad_norm': '26.375', 'counters/examples': 236448, 'counters/updates': 7389}
train stats after 236480 examples: {'rewards_train/chosen': '0.20273', 'rewards_train/rejected': '0.081549', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12118', 'logps_train/rejected': '-147.71', 'logps_train/chosen': '-157.14', 'loss/train': '0.65482', 'examples_per_second': '31.619', 'grad_norm': '27.75', 'counters/examples': 236480, 'counters/updates': 7390}
train stats after 236512 examples: {'rewards_train/chosen': '0.10026', 'rewards_train/rejected': '0.083116', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017147', 'logps_train/rejected': '-95.186', 'logps_train/chosen': '-151.07', 'loss/train': '0.69421', 'examples_per_second': '30.138', 'grad_norm': '24.75', 'counters/examples': 236512, 'counters/updates': 7391}
train stats after 236544 examples: {'rewards_train/chosen': '0.11314', 'rewards_train/rejected': '-0.00078132', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11392', 'logps_train/rejected': '-149.78', 'logps_train/chosen': '-165.25', 'loss/train': '0.65387', 'examples_per_second': '31.233', 'grad_norm': '28.125', 'counters/examples': 236544, 'counters/updates': 7392}
train stats after 236576 examples: {'rewards_train/chosen': '0.062896', 'rewards_train/rejected': '0.064079', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0011838', 'logps_train/rejected': '-110.06', 'logps_train/chosen': '-158.5', 'loss/train': '0.71141', 'examples_per_second': '30.38', 'grad_norm': '30.75', 'counters/examples': 236576, 'counters/updates': 7393}
train stats after 236608 examples: {'rewards_train/chosen': '0.18752', 'rewards_train/rejected': '0.013045', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17448', 'logps_train/rejected': '-151.77', 'logps_train/chosen': '-151.47', 'loss/train': '0.63022', 'examples_per_second': '31.452', 'grad_norm': '26.25', 'counters/examples': 236608, 'counters/updates': 7394}
train stats after 236640 examples: {'rewards_train/chosen': '0.17964', 'rewards_train/rejected': '-0.11768', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.29732', 'logps_train/rejected': '-133.47', 'logps_train/chosen': '-130.68', 'loss/train': '0.57857', 'examples_per_second': '32.499', 'grad_norm': '23.25', 'counters/examples': 236640, 'counters/updates': 7395}
skipping logging after 236672 examples to avoid logging too frequently
train stats after 236704 examples: {'rewards_train/chosen': '0.095369', 'rewards_train/rejected': '0.032249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06312', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-123.64', 'loss/train': '0.67488', 'examples_per_second': '31.542', 'grad_norm': '25.5', 'counters/examples': 236704, 'counters/updates': 7397}
skipping logging after 236736 examples to avoid logging too frequently
train stats after 236768 examples: {'rewards_train/chosen': '0.15739', 'rewards_train/rejected': '0.039421', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11797', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-144.52', 'loss/train': '0.64734', 'examples_per_second': '31.599', 'grad_norm': '23.875', 'counters/examples': 236768, 'counters/updates': 7399}
skipping logging after 236800 examples to avoid logging too frequently
train stats after 236832 examples: {'rewards_train/chosen': '0.050719', 'rewards_train/rejected': '0.053085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0023656', 'logps_train/rejected': '-118.35', 'logps_train/chosen': '-137.37', 'loss/train': '0.70369', 'examples_per_second': '31.494', 'grad_norm': '24.875', 'counters/examples': 236832, 'counters/updates': 7401}
train stats after 236864 examples: {'rewards_train/chosen': '0.12316', 'rewards_train/rejected': '0.010284', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11288', 'logps_train/rejected': '-85.564', 'logps_train/chosen': '-149.58', 'loss/train': '0.65399', 'examples_per_second': '32.357', 'grad_norm': '23', 'counters/examples': 236864, 'counters/updates': 7402}
skipping logging after 236896 examples to avoid logging too frequently
train stats after 236928 examples: {'rewards_train/chosen': '0.14292', 'rewards_train/rejected': '0.083878', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05904', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-133.36', 'loss/train': '0.68196', 'examples_per_second': '30.07', 'grad_norm': '27', 'counters/examples': 236928, 'counters/updates': 7404}
train stats after 236960 examples: {'rewards_train/chosen': '0.15635', 'rewards_train/rejected': '-0.010039', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16639', 'logps_train/rejected': '-110.79', 'logps_train/chosen': '-136.9', 'loss/train': '0.63337', 'examples_per_second': '30.576', 'grad_norm': '24.375', 'counters/examples': 236960, 'counters/updates': 7405}
train stats after 236992 examples: {'rewards_train/chosen': '0.040416', 'rewards_train/rejected': '0.049581', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0091655', 'logps_train/rejected': '-121', 'logps_train/chosen': '-136.81', 'loss/train': '0.71431', 'examples_per_second': '32.873', 'grad_norm': '29.125', 'counters/examples': 236992, 'counters/updates': 7406}
train stats after 237024 examples: {'rewards_train/chosen': '0.039404', 'rewards_train/rejected': '-0.030253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069657', 'logps_train/rejected': '-134.98', 'logps_train/chosen': '-178.21', 'loss/train': '0.67923', 'examples_per_second': '31.954', 'grad_norm': '39.75', 'counters/examples': 237024, 'counters/updates': 7407}
train stats after 237056 examples: {'rewards_train/chosen': '0.12798', 'rewards_train/rejected': '-0.051155', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17913', 'logps_train/rejected': '-141.69', 'logps_train/chosen': '-147.44', 'loss/train': '0.62652', 'examples_per_second': '32.128', 'grad_norm': '27.75', 'counters/examples': 237056, 'counters/updates': 7408}
train stats after 237088 examples: {'rewards_train/chosen': '0.085677', 'rewards_train/rejected': '0.034262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051415', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-145.2', 'loss/train': '0.6813', 'examples_per_second': '32.541', 'grad_norm': '25.25', 'counters/examples': 237088, 'counters/updates': 7409}
skipping logging after 237120 examples to avoid logging too frequently
train stats after 237152 examples: {'rewards_train/chosen': '0.11556', 'rewards_train/rejected': '0.031342', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084218', 'logps_train/rejected': '-121.91', 'logps_train/chosen': '-120.98', 'loss/train': '0.66422', 'examples_per_second': '31.417', 'grad_norm': '25.75', 'counters/examples': 237152, 'counters/updates': 7411}
train stats after 237184 examples: {'rewards_train/chosen': '0.083477', 'rewards_train/rejected': '0.047094', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036383', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-129.83', 'loss/train': '0.68598', 'examples_per_second': '30.541', 'grad_norm': '28.5', 'counters/examples': 237184, 'counters/updates': 7412}
train stats after 237216 examples: {'rewards_train/chosen': '0.18645', 'rewards_train/rejected': '-0.026184', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21263', 'logps_train/rejected': '-116.27', 'logps_train/chosen': '-120.17', 'loss/train': '0.60701', 'examples_per_second': '31.413', 'grad_norm': '21.875', 'counters/examples': 237216, 'counters/updates': 7413}
skipping logging after 237248 examples to avoid logging too frequently
train stats after 237280 examples: {'rewards_train/chosen': '0.045081', 'rewards_train/rejected': '0.006914', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038167', 'logps_train/rejected': '-91.831', 'logps_train/chosen': '-80.985', 'loss/train': '0.68274', 'examples_per_second': '30.501', 'grad_norm': '22.5', 'counters/examples': 237280, 'counters/updates': 7415}
train stats after 237312 examples: {'rewards_train/chosen': '0.17988', 'rewards_train/rejected': '-0.028226', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20811', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-158.98', 'loss/train': '0.61179', 'examples_per_second': '31.905', 'grad_norm': '27', 'counters/examples': 237312, 'counters/updates': 7416}
train stats after 237344 examples: {'rewards_train/chosen': '0.24544', 'rewards_train/rejected': '0.11228', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13316', 'logps_train/rejected': '-95.532', 'logps_train/chosen': '-137.62', 'loss/train': '0.63815', 'examples_per_second': '31.498', 'grad_norm': '29.375', 'counters/examples': 237344, 'counters/updates': 7417}
skipping logging after 237376 examples to avoid logging too frequently
train stats after 237408 examples: {'rewards_train/chosen': '0.13284', 'rewards_train/rejected': '0.053275', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079568', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-134.72', 'loss/train': '0.66908', 'examples_per_second': '30.141', 'grad_norm': '23.875', 'counters/examples': 237408, 'counters/updates': 7419}
train stats after 237440 examples: {'rewards_train/chosen': '0.19992', 'rewards_train/rejected': '0.056146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14378', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-148.14', 'loss/train': '0.63857', 'examples_per_second': '31.574', 'grad_norm': '24.625', 'counters/examples': 237440, 'counters/updates': 7420}
train stats after 237472 examples: {'rewards_train/chosen': '0.15616', 'rewards_train/rejected': '0.029298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12686', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-171.77', 'loss/train': '0.64346', 'examples_per_second': '30.479', 'grad_norm': '27.75', 'counters/examples': 237472, 'counters/updates': 7421}
train stats after 237504 examples: {'rewards_train/chosen': '0.075624', 'rewards_train/rejected': '0.013461', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062163', 'logps_train/rejected': '-95.672', 'logps_train/chosen': '-110.42', 'loss/train': '0.67682', 'examples_per_second': '32.372', 'grad_norm': '24', 'counters/examples': 237504, 'counters/updates': 7422}
train stats after 237536 examples: {'rewards_train/chosen': '0.1533', 'rewards_train/rejected': '-0.032868', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18617', 'logps_train/rejected': '-102.77', 'logps_train/chosen': '-144.74', 'loss/train': '0.61755', 'examples_per_second': '31.31', 'grad_norm': '21.625', 'counters/examples': 237536, 'counters/updates': 7423}
train stats after 237568 examples: {'rewards_train/chosen': '0.098659', 'rewards_train/rejected': '-0.044678', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14334', 'logps_train/rejected': '-107.77', 'logps_train/chosen': '-122.72', 'loss/train': '0.63919', 'examples_per_second': '30.998', 'grad_norm': '23', 'counters/examples': 237568, 'counters/updates': 7424}
train stats after 237600 examples: {'rewards_train/chosen': '0.20215', 'rewards_train/rejected': '-0.0022037', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20435', 'logps_train/rejected': '-107.42', 'logps_train/chosen': '-139.48', 'loss/train': '0.61018', 'examples_per_second': '30.038', 'grad_norm': '23.875', 'counters/examples': 237600, 'counters/updates': 7425}
train stats after 237632 examples: {'rewards_train/chosen': '0.12441', 'rewards_train/rejected': '-0.027748', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15216', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-153.33', 'loss/train': '0.63232', 'examples_per_second': '30.265', 'grad_norm': '21.75', 'counters/examples': 237632, 'counters/updates': 7426}
skipping logging after 237664 examples to avoid logging too frequently
train stats after 237696 examples: {'rewards_train/chosen': '0.14043', 'rewards_train/rejected': '0.054664', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085768', 'logps_train/rejected': '-121.91', 'logps_train/chosen': '-112.47', 'loss/train': '0.66376', 'examples_per_second': '31.48', 'grad_norm': '23.875', 'counters/examples': 237696, 'counters/updates': 7428}
train stats after 237728 examples: {'rewards_train/chosen': '0.17786', 'rewards_train/rejected': '0.00062249', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17723', 'logps_train/rejected': '-117.56', 'logps_train/chosen': '-102.45', 'loss/train': '0.6227', 'examples_per_second': '31.575', 'grad_norm': '25', 'counters/examples': 237728, 'counters/updates': 7429}
skipping logging after 237760 examples to avoid logging too frequently
train stats after 237792 examples: {'rewards_train/chosen': '0.14116', 'rewards_train/rejected': '0.0033948', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13776', 'logps_train/rejected': '-142.57', 'logps_train/chosen': '-143.41', 'loss/train': '0.64837', 'examples_per_second': '31.249', 'grad_norm': '27', 'counters/examples': 237792, 'counters/updates': 7431}
train stats after 237824 examples: {'rewards_train/chosen': '0.25911', 'rewards_train/rejected': '0.14678', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11233', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-141.01', 'loss/train': '0.65063', 'examples_per_second': '31.252', 'grad_norm': '23.125', 'counters/examples': 237824, 'counters/updates': 7432}
train stats after 237856 examples: {'rewards_train/chosen': '0.16672', 'rewards_train/rejected': '0.036294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13043', 'logps_train/rejected': '-145.15', 'logps_train/chosen': '-154.12', 'loss/train': '0.64697', 'examples_per_second': '31.303', 'grad_norm': '27.75', 'counters/examples': 237856, 'counters/updates': 7433}
train stats after 237888 examples: {'rewards_train/chosen': '0.15275', 'rewards_train/rejected': '0.05872', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094034', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-158.05', 'loss/train': '0.66312', 'examples_per_second': '31.541', 'grad_norm': '27', 'counters/examples': 237888, 'counters/updates': 7434}
train stats after 237920 examples: {'rewards_train/chosen': '0.19927', 'rewards_train/rejected': '0.072852', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12642', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-160.24', 'loss/train': '0.6532', 'examples_per_second': '31.814', 'grad_norm': '25.625', 'counters/examples': 237920, 'counters/updates': 7435}
skipping logging after 237952 examples to avoid logging too frequently
train stats after 237984 examples: {'rewards_train/chosen': '0.16189', 'rewards_train/rejected': '0.022598', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13929', 'logps_train/rejected': '-152.53', 'logps_train/chosen': '-123.63', 'loss/train': '0.64532', 'examples_per_second': '33.137', 'grad_norm': '26.375', 'counters/examples': 237984, 'counters/updates': 7437}
skipping logging after 238016 examples to avoid logging too frequently
train stats after 238048 examples: {'rewards_train/chosen': '0.21822', 'rewards_train/rejected': '0.064845', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15338', 'logps_train/rejected': '-97.389', 'logps_train/chosen': '-100.66', 'loss/train': '0.63595', 'examples_per_second': '33.916', 'grad_norm': '20.25', 'counters/examples': 238048, 'counters/updates': 7439}
train stats after 238080 examples: {'rewards_train/chosen': '0.1804', 'rewards_train/rejected': '0.05259', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12781', 'logps_train/rejected': '-97.562', 'logps_train/chosen': '-144.88', 'loss/train': '0.64216', 'examples_per_second': '33.194', 'grad_norm': '26.375', 'counters/examples': 238080, 'counters/updates': 7440}
train stats after 238112 examples: {'rewards_train/chosen': '0.11948', 'rewards_train/rejected': '0.037704', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081778', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-139.6', 'loss/train': '0.66978', 'examples_per_second': '32.512', 'grad_norm': '26.875', 'counters/examples': 238112, 'counters/updates': 7441}
train stats after 238144 examples: {'rewards_train/chosen': '0.081799', 'rewards_train/rejected': '-0.018769', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10057', 'logps_train/rejected': '-134.35', 'logps_train/chosen': '-143.32', 'loss/train': '0.66472', 'examples_per_second': '30.255', 'grad_norm': '28.875', 'counters/examples': 238144, 'counters/updates': 7442}
train stats after 238176 examples: {'rewards_train/chosen': '0.19642', 'rewards_train/rejected': '0.0035238', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1929', 'logps_train/rejected': '-108.87', 'logps_train/chosen': '-153.62', 'loss/train': '0.61048', 'examples_per_second': '31.6', 'grad_norm': '24.75', 'counters/examples': 238176, 'counters/updates': 7443}
train stats after 238208 examples: {'rewards_train/chosen': '0.14229', 'rewards_train/rejected': '0.01426', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12803', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-125.94', 'loss/train': '0.64518', 'examples_per_second': '30.629', 'grad_norm': '25.75', 'counters/examples': 238208, 'counters/updates': 7444}
train stats after 238240 examples: {'rewards_train/chosen': '0.13944', 'rewards_train/rejected': '0.031626', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10782', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-181.18', 'loss/train': '0.65912', 'examples_per_second': '30.478', 'grad_norm': '29.875', 'counters/examples': 238240, 'counters/updates': 7445}
train stats after 238272 examples: {'rewards_train/chosen': '0.074758', 'rewards_train/rejected': '0.060311', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014447', 'logps_train/rejected': '-101.01', 'logps_train/chosen': '-153.91', 'loss/train': '0.70235', 'examples_per_second': '30.938', 'grad_norm': '25.5', 'counters/examples': 238272, 'counters/updates': 7446}
train stats after 238304 examples: {'rewards_train/chosen': '0.11406', 'rewards_train/rejected': '-0.00039448', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11445', 'logps_train/rejected': '-123.97', 'logps_train/chosen': '-146.67', 'loss/train': '0.65039', 'examples_per_second': '31.522', 'grad_norm': '25.875', 'counters/examples': 238304, 'counters/updates': 7447}
train stats after 238336 examples: {'rewards_train/chosen': '0.16752', 'rewards_train/rejected': '-0.0044254', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17194', 'logps_train/rejected': '-127.89', 'logps_train/chosen': '-168.63', 'loss/train': '0.63504', 'examples_per_second': '32.045', 'grad_norm': '28.125', 'counters/examples': 238336, 'counters/updates': 7448}
train stats after 238368 examples: {'rewards_train/chosen': '0.26018', 'rewards_train/rejected': '-0.0030776', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26326', 'logps_train/rejected': '-151.12', 'logps_train/chosen': '-163.09', 'loss/train': '0.59668', 'examples_per_second': '32.063', 'grad_norm': '27.25', 'counters/examples': 238368, 'counters/updates': 7449}
train stats after 238400 examples: {'rewards_train/chosen': '0.11351', 'rewards_train/rejected': '0.12097', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0074616', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-103.91', 'loss/train': '0.70887', 'examples_per_second': '32.376', 'grad_norm': '25.25', 'counters/examples': 238400, 'counters/updates': 7450}
train stats after 238432 examples: {'rewards_train/chosen': '0.20139', 'rewards_train/rejected': '0.02546', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17594', 'logps_train/rejected': '-131.81', 'logps_train/chosen': '-156.7', 'loss/train': '0.62089', 'examples_per_second': '30.766', 'grad_norm': '25.625', 'counters/examples': 238432, 'counters/updates': 7451}
train stats after 238464 examples: {'rewards_train/chosen': '0.1561', 'rewards_train/rejected': '0.060591', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.095513', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-120.19', 'loss/train': '0.66068', 'examples_per_second': '32.694', 'grad_norm': '25.25', 'counters/examples': 238464, 'counters/updates': 7452}
train stats after 238496 examples: {'rewards_train/chosen': '0.071053', 'rewards_train/rejected': '-0.078172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14923', 'logps_train/rejected': '-144.99', 'logps_train/chosen': '-122.54', 'loss/train': '0.63434', 'examples_per_second': '31.618', 'grad_norm': '29', 'counters/examples': 238496, 'counters/updates': 7453}
train stats after 238528 examples: {'rewards_train/chosen': '0.14853', 'rewards_train/rejected': '0.0044566', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14407', 'logps_train/rejected': '-121.39', 'logps_train/chosen': '-123.15', 'loss/train': '0.64065', 'examples_per_second': '30.621', 'grad_norm': '24.875', 'counters/examples': 238528, 'counters/updates': 7454}
train stats after 238560 examples: {'rewards_train/chosen': '0.096322', 'rewards_train/rejected': '-0.052331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14865', 'logps_train/rejected': '-131.4', 'logps_train/chosen': '-151.13', 'loss/train': '0.6305', 'examples_per_second': '31.604', 'grad_norm': '25.875', 'counters/examples': 238560, 'counters/updates': 7455}
train stats after 238592 examples: {'rewards_train/chosen': '0.075511', 'rewards_train/rejected': '-0.020205', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095715', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-131.71', 'loss/train': '0.65802', 'examples_per_second': '32.321', 'grad_norm': '23', 'counters/examples': 238592, 'counters/updates': 7456}
train stats after 238624 examples: {'rewards_train/chosen': '0.13107', 'rewards_train/rejected': '0.0077888', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12328', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-145.8', 'loss/train': '0.64493', 'examples_per_second': '31.983', 'grad_norm': '24.625', 'counters/examples': 238624, 'counters/updates': 7457}
train stats after 238656 examples: {'rewards_train/chosen': '0.12676', 'rewards_train/rejected': '0.036615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090146', 'logps_train/rejected': '-128.69', 'logps_train/chosen': '-154.56', 'loss/train': '0.6742', 'examples_per_second': '30.994', 'grad_norm': '26.125', 'counters/examples': 238656, 'counters/updates': 7458}
train stats after 238688 examples: {'rewards_train/chosen': '0.12597', 'rewards_train/rejected': '0.080598', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045372', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-118.75', 'loss/train': '0.6816', 'examples_per_second': '31.698', 'grad_norm': '24.875', 'counters/examples': 238688, 'counters/updates': 7459}
train stats after 238720 examples: {'rewards_train/chosen': '0.06516', 'rewards_train/rejected': '0.031948', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033212', 'logps_train/rejected': '-153.93', 'logps_train/chosen': '-133.35', 'loss/train': '0.68702', 'examples_per_second': '31.685', 'grad_norm': '33', 'counters/examples': 238720, 'counters/updates': 7460}
train stats after 238752 examples: {'rewards_train/chosen': '0.13072', 'rewards_train/rejected': '-0.0079493', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13867', 'logps_train/rejected': '-131.13', 'logps_train/chosen': '-128.36', 'loss/train': '0.6399', 'examples_per_second': '31.642', 'grad_norm': '25.625', 'counters/examples': 238752, 'counters/updates': 7461}
train stats after 238784 examples: {'rewards_train/chosen': '0.017336', 'rewards_train/rejected': '-0.075736', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093072', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-151.85', 'loss/train': '0.6623', 'examples_per_second': '31.032', 'grad_norm': '29.625', 'counters/examples': 238784, 'counters/updates': 7462}
train stats after 238816 examples: {'rewards_train/chosen': '0.15127', 'rewards_train/rejected': '0.066756', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084516', 'logps_train/rejected': '-128.86', 'logps_train/chosen': '-146.1', 'loss/train': '0.66179', 'examples_per_second': '31.616', 'grad_norm': '27.875', 'counters/examples': 238816, 'counters/updates': 7463}
train stats after 238848 examples: {'rewards_train/chosen': '0.1', 'rewards_train/rejected': '0.037085', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062919', 'logps_train/rejected': '-137.4', 'logps_train/chosen': '-160.75', 'loss/train': '0.67807', 'examples_per_second': '30.636', 'grad_norm': '28.875', 'counters/examples': 238848, 'counters/updates': 7464}
train stats after 238880 examples: {'rewards_train/chosen': '0.063608', 'rewards_train/rejected': '0.017275', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046334', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-108.53', 'loss/train': '0.67514', 'examples_per_second': '31.644', 'grad_norm': '31.875', 'counters/examples': 238880, 'counters/updates': 7465}
skipping logging after 238912 examples to avoid logging too frequently
train stats after 238944 examples: {'rewards_train/chosen': '0.027292', 'rewards_train/rejected': '-0.041819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069111', 'logps_train/rejected': '-100.62', 'logps_train/chosen': '-115.31', 'loss/train': '0.6706', 'examples_per_second': '30.578', 'grad_norm': '22.5', 'counters/examples': 238944, 'counters/updates': 7467}
train stats after 238976 examples: {'rewards_train/chosen': '0.21535', 'rewards_train/rejected': '0.09125', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1241', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-188.87', 'loss/train': '0.65089', 'examples_per_second': '31.617', 'grad_norm': '28.625', 'counters/examples': 238976, 'counters/updates': 7468}
train stats after 239008 examples: {'rewards_train/chosen': '0.022841', 'rewards_train/rejected': '0.048187', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025346', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-151.07', 'loss/train': '0.72049', 'examples_per_second': '31.986', 'grad_norm': '27.75', 'counters/examples': 239008, 'counters/updates': 7469}
train stats after 239040 examples: {'rewards_train/chosen': '0.17154', 'rewards_train/rejected': '0.0031293', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16841', 'logps_train/rejected': '-69.436', 'logps_train/chosen': '-117.87', 'loss/train': '0.62112', 'examples_per_second': '31.882', 'grad_norm': '20.125', 'counters/examples': 239040, 'counters/updates': 7470}
train stats after 239072 examples: {'rewards_train/chosen': '0.23065', 'rewards_train/rejected': '0.054223', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17643', 'logps_train/rejected': '-118.35', 'logps_train/chosen': '-163.96', 'loss/train': '0.62416', 'examples_per_second': '31.65', 'grad_norm': '25.125', 'counters/examples': 239072, 'counters/updates': 7471}
train stats after 239104 examples: {'rewards_train/chosen': '0.15139', 'rewards_train/rejected': '-0.061266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21266', 'logps_train/rejected': '-103.77', 'logps_train/chosen': '-143.01', 'loss/train': '0.61568', 'examples_per_second': '30.564', 'grad_norm': '27', 'counters/examples': 239104, 'counters/updates': 7472}
train stats after 239136 examples: {'rewards_train/chosen': '0.15584', 'rewards_train/rejected': '-0.0076597', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1635', 'logps_train/rejected': '-149.8', 'logps_train/chosen': '-147.5', 'loss/train': '0.62612', 'examples_per_second': '30.101', 'grad_norm': '26.625', 'counters/examples': 239136, 'counters/updates': 7473}
train stats after 239168 examples: {'rewards_train/chosen': '0.029296', 'rewards_train/rejected': '-0.05637', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085666', 'logps_train/rejected': '-104.21', 'logps_train/chosen': '-149.36', 'loss/train': '0.67404', 'examples_per_second': '32.559', 'grad_norm': '27.625', 'counters/examples': 239168, 'counters/updates': 7474}
train stats after 239200 examples: {'rewards_train/chosen': '0.027732', 'rewards_train/rejected': '-0.028843', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056575', 'logps_train/rejected': '-129.6', 'logps_train/chosen': '-141.44', 'loss/train': '0.67947', 'examples_per_second': '30.646', 'grad_norm': '26.5', 'counters/examples': 239200, 'counters/updates': 7475}
train stats after 239232 examples: {'rewards_train/chosen': '0.14799', 'rewards_train/rejected': '0.044695', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1033', 'logps_train/rejected': '-171.47', 'logps_train/chosen': '-168.28', 'loss/train': '0.66523', 'examples_per_second': '31.652', 'grad_norm': '33', 'counters/examples': 239232, 'counters/updates': 7476}
train stats after 239264 examples: {'rewards_train/chosen': '0.13215', 'rewards_train/rejected': '-0.050123', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18228', 'logps_train/rejected': '-110.49', 'logps_train/chosen': '-155.17', 'loss/train': '0.62565', 'examples_per_second': '30.153', 'grad_norm': '48.25', 'counters/examples': 239264, 'counters/updates': 7477}
train stats after 239296 examples: {'rewards_train/chosen': '0.20475', 'rewards_train/rejected': '0.06434', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14041', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-137.56', 'loss/train': '0.64894', 'examples_per_second': '33.097', 'grad_norm': '32.25', 'counters/examples': 239296, 'counters/updates': 7478}
train stats after 239328 examples: {'rewards_train/chosen': '0.1267', 'rewards_train/rejected': '-0.022688', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14939', 'logps_train/rejected': '-98.348', 'logps_train/chosen': '-136.23', 'loss/train': '0.63766', 'examples_per_second': '31.14', 'grad_norm': '22.375', 'counters/examples': 239328, 'counters/updates': 7479}
train stats after 239360 examples: {'rewards_train/chosen': '0.18279', 'rewards_train/rejected': '0.004635', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17815', 'logps_train/rejected': '-157.12', 'logps_train/chosen': '-160.35', 'loss/train': '0.62736', 'examples_per_second': '31.98', 'grad_norm': '27.25', 'counters/examples': 239360, 'counters/updates': 7480}
train stats after 239392 examples: {'rewards_train/chosen': '0.15565', 'rewards_train/rejected': '0.096755', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058896', 'logps_train/rejected': '-133.96', 'logps_train/chosen': '-174.73', 'loss/train': '0.68099', 'examples_per_second': '31.6', 'grad_norm': '30.25', 'counters/examples': 239392, 'counters/updates': 7481}
train stats after 239424 examples: {'rewards_train/chosen': '0.1616', 'rewards_train/rejected': '0.042574', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11902', 'logps_train/rejected': '-143.22', 'logps_train/chosen': '-154.43', 'loss/train': '0.64812', 'examples_per_second': '33.232', 'grad_norm': '26.875', 'counters/examples': 239424, 'counters/updates': 7482}
train stats after 239456 examples: {'rewards_train/chosen': '0.097301', 'rewards_train/rejected': '-0.0024679', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099769', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-137.16', 'loss/train': '0.66755', 'examples_per_second': '30.28', 'grad_norm': '27', 'counters/examples': 239456, 'counters/updates': 7483}
train stats after 239488 examples: {'rewards_train/chosen': '0.1716', 'rewards_train/rejected': '-0.018262', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18986', 'logps_train/rejected': '-97.431', 'logps_train/chosen': '-131.7', 'loss/train': '0.63', 'examples_per_second': '31.629', 'grad_norm': '23.375', 'counters/examples': 239488, 'counters/updates': 7484}
train stats after 239520 examples: {'rewards_train/chosen': '0.12808', 'rewards_train/rejected': '-0.042021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1701', 'logps_train/rejected': '-135.06', 'logps_train/chosen': '-151.03', 'loss/train': '0.62766', 'examples_per_second': '31.632', 'grad_norm': '26.25', 'counters/examples': 239520, 'counters/updates': 7485}
train stats after 239552 examples: {'rewards_train/chosen': '0.17256', 'rewards_train/rejected': '-0.0090179', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18158', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-132.36', 'loss/train': '0.62356', 'examples_per_second': '31.648', 'grad_norm': '24.125', 'counters/examples': 239552, 'counters/updates': 7486}
train stats after 239584 examples: {'rewards_train/chosen': '0.12291', 'rewards_train/rejected': '-0.037241', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16015', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-182.73', 'loss/train': '0.62644', 'examples_per_second': '30.624', 'grad_norm': '24.5', 'counters/examples': 239584, 'counters/updates': 7487}
train stats after 239616 examples: {'rewards_train/chosen': '0.11425', 'rewards_train/rejected': '-0.0094951', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12375', 'logps_train/rejected': '-127.92', 'logps_train/chosen': '-138.31', 'loss/train': '0.64361', 'examples_per_second': '31.569', 'grad_norm': '25.125', 'counters/examples': 239616, 'counters/updates': 7488}
train stats after 239648 examples: {'rewards_train/chosen': '0.14362', 'rewards_train/rejected': '0.053279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090346', 'logps_train/rejected': '-104.86', 'logps_train/chosen': '-133.89', 'loss/train': '0.66457', 'examples_per_second': '31.005', 'grad_norm': '24.75', 'counters/examples': 239648, 'counters/updates': 7489}
skipping logging after 239680 examples to avoid logging too frequently
train stats after 239712 examples: {'rewards_train/chosen': '0.18913', 'rewards_train/rejected': '0.042849', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14628', 'logps_train/rejected': '-102.35', 'logps_train/chosen': '-158.5', 'loss/train': '0.63508', 'examples_per_second': '31.165', 'grad_norm': '25.25', 'counters/examples': 239712, 'counters/updates': 7491}
train stats after 239744 examples: {'rewards_train/chosen': '0.1282', 'rewards_train/rejected': '0.074238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053961', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-124.52', 'loss/train': '0.67615', 'examples_per_second': '32.73', 'grad_norm': '24.875', 'counters/examples': 239744, 'counters/updates': 7492}
train stats after 239776 examples: {'rewards_train/chosen': '0.19393', 'rewards_train/rejected': '0.083679', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11026', 'logps_train/rejected': '-149.97', 'logps_train/chosen': '-119.66', 'loss/train': '0.65408', 'examples_per_second': '31.721', 'grad_norm': '26.375', 'counters/examples': 239776, 'counters/updates': 7493}
skipping logging after 239808 examples to avoid logging too frequently
train stats after 239840 examples: {'rewards_train/chosen': '0.17007', 'rewards_train/rejected': '0.024371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1457', 'logps_train/rejected': '-111.06', 'logps_train/chosen': '-136.68', 'loss/train': '0.63816', 'examples_per_second': '35.69', 'grad_norm': '25', 'counters/examples': 239840, 'counters/updates': 7495}
train stats after 239872 examples: {'rewards_train/chosen': '0.094099', 'rewards_train/rejected': '0.097553', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0034543', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-141.41', 'loss/train': '0.71462', 'examples_per_second': '31.631', 'grad_norm': '32', 'counters/examples': 239872, 'counters/updates': 7496}
train stats after 239904 examples: {'rewards_train/chosen': '0.091693', 'rewards_train/rejected': '0.032064', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059629', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-123.18', 'loss/train': '0.67355', 'examples_per_second': '26.297', 'grad_norm': '25.25', 'counters/examples': 239904, 'counters/updates': 7497}
train stats after 239936 examples: {'rewards_train/chosen': '0.095739', 'rewards_train/rejected': '0.059759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03598', 'logps_train/rejected': '-109.34', 'logps_train/chosen': '-144.73', 'loss/train': '0.68938', 'examples_per_second': '31.635', 'grad_norm': '27.625', 'counters/examples': 239936, 'counters/updates': 7498}
train stats after 239968 examples: {'rewards_train/chosen': '0.039122', 'rewards_train/rejected': '-0.058054', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097176', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-112', 'loss/train': '0.65448', 'examples_per_second': '31.786', 'grad_norm': '22.75', 'counters/examples': 239968, 'counters/updates': 7499}
train stats after 240000 examples: {'rewards_train/chosen': '0.085112', 'rewards_train/rejected': '0.087372', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00226', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-122.16', 'loss/train': '0.71123', 'examples_per_second': '32.671', 'grad_norm': '25.125', 'counters/examples': 240000, 'counters/updates': 7500}
Running evaluation after 240000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.26it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 240000: {'rewards_eval/chosen': '0.14976', 'rewards_eval/rejected': '0.036621', 'rewards_eval/accuracies': '0.59375', 'rewards_eval/margins': '0.11314', 'logps_eval/rejected': '-118.25', 'logps_eval/chosen': '-137.94', 'loss/eval': '0.6557'}
skipping save for non epoch
train stats after 240032 examples: {'rewards_train/chosen': '0.1661', 'rewards_train/rejected': '0.092933', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073171', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-150.61', 'loss/train': '0.684', 'examples_per_second': '30.687', 'grad_norm': '31.125', 'counters/examples': 240032, 'counters/updates': 7501}
train stats after 240064 examples: {'rewards_train/chosen': '0.056119', 'rewards_train/rejected': '-0.050067', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10619', 'logps_train/rejected': '-96.385', 'logps_train/chosen': '-116.63', 'loss/train': '0.65133', 'examples_per_second': '30.776', 'grad_norm': '24.875', 'counters/examples': 240064, 'counters/updates': 7502}
train stats after 240096 examples: {'rewards_train/chosen': '0.064636', 'rewards_train/rejected': '-0.028004', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092641', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-119.66', 'loss/train': '0.66109', 'examples_per_second': '31.071', 'grad_norm': '24.125', 'counters/examples': 240096, 'counters/updates': 7503}
train stats after 240128 examples: {'rewards_train/chosen': '0.19306', 'rewards_train/rejected': '0.10599', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08707', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-168.58', 'loss/train': '0.6651', 'examples_per_second': '31.338', 'grad_norm': '26.625', 'counters/examples': 240128, 'counters/updates': 7504}
train stats after 240160 examples: {'rewards_train/chosen': '0.10026', 'rewards_train/rejected': '0.06418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036076', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-152.99', 'loss/train': '0.68589', 'examples_per_second': '31.057', 'grad_norm': '26.125', 'counters/examples': 240160, 'counters/updates': 7505}
train stats after 240192 examples: {'rewards_train/chosen': '0.1945', 'rewards_train/rejected': '0.046834', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14766', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-135.35', 'loss/train': '0.64424', 'examples_per_second': '30.982', 'grad_norm': '24', 'counters/examples': 240192, 'counters/updates': 7506}
train stats after 240224 examples: {'rewards_train/chosen': '0.19062', 'rewards_train/rejected': '-0.013165', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20379', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-136.33', 'loss/train': '0.61196', 'examples_per_second': '31.413', 'grad_norm': '24.25', 'counters/examples': 240224, 'counters/updates': 7507}
skipping logging after 240256 examples to avoid logging too frequently
train stats after 240288 examples: {'rewards_train/chosen': '0.11598', 'rewards_train/rejected': '0.035138', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080843', 'logps_train/rejected': '-136.82', 'logps_train/chosen': '-131.92', 'loss/train': '0.66294', 'examples_per_second': '30.645', 'grad_norm': '25.125', 'counters/examples': 240288, 'counters/updates': 7509}
train stats after 240320 examples: {'rewards_train/chosen': '0.091126', 'rewards_train/rejected': '0.040792', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050334', 'logps_train/rejected': '-151.99', 'logps_train/chosen': '-152.59', 'loss/train': '0.67907', 'examples_per_second': '30.966', 'grad_norm': '30.25', 'counters/examples': 240320, 'counters/updates': 7510}
skipping logging after 240352 examples to avoid logging too frequently
train stats after 240384 examples: {'rewards_train/chosen': '0.19611', 'rewards_train/rejected': '0.038683', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15743', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-146.99', 'loss/train': '0.64282', 'examples_per_second': '34.828', 'grad_norm': '24.25', 'counters/examples': 240384, 'counters/updates': 7512}
train stats after 240416 examples: {'rewards_train/chosen': '0.11748', 'rewards_train/rejected': '0.049963', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067515', 'logps_train/rejected': '-106.62', 'logps_train/chosen': '-103.81', 'loss/train': '0.67944', 'examples_per_second': '30.853', 'grad_norm': '25.125', 'counters/examples': 240416, 'counters/updates': 7513}
train stats after 240448 examples: {'rewards_train/chosen': '0.12126', 'rewards_train/rejected': '-0.020218', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14148', 'logps_train/rejected': '-114.33', 'logps_train/chosen': '-149.5', 'loss/train': '0.646', 'examples_per_second': '31.558', 'grad_norm': '26.75', 'counters/examples': 240448, 'counters/updates': 7514}
train stats after 240480 examples: {'rewards_train/chosen': '0.15062', 'rewards_train/rejected': '0.07384', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076779', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-130.39', 'loss/train': '0.66481', 'examples_per_second': '30.562', 'grad_norm': '25.125', 'counters/examples': 240480, 'counters/updates': 7515}
train stats after 240512 examples: {'rewards_train/chosen': '0.12075', 'rewards_train/rejected': '0.026919', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093827', 'logps_train/rejected': '-109.61', 'logps_train/chosen': '-153.31', 'loss/train': '0.6595', 'examples_per_second': '30.656', 'grad_norm': '25.625', 'counters/examples': 240512, 'counters/updates': 7516}
skipping logging after 240544 examples to avoid logging too frequently
train stats after 240576 examples: {'rewards_train/chosen': '0.14808', 'rewards_train/rejected': '0.069268', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078815', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-148.62', 'loss/train': '0.66672', 'examples_per_second': '30.664', 'grad_norm': '29', 'counters/examples': 240576, 'counters/updates': 7518}
train stats after 240608 examples: {'rewards_train/chosen': '0.13468', 'rewards_train/rejected': '0.050045', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08464', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-130.79', 'loss/train': '0.66261', 'examples_per_second': '33.318', 'grad_norm': '24.75', 'counters/examples': 240608, 'counters/updates': 7519}
train stats after 240640 examples: {'rewards_train/chosen': '0.11722', 'rewards_train/rejected': '0.040281', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076939', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-129.3', 'loss/train': '0.66803', 'examples_per_second': '24.762', 'grad_norm': '26.875', 'counters/examples': 240640, 'counters/updates': 7520}
skipping logging after 240672 examples to avoid logging too frequently
train stats after 240704 examples: {'rewards_train/chosen': '0.14732', 'rewards_train/rejected': '0.1269', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.020418', 'logps_train/rejected': '-111.32', 'logps_train/chosen': '-128.58', 'loss/train': '0.71269', 'examples_per_second': '35.471', 'grad_norm': '31.625', 'counters/examples': 240704, 'counters/updates': 7522}
train stats after 240736 examples: {'rewards_train/chosen': '0.20116', 'rewards_train/rejected': '0.054175', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14699', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-165.89', 'loss/train': '0.64097', 'examples_per_second': '24.064', 'grad_norm': '32.75', 'counters/examples': 240736, 'counters/updates': 7523}
train stats after 240768 examples: {'rewards_train/chosen': '0.18602', 'rewards_train/rejected': '0.059113', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12691', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-124.62', 'loss/train': '0.64888', 'examples_per_second': '31.018', 'grad_norm': '25.25', 'counters/examples': 240768, 'counters/updates': 7524}
train stats after 240800 examples: {'rewards_train/chosen': '0.10564', 'rewards_train/rejected': '0.043168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062476', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-111.82', 'loss/train': '0.66857', 'examples_per_second': '33.307', 'grad_norm': '25.25', 'counters/examples': 240800, 'counters/updates': 7525}
train stats after 240832 examples: {'rewards_train/chosen': '0.084594', 'rewards_train/rejected': '0.066465', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018129', 'logps_train/rejected': '-122.89', 'logps_train/chosen': '-174.94', 'loss/train': '0.70123', 'examples_per_second': '31.078', 'grad_norm': '31.375', 'counters/examples': 240832, 'counters/updates': 7526}
train stats after 240864 examples: {'rewards_train/chosen': '0.16896', 'rewards_train/rejected': '0.082588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086374', 'logps_train/rejected': '-99.372', 'logps_train/chosen': '-136.64', 'loss/train': '0.65701', 'examples_per_second': '32.432', 'grad_norm': '24.875', 'counters/examples': 240864, 'counters/updates': 7527}
train stats after 240896 examples: {'rewards_train/chosen': '0.19392', 'rewards_train/rejected': '-0.0050066', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19893', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-141.2', 'loss/train': '0.6148', 'examples_per_second': '30.988', 'grad_norm': '24.75', 'counters/examples': 240896, 'counters/updates': 7528}
train stats after 240928 examples: {'rewards_train/chosen': '0.15251', 'rewards_train/rejected': '-0.024175', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17669', 'logps_train/rejected': '-134.18', 'logps_train/chosen': '-119.68', 'loss/train': '0.62315', 'examples_per_second': '31.894', 'grad_norm': '23.875', 'counters/examples': 240928, 'counters/updates': 7529}
skipping logging after 240960 examples to avoid logging too frequently
train stats after 240992 examples: {'rewards_train/chosen': '0.10875', 'rewards_train/rejected': '-0.020732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12948', 'logps_train/rejected': '-94.287', 'logps_train/chosen': '-131.3', 'loss/train': '0.65365', 'examples_per_second': '33.512', 'grad_norm': '22.25', 'counters/examples': 240992, 'counters/updates': 7531}
train stats after 241024 examples: {'rewards_train/chosen': '0.25956', 'rewards_train/rejected': '0.10152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15804', 'logps_train/rejected': '-109.84', 'logps_train/chosen': '-137.2', 'loss/train': '0.63191', 'examples_per_second': '30.538', 'grad_norm': '25.5', 'counters/examples': 241024, 'counters/updates': 7532}
skipping logging after 241056 examples to avoid logging too frequently
train stats after 241088 examples: {'rewards_train/chosen': '0.20007', 'rewards_train/rejected': '0.074036', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12603', 'logps_train/rejected': '-134.35', 'logps_train/chosen': '-136.43', 'loss/train': '0.64267', 'examples_per_second': '30.175', 'grad_norm': '32.75', 'counters/examples': 241088, 'counters/updates': 7534}
train stats after 241120 examples: {'rewards_train/chosen': '0.069098', 'rewards_train/rejected': '0.049164', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019934', 'logps_train/rejected': '-147.87', 'logps_train/chosen': '-126.87', 'loss/train': '0.69681', 'examples_per_second': '31.476', 'grad_norm': '27.625', 'counters/examples': 241120, 'counters/updates': 7535}
train stats after 241152 examples: {'rewards_train/chosen': '0.24427', 'rewards_train/rejected': '0.040945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20332', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-134.27', 'loss/train': '0.60883', 'examples_per_second': '30.666', 'grad_norm': '23.875', 'counters/examples': 241152, 'counters/updates': 7536}
train stats after 241184 examples: {'rewards_train/chosen': '0.29509', 'rewards_train/rejected': '0.13115', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16394', 'logps_train/rejected': '-130.03', 'logps_train/chosen': '-129.71', 'loss/train': '0.63747', 'examples_per_second': '31.06', 'grad_norm': '25.125', 'counters/examples': 241184, 'counters/updates': 7537}
train stats after 241216 examples: {'rewards_train/chosen': '0.18739', 'rewards_train/rejected': '0.033907', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15348', 'logps_train/rejected': '-101.67', 'logps_train/chosen': '-110.47', 'loss/train': '0.6409', 'examples_per_second': '30.541', 'grad_norm': '21.125', 'counters/examples': 241216, 'counters/updates': 7538}
skipping logging after 241248 examples to avoid logging too frequently
train stats after 241280 examples: {'rewards_train/chosen': '0.21844', 'rewards_train/rejected': '0.073994', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14444', 'logps_train/rejected': '-110.39', 'logps_train/chosen': '-128.82', 'loss/train': '0.63866', 'examples_per_second': '33.381', 'grad_norm': '24.75', 'counters/examples': 241280, 'counters/updates': 7540}
train stats after 241312 examples: {'rewards_train/chosen': '0.11736', 'rewards_train/rejected': '-0.0088698', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12623', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-132.72', 'loss/train': '0.64711', 'examples_per_second': '31.97', 'grad_norm': '27', 'counters/examples': 241312, 'counters/updates': 7541}
train stats after 241344 examples: {'rewards_train/chosen': '0.11329', 'rewards_train/rejected': '0.039443', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073851', 'logps_train/rejected': '-156.98', 'logps_train/chosen': '-147.47', 'loss/train': '0.67404', 'examples_per_second': '31.563', 'grad_norm': '28.875', 'counters/examples': 241344, 'counters/updates': 7542}
train stats after 241376 examples: {'rewards_train/chosen': '0.11466', 'rewards_train/rejected': '0.0026314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11203', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-166.67', 'loss/train': '0.65181', 'examples_per_second': '31.663', 'grad_norm': '28.75', 'counters/examples': 241376, 'counters/updates': 7543}
train stats after 241408 examples: {'rewards_train/chosen': '0.026045', 'rewards_train/rejected': '0.060786', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034741', 'logps_train/rejected': '-139.28', 'logps_train/chosen': '-149.9', 'loss/train': '0.72361', 'examples_per_second': '31.042', 'grad_norm': '29.375', 'counters/examples': 241408, 'counters/updates': 7544}
train stats after 241440 examples: {'rewards_train/chosen': '0.27822', 'rewards_train/rejected': '0.095919', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1823', 'logps_train/rejected': '-118.08', 'logps_train/chosen': '-172.87', 'loss/train': '0.61905', 'examples_per_second': '31.631', 'grad_norm': '24.75', 'counters/examples': 241440, 'counters/updates': 7545}
train stats after 241472 examples: {'rewards_train/chosen': '0.19551', 'rewards_train/rejected': '0.022811', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1727', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-135.85', 'loss/train': '0.63241', 'examples_per_second': '32.564', 'grad_norm': '24.625', 'counters/examples': 241472, 'counters/updates': 7546}
train stats after 241504 examples: {'rewards_train/chosen': '0.28288', 'rewards_train/rejected': '0.024691', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25819', 'logps_train/rejected': '-127', 'logps_train/chosen': '-180.25', 'loss/train': '0.59753', 'examples_per_second': '31.559', 'grad_norm': '25.5', 'counters/examples': 241504, 'counters/updates': 7547}
train stats after 241536 examples: {'rewards_train/chosen': '0.14652', 'rewards_train/rejected': '0.024483', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12203', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-153.65', 'loss/train': '0.64516', 'examples_per_second': '31.65', 'grad_norm': '29', 'counters/examples': 241536, 'counters/updates': 7548}
train stats after 241568 examples: {'rewards_train/chosen': '0.12944', 'rewards_train/rejected': '0.012045', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11739', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-166.98', 'loss/train': '0.64972', 'examples_per_second': '30.16', 'grad_norm': '24.125', 'counters/examples': 241568, 'counters/updates': 7549}
train stats after 241600 examples: {'rewards_train/chosen': '0.15271', 'rewards_train/rejected': '0.061391', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.091316', 'logps_train/rejected': '-109.84', 'logps_train/chosen': '-135.65', 'loss/train': '0.66963', 'examples_per_second': '31.709', 'grad_norm': '23.75', 'counters/examples': 241600, 'counters/updates': 7550}
train stats after 241632 examples: {'rewards_train/chosen': '0.16029', 'rewards_train/rejected': '0.07331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086976', 'logps_train/rejected': '-120.83', 'logps_train/chosen': '-133.01', 'loss/train': '0.66805', 'examples_per_second': '32.409', 'grad_norm': '24.75', 'counters/examples': 241632, 'counters/updates': 7551}
train stats after 241664 examples: {'rewards_train/chosen': '0.10498', 'rewards_train/rejected': '-0.031317', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1363', 'logps_train/rejected': '-99.446', 'logps_train/chosen': '-169.99', 'loss/train': '0.63936', 'examples_per_second': '31.997', 'grad_norm': '24.5', 'counters/examples': 241664, 'counters/updates': 7552}
train stats after 241696 examples: {'rewards_train/chosen': '0.1443', 'rewards_train/rejected': '-0.037932', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18223', 'logps_train/rejected': '-85.279', 'logps_train/chosen': '-118.06', 'loss/train': '0.61597', 'examples_per_second': '32.338', 'grad_norm': '23.125', 'counters/examples': 241696, 'counters/updates': 7553}
train stats after 241728 examples: {'rewards_train/chosen': '0.24495', 'rewards_train/rejected': '0.053393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19155', 'logps_train/rejected': '-149.81', 'logps_train/chosen': '-154.48', 'loss/train': '0.62319', 'examples_per_second': '31.553', 'grad_norm': '30', 'counters/examples': 241728, 'counters/updates': 7554}
train stats after 241760 examples: {'rewards_train/chosen': '0.12055', 'rewards_train/rejected': '0.021023', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.099526', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-166.03', 'loss/train': '0.65816', 'examples_per_second': '31.359', 'grad_norm': '27', 'counters/examples': 241760, 'counters/updates': 7555}
train stats after 241792 examples: {'rewards_train/chosen': '0.23491', 'rewards_train/rejected': '0.128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1069', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-137.99', 'loss/train': '0.65595', 'examples_per_second': '31.322', 'grad_norm': '25.125', 'counters/examples': 241792, 'counters/updates': 7556}
train stats after 241824 examples: {'rewards_train/chosen': '0.22344', 'rewards_train/rejected': '0.058783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16466', 'logps_train/rejected': '-141.24', 'logps_train/chosen': '-177.27', 'loss/train': '0.63181', 'examples_per_second': '31.64', 'grad_norm': '26.625', 'counters/examples': 241824, 'counters/updates': 7557}
train stats after 241856 examples: {'rewards_train/chosen': '0.11569', 'rewards_train/rejected': '0.013551', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10214', 'logps_train/rejected': '-138.71', 'logps_train/chosen': '-123.8', 'loss/train': '0.6627', 'examples_per_second': '31.365', 'grad_norm': '25.25', 'counters/examples': 241856, 'counters/updates': 7558}
skipping logging after 241888 examples to avoid logging too frequently
train stats after 241920 examples: {'rewards_train/chosen': '0.15146', 'rewards_train/rejected': '0.1023', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049167', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-141', 'loss/train': '0.68444', 'examples_per_second': '31.428', 'grad_norm': '26.5', 'counters/examples': 241920, 'counters/updates': 7560}
train stats after 241952 examples: {'rewards_train/chosen': '0.30589', 'rewards_train/rejected': '0.079624', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22626', 'logps_train/rejected': '-164.1', 'logps_train/chosen': '-165.25', 'loss/train': '0.61607', 'examples_per_second': '31.653', 'grad_norm': '26.125', 'counters/examples': 241952, 'counters/updates': 7561}
train stats after 241984 examples: {'rewards_train/chosen': '0.11972', 'rewards_train/rejected': '0.0087868', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11093', 'logps_train/rejected': '-150.25', 'logps_train/chosen': '-154.84', 'loss/train': '0.65711', 'examples_per_second': '31.529', 'grad_norm': '30.75', 'counters/examples': 241984, 'counters/updates': 7562}
train stats after 242016 examples: {'rewards_train/chosen': '0.059745', 'rewards_train/rejected': '0.023042', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036703', 'logps_train/rejected': '-103.65', 'logps_train/chosen': '-122.05', 'loss/train': '0.68761', 'examples_per_second': '30.413', 'grad_norm': '25.75', 'counters/examples': 242016, 'counters/updates': 7563}
train stats after 242048 examples: {'rewards_train/chosen': '0.18243', 'rewards_train/rejected': '0.037106', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14533', 'logps_train/rejected': '-120.53', 'logps_train/chosen': '-147.34', 'loss/train': '0.64003', 'examples_per_second': '30.679', 'grad_norm': '25.875', 'counters/examples': 242048, 'counters/updates': 7564}
train stats after 242080 examples: {'rewards_train/chosen': '0.19947', 'rewards_train/rejected': '0.0027722', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1967', 'logps_train/rejected': '-155.05', 'logps_train/chosen': '-166.84', 'loss/train': '0.61993', 'examples_per_second': '31.618', 'grad_norm': '29.25', 'counters/examples': 242080, 'counters/updates': 7565}
train stats after 242112 examples: {'rewards_train/chosen': '0.11951', 'rewards_train/rejected': '-0.00033691', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11985', 'logps_train/rejected': '-129.3', 'logps_train/chosen': '-123.39', 'loss/train': '0.64539', 'examples_per_second': '30.335', 'grad_norm': '24.125', 'counters/examples': 242112, 'counters/updates': 7566}
train stats after 242144 examples: {'rewards_train/chosen': '0.16508', 'rewards_train/rejected': '0.022262', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14282', 'logps_train/rejected': '-154.59', 'logps_train/chosen': '-149.1', 'loss/train': '0.63593', 'examples_per_second': '31.85', 'grad_norm': '30', 'counters/examples': 242144, 'counters/updates': 7567}
skipping logging after 242176 examples to avoid logging too frequently
train stats after 242208 examples: {'rewards_train/chosen': '0.2081', 'rewards_train/rejected': '0.028907', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17919', 'logps_train/rejected': '-135.89', 'logps_train/chosen': '-144.41', 'loss/train': '0.61658', 'examples_per_second': '33.726', 'grad_norm': '27.875', 'counters/examples': 242208, 'counters/updates': 7569}
skipping logging after 242240 examples to avoid logging too frequently
train stats after 242272 examples: {'rewards_train/chosen': '0.17847', 'rewards_train/rejected': '0.0026386', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17583', 'logps_train/rejected': '-142.99', 'logps_train/chosen': '-171.68', 'loss/train': '0.63112', 'examples_per_second': '29.89', 'grad_norm': '27.875', 'counters/examples': 242272, 'counters/updates': 7571}
train stats after 242304 examples: {'rewards_train/chosen': '0.21735', 'rewards_train/rejected': '0.042915', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17443', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-168.97', 'loss/train': '0.6215', 'examples_per_second': '30.229', 'grad_norm': '31.5', 'counters/examples': 242304, 'counters/updates': 7572}
train stats after 242336 examples: {'rewards_train/chosen': '0.16957', 'rewards_train/rejected': '0.027101', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14247', 'logps_train/rejected': '-111.37', 'logps_train/chosen': '-148.89', 'loss/train': '0.63834', 'examples_per_second': '30.644', 'grad_norm': '23.875', 'counters/examples': 242336, 'counters/updates': 7573}
train stats after 242368 examples: {'rewards_train/chosen': '0.043803', 'rewards_train/rejected': '0.036357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0074459', 'logps_train/rejected': '-103.1', 'logps_train/chosen': '-136.83', 'loss/train': '0.70441', 'examples_per_second': '32.262', 'grad_norm': '26.75', 'counters/examples': 242368, 'counters/updates': 7574}
train stats after 242400 examples: {'rewards_train/chosen': '0.15207', 'rewards_train/rejected': '-0.012642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16471', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-150.16', 'loss/train': '0.63323', 'examples_per_second': '31.569', 'grad_norm': '25.75', 'counters/examples': 242400, 'counters/updates': 7575}
train stats after 242432 examples: {'rewards_train/chosen': '0.14992', 'rewards_train/rejected': '0.052334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097589', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-98.808', 'loss/train': '0.65933', 'examples_per_second': '31.619', 'grad_norm': '23.375', 'counters/examples': 242432, 'counters/updates': 7576}
train stats after 242464 examples: {'rewards_train/chosen': '0.18057', 'rewards_train/rejected': '0.023963', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15661', 'logps_train/rejected': '-123.88', 'logps_train/chosen': '-151.1', 'loss/train': '0.63', 'examples_per_second': '30.297', 'grad_norm': '26.625', 'counters/examples': 242464, 'counters/updates': 7577}
train stats after 242496 examples: {'rewards_train/chosen': '0.086211', 'rewards_train/rejected': '0.027826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058385', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-136.57', 'loss/train': '0.67749', 'examples_per_second': '30.833', 'grad_norm': '32.5', 'counters/examples': 242496, 'counters/updates': 7578}
train stats after 242528 examples: {'rewards_train/chosen': '0.17759', 'rewards_train/rejected': '0.006591', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.171', 'logps_train/rejected': '-109.59', 'logps_train/chosen': '-140.12', 'loss/train': '0.62784', 'examples_per_second': '31.612', 'grad_norm': '25.875', 'counters/examples': 242528, 'counters/updates': 7579}
skipping logging after 242560 examples to avoid logging too frequently
train stats after 242592 examples: {'rewards_train/chosen': '0.15432', 'rewards_train/rejected': '0.051638', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10268', 'logps_train/rejected': '-105.75', 'logps_train/chosen': '-131.97', 'loss/train': '0.65388', 'examples_per_second': '30.332', 'grad_norm': '27.875', 'counters/examples': 242592, 'counters/updates': 7581}
train stats after 242624 examples: {'rewards_train/chosen': '0.14222', 'rewards_train/rejected': '-0.015293', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15751', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-127.44', 'loss/train': '0.63196', 'examples_per_second': '30.767', 'grad_norm': '27.125', 'counters/examples': 242624, 'counters/updates': 7582}
train stats after 242656 examples: {'rewards_train/chosen': '0.13979', 'rewards_train/rejected': '-0.017245', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15704', 'logps_train/rejected': '-99.187', 'logps_train/chosen': '-117.94', 'loss/train': '0.63434', 'examples_per_second': '30.106', 'grad_norm': '23.25', 'counters/examples': 242656, 'counters/updates': 7583}
train stats after 242688 examples: {'rewards_train/chosen': '0.12144', 'rewards_train/rejected': '-0.032131', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15358', 'logps_train/rejected': '-101.09', 'logps_train/chosen': '-125.09', 'loss/train': '0.63256', 'examples_per_second': '31.624', 'grad_norm': '24.5', 'counters/examples': 242688, 'counters/updates': 7584}
train stats after 242720 examples: {'rewards_train/chosen': '0.087476', 'rewards_train/rejected': '-0.0023553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089831', 'logps_train/rejected': '-92.722', 'logps_train/chosen': '-110.82', 'loss/train': '0.66383', 'examples_per_second': '30.696', 'grad_norm': '22', 'counters/examples': 242720, 'counters/updates': 7585}
train stats after 242752 examples: {'rewards_train/chosen': '0.10393', 'rewards_train/rejected': '-0.065461', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1694', 'logps_train/rejected': '-144.11', 'logps_train/chosen': '-114.98', 'loss/train': '0.62622', 'examples_per_second': '32.644', 'grad_norm': '23.625', 'counters/examples': 242752, 'counters/updates': 7586}
train stats after 242784 examples: {'rewards_train/chosen': '0.13878', 'rewards_train/rejected': '0.1017', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037082', 'logps_train/rejected': '-103.22', 'logps_train/chosen': '-133.04', 'loss/train': '0.68407', 'examples_per_second': '32.947', 'grad_norm': '24.625', 'counters/examples': 242784, 'counters/updates': 7587}
skipping logging after 242816 examples to avoid logging too frequently
train stats after 242848 examples: {'rewards_train/chosen': '0.17028', 'rewards_train/rejected': '0.10551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064775', 'logps_train/rejected': '-144.21', 'logps_train/chosen': '-155.63', 'loss/train': '0.68769', 'examples_per_second': '31.609', 'grad_norm': '28.25', 'counters/examples': 242848, 'counters/updates': 7589}
train stats after 242880 examples: {'rewards_train/chosen': '0.099903', 'rewards_train/rejected': '0.031994', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067908', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-121.05', 'loss/train': '0.67654', 'examples_per_second': '31.304', 'grad_norm': '27.5', 'counters/examples': 242880, 'counters/updates': 7590}
train stats after 242912 examples: {'rewards_train/chosen': '0.16974', 'rewards_train/rejected': '0.0043315', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16541', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-160.1', 'loss/train': '0.62721', 'examples_per_second': '31.601', 'grad_norm': '25.5', 'counters/examples': 242912, 'counters/updates': 7591}
train stats after 242944 examples: {'rewards_train/chosen': '0.13032', 'rewards_train/rejected': '0.00266', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12766', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-132.04', 'loss/train': '0.63918', 'examples_per_second': '31.42', 'grad_norm': '25.125', 'counters/examples': 242944, 'counters/updates': 7592}
train stats after 242976 examples: {'rewards_train/chosen': '0.11282', 'rewards_train/rejected': '0.053362', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059454', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-142.4', 'loss/train': '0.68394', 'examples_per_second': '31.61', 'grad_norm': '39.5', 'counters/examples': 242976, 'counters/updates': 7593}
train stats after 243008 examples: {'rewards_train/chosen': '0.15991', 'rewards_train/rejected': '0.032753', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12715', 'logps_train/rejected': '-120.44', 'logps_train/chosen': '-135', 'loss/train': '0.64715', 'examples_per_second': '31.447', 'grad_norm': '34.25', 'counters/examples': 243008, 'counters/updates': 7594}
skipping logging after 243040 examples to avoid logging too frequently
train stats after 243072 examples: {'rewards_train/chosen': '0.077797', 'rewards_train/rejected': '0.049621', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028176', 'logps_train/rejected': '-110.43', 'logps_train/chosen': '-147.29', 'loss/train': '0.69441', 'examples_per_second': '34.297', 'grad_norm': '27.5', 'counters/examples': 243072, 'counters/updates': 7596}
train stats after 243104 examples: {'rewards_train/chosen': '0.065442', 'rewards_train/rejected': '-0.021651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087093', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-152.07', 'loss/train': '0.67158', 'examples_per_second': '31.611', 'grad_norm': '28.125', 'counters/examples': 243104, 'counters/updates': 7597}
train stats after 243136 examples: {'rewards_train/chosen': '0.22597', 'rewards_train/rejected': '-0.014883', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24085', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-144.18', 'loss/train': '0.59177', 'examples_per_second': '31.586', 'grad_norm': '24.5', 'counters/examples': 243136, 'counters/updates': 7598}
train stats after 243168 examples: {'rewards_train/chosen': '0.14663', 'rewards_train/rejected': '0.087761', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058872', 'logps_train/rejected': '-137.13', 'logps_train/chosen': '-155.11', 'loss/train': '0.68765', 'examples_per_second': '32.522', 'grad_norm': '27.625', 'counters/examples': 243168, 'counters/updates': 7599}
train stats after 243200 examples: {'rewards_train/chosen': '0.16597', 'rewards_train/rejected': '-0.0020862', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16805', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-103.99', 'loss/train': '0.62062', 'examples_per_second': '30.269', 'grad_norm': '22.125', 'counters/examples': 243200, 'counters/updates': 7600}
train stats after 243232 examples: {'rewards_train/chosen': '0.13934', 'rewards_train/rejected': '-0.023645', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16298', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-144.07', 'loss/train': '0.63278', 'examples_per_second': '31.085', 'grad_norm': '26.5', 'counters/examples': 243232, 'counters/updates': 7601}
train stats after 243264 examples: {'rewards_train/chosen': '0.056939', 'rewards_train/rejected': '-0.005253', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062192', 'logps_train/rejected': '-121.56', 'logps_train/chosen': '-124.41', 'loss/train': '0.67148', 'examples_per_second': '32.905', 'grad_norm': '29.875', 'counters/examples': 243264, 'counters/updates': 7602}
train stats after 243296 examples: {'rewards_train/chosen': '0.054711', 'rewards_train/rejected': '0.05499', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.00027844', 'logps_train/rejected': '-124.89', 'logps_train/chosen': '-151', 'loss/train': '0.71735', 'examples_per_second': '31.313', 'grad_norm': '29.5', 'counters/examples': 243296, 'counters/updates': 7603}
train stats after 243328 examples: {'rewards_train/chosen': '0.2362', 'rewards_train/rejected': '0.13364', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10255', 'logps_train/rejected': '-101.75', 'logps_train/chosen': '-135.95', 'loss/train': '0.66671', 'examples_per_second': '31.658', 'grad_norm': '25.5', 'counters/examples': 243328, 'counters/updates': 7604}
train stats after 243360 examples: {'rewards_train/chosen': '0.25263', 'rewards_train/rejected': '0.055011', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19762', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-138.5', 'loss/train': '0.61397', 'examples_per_second': '31.452', 'grad_norm': '24.375', 'counters/examples': 243360, 'counters/updates': 7605}
skipping logging after 243392 examples to avoid logging too frequently
train stats after 243424 examples: {'rewards_train/chosen': '0.084273', 'rewards_train/rejected': '0.089037', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0047647', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-124.38', 'loss/train': '0.71777', 'examples_per_second': '30.934', 'grad_norm': '27.75', 'counters/examples': 243424, 'counters/updates': 7607}
train stats after 243456 examples: {'rewards_train/chosen': '0.18448', 'rewards_train/rejected': '0.010693', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17378', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-151.64', 'loss/train': '0.63082', 'examples_per_second': '31.606', 'grad_norm': '27.75', 'counters/examples': 243456, 'counters/updates': 7608}
train stats after 243488 examples: {'rewards_train/chosen': '0.15422', 'rewards_train/rejected': '0.055928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09829', 'logps_train/rejected': '-104.18', 'logps_train/chosen': '-114.67', 'loss/train': '0.66048', 'examples_per_second': '31.729', 'grad_norm': '21', 'counters/examples': 243488, 'counters/updates': 7609}
skipping logging after 243520 examples to avoid logging too frequently
train stats after 243552 examples: {'rewards_train/chosen': '0.16959', 'rewards_train/rejected': '0.049742', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.11985', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-162.14', 'loss/train': '0.65589', 'examples_per_second': '30.992', 'grad_norm': '26.25', 'counters/examples': 243552, 'counters/updates': 7611}
train stats after 243584 examples: {'rewards_train/chosen': '0.14345', 'rewards_train/rejected': '-0.0019249', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14537', 'logps_train/rejected': '-102.92', 'logps_train/chosen': '-123.98', 'loss/train': '0.63579', 'examples_per_second': '31.475', 'grad_norm': '23.375', 'counters/examples': 243584, 'counters/updates': 7612}
train stats after 243616 examples: {'rewards_train/chosen': '0.20463', 'rewards_train/rejected': '0.012455', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19218', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-144.72', 'loss/train': '0.61827', 'examples_per_second': '30.338', 'grad_norm': '23.25', 'counters/examples': 243616, 'counters/updates': 7613}
train stats after 243648 examples: {'rewards_train/chosen': '0.12824', 'rewards_train/rejected': '0.037836', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090401', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-150.5', 'loss/train': '0.65905', 'examples_per_second': '30.506', 'grad_norm': '27.25', 'counters/examples': 243648, 'counters/updates': 7614}
train stats after 243680 examples: {'rewards_train/chosen': '0.10792', 'rewards_train/rejected': '-0.013723', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12164', 'logps_train/rejected': '-103.01', 'logps_train/chosen': '-140.6', 'loss/train': '0.6439', 'examples_per_second': '31.636', 'grad_norm': '24.125', 'counters/examples': 243680, 'counters/updates': 7615}
train stats after 243712 examples: {'rewards_train/chosen': '0.13905', 'rewards_train/rejected': '-0.091394', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23045', 'logps_train/rejected': '-91.48', 'logps_train/chosen': '-124.6', 'loss/train': '0.60197', 'examples_per_second': '32.556', 'grad_norm': '21', 'counters/examples': 243712, 'counters/updates': 7616}
train stats after 243744 examples: {'rewards_train/chosen': '0.11515', 'rewards_train/rejected': '0.021244', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093903', 'logps_train/rejected': '-128.45', 'logps_train/chosen': '-152.5', 'loss/train': '0.6672', 'examples_per_second': '30.218', 'grad_norm': '27.875', 'counters/examples': 243744, 'counters/updates': 7617}
train stats after 243776 examples: {'rewards_train/chosen': '0.099982', 'rewards_train/rejected': '-0.031475', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13146', 'logps_train/rejected': '-91.809', 'logps_train/chosen': '-99.823', 'loss/train': '0.63701', 'examples_per_second': '31.622', 'grad_norm': '23.875', 'counters/examples': 243776, 'counters/updates': 7618}
skipping logging after 243808 examples to avoid logging too frequently
train stats after 243840 examples: {'rewards_train/chosen': '0.16404', 'rewards_train/rejected': '0.030663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-133.93', 'loss/train': '0.64689', 'examples_per_second': '31.522', 'grad_norm': '25.5', 'counters/examples': 243840, 'counters/updates': 7620}
train stats after 243872 examples: {'rewards_train/chosen': '0.066949', 'rewards_train/rejected': '0.13955', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.072598', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-133.68', 'loss/train': '0.75111', 'examples_per_second': '31.597', 'grad_norm': '30.25', 'counters/examples': 243872, 'counters/updates': 7621}
train stats after 243904 examples: {'rewards_train/chosen': '0.11092', 'rewards_train/rejected': '-0.040178', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1511', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-142.41', 'loss/train': '0.63489', 'examples_per_second': '31.579', 'grad_norm': '25.25', 'counters/examples': 243904, 'counters/updates': 7622}
train stats after 243936 examples: {'rewards_train/chosen': '0.1516', 'rewards_train/rejected': '-0.010664', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16226', 'logps_train/rejected': '-113.21', 'logps_train/chosen': '-128.15', 'loss/train': '0.62997', 'examples_per_second': '32.756', 'grad_norm': '23.625', 'counters/examples': 243936, 'counters/updates': 7623}
train stats after 243968 examples: {'rewards_train/chosen': '0.2784', 'rewards_train/rejected': '0.042283', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23611', 'logps_train/rejected': '-121.27', 'logps_train/chosen': '-169.89', 'loss/train': '0.59037', 'examples_per_second': '30.627', 'grad_norm': '23.75', 'counters/examples': 243968, 'counters/updates': 7624}
train stats after 244000 examples: {'rewards_train/chosen': '0.19683', 'rewards_train/rejected': '-0.0023086', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19914', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-142.23', 'loss/train': '0.62025', 'examples_per_second': '31.575', 'grad_norm': '24.125', 'counters/examples': 244000, 'counters/updates': 7625}
train stats after 244032 examples: {'rewards_train/chosen': '0.2237', 'rewards_train/rejected': '0.04379', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17991', 'logps_train/rejected': '-133.08', 'logps_train/chosen': '-148.75', 'loss/train': '0.63026', 'examples_per_second': '30.574', 'grad_norm': '24.75', 'counters/examples': 244032, 'counters/updates': 7626}
skipping logging after 244064 examples to avoid logging too frequently
train stats after 244096 examples: {'rewards_train/chosen': '0.22702', 'rewards_train/rejected': '0.080613', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14641', 'logps_train/rejected': '-119.07', 'logps_train/chosen': '-143.83', 'loss/train': '0.64104', 'examples_per_second': '30.943', 'grad_norm': '27.25', 'counters/examples': 244096, 'counters/updates': 7628}
train stats after 244128 examples: {'rewards_train/chosen': '0.024906', 'rewards_train/rejected': '-0.064199', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.089105', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-135.22', 'loss/train': '0.66703', 'examples_per_second': '31.729', 'grad_norm': '25.875', 'counters/examples': 244128, 'counters/updates': 7629}
train stats after 244160 examples: {'rewards_train/chosen': '0.12718', 'rewards_train/rejected': '0.038064', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089111', 'logps_train/rejected': '-104.24', 'logps_train/chosen': '-120.5', 'loss/train': '0.66266', 'examples_per_second': '32.205', 'grad_norm': '22.375', 'counters/examples': 244160, 'counters/updates': 7630}
train stats after 244192 examples: {'rewards_train/chosen': '0.13574', 'rewards_train/rejected': '-0.010167', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14591', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-123.76', 'loss/train': '0.6368', 'examples_per_second': '30.547', 'grad_norm': '24.625', 'counters/examples': 244192, 'counters/updates': 7631}
train stats after 244224 examples: {'rewards_train/chosen': '0.13631', 'rewards_train/rejected': '0.066802', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06951', 'logps_train/rejected': '-132.02', 'logps_train/chosen': '-146.63', 'loss/train': '0.68562', 'examples_per_second': '30.652', 'grad_norm': '28.875', 'counters/examples': 244224, 'counters/updates': 7632}
skipping logging after 244256 examples to avoid logging too frequently
train stats after 244288 examples: {'rewards_train/chosen': '0.13344', 'rewards_train/rejected': '0.024626', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10881', 'logps_train/rejected': '-128.71', 'logps_train/chosen': '-135.05', 'loss/train': '0.65532', 'examples_per_second': '33.065', 'grad_norm': '26.5', 'counters/examples': 244288, 'counters/updates': 7634}
skipping logging after 244320 examples to avoid logging too frequently
train stats after 244352 examples: {'rewards_train/chosen': '0.12013', 'rewards_train/rejected': '0.0042566', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11587', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-117.67', 'loss/train': '0.65692', 'examples_per_second': '33.717', 'grad_norm': '25.25', 'counters/examples': 244352, 'counters/updates': 7636}
skipping logging after 244384 examples to avoid logging too frequently
train stats after 244416 examples: {'rewards_train/chosen': '0.17507', 'rewards_train/rejected': '0.109', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066071', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-142.8', 'loss/train': '0.6962', 'examples_per_second': '30.569', 'grad_norm': '26.125', 'counters/examples': 244416, 'counters/updates': 7638}
train stats after 244448 examples: {'rewards_train/chosen': '0.13087', 'rewards_train/rejected': '0.034688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096178', 'logps_train/rejected': '-108.02', 'logps_train/chosen': '-123.49', 'loss/train': '0.66129', 'examples_per_second': '30.079', 'grad_norm': '22.75', 'counters/examples': 244448, 'counters/updates': 7639}
train stats after 244480 examples: {'rewards_train/chosen': '0.16934', 'rewards_train/rejected': '-0.10887', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.27821', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-148.66', 'loss/train': '0.5837', 'examples_per_second': '31.743', 'grad_norm': '22', 'counters/examples': 244480, 'counters/updates': 7640}
train stats after 244512 examples: {'rewards_train/chosen': '0.24454', 'rewards_train/rejected': '-0.027233', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27177', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-149.75', 'loss/train': '0.58891', 'examples_per_second': '32.115', 'grad_norm': '22', 'counters/examples': 244512, 'counters/updates': 7641}
train stats after 244544 examples: {'rewards_train/chosen': '0.24694', 'rewards_train/rejected': '0.054722', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19222', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-151.24', 'loss/train': '0.61739', 'examples_per_second': '31.59', 'grad_norm': '23.75', 'counters/examples': 244544, 'counters/updates': 7642}
train stats after 244576 examples: {'rewards_train/chosen': '0.12401', 'rewards_train/rejected': '0.047793', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076215', 'logps_train/rejected': '-121.13', 'logps_train/chosen': '-130.53', 'loss/train': '0.66891', 'examples_per_second': '31.793', 'grad_norm': '23.375', 'counters/examples': 244576, 'counters/updates': 7643}
train stats after 244608 examples: {'rewards_train/chosen': '0.15797', 'rewards_train/rejected': '0.052037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10593', 'logps_train/rejected': '-138.52', 'logps_train/chosen': '-122.51', 'loss/train': '0.65918', 'examples_per_second': '31.406', 'grad_norm': '28.625', 'counters/examples': 244608, 'counters/updates': 7644}
train stats after 244640 examples: {'rewards_train/chosen': '0.27613', 'rewards_train/rejected': '0.048187', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22794', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-136', 'loss/train': '0.6073', 'examples_per_second': '31.134', 'grad_norm': '24.75', 'counters/examples': 244640, 'counters/updates': 7645}
train stats after 244672 examples: {'rewards_train/chosen': '0.19531', 'rewards_train/rejected': '0.080007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1153', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-123.31', 'loss/train': '0.65647', 'examples_per_second': '33.118', 'grad_norm': '28.5', 'counters/examples': 244672, 'counters/updates': 7646}
train stats after 244704 examples: {'rewards_train/chosen': '0.13725', 'rewards_train/rejected': '0.046599', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090653', 'logps_train/rejected': '-114.87', 'logps_train/chosen': '-105.3', 'loss/train': '0.66673', 'examples_per_second': '33.298', 'grad_norm': '23.375', 'counters/examples': 244704, 'counters/updates': 7647}
train stats after 244736 examples: {'rewards_train/chosen': '0.067585', 'rewards_train/rejected': '0.052692', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014893', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-121.23', 'loss/train': '0.69807', 'examples_per_second': '31.616', 'grad_norm': '24', 'counters/examples': 244736, 'counters/updates': 7648}
train stats after 244768 examples: {'rewards_train/chosen': '0.20635', 'rewards_train/rejected': '0.023485', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18286', 'logps_train/rejected': '-130.29', 'logps_train/chosen': '-126.7', 'loss/train': '0.638', 'examples_per_second': '30.997', 'grad_norm': '26.5', 'counters/examples': 244768, 'counters/updates': 7649}
train stats after 244800 examples: {'rewards_train/chosen': '0.13262', 'rewards_train/rejected': '0.02717', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10545', 'logps_train/rejected': '-109.75', 'logps_train/chosen': '-121.52', 'loss/train': '0.65111', 'examples_per_second': '32.335', 'grad_norm': '29.25', 'counters/examples': 244800, 'counters/updates': 7650}
train stats after 244832 examples: {'rewards_train/chosen': '0.029838', 'rewards_train/rejected': '-0.088378', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11822', 'logps_train/rejected': '-153.26', 'logps_train/chosen': '-138.04', 'loss/train': '0.64919', 'examples_per_second': '31.698', 'grad_norm': '26.5', 'counters/examples': 244832, 'counters/updates': 7651}
skipping logging after 244864 examples to avoid logging too frequently
train stats after 244896 examples: {'rewards_train/chosen': '0.14675', 'rewards_train/rejected': '-0.090019', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.23677', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-160.41', 'loss/train': '0.59538', 'examples_per_second': '30.629', 'grad_norm': '24.875', 'counters/examples': 244896, 'counters/updates': 7653}
train stats after 244928 examples: {'rewards_train/chosen': '0.08301', 'rewards_train/rejected': '0.026485', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056525', 'logps_train/rejected': '-112.91', 'logps_train/chosen': '-118.35', 'loss/train': '0.67723', 'examples_per_second': '31.24', 'grad_norm': '24.25', 'counters/examples': 244928, 'counters/updates': 7654}
train stats after 244960 examples: {'rewards_train/chosen': '0.21414', 'rewards_train/rejected': '0.11386', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10028', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-149.57', 'loss/train': '0.65702', 'examples_per_second': '31.553', 'grad_norm': '30.125', 'counters/examples': 244960, 'counters/updates': 7655}
train stats after 244992 examples: {'rewards_train/chosen': '0.052203', 'rewards_train/rejected': '-0.061834', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11404', 'logps_train/rejected': '-92.462', 'logps_train/chosen': '-141.92', 'loss/train': '0.65589', 'examples_per_second': '31.633', 'grad_norm': '22.375', 'counters/examples': 244992, 'counters/updates': 7656}
train stats after 245024 examples: {'rewards_train/chosen': '0.087835', 'rewards_train/rejected': '0.057906', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029928', 'logps_train/rejected': '-119.37', 'logps_train/chosen': '-145.2', 'loss/train': '0.69867', 'examples_per_second': '32.224', 'grad_norm': '29.25', 'counters/examples': 245024, 'counters/updates': 7657}
train stats after 245056 examples: {'rewards_train/chosen': '0.041611', 'rewards_train/rejected': '-0.095618', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13723', 'logps_train/rejected': '-105.89', 'logps_train/chosen': '-142.71', 'loss/train': '0.6398', 'examples_per_second': '32.996', 'grad_norm': '23.5', 'counters/examples': 245056, 'counters/updates': 7658}
train stats after 245088 examples: {'rewards_train/chosen': '0.14453', 'rewards_train/rejected': '-0.020709', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16524', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-146.68', 'loss/train': '0.62912', 'examples_per_second': '31.621', 'grad_norm': '29.75', 'counters/examples': 245088, 'counters/updates': 7659}
train stats after 245120 examples: {'rewards_train/chosen': '0.092676', 'rewards_train/rejected': '0.061014', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031662', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-157.64', 'loss/train': '0.6848', 'examples_per_second': '30.934', 'grad_norm': '33', 'counters/examples': 245120, 'counters/updates': 7660}
train stats after 245152 examples: {'rewards_train/chosen': '0.16958', 'rewards_train/rejected': '-0.024388', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19397', 'logps_train/rejected': '-104.48', 'logps_train/chosen': '-145.53', 'loss/train': '0.61096', 'examples_per_second': '31.241', 'grad_norm': '24.5', 'counters/examples': 245152, 'counters/updates': 7661}
train stats after 245184 examples: {'rewards_train/chosen': '0.084473', 'rewards_train/rejected': '0.079008', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0054649', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-119.24', 'loss/train': '0.70022', 'examples_per_second': '31.071', 'grad_norm': '26.75', 'counters/examples': 245184, 'counters/updates': 7662}
skipping logging after 245216 examples to avoid logging too frequently
train stats after 245248 examples: {'rewards_train/chosen': '0.147', 'rewards_train/rejected': '0.00031429', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14669', 'logps_train/rejected': '-144.35', 'logps_train/chosen': '-142.75', 'loss/train': '0.64273', 'examples_per_second': '30.593', 'grad_norm': '27', 'counters/examples': 245248, 'counters/updates': 7664}
train stats after 245280 examples: {'rewards_train/chosen': '0.16547', 'rewards_train/rejected': '0.14842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017056', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-136.11', 'loss/train': '0.69873', 'examples_per_second': '31.357', 'grad_norm': '25.5', 'counters/examples': 245280, 'counters/updates': 7665}
train stats after 245312 examples: {'rewards_train/chosen': '0.12689', 'rewards_train/rejected': '0.051612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075281', 'logps_train/rejected': '-112.59', 'logps_train/chosen': '-159.49', 'loss/train': '0.67431', 'examples_per_second': '30.714', 'grad_norm': '28.5', 'counters/examples': 245312, 'counters/updates': 7666}
train stats after 245344 examples: {'rewards_train/chosen': '0.19279', 'rewards_train/rejected': '0.010135', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18266', 'logps_train/rejected': '-109.63', 'logps_train/chosen': '-167.1', 'loss/train': '0.61963', 'examples_per_second': '31.58', 'grad_norm': '24', 'counters/examples': 245344, 'counters/updates': 7667}
train stats after 245376 examples: {'rewards_train/chosen': '0.11152', 'rewards_train/rejected': '0.022326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089196', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-134.07', 'loss/train': '0.66449', 'examples_per_second': '31.556', 'grad_norm': '24.625', 'counters/examples': 245376, 'counters/updates': 7668}
train stats after 245408 examples: {'rewards_train/chosen': '0.06231', 'rewards_train/rejected': '-0.082377', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14469', 'logps_train/rejected': '-95.376', 'logps_train/chosen': '-125.1', 'loss/train': '0.63344', 'examples_per_second': '31.457', 'grad_norm': '23.625', 'counters/examples': 245408, 'counters/updates': 7669}
train stats after 245440 examples: {'rewards_train/chosen': '0.083376', 'rewards_train/rejected': '0.0094948', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073882', 'logps_train/rejected': '-126.15', 'logps_train/chosen': '-129.86', 'loss/train': '0.67043', 'examples_per_second': '33.358', 'grad_norm': '25.5', 'counters/examples': 245440, 'counters/updates': 7670}
train stats after 245472 examples: {'rewards_train/chosen': '0.086411', 'rewards_train/rejected': '-0.019477', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10589', 'logps_train/rejected': '-107.12', 'logps_train/chosen': '-140.38', 'loss/train': '0.66359', 'examples_per_second': '31.586', 'grad_norm': '23.5', 'counters/examples': 245472, 'counters/updates': 7671}
train stats after 245504 examples: {'rewards_train/chosen': '0.040045', 'rewards_train/rejected': '0.0068151', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033229', 'logps_train/rejected': '-124.68', 'logps_train/chosen': '-125.43', 'loss/train': '0.68543', 'examples_per_second': '31.206', 'grad_norm': '26.375', 'counters/examples': 245504, 'counters/updates': 7672}
train stats after 245536 examples: {'rewards_train/chosen': '0.099931', 'rewards_train/rejected': '0.073541', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02639', 'logps_train/rejected': '-143.72', 'logps_train/chosen': '-148.79', 'loss/train': '0.68652', 'examples_per_second': '31.094', 'grad_norm': '28.5', 'counters/examples': 245536, 'counters/updates': 7673}
train stats after 245568 examples: {'rewards_train/chosen': '0.12716', 'rewards_train/rejected': '0.051298', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075861', 'logps_train/rejected': '-151.14', 'logps_train/chosen': '-169.98', 'loss/train': '0.6722', 'examples_per_second': '31.413', 'grad_norm': '33.25', 'counters/examples': 245568, 'counters/updates': 7674}
train stats after 245600 examples: {'rewards_train/chosen': '0.15189', 'rewards_train/rejected': '-0.084391', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23628', 'logps_train/rejected': '-100.18', 'logps_train/chosen': '-121.79', 'loss/train': '0.59433', 'examples_per_second': '32.311', 'grad_norm': '20.75', 'counters/examples': 245600, 'counters/updates': 7675}
train stats after 245632 examples: {'rewards_train/chosen': '0.12031', 'rewards_train/rejected': '0.10288', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01743', 'logps_train/rejected': '-101.45', 'logps_train/chosen': '-132.37', 'loss/train': '0.69315', 'examples_per_second': '31.4', 'grad_norm': '27.125', 'counters/examples': 245632, 'counters/updates': 7676}
train stats after 245664 examples: {'rewards_train/chosen': '0.12393', 'rewards_train/rejected': '0.13614', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.012219', 'logps_train/rejected': '-151.96', 'logps_train/chosen': '-159.26', 'loss/train': '0.73436', 'examples_per_second': '31.442', 'grad_norm': '33.75', 'counters/examples': 245664, 'counters/updates': 7677}
train stats after 245696 examples: {'rewards_train/chosen': '0.13584', 'rewards_train/rejected': '-0.07892', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21476', 'logps_train/rejected': '-124.45', 'logps_train/chosen': '-150.15', 'loss/train': '0.60798', 'examples_per_second': '31.569', 'grad_norm': '31.875', 'counters/examples': 245696, 'counters/updates': 7678}
train stats after 245728 examples: {'rewards_train/chosen': '0.15975', 'rewards_train/rejected': '0.059458', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1003', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-160.22', 'loss/train': '0.66397', 'examples_per_second': '31.558', 'grad_norm': '28.625', 'counters/examples': 245728, 'counters/updates': 7679}
train stats after 245760 examples: {'rewards_train/chosen': '0.12577', 'rewards_train/rejected': '-0.015169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14094', 'logps_train/rejected': '-106.42', 'logps_train/chosen': '-140.84', 'loss/train': '0.64287', 'examples_per_second': '31.969', 'grad_norm': '25.625', 'counters/examples': 245760, 'counters/updates': 7680}
train stats after 245792 examples: {'rewards_train/chosen': '0.19592', 'rewards_train/rejected': '-0.020032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21596', 'logps_train/rejected': '-138.84', 'logps_train/chosen': '-150.08', 'loss/train': '0.60237', 'examples_per_second': '32.444', 'grad_norm': '24.875', 'counters/examples': 245792, 'counters/updates': 7681}
train stats after 245824 examples: {'rewards_train/chosen': '0.042867', 'rewards_train/rejected': '0.06873', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025863', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-118.95', 'loss/train': '0.71713', 'examples_per_second': '32.751', 'grad_norm': '27.375', 'counters/examples': 245824, 'counters/updates': 7682}
train stats after 245856 examples: {'rewards_train/chosen': '0.026894', 'rewards_train/rejected': '0.002601', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024293', 'logps_train/rejected': '-84.953', 'logps_train/chosen': '-136.78', 'loss/train': '0.69499', 'examples_per_second': '31.408', 'grad_norm': '24.875', 'counters/examples': 245856, 'counters/updates': 7683}
skipping logging after 245888 examples to avoid logging too frequently
train stats after 245920 examples: {'rewards_train/chosen': '0.057131', 'rewards_train/rejected': '-0.024986', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082117', 'logps_train/rejected': '-93.721', 'logps_train/chosen': '-114.47', 'loss/train': '0.66052', 'examples_per_second': '35.666', 'grad_norm': '21.5', 'counters/examples': 245920, 'counters/updates': 7685}
train stats after 245952 examples: {'rewards_train/chosen': '0.20538', 'rewards_train/rejected': '0.046637', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15874', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-140.25', 'loss/train': '0.6399', 'examples_per_second': '31.531', 'grad_norm': '26.125', 'counters/examples': 245952, 'counters/updates': 7686}
train stats after 245984 examples: {'rewards_train/chosen': '0.17396', 'rewards_train/rejected': '0.11656', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0574', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-126.68', 'loss/train': '0.67275', 'examples_per_second': '31.549', 'grad_norm': '25.375', 'counters/examples': 245984, 'counters/updates': 7687}
train stats after 246016 examples: {'rewards_train/chosen': '0.11275', 'rewards_train/rejected': '-0.0016625', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11441', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-129', 'loss/train': '0.65968', 'examples_per_second': '32.523', 'grad_norm': '27.125', 'counters/examples': 246016, 'counters/updates': 7688}
skipping logging after 246048 examples to avoid logging too frequently
train stats after 246080 examples: {'rewards_train/chosen': '0.14198', 'rewards_train/rejected': '0.080388', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061591', 'logps_train/rejected': '-153.15', 'logps_train/chosen': '-142.62', 'loss/train': '0.67167', 'examples_per_second': '30.51', 'grad_norm': '28.125', 'counters/examples': 246080, 'counters/updates': 7690}
train stats after 246112 examples: {'rewards_train/chosen': '0.13837', 'rewards_train/rejected': '-0.015026', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1534', 'logps_train/rejected': '-90.248', 'logps_train/chosen': '-137.31', 'loss/train': '0.63382', 'examples_per_second': '22.791', 'grad_norm': '22.875', 'counters/examples': 246112, 'counters/updates': 7691}
train stats after 246144 examples: {'rewards_train/chosen': '0.18749', 'rewards_train/rejected': '0.029021', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15847', 'logps_train/rejected': '-97.255', 'logps_train/chosen': '-127.43', 'loss/train': '0.63358', 'examples_per_second': '30.956', 'grad_norm': '21', 'counters/examples': 246144, 'counters/updates': 7692}
skipping logging after 246176 examples to avoid logging too frequently
train stats after 246208 examples: {'rewards_train/chosen': '0.19692', 'rewards_train/rejected': '-0.023843', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22077', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-136.06', 'loss/train': '0.61152', 'examples_per_second': '24.553', 'grad_norm': '23.75', 'counters/examples': 246208, 'counters/updates': 7694}
train stats after 246240 examples: {'rewards_train/chosen': '0.10272', 'rewards_train/rejected': '-0.021252', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12397', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-105.57', 'loss/train': '0.65466', 'examples_per_second': '32.406', 'grad_norm': '24.125', 'counters/examples': 246240, 'counters/updates': 7695}
train stats after 246272 examples: {'rewards_train/chosen': '0.14708', 'rewards_train/rejected': '-0.08746', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23454', 'logps_train/rejected': '-87.684', 'logps_train/chosen': '-131.62', 'loss/train': '0.59495', 'examples_per_second': '32.279', 'grad_norm': '21.75', 'counters/examples': 246272, 'counters/updates': 7696}
train stats after 246304 examples: {'rewards_train/chosen': '0.18823', 'rewards_train/rejected': '0.052881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13535', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-180.23', 'loss/train': '0.65197', 'examples_per_second': '30.603', 'grad_norm': '26.875', 'counters/examples': 246304, 'counters/updates': 7697}
train stats after 246336 examples: {'rewards_train/chosen': '0.2926', 'rewards_train/rejected': '0.064505', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2281', 'logps_train/rejected': '-111.14', 'logps_train/chosen': '-146.97', 'loss/train': '0.60937', 'examples_per_second': '30.974', 'grad_norm': '24.375', 'counters/examples': 246336, 'counters/updates': 7698}
train stats after 246368 examples: {'rewards_train/chosen': '0.08079', 'rewards_train/rejected': '0.022071', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058719', 'logps_train/rejected': '-145.05', 'logps_train/chosen': '-108.22', 'loss/train': '0.68369', 'examples_per_second': '31.396', 'grad_norm': '27.125', 'counters/examples': 246368, 'counters/updates': 7699}
train stats after 246400 examples: {'rewards_train/chosen': '0.1239', 'rewards_train/rejected': '-0.016711', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14061', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-115.14', 'loss/train': '0.63923', 'examples_per_second': '32.993', 'grad_norm': '24.375', 'counters/examples': 246400, 'counters/updates': 7700}
train stats after 246432 examples: {'rewards_train/chosen': '0.18214', 'rewards_train/rejected': '-0.026292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20844', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-163.22', 'loss/train': '0.62002', 'examples_per_second': '30.566', 'grad_norm': '23.625', 'counters/examples': 246432, 'counters/updates': 7701}
skipping logging after 246464 examples to avoid logging too frequently
train stats after 246496 examples: {'rewards_train/chosen': '0.11268', 'rewards_train/rejected': '0.06758', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045096', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-117.45', 'loss/train': '0.68997', 'examples_per_second': '31.54', 'grad_norm': '24.5', 'counters/examples': 246496, 'counters/updates': 7703}
train stats after 246528 examples: {'rewards_train/chosen': '0.094672', 'rewards_train/rejected': '0.048453', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04622', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-128.35', 'loss/train': '0.68416', 'examples_per_second': '31.558', 'grad_norm': '32.75', 'counters/examples': 246528, 'counters/updates': 7704}
train stats after 246560 examples: {'rewards_train/chosen': '0.12924', 'rewards_train/rejected': '0.025939', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1033', 'logps_train/rejected': '-105', 'logps_train/chosen': '-100.06', 'loss/train': '0.65427', 'examples_per_second': '30.392', 'grad_norm': '21.5', 'counters/examples': 246560, 'counters/updates': 7705}
train stats after 246592 examples: {'rewards_train/chosen': '0.12909', 'rewards_train/rejected': '0.029229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099861', 'logps_train/rejected': '-108.77', 'logps_train/chosen': '-120.27', 'loss/train': '0.66046', 'examples_per_second': '31.15', 'grad_norm': '22.5', 'counters/examples': 246592, 'counters/updates': 7706}
train stats after 246624 examples: {'rewards_train/chosen': '0.15008', 'rewards_train/rejected': '0.00054726', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14953', 'logps_train/rejected': '-99.066', 'logps_train/chosen': '-142.89', 'loss/train': '0.6286', 'examples_per_second': '30.013', 'grad_norm': '24.75', 'counters/examples': 246624, 'counters/updates': 7707}
train stats after 246656 examples: {'rewards_train/chosen': '0.19372', 'rewards_train/rejected': '0.14539', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048331', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-129.94', 'loss/train': '0.6795', 'examples_per_second': '30.709', 'grad_norm': '25.25', 'counters/examples': 246656, 'counters/updates': 7708}
train stats after 246688 examples: {'rewards_train/chosen': '0.12641', 'rewards_train/rejected': '0.018476', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10793', 'logps_train/rejected': '-106.45', 'logps_train/chosen': '-108.16', 'loss/train': '0.65361', 'examples_per_second': '31.814', 'grad_norm': '21.5', 'counters/examples': 246688, 'counters/updates': 7709}
skipping logging after 246720 examples to avoid logging too frequently
train stats after 246752 examples: {'rewards_train/chosen': '0.030714', 'rewards_train/rejected': '0.072356', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041642', 'logps_train/rejected': '-139.35', 'logps_train/chosen': '-164.26', 'loss/train': '0.72723', 'examples_per_second': '31.546', 'grad_norm': '29.375', 'counters/examples': 246752, 'counters/updates': 7711}
train stats after 246784 examples: {'rewards_train/chosen': '0.11569', 'rewards_train/rejected': '0.073081', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04261', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-129.74', 'loss/train': '0.69214', 'examples_per_second': '24.112', 'grad_norm': '25.75', 'counters/examples': 246784, 'counters/updates': 7712}
train stats after 246816 examples: {'rewards_train/chosen': '0.10867', 'rewards_train/rejected': '0.039456', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069212', 'logps_train/rejected': '-110.08', 'logps_train/chosen': '-105.91', 'loss/train': '0.67575', 'examples_per_second': '31.887', 'grad_norm': '24.125', 'counters/examples': 246816, 'counters/updates': 7713}
train stats after 246848 examples: {'rewards_train/chosen': '0.14368', 'rewards_train/rejected': '0.070685', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07299', 'logps_train/rejected': '-126.55', 'logps_train/chosen': '-131.44', 'loss/train': '0.67183', 'examples_per_second': '31.233', 'grad_norm': '29.75', 'counters/examples': 246848, 'counters/updates': 7714}
train stats after 246880 examples: {'rewards_train/chosen': '0.1323', 'rewards_train/rejected': '0.12554', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0067588', 'logps_train/rejected': '-115.12', 'logps_train/chosen': '-141.28', 'loss/train': '0.70036', 'examples_per_second': '31.752', 'grad_norm': '26', 'counters/examples': 246880, 'counters/updates': 7715}
skipping logging after 246912 examples to avoid logging too frequently
train stats after 246944 examples: {'rewards_train/chosen': '0.016621', 'rewards_train/rejected': '0.038168', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.021546', 'logps_train/rejected': '-109.82', 'logps_train/chosen': '-135.67', 'loss/train': '0.71638', 'examples_per_second': '31.499', 'grad_norm': '25.625', 'counters/examples': 246944, 'counters/updates': 7717}
train stats after 246976 examples: {'rewards_train/chosen': '0.16246', 'rewards_train/rejected': '0.041271', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12119', 'logps_train/rejected': '-140.05', 'logps_train/chosen': '-175.69', 'loss/train': '0.65381', 'examples_per_second': '30.025', 'grad_norm': '28.125', 'counters/examples': 246976, 'counters/updates': 7718}
train stats after 247008 examples: {'rewards_train/chosen': '0.15738', 'rewards_train/rejected': '0.032746', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12463', 'logps_train/rejected': '-150.1', 'logps_train/chosen': '-141.51', 'loss/train': '0.64291', 'examples_per_second': '31.577', 'grad_norm': '27.25', 'counters/examples': 247008, 'counters/updates': 7719}
skipping logging after 247040 examples to avoid logging too frequently
train stats after 247072 examples: {'rewards_train/chosen': '0.071381', 'rewards_train/rejected': '0.017935', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053446', 'logps_train/rejected': '-135.96', 'logps_train/chosen': '-105.63', 'loss/train': '0.68315', 'examples_per_second': '30.514', 'grad_norm': '26.125', 'counters/examples': 247072, 'counters/updates': 7721}
train stats after 247104 examples: {'rewards_train/chosen': '0.25504', 'rewards_train/rejected': '0.087587', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16746', 'logps_train/rejected': '-152.62', 'logps_train/chosen': '-174.76', 'loss/train': '0.62584', 'examples_per_second': '30.126', 'grad_norm': '28', 'counters/examples': 247104, 'counters/updates': 7722}
train stats after 247136 examples: {'rewards_train/chosen': '0.038227', 'rewards_train/rejected': '0.0064054', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031822', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-112.44', 'loss/train': '0.69078', 'examples_per_second': '31.594', 'grad_norm': '24.375', 'counters/examples': 247136, 'counters/updates': 7723}
train stats after 247168 examples: {'rewards_train/chosen': '0.20293', 'rewards_train/rejected': '-0.06465', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.26758', 'logps_train/rejected': '-114.91', 'logps_train/chosen': '-170.46', 'loss/train': '0.57974', 'examples_per_second': '33.232', 'grad_norm': '24.125', 'counters/examples': 247168, 'counters/updates': 7724}
train stats after 247200 examples: {'rewards_train/chosen': '0.1247', 'rewards_train/rejected': '0.12975', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0050471', 'logps_train/rejected': '-157.93', 'logps_train/chosen': '-156.83', 'loss/train': '0.71414', 'examples_per_second': '30.826', 'grad_norm': '31.125', 'counters/examples': 247200, 'counters/updates': 7725}
train stats after 247232 examples: {'rewards_train/chosen': '0.19391', 'rewards_train/rejected': '0.013858', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18005', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-151.59', 'loss/train': '0.61857', 'examples_per_second': '31.427', 'grad_norm': '23.875', 'counters/examples': 247232, 'counters/updates': 7726}
skipping logging after 247264 examples to avoid logging too frequently
train stats after 247296 examples: {'rewards_train/chosen': '0.12075', 'rewards_train/rejected': '0.035326', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085426', 'logps_train/rejected': '-160.88', 'logps_train/chosen': '-142.12', 'loss/train': '0.66694', 'examples_per_second': '30.376', 'grad_norm': '27.625', 'counters/examples': 247296, 'counters/updates': 7728}
train stats after 247328 examples: {'rewards_train/chosen': '0.19318', 'rewards_train/rejected': '0.076943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11623', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-161.44', 'loss/train': '0.64973', 'examples_per_second': '29.97', 'grad_norm': '27.875', 'counters/examples': 247328, 'counters/updates': 7729}
skipping logging after 247360 examples to avoid logging too frequently
train stats after 247392 examples: {'rewards_train/chosen': '0.09661', 'rewards_train/rejected': '0.013072', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083539', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-171.4', 'loss/train': '0.66485', 'examples_per_second': '31.575', 'grad_norm': '27', 'counters/examples': 247392, 'counters/updates': 7731}
train stats after 247424 examples: {'rewards_train/chosen': '0.17079', 'rewards_train/rejected': '-0.04519', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21598', 'logps_train/rejected': '-105.37', 'logps_train/chosen': '-118.31', 'loss/train': '0.60243', 'examples_per_second': '32.045', 'grad_norm': '22.625', 'counters/examples': 247424, 'counters/updates': 7732}
train stats after 247456 examples: {'rewards_train/chosen': '0.13407', 'rewards_train/rejected': '0.052172', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081895', 'logps_train/rejected': '-132.22', 'logps_train/chosen': '-122.77', 'loss/train': '0.67034', 'examples_per_second': '30.588', 'grad_norm': '26.875', 'counters/examples': 247456, 'counters/updates': 7733}
train stats after 247488 examples: {'rewards_train/chosen': '0.10872', 'rewards_train/rejected': '0.028175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08054', 'logps_train/rejected': '-110.65', 'logps_train/chosen': '-132.79', 'loss/train': '0.67102', 'examples_per_second': '32.123', 'grad_norm': '25.375', 'counters/examples': 247488, 'counters/updates': 7734}
skipping logging after 247520 examples to avoid logging too frequently
train stats after 247552 examples: {'rewards_train/chosen': '0.21463', 'rewards_train/rejected': '0.04396', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17067', 'logps_train/rejected': '-100.42', 'logps_train/chosen': '-148.72', 'loss/train': '0.62858', 'examples_per_second': '32.354', 'grad_norm': '28.375', 'counters/examples': 247552, 'counters/updates': 7736}
train stats after 247584 examples: {'rewards_train/chosen': '0.18245', 'rewards_train/rejected': '-0.072769', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25522', 'logps_train/rejected': '-133.37', 'logps_train/chosen': '-127.61', 'loss/train': '0.5912', 'examples_per_second': '33.009', 'grad_norm': '24.75', 'counters/examples': 247584, 'counters/updates': 7737}
train stats after 247616 examples: {'rewards_train/chosen': '0.12228', 'rewards_train/rejected': '0.07052', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051761', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-128.92', 'loss/train': '0.68314', 'examples_per_second': '31.318', 'grad_norm': '28.625', 'counters/examples': 247616, 'counters/updates': 7738}
skipping logging after 247648 examples to avoid logging too frequently
train stats after 247680 examples: {'rewards_train/chosen': '0.036824', 'rewards_train/rejected': '-0.071153', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10798', 'logps_train/rejected': '-98.49', 'logps_train/chosen': '-121.08', 'loss/train': '0.6547', 'examples_per_second': '32.564', 'grad_norm': '22.625', 'counters/examples': 247680, 'counters/updates': 7740}
train stats after 247712 examples: {'rewards_train/chosen': '0.048517', 'rewards_train/rejected': '-0.023705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072222', 'logps_train/rejected': '-105.8', 'logps_train/chosen': '-165.42', 'loss/train': '0.67099', 'examples_per_second': '31.482', 'grad_norm': '28.25', 'counters/examples': 247712, 'counters/updates': 7741}
train stats after 247744 examples: {'rewards_train/chosen': '0.10273', 'rewards_train/rejected': '0.004773', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097957', 'logps_train/rejected': '-119.73', 'logps_train/chosen': '-115.66', 'loss/train': '0.65502', 'examples_per_second': '32.822', 'grad_norm': '24.5', 'counters/examples': 247744, 'counters/updates': 7742}
train stats after 247776 examples: {'rewards_train/chosen': '0.11811', 'rewards_train/rejected': '0.065356', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052751', 'logps_train/rejected': '-122.4', 'logps_train/chosen': '-113.57', 'loss/train': '0.68199', 'examples_per_second': '33.181', 'grad_norm': '25.25', 'counters/examples': 247776, 'counters/updates': 7743}
train stats after 247808 examples: {'rewards_train/chosen': '0.11761', 'rewards_train/rejected': '-0.034463', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15207', 'logps_train/rejected': '-85.63', 'logps_train/chosen': '-110.05', 'loss/train': '0.63542', 'examples_per_second': '31.553', 'grad_norm': '24.625', 'counters/examples': 247808, 'counters/updates': 7744}
train stats after 247840 examples: {'rewards_train/chosen': '0.10859', 'rewards_train/rejected': '0.041665', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066923', 'logps_train/rejected': '-93.485', 'logps_train/chosen': '-114.65', 'loss/train': '0.67108', 'examples_per_second': '30.382', 'grad_norm': '21.125', 'counters/examples': 247840, 'counters/updates': 7745}
train stats after 247872 examples: {'rewards_train/chosen': '0.18597', 'rewards_train/rejected': '0.10656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07941', 'logps_train/rejected': '-107.6', 'logps_train/chosen': '-113.41', 'loss/train': '0.67561', 'examples_per_second': '30.555', 'grad_norm': '25.625', 'counters/examples': 247872, 'counters/updates': 7746}
train stats after 247904 examples: {'rewards_train/chosen': '0.12682', 'rewards_train/rejected': '0.0092682', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11755', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-134.59', 'loss/train': '0.64675', 'examples_per_second': '31.387', 'grad_norm': '33', 'counters/examples': 247904, 'counters/updates': 7747}
train stats after 247936 examples: {'rewards_train/chosen': '0.13897', 'rewards_train/rejected': '0.069559', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.069412', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-154.31', 'loss/train': '0.6712', 'examples_per_second': '33.067', 'grad_norm': '26.625', 'counters/examples': 247936, 'counters/updates': 7748}
train stats after 247968 examples: {'rewards_train/chosen': '0.22878', 'rewards_train/rejected': '0.026747', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20204', 'logps_train/rejected': '-119.87', 'logps_train/chosen': '-137.21', 'loss/train': '0.60838', 'examples_per_second': '30.618', 'grad_norm': '25.375', 'counters/examples': 247968, 'counters/updates': 7749}
train stats after 248000 examples: {'rewards_train/chosen': '0.10325', 'rewards_train/rejected': '0.031618', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071636', 'logps_train/rejected': '-126.18', 'logps_train/chosen': '-114.52', 'loss/train': '0.67074', 'examples_per_second': '31.232', 'grad_norm': '26.625', 'counters/examples': 248000, 'counters/updates': 7750}
train stats after 248032 examples: {'rewards_train/chosen': '0.19299', 'rewards_train/rejected': '-0.020367', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21336', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-174.11', 'loss/train': '0.61145', 'examples_per_second': '31.276', 'grad_norm': '28.25', 'counters/examples': 248032, 'counters/updates': 7751}
train stats after 248064 examples: {'rewards_train/chosen': '0.16937', 'rewards_train/rejected': '0.006085', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16329', 'logps_train/rejected': '-115.93', 'logps_train/chosen': '-140.51', 'loss/train': '0.6274', 'examples_per_second': '31.534', 'grad_norm': '25.375', 'counters/examples': 248064, 'counters/updates': 7752}
train stats after 248096 examples: {'rewards_train/chosen': '0.092031', 'rewards_train/rejected': '0.025055', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066977', 'logps_train/rejected': '-116.53', 'logps_train/chosen': '-124.59', 'loss/train': '0.66778', 'examples_per_second': '30.419', 'grad_norm': '24.625', 'counters/examples': 248096, 'counters/updates': 7753}
train stats after 248128 examples: {'rewards_train/chosen': '0.23861', 'rewards_train/rejected': '-0.031822', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27043', 'logps_train/rejected': '-138.02', 'logps_train/chosen': '-174.39', 'loss/train': '0.58975', 'examples_per_second': '31.43', 'grad_norm': '27.25', 'counters/examples': 248128, 'counters/updates': 7754}
train stats after 248160 examples: {'rewards_train/chosen': '0.14772', 'rewards_train/rejected': '0.033886', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11384', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-125.8', 'loss/train': '0.65601', 'examples_per_second': '30.999', 'grad_norm': '23.5', 'counters/examples': 248160, 'counters/updates': 7755}
train stats after 248192 examples: {'rewards_train/chosen': '0.076569', 'rewards_train/rejected': '0.0070475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069521', 'logps_train/rejected': '-102.09', 'logps_train/chosen': '-126.55', 'loss/train': '0.67448', 'examples_per_second': '31.701', 'grad_norm': '23.375', 'counters/examples': 248192, 'counters/updates': 7756}
train stats after 248224 examples: {'rewards_train/chosen': '0.19011', 'rewards_train/rejected': '-0.05327', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24338', 'logps_train/rejected': '-97.915', 'logps_train/chosen': '-159.81', 'loss/train': '0.60061', 'examples_per_second': '32.84', 'grad_norm': '23.625', 'counters/examples': 248224, 'counters/updates': 7757}
skipping logging after 248256 examples to avoid logging too frequently
train stats after 248288 examples: {'rewards_train/chosen': '0.21185', 'rewards_train/rejected': '0.01519', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19666', 'logps_train/rejected': '-150.67', 'logps_train/chosen': '-132.31', 'loss/train': '0.61876', 'examples_per_second': '31.488', 'grad_norm': '24.5', 'counters/examples': 248288, 'counters/updates': 7759}
train stats after 248320 examples: {'rewards_train/chosen': '0.15693', 'rewards_train/rejected': '0.026431', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1305', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-121.58', 'loss/train': '0.64504', 'examples_per_second': '30.766', 'grad_norm': '25.25', 'counters/examples': 248320, 'counters/updates': 7760}
train stats after 248352 examples: {'rewards_train/chosen': '0.077909', 'rewards_train/rejected': '0.0096074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068301', 'logps_train/rejected': '-117.23', 'logps_train/chosen': '-159.84', 'loss/train': '0.67256', 'examples_per_second': '33.243', 'grad_norm': '26.125', 'counters/examples': 248352, 'counters/updates': 7761}
skipping logging after 248384 examples to avoid logging too frequently
train stats after 248416 examples: {'rewards_train/chosen': '0.13537', 'rewards_train/rejected': '0.054989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080385', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-129.53', 'loss/train': '0.66729', 'examples_per_second': '30.213', 'grad_norm': '26.375', 'counters/examples': 248416, 'counters/updates': 7763}
skipping logging after 248448 examples to avoid logging too frequently
train stats after 248480 examples: {'rewards_train/chosen': '0.23587', 'rewards_train/rejected': '-0.02836', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26423', 'logps_train/rejected': '-97.136', 'logps_train/chosen': '-133.43', 'loss/train': '0.58112', 'examples_per_second': '31.563', 'grad_norm': '22.75', 'counters/examples': 248480, 'counters/updates': 7765}
train stats after 248512 examples: {'rewards_train/chosen': '0.20442', 'rewards_train/rejected': '-0.036929', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24135', 'logps_train/rejected': '-139.79', 'logps_train/chosen': '-155.38', 'loss/train': '0.58937', 'examples_per_second': '31.136', 'grad_norm': '24.5', 'counters/examples': 248512, 'counters/updates': 7766}
train stats after 248544 examples: {'rewards_train/chosen': '0.21866', 'rewards_train/rejected': '0.10021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11845', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-155.42', 'loss/train': '0.6543', 'examples_per_second': '31.522', 'grad_norm': '27.875', 'counters/examples': 248544, 'counters/updates': 7767}
train stats after 248576 examples: {'rewards_train/chosen': '0.14956', 'rewards_train/rejected': '0.14374', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0058222', 'logps_train/rejected': '-155.43', 'logps_train/chosen': '-154.19', 'loss/train': '0.71078', 'examples_per_second': '30.144', 'grad_norm': '28.75', 'counters/examples': 248576, 'counters/updates': 7768}
train stats after 248608 examples: {'rewards_train/chosen': '0.14138', 'rewards_train/rejected': '0.13667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0047122', 'logps_train/rejected': '-161.25', 'logps_train/chosen': '-135.56', 'loss/train': '0.70619', 'examples_per_second': '31.541', 'grad_norm': '28.125', 'counters/examples': 248608, 'counters/updates': 7769}
train stats after 248640 examples: {'rewards_train/chosen': '0.1074', 'rewards_train/rejected': '-0.00044715', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10785', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-127.75', 'loss/train': '0.65527', 'examples_per_second': '31.444', 'grad_norm': '25.125', 'counters/examples': 248640, 'counters/updates': 7770}
skipping logging after 248672 examples to avoid logging too frequently
train stats after 248704 examples: {'rewards_train/chosen': '0.15522', 'rewards_train/rejected': '0.045427', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1098', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-122.1', 'loss/train': '0.6566', 'examples_per_second': '33.014', 'grad_norm': '29.375', 'counters/examples': 248704, 'counters/updates': 7772}
train stats after 248736 examples: {'rewards_train/chosen': '0.22411', 'rewards_train/rejected': '0.12714', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096968', 'logps_train/rejected': '-194.21', 'logps_train/chosen': '-173.85', 'loss/train': '0.66362', 'examples_per_second': '31.536', 'grad_norm': '30', 'counters/examples': 248736, 'counters/updates': 7773}
train stats after 248768 examples: {'rewards_train/chosen': '0.16763', 'rewards_train/rejected': '0.081098', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08653', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-131.86', 'loss/train': '0.66675', 'examples_per_second': '31.264', 'grad_norm': '27.25', 'counters/examples': 248768, 'counters/updates': 7774}
skipping logging after 248800 examples to avoid logging too frequently
train stats after 248832 examples: {'rewards_train/chosen': '0.21994', 'rewards_train/rejected': '0.054478', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16546', 'logps_train/rejected': '-130.4', 'logps_train/chosen': '-153.59', 'loss/train': '0.63007', 'examples_per_second': '30.075', 'grad_norm': '25.5', 'counters/examples': 248832, 'counters/updates': 7776}
train stats after 248864 examples: {'rewards_train/chosen': '0.1859', 'rewards_train/rejected': '0.095366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090537', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-141.01', 'loss/train': '0.67113', 'examples_per_second': '30.034', 'grad_norm': '32.25', 'counters/examples': 248864, 'counters/updates': 7777}
train stats after 248896 examples: {'rewards_train/chosen': '0.15073', 'rewards_train/rejected': '0.077839', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072891', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-159.92', 'loss/train': '0.66879', 'examples_per_second': '33.041', 'grad_norm': '31.5', 'counters/examples': 248896, 'counters/updates': 7778}
skipping logging after 248928 examples to avoid logging too frequently
train stats after 248960 examples: {'rewards_train/chosen': '0.10535', 'rewards_train/rejected': '0.04779', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057558', 'logps_train/rejected': '-119.72', 'logps_train/chosen': '-136.24', 'loss/train': '0.67825', 'examples_per_second': '31.552', 'grad_norm': '24.5', 'counters/examples': 248960, 'counters/updates': 7780}
train stats after 248992 examples: {'rewards_train/chosen': '0.15081', 'rewards_train/rejected': '-0.047965', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19878', 'logps_train/rejected': '-111', 'logps_train/chosen': '-207.14', 'loss/train': '0.60907', 'examples_per_second': '32.837', 'grad_norm': '29.125', 'counters/examples': 248992, 'counters/updates': 7781}
skipping logging after 249024 examples to avoid logging too frequently
train stats after 249056 examples: {'rewards_train/chosen': '0.12292', 'rewards_train/rejected': '-0.0073172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13024', 'logps_train/rejected': '-92.639', 'logps_train/chosen': '-113.58', 'loss/train': '0.65029', 'examples_per_second': '30.38', 'grad_norm': '21.5', 'counters/examples': 249056, 'counters/updates': 7783}
train stats after 249088 examples: {'rewards_train/chosen': '0.25979', 'rewards_train/rejected': '0.070568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18922', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-155.19', 'loss/train': '0.63512', 'examples_per_second': '31.286', 'grad_norm': '26.375', 'counters/examples': 249088, 'counters/updates': 7784}
train stats after 249120 examples: {'rewards_train/chosen': '0.055301', 'rewards_train/rejected': '0.040606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014695', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-155.22', 'loss/train': '0.69186', 'examples_per_second': '31.52', 'grad_norm': '26.625', 'counters/examples': 249120, 'counters/updates': 7785}
skipping logging after 249152 examples to avoid logging too frequently
train stats after 249184 examples: {'rewards_train/chosen': '0.086999', 'rewards_train/rejected': '0.027909', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05909', 'logps_train/rejected': '-132', 'logps_train/chosen': '-113.92', 'loss/train': '0.6748', 'examples_per_second': '30.701', 'grad_norm': '23.625', 'counters/examples': 249184, 'counters/updates': 7787}
train stats after 249216 examples: {'rewards_train/chosen': '0.22385', 'rewards_train/rejected': '0.0005486', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2233', 'logps_train/rejected': '-105.52', 'logps_train/chosen': '-169.88', 'loss/train': '0.6018', 'examples_per_second': '30.433', 'grad_norm': '25.5', 'counters/examples': 249216, 'counters/updates': 7788}
skipping logging after 249248 examples to avoid logging too frequently
train stats after 249280 examples: {'rewards_train/chosen': '0.12354', 'rewards_train/rejected': '0.029966', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093572', 'logps_train/rejected': '-141.86', 'logps_train/chosen': '-155.22', 'loss/train': '0.67081', 'examples_per_second': '29.853', 'grad_norm': '29.875', 'counters/examples': 249280, 'counters/updates': 7790}
train stats after 249312 examples: {'rewards_train/chosen': '0.018161', 'rewards_train/rejected': '0.0016533', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.016508', 'logps_train/rejected': '-119.33', 'logps_train/chosen': '-91.667', 'loss/train': '0.70154', 'examples_per_second': '30.823', 'grad_norm': '23.25', 'counters/examples': 249312, 'counters/updates': 7791}
train stats after 249344 examples: {'rewards_train/chosen': '0.20345', 'rewards_train/rejected': '0.042484', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16097', 'logps_train/rejected': '-101.04', 'logps_train/chosen': '-144.55', 'loss/train': '0.62776', 'examples_per_second': '30.398', 'grad_norm': '23.375', 'counters/examples': 249344, 'counters/updates': 7792}
train stats after 249376 examples: {'rewards_train/chosen': '0.11674', 'rewards_train/rejected': '0.053196', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063542', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-142.06', 'loss/train': '0.68651', 'examples_per_second': '31.206', 'grad_norm': '30', 'counters/examples': 249376, 'counters/updates': 7793}
train stats after 249408 examples: {'rewards_train/chosen': '0.18735', 'rewards_train/rejected': '0.022847', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1645', 'logps_train/rejected': '-101.61', 'logps_train/chosen': '-161.57', 'loss/train': '0.6263', 'examples_per_second': '32.113', 'grad_norm': '26.75', 'counters/examples': 249408, 'counters/updates': 7794}
train stats after 249440 examples: {'rewards_train/chosen': '0.18741', 'rewards_train/rejected': '0.00019389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18721', 'logps_train/rejected': '-158.02', 'logps_train/chosen': '-175.02', 'loss/train': '0.63089', 'examples_per_second': '31.33', 'grad_norm': '27.375', 'counters/examples': 249440, 'counters/updates': 7795}
skipping logging after 249472 examples to avoid logging too frequently
train stats after 249504 examples: {'rewards_train/chosen': '0.030951', 'rewards_train/rejected': '0.016867', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014084', 'logps_train/rejected': '-87.943', 'logps_train/chosen': '-110.63', 'loss/train': '0.69881', 'examples_per_second': '41.223', 'grad_norm': '22.5', 'counters/examples': 249504, 'counters/updates': 7797}
train stats after 249536 examples: {'rewards_train/chosen': '0.084637', 'rewards_train/rejected': '0.05678', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027858', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-107.81', 'loss/train': '0.68814', 'examples_per_second': '30.693', 'grad_norm': '22', 'counters/examples': 249536, 'counters/updates': 7798}
train stats after 249568 examples: {'rewards_train/chosen': '0.14668', 'rewards_train/rejected': '-0.029991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17667', 'logps_train/rejected': '-115.25', 'logps_train/chosen': '-127.05', 'loss/train': '0.62502', 'examples_per_second': '31.567', 'grad_norm': '22.375', 'counters/examples': 249568, 'counters/updates': 7799}
train stats after 249600 examples: {'rewards_train/chosen': '0.21561', 'rewards_train/rejected': '0.0018953', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21371', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-147.13', 'loss/train': '0.6056', 'examples_per_second': '33.217', 'grad_norm': '27.25', 'counters/examples': 249600, 'counters/updates': 7800}
train stats after 249632 examples: {'rewards_train/chosen': '0.31421', 'rewards_train/rejected': '0.15496', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15925', 'logps_train/rejected': '-136.03', 'logps_train/chosen': '-197.7', 'loss/train': '0.6381', 'examples_per_second': '31.393', 'grad_norm': '50', 'counters/examples': 249632, 'counters/updates': 7801}
train stats after 249664 examples: {'rewards_train/chosen': '0.15832', 'rewards_train/rejected': '-0.0017322', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16005', 'logps_train/rejected': '-172.15', 'logps_train/chosen': '-135.75', 'loss/train': '0.62485', 'examples_per_second': '30.951', 'grad_norm': '32', 'counters/examples': 249664, 'counters/updates': 7802}
train stats after 249696 examples: {'rewards_train/chosen': '0.16885', 'rewards_train/rejected': '-0.028738', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19759', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-143.29', 'loss/train': '0.61612', 'examples_per_second': '32.569', 'grad_norm': '24', 'counters/examples': 249696, 'counters/updates': 7803}
train stats after 249728 examples: {'rewards_train/chosen': '0.12211', 'rewards_train/rejected': '0.033234', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088876', 'logps_train/rejected': '-93.808', 'logps_train/chosen': '-129.21', 'loss/train': '0.66504', 'examples_per_second': '30.549', 'grad_norm': '28.125', 'counters/examples': 249728, 'counters/updates': 7804}
skipping logging after 249760 examples to avoid logging too frequently
train stats after 249792 examples: {'rewards_train/chosen': '0.18806', 'rewards_train/rejected': '0.030963', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1571', 'logps_train/rejected': '-104.95', 'logps_train/chosen': '-109.07', 'loss/train': '0.63892', 'examples_per_second': '30.581', 'grad_norm': '24.375', 'counters/examples': 249792, 'counters/updates': 7806}
train stats after 249824 examples: {'rewards_train/chosen': '0.12956', 'rewards_train/rejected': '0.10783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021726', 'logps_train/rejected': '-149', 'logps_train/chosen': '-150.61', 'loss/train': '0.69419', 'examples_per_second': '31.575', 'grad_norm': '30.625', 'counters/examples': 249824, 'counters/updates': 7807}
train stats after 249856 examples: {'rewards_train/chosen': '0.14059', 'rewards_train/rejected': '0.059544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081047', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-123.61', 'loss/train': '0.66424', 'examples_per_second': '32.718', 'grad_norm': '25', 'counters/examples': 249856, 'counters/updates': 7808}
train stats after 249888 examples: {'rewards_train/chosen': '0.092249', 'rewards_train/rejected': '0.0041253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088123', 'logps_train/rejected': '-90.129', 'logps_train/chosen': '-107.33', 'loss/train': '0.66061', 'examples_per_second': '31.538', 'grad_norm': '22', 'counters/examples': 249888, 'counters/updates': 7809}
train stats after 249920 examples: {'rewards_train/chosen': '0.13306', 'rewards_train/rejected': '0.032831', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10023', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-143.08', 'loss/train': '0.6575', 'examples_per_second': '30.598', 'grad_norm': '24.75', 'counters/examples': 249920, 'counters/updates': 7810}
train stats after 249952 examples: {'rewards_train/chosen': '0.17792', 'rewards_train/rejected': '0.0010703', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17685', 'logps_train/rejected': '-97.905', 'logps_train/chosen': '-119.81', 'loss/train': '0.62233', 'examples_per_second': '31.56', 'grad_norm': '21.75', 'counters/examples': 249952, 'counters/updates': 7811}
train stats after 249984 examples: {'rewards_train/chosen': '0.074516', 'rewards_train/rejected': '0.098015', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023499', 'logps_train/rejected': '-144.88', 'logps_train/chosen': '-151.06', 'loss/train': '0.72454', 'examples_per_second': '30.037', 'grad_norm': '29', 'counters/examples': 249984, 'counters/updates': 7812}
skipping logging after 250016 examples to avoid logging too frequently
train stats after 250048 examples: {'rewards_train/chosen': '0.15065', 'rewards_train/rejected': '0.044423', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10623', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-145.52', 'loss/train': '0.64887', 'examples_per_second': '31.551', 'grad_norm': '27.375', 'counters/examples': 250048, 'counters/updates': 7814}
train stats after 250080 examples: {'rewards_train/chosen': '0.083937', 'rewards_train/rejected': '0.012837', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.071099', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-114.4', 'loss/train': '0.67109', 'examples_per_second': '32.683', 'grad_norm': '24.5', 'counters/examples': 250080, 'counters/updates': 7815}
skipping logging after 250112 examples to avoid logging too frequently
train stats after 250144 examples: {'rewards_train/chosen': '0.17676', 'rewards_train/rejected': '0.051392', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12537', 'logps_train/rejected': '-131.27', 'logps_train/chosen': '-167.14', 'loss/train': '0.64889', 'examples_per_second': '32.862', 'grad_norm': '30.25', 'counters/examples': 250144, 'counters/updates': 7817}
train stats after 250176 examples: {'rewards_train/chosen': '0.1476', 'rewards_train/rejected': '0.047539', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10006', 'logps_train/rejected': '-124.84', 'logps_train/chosen': '-134.69', 'loss/train': '0.6555', 'examples_per_second': '29.968', 'grad_norm': '25.75', 'counters/examples': 250176, 'counters/updates': 7818}
skipping logging after 250208 examples to avoid logging too frequently
train stats after 250240 examples: {'rewards_train/chosen': '0.079699', 'rewards_train/rejected': '-0.030519', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11022', 'logps_train/rejected': '-96.117', 'logps_train/chosen': '-134.41', 'loss/train': '0.65333', 'examples_per_second': '33.344', 'grad_norm': '25.5', 'counters/examples': 250240, 'counters/updates': 7820}
train stats after 250272 examples: {'rewards_train/chosen': '0.13384', 'rewards_train/rejected': '-0.083001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21684', 'logps_train/rejected': '-100.07', 'logps_train/chosen': '-152.32', 'loss/train': '0.61193', 'examples_per_second': '32.394', 'grad_norm': '24.75', 'counters/examples': 250272, 'counters/updates': 7821}
train stats after 250304 examples: {'rewards_train/chosen': '0.028278', 'rewards_train/rejected': '-0.082542', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11082', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-114.14', 'loss/train': '0.65157', 'examples_per_second': '32.696', 'grad_norm': '21.5', 'counters/examples': 250304, 'counters/updates': 7822}
train stats after 250336 examples: {'rewards_train/chosen': '0.12507', 'rewards_train/rejected': '-0.0055317', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1306', 'logps_train/rejected': '-103.57', 'logps_train/chosen': '-136.78', 'loss/train': '0.64824', 'examples_per_second': '31.348', 'grad_norm': '24', 'counters/examples': 250336, 'counters/updates': 7823}
train stats after 250368 examples: {'rewards_train/chosen': '0.10351', 'rewards_train/rejected': '-0.044954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14846', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-122.82', 'loss/train': '0.63683', 'examples_per_second': '31.548', 'grad_norm': '27', 'counters/examples': 250368, 'counters/updates': 7824}
skipping logging after 250400 examples to avoid logging too frequently
train stats after 250432 examples: {'rewards_train/chosen': '0.1685', 'rewards_train/rejected': '0.1578', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010694', 'logps_train/rejected': '-182.83', 'logps_train/chosen': '-145.53', 'loss/train': '0.71214', 'examples_per_second': '31.703', 'grad_norm': '29.375', 'counters/examples': 250432, 'counters/updates': 7826}
train stats after 250464 examples: {'rewards_train/chosen': '0.24264', 'rewards_train/rejected': '0.089474', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15317', 'logps_train/rejected': '-170.99', 'logps_train/chosen': '-150.46', 'loss/train': '0.63352', 'examples_per_second': '30.109', 'grad_norm': '42.75', 'counters/examples': 250464, 'counters/updates': 7827}
skipping logging after 250496 examples to avoid logging too frequently
train stats after 250528 examples: {'rewards_train/chosen': '0.13656', 'rewards_train/rejected': '0.018745', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11781', 'logps_train/rejected': '-99.87', 'logps_train/chosen': '-123.02', 'loss/train': '0.6442', 'examples_per_second': '31.653', 'grad_norm': '22.25', 'counters/examples': 250528, 'counters/updates': 7829}
train stats after 250560 examples: {'rewards_train/chosen': '0.13267', 'rewards_train/rejected': '-0.063701', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19637', 'logps_train/rejected': '-103.98', 'logps_train/chosen': '-110.32', 'loss/train': '0.61517', 'examples_per_second': '32.742', 'grad_norm': '21.375', 'counters/examples': 250560, 'counters/updates': 7830}
train stats after 250592 examples: {'rewards_train/chosen': '0.1406', 'rewards_train/rejected': '0.049104', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091497', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-128.08', 'loss/train': '0.6608', 'examples_per_second': '30.99', 'grad_norm': '30.875', 'counters/examples': 250592, 'counters/updates': 7831}
train stats after 250624 examples: {'rewards_train/chosen': '0.127', 'rewards_train/rejected': '0.052363', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074641', 'logps_train/rejected': '-126.66', 'logps_train/chosen': '-140.22', 'loss/train': '0.66625', 'examples_per_second': '31.62', 'grad_norm': '27.125', 'counters/examples': 250624, 'counters/updates': 7832}
train stats after 250656 examples: {'rewards_train/chosen': '0.16868', 'rewards_train/rejected': '0.02259', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14609', 'logps_train/rejected': '-104.06', 'logps_train/chosen': '-134.36', 'loss/train': '0.6418', 'examples_per_second': '32.915', 'grad_norm': '24.25', 'counters/examples': 250656, 'counters/updates': 7833}
train stats after 250688 examples: {'rewards_train/chosen': '0.1497', 'rewards_train/rejected': '-0.025142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17485', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-171.66', 'loss/train': '0.62072', 'examples_per_second': '31.894', 'grad_norm': '26.25', 'counters/examples': 250688, 'counters/updates': 7834}
train stats after 250720 examples: {'rewards_train/chosen': '0.21073', 'rewards_train/rejected': '0.026915', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18382', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-159.06', 'loss/train': '0.62844', 'examples_per_second': '31.431', 'grad_norm': '24.75', 'counters/examples': 250720, 'counters/updates': 7835}
train stats after 250752 examples: {'rewards_train/chosen': '0.13909', 'rewards_train/rejected': '0.033247', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10584', 'logps_train/rejected': '-107.03', 'logps_train/chosen': '-111.24', 'loss/train': '0.65137', 'examples_per_second': '32.105', 'grad_norm': '22.125', 'counters/examples': 250752, 'counters/updates': 7836}
train stats after 250784 examples: {'rewards_train/chosen': '0.20429', 'rewards_train/rejected': '0.040492', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1638', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-126.43', 'loss/train': '0.62586', 'examples_per_second': '31.628', 'grad_norm': '22.375', 'counters/examples': 250784, 'counters/updates': 7837}
train stats after 250816 examples: {'rewards_train/chosen': '0.17024', 'rewards_train/rejected': '0.061827', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10841', 'logps_train/rejected': '-128.01', 'logps_train/chosen': '-140.84', 'loss/train': '0.65907', 'examples_per_second': '30.922', 'grad_norm': '29.375', 'counters/examples': 250816, 'counters/updates': 7838}
train stats after 250848 examples: {'rewards_train/chosen': '0.15329', 'rewards_train/rejected': '-0.081309', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2346', 'logps_train/rejected': '-130.56', 'logps_train/chosen': '-129.37', 'loss/train': '0.59862', 'examples_per_second': '30.7', 'grad_norm': '23.75', 'counters/examples': 250848, 'counters/updates': 7839}
skipping logging after 250880 examples to avoid logging too frequently
train stats after 250912 examples: {'rewards_train/chosen': '0.16054', 'rewards_train/rejected': '0.17239', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011849', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-158.59', 'loss/train': '0.72437', 'examples_per_second': '30.12', 'grad_norm': '33', 'counters/examples': 250912, 'counters/updates': 7841}
train stats after 250944 examples: {'rewards_train/chosen': '0.041525', 'rewards_train/rejected': '0.0094645', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032061', 'logps_train/rejected': '-94.389', 'logps_train/chosen': '-108.6', 'loss/train': '0.68913', 'examples_per_second': '30.54', 'grad_norm': '27.125', 'counters/examples': 250944, 'counters/updates': 7842}
train stats after 250976 examples: {'rewards_train/chosen': '0.15555', 'rewards_train/rejected': '0.013365', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14218', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-140.82', 'loss/train': '0.63946', 'examples_per_second': '31.959', 'grad_norm': '25', 'counters/examples': 250976, 'counters/updates': 7843}
train stats after 251008 examples: {'rewards_train/chosen': '0.11592', 'rewards_train/rejected': '-0.0025118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11843', 'logps_train/rejected': '-153.37', 'logps_train/chosen': '-149.36', 'loss/train': '0.64529', 'examples_per_second': '31.383', 'grad_norm': '26.375', 'counters/examples': 251008, 'counters/updates': 7844}
skipping logging after 251040 examples to avoid logging too frequently
train stats after 251072 examples: {'rewards_train/chosen': '0.028351', 'rewards_train/rejected': '-0.040317', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068668', 'logps_train/rejected': '-94.471', 'logps_train/chosen': '-107.6', 'loss/train': '0.67122', 'examples_per_second': '34.977', 'grad_norm': '20.625', 'counters/examples': 251072, 'counters/updates': 7846}
skipping logging after 251104 examples to avoid logging too frequently
train stats after 251136 examples: {'rewards_train/chosen': '0.13951', 'rewards_train/rejected': '0.15554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01603', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-153.55', 'loss/train': '0.71192', 'examples_per_second': '31.648', 'grad_norm': '27.375', 'counters/examples': 251136, 'counters/updates': 7848}
train stats after 251168 examples: {'rewards_train/chosen': '0.14163', 'rewards_train/rejected': '0.092184', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049447', 'logps_train/rejected': '-114.21', 'logps_train/chosen': '-112.87', 'loss/train': '0.68116', 'examples_per_second': '31.981', 'grad_norm': '22.75', 'counters/examples': 251168, 'counters/updates': 7849}
skipping logging after 251200 examples to avoid logging too frequently
train stats after 251232 examples: {'rewards_train/chosen': '0.13356', 'rewards_train/rejected': '0.052322', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081238', 'logps_train/rejected': '-114.17', 'logps_train/chosen': '-157.46', 'loss/train': '0.66625', 'examples_per_second': '33.217', 'grad_norm': '27.25', 'counters/examples': 251232, 'counters/updates': 7851}
train stats after 251264 examples: {'rewards_train/chosen': '0.15518', 'rewards_train/rejected': '-0.0129', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16808', 'logps_train/rejected': '-111.74', 'logps_train/chosen': '-147.91', 'loss/train': '0.63302', 'examples_per_second': '31.4', 'grad_norm': '24.375', 'counters/examples': 251264, 'counters/updates': 7852}
train stats after 251296 examples: {'rewards_train/chosen': '0.15024', 'rewards_train/rejected': '-0.10963', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25987', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-124.29', 'loss/train': '0.59114', 'examples_per_second': '31.655', 'grad_norm': '25.375', 'counters/examples': 251296, 'counters/updates': 7853}
train stats after 251328 examples: {'rewards_train/chosen': '0.25797', 'rewards_train/rejected': '0.042158', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21581', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-153.1', 'loss/train': '0.60373', 'examples_per_second': '31.625', 'grad_norm': '24.625', 'counters/examples': 251328, 'counters/updates': 7854}
train stats after 251360 examples: {'rewards_train/chosen': '0.11685', 'rewards_train/rejected': '0.024492', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092361', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-123.03', 'loss/train': '0.66702', 'examples_per_second': '31.777', 'grad_norm': '24.875', 'counters/examples': 251360, 'counters/updates': 7855}
train stats after 251392 examples: {'rewards_train/chosen': '0.18148', 'rewards_train/rejected': '0.06859', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11289', 'logps_train/rejected': '-111.21', 'logps_train/chosen': '-124.89', 'loss/train': '0.64978', 'examples_per_second': '31.648', 'grad_norm': '27.625', 'counters/examples': 251392, 'counters/updates': 7856}
skipping logging after 251424 examples to avoid logging too frequently
train stats after 251456 examples: {'rewards_train/chosen': '0.14401', 'rewards_train/rejected': '-0.020568', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16457', 'logps_train/rejected': '-156.83', 'logps_train/chosen': '-144.43', 'loss/train': '0.63091', 'examples_per_second': '31.013', 'grad_norm': '27.75', 'counters/examples': 251456, 'counters/updates': 7858}
train stats after 251488 examples: {'rewards_train/chosen': '0.16652', 'rewards_train/rejected': '-0.002741', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16926', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-125.26', 'loss/train': '0.63141', 'examples_per_second': '32.269', 'grad_norm': '22.875', 'counters/examples': 251488, 'counters/updates': 7859}
train stats after 251520 examples: {'rewards_train/chosen': '0.17452', 'rewards_train/rejected': '0.015467', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15905', 'logps_train/rejected': '-131.73', 'logps_train/chosen': '-131.28', 'loss/train': '0.63128', 'examples_per_second': '30.804', 'grad_norm': '28.75', 'counters/examples': 251520, 'counters/updates': 7860}
train stats after 251552 examples: {'rewards_train/chosen': '0.15027', 'rewards_train/rejected': '0.05614', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09413', 'logps_train/rejected': '-149.89', 'logps_train/chosen': '-137', 'loss/train': '0.67041', 'examples_per_second': '31.654', 'grad_norm': '27', 'counters/examples': 251552, 'counters/updates': 7861}
train stats after 251584 examples: {'rewards_train/chosen': '0.17003', 'rewards_train/rejected': '0.078415', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091613', 'logps_train/rejected': '-146.69', 'logps_train/chosen': '-192.91', 'loss/train': '0.66101', 'examples_per_second': '24.407', 'grad_norm': '29.375', 'counters/examples': 251584, 'counters/updates': 7862}
train stats after 251616 examples: {'rewards_train/chosen': '0.13351', 'rewards_train/rejected': '0.039858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093649', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-166.17', 'loss/train': '0.66257', 'examples_per_second': '31.539', 'grad_norm': '27', 'counters/examples': 251616, 'counters/updates': 7863}
train stats after 251648 examples: {'rewards_train/chosen': '0.11416', 'rewards_train/rejected': '0.056856', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057306', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-139.6', 'loss/train': '0.67962', 'examples_per_second': '31.655', 'grad_norm': '25.125', 'counters/examples': 251648, 'counters/updates': 7864}
train stats after 251680 examples: {'rewards_train/chosen': '0.15025', 'rewards_train/rejected': '0.016861', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13339', 'logps_train/rejected': '-98.854', 'logps_train/chosen': '-134.72', 'loss/train': '0.64506', 'examples_per_second': '24.321', 'grad_norm': '25.125', 'counters/examples': 251680, 'counters/updates': 7865}
train stats after 251712 examples: {'rewards_train/chosen': '0.16987', 'rewards_train/rejected': '0.043158', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12671', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-156.73', 'loss/train': '0.64086', 'examples_per_second': '30.227', 'grad_norm': '43.5', 'counters/examples': 251712, 'counters/updates': 7866}
train stats after 251744 examples: {'rewards_train/chosen': '0.10967', 'rewards_train/rejected': '0.08366', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026012', 'logps_train/rejected': '-168.31', 'logps_train/chosen': '-161.03', 'loss/train': '0.69633', 'examples_per_second': '31.651', 'grad_norm': '29.375', 'counters/examples': 251744, 'counters/updates': 7867}
train stats after 251776 examples: {'rewards_train/chosen': '0.25095', 'rewards_train/rejected': '0.088761', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16219', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-169.13', 'loss/train': '0.63284', 'examples_per_second': '31.673', 'grad_norm': '27.625', 'counters/examples': 251776, 'counters/updates': 7868}
skipping logging after 251808 examples to avoid logging too frequently
train stats after 251840 examples: {'rewards_train/chosen': '0.089183', 'rewards_train/rejected': '0.028621', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060562', 'logps_train/rejected': '-100.99', 'logps_train/chosen': '-143.21', 'loss/train': '0.68882', 'examples_per_second': '31.33', 'grad_norm': '28.875', 'counters/examples': 251840, 'counters/updates': 7870}
skipping logging after 251872 examples to avoid logging too frequently
train stats after 251904 examples: {'rewards_train/chosen': '0.23168', 'rewards_train/rejected': '0.067011', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16467', 'logps_train/rejected': '-128.74', 'logps_train/chosen': '-132.5', 'loss/train': '0.63004', 'examples_per_second': '31.676', 'grad_norm': '24.625', 'counters/examples': 251904, 'counters/updates': 7872}
train stats after 251936 examples: {'rewards_train/chosen': '0.17037', 'rewards_train/rejected': '0.02253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14784', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-152.57', 'loss/train': '0.64448', 'examples_per_second': '31.704', 'grad_norm': '27.125', 'counters/examples': 251936, 'counters/updates': 7873}
skipping logging after 251968 examples to avoid logging too frequently
train stats after 252000 examples: {'rewards_train/chosen': '0.21838', 'rewards_train/rejected': '0.044306', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17408', 'logps_train/rejected': '-157.69', 'logps_train/chosen': '-146.27', 'loss/train': '0.63345', 'examples_per_second': '30.565', 'grad_norm': '27.125', 'counters/examples': 252000, 'counters/updates': 7875}
Running evaluation after 252000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.27it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.95it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.96it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 252000: {'rewards_eval/chosen': '0.14597', 'rewards_eval/rejected': '0.036578', 'rewards_eval/accuracies': '0.60938', 'rewards_eval/margins': '0.10939', 'logps_eval/rejected': '-118.25', 'logps_eval/chosen': '-137.97', 'loss/eval': '0.65696'}
skipping save for non epoch
train stats after 252032 examples: {'rewards_train/chosen': '0.086282', 'rewards_train/rejected': '0.069459', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016823', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-139.58', 'loss/train': '0.69978', 'examples_per_second': '32.785', 'grad_norm': '27', 'counters/examples': 252032, 'counters/updates': 7876}
train stats after 252064 examples: {'rewards_train/chosen': '0.22818', 'rewards_train/rejected': '0.11481', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11337', 'logps_train/rejected': '-119.29', 'logps_train/chosen': '-127.72', 'loss/train': '0.64565', 'examples_per_second': '31.501', 'grad_norm': '25.75', 'counters/examples': 252064, 'counters/updates': 7877}
train stats after 252096 examples: {'rewards_train/chosen': '0.16044', 'rewards_train/rejected': '0.014962', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14548', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-151.34', 'loss/train': '0.63624', 'examples_per_second': '30.702', 'grad_norm': '26.125', 'counters/examples': 252096, 'counters/updates': 7878}
train stats after 252128 examples: {'rewards_train/chosen': '0.20098', 'rewards_train/rejected': '0.084502', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11648', 'logps_train/rejected': '-141.93', 'logps_train/chosen': '-149.11', 'loss/train': '0.65595', 'examples_per_second': '30.98', 'grad_norm': '24', 'counters/examples': 252128, 'counters/updates': 7879}
train stats after 252160 examples: {'rewards_train/chosen': '0.14827', 'rewards_train/rejected': '0.027047', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12122', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-129.2', 'loss/train': '0.65561', 'examples_per_second': '31.919', 'grad_norm': '22.875', 'counters/examples': 252160, 'counters/updates': 7880}
skipping logging after 252192 examples to avoid logging too frequently
train stats after 252224 examples: {'rewards_train/chosen': '0.081563', 'rewards_train/rejected': '-0.0039896', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085553', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-131.83', 'loss/train': '0.66358', 'examples_per_second': '30.823', 'grad_norm': '26.625', 'counters/examples': 252224, 'counters/updates': 7882}
skipping logging after 252256 examples to avoid logging too frequently
train stats after 252288 examples: {'rewards_train/chosen': '0.077169', 'rewards_train/rejected': '0.032505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044663', 'logps_train/rejected': '-127.6', 'logps_train/chosen': '-119.84', 'loss/train': '0.68609', 'examples_per_second': '32.03', 'grad_norm': '26.125', 'counters/examples': 252288, 'counters/updates': 7884}
train stats after 252320 examples: {'rewards_train/chosen': '0.12665', 'rewards_train/rejected': '0.0085343', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11811', 'logps_train/rejected': '-105.52', 'logps_train/chosen': '-114.31', 'loss/train': '0.64744', 'examples_per_second': '31.021', 'grad_norm': '22.75', 'counters/examples': 252320, 'counters/updates': 7885}
train stats after 252352 examples: {'rewards_train/chosen': '0.10013', 'rewards_train/rejected': '-0.025329', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12546', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-153.98', 'loss/train': '0.64459', 'examples_per_second': '30.155', 'grad_norm': '28', 'counters/examples': 252352, 'counters/updates': 7886}
train stats after 252384 examples: {'rewards_train/chosen': '0.20602', 'rewards_train/rejected': '0.050555', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15547', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-156.34', 'loss/train': '0.63165', 'examples_per_second': '25.413', 'grad_norm': '27.75', 'counters/examples': 252384, 'counters/updates': 7887}
train stats after 252416 examples: {'rewards_train/chosen': '0.19133', 'rewards_train/rejected': '-0.0055038', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19683', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-160.94', 'loss/train': '0.61608', 'examples_per_second': '31.991', 'grad_norm': '25.625', 'counters/examples': 252416, 'counters/updates': 7888}
train stats after 252448 examples: {'rewards_train/chosen': '0.26259', 'rewards_train/rejected': '-0.059712', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.3223', 'logps_train/rejected': '-98.772', 'logps_train/chosen': '-165.69', 'loss/train': '0.56555', 'examples_per_second': '30.56', 'grad_norm': '23.375', 'counters/examples': 252448, 'counters/updates': 7889}
skipping logging after 252480 examples to avoid logging too frequently
train stats after 252512 examples: {'rewards_train/chosen': '0.066661', 'rewards_train/rejected': '-0.030375', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097036', 'logps_train/rejected': '-97.101', 'logps_train/chosen': '-128.74', 'loss/train': '0.67049', 'examples_per_second': '31.107', 'grad_norm': '26.75', 'counters/examples': 252512, 'counters/updates': 7891}
train stats after 252544 examples: {'rewards_train/chosen': '0.0796', 'rewards_train/rejected': '-0.0091099', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08871', 'logps_train/rejected': '-109.73', 'logps_train/chosen': '-132.42', 'loss/train': '0.66455', 'examples_per_second': '30.679', 'grad_norm': '27.5', 'counters/examples': 252544, 'counters/updates': 7892}
train stats after 252576 examples: {'rewards_train/chosen': '0.1887', 'rewards_train/rejected': '0.057634', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13107', 'logps_train/rejected': '-99.796', 'logps_train/chosen': '-112.12', 'loss/train': '0.65072', 'examples_per_second': '30.255', 'grad_norm': '26.75', 'counters/examples': 252576, 'counters/updates': 7893}
train stats after 252608 examples: {'rewards_train/chosen': '0.20313', 'rewards_train/rejected': '-0.011848', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21497', 'logps_train/rejected': '-141.53', 'logps_train/chosen': '-170.27', 'loss/train': '0.63147', 'examples_per_second': '31.387', 'grad_norm': '35', 'counters/examples': 252608, 'counters/updates': 7894}
train stats after 252640 examples: {'rewards_train/chosen': '0.20157', 'rewards_train/rejected': '0.0081305', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19344', 'logps_train/rejected': '-115.7', 'logps_train/chosen': '-159.72', 'loss/train': '0.62176', 'examples_per_second': '31.638', 'grad_norm': '25.5', 'counters/examples': 252640, 'counters/updates': 7895}
train stats after 252672 examples: {'rewards_train/chosen': '0.14633', 'rewards_train/rejected': '-0.015787', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16211', 'logps_train/rejected': '-81.107', 'logps_train/chosen': '-126.65', 'loss/train': '0.62746', 'examples_per_second': '31.595', 'grad_norm': '22.125', 'counters/examples': 252672, 'counters/updates': 7896}
skipping logging after 252704 examples to avoid logging too frequently
train stats after 252736 examples: {'rewards_train/chosen': '0.16052', 'rewards_train/rejected': '0.10188', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058633', 'logps_train/rejected': '-139.02', 'logps_train/chosen': '-178.91', 'loss/train': '0.67856', 'examples_per_second': '33.888', 'grad_norm': '36', 'counters/examples': 252736, 'counters/updates': 7898}
train stats after 252768 examples: {'rewards_train/chosen': '0.17043', 'rewards_train/rejected': '0.12626', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044171', 'logps_train/rejected': '-129.31', 'logps_train/chosen': '-126.21', 'loss/train': '0.68881', 'examples_per_second': '30.841', 'grad_norm': '24.875', 'counters/examples': 252768, 'counters/updates': 7899}
train stats after 252800 examples: {'rewards_train/chosen': '0.18926', 'rewards_train/rejected': '0.059825', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12944', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-143.97', 'loss/train': '0.6513', 'examples_per_second': '30.579', 'grad_norm': '27.75', 'counters/examples': 252800, 'counters/updates': 7900}
train stats after 252832 examples: {'rewards_train/chosen': '0.13633', 'rewards_train/rejected': '0.11895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017371', 'logps_train/rejected': '-132.33', 'logps_train/chosen': '-159.83', 'loss/train': '0.70133', 'examples_per_second': '31.544', 'grad_norm': '33', 'counters/examples': 252832, 'counters/updates': 7901}
train stats after 252864 examples: {'rewards_train/chosen': '0.14775', 'rewards_train/rejected': '-0.084996', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23274', 'logps_train/rejected': '-97.653', 'logps_train/chosen': '-158.56', 'loss/train': '0.5952', 'examples_per_second': '32.7', 'grad_norm': '21.5', 'counters/examples': 252864, 'counters/updates': 7902}
train stats after 252896 examples: {'rewards_train/chosen': '0.1129', 'rewards_train/rejected': '0.0047726', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10813', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-127.14', 'loss/train': '0.65616', 'examples_per_second': '30.57', 'grad_norm': '32.5', 'counters/examples': 252896, 'counters/updates': 7903}
skipping logging after 252928 examples to avoid logging too frequently
train stats after 252960 examples: {'rewards_train/chosen': '0.045971', 'rewards_train/rejected': '-0.022942', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068913', 'logps_train/rejected': '-110.59', 'logps_train/chosen': '-160.14', 'loss/train': '0.67059', 'examples_per_second': '34.376', 'grad_norm': '28.125', 'counters/examples': 252960, 'counters/updates': 7905}
train stats after 252992 examples: {'rewards_train/chosen': '0.18664', 'rewards_train/rejected': '-0.026816', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21345', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-162.5', 'loss/train': '0.61844', 'examples_per_second': '31.533', 'grad_norm': '26.125', 'counters/examples': 252992, 'counters/updates': 7906}
train stats after 253024 examples: {'rewards_train/chosen': '0.17713', 'rewards_train/rejected': '2.2998e-05', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1771', 'logps_train/rejected': '-98.139', 'logps_train/chosen': '-146.33', 'loss/train': '0.62583', 'examples_per_second': '32.477', 'grad_norm': '23.875', 'counters/examples': 253024, 'counters/updates': 7907}
train stats after 253056 examples: {'rewards_train/chosen': '0.19556', 'rewards_train/rejected': '-0.0014717', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19703', 'logps_train/rejected': '-142.14', 'logps_train/chosen': '-134.54', 'loss/train': '0.62802', 'examples_per_second': '30.578', 'grad_norm': '44.25', 'counters/examples': 253056, 'counters/updates': 7908}
skipping logging after 253088 examples to avoid logging too frequently
train stats after 253120 examples: {'rewards_train/chosen': '0.21754', 'rewards_train/rejected': '-0.016697', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23424', 'logps_train/rejected': '-143.91', 'logps_train/chosen': '-131.8', 'loss/train': '0.593', 'examples_per_second': '34.651', 'grad_norm': '24.875', 'counters/examples': 253120, 'counters/updates': 7910}
train stats after 253152 examples: {'rewards_train/chosen': '0.12522', 'rewards_train/rejected': '0.083014', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042208', 'logps_train/rejected': '-121.25', 'logps_train/chosen': '-131.17', 'loss/train': '0.69427', 'examples_per_second': '31.469', 'grad_norm': '25', 'counters/examples': 253152, 'counters/updates': 7911}
skipping logging after 253184 examples to avoid logging too frequently
train stats after 253216 examples: {'rewards_train/chosen': '0.11024', 'rewards_train/rejected': '0.033205', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077033', 'logps_train/rejected': '-97.883', 'logps_train/chosen': '-139.39', 'loss/train': '0.67066', 'examples_per_second': '31.112', 'grad_norm': '24.375', 'counters/examples': 253216, 'counters/updates': 7913}
train stats after 253248 examples: {'rewards_train/chosen': '0.073546', 'rewards_train/rejected': '0.057983', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015563', 'logps_train/rejected': '-102.32', 'logps_train/chosen': '-104.84', 'loss/train': '0.69982', 'examples_per_second': '32.335', 'grad_norm': '23.75', 'counters/examples': 253248, 'counters/updates': 7914}
train stats after 253280 examples: {'rewards_train/chosen': '0.15564', 'rewards_train/rejected': '0.017717', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13793', 'logps_train/rejected': '-151.56', 'logps_train/chosen': '-176.66', 'loss/train': '0.64433', 'examples_per_second': '31.678', 'grad_norm': '47', 'counters/examples': 253280, 'counters/updates': 7915}
train stats after 253312 examples: {'rewards_train/chosen': '0.083608', 'rewards_train/rejected': '-0.0061344', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089743', 'logps_train/rejected': '-108.89', 'logps_train/chosen': '-144.28', 'loss/train': '0.67256', 'examples_per_second': '30.998', 'grad_norm': '27.75', 'counters/examples': 253312, 'counters/updates': 7916}
skipping logging after 253344 examples to avoid logging too frequently
train stats after 253376 examples: {'rewards_train/chosen': '0.099362', 'rewards_train/rejected': '0.015475', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083887', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-113.98', 'loss/train': '0.66089', 'examples_per_second': '31.419', 'grad_norm': '26', 'counters/examples': 253376, 'counters/updates': 7918}
skipping logging after 253408 examples to avoid logging too frequently
train stats after 253440 examples: {'rewards_train/chosen': '0.10459', 'rewards_train/rejected': '0.010644', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093947', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-124.46', 'loss/train': '0.65786', 'examples_per_second': '36.192', 'grad_norm': '24.5', 'counters/examples': 253440, 'counters/updates': 7920}
train stats after 253472 examples: {'rewards_train/chosen': '0.14972', 'rewards_train/rejected': '0.042365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10736', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-162.52', 'loss/train': '0.65333', 'examples_per_second': '31.659', 'grad_norm': '28.375', 'counters/examples': 253472, 'counters/updates': 7921}
train stats after 253504 examples: {'rewards_train/chosen': '0.14042', 'rewards_train/rejected': '-0.024528', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16495', 'logps_train/rejected': '-84.07', 'logps_train/chosen': '-147.27', 'loss/train': '0.62691', 'examples_per_second': '32.506', 'grad_norm': '21.125', 'counters/examples': 253504, 'counters/updates': 7922}
train stats after 253536 examples: {'rewards_train/chosen': '0.10549', 'rewards_train/rejected': '0.04206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063426', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-154.31', 'loss/train': '0.68241', 'examples_per_second': '31.715', 'grad_norm': '31', 'counters/examples': 253536, 'counters/updates': 7923}
train stats after 253568 examples: {'rewards_train/chosen': '0.17682', 'rewards_train/rejected': '0.0044914', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17233', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-140.95', 'loss/train': '0.62622', 'examples_per_second': '31.374', 'grad_norm': '24.625', 'counters/examples': 253568, 'counters/updates': 7924}
train stats after 253600 examples: {'rewards_train/chosen': '0.12236', 'rewards_train/rejected': '0.022383', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.099979', 'logps_train/rejected': '-125.54', 'logps_train/chosen': '-113.15', 'loss/train': '0.65932', 'examples_per_second': '31.241', 'grad_norm': '23.375', 'counters/examples': 253600, 'counters/updates': 7925}
train stats after 253632 examples: {'rewards_train/chosen': '0.14581', 'rewards_train/rejected': '0.0024759', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14333', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-122.6', 'loss/train': '0.63636', 'examples_per_second': '32.842', 'grad_norm': '21.5', 'counters/examples': 253632, 'counters/updates': 7926}
train stats after 253664 examples: {'rewards_train/chosen': '0.21507', 'rewards_train/rejected': '-0.013254', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22832', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-143.97', 'loss/train': '0.61242', 'examples_per_second': '30.949', 'grad_norm': '25', 'counters/examples': 253664, 'counters/updates': 7927}
train stats after 253696 examples: {'rewards_train/chosen': '0.11883', 'rewards_train/rejected': '0.03387', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084961', 'logps_train/rejected': '-111.65', 'logps_train/chosen': '-116.75', 'loss/train': '0.65895', 'examples_per_second': '30.641', 'grad_norm': '22.125', 'counters/examples': 253696, 'counters/updates': 7928}
train stats after 253728 examples: {'rewards_train/chosen': '0.1627', 'rewards_train/rejected': '0.039817', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12289', 'logps_train/rejected': '-156.65', 'logps_train/chosen': '-164.07', 'loss/train': '0.65145', 'examples_per_second': '31.615', 'grad_norm': '26.375', 'counters/examples': 253728, 'counters/updates': 7929}
train stats after 253760 examples: {'rewards_train/chosen': '0.17461', 'rewards_train/rejected': '-0.022421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19703', 'logps_train/rejected': '-98.595', 'logps_train/chosen': '-120.83', 'loss/train': '0.61389', 'examples_per_second': '30.049', 'grad_norm': '21', 'counters/examples': 253760, 'counters/updates': 7930}
train stats after 253792 examples: {'rewards_train/chosen': '0.25252', 'rewards_train/rejected': '0.029719', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2228', 'logps_train/rejected': '-135.4', 'logps_train/chosen': '-155.66', 'loss/train': '0.6078', 'examples_per_second': '31.379', 'grad_norm': '49.75', 'counters/examples': 253792, 'counters/updates': 7931}
skipping logging after 253824 examples to avoid logging too frequently
train stats after 253856 examples: {'rewards_train/chosen': '0.14529', 'rewards_train/rejected': '0.059357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08593', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-147.57', 'loss/train': '0.66954', 'examples_per_second': '31.624', 'grad_norm': '25.625', 'counters/examples': 253856, 'counters/updates': 7933}
skipping logging after 253888 examples to avoid logging too frequently
train stats after 253920 examples: {'rewards_train/chosen': '0.16373', 'rewards_train/rejected': '0.10631', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.057426', 'logps_train/rejected': '-148.13', 'logps_train/chosen': '-181.69', 'loss/train': '0.6761', 'examples_per_second': '31.648', 'grad_norm': '30.625', 'counters/examples': 253920, 'counters/updates': 7935}
train stats after 253952 examples: {'rewards_train/chosen': '0.16347', 'rewards_train/rejected': '0.090034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073437', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-171.39', 'loss/train': '0.66784', 'examples_per_second': '31.788', 'grad_norm': '27.125', 'counters/examples': 253952, 'counters/updates': 7936}
train stats after 253984 examples: {'rewards_train/chosen': '0.14583', 'rewards_train/rejected': '-0.001639', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14747', 'logps_train/rejected': '-112.93', 'logps_train/chosen': '-126.32', 'loss/train': '0.63266', 'examples_per_second': '31.025', 'grad_norm': '22.875', 'counters/examples': 253984, 'counters/updates': 7937}
skipping logging after 254016 examples to avoid logging too frequently
train stats after 254048 examples: {'rewards_train/chosen': '0.12428', 'rewards_train/rejected': '-0.081446', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20572', 'logps_train/rejected': '-106.34', 'logps_train/chosen': '-115.86', 'loss/train': '0.61015', 'examples_per_second': '36.247', 'grad_norm': '21.75', 'counters/examples': 254048, 'counters/updates': 7939}
train stats after 254080 examples: {'rewards_train/chosen': '0.11006', 'rewards_train/rejected': '0.041151', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068909', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-124.95', 'loss/train': '0.66958', 'examples_per_second': '31.616', 'grad_norm': '23.375', 'counters/examples': 254080, 'counters/updates': 7940}
train stats after 254112 examples: {'rewards_train/chosen': '0.19245', 'rewards_train/rejected': '0.020738', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17171', 'logps_train/rejected': '-96.528', 'logps_train/chosen': '-130.03', 'loss/train': '0.62984', 'examples_per_second': '31.955', 'grad_norm': '23.25', 'counters/examples': 254112, 'counters/updates': 7941}
skipping logging after 254144 examples to avoid logging too frequently
train stats after 254176 examples: {'rewards_train/chosen': '0.25693', 'rewards_train/rejected': '0.12714', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1298', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-131.03', 'loss/train': '0.65305', 'examples_per_second': '30.801', 'grad_norm': '30.125', 'counters/examples': 254176, 'counters/updates': 7943}
train stats after 254208 examples: {'rewards_train/chosen': '0.14988', 'rewards_train/rejected': '-0.013086', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16297', 'logps_train/rejected': '-118.36', 'logps_train/chosen': '-149.29', 'loss/train': '0.62787', 'examples_per_second': '31.318', 'grad_norm': '27.375', 'counters/examples': 254208, 'counters/updates': 7944}
train stats after 254240 examples: {'rewards_train/chosen': '0.07833', 'rewards_train/rejected': '0.039984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038346', 'logps_train/rejected': '-105.28', 'logps_train/chosen': '-122.43', 'loss/train': '0.69398', 'examples_per_second': '31.581', 'grad_norm': '24.5', 'counters/examples': 254240, 'counters/updates': 7945}
skipping logging after 254272 examples to avoid logging too frequently
train stats after 254304 examples: {'rewards_train/chosen': '0.16582', 'rewards_train/rejected': '0.068965', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096858', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-133.14', 'loss/train': '0.6585', 'examples_per_second': '30.518', 'grad_norm': '25.5', 'counters/examples': 254304, 'counters/updates': 7947}
train stats after 254336 examples: {'rewards_train/chosen': '0.12671', 'rewards_train/rejected': '0.099615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027097', 'logps_train/rejected': '-163.26', 'logps_train/chosen': '-121.65', 'loss/train': '0.69473', 'examples_per_second': '32.576', 'grad_norm': '27', 'counters/examples': 254336, 'counters/updates': 7948}
train stats after 254368 examples: {'rewards_train/chosen': '0.12472', 'rewards_train/rejected': '-0.067169', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19189', 'logps_train/rejected': '-107.72', 'logps_train/chosen': '-141.59', 'loss/train': '0.61919', 'examples_per_second': '30.168', 'grad_norm': '22.5', 'counters/examples': 254368, 'counters/updates': 7949}
train stats after 254400 examples: {'rewards_train/chosen': '0.056455', 'rewards_train/rejected': '0.014004', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04245', 'logps_train/rejected': '-94.024', 'logps_train/chosen': '-158.7', 'loss/train': '0.68184', 'examples_per_second': '32.938', 'grad_norm': '27.875', 'counters/examples': 254400, 'counters/updates': 7950}
train stats after 254432 examples: {'rewards_train/chosen': '0.14286', 'rewards_train/rejected': '0.047627', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095237', 'logps_train/rejected': '-126.98', 'logps_train/chosen': '-133.01', 'loss/train': '0.66102', 'examples_per_second': '31.571', 'grad_norm': '28.25', 'counters/examples': 254432, 'counters/updates': 7951}
train stats after 254464 examples: {'rewards_train/chosen': '0.0904', 'rewards_train/rejected': '0.042403', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047997', 'logps_train/rejected': '-123.16', 'logps_train/chosen': '-144.97', 'loss/train': '0.68031', 'examples_per_second': '30.26', 'grad_norm': '25.375', 'counters/examples': 254464, 'counters/updates': 7952}
skipping logging after 254496 examples to avoid logging too frequently
train stats after 254528 examples: {'rewards_train/chosen': '0.17634', 'rewards_train/rejected': '-0.0097901', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18613', 'logps_train/rejected': '-145.36', 'logps_train/chosen': '-144.15', 'loss/train': '0.62128', 'examples_per_second': '31.668', 'grad_norm': '25.25', 'counters/examples': 254528, 'counters/updates': 7954}
train stats after 254560 examples: {'rewards_train/chosen': '0.17281', 'rewards_train/rejected': '0.034777', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13803', 'logps_train/rejected': '-101.57', 'logps_train/chosen': '-138.67', 'loss/train': '0.63833', 'examples_per_second': '30.563', 'grad_norm': '25.75', 'counters/examples': 254560, 'counters/updates': 7955}
train stats after 254592 examples: {'rewards_train/chosen': '0.15557', 'rewards_train/rejected': '0.10698', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048586', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-122.12', 'loss/train': '0.68952', 'examples_per_second': '31.647', 'grad_norm': '27.25', 'counters/examples': 254592, 'counters/updates': 7956}
train stats after 254624 examples: {'rewards_train/chosen': '0.12483', 'rewards_train/rejected': '0.057722', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067108', 'logps_train/rejected': '-104.36', 'logps_train/chosen': '-120.14', 'loss/train': '0.67066', 'examples_per_second': '31.691', 'grad_norm': '24.5', 'counters/examples': 254624, 'counters/updates': 7957}
skipping logging after 254656 examples to avoid logging too frequently
train stats after 254688 examples: {'rewards_train/chosen': '0.23065', 'rewards_train/rejected': '0.08609', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14456', 'logps_train/rejected': '-134.47', 'logps_train/chosen': '-159.85', 'loss/train': '0.63719', 'examples_per_second': '31.473', 'grad_norm': '28.375', 'counters/examples': 254688, 'counters/updates': 7959}
skipping logging after 254720 examples to avoid logging too frequently
train stats after 254752 examples: {'rewards_train/chosen': '0.16778', 'rewards_train/rejected': '0.019811', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14797', 'logps_train/rejected': '-122.65', 'logps_train/chosen': '-139.67', 'loss/train': '0.63017', 'examples_per_second': '30.566', 'grad_norm': '24.375', 'counters/examples': 254752, 'counters/updates': 7961}
train stats after 254784 examples: {'rewards_train/chosen': '0.066812', 'rewards_train/rejected': '0.031528', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035284', 'logps_train/rejected': '-152.42', 'logps_train/chosen': '-135.74', 'loss/train': '0.68385', 'examples_per_second': '31.564', 'grad_norm': '27.5', 'counters/examples': 254784, 'counters/updates': 7962}
train stats after 254816 examples: {'rewards_train/chosen': '0.10239', 'rewards_train/rejected': '0.01312', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.089271', 'logps_train/rejected': '-160.57', 'logps_train/chosen': '-141.75', 'loss/train': '0.66465', 'examples_per_second': '30.332', 'grad_norm': '27.5', 'counters/examples': 254816, 'counters/updates': 7963}
skipping logging after 254848 examples to avoid logging too frequently
train stats after 254880 examples: {'rewards_train/chosen': '0.16544', 'rewards_train/rejected': '0.14465', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020787', 'logps_train/rejected': '-175.08', 'logps_train/chosen': '-158.93', 'loss/train': '0.69536', 'examples_per_second': '31.615', 'grad_norm': '42.75', 'counters/examples': 254880, 'counters/updates': 7965}
skipping logging after 254912 examples to avoid logging too frequently
train stats after 254944 examples: {'rewards_train/chosen': '0.091321', 'rewards_train/rejected': '0.049272', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042049', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-137.43', 'loss/train': '0.68593', 'examples_per_second': '34.56', 'grad_norm': '26.625', 'counters/examples': 254944, 'counters/updates': 7967}
train stats after 254976 examples: {'rewards_train/chosen': '0.11144', 'rewards_train/rejected': '0.036126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075318', 'logps_train/rejected': '-119.05', 'logps_train/chosen': '-110.19', 'loss/train': '0.66686', 'examples_per_second': '31.331', 'grad_norm': '23.75', 'counters/examples': 254976, 'counters/updates': 7968}
skipping logging after 255008 examples to avoid logging too frequently
train stats after 255040 examples: {'rewards_train/chosen': '0.25274', 'rewards_train/rejected': '0.079093', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17364', 'logps_train/rejected': '-129.54', 'logps_train/chosen': '-166.16', 'loss/train': '0.62365', 'examples_per_second': '31.67', 'grad_norm': '27.875', 'counters/examples': 255040, 'counters/updates': 7970}
train stats after 255072 examples: {'rewards_train/chosen': '0.19611', 'rewards_train/rejected': '0.070726', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12538', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-161.56', 'loss/train': '0.64733', 'examples_per_second': '31.645', 'grad_norm': '34', 'counters/examples': 255072, 'counters/updates': 7971}
train stats after 255104 examples: {'rewards_train/chosen': '0.16254', 'rewards_train/rejected': '-0.0065115', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16905', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-152.82', 'loss/train': '0.6298', 'examples_per_second': '30.524', 'grad_norm': '24.25', 'counters/examples': 255104, 'counters/updates': 7972}
train stats after 255136 examples: {'rewards_train/chosen': '0.16961', 'rewards_train/rejected': '-0.012025', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18164', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-136.58', 'loss/train': '0.62488', 'examples_per_second': '32.382', 'grad_norm': '27.125', 'counters/examples': 255136, 'counters/updates': 7973}
train stats after 255168 examples: {'rewards_train/chosen': '0.19211', 'rewards_train/rejected': '-0.027785', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.2199', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-166.01', 'loss/train': '0.61478', 'examples_per_second': '31.025', 'grad_norm': '25.625', 'counters/examples': 255168, 'counters/updates': 7974}
train stats after 255200 examples: {'rewards_train/chosen': '0.18147', 'rewards_train/rejected': '-0.01168', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19315', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-134.04', 'loss/train': '0.61392', 'examples_per_second': '30.631', 'grad_norm': '23.5', 'counters/examples': 255200, 'counters/updates': 7975}
skipping logging after 255232 examples to avoid logging too frequently
train stats after 255264 examples: {'rewards_train/chosen': '0.11596', 'rewards_train/rejected': '0.014814', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10114', 'logps_train/rejected': '-127.63', 'logps_train/chosen': '-156.4', 'loss/train': '0.65765', 'examples_per_second': '33.566', 'grad_norm': '28.625', 'counters/examples': 255264, 'counters/updates': 7977}
train stats after 255296 examples: {'rewards_train/chosen': '0.13958', 'rewards_train/rejected': '0.021783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1178', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-120.96', 'loss/train': '0.66771', 'examples_per_second': '31', 'grad_norm': '27.375', 'counters/examples': 255296, 'counters/updates': 7978}
train stats after 255328 examples: {'rewards_train/chosen': '0.11324', 'rewards_train/rejected': '0.023412', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089826', 'logps_train/rejected': '-105.54', 'logps_train/chosen': '-109.61', 'loss/train': '0.66615', 'examples_per_second': '31.77', 'grad_norm': '24.875', 'counters/examples': 255328, 'counters/updates': 7979}
train stats after 255360 examples: {'rewards_train/chosen': '0.12437', 'rewards_train/rejected': '-0.041412', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16578', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-149.24', 'loss/train': '0.62767', 'examples_per_second': '31.791', 'grad_norm': '25.125', 'counters/examples': 255360, 'counters/updates': 7980}
skipping logging after 255392 examples to avoid logging too frequently
train stats after 255424 examples: {'rewards_train/chosen': '0.21377', 'rewards_train/rejected': '-0.013895', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22767', 'logps_train/rejected': '-109.2', 'logps_train/chosen': '-155.05', 'loss/train': '0.61986', 'examples_per_second': '34.384', 'grad_norm': '23.5', 'counters/examples': 255424, 'counters/updates': 7982}
train stats after 255456 examples: {'rewards_train/chosen': '0.13298', 'rewards_train/rejected': '0.062098', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070879', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-131.18', 'loss/train': '0.66692', 'examples_per_second': '29.963', 'grad_norm': '22.875', 'counters/examples': 255456, 'counters/updates': 7983}
train stats after 255488 examples: {'rewards_train/chosen': '0.07657', 'rewards_train/rejected': '0.007477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069093', 'logps_train/rejected': '-147.58', 'logps_train/chosen': '-167.62', 'loss/train': '0.67644', 'examples_per_second': '31.635', 'grad_norm': '31.25', 'counters/examples': 255488, 'counters/updates': 7984}
train stats after 255520 examples: {'rewards_train/chosen': '0.085868', 'rewards_train/rejected': '0.040239', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.045629', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-101.87', 'loss/train': '0.67922', 'examples_per_second': '30.516', 'grad_norm': '22.125', 'counters/examples': 255520, 'counters/updates': 7985}
train stats after 255552 examples: {'rewards_train/chosen': '0.10458', 'rewards_train/rejected': '0.079226', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025351', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-144.82', 'loss/train': '0.69408', 'examples_per_second': '32.524', 'grad_norm': '28.75', 'counters/examples': 255552, 'counters/updates': 7986}
train stats after 255584 examples: {'rewards_train/chosen': '0.22586', 'rewards_train/rejected': '0.064683', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16118', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-148.4', 'loss/train': '0.63441', 'examples_per_second': '30.796', 'grad_norm': '27.25', 'counters/examples': 255584, 'counters/updates': 7987}
train stats after 255616 examples: {'rewards_train/chosen': '0.097256', 'rewards_train/rejected': '0.05406', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043196', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-170.61', 'loss/train': '0.68194', 'examples_per_second': '31.656', 'grad_norm': '28.125', 'counters/examples': 255616, 'counters/updates': 7988}
train stats after 255648 examples: {'rewards_train/chosen': '0.18815', 'rewards_train/rejected': '0.092698', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095455', 'logps_train/rejected': '-96.069', 'logps_train/chosen': '-93.87', 'loss/train': '0.65773', 'examples_per_second': '32.275', 'grad_norm': '19.75', 'counters/examples': 255648, 'counters/updates': 7989}
train stats after 255680 examples: {'rewards_train/chosen': '0.13682', 'rewards_train/rejected': '0.11183', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024992', 'logps_train/rejected': '-188.08', 'logps_train/chosen': '-177.94', 'loss/train': '0.7018', 'examples_per_second': '31.687', 'grad_norm': '31.5', 'counters/examples': 255680, 'counters/updates': 7990}
train stats after 255712 examples: {'rewards_train/chosen': '0.15089', 'rewards_train/rejected': '0.11326', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037638', 'logps_train/rejected': '-108.48', 'logps_train/chosen': '-131.59', 'loss/train': '0.69004', 'examples_per_second': '30.963', 'grad_norm': '27.25', 'counters/examples': 255712, 'counters/updates': 7991}
train stats after 255744 examples: {'rewards_train/chosen': '0.18051', 'rewards_train/rejected': '-0.062', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24251', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-140.24', 'loss/train': '0.59743', 'examples_per_second': '31.627', 'grad_norm': '24.25', 'counters/examples': 255744, 'counters/updates': 7992}
skipping logging after 255776 examples to avoid logging too frequently
train stats after 255808 examples: {'rewards_train/chosen': '0.13291', 'rewards_train/rejected': '0.049198', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083708', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-115.3', 'loss/train': '0.66764', 'examples_per_second': '33.36', 'grad_norm': '25.5', 'counters/examples': 255808, 'counters/updates': 7994}
train stats after 255840 examples: {'rewards_train/chosen': '0.15262', 'rewards_train/rejected': '0.01179', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14083', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-119.68', 'loss/train': '0.6389', 'examples_per_second': '30.179', 'grad_norm': '29.125', 'counters/examples': 255840, 'counters/updates': 7995}
train stats after 255872 examples: {'rewards_train/chosen': '0.14544', 'rewards_train/rejected': '-0.046606', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19205', 'logps_train/rejected': '-100.09', 'logps_train/chosen': '-130.05', 'loss/train': '0.61583', 'examples_per_second': '30.188', 'grad_norm': '21.375', 'counters/examples': 255872, 'counters/updates': 7996}
train stats after 255904 examples: {'rewards_train/chosen': '0.16621', 'rewards_train/rejected': '0.056775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10944', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-148.84', 'loss/train': '0.65382', 'examples_per_second': '31.638', 'grad_norm': '28.75', 'counters/examples': 255904, 'counters/updates': 7997}
train stats after 255936 examples: {'rewards_train/chosen': '0.14698', 'rewards_train/rejected': '-0.0053063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15229', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-156.81', 'loss/train': '0.63589', 'examples_per_second': '32.702', 'grad_norm': '28.75', 'counters/examples': 255936, 'counters/updates': 7998}
train stats after 255968 examples: {'rewards_train/chosen': '0.17399', 'rewards_train/rejected': '0.019938', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15405', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-156.16', 'loss/train': '0.62794', 'examples_per_second': '31.424', 'grad_norm': '27.5', 'counters/examples': 255968, 'counters/updates': 7999}
train stats after 256000 examples: {'rewards_train/chosen': '0.23311', 'rewards_train/rejected': '0.021574', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21153', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-124.02', 'loss/train': '0.61431', 'examples_per_second': '31.638', 'grad_norm': '26.625', 'counters/examples': 256000, 'counters/updates': 8000}
skipping logging after 256032 examples to avoid logging too frequently
train stats after 256064 examples: {'rewards_train/chosen': '0.21209', 'rewards_train/rejected': '-0.0061264', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21822', 'logps_train/rejected': '-117.5', 'logps_train/chosen': '-153.67', 'loss/train': '0.60587', 'examples_per_second': '31.396', 'grad_norm': '25.625', 'counters/examples': 256064, 'counters/updates': 8002}
skipping logging after 256096 examples to avoid logging too frequently
train stats after 256128 examples: {'rewards_train/chosen': '0.052075', 'rewards_train/rejected': '0.023237', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028838', 'logps_train/rejected': '-111.78', 'logps_train/chosen': '-121.67', 'loss/train': '0.68946', 'examples_per_second': '31.678', 'grad_norm': '25.25', 'counters/examples': 256128, 'counters/updates': 8004}
train stats after 256160 examples: {'rewards_train/chosen': '0.15797', 'rewards_train/rejected': '-0.018115', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17608', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-139.9', 'loss/train': '0.62306', 'examples_per_second': '31.104', 'grad_norm': '27.5', 'counters/examples': 256160, 'counters/updates': 8005}
train stats after 256192 examples: {'rewards_train/chosen': '0.11293', 'rewards_train/rejected': '0.084051', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02888', 'logps_train/rejected': '-100.85', 'logps_train/chosen': '-119.35', 'loss/train': '0.69251', 'examples_per_second': '31.595', 'grad_norm': '25.625', 'counters/examples': 256192, 'counters/updates': 8006}
skipping logging after 256224 examples to avoid logging too frequently
train stats after 256256 examples: {'rewards_train/chosen': '0.20313', 'rewards_train/rejected': '-0.0025023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20563', 'logps_train/rejected': '-115.16', 'logps_train/chosen': '-153.44', 'loss/train': '0.62078', 'examples_per_second': '31.088', 'grad_norm': '23.75', 'counters/examples': 256256, 'counters/updates': 8008}
train stats after 256288 examples: {'rewards_train/chosen': '0.18189', 'rewards_train/rejected': '0.10301', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.078881', 'logps_train/rejected': '-110.79', 'logps_train/chosen': '-148.16', 'loss/train': '0.67384', 'examples_per_second': '30.623', 'grad_norm': '24', 'counters/examples': 256288, 'counters/updates': 8009}
train stats after 256320 examples: {'rewards_train/chosen': '0.21882', 'rewards_train/rejected': '0.036231', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18259', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-160.11', 'loss/train': '0.62469', 'examples_per_second': '31.343', 'grad_norm': '27.625', 'counters/examples': 256320, 'counters/updates': 8010}
train stats after 256352 examples: {'rewards_train/chosen': '0.068864', 'rewards_train/rejected': '-0.023982', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092846', 'logps_train/rejected': '-106.78', 'logps_train/chosen': '-120.84', 'loss/train': '0.65989', 'examples_per_second': '30.488', 'grad_norm': '22.25', 'counters/examples': 256352, 'counters/updates': 8011}
skipping logging after 256384 examples to avoid logging too frequently
train stats after 256416 examples: {'rewards_train/chosen': '0.071354', 'rewards_train/rejected': '0.067408', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0039458', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-117.2', 'loss/train': '0.70106', 'examples_per_second': '32.354', 'grad_norm': '24.625', 'counters/examples': 256416, 'counters/updates': 8013}
train stats after 256448 examples: {'rewards_train/chosen': '0.29534', 'rewards_train/rejected': '0.12086', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17448', 'logps_train/rejected': '-170.17', 'logps_train/chosen': '-178.81', 'loss/train': '0.63439', 'examples_per_second': '30.06', 'grad_norm': '29.25', 'counters/examples': 256448, 'counters/updates': 8014}
skipping logging after 256480 examples to avoid logging too frequently
train stats after 256512 examples: {'rewards_train/chosen': '0.163', 'rewards_train/rejected': '0.010303', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1527', 'logps_train/rejected': '-96.873', 'logps_train/chosen': '-145.76', 'loss/train': '0.63248', 'examples_per_second': '30.688', 'grad_norm': '25.875', 'counters/examples': 256512, 'counters/updates': 8016}
skipping logging after 256544 examples to avoid logging too frequently
train stats after 256576 examples: {'rewards_train/chosen': '0.15164', 'rewards_train/rejected': '0.13729', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014344', 'logps_train/rejected': '-147.74', 'logps_train/chosen': '-122.17', 'loss/train': '0.69301', 'examples_per_second': '30.823', 'grad_norm': '28.5', 'counters/examples': 256576, 'counters/updates': 8018}
train stats after 256608 examples: {'rewards_train/chosen': '0.16063', 'rewards_train/rejected': '0.070988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08964', 'logps_train/rejected': '-147.57', 'logps_train/chosen': '-197.02', 'loss/train': '0.6626', 'examples_per_second': '31.639', 'grad_norm': '32.25', 'counters/examples': 256608, 'counters/updates': 8019}
train stats after 256640 examples: {'rewards_train/chosen': '0.12818', 'rewards_train/rejected': '0.079134', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049046', 'logps_train/rejected': '-97.43', 'logps_train/chosen': '-106.94', 'loss/train': '0.67635', 'examples_per_second': '30.35', 'grad_norm': '23.125', 'counters/examples': 256640, 'counters/updates': 8020}
train stats after 256672 examples: {'rewards_train/chosen': '0.16515', 'rewards_train/rejected': '0.01998', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14517', 'logps_train/rejected': '-123.62', 'logps_train/chosen': '-158.03', 'loss/train': '0.64003', 'examples_per_second': '32.398', 'grad_norm': '23.625', 'counters/examples': 256672, 'counters/updates': 8021}
train stats after 256704 examples: {'rewards_train/chosen': '0.084059', 'rewards_train/rejected': '0.030916', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053143', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-133.45', 'loss/train': '0.6862', 'examples_per_second': '32.429', 'grad_norm': '29.125', 'counters/examples': 256704, 'counters/updates': 8022}
train stats after 256736 examples: {'rewards_train/chosen': '0.075959', 'rewards_train/rejected': '0.042808', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033151', 'logps_train/rejected': '-141.43', 'logps_train/chosen': '-140.28', 'loss/train': '0.69763', 'examples_per_second': '31.488', 'grad_norm': '28.375', 'counters/examples': 256736, 'counters/updates': 8023}
train stats after 256768 examples: {'rewards_train/chosen': '0.21518', 'rewards_train/rejected': '0.074955', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14023', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-166.4', 'loss/train': '0.64284', 'examples_per_second': '31.651', 'grad_norm': '29.125', 'counters/examples': 256768, 'counters/updates': 8024}
train stats after 256800 examples: {'rewards_train/chosen': '0.14686', 'rewards_train/rejected': '-0.0375', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18436', 'logps_train/rejected': '-98.99', 'logps_train/chosen': '-119.57', 'loss/train': '0.61656', 'examples_per_second': '32.403', 'grad_norm': '23.125', 'counters/examples': 256800, 'counters/updates': 8025}
train stats after 256832 examples: {'rewards_train/chosen': '0.13012', 'rewards_train/rejected': '0.0097492', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12037', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-162.12', 'loss/train': '0.65406', 'examples_per_second': '31.691', 'grad_norm': '25.875', 'counters/examples': 256832, 'counters/updates': 8026}
skipping logging after 256864 examples to avoid logging too frequently
train stats after 256896 examples: {'rewards_train/chosen': '0.08978', 'rewards_train/rejected': '-0.0058553', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095635', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-99.791', 'loss/train': '0.65708', 'examples_per_second': '31.688', 'grad_norm': '23.375', 'counters/examples': 256896, 'counters/updates': 8028}
skipping logging after 256928 examples to avoid logging too frequently
train stats after 256960 examples: {'rewards_train/chosen': '0.072424', 'rewards_train/rejected': '0.0059564', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066468', 'logps_train/rejected': '-149.24', 'logps_train/chosen': '-175.31', 'loss/train': '0.67601', 'examples_per_second': '30.139', 'grad_norm': '33.75', 'counters/examples': 256960, 'counters/updates': 8030}
skipping logging after 256992 examples to avoid logging too frequently
train stats after 257024 examples: {'rewards_train/chosen': '0.11182', 'rewards_train/rejected': '0.017397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094422', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-144.91', 'loss/train': '0.65828', 'examples_per_second': '30.294', 'grad_norm': '27.75', 'counters/examples': 257024, 'counters/updates': 8032}
train stats after 257056 examples: {'rewards_train/chosen': '0.14053', 'rewards_train/rejected': '-0.057056', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19759', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-147.36', 'loss/train': '0.6085', 'examples_per_second': '24.19', 'grad_norm': '24', 'counters/examples': 257056, 'counters/updates': 8033}
train stats after 257088 examples: {'rewards_train/chosen': '0.1443', 'rewards_train/rejected': '0.0020215', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14227', 'logps_train/rejected': '-106.82', 'logps_train/chosen': '-143.9', 'loss/train': '0.63669', 'examples_per_second': '30.164', 'grad_norm': '26.5', 'counters/examples': 257088, 'counters/updates': 8034}
train stats after 257120 examples: {'rewards_train/chosen': '0.19101', 'rewards_train/rejected': '0.053325', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13769', 'logps_train/rejected': '-165.21', 'logps_train/chosen': '-138.13', 'loss/train': '0.63975', 'examples_per_second': '31.598', 'grad_norm': '27.375', 'counters/examples': 257120, 'counters/updates': 8035}
train stats after 257152 examples: {'rewards_train/chosen': '0.19038', 'rewards_train/rejected': '0.098943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09144', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-148.44', 'loss/train': '0.67058', 'examples_per_second': '24.293', 'grad_norm': '28.875', 'counters/examples': 257152, 'counters/updates': 8036}
train stats after 257184 examples: {'rewards_train/chosen': '0.11895', 'rewards_train/rejected': '0.035094', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.083853', 'logps_train/rejected': '-163.98', 'logps_train/chosen': '-166.12', 'loss/train': '0.66058', 'examples_per_second': '30.169', 'grad_norm': '30.25', 'counters/examples': 257184, 'counters/updates': 8037}
train stats after 257216 examples: {'rewards_train/chosen': '0.06236', 'rewards_train/rejected': '-0.015118', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077477', 'logps_train/rejected': '-106.2', 'logps_train/chosen': '-118.15', 'loss/train': '0.66543', 'examples_per_second': '30.208', 'grad_norm': '23.375', 'counters/examples': 257216, 'counters/updates': 8038}
train stats after 257248 examples: {'rewards_train/chosen': '0.22715', 'rewards_train/rejected': '0.034079', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.19307', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-135.96', 'loss/train': '0.6152', 'examples_per_second': '30.041', 'grad_norm': '25.75', 'counters/examples': 257248, 'counters/updates': 8039}
train stats after 257280 examples: {'rewards_train/chosen': '0.17512', 'rewards_train/rejected': '0.031626', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1435', 'logps_train/rejected': '-152.4', 'logps_train/chosen': '-167', 'loss/train': '0.63893', 'examples_per_second': '30.31', 'grad_norm': '27.375', 'counters/examples': 257280, 'counters/updates': 8040}
skipping logging after 257312 examples to avoid logging too frequently
train stats after 257344 examples: {'rewards_train/chosen': '0.18325', 'rewards_train/rejected': '-0.068875', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25213', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-124.72', 'loss/train': '0.59037', 'examples_per_second': '31.306', 'grad_norm': '22.375', 'counters/examples': 257344, 'counters/updates': 8042}
train stats after 257376 examples: {'rewards_train/chosen': '0.20871', 'rewards_train/rejected': '0.029837', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17887', 'logps_train/rejected': '-142.31', 'logps_train/chosen': '-131.88', 'loss/train': '0.62706', 'examples_per_second': '32.762', 'grad_norm': '27.75', 'counters/examples': 257376, 'counters/updates': 8043}
train stats after 257408 examples: {'rewards_train/chosen': '0.078356', 'rewards_train/rejected': '-0.038998', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11735', 'logps_train/rejected': '-115.55', 'logps_train/chosen': '-117.89', 'loss/train': '0.65234', 'examples_per_second': '31.411', 'grad_norm': '25.25', 'counters/examples': 257408, 'counters/updates': 8044}
skipping logging after 257440 examples to avoid logging too frequently
train stats after 257472 examples: {'rewards_train/chosen': '0.12326', 'rewards_train/rejected': '0.18666', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.063398', 'logps_train/rejected': '-111.8', 'logps_train/chosen': '-158.46', 'loss/train': '0.76481', 'examples_per_second': '31.548', 'grad_norm': '39.75', 'counters/examples': 257472, 'counters/updates': 8046}
train stats after 257504 examples: {'rewards_train/chosen': '0.10606', 'rewards_train/rejected': '0.026359', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079703', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-120.12', 'loss/train': '0.66707', 'examples_per_second': '33.032', 'grad_norm': '26', 'counters/examples': 257504, 'counters/updates': 8047}
train stats after 257536 examples: {'rewards_train/chosen': '0.18839', 'rewards_train/rejected': '-0.015178', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20357', 'logps_train/rejected': '-101.02', 'logps_train/chosen': '-141.26', 'loss/train': '0.6171', 'examples_per_second': '31.108', 'grad_norm': '23.875', 'counters/examples': 257536, 'counters/updates': 8048}
train stats after 257568 examples: {'rewards_train/chosen': '0.13958', 'rewards_train/rejected': '0.049887', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08969', 'logps_train/rejected': '-132.5', 'logps_train/chosen': '-137.56', 'loss/train': '0.66937', 'examples_per_second': '32.336', 'grad_norm': '25.75', 'counters/examples': 257568, 'counters/updates': 8049}
train stats after 257600 examples: {'rewards_train/chosen': '0.073703', 'rewards_train/rejected': '0.042369', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.031334', 'logps_train/rejected': '-104.54', 'logps_train/chosen': '-129.49', 'loss/train': '0.68946', 'examples_per_second': '32.208', 'grad_norm': '24.75', 'counters/examples': 257600, 'counters/updates': 8050}
train stats after 257632 examples: {'rewards_train/chosen': '0.14451', 'rewards_train/rejected': '0.07965', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064864', 'logps_train/rejected': '-113.94', 'logps_train/chosen': '-139.41', 'loss/train': '0.67737', 'examples_per_second': '31.498', 'grad_norm': '25.25', 'counters/examples': 257632, 'counters/updates': 8051}
train stats after 257664 examples: {'rewards_train/chosen': '0.17718', 'rewards_train/rejected': '0.0090857', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16809', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-166.7', 'loss/train': '0.63071', 'examples_per_second': '30.641', 'grad_norm': '29.5', 'counters/examples': 257664, 'counters/updates': 8052}
train stats after 257696 examples: {'rewards_train/chosen': '0.067394', 'rewards_train/rejected': '-0.028913', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096307', 'logps_train/rejected': '-180.49', 'logps_train/chosen': '-137.76', 'loss/train': '0.66065', 'examples_per_second': '31.244', 'grad_norm': '30.25', 'counters/examples': 257696, 'counters/updates': 8053}
train stats after 257728 examples: {'rewards_train/chosen': '0.18567', 'rewards_train/rejected': '0.022776', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16289', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-134.13', 'loss/train': '0.62347', 'examples_per_second': '31.6', 'grad_norm': '24.125', 'counters/examples': 257728, 'counters/updates': 8054}
train stats after 257760 examples: {'rewards_train/chosen': '0.044975', 'rewards_train/rejected': '-0.043524', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088499', 'logps_train/rejected': '-135.6', 'logps_train/chosen': '-162.51', 'loss/train': '0.66365', 'examples_per_second': '32.404', 'grad_norm': '30.375', 'counters/examples': 257760, 'counters/updates': 8055}
train stats after 257792 examples: {'rewards_train/chosen': '0.1859', 'rewards_train/rejected': '0.068888', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11701', 'logps_train/rejected': '-122.09', 'logps_train/chosen': '-132.61', 'loss/train': '0.67101', 'examples_per_second': '32.545', 'grad_norm': '25', 'counters/examples': 257792, 'counters/updates': 8056}
train stats after 257824 examples: {'rewards_train/chosen': '0.11334', 'rewards_train/rejected': '0.063178', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050165', 'logps_train/rejected': '-121.1', 'logps_train/chosen': '-122.05', 'loss/train': '0.68176', 'examples_per_second': '31.998', 'grad_norm': '25.75', 'counters/examples': 257824, 'counters/updates': 8057}
train stats after 257856 examples: {'rewards_train/chosen': '0.18535', 'rewards_train/rejected': '-0.056227', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24158', 'logps_train/rejected': '-144.36', 'logps_train/chosen': '-139.34', 'loss/train': '0.60604', 'examples_per_second': '30.476', 'grad_norm': '26.5', 'counters/examples': 257856, 'counters/updates': 8058}
train stats after 257888 examples: {'rewards_train/chosen': '0.14865', 'rewards_train/rejected': '0.11955', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0291', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-118.87', 'loss/train': '0.70162', 'examples_per_second': '32.228', 'grad_norm': '24.75', 'counters/examples': 257888, 'counters/updates': 8059}
skipping logging after 257920 examples to avoid logging too frequently
train stats after 257952 examples: {'rewards_train/chosen': '0.087957', 'rewards_train/rejected': '0.0011783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086779', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-126.73', 'loss/train': '0.66388', 'examples_per_second': '24.449', 'grad_norm': '26.75', 'counters/examples': 257952, 'counters/updates': 8061}
train stats after 257984 examples: {'rewards_train/chosen': '0.10058', 'rewards_train/rejected': '-0.0012112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10179', 'logps_train/rejected': '-124.73', 'logps_train/chosen': '-141.79', 'loss/train': '0.66137', 'examples_per_second': '32.275', 'grad_norm': '24', 'counters/examples': 257984, 'counters/updates': 8062}
train stats after 258016 examples: {'rewards_train/chosen': '0.15638', 'rewards_train/rejected': '0.056379', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099997', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-132.35', 'loss/train': '0.65758', 'examples_per_second': '32.405', 'grad_norm': '24.875', 'counters/examples': 258016, 'counters/updates': 8063}
train stats after 258048 examples: {'rewards_train/chosen': '0.093175', 'rewards_train/rejected': '0.041461', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051714', 'logps_train/rejected': '-101.26', 'logps_train/chosen': '-158.35', 'loss/train': '0.68225', 'examples_per_second': '31.675', 'grad_norm': '30.625', 'counters/examples': 258048, 'counters/updates': 8064}
train stats after 258080 examples: {'rewards_train/chosen': '0.20953', 'rewards_train/rejected': '0.025854', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18367', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-161.07', 'loss/train': '0.63373', 'examples_per_second': '30.122', 'grad_norm': '25.875', 'counters/examples': 258080, 'counters/updates': 8065}
train stats after 258112 examples: {'rewards_train/chosen': '0.19013', 'rewards_train/rejected': '0.033552', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15658', 'logps_train/rejected': '-174.24', 'logps_train/chosen': '-155.01', 'loss/train': '0.64331', 'examples_per_second': '30.158', 'grad_norm': '29.75', 'counters/examples': 258112, 'counters/updates': 8066}
skipping logging after 258144 examples to avoid logging too frequently
train stats after 258176 examples: {'rewards_train/chosen': '0.20945', 'rewards_train/rejected': '0.02584', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18361', 'logps_train/rejected': '-100.66', 'logps_train/chosen': '-164.78', 'loss/train': '0.63719', 'examples_per_second': '30.324', 'grad_norm': '24.5', 'counters/examples': 258176, 'counters/updates': 8068}
skipping logging after 258208 examples to avoid logging too frequently
train stats after 258240 examples: {'rewards_train/chosen': '0.1467', 'rewards_train/rejected': '0.034707', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11199', 'logps_train/rejected': '-135.16', 'logps_train/chosen': '-160.77', 'loss/train': '0.66535', 'examples_per_second': '31.463', 'grad_norm': '28.25', 'counters/examples': 258240, 'counters/updates': 8070}
train stats after 258272 examples: {'rewards_train/chosen': '0.012334', 'rewards_train/rejected': '0.0056802', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0066537', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-119.06', 'loss/train': '0.69656', 'examples_per_second': '30.95', 'grad_norm': '24.5', 'counters/examples': 258272, 'counters/updates': 8071}
train stats after 258304 examples: {'rewards_train/chosen': '0.078417', 'rewards_train/rejected': '0.024221', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054196', 'logps_train/rejected': '-115.89', 'logps_train/chosen': '-134.38', 'loss/train': '0.67718', 'examples_per_second': '32.694', 'grad_norm': '26.75', 'counters/examples': 258304, 'counters/updates': 8072}
train stats after 258336 examples: {'rewards_train/chosen': '0.19871', 'rewards_train/rejected': '0.030967', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.16774', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-148.41', 'loss/train': '0.62151', 'examples_per_second': '31.474', 'grad_norm': '25.875', 'counters/examples': 258336, 'counters/updates': 8073}
train stats after 258368 examples: {'rewards_train/chosen': '0.13105', 'rewards_train/rejected': '0.10764', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023409', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-117.87', 'loss/train': '0.69006', 'examples_per_second': '31.639', 'grad_norm': '24.75', 'counters/examples': 258368, 'counters/updates': 8074}
train stats after 258400 examples: {'rewards_train/chosen': '0.17609', 'rewards_train/rejected': '0.094355', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081734', 'logps_train/rejected': '-150.98', 'logps_train/chosen': '-153.39', 'loss/train': '0.66532', 'examples_per_second': '31.611', 'grad_norm': '27.125', 'counters/examples': 258400, 'counters/updates': 8075}
train stats after 258432 examples: {'rewards_train/chosen': '0.20837', 'rewards_train/rejected': '0.030931', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17744', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-139.55', 'loss/train': '0.61993', 'examples_per_second': '31.507', 'grad_norm': '25.375', 'counters/examples': 258432, 'counters/updates': 8076}
train stats after 258464 examples: {'rewards_train/chosen': '0.14149', 'rewards_train/rejected': '0.018653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12284', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-155.9', 'loss/train': '0.64238', 'examples_per_second': '30.13', 'grad_norm': '24.375', 'counters/examples': 258464, 'counters/updates': 8077}
train stats after 258496 examples: {'rewards_train/chosen': '0.25922', 'rewards_train/rejected': '0.095781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16344', 'logps_train/rejected': '-129.28', 'logps_train/chosen': '-155.29', 'loss/train': '0.63582', 'examples_per_second': '32.461', 'grad_norm': '24.25', 'counters/examples': 258496, 'counters/updates': 8078}
train stats after 258528 examples: {'rewards_train/chosen': '0.15681', 'rewards_train/rejected': '0.048347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10847', 'logps_train/rejected': '-143.15', 'logps_train/chosen': '-161.23', 'loss/train': '0.65734', 'examples_per_second': '30.165', 'grad_norm': '25.875', 'counters/examples': 258528, 'counters/updates': 8079}
skipping logging after 258560 examples to avoid logging too frequently
train stats after 258592 examples: {'rewards_train/chosen': '0.086808', 'rewards_train/rejected': '0.077813', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0089952', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-143.02', 'loss/train': '0.70105', 'examples_per_second': '31.924', 'grad_norm': '30.375', 'counters/examples': 258592, 'counters/updates': 8081}
train stats after 258624 examples: {'rewards_train/chosen': '0.16202', 'rewards_train/rejected': '0.13789', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.024122', 'logps_train/rejected': '-156.33', 'logps_train/chosen': '-169.02', 'loss/train': '0.69625', 'examples_per_second': '31.585', 'grad_norm': '31.875', 'counters/examples': 258624, 'counters/updates': 8082}
train stats after 258656 examples: {'rewards_train/chosen': '0.22193', 'rewards_train/rejected': '0.092051', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12988', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-172.28', 'loss/train': '0.64582', 'examples_per_second': '31.655', 'grad_norm': '27.625', 'counters/examples': 258656, 'counters/updates': 8083}
skipping logging after 258688 examples to avoid logging too frequently
train stats after 258720 examples: {'rewards_train/chosen': '0.20652', 'rewards_train/rejected': '-0.026125', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23264', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-162', 'loss/train': '0.60144', 'examples_per_second': '31.562', 'grad_norm': '23.875', 'counters/examples': 258720, 'counters/updates': 8085}
skipping logging after 258752 examples to avoid logging too frequently
train stats after 258784 examples: {'rewards_train/chosen': '0.17679', 'rewards_train/rejected': '-0.027246', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20404', 'logps_train/rejected': '-90.025', 'logps_train/chosen': '-146.51', 'loss/train': '0.61904', 'examples_per_second': '34.795', 'grad_norm': '22.75', 'counters/examples': 258784, 'counters/updates': 8087}
train stats after 258816 examples: {'rewards_train/chosen': '0.17375', 'rewards_train/rejected': '0.056837', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11691', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-165.89', 'loss/train': '0.65257', 'examples_per_second': '30.288', 'grad_norm': '29.375', 'counters/examples': 258816, 'counters/updates': 8088}
train stats after 258848 examples: {'rewards_train/chosen': '0.19764', 'rewards_train/rejected': '0.034705', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16294', 'logps_train/rejected': '-123.63', 'logps_train/chosen': '-153.36', 'loss/train': '0.62352', 'examples_per_second': '31.629', 'grad_norm': '26.5', 'counters/examples': 258848, 'counters/updates': 8089}
train stats after 258880 examples: {'rewards_train/chosen': '0.23799', 'rewards_train/rejected': '0.099166', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13883', 'logps_train/rejected': '-162.61', 'logps_train/chosen': '-150.98', 'loss/train': '0.64381', 'examples_per_second': '30.893', 'grad_norm': '27.75', 'counters/examples': 258880, 'counters/updates': 8090}
train stats after 258912 examples: {'rewards_train/chosen': '0.14608', 'rewards_train/rejected': '0.057094', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088981', 'logps_train/rejected': '-165.56', 'logps_train/chosen': '-139.2', 'loss/train': '0.6748', 'examples_per_second': '30.367', 'grad_norm': '31.875', 'counters/examples': 258912, 'counters/updates': 8091}
train stats after 258944 examples: {'rewards_train/chosen': '0.10947', 'rewards_train/rejected': '0.038215', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071259', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-143.09', 'loss/train': '0.67562', 'examples_per_second': '31.629', 'grad_norm': '27.875', 'counters/examples': 258944, 'counters/updates': 8092}
train stats after 258976 examples: {'rewards_train/chosen': '0.14488', 'rewards_train/rejected': '-0.019133', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16401', 'logps_train/rejected': '-100.18', 'logps_train/chosen': '-156.89', 'loss/train': '0.62398', 'examples_per_second': '32.571', 'grad_norm': '23', 'counters/examples': 258976, 'counters/updates': 8093}
train stats after 259008 examples: {'rewards_train/chosen': '0.14855', 'rewards_train/rejected': '0.036355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1122', 'logps_train/rejected': '-120.3', 'logps_train/chosen': '-158.21', 'loss/train': '0.66196', 'examples_per_second': '31.552', 'grad_norm': '27.5', 'counters/examples': 259008, 'counters/updates': 8094}
train stats after 259040 examples: {'rewards_train/chosen': '0.20797', 'rewards_train/rejected': '0.035992', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17198', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-182.36', 'loss/train': '0.63204', 'examples_per_second': '31.65', 'grad_norm': '26.375', 'counters/examples': 259040, 'counters/updates': 8095}
train stats after 259072 examples: {'rewards_train/chosen': '0.1503', 'rewards_train/rejected': '0.059489', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090814', 'logps_train/rejected': '-128.76', 'logps_train/chosen': '-152.31', 'loss/train': '0.67268', 'examples_per_second': '30.958', 'grad_norm': '27.875', 'counters/examples': 259072, 'counters/updates': 8096}
train stats after 259104 examples: {'rewards_train/chosen': '0.11341', 'rewards_train/rejected': '-0.019029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13244', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-159.93', 'loss/train': '0.64879', 'examples_per_second': '31.633', 'grad_norm': '25.625', 'counters/examples': 259104, 'counters/updates': 8097}
train stats after 259136 examples: {'rewards_train/chosen': '0.18719', 'rewards_train/rejected': '0.071017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11617', 'logps_train/rejected': '-92.531', 'logps_train/chosen': '-127.53', 'loss/train': '0.64826', 'examples_per_second': '31.96', 'grad_norm': '22.75', 'counters/examples': 259136, 'counters/updates': 8098}
train stats after 259168 examples: {'rewards_train/chosen': '0.083379', 'rewards_train/rejected': '-0.033938', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11732', 'logps_train/rejected': '-94.375', 'logps_train/chosen': '-119.82', 'loss/train': '0.64651', 'examples_per_second': '30.636', 'grad_norm': '22.875', 'counters/examples': 259168, 'counters/updates': 8099}
train stats after 259200 examples: {'rewards_train/chosen': '0.069189', 'rewards_train/rejected': '-0.16044', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22963', 'logps_train/rejected': '-155.9', 'logps_train/chosen': '-153.9', 'loss/train': '0.60579', 'examples_per_second': '31.675', 'grad_norm': '24.5', 'counters/examples': 259200, 'counters/updates': 8100}
train stats after 259232 examples: {'rewards_train/chosen': '0.2126', 'rewards_train/rejected': '0.062491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15011', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-148.77', 'loss/train': '0.63288', 'examples_per_second': '30.642', 'grad_norm': '31.375', 'counters/examples': 259232, 'counters/updates': 8101}
train stats after 259264 examples: {'rewards_train/chosen': '0.14258', 'rewards_train/rejected': '0.025905', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11668', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-136.95', 'loss/train': '0.65008', 'examples_per_second': '30.474', 'grad_norm': '25.625', 'counters/examples': 259264, 'counters/updates': 8102}
train stats after 259296 examples: {'rewards_train/chosen': '0.10739', 'rewards_train/rejected': '-0.019026', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12642', 'logps_train/rejected': '-99.914', 'logps_train/chosen': '-113.76', 'loss/train': '0.64458', 'examples_per_second': '31.205', 'grad_norm': '22.75', 'counters/examples': 259296, 'counters/updates': 8103}
train stats after 259328 examples: {'rewards_train/chosen': '0.12969', 'rewards_train/rejected': '0.057625', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.072063', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-135.15', 'loss/train': '0.6724', 'examples_per_second': '32.966', 'grad_norm': '25.75', 'counters/examples': 259328, 'counters/updates': 8104}
skipping logging after 259360 examples to avoid logging too frequently
train stats after 259392 examples: {'rewards_train/chosen': '0.12976', 'rewards_train/rejected': '0.092259', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037503', 'logps_train/rejected': '-100.47', 'logps_train/chosen': '-126.03', 'loss/train': '0.69144', 'examples_per_second': '32.059', 'grad_norm': '23.375', 'counters/examples': 259392, 'counters/updates': 8106}
train stats after 259424 examples: {'rewards_train/chosen': '0.034025', 'rewards_train/rejected': '-0.0098069', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043831', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-142.84', 'loss/train': '0.68583', 'examples_per_second': '31.477', 'grad_norm': '33', 'counters/examples': 259424, 'counters/updates': 8107}
train stats after 259456 examples: {'rewards_train/chosen': '0.17012', 'rewards_train/rejected': '-0.0068009', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17692', 'logps_train/rejected': '-123.62', 'logps_train/chosen': '-160.16', 'loss/train': '0.62539', 'examples_per_second': '31.648', 'grad_norm': '24', 'counters/examples': 259456, 'counters/updates': 8108}
train stats after 259488 examples: {'rewards_train/chosen': '0.12325', 'rewards_train/rejected': '0.0081354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11512', 'logps_train/rejected': '-105.91', 'logps_train/chosen': '-131.15', 'loss/train': '0.6476', 'examples_per_second': '30.83', 'grad_norm': '22.625', 'counters/examples': 259488, 'counters/updates': 8109}
train stats after 259520 examples: {'rewards_train/chosen': '0.12062', 'rewards_train/rejected': '-0.0044136', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12503', 'logps_train/rejected': '-131.05', 'logps_train/chosen': '-158.52', 'loss/train': '0.65616', 'examples_per_second': '30.486', 'grad_norm': '27.625', 'counters/examples': 259520, 'counters/updates': 8110}
train stats after 259552 examples: {'rewards_train/chosen': '0.12355', 'rewards_train/rejected': '0.023961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099588', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-116.58', 'loss/train': '0.66053', 'examples_per_second': '30.609', 'grad_norm': '22.625', 'counters/examples': 259552, 'counters/updates': 8111}
train stats after 259584 examples: {'rewards_train/chosen': '0.179', 'rewards_train/rejected': '-0.001928', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18093', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-140.94', 'loss/train': '0.62297', 'examples_per_second': '31.675', 'grad_norm': '24.625', 'counters/examples': 259584, 'counters/updates': 8112}
train stats after 259616 examples: {'rewards_train/chosen': '0.17442', 'rewards_train/rejected': '0.039268', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13516', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-143.13', 'loss/train': '0.64218', 'examples_per_second': '30.674', 'grad_norm': '26.125', 'counters/examples': 259616, 'counters/updates': 8113}
skipping logging after 259648 examples to avoid logging too frequently
train stats after 259680 examples: {'rewards_train/chosen': '0.11136', 'rewards_train/rejected': '-0.013241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12461', 'logps_train/rejected': '-110.41', 'logps_train/chosen': '-198.58', 'loss/train': '0.65023', 'examples_per_second': '31.646', 'grad_norm': '28.5', 'counters/examples': 259680, 'counters/updates': 8115}
train stats after 259712 examples: {'rewards_train/chosen': '0.24534', 'rewards_train/rejected': '0.032367', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21298', 'logps_train/rejected': '-141.41', 'logps_train/chosen': '-170.53', 'loss/train': '0.61246', 'examples_per_second': '31.647', 'grad_norm': '25.875', 'counters/examples': 259712, 'counters/updates': 8116}
train stats after 259744 examples: {'rewards_train/chosen': '0.20474', 'rewards_train/rejected': '0.038773', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16597', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-148', 'loss/train': '0.62715', 'examples_per_second': '32.599', 'grad_norm': '24.25', 'counters/examples': 259744, 'counters/updates': 8117}
train stats after 259776 examples: {'rewards_train/chosen': '0.10999', 'rewards_train/rejected': '0.022443', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087546', 'logps_train/rejected': '-89.485', 'logps_train/chosen': '-164.15', 'loss/train': '0.66344', 'examples_per_second': '31.672', 'grad_norm': '24.875', 'counters/examples': 259776, 'counters/updates': 8118}
train stats after 259808 examples: {'rewards_train/chosen': '0.15847', 'rewards_train/rejected': '0.062937', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095534', 'logps_train/rejected': '-138.51', 'logps_train/chosen': '-137.32', 'loss/train': '0.65833', 'examples_per_second': '31.618', 'grad_norm': '26.5', 'counters/examples': 259808, 'counters/updates': 8119}
skipping logging after 259840 examples to avoid logging too frequently
train stats after 259872 examples: {'rewards_train/chosen': '0.065246', 'rewards_train/rejected': '-0.02243', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087676', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-123.73', 'loss/train': '0.66292', 'examples_per_second': '34.275', 'grad_norm': '25.375', 'counters/examples': 259872, 'counters/updates': 8121}
train stats after 259904 examples: {'rewards_train/chosen': '0.1248', 'rewards_train/rejected': '0.011109', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11369', 'logps_train/rejected': '-101.91', 'logps_train/chosen': '-135.79', 'loss/train': '0.65577', 'examples_per_second': '32.161', 'grad_norm': '23.625', 'counters/examples': 259904, 'counters/updates': 8122}
train stats after 259936 examples: {'rewards_train/chosen': '0.20949', 'rewards_train/rejected': '0.089815', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11967', 'logps_train/rejected': '-114.91', 'logps_train/chosen': '-193.43', 'loss/train': '0.64761', 'examples_per_second': '33.243', 'grad_norm': '30.375', 'counters/examples': 259936, 'counters/updates': 8123}
train stats after 259968 examples: {'rewards_train/chosen': '0.16185', 'rewards_train/rejected': '0.014119', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14773', 'logps_train/rejected': '-98.915', 'logps_train/chosen': '-107.19', 'loss/train': '0.64706', 'examples_per_second': '31.899', 'grad_norm': '25.5', 'counters/examples': 259968, 'counters/updates': 8124}
train stats after 260000 examples: {'rewards_train/chosen': '0.14361', 'rewards_train/rejected': '0.0021244', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14149', 'logps_train/rejected': '-149.34', 'logps_train/chosen': '-128.1', 'loss/train': '0.63832', 'examples_per_second': '31.067', 'grad_norm': '27.75', 'counters/examples': 260000, 'counters/updates': 8125}
train stats after 260032 examples: {'rewards_train/chosen': '0.055536', 'rewards_train/rejected': '-0.013059', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068595', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-116.09', 'loss/train': '0.67856', 'examples_per_second': '31.346', 'grad_norm': '25.375', 'counters/examples': 260032, 'counters/updates': 8126}
skipping logging after 260064 examples to avoid logging too frequently
train stats after 260096 examples: {'rewards_train/chosen': '0.0624', 'rewards_train/rejected': '0.019916', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042484', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-139.32', 'loss/train': '0.68453', 'examples_per_second': '31.688', 'grad_norm': '26.5', 'counters/examples': 260096, 'counters/updates': 8128}
train stats after 260128 examples: {'rewards_train/chosen': '0.11386', 'rewards_train/rejected': '0.032927', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080937', 'logps_train/rejected': '-118.61', 'logps_train/chosen': '-151.86', 'loss/train': '0.66245', 'examples_per_second': '30.709', 'grad_norm': '26.75', 'counters/examples': 260128, 'counters/updates': 8129}
train stats after 260160 examples: {'rewards_train/chosen': '0.16452', 'rewards_train/rejected': '0.07164', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092878', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-134.98', 'loss/train': '0.66169', 'examples_per_second': '31.603', 'grad_norm': '23.375', 'counters/examples': 260160, 'counters/updates': 8130}
train stats after 260192 examples: {'rewards_train/chosen': '0.053471', 'rewards_train/rejected': '-0.035127', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088599', 'logps_train/rejected': '-145.81', 'logps_train/chosen': '-127.14', 'loss/train': '0.66727', 'examples_per_second': '30.475', 'grad_norm': '28.625', 'counters/examples': 260192, 'counters/updates': 8131}
train stats after 260224 examples: {'rewards_train/chosen': '0.15508', 'rewards_train/rejected': '0.057088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097994', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-148.01', 'loss/train': '0.6554', 'examples_per_second': '31.633', 'grad_norm': '24.125', 'counters/examples': 260224, 'counters/updates': 8132}
train stats after 260256 examples: {'rewards_train/chosen': '0.090772', 'rewards_train/rejected': '0.092783', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.002011', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-126.92', 'loss/train': '0.7175', 'examples_per_second': '31.392', 'grad_norm': '26.625', 'counters/examples': 260256, 'counters/updates': 8133}
train stats after 260288 examples: {'rewards_train/chosen': '0.15952', 'rewards_train/rejected': '0.0017067', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15781', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-148.03', 'loss/train': '0.63158', 'examples_per_second': '32.87', 'grad_norm': '28.75', 'counters/examples': 260288, 'counters/updates': 8134}
skipping logging after 260320 examples to avoid logging too frequently
train stats after 260352 examples: {'rewards_train/chosen': '0.13939', 'rewards_train/rejected': '0.02484', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11455', 'logps_train/rejected': '-136.62', 'logps_train/chosen': '-149.78', 'loss/train': '0.65716', 'examples_per_second': '33.398', 'grad_norm': '25.75', 'counters/examples': 260352, 'counters/updates': 8136}
train stats after 260384 examples: {'rewards_train/chosen': '0.14904', 'rewards_train/rejected': '0.074035', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075001', 'logps_train/rejected': '-156.19', 'logps_train/chosen': '-156.98', 'loss/train': '0.66629', 'examples_per_second': '32.509', 'grad_norm': '28.875', 'counters/examples': 260384, 'counters/updates': 8137}
train stats after 260416 examples: {'rewards_train/chosen': '0.19236', 'rewards_train/rejected': '0.071989', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12037', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-131.07', 'loss/train': '0.64612', 'examples_per_second': '32.29', 'grad_norm': '24.375', 'counters/examples': 260416, 'counters/updates': 8138}
train stats after 260448 examples: {'rewards_train/chosen': '0.15058', 'rewards_train/rejected': '0.029892', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12068', 'logps_train/rejected': '-122.68', 'logps_train/chosen': '-137.38', 'loss/train': '0.64499', 'examples_per_second': '31.745', 'grad_norm': '25.5', 'counters/examples': 260448, 'counters/updates': 8139}
train stats after 260480 examples: {'rewards_train/chosen': '0.078586', 'rewards_train/rejected': '0.0011415', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077444', 'logps_train/rejected': '-127.71', 'logps_train/chosen': '-161.77', 'loss/train': '0.67728', 'examples_per_second': '30.225', 'grad_norm': '27.125', 'counters/examples': 260480, 'counters/updates': 8140}
train stats after 260512 examples: {'rewards_train/chosen': '0.13706', 'rewards_train/rejected': '-0.01615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15321', 'logps_train/rejected': '-103.97', 'logps_train/chosen': '-115.27', 'loss/train': '0.6394', 'examples_per_second': '32.731', 'grad_norm': '21.375', 'counters/examples': 260512, 'counters/updates': 8141}
train stats after 260544 examples: {'rewards_train/chosen': '0.15032', 'rewards_train/rejected': '0.021793', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12853', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-114.37', 'loss/train': '0.64513', 'examples_per_second': '30.996', 'grad_norm': '24.375', 'counters/examples': 260544, 'counters/updates': 8142}
train stats after 260576 examples: {'rewards_train/chosen': '0.090506', 'rewards_train/rejected': '0.084934', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0055714', 'logps_train/rejected': '-145.27', 'logps_train/chosen': '-138.69', 'loss/train': '0.71294', 'examples_per_second': '31.638', 'grad_norm': '28.75', 'counters/examples': 260576, 'counters/updates': 8143}
train stats after 260608 examples: {'rewards_train/chosen': '0.1147', 'rewards_train/rejected': '0.042168', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072529', 'logps_train/rejected': '-121.91', 'logps_train/chosen': '-172.01', 'loss/train': '0.66578', 'examples_per_second': '31.174', 'grad_norm': '26', 'counters/examples': 260608, 'counters/updates': 8144}
train stats after 260640 examples: {'rewards_train/chosen': '0.19803', 'rewards_train/rejected': '0.062469', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13556', 'logps_train/rejected': '-143.32', 'logps_train/chosen': '-180.84', 'loss/train': '0.65883', 'examples_per_second': '30.835', 'grad_norm': '34.75', 'counters/examples': 260640, 'counters/updates': 8145}
train stats after 260672 examples: {'rewards_train/chosen': '0.028554', 'rewards_train/rejected': '0.048103', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019549', 'logps_train/rejected': '-161.86', 'logps_train/chosen': '-145.39', 'loss/train': '0.71789', 'examples_per_second': '31.188', 'grad_norm': '29.25', 'counters/examples': 260672, 'counters/updates': 8146}
skipping logging after 260704 examples to avoid logging too frequently
train stats after 260736 examples: {'rewards_train/chosen': '0.083123', 'rewards_train/rejected': '0.066621', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016502', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-146.07', 'loss/train': '0.69816', 'examples_per_second': '34.989', 'grad_norm': '26.125', 'counters/examples': 260736, 'counters/updates': 8148}
train stats after 260768 examples: {'rewards_train/chosen': '0.16522', 'rewards_train/rejected': '-0.032877', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1981', 'logps_train/rejected': '-96.162', 'logps_train/chosen': '-143.51', 'loss/train': '0.61994', 'examples_per_second': '30.939', 'grad_norm': '21.75', 'counters/examples': 260768, 'counters/updates': 8149}
skipping logging after 260800 examples to avoid logging too frequently
train stats after 260832 examples: {'rewards_train/chosen': '0.18404', 'rewards_train/rejected': '-0.018084', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20213', 'logps_train/rejected': '-117.84', 'logps_train/chosen': '-144.9', 'loss/train': '0.6128', 'examples_per_second': '31.314', 'grad_norm': '24.125', 'counters/examples': 260832, 'counters/updates': 8151}
skipping logging after 260864 examples to avoid logging too frequently
train stats after 260896 examples: {'rewards_train/chosen': '0.17593', 'rewards_train/rejected': '0.026311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14962', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-116.99', 'loss/train': '0.64088', 'examples_per_second': '33.101', 'grad_norm': '27.125', 'counters/examples': 260896, 'counters/updates': 8153}
train stats after 260928 examples: {'rewards_train/chosen': '0.14666', 'rewards_train/rejected': '0.032744', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11391', 'logps_train/rejected': '-132.22', 'logps_train/chosen': '-146.85', 'loss/train': '0.64882', 'examples_per_second': '31.609', 'grad_norm': '24.375', 'counters/examples': 260928, 'counters/updates': 8154}
train stats after 260960 examples: {'rewards_train/chosen': '0.19064', 'rewards_train/rejected': '0.092723', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097915', 'logps_train/rejected': '-94.924', 'logps_train/chosen': '-107.17', 'loss/train': '0.66032', 'examples_per_second': '31.703', 'grad_norm': '22.125', 'counters/examples': 260960, 'counters/updates': 8155}
skipping logging after 260992 examples to avoid logging too frequently
train stats after 261024 examples: {'rewards_train/chosen': '0.10059', 'rewards_train/rejected': '0.026504', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.074091', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-115.69', 'loss/train': '0.67829', 'examples_per_second': '30.459', 'grad_norm': '27.375', 'counters/examples': 261024, 'counters/updates': 8157}
train stats after 261056 examples: {'rewards_train/chosen': '0.099409', 'rewards_train/rejected': '0.046671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052737', 'logps_train/rejected': '-137.73', 'logps_train/chosen': '-142.95', 'loss/train': '0.6839', 'examples_per_second': '30.812', 'grad_norm': '28.625', 'counters/examples': 261056, 'counters/updates': 8158}
train stats after 261088 examples: {'rewards_train/chosen': '0.17936', 'rewards_train/rejected': '-0.0047857', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18415', 'logps_train/rejected': '-109.81', 'logps_train/chosen': '-153.31', 'loss/train': '0.61704', 'examples_per_second': '32.725', 'grad_norm': '27.875', 'counters/examples': 261088, 'counters/updates': 8159}
train stats after 261120 examples: {'rewards_train/chosen': '0.13948', 'rewards_train/rejected': '-0.015581', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15506', 'logps_train/rejected': '-95.25', 'logps_train/chosen': '-120.21', 'loss/train': '0.62702', 'examples_per_second': '30.659', 'grad_norm': '22.25', 'counters/examples': 261120, 'counters/updates': 8160}
skipping logging after 261152 examples to avoid logging too frequently
train stats after 261184 examples: {'rewards_train/chosen': '0.10294', 'rewards_train/rejected': '0.030675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072261', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-138.31', 'loss/train': '0.67531', 'examples_per_second': '31.174', 'grad_norm': '26.75', 'counters/examples': 261184, 'counters/updates': 8162}
train stats after 261216 examples: {'rewards_train/chosen': '0.19255', 'rewards_train/rejected': '0.034465', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15808', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-160.19', 'loss/train': '0.64528', 'examples_per_second': '30.672', 'grad_norm': '31', 'counters/examples': 261216, 'counters/updates': 8163}
train stats after 261248 examples: {'rewards_train/chosen': '0.17836', 'rewards_train/rejected': '-0.036028', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21439', 'logps_train/rejected': '-105.6', 'logps_train/chosen': '-129.89', 'loss/train': '0.60786', 'examples_per_second': '31.034', 'grad_norm': '22', 'counters/examples': 261248, 'counters/updates': 8164}
train stats after 261280 examples: {'rewards_train/chosen': '0.062599', 'rewards_train/rejected': '6.5793e-05', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.062533', 'logps_train/rejected': '-83.942', 'logps_train/chosen': '-134.33', 'loss/train': '0.68086', 'examples_per_second': '30.944', 'grad_norm': '25.5', 'counters/examples': 261280, 'counters/updates': 8165}
train stats after 261312 examples: {'rewards_train/chosen': '0.11887', 'rewards_train/rejected': '-0.0051993', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12406', 'logps_train/rejected': '-121.6', 'logps_train/chosen': '-133.4', 'loss/train': '0.64968', 'examples_per_second': '31.657', 'grad_norm': '29.75', 'counters/examples': 261312, 'counters/updates': 8166}
train stats after 261344 examples: {'rewards_train/chosen': '0.11479', 'rewards_train/rejected': '0.12086', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.006075', 'logps_train/rejected': '-170.11', 'logps_train/chosen': '-174.8', 'loss/train': '0.7109', 'examples_per_second': '32.81', 'grad_norm': '35.75', 'counters/examples': 261344, 'counters/updates': 8167}
train stats after 261376 examples: {'rewards_train/chosen': '0.094855', 'rewards_train/rejected': '-0.032708', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12756', 'logps_train/rejected': '-121.51', 'logps_train/chosen': '-111.12', 'loss/train': '0.64499', 'examples_per_second': '30.191', 'grad_norm': '23.25', 'counters/examples': 261376, 'counters/updates': 8168}
skipping logging after 261408 examples to avoid logging too frequently
train stats after 261440 examples: {'rewards_train/chosen': '0.29073', 'rewards_train/rejected': '0.051682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23904', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-166.88', 'loss/train': '0.60171', 'examples_per_second': '31.085', 'grad_norm': '26.25', 'counters/examples': 261440, 'counters/updates': 8170}
train stats after 261472 examples: {'rewards_train/chosen': '0.10458', 'rewards_train/rejected': '0.0084414', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096138', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-118.28', 'loss/train': '0.66381', 'examples_per_second': '31.597', 'grad_norm': '25.125', 'counters/examples': 261472, 'counters/updates': 8171}
skipping logging after 261504 examples to avoid logging too frequently
train stats after 261536 examples: {'rewards_train/chosen': '0.23964', 'rewards_train/rejected': '0.13275', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10689', 'logps_train/rejected': '-139.16', 'logps_train/chosen': '-142.56', 'loss/train': '0.66488', 'examples_per_second': '31.457', 'grad_norm': '27', 'counters/examples': 261536, 'counters/updates': 8173}
train stats after 261568 examples: {'rewards_train/chosen': '0.090105', 'rewards_train/rejected': '0.052284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037821', 'logps_train/rejected': '-102.77', 'logps_train/chosen': '-138.4', 'loss/train': '0.68853', 'examples_per_second': '32.159', 'grad_norm': '27.125', 'counters/examples': 261568, 'counters/updates': 8174}
train stats after 261600 examples: {'rewards_train/chosen': '0.092108', 'rewards_train/rejected': '-0.023923', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11603', 'logps_train/rejected': '-100.68', 'logps_train/chosen': '-106.78', 'loss/train': '0.65431', 'examples_per_second': '31.383', 'grad_norm': '20.625', 'counters/examples': 261600, 'counters/updates': 8175}
train stats after 261632 examples: {'rewards_train/chosen': '0.10075', 'rewards_train/rejected': '-0.026607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12735', 'logps_train/rejected': '-140.49', 'logps_train/chosen': '-151.07', 'loss/train': '0.65914', 'examples_per_second': '29.924', 'grad_norm': '28.625', 'counters/examples': 261632, 'counters/updates': 8176}
skipping logging after 261664 examples to avoid logging too frequently
train stats after 261696 examples: {'rewards_train/chosen': '0.12821', 'rewards_train/rejected': '0.013626', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11458', 'logps_train/rejected': '-98.978', 'logps_train/chosen': '-107.08', 'loss/train': '0.66098', 'examples_per_second': '31.851', 'grad_norm': '25.5', 'counters/examples': 261696, 'counters/updates': 8178}
skipping logging after 261728 examples to avoid logging too frequently
train stats after 261760 examples: {'rewards_train/chosen': '0.10091', 'rewards_train/rejected': '-0.054787', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1557', 'logps_train/rejected': '-92.662', 'logps_train/chosen': '-140.35', 'loss/train': '0.63396', 'examples_per_second': '33.714', 'grad_norm': '23.25', 'counters/examples': 261760, 'counters/updates': 8180}
train stats after 261792 examples: {'rewards_train/chosen': '0.16954', 'rewards_train/rejected': '0.0049109', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16463', 'logps_train/rejected': '-115.87', 'logps_train/chosen': '-129.66', 'loss/train': '0.62448', 'examples_per_second': '31.478', 'grad_norm': '21.625', 'counters/examples': 261792, 'counters/updates': 8181}
train stats after 261824 examples: {'rewards_train/chosen': '0.18893', 'rewards_train/rejected': '-0.020673', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2096', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-136.88', 'loss/train': '0.6111', 'examples_per_second': '32.549', 'grad_norm': '23.875', 'counters/examples': 261824, 'counters/updates': 8182}
train stats after 261856 examples: {'rewards_train/chosen': '0.26798', 'rewards_train/rejected': '0.054544', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21344', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-143.42', 'loss/train': '0.61097', 'examples_per_second': '31.813', 'grad_norm': '25.625', 'counters/examples': 261856, 'counters/updates': 8183}
train stats after 261888 examples: {'rewards_train/chosen': '0.19604', 'rewards_train/rejected': '-0.014866', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21091', 'logps_train/rejected': '-110.73', 'logps_train/chosen': '-162.01', 'loss/train': '0.60785', 'examples_per_second': '31.393', 'grad_norm': '27.375', 'counters/examples': 261888, 'counters/updates': 8184}
train stats after 261920 examples: {'rewards_train/chosen': '0.13857', 'rewards_train/rejected': '0.069621', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068944', 'logps_train/rejected': '-150.73', 'logps_train/chosen': '-147.65', 'loss/train': '0.67604', 'examples_per_second': '31.021', 'grad_norm': '28.375', 'counters/examples': 261920, 'counters/updates': 8185}
train stats after 261952 examples: {'rewards_train/chosen': '0.10167', 'rewards_train/rejected': '0.048624', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05305', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-127.24', 'loss/train': '0.67697', 'examples_per_second': '33.109', 'grad_norm': '28.375', 'counters/examples': 261952, 'counters/updates': 8186}
skipping logging after 261984 examples to avoid logging too frequently
train stats after 262016 examples: {'rewards_train/chosen': '0.18827', 'rewards_train/rejected': '-0.063349', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25162', 'logps_train/rejected': '-95.429', 'logps_train/chosen': '-141.69', 'loss/train': '0.60507', 'examples_per_second': '32.386', 'grad_norm': '23.5', 'counters/examples': 262016, 'counters/updates': 8188}
train stats after 262048 examples: {'rewards_train/chosen': '0.18979', 'rewards_train/rejected': '0.047655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14214', 'logps_train/rejected': '-160.45', 'logps_train/chosen': '-165.43', 'loss/train': '0.63967', 'examples_per_second': '30.207', 'grad_norm': '26.75', 'counters/examples': 262048, 'counters/updates': 8189}
train stats after 262080 examples: {'rewards_train/chosen': '0.19125', 'rewards_train/rejected': '0.0044628', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18679', 'logps_train/rejected': '-101.94', 'logps_train/chosen': '-130.93', 'loss/train': '0.62688', 'examples_per_second': '31.939', 'grad_norm': '21.625', 'counters/examples': 262080, 'counters/updates': 8190}
train stats after 262112 examples: {'rewards_train/chosen': '0.11271', 'rewards_train/rejected': '0.084637', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028075', 'logps_train/rejected': '-156.53', 'logps_train/chosen': '-158.77', 'loss/train': '0.69598', 'examples_per_second': '30.784', 'grad_norm': '29.5', 'counters/examples': 262112, 'counters/updates': 8191}
train stats after 262144 examples: {'rewards_train/chosen': '0.14419', 'rewards_train/rejected': '0.035789', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1084', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-150.56', 'loss/train': '0.65565', 'examples_per_second': '31.677', 'grad_norm': '28.5', 'counters/examples': 262144, 'counters/updates': 8192}
train stats after 262176 examples: {'rewards_train/chosen': '0.14777', 'rewards_train/rejected': '0.015976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13179', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-129.76', 'loss/train': '0.65106', 'examples_per_second': '30.656', 'grad_norm': '26.5', 'counters/examples': 262176, 'counters/updates': 8193}
train stats after 262208 examples: {'rewards_train/chosen': '0.15407', 'rewards_train/rejected': '0.12583', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028234', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-130.77', 'loss/train': '0.69621', 'examples_per_second': '32.554', 'grad_norm': '30.625', 'counters/examples': 262208, 'counters/updates': 8194}
train stats after 262240 examples: {'rewards_train/chosen': '0.047632', 'rewards_train/rejected': '0.064875', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017243', 'logps_train/rejected': '-146.97', 'logps_train/chosen': '-154.59', 'loss/train': '0.71388', 'examples_per_second': '31.475', 'grad_norm': '32.75', 'counters/examples': 262240, 'counters/updates': 8195}
train stats after 262272 examples: {'rewards_train/chosen': '0.094007', 'rewards_train/rejected': '0.017582', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.076426', 'logps_train/rejected': '-104.52', 'logps_train/chosen': '-128.52', 'loss/train': '0.66969', 'examples_per_second': '31.745', 'grad_norm': '27.625', 'counters/examples': 262272, 'counters/updates': 8196}
train stats after 262304 examples: {'rewards_train/chosen': '0.15717', 'rewards_train/rejected': '-0.029704', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18687', 'logps_train/rejected': '-98.218', 'logps_train/chosen': '-140.07', 'loss/train': '0.61762', 'examples_per_second': '32.157', 'grad_norm': '20.25', 'counters/examples': 262304, 'counters/updates': 8197}
train stats after 262336 examples: {'rewards_train/chosen': '0.23503', 'rewards_train/rejected': '0.098639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1364', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-143.61', 'loss/train': '0.64489', 'examples_per_second': '31.537', 'grad_norm': '28.875', 'counters/examples': 262336, 'counters/updates': 8198}
train stats after 262368 examples: {'rewards_train/chosen': '0.1159', 'rewards_train/rejected': '0.052813', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063092', 'logps_train/rejected': '-132.54', 'logps_train/chosen': '-141.13', 'loss/train': '0.67121', 'examples_per_second': '31.551', 'grad_norm': '27', 'counters/examples': 262368, 'counters/updates': 8199}
train stats after 262400 examples: {'rewards_train/chosen': '0.031322', 'rewards_train/rejected': '0.01465', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016672', 'logps_train/rejected': '-139.65', 'logps_train/chosen': '-100.99', 'loss/train': '0.70049', 'examples_per_second': '31.606', 'grad_norm': '24.125', 'counters/examples': 262400, 'counters/updates': 8200}
train stats after 262432 examples: {'rewards_train/chosen': '0.10758', 'rewards_train/rejected': '0.020821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086758', 'logps_train/rejected': '-126.43', 'logps_train/chosen': '-109.2', 'loss/train': '0.67304', 'examples_per_second': '30.273', 'grad_norm': '22.25', 'counters/examples': 262432, 'counters/updates': 8201}
skipping logging after 262464 examples to avoid logging too frequently
train stats after 262496 examples: {'rewards_train/chosen': '0.24666', 'rewards_train/rejected': '0.051623', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19504', 'logps_train/rejected': '-136.29', 'logps_train/chosen': '-157.36', 'loss/train': '0.61598', 'examples_per_second': '31.742', 'grad_norm': '26.25', 'counters/examples': 262496, 'counters/updates': 8203}
train stats after 262528 examples: {'rewards_train/chosen': '0.18509', 'rewards_train/rejected': '0.072292', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1128', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-144.83', 'loss/train': '0.65461', 'examples_per_second': '24.308', 'grad_norm': '27', 'counters/examples': 262528, 'counters/updates': 8204}
train stats after 262560 examples: {'rewards_train/chosen': '0.20406', 'rewards_train/rejected': '0.076271', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12779', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-137.39', 'loss/train': '0.64366', 'examples_per_second': '31.638', 'grad_norm': '27.125', 'counters/examples': 262560, 'counters/updates': 8205}
train stats after 262592 examples: {'rewards_train/chosen': '0.072449', 'rewards_train/rejected': '0.0030314', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069418', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-140.29', 'loss/train': '0.67267', 'examples_per_second': '31.666', 'grad_norm': '25.625', 'counters/examples': 262592, 'counters/updates': 8206}
train stats after 262624 examples: {'rewards_train/chosen': '0.16666', 'rewards_train/rejected': '0.058422', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10823', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-153.09', 'loss/train': '0.67218', 'examples_per_second': '26.844', 'grad_norm': '34.75', 'counters/examples': 262624, 'counters/updates': 8207}
train stats after 262656 examples: {'rewards_train/chosen': '0.17724', 'rewards_train/rejected': '0.12371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053528', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-132.96', 'loss/train': '0.6892', 'examples_per_second': '30.598', 'grad_norm': '25.75', 'counters/examples': 262656, 'counters/updates': 8208}
train stats after 262688 examples: {'rewards_train/chosen': '0.24963', 'rewards_train/rejected': '0.081555', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16808', 'logps_train/rejected': '-144.46', 'logps_train/chosen': '-173.92', 'loss/train': '0.63119', 'examples_per_second': '33.268', 'grad_norm': '25.875', 'counters/examples': 262688, 'counters/updates': 8209}
skipping logging after 262720 examples to avoid logging too frequently
train stats after 262752 examples: {'rewards_train/chosen': '0.052001', 'rewards_train/rejected': '0.013588', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038412', 'logps_train/rejected': '-154.49', 'logps_train/chosen': '-127.24', 'loss/train': '0.69539', 'examples_per_second': '31.771', 'grad_norm': '32.5', 'counters/examples': 262752, 'counters/updates': 8211}
train stats after 262784 examples: {'rewards_train/chosen': '0.20333', 'rewards_train/rejected': '-0.053728', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25705', 'logps_train/rejected': '-147.96', 'logps_train/chosen': '-142.55', 'loss/train': '0.59187', 'examples_per_second': '32.159', 'grad_norm': '24', 'counters/examples': 262784, 'counters/updates': 8212}
train stats after 262816 examples: {'rewards_train/chosen': '0.1662', 'rewards_train/rejected': '0.1134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052801', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-145.04', 'loss/train': '0.68455', 'examples_per_second': '31.373', 'grad_norm': '26.625', 'counters/examples': 262816, 'counters/updates': 8213}
train stats after 262848 examples: {'rewards_train/chosen': '0.14767', 'rewards_train/rejected': '0.09156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056106', 'logps_train/rejected': '-154.59', 'logps_train/chosen': '-179.26', 'loss/train': '0.68244', 'examples_per_second': '30.083', 'grad_norm': '29.625', 'counters/examples': 262848, 'counters/updates': 8214}
skipping logging after 262880 examples to avoid logging too frequently
train stats after 262912 examples: {'rewards_train/chosen': '0.22663', 'rewards_train/rejected': '0.078953', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14767', 'logps_train/rejected': '-135.29', 'logps_train/chosen': '-158.67', 'loss/train': '0.63918', 'examples_per_second': '34.317', 'grad_norm': '26.875', 'counters/examples': 262912, 'counters/updates': 8216}
skipping logging after 262944 examples to avoid logging too frequently
train stats after 262976 examples: {'rewards_train/chosen': '0.21372', 'rewards_train/rejected': '0.01301', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20071', 'logps_train/rejected': '-154.57', 'logps_train/chosen': '-150.94', 'loss/train': '0.61858', 'examples_per_second': '31.446', 'grad_norm': '26.375', 'counters/examples': 262976, 'counters/updates': 8218}
train stats after 263008 examples: {'rewards_train/chosen': '0.19814', 'rewards_train/rejected': '0.056818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14132', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-126.66', 'loss/train': '0.6436', 'examples_per_second': '30.42', 'grad_norm': '24.75', 'counters/examples': 263008, 'counters/updates': 8219}
skipping logging after 263040 examples to avoid logging too frequently
train stats after 263072 examples: {'rewards_train/chosen': '0.20132', 'rewards_train/rejected': '0.012344', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18898', 'logps_train/rejected': '-140.84', 'logps_train/chosen': '-144.57', 'loss/train': '0.62024', 'examples_per_second': '30.933', 'grad_norm': '25', 'counters/examples': 263072, 'counters/updates': 8221}
skipping logging after 263104 examples to avoid logging too frequently
train stats after 263136 examples: {'rewards_train/chosen': '0.14681', 'rewards_train/rejected': '0.11171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035099', 'logps_train/rejected': '-151.44', 'logps_train/chosen': '-159.66', 'loss/train': '0.69498', 'examples_per_second': '31.623', 'grad_norm': '31.625', 'counters/examples': 263136, 'counters/updates': 8223}
train stats after 263168 examples: {'rewards_train/chosen': '0.14837', 'rewards_train/rejected': '-0.014286', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16266', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-158.22', 'loss/train': '0.63625', 'examples_per_second': '32.718', 'grad_norm': '27.875', 'counters/examples': 263168, 'counters/updates': 8224}
train stats after 263200 examples: {'rewards_train/chosen': '0.15978', 'rewards_train/rejected': '0.0081711', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15161', 'logps_train/rejected': '-106.31', 'logps_train/chosen': '-139.28', 'loss/train': '0.63725', 'examples_per_second': '30.918', 'grad_norm': '23.625', 'counters/examples': 263200, 'counters/updates': 8225}
train stats after 263232 examples: {'rewards_train/chosen': '0.12725', 'rewards_train/rejected': '0.0060064', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12124', 'logps_train/rejected': '-111.23', 'logps_train/chosen': '-167.18', 'loss/train': '0.66108', 'examples_per_second': '32.896', 'grad_norm': '30.125', 'counters/examples': 263232, 'counters/updates': 8226}
train stats after 263264 examples: {'rewards_train/chosen': '0.25212', 'rewards_train/rejected': '0.1113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14082', 'logps_train/rejected': '-168.48', 'logps_train/chosen': '-185.3', 'loss/train': '0.64094', 'examples_per_second': '31.531', 'grad_norm': '36', 'counters/examples': 263264, 'counters/updates': 8227}
train stats after 263296 examples: {'rewards_train/chosen': '0.14989', 'rewards_train/rejected': '0.0086915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1412', 'logps_train/rejected': '-141.06', 'logps_train/chosen': '-180.3', 'loss/train': '0.64922', 'examples_per_second': '31.069', 'grad_norm': '32', 'counters/examples': 263296, 'counters/updates': 8228}
train stats after 263328 examples: {'rewards_train/chosen': '0.22738', 'rewards_train/rejected': '-0.042125', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.2695', 'logps_train/rejected': '-119.18', 'logps_train/chosen': '-153.63', 'loss/train': '0.57731', 'examples_per_second': '32.651', 'grad_norm': '27.875', 'counters/examples': 263328, 'counters/updates': 8229}
train stats after 263360 examples: {'rewards_train/chosen': '0.20129', 'rewards_train/rejected': '-0.023031', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22432', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-167.72', 'loss/train': '0.60399', 'examples_per_second': '31.745', 'grad_norm': '25.625', 'counters/examples': 263360, 'counters/updates': 8230}
train stats after 263392 examples: {'rewards_train/chosen': '0.20073', 'rewards_train/rejected': '0.03907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16166', 'logps_train/rejected': '-100.66', 'logps_train/chosen': '-145.48', 'loss/train': '0.63746', 'examples_per_second': '31.606', 'grad_norm': '28', 'counters/examples': 263392, 'counters/updates': 8231}
train stats after 263424 examples: {'rewards_train/chosen': '0.081273', 'rewards_train/rejected': '-0.014918', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096191', 'logps_train/rejected': '-96.62', 'logps_train/chosen': '-150.68', 'loss/train': '0.65605', 'examples_per_second': '31.603', 'grad_norm': '26.625', 'counters/examples': 263424, 'counters/updates': 8232}
train stats after 263456 examples: {'rewards_train/chosen': '0.13182', 'rewards_train/rejected': '-0.033461', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16528', 'logps_train/rejected': '-97.521', 'logps_train/chosen': '-119.9', 'loss/train': '0.63189', 'examples_per_second': '31.595', 'grad_norm': '22.625', 'counters/examples': 263456, 'counters/updates': 8233}
train stats after 263488 examples: {'rewards_train/chosen': '0.22413', 'rewards_train/rejected': '0.032741', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19139', 'logps_train/rejected': '-157.59', 'logps_train/chosen': '-137.02', 'loss/train': '0.62304', 'examples_per_second': '31.611', 'grad_norm': '26', 'counters/examples': 263488, 'counters/updates': 8234}
train stats after 263520 examples: {'rewards_train/chosen': '0.2377', 'rewards_train/rejected': '0.031263', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20643', 'logps_train/rejected': '-146.86', 'logps_train/chosen': '-170.27', 'loss/train': '0.60914', 'examples_per_second': '23.88', 'grad_norm': '28.25', 'counters/examples': 263520, 'counters/updates': 8235}
train stats after 263552 examples: {'rewards_train/chosen': '0.11468', 'rewards_train/rejected': '0.076119', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038563', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-144.38', 'loss/train': '0.70025', 'examples_per_second': '30.166', 'grad_norm': '29.875', 'counters/examples': 263552, 'counters/updates': 8236}
train stats after 263584 examples: {'rewards_train/chosen': '0.11157', 'rewards_train/rejected': '0.043023', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068544', 'logps_train/rejected': '-101.36', 'logps_train/chosen': '-119.5', 'loss/train': '0.6738', 'examples_per_second': '31.362', 'grad_norm': '24.5', 'counters/examples': 263584, 'counters/updates': 8237}
skipping logging after 263616 examples to avoid logging too frequently
train stats after 263648 examples: {'rewards_train/chosen': '0.11658', 'rewards_train/rejected': '-0.011848', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12842', 'logps_train/rejected': '-132.29', 'logps_train/chosen': '-121.08', 'loss/train': '0.63862', 'examples_per_second': '36.298', 'grad_norm': '24.5', 'counters/examples': 263648, 'counters/updates': 8239}
train stats after 263680 examples: {'rewards_train/chosen': '0.084549', 'rewards_train/rejected': '0.013993', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070556', 'logps_train/rejected': '-113.96', 'logps_train/chosen': '-115.05', 'loss/train': '0.67244', 'examples_per_second': '32.095', 'grad_norm': '23.375', 'counters/examples': 263680, 'counters/updates': 8240}
train stats after 263712 examples: {'rewards_train/chosen': '0.21608', 'rewards_train/rejected': '0.061329', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15475', 'logps_train/rejected': '-149.91', 'logps_train/chosen': '-148.96', 'loss/train': '0.63444', 'examples_per_second': '32.107', 'grad_norm': '29.375', 'counters/examples': 263712, 'counters/updates': 8241}
train stats after 263744 examples: {'rewards_train/chosen': '0.087553', 'rewards_train/rejected': '0.079571', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0079818', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-114.4', 'loss/train': '0.70051', 'examples_per_second': '30.597', 'grad_norm': '28.5', 'counters/examples': 263744, 'counters/updates': 8242}
train stats after 263776 examples: {'rewards_train/chosen': '0.12359', 'rewards_train/rejected': '0.064178', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05941', 'logps_train/rejected': '-119', 'logps_train/chosen': '-106.05', 'loss/train': '0.67655', 'examples_per_second': '31.495', 'grad_norm': '24.375', 'counters/examples': 263776, 'counters/updates': 8243}
train stats after 263808 examples: {'rewards_train/chosen': '0.21259', 'rewards_train/rejected': '0.062265', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15033', 'logps_train/rejected': '-153.56', 'logps_train/chosen': '-188.79', 'loss/train': '0.64194', 'examples_per_second': '31.712', 'grad_norm': '28', 'counters/examples': 263808, 'counters/updates': 8244}
train stats after 263840 examples: {'rewards_train/chosen': '0.18882', 'rewards_train/rejected': '0.019076', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16974', 'logps_train/rejected': '-155.01', 'logps_train/chosen': '-163.75', 'loss/train': '0.63057', 'examples_per_second': '30.124', 'grad_norm': '25.5', 'counters/examples': 263840, 'counters/updates': 8245}
skipping logging after 263872 examples to avoid logging too frequently
train stats after 263904 examples: {'rewards_train/chosen': '0.15242', 'rewards_train/rejected': '0.0016333', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15078', 'logps_train/rejected': '-91.491', 'logps_train/chosen': '-135.73', 'loss/train': '0.63402', 'examples_per_second': '31.45', 'grad_norm': '23.625', 'counters/examples': 263904, 'counters/updates': 8247}
train stats after 263936 examples: {'rewards_train/chosen': '0.23914', 'rewards_train/rejected': '0.10213', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.137', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-169.02', 'loss/train': '0.65192', 'examples_per_second': '29.996', 'grad_norm': '48.5', 'counters/examples': 263936, 'counters/updates': 8248}
train stats after 263968 examples: {'rewards_train/chosen': '0.1048', 'rewards_train/rejected': '0.04586', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058939', 'logps_train/rejected': '-112.84', 'logps_train/chosen': '-142.48', 'loss/train': '0.67661', 'examples_per_second': '31.168', 'grad_norm': '29.25', 'counters/examples': 263968, 'counters/updates': 8249}
train stats after 264000 examples: {'rewards_train/chosen': '0.14381', 'rewards_train/rejected': '0.021464', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12235', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-170.59', 'loss/train': '0.6419', 'examples_per_second': '30.877', 'grad_norm': '27.375', 'counters/examples': 264000, 'counters/updates': 8250}
Running evaluation after 264000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 264000: {'rewards_eval/chosen': '0.16261', 'rewards_eval/rejected': '0.040174', 'rewards_eval/accuracies': '0.64062', 'rewards_eval/margins': '0.12243', 'logps_eval/rejected': '-118.21', 'logps_eval/chosen': '-137.81', 'loss/eval': '0.65385'}
skipping save for non epoch
train stats after 264032 examples: {'rewards_train/chosen': '0.064218', 'rewards_train/rejected': '0.01627', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047949', 'logps_train/rejected': '-131.03', 'logps_train/chosen': '-145.4', 'loss/train': '0.69173', 'examples_per_second': '36.533', 'grad_norm': '28.375', 'counters/examples': 264032, 'counters/updates': 8251}
train stats after 264064 examples: {'rewards_train/chosen': '0.11884', 'rewards_train/rejected': '0.018543', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1003', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-126.7', 'loss/train': '0.651', 'examples_per_second': '32.476', 'grad_norm': '22.375', 'counters/examples': 264064, 'counters/updates': 8252}
skipping logging after 264096 examples to avoid logging too frequently
train stats after 264128 examples: {'rewards_train/chosen': '0.083338', 'rewards_train/rejected': '-0.048522', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13186', 'logps_train/rejected': '-135.42', 'logps_train/chosen': '-165.46', 'loss/train': '0.6402', 'examples_per_second': '31.399', 'grad_norm': '26.5', 'counters/examples': 264128, 'counters/updates': 8254}
train stats after 264160 examples: {'rewards_train/chosen': '0.21388', 'rewards_train/rejected': '-0.020056', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23394', 'logps_train/rejected': '-90.241', 'logps_train/chosen': '-142.67', 'loss/train': '0.60316', 'examples_per_second': '31.733', 'grad_norm': '25.125', 'counters/examples': 264160, 'counters/updates': 8255}
train stats after 264192 examples: {'rewards_train/chosen': '0.036482', 'rewards_train/rejected': '0.044595', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0081131', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-145.44', 'loss/train': '0.71024', 'examples_per_second': '32.048', 'grad_norm': '28.625', 'counters/examples': 264192, 'counters/updates': 8256}
train stats after 264224 examples: {'rewards_train/chosen': '0.19845', 'rewards_train/rejected': '-0.00065798', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19911', 'logps_train/rejected': '-93.465', 'logps_train/chosen': '-116.87', 'loss/train': '0.61477', 'examples_per_second': '31.032', 'grad_norm': '23.75', 'counters/examples': 264224, 'counters/updates': 8257}
train stats after 264256 examples: {'rewards_train/chosen': '0.13592', 'rewards_train/rejected': '0.027871', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10805', 'logps_train/rejected': '-132.63', 'logps_train/chosen': '-112.73', 'loss/train': '0.65605', 'examples_per_second': '30.731', 'grad_norm': '25.625', 'counters/examples': 264256, 'counters/updates': 8258}
skipping logging after 264288 examples to avoid logging too frequently
train stats after 264320 examples: {'rewards_train/chosen': '0.10984', 'rewards_train/rejected': '0.088332', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.021503', 'logps_train/rejected': '-105.86', 'logps_train/chosen': '-156.71', 'loss/train': '0.69999', 'examples_per_second': '31.589', 'grad_norm': '26.5', 'counters/examples': 264320, 'counters/updates': 8260}
train stats after 264352 examples: {'rewards_train/chosen': '0.074348', 'rewards_train/rejected': '0.059033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015315', 'logps_train/rejected': '-110.16', 'logps_train/chosen': '-111.03', 'loss/train': '0.69138', 'examples_per_second': '31.306', 'grad_norm': '26.375', 'counters/examples': 264352, 'counters/updates': 8261}
train stats after 264384 examples: {'rewards_train/chosen': '0.11246', 'rewards_train/rejected': '0.024878', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087581', 'logps_train/rejected': '-142.15', 'logps_train/chosen': '-145.35', 'loss/train': '0.6705', 'examples_per_second': '31.398', 'grad_norm': '26.875', 'counters/examples': 264384, 'counters/updates': 8262}
train stats after 264416 examples: {'rewards_train/chosen': '0.1062', 'rewards_train/rejected': '0.11592', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0097231', 'logps_train/rejected': '-149.06', 'logps_train/chosen': '-111.36', 'loss/train': '0.71319', 'examples_per_second': '31.543', 'grad_norm': '26.25', 'counters/examples': 264416, 'counters/updates': 8263}
train stats after 264448 examples: {'rewards_train/chosen': '0.063387', 'rewards_train/rejected': '0.11137', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.047985', 'logps_train/rejected': '-164.02', 'logps_train/chosen': '-163.72', 'loss/train': '0.72776', 'examples_per_second': '29.927', 'grad_norm': '30.875', 'counters/examples': 264448, 'counters/updates': 8264}
train stats after 264480 examples: {'rewards_train/chosen': '0.22128', 'rewards_train/rejected': '0.03273', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18855', 'logps_train/rejected': '-131.71', 'logps_train/chosen': '-128.57', 'loss/train': '0.62092', 'examples_per_second': '32.329', 'grad_norm': '31.125', 'counters/examples': 264480, 'counters/updates': 8265}
train stats after 264512 examples: {'rewards_train/chosen': '0.16662', 'rewards_train/rejected': '-0.018213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18483', 'logps_train/rejected': '-114.52', 'logps_train/chosen': '-139.05', 'loss/train': '0.61614', 'examples_per_second': '31.582', 'grad_norm': '27.25', 'counters/examples': 264512, 'counters/updates': 8266}
train stats after 264544 examples: {'rewards_train/chosen': '0.14101', 'rewards_train/rejected': '0.011383', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12963', 'logps_train/rejected': '-112.18', 'logps_train/chosen': '-137.97', 'loss/train': '0.65242', 'examples_per_second': '30.797', 'grad_norm': '27.375', 'counters/examples': 264544, 'counters/updates': 8267}
train stats after 264576 examples: {'rewards_train/chosen': '0.065726', 'rewards_train/rejected': '0.069403', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0036766', 'logps_train/rejected': '-130.23', 'logps_train/chosen': '-133.48', 'loss/train': '0.71407', 'examples_per_second': '30.205', 'grad_norm': '27.375', 'counters/examples': 264576, 'counters/updates': 8268}
train stats after 264608 examples: {'rewards_train/chosen': '0.20898', 'rewards_train/rejected': '0.050973', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15801', 'logps_train/rejected': '-104.91', 'logps_train/chosen': '-165.16', 'loss/train': '0.63579', 'examples_per_second': '30.043', 'grad_norm': '25.75', 'counters/examples': 264608, 'counters/updates': 8269}
train stats after 264640 examples: {'rewards_train/chosen': '0.16336', 'rewards_train/rejected': '0.040554', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12281', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-145.28', 'loss/train': '0.64925', 'examples_per_second': '33.276', 'grad_norm': '26.75', 'counters/examples': 264640, 'counters/updates': 8270}
train stats after 264672 examples: {'rewards_train/chosen': '0.078642', 'rewards_train/rejected': '-0.035975', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11462', 'logps_train/rejected': '-109.72', 'logps_train/chosen': '-107.02', 'loss/train': '0.66037', 'examples_per_second': '33.052', 'grad_norm': '23.25', 'counters/examples': 264672, 'counters/updates': 8271}
train stats after 264704 examples: {'rewards_train/chosen': '0.21084', 'rewards_train/rejected': '0.0013118', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20953', 'logps_train/rejected': '-112.27', 'logps_train/chosen': '-144.57', 'loss/train': '0.60959', 'examples_per_second': '32.142', 'grad_norm': '24.5', 'counters/examples': 264704, 'counters/updates': 8272}
train stats after 264736 examples: {'rewards_train/chosen': '0.076741', 'rewards_train/rejected': '-0.098819', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17556', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-122.13', 'loss/train': '0.62176', 'examples_per_second': '32.955', 'grad_norm': '22.75', 'counters/examples': 264736, 'counters/updates': 8273}
skipping logging after 264768 examples to avoid logging too frequently
train stats after 264800 examples: {'rewards_train/chosen': '0.1723', 'rewards_train/rejected': '-0.0037091', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17601', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-138.31', 'loss/train': '0.62009', 'examples_per_second': '34.275', 'grad_norm': '24.375', 'counters/examples': 264800, 'counters/updates': 8275}
train stats after 264832 examples: {'rewards_train/chosen': '0.08706', 'rewards_train/rejected': '0.052783', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034277', 'logps_train/rejected': '-118.7', 'logps_train/chosen': '-120.77', 'loss/train': '0.6926', 'examples_per_second': '31.634', 'grad_norm': '28.375', 'counters/examples': 264832, 'counters/updates': 8276}
train stats after 264864 examples: {'rewards_train/chosen': '0.14566', 'rewards_train/rejected': '0.0095651', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13609', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-134.32', 'loss/train': '0.64064', 'examples_per_second': '30.982', 'grad_norm': '26.5', 'counters/examples': 264864, 'counters/updates': 8277}
train stats after 264896 examples: {'rewards_train/chosen': '0.1723', 'rewards_train/rejected': '0.08537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086932', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-142.07', 'loss/train': '0.6591', 'examples_per_second': '32.336', 'grad_norm': '23.875', 'counters/examples': 264896, 'counters/updates': 8278}
train stats after 264928 examples: {'rewards_train/chosen': '0.17937', 'rewards_train/rejected': '0.073024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10635', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-136.35', 'loss/train': '0.65809', 'examples_per_second': '30.649', 'grad_norm': '26', 'counters/examples': 264928, 'counters/updates': 8279}
train stats after 264960 examples: {'rewards_train/chosen': '0.18523', 'rewards_train/rejected': '0.10827', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076957', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-147.83', 'loss/train': '0.67166', 'examples_per_second': '30.529', 'grad_norm': '37.25', 'counters/examples': 264960, 'counters/updates': 8280}
train stats after 264992 examples: {'rewards_train/chosen': '0.25231', 'rewards_train/rejected': '0.032001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22031', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-157.61', 'loss/train': '0.60587', 'examples_per_second': '32.206', 'grad_norm': '27.5', 'counters/examples': 264992, 'counters/updates': 8281}
train stats after 265024 examples: {'rewards_train/chosen': '0.1802', 'rewards_train/rejected': '-0.026293', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2065', 'logps_train/rejected': '-118.82', 'logps_train/chosen': '-130.11', 'loss/train': '0.60869', 'examples_per_second': '31.722', 'grad_norm': '22.5', 'counters/examples': 265024, 'counters/updates': 8282}
train stats after 265056 examples: {'rewards_train/chosen': '0.25645', 'rewards_train/rejected': '0.14344', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11301', 'logps_train/rejected': '-147.95', 'logps_train/chosen': '-161.25', 'loss/train': '0.6578', 'examples_per_second': '30.055', 'grad_norm': '26.75', 'counters/examples': 265056, 'counters/updates': 8283}
train stats after 265088 examples: {'rewards_train/chosen': '0.1076', 'rewards_train/rejected': '0.033663', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073933', 'logps_train/rejected': '-114.82', 'logps_train/chosen': '-126.95', 'loss/train': '0.67535', 'examples_per_second': '32.542', 'grad_norm': '26.875', 'counters/examples': 265088, 'counters/updates': 8284}
train stats after 265120 examples: {'rewards_train/chosen': '0.068504', 'rewards_train/rejected': '0.037479', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031025', 'logps_train/rejected': '-138.67', 'logps_train/chosen': '-146.85', 'loss/train': '0.69505', 'examples_per_second': '31.587', 'grad_norm': '28', 'counters/examples': 265120, 'counters/updates': 8285}
train stats after 265152 examples: {'rewards_train/chosen': '0.12119', 'rewards_train/rejected': '0.049198', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07199', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-125.21', 'loss/train': '0.67693', 'examples_per_second': '30.974', 'grad_norm': '25.75', 'counters/examples': 265152, 'counters/updates': 8286}
train stats after 265184 examples: {'rewards_train/chosen': '0.027853', 'rewards_train/rejected': '-0.034059', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061912', 'logps_train/rejected': '-135.48', 'logps_train/chosen': '-115.98', 'loss/train': '0.67533', 'examples_per_second': '32.416', 'grad_norm': '25.5', 'counters/examples': 265184, 'counters/updates': 8287}
skipping logging after 265216 examples to avoid logging too frequently
train stats after 265248 examples: {'rewards_train/chosen': '0.15702', 'rewards_train/rejected': '0.01909', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13793', 'logps_train/rejected': '-94.743', 'logps_train/chosen': '-158.73', 'loss/train': '0.64431', 'examples_per_second': '30.269', 'grad_norm': '25', 'counters/examples': 265248, 'counters/updates': 8289}
train stats after 265280 examples: {'rewards_train/chosen': '0.077194', 'rewards_train/rejected': '-0.027884', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10508', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-157.21', 'loss/train': '0.65357', 'examples_per_second': '32.53', 'grad_norm': '25.75', 'counters/examples': 265280, 'counters/updates': 8290}
train stats after 265312 examples: {'rewards_train/chosen': '0.12291', 'rewards_train/rejected': '0.0031923', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11972', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-165.77', 'loss/train': '0.65191', 'examples_per_second': '31.584', 'grad_norm': '27.5', 'counters/examples': 265312, 'counters/updates': 8291}
train stats after 265344 examples: {'rewards_train/chosen': '0.21178', 'rewards_train/rejected': '0.014416', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19736', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-125.79', 'loss/train': '0.61639', 'examples_per_second': '33.024', 'grad_norm': '25.375', 'counters/examples': 265344, 'counters/updates': 8292}
train stats after 265376 examples: {'rewards_train/chosen': '0.13927', 'rewards_train/rejected': '0.039827', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099439', 'logps_train/rejected': '-102.96', 'logps_train/chosen': '-131.43', 'loss/train': '0.65376', 'examples_per_second': '31.576', 'grad_norm': '23.125', 'counters/examples': 265376, 'counters/updates': 8293}
train stats after 265408 examples: {'rewards_train/chosen': '0.14211', 'rewards_train/rejected': '0.047683', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094427', 'logps_train/rejected': '-140.92', 'logps_train/chosen': '-134.99', 'loss/train': '0.66487', 'examples_per_second': '31.307', 'grad_norm': '26', 'counters/examples': 265408, 'counters/updates': 8294}
skipping logging after 265440 examples to avoid logging too frequently
train stats after 265472 examples: {'rewards_train/chosen': '0.068424', 'rewards_train/rejected': '-0.0025034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070927', 'logps_train/rejected': '-99.03', 'logps_train/chosen': '-104.19', 'loss/train': '0.67574', 'examples_per_second': '34.359', 'grad_norm': '23.25', 'counters/examples': 265472, 'counters/updates': 8296}
train stats after 265504 examples: {'rewards_train/chosen': '0.23528', 'rewards_train/rejected': '0.1011', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13418', 'logps_train/rejected': '-122.28', 'logps_train/chosen': '-168.77', 'loss/train': '0.64659', 'examples_per_second': '30.112', 'grad_norm': '27.625', 'counters/examples': 265504, 'counters/updates': 8297}
train stats after 265536 examples: {'rewards_train/chosen': '0.10325', 'rewards_train/rejected': '-0.0012458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1045', 'logps_train/rejected': '-101.71', 'logps_train/chosen': '-141.22', 'loss/train': '0.65725', 'examples_per_second': '32.01', 'grad_norm': '24.125', 'counters/examples': 265536, 'counters/updates': 8298}
skipping logging after 265568 examples to avoid logging too frequently
train stats after 265600 examples: {'rewards_train/chosen': '0.17148', 'rewards_train/rejected': '0.056095', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11539', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-157.57', 'loss/train': '0.65608', 'examples_per_second': '33.825', 'grad_norm': '24.875', 'counters/examples': 265600, 'counters/updates': 8300}
train stats after 265632 examples: {'rewards_train/chosen': '0.13146', 'rewards_train/rejected': '-0.045917', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17737', 'logps_train/rejected': '-94.234', 'logps_train/chosen': '-121.01', 'loss/train': '0.62313', 'examples_per_second': '32.661', 'grad_norm': '19.75', 'counters/examples': 265632, 'counters/updates': 8301}
train stats after 265664 examples: {'rewards_train/chosen': '0.14709', 'rewards_train/rejected': '0.06891', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078184', 'logps_train/rejected': '-147.05', 'logps_train/chosen': '-142.11', 'loss/train': '0.67412', 'examples_per_second': '30.397', 'grad_norm': '32.25', 'counters/examples': 265664, 'counters/updates': 8302}
train stats after 265696 examples: {'rewards_train/chosen': '0.23395', 'rewards_train/rejected': '0.046171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18778', 'logps_train/rejected': '-149.19', 'logps_train/chosen': '-177.18', 'loss/train': '0.62287', 'examples_per_second': '30.041', 'grad_norm': '27.625', 'counters/examples': 265696, 'counters/updates': 8303}
train stats after 265728 examples: {'rewards_train/chosen': '0.16423', 'rewards_train/rejected': '0.046856', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11737', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-130.8', 'loss/train': '0.6569', 'examples_per_second': '32.464', 'grad_norm': '25.5', 'counters/examples': 265728, 'counters/updates': 8304}
train stats after 265760 examples: {'rewards_train/chosen': '0.11364', 'rewards_train/rejected': '0.084159', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029483', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-129.26', 'loss/train': '0.69828', 'examples_per_second': '31.543', 'grad_norm': '27.625', 'counters/examples': 265760, 'counters/updates': 8305}
train stats after 265792 examples: {'rewards_train/chosen': '0.16629', 'rewards_train/rejected': '-0.01427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18056', 'logps_train/rejected': '-125.48', 'logps_train/chosen': '-182.57', 'loss/train': '0.62889', 'examples_per_second': '31.679', 'grad_norm': '28', 'counters/examples': 265792, 'counters/updates': 8306}
train stats after 265824 examples: {'rewards_train/chosen': '0.17388', 'rewards_train/rejected': '-0.05114', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22502', 'logps_train/rejected': '-101.1', 'logps_train/chosen': '-165.55', 'loss/train': '0.60107', 'examples_per_second': '32.842', 'grad_norm': '22.625', 'counters/examples': 265824, 'counters/updates': 8307}
train stats after 265856 examples: {'rewards_train/chosen': '0.13865', 'rewards_train/rejected': '0.084417', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054233', 'logps_train/rejected': '-109.58', 'logps_train/chosen': '-134.75', 'loss/train': '0.68068', 'examples_per_second': '31.514', 'grad_norm': '27.75', 'counters/examples': 265856, 'counters/updates': 8308}
train stats after 265888 examples: {'rewards_train/chosen': '0.1837', 'rewards_train/rejected': '0.07361', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11009', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-127.5', 'loss/train': '0.6574', 'examples_per_second': '30.682', 'grad_norm': '24.25', 'counters/examples': 265888, 'counters/updates': 8309}
train stats after 265920 examples: {'rewards_train/chosen': '0.18338', 'rewards_train/rejected': '0.076584', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10679', 'logps_train/rejected': '-146.44', 'logps_train/chosen': '-158.04', 'loss/train': '0.65331', 'examples_per_second': '31.86', 'grad_norm': '29.5', 'counters/examples': 265920, 'counters/updates': 8310}
train stats after 265952 examples: {'rewards_train/chosen': '0.22678', 'rewards_train/rejected': '0.046574', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18021', 'logps_train/rejected': '-135.18', 'logps_train/chosen': '-164.67', 'loss/train': '0.6272', 'examples_per_second': '32.095', 'grad_norm': '29.625', 'counters/examples': 265952, 'counters/updates': 8311}
train stats after 265984 examples: {'rewards_train/chosen': '0.17877', 'rewards_train/rejected': '-0.054745', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23351', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-140.34', 'loss/train': '0.59638', 'examples_per_second': '31.557', 'grad_norm': '26', 'counters/examples': 265984, 'counters/updates': 8312}
train stats after 266016 examples: {'rewards_train/chosen': '0.12781', 'rewards_train/rejected': '-0.03877', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16658', 'logps_train/rejected': '-110.56', 'logps_train/chosen': '-141.73', 'loss/train': '0.62168', 'examples_per_second': '30.515', 'grad_norm': '25', 'counters/examples': 266016, 'counters/updates': 8313}
train stats after 266048 examples: {'rewards_train/chosen': '0.052198', 'rewards_train/rejected': '-0.036486', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088684', 'logps_train/rejected': '-105.03', 'logps_train/chosen': '-191.24', 'loss/train': '0.66425', 'examples_per_second': '30.519', 'grad_norm': '26', 'counters/examples': 266048, 'counters/updates': 8314}
train stats after 266080 examples: {'rewards_train/chosen': '0.09099', 'rewards_train/rejected': '0.048885', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042104', 'logps_train/rejected': '-120.93', 'logps_train/chosen': '-159.68', 'loss/train': '0.69035', 'examples_per_second': '31.419', 'grad_norm': '26.375', 'counters/examples': 266080, 'counters/updates': 8315}
train stats after 266112 examples: {'rewards_train/chosen': '0.14344', 'rewards_train/rejected': '0.026327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11711', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-124.62', 'loss/train': '0.64703', 'examples_per_second': '31.958', 'grad_norm': '25.625', 'counters/examples': 266112, 'counters/updates': 8316}
train stats after 266144 examples: {'rewards_train/chosen': '0.17454', 'rewards_train/rejected': '0.11416', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060389', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-156.21', 'loss/train': '0.67531', 'examples_per_second': '30.12', 'grad_norm': '26.5', 'counters/examples': 266144, 'counters/updates': 8317}
train stats after 266176 examples: {'rewards_train/chosen': '0.10507', 'rewards_train/rejected': '-0.058712', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16378', 'logps_train/rejected': '-99.344', 'logps_train/chosen': '-121.48', 'loss/train': '0.63427', 'examples_per_second': '30.726', 'grad_norm': '22.125', 'counters/examples': 266176, 'counters/updates': 8318}
train stats after 266208 examples: {'rewards_train/chosen': '0.082308', 'rewards_train/rejected': '0.06071', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021598', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-127.33', 'loss/train': '0.69128', 'examples_per_second': '31.525', 'grad_norm': '27.625', 'counters/examples': 266208, 'counters/updates': 8319}
train stats after 266240 examples: {'rewards_train/chosen': '0.1575', 'rewards_train/rejected': '0.030615', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12689', 'logps_train/rejected': '-100.45', 'logps_train/chosen': '-140.47', 'loss/train': '0.64478', 'examples_per_second': '31.37', 'grad_norm': '23.375', 'counters/examples': 266240, 'counters/updates': 8320}
train stats after 266272 examples: {'rewards_train/chosen': '0.16567', 'rewards_train/rejected': '-0.034394', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20006', 'logps_train/rejected': '-145.48', 'logps_train/chosen': '-169.99', 'loss/train': '0.62489', 'examples_per_second': '31.433', 'grad_norm': '36.75', 'counters/examples': 266272, 'counters/updates': 8321}
skipping logging after 266304 examples to avoid logging too frequently
train stats after 266336 examples: {'rewards_train/chosen': '0.14761', 'rewards_train/rejected': '0.0056537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14195', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-153', 'loss/train': '0.63843', 'examples_per_second': '36.103', 'grad_norm': '25', 'counters/examples': 266336, 'counters/updates': 8323}
train stats after 266368 examples: {'rewards_train/chosen': '0.097688', 'rewards_train/rejected': '-0.010499', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10819', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-166.99', 'loss/train': '0.64974', 'examples_per_second': '31.534', 'grad_norm': '26.75', 'counters/examples': 266368, 'counters/updates': 8324}
skipping logging after 266400 examples to avoid logging too frequently
train stats after 266432 examples: {'rewards_train/chosen': '0.036594', 'rewards_train/rejected': '-0.0085984', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045193', 'logps_train/rejected': '-107.36', 'logps_train/chosen': '-100.78', 'loss/train': '0.68372', 'examples_per_second': '33.577', 'grad_norm': '21.625', 'counters/examples': 266432, 'counters/updates': 8326}
skipping logging after 266464 examples to avoid logging too frequently
train stats after 266496 examples: {'rewards_train/chosen': '0.20635', 'rewards_train/rejected': '0.024281', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18207', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-152.17', 'loss/train': '0.62506', 'examples_per_second': '31.557', 'grad_norm': '27', 'counters/examples': 266496, 'counters/updates': 8328}
train stats after 266528 examples: {'rewards_train/chosen': '0.12647', 'rewards_train/rejected': '-0.023356', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14982', 'logps_train/rejected': '-99.224', 'logps_train/chosen': '-136.94', 'loss/train': '0.63297', 'examples_per_second': '30.045', 'grad_norm': '22.75', 'counters/examples': 266528, 'counters/updates': 8329}
skipping logging after 266560 examples to avoid logging too frequently
train stats after 266592 examples: {'rewards_train/chosen': '0.17092', 'rewards_train/rejected': '0.022034', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14889', 'logps_train/rejected': '-158.56', 'logps_train/chosen': '-187.25', 'loss/train': '0.64587', 'examples_per_second': '30.971', 'grad_norm': '28.625', 'counters/examples': 266592, 'counters/updates': 8331}
train stats after 266624 examples: {'rewards_train/chosen': '0.15015', 'rewards_train/rejected': '0.10004', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.050104', 'logps_train/rejected': '-129.22', 'logps_train/chosen': '-145.75', 'loss/train': '0.69246', 'examples_per_second': '31.538', 'grad_norm': '28.625', 'counters/examples': 266624, 'counters/updates': 8332}
train stats after 266656 examples: {'rewards_train/chosen': '0.14381', 'rewards_train/rejected': '0.065142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078667', 'logps_train/rejected': '-107.83', 'logps_train/chosen': '-132.29', 'loss/train': '0.66865', 'examples_per_second': '30.721', 'grad_norm': '26.5', 'counters/examples': 266656, 'counters/updates': 8333}
train stats after 266688 examples: {'rewards_train/chosen': '0.15272', 'rewards_train/rejected': '0.15277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-4.1151e-05', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-123.2', 'loss/train': '0.71399', 'examples_per_second': '29.996', 'grad_norm': '28.5', 'counters/examples': 266688, 'counters/updates': 8334}
train stats after 266720 examples: {'rewards_train/chosen': '0.20177', 'rewards_train/rejected': '0.058682', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14309', 'logps_train/rejected': '-150.71', 'logps_train/chosen': '-130.03', 'loss/train': '0.635', 'examples_per_second': '30.588', 'grad_norm': '26.625', 'counters/examples': 266720, 'counters/updates': 8335}
train stats after 266752 examples: {'rewards_train/chosen': '0.10484', 'rewards_train/rejected': '0.00017707', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10466', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-167.93', 'loss/train': '0.66743', 'examples_per_second': '31.786', 'grad_norm': '28.25', 'counters/examples': 266752, 'counters/updates': 8336}
train stats after 266784 examples: {'rewards_train/chosen': '0.11085', 'rewards_train/rejected': '0.040445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070405', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-158.08', 'loss/train': '0.6686', 'examples_per_second': '32.512', 'grad_norm': '26.5', 'counters/examples': 266784, 'counters/updates': 8337}
train stats after 266816 examples: {'rewards_train/chosen': '0.26252', 'rewards_train/rejected': '0.035279', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22724', 'logps_train/rejected': '-100.33', 'logps_train/chosen': '-136.48', 'loss/train': '0.59417', 'examples_per_second': '29.937', 'grad_norm': '22.875', 'counters/examples': 266816, 'counters/updates': 8338}
train stats after 266848 examples: {'rewards_train/chosen': '0.17454', 'rewards_train/rejected': '-0.0087443', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18329', 'logps_train/rejected': '-113.86', 'logps_train/chosen': '-138.92', 'loss/train': '0.62397', 'examples_per_second': '31.546', 'grad_norm': '25.375', 'counters/examples': 266848, 'counters/updates': 8339}
train stats after 266880 examples: {'rewards_train/chosen': '0.22234', 'rewards_train/rejected': '0.04127', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18107', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-161.62', 'loss/train': '0.62399', 'examples_per_second': '30.126', 'grad_norm': '28.5', 'counters/examples': 266880, 'counters/updates': 8340}
train stats after 266912 examples: {'rewards_train/chosen': '0.19511', 'rewards_train/rejected': '0.021702', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.17341', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-173.55', 'loss/train': '0.61792', 'examples_per_second': '29.998', 'grad_norm': '25.25', 'counters/examples': 266912, 'counters/updates': 8341}
train stats after 266944 examples: {'rewards_train/chosen': '0.12795', 'rewards_train/rejected': '0.088727', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.039219', 'logps_train/rejected': '-178.44', 'logps_train/chosen': '-149.47', 'loss/train': '0.69023', 'examples_per_second': '31.089', 'grad_norm': '34', 'counters/examples': 266944, 'counters/updates': 8342}
train stats after 266976 examples: {'rewards_train/chosen': '0.17986', 'rewards_train/rejected': '-0.022835', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20269', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-117.74', 'loss/train': '0.61038', 'examples_per_second': '30.898', 'grad_norm': '22.25', 'counters/examples': 266976, 'counters/updates': 8343}
train stats after 267008 examples: {'rewards_train/chosen': '0.14055', 'rewards_train/rejected': '0.070422', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070129', 'logps_train/rejected': '-99.516', 'logps_train/chosen': '-138.95', 'loss/train': '0.6791', 'examples_per_second': '30.362', 'grad_norm': '24.5', 'counters/examples': 267008, 'counters/updates': 8344}
train stats after 267040 examples: {'rewards_train/chosen': '0.1806', 'rewards_train/rejected': '-0.043369', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22397', 'logps_train/rejected': '-86.447', 'logps_train/chosen': '-106.79', 'loss/train': '0.59987', 'examples_per_second': '31.312', 'grad_norm': '18.5', 'counters/examples': 267040, 'counters/updates': 8345}
train stats after 267072 examples: {'rewards_train/chosen': '0.22949', 'rewards_train/rejected': '-0.029981', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25947', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-153.93', 'loss/train': '0.58802', 'examples_per_second': '31.528', 'grad_norm': '26.375', 'counters/examples': 267072, 'counters/updates': 8346}
train stats after 267104 examples: {'rewards_train/chosen': '0.21327', 'rewards_train/rejected': '0.11254', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10073', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-133.74', 'loss/train': '0.65759', 'examples_per_second': '30.427', 'grad_norm': '26.75', 'counters/examples': 267104, 'counters/updates': 8347}
train stats after 267136 examples: {'rewards_train/chosen': '0.21037', 'rewards_train/rejected': '-0.028751', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23912', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-182.1', 'loss/train': '0.59671', 'examples_per_second': '30.011', 'grad_norm': '25', 'counters/examples': 267136, 'counters/updates': 8348}
train stats after 267168 examples: {'rewards_train/chosen': '0.13811', 'rewards_train/rejected': '-0.029479', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16759', 'logps_train/rejected': '-93.849', 'logps_train/chosen': '-124.98', 'loss/train': '0.62396', 'examples_per_second': '32.09', 'grad_norm': '22.75', 'counters/examples': 267168, 'counters/updates': 8349}
train stats after 267200 examples: {'rewards_train/chosen': '0.18414', 'rewards_train/rejected': '0.054706', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12943', 'logps_train/rejected': '-134.62', 'logps_train/chosen': '-151.96', 'loss/train': '0.6409', 'examples_per_second': '31.542', 'grad_norm': '26.625', 'counters/examples': 267200, 'counters/updates': 8350}
skipping logging after 267232 examples to avoid logging too frequently
train stats after 267264 examples: {'rewards_train/chosen': '0.15216', 'rewards_train/rejected': '0.0088509', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14331', 'logps_train/rejected': '-130.5', 'logps_train/chosen': '-134.85', 'loss/train': '0.64031', 'examples_per_second': '31.019', 'grad_norm': '24.375', 'counters/examples': 267264, 'counters/updates': 8352}
skipping logging after 267296 examples to avoid logging too frequently
train stats after 267328 examples: {'rewards_train/chosen': '0.17681', 'rewards_train/rejected': '0.081619', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095194', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-121.85', 'loss/train': '0.66117', 'examples_per_second': '31.492', 'grad_norm': '24.75', 'counters/examples': 267328, 'counters/updates': 8354}
train stats after 267360 examples: {'rewards_train/chosen': '0.25273', 'rewards_train/rejected': '-0.0070329', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25977', 'logps_train/rejected': '-93.855', 'logps_train/chosen': '-150.9', 'loss/train': '0.5887', 'examples_per_second': '30.554', 'grad_norm': '21.875', 'counters/examples': 267360, 'counters/updates': 8355}
train stats after 267392 examples: {'rewards_train/chosen': '0.088179', 'rewards_train/rejected': '0.12166', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.033484', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-146.94', 'loss/train': '0.74331', 'examples_per_second': '31.343', 'grad_norm': '28', 'counters/examples': 267392, 'counters/updates': 8356}
train stats after 267424 examples: {'rewards_train/chosen': '0.073795', 'rewards_train/rejected': '-0.055217', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12901', 'logps_train/rejected': '-129.6', 'logps_train/chosen': '-125.67', 'loss/train': '0.63854', 'examples_per_second': '32.676', 'grad_norm': '25.625', 'counters/examples': 267424, 'counters/updates': 8357}
train stats after 267456 examples: {'rewards_train/chosen': '0.19218', 'rewards_train/rejected': '0.054221', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13796', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-190.2', 'loss/train': '0.64561', 'examples_per_second': '31.545', 'grad_norm': '32.75', 'counters/examples': 267456, 'counters/updates': 8358}
train stats after 267488 examples: {'rewards_train/chosen': '0.10643', 'rewards_train/rejected': '0.055697', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050732', 'logps_train/rejected': '-111.88', 'logps_train/chosen': '-136.4', 'loss/train': '0.68842', 'examples_per_second': '31.225', 'grad_norm': '24.75', 'counters/examples': 267488, 'counters/updates': 8359}
train stats after 267520 examples: {'rewards_train/chosen': '0.23436', 'rewards_train/rejected': '-0.024005', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25836', 'logps_train/rejected': '-125.67', 'logps_train/chosen': '-136.92', 'loss/train': '0.59289', 'examples_per_second': '31.729', 'grad_norm': '23.625', 'counters/examples': 267520, 'counters/updates': 8360}
train stats after 267552 examples: {'rewards_train/chosen': '0.15933', 'rewards_train/rejected': '-0.019607', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17894', 'logps_train/rejected': '-122.6', 'logps_train/chosen': '-147.6', 'loss/train': '0.63194', 'examples_per_second': '31.584', 'grad_norm': '25.375', 'counters/examples': 267552, 'counters/updates': 8361}
train stats after 267584 examples: {'rewards_train/chosen': '0.18249', 'rewards_train/rejected': '0.0067983', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17569', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-147.89', 'loss/train': '0.63465', 'examples_per_second': '33.385', 'grad_norm': '24.625', 'counters/examples': 267584, 'counters/updates': 8362}
skipping logging after 267616 examples to avoid logging too frequently
train stats after 267648 examples: {'rewards_train/chosen': '0.13143', 'rewards_train/rejected': '-0.0048267', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13626', 'logps_train/rejected': '-104.42', 'logps_train/chosen': '-122.97', 'loss/train': '0.63978', 'examples_per_second': '30.774', 'grad_norm': '24.5', 'counters/examples': 267648, 'counters/updates': 8364}
train stats after 267680 examples: {'rewards_train/chosen': '0.35611', 'rewards_train/rejected': '0.072378', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28373', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-191.77', 'loss/train': '0.5922', 'examples_per_second': '30.051', 'grad_norm': '25.5', 'counters/examples': 267680, 'counters/updates': 8365}
train stats after 267712 examples: {'rewards_train/chosen': '0.10916', 'rewards_train/rejected': '0.084951', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024213', 'logps_train/rejected': '-103.62', 'logps_train/chosen': '-94.097', 'loss/train': '0.69299', 'examples_per_second': '31.64', 'grad_norm': '22.75', 'counters/examples': 267712, 'counters/updates': 8366}
skipping logging after 267744 examples to avoid logging too frequently
train stats after 267776 examples: {'rewards_train/chosen': '0.11038', 'rewards_train/rejected': '-0.041808', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15219', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-141.29', 'loss/train': '0.63307', 'examples_per_second': '31.452', 'grad_norm': '24.25', 'counters/examples': 267776, 'counters/updates': 8368}
train stats after 267808 examples: {'rewards_train/chosen': '0.24292', 'rewards_train/rejected': '0.052773', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19014', 'logps_train/rejected': '-93.028', 'logps_train/chosen': '-136.96', 'loss/train': '0.61523', 'examples_per_second': '30.677', 'grad_norm': '22.75', 'counters/examples': 267808, 'counters/updates': 8369}
train stats after 267840 examples: {'rewards_train/chosen': '0.11473', 'rewards_train/rejected': '-0.066894', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18163', 'logps_train/rejected': '-92.122', 'logps_train/chosen': '-124.89', 'loss/train': '0.62373', 'examples_per_second': '30.992', 'grad_norm': '21.625', 'counters/examples': 267840, 'counters/updates': 8370}
skipping logging after 267872 examples to avoid logging too frequently
train stats after 267904 examples: {'rewards_train/chosen': '0.1146', 'rewards_train/rejected': '-0.054393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.169', 'logps_train/rejected': '-99.406', 'logps_train/chosen': '-134.16', 'loss/train': '0.62521', 'examples_per_second': '32.593', 'grad_norm': '23.625', 'counters/examples': 267904, 'counters/updates': 8372}
train stats after 267936 examples: {'rewards_train/chosen': '0.15069', 'rewards_train/rejected': '0.029826', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12087', 'logps_train/rejected': '-123.95', 'logps_train/chosen': '-131.4', 'loss/train': '0.64927', 'examples_per_second': '30.582', 'grad_norm': '23.875', 'counters/examples': 267936, 'counters/updates': 8373}
train stats after 267968 examples: {'rewards_train/chosen': '0.23181', 'rewards_train/rejected': '0.060038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17177', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-155.24', 'loss/train': '0.63327', 'examples_per_second': '30.337', 'grad_norm': '24.25', 'counters/examples': 267968, 'counters/updates': 8374}
train stats after 268000 examples: {'rewards_train/chosen': '0.15799', 'rewards_train/rejected': '0.070365', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087629', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-157.59', 'loss/train': '0.67542', 'examples_per_second': '22.909', 'grad_norm': '27.75', 'counters/examples': 268000, 'counters/updates': 8375}
train stats after 268032 examples: {'rewards_train/chosen': '0.084539', 'rewards_train/rejected': '-0.079448', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16399', 'logps_train/rejected': '-141.57', 'logps_train/chosen': '-136.44', 'loss/train': '0.62556', 'examples_per_second': '30.868', 'grad_norm': '27', 'counters/examples': 268032, 'counters/updates': 8376}
train stats after 268064 examples: {'rewards_train/chosen': '0.20783', 'rewards_train/rejected': '-0.077419', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28525', 'logps_train/rejected': '-102.51', 'logps_train/chosen': '-122.98', 'loss/train': '0.57894', 'examples_per_second': '30.523', 'grad_norm': '21', 'counters/examples': 268064, 'counters/updates': 8377}
train stats after 268096 examples: {'rewards_train/chosen': '0.26057', 'rewards_train/rejected': '0.099643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16092', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-181.73', 'loss/train': '0.63451', 'examples_per_second': '24.187', 'grad_norm': '28.25', 'counters/examples': 268096, 'counters/updates': 8378}
train stats after 268128 examples: {'rewards_train/chosen': '0.14916', 'rewards_train/rejected': '-0.014496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16365', 'logps_train/rejected': '-125.2', 'logps_train/chosen': '-129.78', 'loss/train': '0.63626', 'examples_per_second': '31.455', 'grad_norm': '23.5', 'counters/examples': 268128, 'counters/updates': 8379}
skipping logging after 268160 examples to avoid logging too frequently
train stats after 268192 examples: {'rewards_train/chosen': '0.058562', 'rewards_train/rejected': '0.041175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017387', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-151.54', 'loss/train': '0.70179', 'examples_per_second': '32.448', 'grad_norm': '27', 'counters/examples': 268192, 'counters/updates': 8381}
train stats after 268224 examples: {'rewards_train/chosen': '0.11063', 'rewards_train/rejected': '0.028735', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081898', 'logps_train/rejected': '-97.437', 'logps_train/chosen': '-96.093', 'loss/train': '0.66448', 'examples_per_second': '31.531', 'grad_norm': '21.5', 'counters/examples': 268224, 'counters/updates': 8382}
train stats after 268256 examples: {'rewards_train/chosen': '0.10385', 'rewards_train/rejected': '0.12142', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017565', 'logps_train/rejected': '-117.84', 'logps_train/chosen': '-140.35', 'loss/train': '0.71509', 'examples_per_second': '30.518', 'grad_norm': '27.5', 'counters/examples': 268256, 'counters/updates': 8383}
train stats after 268288 examples: {'rewards_train/chosen': '0.12407', 'rewards_train/rejected': '0.045433', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078637', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-163.73', 'loss/train': '0.66312', 'examples_per_second': '30.934', 'grad_norm': '26.5', 'counters/examples': 268288, 'counters/updates': 8384}
train stats after 268320 examples: {'rewards_train/chosen': '0.3088', 'rewards_train/rejected': '0.068673', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24013', 'logps_train/rejected': '-131.97', 'logps_train/chosen': '-182.77', 'loss/train': '0.60641', 'examples_per_second': '30.326', 'grad_norm': '25.25', 'counters/examples': 268320, 'counters/updates': 8385}
train stats after 268352 examples: {'rewards_train/chosen': '0.14842', 'rewards_train/rejected': '-0.039899', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18832', 'logps_train/rejected': '-185.82', 'logps_train/chosen': '-120.59', 'loss/train': '0.62151', 'examples_per_second': '31.42', 'grad_norm': '28.875', 'counters/examples': 268352, 'counters/updates': 8386}
train stats after 268384 examples: {'rewards_train/chosen': '0.13902', 'rewards_train/rejected': '0.033078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10594', 'logps_train/rejected': '-160.12', 'logps_train/chosen': '-152.12', 'loss/train': '0.65508', 'examples_per_second': '31.3', 'grad_norm': '29.625', 'counters/examples': 268384, 'counters/updates': 8387}
train stats after 268416 examples: {'rewards_train/chosen': '0.23182', 'rewards_train/rejected': '0.07089', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.16093', 'logps_train/rejected': '-160.03', 'logps_train/chosen': '-188.22', 'loss/train': '0.63652', 'examples_per_second': '31.475', 'grad_norm': '28.75', 'counters/examples': 268416, 'counters/updates': 8388}
skipping logging after 268448 examples to avoid logging too frequently
train stats after 268480 examples: {'rewards_train/chosen': '0.10814', 'rewards_train/rejected': '-0.03695', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14509', 'logps_train/rejected': '-119.73', 'logps_train/chosen': '-130.96', 'loss/train': '0.62971', 'examples_per_second': '31.515', 'grad_norm': '23.625', 'counters/examples': 268480, 'counters/updates': 8390}
skipping logging after 268512 examples to avoid logging too frequently
train stats after 268544 examples: {'rewards_train/chosen': '0.13223', 'rewards_train/rejected': '0.057725', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074505', 'logps_train/rejected': '-112.63', 'logps_train/chosen': '-121.58', 'loss/train': '0.6751', 'examples_per_second': '33.694', 'grad_norm': '24.5', 'counters/examples': 268544, 'counters/updates': 8392}
train stats after 268576 examples: {'rewards_train/chosen': '0.076788', 'rewards_train/rejected': '0.05366', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023128', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-127.85', 'loss/train': '0.69862', 'examples_per_second': '31.516', 'grad_norm': '26.25', 'counters/examples': 268576, 'counters/updates': 8393}
skipping logging after 268608 examples to avoid logging too frequently
train stats after 268640 examples: {'rewards_train/chosen': '0.13807', 'rewards_train/rejected': '0.030972', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1071', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-171.56', 'loss/train': '0.67388', 'examples_per_second': '30.048', 'grad_norm': '26.25', 'counters/examples': 268640, 'counters/updates': 8395}
train stats after 268672 examples: {'rewards_train/chosen': '0.22596', 'rewards_train/rejected': '-0.0015779', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22754', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-163.55', 'loss/train': '0.60114', 'examples_per_second': '31.549', 'grad_norm': '25.875', 'counters/examples': 268672, 'counters/updates': 8396}
train stats after 268704 examples: {'rewards_train/chosen': '0.094189', 'rewards_train/rejected': '-0.075072', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16926', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-122.72', 'loss/train': '0.62315', 'examples_per_second': '30.565', 'grad_norm': '24.75', 'counters/examples': 268704, 'counters/updates': 8397}
train stats after 268736 examples: {'rewards_train/chosen': '0.25725', 'rewards_train/rejected': '0.012603', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24465', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-137.21', 'loss/train': '0.60622', 'examples_per_second': '31.588', 'grad_norm': '24.375', 'counters/examples': 268736, 'counters/updates': 8398}
train stats after 268768 examples: {'rewards_train/chosen': '0.073588', 'rewards_train/rejected': '0.050743', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022844', 'logps_train/rejected': '-143.08', 'logps_train/chosen': '-146.02', 'loss/train': '0.69901', 'examples_per_second': '31.549', 'grad_norm': '35.75', 'counters/examples': 268768, 'counters/updates': 8399}
skipping logging after 268800 examples to avoid logging too frequently
train stats after 268832 examples: {'rewards_train/chosen': '0.15293', 'rewards_train/rejected': '-0.042426', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19535', 'logps_train/rejected': '-135.26', 'logps_train/chosen': '-138.46', 'loss/train': '0.61796', 'examples_per_second': '31.327', 'grad_norm': '25.25', 'counters/examples': 268832, 'counters/updates': 8401}
train stats after 268864 examples: {'rewards_train/chosen': '0.23752', 'rewards_train/rejected': '0.035652', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20187', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-172.34', 'loss/train': '0.618', 'examples_per_second': '30.952', 'grad_norm': '24.875', 'counters/examples': 268864, 'counters/updates': 8402}
skipping logging after 268896 examples to avoid logging too frequently
train stats after 268928 examples: {'rewards_train/chosen': '0.17892', 'rewards_train/rejected': '0.089619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089306', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-108.12', 'loss/train': '0.66632', 'examples_per_second': '31.358', 'grad_norm': '37', 'counters/examples': 268928, 'counters/updates': 8404}
skipping logging after 268960 examples to avoid logging too frequently
train stats after 268992 examples: {'rewards_train/chosen': '0.16715', 'rewards_train/rejected': '0.086308', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080838', 'logps_train/rejected': '-101.21', 'logps_train/chosen': '-105.99', 'loss/train': '0.6658', 'examples_per_second': '34.41', 'grad_norm': '22.375', 'counters/examples': 268992, 'counters/updates': 8406}
train stats after 269024 examples: {'rewards_train/chosen': '0.13641', 'rewards_train/rejected': '0.10841', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028003', 'logps_train/rejected': '-150.28', 'logps_train/chosen': '-125.66', 'loss/train': '0.69344', 'examples_per_second': '30.608', 'grad_norm': '28.5', 'counters/examples': 269024, 'counters/updates': 8407}
skipping logging after 269056 examples to avoid logging too frequently
train stats after 269088 examples: {'rewards_train/chosen': '0.10109', 'rewards_train/rejected': '-0.033755', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13485', 'logps_train/rejected': '-116.51', 'logps_train/chosen': '-168.28', 'loss/train': '0.64901', 'examples_per_second': '23.593', 'grad_norm': '33.25', 'counters/examples': 269088, 'counters/updates': 8409}
train stats after 269120 examples: {'rewards_train/chosen': '0.18817', 'rewards_train/rejected': '0.034463', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15371', 'logps_train/rejected': '-122.7', 'logps_train/chosen': '-119.57', 'loss/train': '0.62967', 'examples_per_second': '31.559', 'grad_norm': '23.75', 'counters/examples': 269120, 'counters/updates': 8410}
train stats after 269152 examples: {'rewards_train/chosen': '0.065735', 'rewards_train/rejected': '0.084489', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.018754', 'logps_train/rejected': '-137.58', 'logps_train/chosen': '-107.65', 'loss/train': '0.7261', 'examples_per_second': '32.416', 'grad_norm': '27.875', 'counters/examples': 269152, 'counters/updates': 8411}
skipping logging after 269184 examples to avoid logging too frequently
train stats after 269216 examples: {'rewards_train/chosen': '0.11562', 'rewards_train/rejected': '-0.032985', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14861', 'logps_train/rejected': '-122.29', 'logps_train/chosen': '-157.74', 'loss/train': '0.63359', 'examples_per_second': '32.067', 'grad_norm': '25.75', 'counters/examples': 269216, 'counters/updates': 8413}
train stats after 269248 examples: {'rewards_train/chosen': '0.15296', 'rewards_train/rejected': '-0.00080399', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15376', 'logps_train/rejected': '-117.21', 'logps_train/chosen': '-150.81', 'loss/train': '0.64407', 'examples_per_second': '31.713', 'grad_norm': '28', 'counters/examples': 269248, 'counters/updates': 8414}
train stats after 269280 examples: {'rewards_train/chosen': '0.21835', 'rewards_train/rejected': '0.026478', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19187', 'logps_train/rejected': '-161.35', 'logps_train/chosen': '-162.99', 'loss/train': '0.60934', 'examples_per_second': '31.536', 'grad_norm': '25.875', 'counters/examples': 269280, 'counters/updates': 8415}
train stats after 269312 examples: {'rewards_train/chosen': '0.1303', 'rewards_train/rejected': '0.027516', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10278', 'logps_train/rejected': '-93.721', 'logps_train/chosen': '-155.77', 'loss/train': '0.66528', 'examples_per_second': '32.173', 'grad_norm': '27.375', 'counters/examples': 269312, 'counters/updates': 8416}
train stats after 269344 examples: {'rewards_train/chosen': '0.23362', 'rewards_train/rejected': '0.041206', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19242', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-139.73', 'loss/train': '0.61719', 'examples_per_second': '31.369', 'grad_norm': '26', 'counters/examples': 269344, 'counters/updates': 8417}
train stats after 269376 examples: {'rewards_train/chosen': '0.20118', 'rewards_train/rejected': '-0.041553', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.24273', 'logps_train/rejected': '-104.09', 'logps_train/chosen': '-107.85', 'loss/train': '0.59885', 'examples_per_second': '32.349', 'grad_norm': '27.25', 'counters/examples': 269376, 'counters/updates': 8418}
skipping logging after 269408 examples to avoid logging too frequently
train stats after 269440 examples: {'rewards_train/chosen': '0.15957', 'rewards_train/rejected': '0.044376', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11519', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-128.18', 'loss/train': '0.65159', 'examples_per_second': '30.114', 'grad_norm': '28.125', 'counters/examples': 269440, 'counters/updates': 8420}
train stats after 269472 examples: {'rewards_train/chosen': '0.18482', 'rewards_train/rejected': '-0.024514', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20933', 'logps_train/rejected': '-159.62', 'logps_train/chosen': '-125.32', 'loss/train': '0.60868', 'examples_per_second': '30.611', 'grad_norm': '23.375', 'counters/examples': 269472, 'counters/updates': 8421}
train stats after 269504 examples: {'rewards_train/chosen': '0.14104', 'rewards_train/rejected': '0.087639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053405', 'logps_train/rejected': '-152.34', 'logps_train/chosen': '-141.96', 'loss/train': '0.67519', 'examples_per_second': '30.073', 'grad_norm': '27.375', 'counters/examples': 269504, 'counters/updates': 8422}
train stats after 269536 examples: {'rewards_train/chosen': '0.092484', 'rewards_train/rejected': '0.037476', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055007', 'logps_train/rejected': '-138.36', 'logps_train/chosen': '-189.68', 'loss/train': '0.68648', 'examples_per_second': '32.249', 'grad_norm': '28.375', 'counters/examples': 269536, 'counters/updates': 8423}
train stats after 269568 examples: {'rewards_train/chosen': '0.17253', 'rewards_train/rejected': '0.060729', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1118', 'logps_train/rejected': '-106.02', 'logps_train/chosen': '-152.48', 'loss/train': '0.65158', 'examples_per_second': '30', 'grad_norm': '23', 'counters/examples': 269568, 'counters/updates': 8424}
train stats after 269600 examples: {'rewards_train/chosen': '0.079555', 'rewards_train/rejected': '0.036925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04263', 'logps_train/rejected': '-118.3', 'logps_train/chosen': '-139.2', 'loss/train': '0.68595', 'examples_per_second': '31.284', 'grad_norm': '27.875', 'counters/examples': 269600, 'counters/updates': 8425}
skipping logging after 269632 examples to avoid logging too frequently
train stats after 269664 examples: {'rewards_train/chosen': '0.18783', 'rewards_train/rejected': '0.024451', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16338', 'logps_train/rejected': '-131.43', 'logps_train/chosen': '-143.96', 'loss/train': '0.62992', 'examples_per_second': '31.256', 'grad_norm': '29', 'counters/examples': 269664, 'counters/updates': 8427}
train stats after 269696 examples: {'rewards_train/chosen': '0.16917', 'rewards_train/rejected': '0.063711', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10546', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-136.78', 'loss/train': '0.65345', 'examples_per_second': '31.522', 'grad_norm': '30.125', 'counters/examples': 269696, 'counters/updates': 8428}
train stats after 269728 examples: {'rewards_train/chosen': '0.1243', 'rewards_train/rejected': '-0.038154', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16245', 'logps_train/rejected': '-93.387', 'logps_train/chosen': '-109.2', 'loss/train': '0.63401', 'examples_per_second': '31.994', 'grad_norm': '24.125', 'counters/examples': 269728, 'counters/updates': 8429}
train stats after 269760 examples: {'rewards_train/chosen': '0.19187', 'rewards_train/rejected': '-0.01112', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20299', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-143.89', 'loss/train': '0.61984', 'examples_per_second': '30.542', 'grad_norm': '23.25', 'counters/examples': 269760, 'counters/updates': 8430}
train stats after 269792 examples: {'rewards_train/chosen': '0.10615', 'rewards_train/rejected': '-0.017839', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12399', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-128.14', 'loss/train': '0.64694', 'examples_per_second': '32.641', 'grad_norm': '25.375', 'counters/examples': 269792, 'counters/updates': 8431}
skipping logging after 269824 examples to avoid logging too frequently
train stats after 269856 examples: {'rewards_train/chosen': '0.19036', 'rewards_train/rejected': '0.022672', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16769', 'logps_train/rejected': '-127.79', 'logps_train/chosen': '-143.42', 'loss/train': '0.62155', 'examples_per_second': '31.581', 'grad_norm': '27.25', 'counters/examples': 269856, 'counters/updates': 8433}
train stats after 269888 examples: {'rewards_train/chosen': '0.17712', 'rewards_train/rejected': '0.061187', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11593', 'logps_train/rejected': '-93.156', 'logps_train/chosen': '-140.55', 'loss/train': '0.65297', 'examples_per_second': '30.79', 'grad_norm': '26.75', 'counters/examples': 269888, 'counters/updates': 8434}
train stats after 269920 examples: {'rewards_train/chosen': '0.16217', 'rewards_train/rejected': '0.10515', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057011', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-109.35', 'loss/train': '0.67432', 'examples_per_second': '30.638', 'grad_norm': '27.25', 'counters/examples': 269920, 'counters/updates': 8435}
train stats after 269952 examples: {'rewards_train/chosen': '0.18204', 'rewards_train/rejected': '0.0089868', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17306', 'logps_train/rejected': '-143.63', 'logps_train/chosen': '-137.07', 'loss/train': '0.62273', 'examples_per_second': '30.595', 'grad_norm': '26.625', 'counters/examples': 269952, 'counters/updates': 8436}
train stats after 269984 examples: {'rewards_train/chosen': '0.14254', 'rewards_train/rejected': '-0.02314', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16568', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-168.6', 'loss/train': '0.62764', 'examples_per_second': '32.79', 'grad_norm': '24.25', 'counters/examples': 269984, 'counters/updates': 8437}
train stats after 270016 examples: {'rewards_train/chosen': '0.22979', 'rewards_train/rejected': '0.10465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12514', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-167.06', 'loss/train': '0.65139', 'examples_per_second': '32.576', 'grad_norm': '28.5', 'counters/examples': 270016, 'counters/updates': 8438}
skipping logging after 270048 examples to avoid logging too frequently
train stats after 270080 examples: {'rewards_train/chosen': '0.24393', 'rewards_train/rejected': '0.008898', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23503', 'logps_train/rejected': '-98.837', 'logps_train/chosen': '-162.75', 'loss/train': '0.59441', 'examples_per_second': '31.294', 'grad_norm': '21.75', 'counters/examples': 270080, 'counters/updates': 8440}
train stats after 270112 examples: {'rewards_train/chosen': '0.20551', 'rewards_train/rejected': '-0.017363', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22288', 'logps_train/rejected': '-122.25', 'logps_train/chosen': '-154.72', 'loss/train': '0.60488', 'examples_per_second': '30.88', 'grad_norm': '25.75', 'counters/examples': 270112, 'counters/updates': 8441}
train stats after 270144 examples: {'rewards_train/chosen': '0.13565', 'rewards_train/rejected': '0.011991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12366', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-113.39', 'loss/train': '0.65261', 'examples_per_second': '30.7', 'grad_norm': '23.75', 'counters/examples': 270144, 'counters/updates': 8442}
train stats after 270176 examples: {'rewards_train/chosen': '0.17747', 'rewards_train/rejected': '0.03102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14645', 'logps_train/rejected': '-119.31', 'logps_train/chosen': '-169.94', 'loss/train': '0.64164', 'examples_per_second': '31.362', 'grad_norm': '26', 'counters/examples': 270176, 'counters/updates': 8443}
train stats after 270208 examples: {'rewards_train/chosen': '0.13784', 'rewards_train/rejected': '-0.063269', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20111', 'logps_train/rejected': '-120.82', 'logps_train/chosen': '-139.53', 'loss/train': '0.61859', 'examples_per_second': '31.83', 'grad_norm': '23.5', 'counters/examples': 270208, 'counters/updates': 8444}
train stats after 270240 examples: {'rewards_train/chosen': '0.22592', 'rewards_train/rejected': '0.051921', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.174', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-163.96', 'loss/train': '0.62673', 'examples_per_second': '31.614', 'grad_norm': '28.625', 'counters/examples': 270240, 'counters/updates': 8445}
train stats after 270272 examples: {'rewards_train/chosen': '0.13679', 'rewards_train/rejected': '0.0070601', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12973', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-146.83', 'loss/train': '0.63951', 'examples_per_second': '32.423', 'grad_norm': '23.5', 'counters/examples': 270272, 'counters/updates': 8446}
train stats after 270304 examples: {'rewards_train/chosen': '0.13266', 'rewards_train/rejected': '0.026349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10631', 'logps_train/rejected': '-133.03', 'logps_train/chosen': '-170.25', 'loss/train': '0.65461', 'examples_per_second': '31.704', 'grad_norm': '26.125', 'counters/examples': 270304, 'counters/updates': 8447}
train stats after 270336 examples: {'rewards_train/chosen': '0.12502', 'rewards_train/rejected': '0.040751', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084273', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-134.76', 'loss/train': '0.67493', 'examples_per_second': '32.429', 'grad_norm': '27.75', 'counters/examples': 270336, 'counters/updates': 8448}
train stats after 270368 examples: {'rewards_train/chosen': '0.15847', 'rewards_train/rejected': '-0.047571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20604', 'logps_train/rejected': '-93.195', 'logps_train/chosen': '-130.41', 'loss/train': '0.62771', 'examples_per_second': '32.252', 'grad_norm': '23.75', 'counters/examples': 270368, 'counters/updates': 8449}
skipping logging after 270400 examples to avoid logging too frequently
train stats after 270432 examples: {'rewards_train/chosen': '0.19069', 'rewards_train/rejected': '0.012118', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17857', 'logps_train/rejected': '-111.47', 'logps_train/chosen': '-135.52', 'loss/train': '0.62501', 'examples_per_second': '32.377', 'grad_norm': '23.5', 'counters/examples': 270432, 'counters/updates': 8451}
train stats after 270464 examples: {'rewards_train/chosen': '0.19614', 'rewards_train/rejected': '0.012301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18384', 'logps_train/rejected': '-148.3', 'logps_train/chosen': '-157.73', 'loss/train': '0.62329', 'examples_per_second': '30.517', 'grad_norm': '27.5', 'counters/examples': 270464, 'counters/updates': 8452}
skipping logging after 270496 examples to avoid logging too frequently
train stats after 270528 examples: {'rewards_train/chosen': '0.11353', 'rewards_train/rejected': '-0.010936', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12446', 'logps_train/rejected': '-154.55', 'logps_train/chosen': '-144.16', 'loss/train': '0.64551', 'examples_per_second': '30.005', 'grad_norm': '26.375', 'counters/examples': 270528, 'counters/updates': 8454}
skipping logging after 270560 examples to avoid logging too frequently
train stats after 270592 examples: {'rewards_train/chosen': '0.29598', 'rewards_train/rejected': '0.08268', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2133', 'logps_train/rejected': '-143.54', 'logps_train/chosen': '-156.12', 'loss/train': '0.623', 'examples_per_second': '31.54', 'grad_norm': '26.875', 'counters/examples': 270592, 'counters/updates': 8456}
train stats after 270624 examples: {'rewards_train/chosen': '0.17986', 'rewards_train/rejected': '0.1092', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070661', 'logps_train/rejected': '-153.03', 'logps_train/chosen': '-126.72', 'loss/train': '0.68902', 'examples_per_second': '32.003', 'grad_norm': '26.5', 'counters/examples': 270624, 'counters/updates': 8457}
train stats after 270656 examples: {'rewards_train/chosen': '0.18171', 'rewards_train/rejected': '-0.038605', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22032', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-160.2', 'loss/train': '0.6078', 'examples_per_second': '30.507', 'grad_norm': '27.875', 'counters/examples': 270656, 'counters/updates': 8458}
train stats after 270688 examples: {'rewards_train/chosen': '0.20763', 'rewards_train/rejected': '-0.0074231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21505', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-144.25', 'loss/train': '0.62423', 'examples_per_second': '31.575', 'grad_norm': '26.625', 'counters/examples': 270688, 'counters/updates': 8459}
train stats after 270720 examples: {'rewards_train/chosen': '0.11674', 'rewards_train/rejected': '-0.050029', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16677', 'logps_train/rejected': '-96.963', 'logps_train/chosen': '-114.3', 'loss/train': '0.63449', 'examples_per_second': '31.28', 'grad_norm': '23', 'counters/examples': 270720, 'counters/updates': 8460}
train stats after 270752 examples: {'rewards_train/chosen': '0.13424', 'rewards_train/rejected': '0.017541', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1167', 'logps_train/rejected': '-117.38', 'logps_train/chosen': '-114.39', 'loss/train': '0.64589', 'examples_per_second': '30.397', 'grad_norm': '22.5', 'counters/examples': 270752, 'counters/updates': 8461}
train stats after 270784 examples: {'rewards_train/chosen': '0.11881', 'rewards_train/rejected': '0.015596', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10322', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-115.98', 'loss/train': '0.65267', 'examples_per_second': '31.093', 'grad_norm': '24.25', 'counters/examples': 270784, 'counters/updates': 8462}
skipping logging after 270816 examples to avoid logging too frequently
train stats after 270848 examples: {'rewards_train/chosen': '0.12824', 'rewards_train/rejected': '-0.0043595', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1326', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-110.78', 'loss/train': '0.641', 'examples_per_second': '36.11', 'grad_norm': '21.625', 'counters/examples': 270848, 'counters/updates': 8464}
train stats after 270880 examples: {'rewards_train/chosen': '0.11867', 'rewards_train/rejected': '0.012924', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10575', 'logps_train/rejected': '-135.96', 'logps_train/chosen': '-131.5', 'loss/train': '0.65448', 'examples_per_second': '32.283', 'grad_norm': '26.875', 'counters/examples': 270880, 'counters/updates': 8465}
skipping logging after 270912 examples to avoid logging too frequently
train stats after 270944 examples: {'rewards_train/chosen': '0.15452', 'rewards_train/rejected': '0.01307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14145', 'logps_train/rejected': '-112.71', 'logps_train/chosen': '-118.27', 'loss/train': '0.63797', 'examples_per_second': '31.592', 'grad_norm': '22.25', 'counters/examples': 270944, 'counters/updates': 8467}
skipping logging after 270976 examples to avoid logging too frequently
train stats after 271008 examples: {'rewards_train/chosen': '0.17063', 'rewards_train/rejected': '0.093504', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07713', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-108.12', 'loss/train': '0.67494', 'examples_per_second': '30.573', 'grad_norm': '27.625', 'counters/examples': 271008, 'counters/updates': 8469}
skipping logging after 271040 examples to avoid logging too frequently
train stats after 271072 examples: {'rewards_train/chosen': '0.2229', 'rewards_train/rejected': '-0.0062181', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22912', 'logps_train/rejected': '-125.18', 'logps_train/chosen': '-145.43', 'loss/train': '0.60076', 'examples_per_second': '36.113', 'grad_norm': '28.25', 'counters/examples': 271072, 'counters/updates': 8471}
train stats after 271104 examples: {'rewards_train/chosen': '0.090756', 'rewards_train/rejected': '-0.0094185', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10017', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-140.49', 'loss/train': '0.66093', 'examples_per_second': '31.594', 'grad_norm': '24.625', 'counters/examples': 271104, 'counters/updates': 8472}
train stats after 271136 examples: {'rewards_train/chosen': '0.12255', 'rewards_train/rejected': '-0.047797', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17034', 'logps_train/rejected': '-100.09', 'logps_train/chosen': '-101.67', 'loss/train': '0.62047', 'examples_per_second': '30.2', 'grad_norm': '21.875', 'counters/examples': 271136, 'counters/updates': 8473}
train stats after 271168 examples: {'rewards_train/chosen': '0.13632', 'rewards_train/rejected': '0.016334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11999', 'logps_train/rejected': '-116.21', 'logps_train/chosen': '-118.18', 'loss/train': '0.64472', 'examples_per_second': '31.342', 'grad_norm': '30', 'counters/examples': 271168, 'counters/updates': 8474}
train stats after 271200 examples: {'rewards_train/chosen': '0.20515', 'rewards_train/rejected': '-0.0009912', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20615', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-127.95', 'loss/train': '0.61143', 'examples_per_second': '30.56', 'grad_norm': '22.875', 'counters/examples': 271200, 'counters/updates': 8475}
train stats after 271232 examples: {'rewards_train/chosen': '0.14929', 'rewards_train/rejected': '0.052004', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097281', 'logps_train/rejected': '-148.43', 'logps_train/chosen': '-135.6', 'loss/train': '0.67726', 'examples_per_second': '31.464', 'grad_norm': '28.5', 'counters/examples': 271232, 'counters/updates': 8476}
train stats after 271264 examples: {'rewards_train/chosen': '0.27638', 'rewards_train/rejected': '0.026158', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25023', 'logps_train/rejected': '-122.15', 'logps_train/chosen': '-155.03', 'loss/train': '0.59579', 'examples_per_second': '30.51', 'grad_norm': '25.625', 'counters/examples': 271264, 'counters/updates': 8477}
train stats after 271296 examples: {'rewards_train/chosen': '0.01014', 'rewards_train/rejected': '-0.018295', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028435', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-141.83', 'loss/train': '0.69467', 'examples_per_second': '31.65', 'grad_norm': '27.375', 'counters/examples': 271296, 'counters/updates': 8478}
train stats after 271328 examples: {'rewards_train/chosen': '0.20613', 'rewards_train/rejected': '0.079037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12709', 'logps_train/rejected': '-131.68', 'logps_train/chosen': '-147.98', 'loss/train': '0.65243', 'examples_per_second': '31.483', 'grad_norm': '28.5', 'counters/examples': 271328, 'counters/updates': 8479}
train stats after 271360 examples: {'rewards_train/chosen': '0.25702', 'rewards_train/rejected': '0.041385', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21564', 'logps_train/rejected': '-129.67', 'logps_train/chosen': '-151.07', 'loss/train': '0.60746', 'examples_per_second': '31.579', 'grad_norm': '52', 'counters/examples': 271360, 'counters/updates': 8480}
train stats after 271392 examples: {'rewards_train/chosen': '0.1786', 'rewards_train/rejected': '0.05187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12673', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-141.91', 'loss/train': '0.6438', 'examples_per_second': '30.206', 'grad_norm': '26.125', 'counters/examples': 271392, 'counters/updates': 8481}
train stats after 271424 examples: {'rewards_train/chosen': '0.15543', 'rewards_train/rejected': '0.026185', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12924', 'logps_train/rejected': '-122.92', 'logps_train/chosen': '-107.49', 'loss/train': '0.65361', 'examples_per_second': '30.874', 'grad_norm': '21.625', 'counters/examples': 271424, 'counters/updates': 8482}
train stats after 271456 examples: {'rewards_train/chosen': '0.24735', 'rewards_train/rejected': '0.054549', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1928', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-180.71', 'loss/train': '0.62166', 'examples_per_second': '31.556', 'grad_norm': '27.625', 'counters/examples': 271456, 'counters/updates': 8483}
skipping logging after 271488 examples to avoid logging too frequently
train stats after 271520 examples: {'rewards_train/chosen': '0.048485', 'rewards_train/rejected': '0.007483', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041002', 'logps_train/rejected': '-154.91', 'logps_train/chosen': '-115.72', 'loss/train': '0.68583', 'examples_per_second': '30.591', 'grad_norm': '32.25', 'counters/examples': 271520, 'counters/updates': 8485}
skipping logging after 271552 examples to avoid logging too frequently
train stats after 271584 examples: {'rewards_train/chosen': '0.1516', 'rewards_train/rejected': '0.0066402', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14496', 'logps_train/rejected': '-128.01', 'logps_train/chosen': '-166.82', 'loss/train': '0.63612', 'examples_per_second': '30.582', 'grad_norm': '27.875', 'counters/examples': 271584, 'counters/updates': 8487}
skipping logging after 271616 examples to avoid logging too frequently
train stats after 271648 examples: {'rewards_train/chosen': '0.20739', 'rewards_train/rejected': '0.14233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065063', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-149.7', 'loss/train': '0.66809', 'examples_per_second': '31.337', 'grad_norm': '27.875', 'counters/examples': 271648, 'counters/updates': 8489}
train stats after 271680 examples: {'rewards_train/chosen': '0.11134', 'rewards_train/rejected': '0.053885', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057458', 'logps_train/rejected': '-121.31', 'logps_train/chosen': '-120.84', 'loss/train': '0.6762', 'examples_per_second': '30.712', 'grad_norm': '25.625', 'counters/examples': 271680, 'counters/updates': 8490}
train stats after 271712 examples: {'rewards_train/chosen': '0.10989', 'rewards_train/rejected': '-0.0092062', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1191', 'logps_train/rejected': '-91.216', 'logps_train/chosen': '-120.44', 'loss/train': '0.64665', 'examples_per_second': '32.621', 'grad_norm': '26.625', 'counters/examples': 271712, 'counters/updates': 8491}
train stats after 271744 examples: {'rewards_train/chosen': '0.17733', 'rewards_train/rejected': '0.07449', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10284', 'logps_train/rejected': '-141.93', 'logps_train/chosen': '-149.3', 'loss/train': '0.66545', 'examples_per_second': '31.535', 'grad_norm': '25.875', 'counters/examples': 271744, 'counters/updates': 8492}
train stats after 271776 examples: {'rewards_train/chosen': '0.22601', 'rewards_train/rejected': '0.042847', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18316', 'logps_train/rejected': '-179.06', 'logps_train/chosen': '-192.69', 'loss/train': '0.62089', 'examples_per_second': '30.571', 'grad_norm': '27.75', 'counters/examples': 271776, 'counters/updates': 8493}
train stats after 271808 examples: {'rewards_train/chosen': '0.1643', 'rewards_train/rejected': '0.086858', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077439', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-134.53', 'loss/train': '0.66984', 'examples_per_second': '31.545', 'grad_norm': '24.5', 'counters/examples': 271808, 'counters/updates': 8494}
train stats after 271840 examples: {'rewards_train/chosen': '0.22502', 'rewards_train/rejected': '0.027161', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19786', 'logps_train/rejected': '-118.91', 'logps_train/chosen': '-133.49', 'loss/train': '0.6157', 'examples_per_second': '31.546', 'grad_norm': '24.75', 'counters/examples': 271840, 'counters/updates': 8495}
train stats after 271872 examples: {'rewards_train/chosen': '0.18454', 'rewards_train/rejected': '0.027382', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15716', 'logps_train/rejected': '-116.42', 'logps_train/chosen': '-160.56', 'loss/train': '0.63305', 'examples_per_second': '31.444', 'grad_norm': '26.375', 'counters/examples': 271872, 'counters/updates': 8496}
train stats after 271904 examples: {'rewards_train/chosen': '0.1085', 'rewards_train/rejected': '0.0074059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1011', 'logps_train/rejected': '-128.46', 'logps_train/chosen': '-109.96', 'loss/train': '0.65377', 'examples_per_second': '31.957', 'grad_norm': '23.75', 'counters/examples': 271904, 'counters/updates': 8497}
skipping logging after 271936 examples to avoid logging too frequently
train stats after 271968 examples: {'rewards_train/chosen': '0.049822', 'rewards_train/rejected': '0.026812', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02301', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-137.01', 'loss/train': '0.69862', 'examples_per_second': '33.041', 'grad_norm': '27.625', 'counters/examples': 271968, 'counters/updates': 8499}
train stats after 272000 examples: {'rewards_train/chosen': '0.029552', 'rewards_train/rejected': '-0.057248', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086799', 'logps_train/rejected': '-114.45', 'logps_train/chosen': '-107.36', 'loss/train': '0.65878', 'examples_per_second': '31.425', 'grad_norm': '26.375', 'counters/examples': 272000, 'counters/updates': 8500}
train stats after 272032 examples: {'rewards_train/chosen': '0.11184', 'rewards_train/rejected': '-0.10464', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21648', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-141.85', 'loss/train': '0.60748', 'examples_per_second': '32.5', 'grad_norm': '24.75', 'counters/examples': 272032, 'counters/updates': 8501}
train stats after 272064 examples: {'rewards_train/chosen': '0.072999', 'rewards_train/rejected': '-0.081424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15442', 'logps_train/rejected': '-99.318', 'logps_train/chosen': '-113.45', 'loss/train': '0.64307', 'examples_per_second': '32.577', 'grad_norm': '21.125', 'counters/examples': 272064, 'counters/updates': 8502}
train stats after 272096 examples: {'rewards_train/chosen': '0.10633', 'rewards_train/rejected': '-0.0035653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10989', 'logps_train/rejected': '-149.08', 'logps_train/chosen': '-186.3', 'loss/train': '0.65503', 'examples_per_second': '30.301', 'grad_norm': '30', 'counters/examples': 272096, 'counters/updates': 8503}
skipping logging after 272128 examples to avoid logging too frequently
train stats after 272160 examples: {'rewards_train/chosen': '0.11109', 'rewards_train/rejected': '0.0081436', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10295', 'logps_train/rejected': '-150.2', 'logps_train/chosen': '-155.62', 'loss/train': '0.67291', 'examples_per_second': '30.83', 'grad_norm': '30.5', 'counters/examples': 272160, 'counters/updates': 8505}
train stats after 272192 examples: {'rewards_train/chosen': '0.18346', 'rewards_train/rejected': '0.024274', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15919', 'logps_train/rejected': '-122.67', 'logps_train/chosen': '-134.21', 'loss/train': '0.63044', 'examples_per_second': '31.627', 'grad_norm': '25', 'counters/examples': 272192, 'counters/updates': 8506}
skipping logging after 272224 examples to avoid logging too frequently
train stats after 272256 examples: {'rewards_train/chosen': '0.10863', 'rewards_train/rejected': '0.086784', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021849', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-158.71', 'loss/train': '0.70921', 'examples_per_second': '31.72', 'grad_norm': '31.25', 'counters/examples': 272256, 'counters/updates': 8508}
train stats after 272288 examples: {'rewards_train/chosen': '0.13778', 'rewards_train/rejected': '-0.0065016', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14428', 'logps_train/rejected': '-80.564', 'logps_train/chosen': '-102.3', 'loss/train': '0.63657', 'examples_per_second': '30.65', 'grad_norm': '21.125', 'counters/examples': 272288, 'counters/updates': 8509}
train stats after 272320 examples: {'rewards_train/chosen': '0.06101', 'rewards_train/rejected': '-0.026289', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087299', 'logps_train/rejected': '-115.4', 'logps_train/chosen': '-130.55', 'loss/train': '0.67008', 'examples_per_second': '31.516', 'grad_norm': '37.25', 'counters/examples': 272320, 'counters/updates': 8510}
train stats after 272352 examples: {'rewards_train/chosen': '0.1457', 'rewards_train/rejected': '0.0033037', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1424', 'logps_train/rejected': '-109.14', 'logps_train/chosen': '-146.68', 'loss/train': '0.655', 'examples_per_second': '31.76', 'grad_norm': '25.75', 'counters/examples': 272352, 'counters/updates': 8511}
train stats after 272384 examples: {'rewards_train/chosen': '0.13712', 'rewards_train/rejected': '0.090779', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046346', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-166.17', 'loss/train': '0.67743', 'examples_per_second': '31.631', 'grad_norm': '29.625', 'counters/examples': 272384, 'counters/updates': 8512}
train stats after 272416 examples: {'rewards_train/chosen': '0.23343', 'rewards_train/rejected': '0.064255', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16918', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-147.03', 'loss/train': '0.62471', 'examples_per_second': '31.674', 'grad_norm': '26.125', 'counters/examples': 272416, 'counters/updates': 8513}
train stats after 272448 examples: {'rewards_train/chosen': '0.20611', 'rewards_train/rejected': '0.0085534', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19756', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-195.02', 'loss/train': '0.62073', 'examples_per_second': '30.105', 'grad_norm': '29', 'counters/examples': 272448, 'counters/updates': 8514}
train stats after 272480 examples: {'rewards_train/chosen': '0.14659', 'rewards_train/rejected': '-0.02177', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16836', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-141.78', 'loss/train': '0.63509', 'examples_per_second': '32.572', 'grad_norm': '24.5', 'counters/examples': 272480, 'counters/updates': 8515}
train stats after 272512 examples: {'rewards_train/chosen': '0.077646', 'rewards_train/rejected': '-0.050354', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.128', 'logps_train/rejected': '-107.84', 'logps_train/chosen': '-110.57', 'loss/train': '0.64082', 'examples_per_second': '30.218', 'grad_norm': '21.625', 'counters/examples': 272512, 'counters/updates': 8516}
train stats after 272544 examples: {'rewards_train/chosen': '0.14844', 'rewards_train/rejected': '-0.031328', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17977', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-156.61', 'loss/train': '0.62117', 'examples_per_second': '31.626', 'grad_norm': '26.125', 'counters/examples': 272544, 'counters/updates': 8517}
skipping logging after 272576 examples to avoid logging too frequently
train stats after 272608 examples: {'rewards_train/chosen': '0.11906', 'rewards_train/rejected': '-0.025338', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1444', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-111.2', 'loss/train': '0.63594', 'examples_per_second': '33.356', 'grad_norm': '23.75', 'counters/examples': 272608, 'counters/updates': 8519}
skipping logging after 272640 examples to avoid logging too frequently
train stats after 272672 examples: {'rewards_train/chosen': '0.22462', 'rewards_train/rejected': '0.053603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17101', 'logps_train/rejected': '-110.25', 'logps_train/chosen': '-150.84', 'loss/train': '0.62514', 'examples_per_second': '37.272', 'grad_norm': '24.125', 'counters/examples': 272672, 'counters/updates': 8521}
train stats after 272704 examples: {'rewards_train/chosen': '0.14942', 'rewards_train/rejected': '0.039468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10995', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-130.6', 'loss/train': '0.66436', 'examples_per_second': '32.37', 'grad_norm': '26.875', 'counters/examples': 272704, 'counters/updates': 8522}
skipping logging after 272736 examples to avoid logging too frequently
train stats after 272768 examples: {'rewards_train/chosen': '0.14617', 'rewards_train/rejected': '-0.023599', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16977', 'logps_train/rejected': '-81.297', 'logps_train/chosen': '-140.1', 'loss/train': '0.62072', 'examples_per_second': '31.267', 'grad_norm': '22.75', 'counters/examples': 272768, 'counters/updates': 8524}
train stats after 272800 examples: {'rewards_train/chosen': '0.13625', 'rewards_train/rejected': '-0.037582', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17383', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-132.07', 'loss/train': '0.62281', 'examples_per_second': '31.692', 'grad_norm': '24.625', 'counters/examples': 272800, 'counters/updates': 8525}
train stats after 272832 examples: {'rewards_train/chosen': '0.22085', 'rewards_train/rejected': '0.17569', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04516', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-125.35', 'loss/train': '0.6884', 'examples_per_second': '31.642', 'grad_norm': '35', 'counters/examples': 272832, 'counters/updates': 8526}
train stats after 272864 examples: {'rewards_train/chosen': '0.15742', 'rewards_train/rejected': '-0.0389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19632', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-141.54', 'loss/train': '0.6165', 'examples_per_second': '30.288', 'grad_norm': '22.25', 'counters/examples': 272864, 'counters/updates': 8527}
train stats after 272896 examples: {'rewards_train/chosen': '0.12109', 'rewards_train/rejected': '-0.069647', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19074', 'logps_train/rejected': '-95.05', 'logps_train/chosen': '-106.18', 'loss/train': '0.615', 'examples_per_second': '32.97', 'grad_norm': '20.25', 'counters/examples': 272896, 'counters/updates': 8528}
train stats after 272928 examples: {'rewards_train/chosen': '0.11592', 'rewards_train/rejected': '0.0066151', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10931', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-140.34', 'loss/train': '0.66335', 'examples_per_second': '32.044', 'grad_norm': '25', 'counters/examples': 272928, 'counters/updates': 8529}
skipping logging after 272960 examples to avoid logging too frequently
train stats after 272992 examples: {'rewards_train/chosen': '0.10133', 'rewards_train/rejected': '-0.0017027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10304', 'logps_train/rejected': '-115.38', 'logps_train/chosen': '-101.53', 'loss/train': '0.65238', 'examples_per_second': '31.653', 'grad_norm': '23.625', 'counters/examples': 272992, 'counters/updates': 8531}
skipping logging after 273024 examples to avoid logging too frequently
train stats after 273056 examples: {'rewards_train/chosen': '0.13217', 'rewards_train/rejected': '0.011759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12042', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-136.78', 'loss/train': '0.64845', 'examples_per_second': '30.335', 'grad_norm': '23.125', 'counters/examples': 273056, 'counters/updates': 8533}
train stats after 273088 examples: {'rewards_train/chosen': '0.11783', 'rewards_train/rejected': '-0.12094', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23877', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-175.92', 'loss/train': '0.59949', 'examples_per_second': '32.72', 'grad_norm': '31.125', 'counters/examples': 273088, 'counters/updates': 8534}
train stats after 273120 examples: {'rewards_train/chosen': '0.16495', 'rewards_train/rejected': '0.072139', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092814', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-134.82', 'loss/train': '0.65912', 'examples_per_second': '32.693', 'grad_norm': '25.875', 'counters/examples': 273120, 'counters/updates': 8535}
train stats after 273152 examples: {'rewards_train/chosen': '0.188', 'rewards_train/rejected': '0.03674', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15126', 'logps_train/rejected': '-106.59', 'logps_train/chosen': '-147.18', 'loss/train': '0.63484', 'examples_per_second': '32.138', 'grad_norm': '26.625', 'counters/examples': 273152, 'counters/updates': 8536}
skipping logging after 273184 examples to avoid logging too frequently
train stats after 273216 examples: {'rewards_train/chosen': '0.18543', 'rewards_train/rejected': '0.018739', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16669', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-141.01', 'loss/train': '0.62465', 'examples_per_second': '32.014', 'grad_norm': '35', 'counters/examples': 273216, 'counters/updates': 8538}
train stats after 273248 examples: {'rewards_train/chosen': '0.099027', 'rewards_train/rejected': '-0.050764', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14979', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-97.625', 'loss/train': '0.63471', 'examples_per_second': '30.872', 'grad_norm': '20.75', 'counters/examples': 273248, 'counters/updates': 8539}
train stats after 273280 examples: {'rewards_train/chosen': '0.081869', 'rewards_train/rejected': '0.025549', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05632', 'logps_train/rejected': '-110.43', 'logps_train/chosen': '-142.32', 'loss/train': '0.67587', 'examples_per_second': '32.057', 'grad_norm': '26', 'counters/examples': 273280, 'counters/updates': 8540}
train stats after 273312 examples: {'rewards_train/chosen': '0.12599', 'rewards_train/rejected': '0.035248', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090741', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-151.47', 'loss/train': '0.66724', 'examples_per_second': '30.653', 'grad_norm': '28.875', 'counters/examples': 273312, 'counters/updates': 8541}
train stats after 273344 examples: {'rewards_train/chosen': '0.22998', 'rewards_train/rejected': '0.14387', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086107', 'logps_train/rejected': '-146.72', 'logps_train/chosen': '-151.13', 'loss/train': '0.66257', 'examples_per_second': '31.575', 'grad_norm': '29', 'counters/examples': 273344, 'counters/updates': 8542}
train stats after 273376 examples: {'rewards_train/chosen': '0.086354', 'rewards_train/rejected': '-0.019356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10571', 'logps_train/rejected': '-87.933', 'logps_train/chosen': '-91.694', 'loss/train': '0.65695', 'examples_per_second': '31.512', 'grad_norm': '20.125', 'counters/examples': 273376, 'counters/updates': 8543}
train stats after 273408 examples: {'rewards_train/chosen': '0.1906', 'rewards_train/rejected': '0.13241', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058192', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-133.03', 'loss/train': '0.67681', 'examples_per_second': '30.431', 'grad_norm': '23.125', 'counters/examples': 273408, 'counters/updates': 8544}
skipping logging after 273440 examples to avoid logging too frequently
train stats after 273472 examples: {'rewards_train/chosen': '0.09093', 'rewards_train/rejected': '-0.046679', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13761', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-158.37', 'loss/train': '0.64043', 'examples_per_second': '25.587', 'grad_norm': '26.125', 'counters/examples': 273472, 'counters/updates': 8546}
train stats after 273504 examples: {'rewards_train/chosen': '0.095482', 'rewards_train/rejected': '0.024135', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071346', 'logps_train/rejected': '-98.947', 'logps_train/chosen': '-126.22', 'loss/train': '0.66936', 'examples_per_second': '30.511', 'grad_norm': '25.875', 'counters/examples': 273504, 'counters/updates': 8547}
train stats after 273536 examples: {'rewards_train/chosen': '0.16283', 'rewards_train/rejected': '0.034046', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12879', 'logps_train/rejected': '-151.29', 'logps_train/chosen': '-136.39', 'loss/train': '0.64462', 'examples_per_second': '31.782', 'grad_norm': '27.25', 'counters/examples': 273536, 'counters/updates': 8548}
train stats after 273568 examples: {'rewards_train/chosen': '0.26157', 'rewards_train/rejected': '-0.026671', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.28825', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-178.07', 'loss/train': '0.56904', 'examples_per_second': '25.262', 'grad_norm': '24.25', 'counters/examples': 273568, 'counters/updates': 8549}
train stats after 273600 examples: {'rewards_train/chosen': '0.16225', 'rewards_train/rejected': '0.0039252', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15833', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-126.33', 'loss/train': '0.62856', 'examples_per_second': '30.993', 'grad_norm': '21.375', 'counters/examples': 273600, 'counters/updates': 8550}
train stats after 273632 examples: {'rewards_train/chosen': '0.13366', 'rewards_train/rejected': '0.032337', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10132', 'logps_train/rejected': '-152.47', 'logps_train/chosen': '-119', 'loss/train': '0.65879', 'examples_per_second': '31.192', 'grad_norm': '26.5', 'counters/examples': 273632, 'counters/updates': 8551}
train stats after 273664 examples: {'rewards_train/chosen': '0.16563', 'rewards_train/rejected': '-0.090491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25612', 'logps_train/rejected': '-116.51', 'logps_train/chosen': '-122.24', 'loss/train': '0.58812', 'examples_per_second': '31.597', 'grad_norm': '22.375', 'counters/examples': 273664, 'counters/updates': 8552}
train stats after 273696 examples: {'rewards_train/chosen': '0.18444', 'rewards_train/rejected': '0.069083', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11536', 'logps_train/rejected': '-116.04', 'logps_train/chosen': '-130.73', 'loss/train': '0.65098', 'examples_per_second': '30.697', 'grad_norm': '26.625', 'counters/examples': 273696, 'counters/updates': 8553}
skipping logging after 273728 examples to avoid logging too frequently
train stats after 273760 examples: {'rewards_train/chosen': '0.1458', 'rewards_train/rejected': '-0.010571', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15638', 'logps_train/rejected': '-114.07', 'logps_train/chosen': '-124.24', 'loss/train': '0.63711', 'examples_per_second': '32.249', 'grad_norm': '25.5', 'counters/examples': 273760, 'counters/updates': 8555}
train stats after 273792 examples: {'rewards_train/chosen': '0.15293', 'rewards_train/rejected': '-0.0023157', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15524', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-141.67', 'loss/train': '0.6371', 'examples_per_second': '30.42', 'grad_norm': '25.25', 'counters/examples': 273792, 'counters/updates': 8556}
skipping logging after 273824 examples to avoid logging too frequently
train stats after 273856 examples: {'rewards_train/chosen': '0.13207', 'rewards_train/rejected': '0.033954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098113', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-170.59', 'loss/train': '0.67032', 'examples_per_second': '38.384', 'grad_norm': '27.5', 'counters/examples': 273856, 'counters/updates': 8558}
train stats after 273888 examples: {'rewards_train/chosen': '0.14935', 'rewards_train/rejected': '0.11979', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029564', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-127.41', 'loss/train': '0.69393', 'examples_per_second': '29.779', 'grad_norm': '29.75', 'counters/examples': 273888, 'counters/updates': 8559}
train stats after 273920 examples: {'rewards_train/chosen': '0.15576', 'rewards_train/rejected': '0.065979', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089781', 'logps_train/rejected': '-144.89', 'logps_train/chosen': '-140.38', 'loss/train': '0.66735', 'examples_per_second': '30.861', 'grad_norm': '26.125', 'counters/examples': 273920, 'counters/updates': 8560}
train stats after 273952 examples: {'rewards_train/chosen': '0.16105', 'rewards_train/rejected': '0.036555', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1245', 'logps_train/rejected': '-118.05', 'logps_train/chosen': '-132.93', 'loss/train': '0.6427', 'examples_per_second': '31.37', 'grad_norm': '24.75', 'counters/examples': 273952, 'counters/updates': 8561}
train stats after 273984 examples: {'rewards_train/chosen': '0.12781', 'rewards_train/rejected': '0.0094761', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11834', 'logps_train/rejected': '-129.47', 'logps_train/chosen': '-113.99', 'loss/train': '0.6514', 'examples_per_second': '30.82', 'grad_norm': '25.375', 'counters/examples': 273984, 'counters/updates': 8562}
train stats after 274016 examples: {'rewards_train/chosen': '0.22511', 'rewards_train/rejected': '0.021957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20315', 'logps_train/rejected': '-112.33', 'logps_train/chosen': '-123.13', 'loss/train': '0.62056', 'examples_per_second': '29.957', 'grad_norm': '22.5', 'counters/examples': 274016, 'counters/updates': 8563}
skipping logging after 274048 examples to avoid logging too frequently
train stats after 274080 examples: {'rewards_train/chosen': '0.12114', 'rewards_train/rejected': '0.0059579', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11518', 'logps_train/rejected': '-108.68', 'logps_train/chosen': '-143.06', 'loss/train': '0.65019', 'examples_per_second': '33.071', 'grad_norm': '24.75', 'counters/examples': 274080, 'counters/updates': 8565}
train stats after 274112 examples: {'rewards_train/chosen': '0.13613', 'rewards_train/rejected': '-0.035999', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17213', 'logps_train/rejected': '-116.5', 'logps_train/chosen': '-156.07', 'loss/train': '0.6265', 'examples_per_second': '30.125', 'grad_norm': '31.125', 'counters/examples': 274112, 'counters/updates': 8566}
train stats after 274144 examples: {'rewards_train/chosen': '0.20512', 'rewards_train/rejected': '0.076084', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12904', 'logps_train/rejected': '-166.18', 'logps_train/chosen': '-159.02', 'loss/train': '0.65067', 'examples_per_second': '31.597', 'grad_norm': '28.625', 'counters/examples': 274144, 'counters/updates': 8567}
train stats after 274176 examples: {'rewards_train/chosen': '0.13246', 'rewards_train/rejected': '0.012094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12036', 'logps_train/rejected': '-120.8', 'logps_train/chosen': '-144.58', 'loss/train': '0.6577', 'examples_per_second': '32.874', 'grad_norm': '27.125', 'counters/examples': 274176, 'counters/updates': 8568}
skipping logging after 274208 examples to avoid logging too frequently
train stats after 274240 examples: {'rewards_train/chosen': '0.23424', 'rewards_train/rejected': '0.052213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18202', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-123.95', 'loss/train': '0.6268', 'examples_per_second': '31.659', 'grad_norm': '25.875', 'counters/examples': 274240, 'counters/updates': 8570}
train stats after 274272 examples: {'rewards_train/chosen': '0.073266', 'rewards_train/rejected': '-0.04739', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12066', 'logps_train/rejected': '-130.82', 'logps_train/chosen': '-150.02', 'loss/train': '0.65048', 'examples_per_second': '31.588', 'grad_norm': '29.125', 'counters/examples': 274272, 'counters/updates': 8571}
train stats after 274304 examples: {'rewards_train/chosen': '0.20601', 'rewards_train/rejected': '-0.012712', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21872', 'logps_train/rejected': '-120.94', 'logps_train/chosen': '-111.3', 'loss/train': '0.61362', 'examples_per_second': '33.103', 'grad_norm': '21.375', 'counters/examples': 274304, 'counters/updates': 8572}
train stats after 274336 examples: {'rewards_train/chosen': '0.071853', 'rewards_train/rejected': '-0.068332', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14018', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-135.77', 'loss/train': '0.63831', 'examples_per_second': '31.975', 'grad_norm': '24.875', 'counters/examples': 274336, 'counters/updates': 8573}
train stats after 274368 examples: {'rewards_train/chosen': '0.057538', 'rewards_train/rejected': '-0.046738', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10428', 'logps_train/rejected': '-123.52', 'logps_train/chosen': '-131.8', 'loss/train': '0.65371', 'examples_per_second': '31.836', 'grad_norm': '24.5', 'counters/examples': 274368, 'counters/updates': 8574}
skipping logging after 274400 examples to avoid logging too frequently
train stats after 274432 examples: {'rewards_train/chosen': '0.053149', 'rewards_train/rejected': '0.084722', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.031573', 'logps_train/rejected': '-100.49', 'logps_train/chosen': '-120.7', 'loss/train': '0.72437', 'examples_per_second': '31.624', 'grad_norm': '30.375', 'counters/examples': 274432, 'counters/updates': 8576}
train stats after 274464 examples: {'rewards_train/chosen': '0.16014', 'rewards_train/rejected': '0.06185', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098293', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-142.13', 'loss/train': '0.65971', 'examples_per_second': '31.653', 'grad_norm': '27.875', 'counters/examples': 274464, 'counters/updates': 8577}
train stats after 274496 examples: {'rewards_train/chosen': '0.15485', 'rewards_train/rejected': '0.077588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077258', 'logps_train/rejected': '-111.24', 'logps_train/chosen': '-120.51', 'loss/train': '0.67398', 'examples_per_second': '31.783', 'grad_norm': '30.5', 'counters/examples': 274496, 'counters/updates': 8578}
train stats after 274528 examples: {'rewards_train/chosen': '0.14819', 'rewards_train/rejected': '-0.051675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19986', 'logps_train/rejected': '-133.02', 'logps_train/chosen': '-142.23', 'loss/train': '0.6237', 'examples_per_second': '29.825', 'grad_norm': '25', 'counters/examples': 274528, 'counters/updates': 8579}
train stats after 274560 examples: {'rewards_train/chosen': '0.12358', 'rewards_train/rejected': '0.093024', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030555', 'logps_train/rejected': '-160.44', 'logps_train/chosen': '-157.44', 'loss/train': '0.69521', 'examples_per_second': '31.292', 'grad_norm': '31.125', 'counters/examples': 274560, 'counters/updates': 8580}
skipping logging after 274592 examples to avoid logging too frequently
train stats after 274624 examples: {'rewards_train/chosen': '0.070581', 'rewards_train/rejected': '-0.12988', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20046', 'logps_train/rejected': '-79.936', 'logps_train/chosen': '-103.44', 'loss/train': '0.61054', 'examples_per_second': '34.267', 'grad_norm': '19', 'counters/examples': 274624, 'counters/updates': 8582}
train stats after 274656 examples: {'rewards_train/chosen': '0.03293', 'rewards_train/rejected': '0.019126', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013803', 'logps_train/rejected': '-156.14', 'logps_train/chosen': '-170.1', 'loss/train': '0.70273', 'examples_per_second': '25.661', 'grad_norm': '29.375', 'counters/examples': 274656, 'counters/updates': 8583}
train stats after 274688 examples: {'rewards_train/chosen': '0.17206', 'rewards_train/rejected': '-7.1164e-05', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17213', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-137.75', 'loss/train': '0.62247', 'examples_per_second': '31.19', 'grad_norm': '22.875', 'counters/examples': 274688, 'counters/updates': 8584}
train stats after 274720 examples: {'rewards_train/chosen': '0.081164', 'rewards_train/rejected': '-0.057648', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13881', 'logps_train/rejected': '-162.77', 'logps_train/chosen': '-134.1', 'loss/train': '0.63983', 'examples_per_second': '31.065', 'grad_norm': '25.125', 'counters/examples': 274720, 'counters/updates': 8585}
train stats after 274752 examples: {'rewards_train/chosen': '0.016675', 'rewards_train/rejected': '-0.021613', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038288', 'logps_train/rejected': '-96.99', 'logps_train/chosen': '-157.1', 'loss/train': '0.68592', 'examples_per_second': '31.524', 'grad_norm': '26.875', 'counters/examples': 274752, 'counters/updates': 8586}
train stats after 274784 examples: {'rewards_train/chosen': '0.13747', 'rewards_train/rejected': '0.025986', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11149', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-147.02', 'loss/train': '0.6502', 'examples_per_second': '30.815', 'grad_norm': '23.625', 'counters/examples': 274784, 'counters/updates': 8587}
train stats after 274816 examples: {'rewards_train/chosen': '0.19246', 'rewards_train/rejected': '0.061379', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13109', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-137.57', 'loss/train': '0.63953', 'examples_per_second': '31.568', 'grad_norm': '31.25', 'counters/examples': 274816, 'counters/updates': 8588}
skipping logging after 274848 examples to avoid logging too frequently
train stats after 274880 examples: {'rewards_train/chosen': '0.1903', 'rewards_train/rejected': '0.083521', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10677', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-140.71', 'loss/train': '0.65101', 'examples_per_second': '31.294', 'grad_norm': '26', 'counters/examples': 274880, 'counters/updates': 8590}
train stats after 274912 examples: {'rewards_train/chosen': '0.10233', 'rewards_train/rejected': '0.038708', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063623', 'logps_train/rejected': '-134.72', 'logps_train/chosen': '-154.5', 'loss/train': '0.67356', 'examples_per_second': '31.641', 'grad_norm': '27', 'counters/examples': 274912, 'counters/updates': 8591}
train stats after 274944 examples: {'rewards_train/chosen': '0.1085', 'rewards_train/rejected': '-0.00020631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1087', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-106.78', 'loss/train': '0.65504', 'examples_per_second': '30.123', 'grad_norm': '22.375', 'counters/examples': 274944, 'counters/updates': 8592}
skipping logging after 274976 examples to avoid logging too frequently
train stats after 275008 examples: {'rewards_train/chosen': '0.22965', 'rewards_train/rejected': '0.010764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21889', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-133.12', 'loss/train': '0.62711', 'examples_per_second': '30.499', 'grad_norm': '22.5', 'counters/examples': 275008, 'counters/updates': 8594}
skipping logging after 275040 examples to avoid logging too frequently
train stats after 275072 examples: {'rewards_train/chosen': '0.20699', 'rewards_train/rejected': '0.030883', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17611', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-133.74', 'loss/train': '0.63274', 'examples_per_second': '30.57', 'grad_norm': '25.375', 'counters/examples': 275072, 'counters/updates': 8596}
train stats after 275104 examples: {'rewards_train/chosen': '0.16557', 'rewards_train/rejected': '0.098216', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067349', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-181.04', 'loss/train': '0.67355', 'examples_per_second': '31.49', 'grad_norm': '29.125', 'counters/examples': 275104, 'counters/updates': 8597}
train stats after 275136 examples: {'rewards_train/chosen': '0.17736', 'rewards_train/rejected': '0.046823', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13053', 'logps_train/rejected': '-103.57', 'logps_train/chosen': '-133.78', 'loss/train': '0.6523', 'examples_per_second': '31.35', 'grad_norm': '22.75', 'counters/examples': 275136, 'counters/updates': 8598}
train stats after 275168 examples: {'rewards_train/chosen': '0.20564', 'rewards_train/rejected': '0.037465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16817', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-111.09', 'loss/train': '0.62655', 'examples_per_second': '32.68', 'grad_norm': '23.125', 'counters/examples': 275168, 'counters/updates': 8599}
skipping logging after 275200 examples to avoid logging too frequently
train stats after 275232 examples: {'rewards_train/chosen': '0.11617', 'rewards_train/rejected': '0.041961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074208', 'logps_train/rejected': '-112.25', 'logps_train/chosen': '-135.49', 'loss/train': '0.68264', 'examples_per_second': '31.465', 'grad_norm': '24.25', 'counters/examples': 275232, 'counters/updates': 8601}
train stats after 275264 examples: {'rewards_train/chosen': '0.13676', 'rewards_train/rejected': '0.03532', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10144', 'logps_train/rejected': '-103.8', 'logps_train/chosen': '-125.85', 'loss/train': '0.65949', 'examples_per_second': '30.739', 'grad_norm': '23.375', 'counters/examples': 275264, 'counters/updates': 8602}
train stats after 275296 examples: {'rewards_train/chosen': '0.17126', 'rewards_train/rejected': '0.014003', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15726', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-137.95', 'loss/train': '0.63868', 'examples_per_second': '31.299', 'grad_norm': '25.5', 'counters/examples': 275296, 'counters/updates': 8603}
train stats after 275328 examples: {'rewards_train/chosen': '0.090357', 'rewards_train/rejected': '-0.053531', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14389', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-98.245', 'loss/train': '0.64099', 'examples_per_second': '30.928', 'grad_norm': '22.625', 'counters/examples': 275328, 'counters/updates': 8604}
train stats after 275360 examples: {'rewards_train/chosen': '0.16494', 'rewards_train/rejected': '0.049592', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11535', 'logps_train/rejected': '-136.66', 'logps_train/chosen': '-165.17', 'loss/train': '0.6506', 'examples_per_second': '31.564', 'grad_norm': '27.75', 'counters/examples': 275360, 'counters/updates': 8605}
train stats after 275392 examples: {'rewards_train/chosen': '0.25363', 'rewards_train/rejected': '0.0019908', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25164', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-135.12', 'loss/train': '0.61283', 'examples_per_second': '32.397', 'grad_norm': '25.375', 'counters/examples': 275392, 'counters/updates': 8606}
train stats after 275424 examples: {'rewards_train/chosen': '0.1714', 'rewards_train/rejected': '0.097383', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074014', 'logps_train/rejected': '-111.08', 'logps_train/chosen': '-136.47', 'loss/train': '0.68645', 'examples_per_second': '32.254', 'grad_norm': '27.375', 'counters/examples': 275424, 'counters/updates': 8607}
train stats after 275456 examples: {'rewards_train/chosen': '0.17429', 'rewards_train/rejected': '0.097961', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07633', 'logps_train/rejected': '-140.08', 'logps_train/chosen': '-137.15', 'loss/train': '0.67482', 'examples_per_second': '31.664', 'grad_norm': '29', 'counters/examples': 275456, 'counters/updates': 8608}
train stats after 275488 examples: {'rewards_train/chosen': '0.16883', 'rewards_train/rejected': '0.053267', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11556', 'logps_train/rejected': '-111', 'logps_train/chosen': '-128.25', 'loss/train': '0.66547', 'examples_per_second': '30.046', 'grad_norm': '27', 'counters/examples': 275488, 'counters/updates': 8609}
train stats after 275520 examples: {'rewards_train/chosen': '0.17764', 'rewards_train/rejected': '0.032138', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1455', 'logps_train/rejected': '-111.99', 'logps_train/chosen': '-160.36', 'loss/train': '0.63774', 'examples_per_second': '30.946', 'grad_norm': '26.375', 'counters/examples': 275520, 'counters/updates': 8610}
skipping logging after 275552 examples to avoid logging too frequently
train stats after 275584 examples: {'rewards_train/chosen': '0.16191', 'rewards_train/rejected': '0.09556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066353', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-157.59', 'loss/train': '0.6694', 'examples_per_second': '33.112', 'grad_norm': '26.375', 'counters/examples': 275584, 'counters/updates': 8612}
train stats after 275616 examples: {'rewards_train/chosen': '0.15316', 'rewards_train/rejected': '0.014771', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13839', 'logps_train/rejected': '-104.67', 'logps_train/chosen': '-150.24', 'loss/train': '0.64795', 'examples_per_second': '31.59', 'grad_norm': '23.875', 'counters/examples': 275616, 'counters/updates': 8613}
train stats after 275648 examples: {'rewards_train/chosen': '0.08332', 'rewards_train/rejected': '-0.019955', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10327', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-98.163', 'loss/train': '0.65229', 'examples_per_second': '31.973', 'grad_norm': '22.625', 'counters/examples': 275648, 'counters/updates': 8614}
skipping logging after 275680 examples to avoid logging too frequently
train stats after 275712 examples: {'rewards_train/chosen': '0.17228', 'rewards_train/rejected': '0.016435', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15585', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-121.32', 'loss/train': '0.62692', 'examples_per_second': '31.353', 'grad_norm': '22.5', 'counters/examples': 275712, 'counters/updates': 8616}
train stats after 275744 examples: {'rewards_train/chosen': '0.14924', 'rewards_train/rejected': '0.056023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093221', 'logps_train/rejected': '-146.05', 'logps_train/chosen': '-177.78', 'loss/train': '0.66774', 'examples_per_second': '33.01', 'grad_norm': '30.125', 'counters/examples': 275744, 'counters/updates': 8617}
train stats after 275776 examples: {'rewards_train/chosen': '0.17847', 'rewards_train/rejected': '-0.054537', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23301', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-149.42', 'loss/train': '0.59873', 'examples_per_second': '30.284', 'grad_norm': '26.75', 'counters/examples': 275776, 'counters/updates': 8618}
train stats after 275808 examples: {'rewards_train/chosen': '0.24494', 'rewards_train/rejected': '0.028782', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21616', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-144.65', 'loss/train': '0.60805', 'examples_per_second': '31.675', 'grad_norm': '23.375', 'counters/examples': 275808, 'counters/updates': 8619}
train stats after 275840 examples: {'rewards_train/chosen': '0.11638', 'rewards_train/rejected': '0.11464', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0017453', 'logps_train/rejected': '-142.08', 'logps_train/chosen': '-124.94', 'loss/train': '0.70608', 'examples_per_second': '31.308', 'grad_norm': '38.25', 'counters/examples': 275840, 'counters/updates': 8620}
train stats after 275872 examples: {'rewards_train/chosen': '0.22156', 'rewards_train/rejected': '0.14624', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075316', 'logps_train/rejected': '-140.84', 'logps_train/chosen': '-165.32', 'loss/train': '0.6754', 'examples_per_second': '31.065', 'grad_norm': '29.625', 'counters/examples': 275872, 'counters/updates': 8621}
train stats after 275904 examples: {'rewards_train/chosen': '0.080939', 'rewards_train/rejected': '0.0035887', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07735', 'logps_train/rejected': '-96.804', 'logps_train/chosen': '-82.708', 'loss/train': '0.67026', 'examples_per_second': '31.967', 'grad_norm': '21.375', 'counters/examples': 275904, 'counters/updates': 8622}
skipping logging after 275936 examples to avoid logging too frequently
train stats after 275968 examples: {'rewards_train/chosen': '0.13835', 'rewards_train/rejected': '0.0090183', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12933', 'logps_train/rejected': '-126.79', 'logps_train/chosen': '-149.45', 'loss/train': '0.64703', 'examples_per_second': '31.507', 'grad_norm': '29.5', 'counters/examples': 275968, 'counters/updates': 8624}
train stats after 276000 examples: {'rewards_train/chosen': '0.21065', 'rewards_train/rejected': '0.096726', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11392', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-111.8', 'loss/train': '0.65401', 'examples_per_second': '32.153', 'grad_norm': '24.75', 'counters/examples': 276000, 'counters/updates': 8625}
Running evaluation after 276000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.27it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 276000: {'rewards_eval/chosen': '0.15135', 'rewards_eval/rejected': '0.032322', 'rewards_eval/accuracies': '0.61719', 'rewards_eval/margins': '0.11903', 'logps_eval/rejected': '-118.29', 'logps_eval/chosen': '-137.92', 'loss/eval': '0.65279'}
skipping save for non epoch
train stats after 276032 examples: {'rewards_train/chosen': '0.18442', 'rewards_train/rejected': '-0.049018', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23344', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-137.94', 'loss/train': '0.59994', 'examples_per_second': '33.362', 'grad_norm': '25.625', 'counters/examples': 276032, 'counters/updates': 8626}
skipping logging after 276064 examples to avoid logging too frequently
train stats after 276096 examples: {'rewards_train/chosen': '0.12878', 'rewards_train/rejected': '0.14982', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.021042', 'logps_train/rejected': '-148.47', 'logps_train/chosen': '-145.14', 'loss/train': '0.71724', 'examples_per_second': '34.522', 'grad_norm': '29.25', 'counters/examples': 276096, 'counters/updates': 8628}
train stats after 276128 examples: {'rewards_train/chosen': '0.15624', 'rewards_train/rejected': '0.043662', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11258', 'logps_train/rejected': '-107.07', 'logps_train/chosen': '-128.84', 'loss/train': '0.65108', 'examples_per_second': '30.559', 'grad_norm': '24.875', 'counters/examples': 276128, 'counters/updates': 8629}
train stats after 276160 examples: {'rewards_train/chosen': '0.12784', 'rewards_train/rejected': '-0.0065389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13438', 'logps_train/rejected': '-102.34', 'logps_train/chosen': '-128.78', 'loss/train': '0.64463', 'examples_per_second': '31.553', 'grad_norm': '23.25', 'counters/examples': 276160, 'counters/updates': 8630}
skipping logging after 276192 examples to avoid logging too frequently
train stats after 276224 examples: {'rewards_train/chosen': '0.075915', 'rewards_train/rejected': '0.13415', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.058235', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-125.8', 'loss/train': '0.73683', 'examples_per_second': '31.326', 'grad_norm': '33', 'counters/examples': 276224, 'counters/updates': 8632}
train stats after 276256 examples: {'rewards_train/chosen': '0.027941', 'rewards_train/rejected': '-0.011775', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.039716', 'logps_train/rejected': '-139.73', 'logps_train/chosen': '-180.8', 'loss/train': '0.69733', 'examples_per_second': '31.299', 'grad_norm': '205', 'counters/examples': 276256, 'counters/updates': 8633}
train stats after 276288 examples: {'rewards_train/chosen': '0.17421', 'rewards_train/rejected': '0.03014', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14407', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-146.91', 'loss/train': '0.64411', 'examples_per_second': '32.204', 'grad_norm': '27.625', 'counters/examples': 276288, 'counters/updates': 8634}
train stats after 276320 examples: {'rewards_train/chosen': '0.13115', 'rewards_train/rejected': '-0.047288', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17844', 'logps_train/rejected': '-131.34', 'logps_train/chosen': '-156.22', 'loss/train': '0.61856', 'examples_per_second': '30.689', 'grad_norm': '24.625', 'counters/examples': 276320, 'counters/updates': 8635}
train stats after 276352 examples: {'rewards_train/chosen': '0.15372', 'rewards_train/rejected': '0.052279', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10144', 'logps_train/rejected': '-154.6', 'logps_train/chosen': '-152.52', 'loss/train': '0.66046', 'examples_per_second': '31.451', 'grad_norm': '26.25', 'counters/examples': 276352, 'counters/updates': 8636}
train stats after 276384 examples: {'rewards_train/chosen': '0.16767', 'rewards_train/rejected': '0.025316', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14236', 'logps_train/rejected': '-96.101', 'logps_train/chosen': '-116.38', 'loss/train': '0.63717', 'examples_per_second': '30.811', 'grad_norm': '24.625', 'counters/examples': 276384, 'counters/updates': 8637}
skipping logging after 276416 examples to avoid logging too frequently
train stats after 276448 examples: {'rewards_train/chosen': '0.15808', 'rewards_train/rejected': '0.1185', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03958', 'logps_train/rejected': '-151.95', 'logps_train/chosen': '-164.03', 'loss/train': '0.69098', 'examples_per_second': '31.458', 'grad_norm': '28.75', 'counters/examples': 276448, 'counters/updates': 8639}
skipping logging after 276480 examples to avoid logging too frequently
train stats after 276512 examples: {'rewards_train/chosen': '0.2051', 'rewards_train/rejected': '0.0039355', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20116', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-127.2', 'loss/train': '0.61407', 'examples_per_second': '34.029', 'grad_norm': '22', 'counters/examples': 276512, 'counters/updates': 8641}
train stats after 276544 examples: {'rewards_train/chosen': '0.048024', 'rewards_train/rejected': '-0.014541', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062565', 'logps_train/rejected': '-108.72', 'logps_train/chosen': '-116.51', 'loss/train': '0.67864', 'examples_per_second': '32.595', 'grad_norm': '23.75', 'counters/examples': 276544, 'counters/updates': 8642}
train stats after 276576 examples: {'rewards_train/chosen': '0.1436', 'rewards_train/rejected': '-0.10932', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25292', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-161.33', 'loss/train': '0.59095', 'examples_per_second': '31.607', 'grad_norm': '24.875', 'counters/examples': 276576, 'counters/updates': 8643}
train stats after 276608 examples: {'rewards_train/chosen': '0.15871', 'rewards_train/rejected': '-0.00056887', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15928', 'logps_train/rejected': '-104.96', 'logps_train/chosen': '-147.77', 'loss/train': '0.62982', 'examples_per_second': '31.91', 'grad_norm': '24.625', 'counters/examples': 276608, 'counters/updates': 8644}
train stats after 276640 examples: {'rewards_train/chosen': '0.15718', 'rewards_train/rejected': '0.039941', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11724', 'logps_train/rejected': '-103.79', 'logps_train/chosen': '-138.08', 'loss/train': '0.64649', 'examples_per_second': '30.282', 'grad_norm': '24.5', 'counters/examples': 276640, 'counters/updates': 8645}
train stats after 276672 examples: {'rewards_train/chosen': '0.10069', 'rewards_train/rejected': '-0.0056', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10629', 'logps_train/rejected': '-158.45', 'logps_train/chosen': '-131.77', 'loss/train': '0.6579', 'examples_per_second': '31.426', 'grad_norm': '25.125', 'counters/examples': 276672, 'counters/updates': 8646}
train stats after 276704 examples: {'rewards_train/chosen': '0.054992', 'rewards_train/rejected': '-0.030821', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085814', 'logps_train/rejected': '-109.38', 'logps_train/chosen': '-141', 'loss/train': '0.67685', 'examples_per_second': '30.191', 'grad_norm': '24.875', 'counters/examples': 276704, 'counters/updates': 8647}
skipping logging after 276736 examples to avoid logging too frequently
train stats after 276768 examples: {'rewards_train/chosen': '0.18075', 'rewards_train/rejected': '0.016097', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16466', 'logps_train/rejected': '-107.36', 'logps_train/chosen': '-124.77', 'loss/train': '0.6333', 'examples_per_second': '33.342', 'grad_norm': '22.75', 'counters/examples': 276768, 'counters/updates': 8649}
train stats after 276800 examples: {'rewards_train/chosen': '0.10336', 'rewards_train/rejected': '0.030088', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.073268', 'logps_train/rejected': '-132.43', 'logps_train/chosen': '-162.68', 'loss/train': '0.67406', 'examples_per_second': '30.089', 'grad_norm': '27.125', 'counters/examples': 276800, 'counters/updates': 8650}
train stats after 276832 examples: {'rewards_train/chosen': '0.063459', 'rewards_train/rejected': '-0.040664', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10412', 'logps_train/rejected': '-108.23', 'logps_train/chosen': '-91.616', 'loss/train': '0.65693', 'examples_per_second': '31.89', 'grad_norm': '24', 'counters/examples': 276832, 'counters/updates': 8651}
train stats after 276864 examples: {'rewards_train/chosen': '0.094958', 'rewards_train/rejected': '-0.04987', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14483', 'logps_train/rejected': '-94.469', 'logps_train/chosen': '-129.14', 'loss/train': '0.63687', 'examples_per_second': '31.661', 'grad_norm': '23.625', 'counters/examples': 276864, 'counters/updates': 8652}
train stats after 276896 examples: {'rewards_train/chosen': '0.11403', 'rewards_train/rejected': '-0.012996', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12703', 'logps_train/rejected': '-156.69', 'logps_train/chosen': '-119.51', 'loss/train': '0.64203', 'examples_per_second': '31.675', 'grad_norm': '25.5', 'counters/examples': 276896, 'counters/updates': 8653}
train stats after 276928 examples: {'rewards_train/chosen': '0.18894', 'rewards_train/rejected': '0.020298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16864', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-155.15', 'loss/train': '0.63266', 'examples_per_second': '32.63', 'grad_norm': '25.875', 'counters/examples': 276928, 'counters/updates': 8654}
train stats after 276960 examples: {'rewards_train/chosen': '0.10124', 'rewards_train/rejected': '0.042986', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058252', 'logps_train/rejected': '-131.46', 'logps_train/chosen': '-108.3', 'loss/train': '0.67877', 'examples_per_second': '32.228', 'grad_norm': '24.5', 'counters/examples': 276960, 'counters/updates': 8655}
train stats after 276992 examples: {'rewards_train/chosen': '0.17046', 'rewards_train/rejected': '0.062116', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10835', 'logps_train/rejected': '-96.063', 'logps_train/chosen': '-137.39', 'loss/train': '0.65469', 'examples_per_second': '31.672', 'grad_norm': '26.375', 'counters/examples': 276992, 'counters/updates': 8656}
skipping logging after 277024 examples to avoid logging too frequently
train stats after 277056 examples: {'rewards_train/chosen': '0.23062', 'rewards_train/rejected': '0.066473', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16414', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-145.11', 'loss/train': '0.64444', 'examples_per_second': '32.895', 'grad_norm': '27.375', 'counters/examples': 277056, 'counters/updates': 8658}
train stats after 277088 examples: {'rewards_train/chosen': '0.12333', 'rewards_train/rejected': '-0.062519', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18584', 'logps_train/rejected': '-110.09', 'logps_train/chosen': '-122.34', 'loss/train': '0.62309', 'examples_per_second': '32.434', 'grad_norm': '23.25', 'counters/examples': 277088, 'counters/updates': 8659}
train stats after 277120 examples: {'rewards_train/chosen': '0.20693', 'rewards_train/rejected': '0.059743', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14718', 'logps_train/rejected': '-177.42', 'logps_train/chosen': '-142.29', 'loss/train': '0.64166', 'examples_per_second': '31.656', 'grad_norm': '29.25', 'counters/examples': 277120, 'counters/updates': 8660}
train stats after 277152 examples: {'rewards_train/chosen': '0.1269', 'rewards_train/rejected': '-0.0013651', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12826', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-126.53', 'loss/train': '0.65752', 'examples_per_second': '33.113', 'grad_norm': '24.25', 'counters/examples': 277152, 'counters/updates': 8661}
skipping logging after 277184 examples to avoid logging too frequently
train stats after 277216 examples: {'rewards_train/chosen': '0.066867', 'rewards_train/rejected': '0.029827', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037041', 'logps_train/rejected': '-120.18', 'logps_train/chosen': '-122.65', 'loss/train': '0.71273', 'examples_per_second': '34.334', 'grad_norm': '28.75', 'counters/examples': 277216, 'counters/updates': 8663}
skipping logging after 277248 examples to avoid logging too frequently
train stats after 277280 examples: {'rewards_train/chosen': '0.16783', 'rewards_train/rejected': '0.038794', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12903', 'logps_train/rejected': '-145.39', 'logps_train/chosen': '-169.05', 'loss/train': '0.64176', 'examples_per_second': '31.507', 'grad_norm': '31.5', 'counters/examples': 277280, 'counters/updates': 8665}
train stats after 277312 examples: {'rewards_train/chosen': '0.17623', 'rewards_train/rejected': '0.021505', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15473', 'logps_train/rejected': '-141.43', 'logps_train/chosen': '-124.47', 'loss/train': '0.63041', 'examples_per_second': '30.831', 'grad_norm': '24', 'counters/examples': 277312, 'counters/updates': 8666}
train stats after 277344 examples: {'rewards_train/chosen': '0.20827', 'rewards_train/rejected': '0.12464', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083632', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-138.27', 'loss/train': '0.68139', 'examples_per_second': '31.383', 'grad_norm': '27.875', 'counters/examples': 277344, 'counters/updates': 8667}
train stats after 277376 examples: {'rewards_train/chosen': '0.24121', 'rewards_train/rejected': '0.025396', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21582', 'logps_train/rejected': '-113.25', 'logps_train/chosen': '-132.53', 'loss/train': '0.61015', 'examples_per_second': '31.829', 'grad_norm': '22.75', 'counters/examples': 277376, 'counters/updates': 8668}
train stats after 277408 examples: {'rewards_train/chosen': '0.14651', 'rewards_train/rejected': '0.010195', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13631', 'logps_train/rejected': '-97.657', 'logps_train/chosen': '-164.26', 'loss/train': '0.64155', 'examples_per_second': '31.742', 'grad_norm': '24.25', 'counters/examples': 277408, 'counters/updates': 8669}
train stats after 277440 examples: {'rewards_train/chosen': '0.16292', 'rewards_train/rejected': '0.15468', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0082427', 'logps_train/rejected': '-145.38', 'logps_train/chosen': '-140.42', 'loss/train': '0.71196', 'examples_per_second': '32.137', 'grad_norm': '30.875', 'counters/examples': 277440, 'counters/updates': 8670}
train stats after 277472 examples: {'rewards_train/chosen': '0.16286', 'rewards_train/rejected': '0.13937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02349', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-124.19', 'loss/train': '0.69636', 'examples_per_second': '30.895', 'grad_norm': '26', 'counters/examples': 277472, 'counters/updates': 8671}
train stats after 277504 examples: {'rewards_train/chosen': '0.12303', 'rewards_train/rejected': '0.06639', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056639', 'logps_train/rejected': '-153.27', 'logps_train/chosen': '-143.76', 'loss/train': '0.68342', 'examples_per_second': '30.244', 'grad_norm': '30', 'counters/examples': 277504, 'counters/updates': 8672}
train stats after 277536 examples: {'rewards_train/chosen': '0.10836', 'rewards_train/rejected': '0.067259', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041101', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-101.03', 'loss/train': '0.68483', 'examples_per_second': '32.426', 'grad_norm': '25', 'counters/examples': 277536, 'counters/updates': 8673}
train stats after 277568 examples: {'rewards_train/chosen': '0.27414', 'rewards_train/rejected': '0.081552', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19259', 'logps_train/rejected': '-144.69', 'logps_train/chosen': '-127.64', 'loss/train': '0.62677', 'examples_per_second': '30.146', 'grad_norm': '24.375', 'counters/examples': 277568, 'counters/updates': 8674}
train stats after 277600 examples: {'rewards_train/chosen': '0.17054', 'rewards_train/rejected': '0.096904', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073631', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-128.74', 'loss/train': '0.67848', 'examples_per_second': '31.656', 'grad_norm': '25.5', 'counters/examples': 277600, 'counters/updates': 8675}
skipping logging after 277632 examples to avoid logging too frequently
train stats after 277664 examples: {'rewards_train/chosen': '0.10342', 'rewards_train/rejected': '0.0030544', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10037', 'logps_train/rejected': '-122.28', 'logps_train/chosen': '-106.57', 'loss/train': '0.65279', 'examples_per_second': '30.625', 'grad_norm': '23.125', 'counters/examples': 277664, 'counters/updates': 8677}
train stats after 277696 examples: {'rewards_train/chosen': '0.2164', 'rewards_train/rejected': '0.085949', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13045', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-129.56', 'loss/train': '0.652', 'examples_per_second': '33.006', 'grad_norm': '27.625', 'counters/examples': 277696, 'counters/updates': 8678}
skipping logging after 277728 examples to avoid logging too frequently
train stats after 277760 examples: {'rewards_train/chosen': '0.11518', 'rewards_train/rejected': '0.045287', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069897', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-167.4', 'loss/train': '0.66725', 'examples_per_second': '33.935', 'grad_norm': '26.5', 'counters/examples': 277760, 'counters/updates': 8680}
train stats after 277792 examples: {'rewards_train/chosen': '0.14754', 'rewards_train/rejected': '0.026003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12154', 'logps_train/rejected': '-162.93', 'logps_train/chosen': '-162.51', 'loss/train': '0.6497', 'examples_per_second': '31.658', 'grad_norm': '31', 'counters/examples': 277792, 'counters/updates': 8681}
train stats after 277824 examples: {'rewards_train/chosen': '0.12162', 'rewards_train/rejected': '-0.0041703', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12579', 'logps_train/rejected': '-115.25', 'logps_train/chosen': '-135.97', 'loss/train': '0.64957', 'examples_per_second': '31.277', 'grad_norm': '28.875', 'counters/examples': 277824, 'counters/updates': 8682}
train stats after 277856 examples: {'rewards_train/chosen': '0.22426', 'rewards_train/rejected': '0.076529', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14773', 'logps_train/rejected': '-130.03', 'logps_train/chosen': '-156.46', 'loss/train': '0.63522', 'examples_per_second': '31.658', 'grad_norm': '24.5', 'counters/examples': 277856, 'counters/updates': 8683}
train stats after 277888 examples: {'rewards_train/chosen': '0.21072', 'rewards_train/rejected': '-0.026321', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23704', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-138.16', 'loss/train': '0.59517', 'examples_per_second': '31.057', 'grad_norm': '24.125', 'counters/examples': 277888, 'counters/updates': 8684}
train stats after 277920 examples: {'rewards_train/chosen': '0.19659', 'rewards_train/rejected': '0.028544', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16804', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-155.54', 'loss/train': '0.62614', 'examples_per_second': '31.266', 'grad_norm': '25.375', 'counters/examples': 277920, 'counters/updates': 8685}
train stats after 277952 examples: {'rewards_train/chosen': '0.16231', 'rewards_train/rejected': '0.014598', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14771', 'logps_train/rejected': '-85.316', 'logps_train/chosen': '-108.96', 'loss/train': '0.63498', 'examples_per_second': '31.681', 'grad_norm': '21.625', 'counters/examples': 277952, 'counters/updates': 8686}
train stats after 277984 examples: {'rewards_train/chosen': '0.11839', 'rewards_train/rejected': '-0.0077113', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1261', 'logps_train/rejected': '-95.231', 'logps_train/chosen': '-131.33', 'loss/train': '0.65191', 'examples_per_second': '30.567', 'grad_norm': '21.125', 'counters/examples': 277984, 'counters/updates': 8687}
train stats after 278016 examples: {'rewards_train/chosen': '0.20856', 'rewards_train/rejected': '-0.11855', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.32712', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-151.61', 'loss/train': '0.5838', 'examples_per_second': '31.655', 'grad_norm': '25.75', 'counters/examples': 278016, 'counters/updates': 8688}
train stats after 278048 examples: {'rewards_train/chosen': '0.06022', 'rewards_train/rejected': '0.0011994', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059021', 'logps_train/rejected': '-103.81', 'logps_train/chosen': '-128.77', 'loss/train': '0.67788', 'examples_per_second': '31.476', 'grad_norm': '26.625', 'counters/examples': 278048, 'counters/updates': 8689}
skipping logging after 278080 examples to avoid logging too frequently
train stats after 278112 examples: {'rewards_train/chosen': '0.32045', 'rewards_train/rejected': '-0.014995', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33545', 'logps_train/rejected': '-111.84', 'logps_train/chosen': '-156.59', 'loss/train': '0.57527', 'examples_per_second': '31.916', 'grad_norm': '23.25', 'counters/examples': 278112, 'counters/updates': 8691}
train stats after 278144 examples: {'rewards_train/chosen': '0.10767', 'rewards_train/rejected': '0.02617', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.081504', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-134.09', 'loss/train': '0.67231', 'examples_per_second': '31.51', 'grad_norm': '26.5', 'counters/examples': 278144, 'counters/updates': 8692}
train stats after 278176 examples: {'rewards_train/chosen': '0.17296', 'rewards_train/rejected': '0.14498', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027977', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-126.98', 'loss/train': '0.69625', 'examples_per_second': '32.477', 'grad_norm': '27.25', 'counters/examples': 278176, 'counters/updates': 8693}
train stats after 278208 examples: {'rewards_train/chosen': '0.2047', 'rewards_train/rejected': '0.049087', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15561', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-167.41', 'loss/train': '0.65478', 'examples_per_second': '30.251', 'grad_norm': '30.875', 'counters/examples': 278208, 'counters/updates': 8694}
train stats after 278240 examples: {'rewards_train/chosen': '0.1027', 'rewards_train/rejected': '0.12569', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02299', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-138.94', 'loss/train': '0.7229', 'examples_per_second': '31.12', 'grad_norm': '26.75', 'counters/examples': 278240, 'counters/updates': 8695}
train stats after 278272 examples: {'rewards_train/chosen': '0.17168', 'rewards_train/rejected': '0.12551', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046173', 'logps_train/rejected': '-133.54', 'logps_train/chosen': '-128.51', 'loss/train': '0.68094', 'examples_per_second': '31.431', 'grad_norm': '31', 'counters/examples': 278272, 'counters/updates': 8696}
train stats after 278304 examples: {'rewards_train/chosen': '0.23125', 'rewards_train/rejected': '0.059021', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17223', 'logps_train/rejected': '-94.177', 'logps_train/chosen': '-155.28', 'loss/train': '0.64081', 'examples_per_second': '30.975', 'grad_norm': '24.875', 'counters/examples': 278304, 'counters/updates': 8697}
train stats after 278336 examples: {'rewards_train/chosen': '0.10797', 'rewards_train/rejected': '-0.027671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13564', 'logps_train/rejected': '-105.37', 'logps_train/chosen': '-143.33', 'loss/train': '0.63854', 'examples_per_second': '32.148', 'grad_norm': '23.5', 'counters/examples': 278336, 'counters/updates': 8698}
train stats after 278368 examples: {'rewards_train/chosen': '0.16579', 'rewards_train/rejected': '0.10775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058037', 'logps_train/rejected': '-130.87', 'logps_train/chosen': '-143.89', 'loss/train': '0.67858', 'examples_per_second': '31.594', 'grad_norm': '28.125', 'counters/examples': 278368, 'counters/updates': 8699}
train stats after 278400 examples: {'rewards_train/chosen': '0.14886', 'rewards_train/rejected': '0.0023479', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14651', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-148.74', 'loss/train': '0.64237', 'examples_per_second': '31.592', 'grad_norm': '27.75', 'counters/examples': 278400, 'counters/updates': 8700}
train stats after 278432 examples: {'rewards_train/chosen': '0.15091', 'rewards_train/rejected': '-0.05241', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20332', 'logps_train/rejected': '-107.75', 'logps_train/chosen': '-142.5', 'loss/train': '0.62075', 'examples_per_second': '31.695', 'grad_norm': '22.75', 'counters/examples': 278432, 'counters/updates': 8701}
train stats after 278464 examples: {'rewards_train/chosen': '-0.0015516', 'rewards_train/rejected': '-0.059995', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058443', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-158.76', 'loss/train': '0.6781', 'examples_per_second': '30.475', 'grad_norm': '27.25', 'counters/examples': 278464, 'counters/updates': 8702}
train stats after 278496 examples: {'rewards_train/chosen': '0.10954', 'rewards_train/rejected': '-0.0087874', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11833', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-149.17', 'loss/train': '0.6606', 'examples_per_second': '32.389', 'grad_norm': '28.25', 'counters/examples': 278496, 'counters/updates': 8703}
train stats after 278528 examples: {'rewards_train/chosen': '0.10208', 'rewards_train/rejected': '0.109', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.006921', 'logps_train/rejected': '-127.28', 'logps_train/chosen': '-146.71', 'loss/train': '0.71341', 'examples_per_second': '30.587', 'grad_norm': '29', 'counters/examples': 278528, 'counters/updates': 8704}
train stats after 278560 examples: {'rewards_train/chosen': '0.1393', 'rewards_train/rejected': '-0.017619', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15692', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-120.2', 'loss/train': '0.62553', 'examples_per_second': '30.912', 'grad_norm': '21.375', 'counters/examples': 278560, 'counters/updates': 8705}
train stats after 278592 examples: {'rewards_train/chosen': '0.14431', 'rewards_train/rejected': '0.0082578', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13605', 'logps_train/rejected': '-120.3', 'logps_train/chosen': '-126.82', 'loss/train': '0.64665', 'examples_per_second': '30.976', 'grad_norm': '24.125', 'counters/examples': 278592, 'counters/updates': 8706}
train stats after 278624 examples: {'rewards_train/chosen': '0.19491', 'rewards_train/rejected': '0.11518', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079723', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-125.77', 'loss/train': '0.68794', 'examples_per_second': '32.92', 'grad_norm': '27.125', 'counters/examples': 278624, 'counters/updates': 8707}
train stats after 278656 examples: {'rewards_train/chosen': '0.23108', 'rewards_train/rejected': '0.038963', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19212', 'logps_train/rejected': '-112.82', 'logps_train/chosen': '-202.54', 'loss/train': '0.64259', 'examples_per_second': '31.6', 'grad_norm': '29', 'counters/examples': 278656, 'counters/updates': 8708}
train stats after 278688 examples: {'rewards_train/chosen': '0.19894', 'rewards_train/rejected': '0.051614', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14733', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-181.02', 'loss/train': '0.63416', 'examples_per_second': '30.092', 'grad_norm': '27.75', 'counters/examples': 278688, 'counters/updates': 8709}
skipping logging after 278720 examples to avoid logging too frequently
train stats after 278752 examples: {'rewards_train/chosen': '0.072954', 'rewards_train/rejected': '0.001355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071599', 'logps_train/rejected': '-126.4', 'logps_train/chosen': '-131.2', 'loss/train': '0.67198', 'examples_per_second': '31.356', 'grad_norm': '28.25', 'counters/examples': 278752, 'counters/updates': 8711}
train stats after 278784 examples: {'rewards_train/chosen': '0.19091', 'rewards_train/rejected': '0.0031282', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18778', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-135.2', 'loss/train': '0.62801', 'examples_per_second': '30.584', 'grad_norm': '23.625', 'counters/examples': 278784, 'counters/updates': 8712}
train stats after 278816 examples: {'rewards_train/chosen': '0.098195', 'rewards_train/rejected': '-0.058172', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15637', 'logps_train/rejected': '-100.99', 'logps_train/chosen': '-115.96', 'loss/train': '0.64351', 'examples_per_second': '31.85', 'grad_norm': '23.25', 'counters/examples': 278816, 'counters/updates': 8713}
train stats after 278848 examples: {'rewards_train/chosen': '0.19607', 'rewards_train/rejected': '0.079424', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11665', 'logps_train/rejected': '-142.73', 'logps_train/chosen': '-143.2', 'loss/train': '0.64934', 'examples_per_second': '31.607', 'grad_norm': '28.625', 'counters/examples': 278848, 'counters/updates': 8714}
skipping logging after 278880 examples to avoid logging too frequently
train stats after 278912 examples: {'rewards_train/chosen': '0.13209', 'rewards_train/rejected': '-0.011687', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14378', 'logps_train/rejected': '-110.45', 'logps_train/chosen': '-131.35', 'loss/train': '0.64327', 'examples_per_second': '32.527', 'grad_norm': '38.5', 'counters/examples': 278912, 'counters/updates': 8716}
train stats after 278944 examples: {'rewards_train/chosen': '0.14522', 'rewards_train/rejected': '0.067847', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077378', 'logps_train/rejected': '-154.7', 'logps_train/chosen': '-157.33', 'loss/train': '0.67137', 'examples_per_second': '23.78', 'grad_norm': '28.125', 'counters/examples': 278944, 'counters/updates': 8717}
skipping logging after 278976 examples to avoid logging too frequently
train stats after 279008 examples: {'rewards_train/chosen': '0.18427', 'rewards_train/rejected': '0.032888', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15138', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-129.54', 'loss/train': '0.64036', 'examples_per_second': '31.539', 'grad_norm': '25.625', 'counters/examples': 279008, 'counters/updates': 8719}
train stats after 279040 examples: {'rewards_train/chosen': '0.23278', 'rewards_train/rejected': '-0.013718', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24649', 'logps_train/rejected': '-107.45', 'logps_train/chosen': '-135.31', 'loss/train': '0.59553', 'examples_per_second': '24.352', 'grad_norm': '22.625', 'counters/examples': 279040, 'counters/updates': 8720}
skipping logging after 279072 examples to avoid logging too frequently
train stats after 279104 examples: {'rewards_train/chosen': '0.11694', 'rewards_train/rejected': '0.053499', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063438', 'logps_train/rejected': '-155.59', 'logps_train/chosen': '-156.25', 'loss/train': '0.68146', 'examples_per_second': '32.127', 'grad_norm': '27.25', 'counters/examples': 279104, 'counters/updates': 8722}
train stats after 279136 examples: {'rewards_train/chosen': '0.17603', 'rewards_train/rejected': '0.038861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13717', 'logps_train/rejected': '-164.62', 'logps_train/chosen': '-164.83', 'loss/train': '0.64356', 'examples_per_second': '31.517', 'grad_norm': '27', 'counters/examples': 279136, 'counters/updates': 8723}
train stats after 279168 examples: {'rewards_train/chosen': '0.10703', 'rewards_train/rejected': '0.10714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00011376', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-154.87', 'loss/train': '0.71356', 'examples_per_second': '32.319', 'grad_norm': '30', 'counters/examples': 279168, 'counters/updates': 8724}
train stats after 279200 examples: {'rewards_train/chosen': '0.14114', 'rewards_train/rejected': '-0.014959', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1561', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-147.64', 'loss/train': '0.63536', 'examples_per_second': '30.043', 'grad_norm': '24.5', 'counters/examples': 279200, 'counters/updates': 8725}
train stats after 279232 examples: {'rewards_train/chosen': '0.20442', 'rewards_train/rejected': '0.0020654', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20236', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-157.52', 'loss/train': '0.60808', 'examples_per_second': '31.581', 'grad_norm': '24.125', 'counters/examples': 279232, 'counters/updates': 8726}
train stats after 279264 examples: {'rewards_train/chosen': '0.11678', 'rewards_train/rejected': '-0.0026274', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1194', 'logps_train/rejected': '-120.07', 'logps_train/chosen': '-117.52', 'loss/train': '0.65015', 'examples_per_second': '31.036', 'grad_norm': '24.125', 'counters/examples': 279264, 'counters/updates': 8727}
train stats after 279296 examples: {'rewards_train/chosen': '0.0424', 'rewards_train/rejected': '0.0020339', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.040366', 'logps_train/rejected': '-114.94', 'logps_train/chosen': '-133.6', 'loss/train': '0.68792', 'examples_per_second': '30.096', 'grad_norm': '31.625', 'counters/examples': 279296, 'counters/updates': 8728}
train stats after 279328 examples: {'rewards_train/chosen': '0.13499', 'rewards_train/rejected': '-0.019556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15455', 'logps_train/rejected': '-93.011', 'logps_train/chosen': '-112.41', 'loss/train': '0.64908', 'examples_per_second': '32.793', 'grad_norm': '22.125', 'counters/examples': 279328, 'counters/updates': 8729}
train stats after 279360 examples: {'rewards_train/chosen': '0.16171', 'rewards_train/rejected': '0.15295', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0087594', 'logps_train/rejected': '-168.81', 'logps_train/chosen': '-165.78', 'loss/train': '0.71107', 'examples_per_second': '31.609', 'grad_norm': '30.625', 'counters/examples': 279360, 'counters/updates': 8730}
train stats after 279392 examples: {'rewards_train/chosen': '0.13912', 'rewards_train/rejected': '-0.0092244', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14835', 'logps_train/rejected': '-140.07', 'logps_train/chosen': '-155.11', 'loss/train': '0.63687', 'examples_per_second': '31.617', 'grad_norm': '25.875', 'counters/examples': 279392, 'counters/updates': 8731}
skipping logging after 279424 examples to avoid logging too frequently
train stats after 279456 examples: {'rewards_train/chosen': '0.18601', 'rewards_train/rejected': '0.04997', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13604', 'logps_train/rejected': '-110.24', 'logps_train/chosen': '-130.16', 'loss/train': '0.63922', 'examples_per_second': '30.132', 'grad_norm': '22.375', 'counters/examples': 279456, 'counters/updates': 8733}
train stats after 279488 examples: {'rewards_train/chosen': '0.13847', 'rewards_train/rejected': '0.022239', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11624', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-111.27', 'loss/train': '0.6528', 'examples_per_second': '31.594', 'grad_norm': '27', 'counters/examples': 279488, 'counters/updates': 8734}
train stats after 279520 examples: {'rewards_train/chosen': '0.1932', 'rewards_train/rejected': '0.048231', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14496', 'logps_train/rejected': '-111.86', 'logps_train/chosen': '-116.42', 'loss/train': '0.64328', 'examples_per_second': '31.928', 'grad_norm': '25', 'counters/examples': 279520, 'counters/updates': 8735}
train stats after 279552 examples: {'rewards_train/chosen': '0.11805', 'rewards_train/rejected': '-0.033358', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15141', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-139.66', 'loss/train': '0.63462', 'examples_per_second': '32.029', 'grad_norm': '26', 'counters/examples': 279552, 'counters/updates': 8736}
train stats after 279584 examples: {'rewards_train/chosen': '0.1381', 'rewards_train/rejected': '0.12413', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013971', 'logps_train/rejected': '-135.34', 'logps_train/chosen': '-163.92', 'loss/train': '0.70671', 'examples_per_second': '31.583', 'grad_norm': '31.625', 'counters/examples': 279584, 'counters/updates': 8737}
train stats after 279616 examples: {'rewards_train/chosen': '0.12696', 'rewards_train/rejected': '0.066761', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060202', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-185.49', 'loss/train': '0.68148', 'examples_per_second': '31.831', 'grad_norm': '30.5', 'counters/examples': 279616, 'counters/updates': 8738}
train stats after 279648 examples: {'rewards_train/chosen': '0.12037', 'rewards_train/rejected': '0.093718', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026653', 'logps_train/rejected': '-145.57', 'logps_train/chosen': '-157.58', 'loss/train': '0.69253', 'examples_per_second': '31.012', 'grad_norm': '26.125', 'counters/examples': 279648, 'counters/updates': 8739}
train stats after 279680 examples: {'rewards_train/chosen': '0.047805', 'rewards_train/rejected': '-0.0086999', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056505', 'logps_train/rejected': '-145.37', 'logps_train/chosen': '-129.89', 'loss/train': '0.67991', 'examples_per_second': '31.217', 'grad_norm': '24.625', 'counters/examples': 279680, 'counters/updates': 8740}
train stats after 279712 examples: {'rewards_train/chosen': '0.1542', 'rewards_train/rejected': '-0.017153', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17135', 'logps_train/rejected': '-103.43', 'logps_train/chosen': '-131.02', 'loss/train': '0.62221', 'examples_per_second': '31.19', 'grad_norm': '24.75', 'counters/examples': 279712, 'counters/updates': 8741}
train stats after 279744 examples: {'rewards_train/chosen': '0.16713', 'rewards_train/rejected': '0.057358', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10977', 'logps_train/rejected': '-127.8', 'logps_train/chosen': '-190.17', 'loss/train': '0.64973', 'examples_per_second': '31.577', 'grad_norm': '26', 'counters/examples': 279744, 'counters/updates': 8742}
skipping logging after 279776 examples to avoid logging too frequently
train stats after 279808 examples: {'rewards_train/chosen': '0.17029', 'rewards_train/rejected': '-0.0010129', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17131', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-119.21', 'loss/train': '0.6273', 'examples_per_second': '30.09', 'grad_norm': '25', 'counters/examples': 279808, 'counters/updates': 8744}
skipping logging after 279840 examples to avoid logging too frequently
train stats after 279872 examples: {'rewards_train/chosen': '0.02813', 'rewards_train/rejected': '-0.032689', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06082', 'logps_train/rejected': '-88.742', 'logps_train/chosen': '-103.29', 'loss/train': '0.67342', 'examples_per_second': '31.316', 'grad_norm': '25.75', 'counters/examples': 279872, 'counters/updates': 8746}
skipping logging after 279904 examples to avoid logging too frequently
train stats after 279936 examples: {'rewards_train/chosen': '0.13493', 'rewards_train/rejected': '0.095397', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039533', 'logps_train/rejected': '-98.425', 'logps_train/chosen': '-129.29', 'loss/train': '0.68419', 'examples_per_second': '31.243', 'grad_norm': '22.875', 'counters/examples': 279936, 'counters/updates': 8748}
train stats after 279968 examples: {'rewards_train/chosen': '0.20134', 'rewards_train/rejected': '0.080651', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12069', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-165.92', 'loss/train': '0.65544', 'examples_per_second': '31.526', 'grad_norm': '28.125', 'counters/examples': 279968, 'counters/updates': 8749}
train stats after 280000 examples: {'rewards_train/chosen': '0.19437', 'rewards_train/rejected': '0.03944', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15493', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-133.08', 'loss/train': '0.63054', 'examples_per_second': '30.681', 'grad_norm': '23', 'counters/examples': 280000, 'counters/updates': 8750}
train stats after 280032 examples: {'rewards_train/chosen': '0.14308', 'rewards_train/rejected': '0.06184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081241', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-108.45', 'loss/train': '0.66512', 'examples_per_second': '31.158', 'grad_norm': '25', 'counters/examples': 280032, 'counters/updates': 8751}
train stats after 280064 examples: {'rewards_train/chosen': '0.061634', 'rewards_train/rejected': '0.054559', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0070754', 'logps_train/rejected': '-135.05', 'logps_train/chosen': '-135.54', 'loss/train': '0.70481', 'examples_per_second': '32.284', 'grad_norm': '26.625', 'counters/examples': 280064, 'counters/updates': 8752}
train stats after 280096 examples: {'rewards_train/chosen': '0.12462', 'rewards_train/rejected': '0.034983', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089642', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-118.49', 'loss/train': '0.67219', 'examples_per_second': '31.551', 'grad_norm': '37.25', 'counters/examples': 280096, 'counters/updates': 8753}
train stats after 280128 examples: {'rewards_train/chosen': '0.1412', 'rewards_train/rejected': '0.011085', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13012', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-108.01', 'loss/train': '0.64056', 'examples_per_second': '31.557', 'grad_norm': '28.625', 'counters/examples': 280128, 'counters/updates': 8754}
train stats after 280160 examples: {'rewards_train/chosen': '0.095315', 'rewards_train/rejected': '-0.023853', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11917', 'logps_train/rejected': '-91.195', 'logps_train/chosen': '-123.61', 'loss/train': '0.64832', 'examples_per_second': '32.519', 'grad_norm': '21.75', 'counters/examples': 280160, 'counters/updates': 8755}
train stats after 280192 examples: {'rewards_train/chosen': '0.17248', 'rewards_train/rejected': '0.010255', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16223', 'logps_train/rejected': '-143.89', 'logps_train/chosen': '-159.01', 'loss/train': '0.62748', 'examples_per_second': '30.053', 'grad_norm': '44.75', 'counters/examples': 280192, 'counters/updates': 8756}
train stats after 280224 examples: {'rewards_train/chosen': '0.063543', 'rewards_train/rejected': '0.03334', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.030202', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-123.48', 'loss/train': '0.69955', 'examples_per_second': '24.694', 'grad_norm': '26.5', 'counters/examples': 280224, 'counters/updates': 8757}
train stats after 280256 examples: {'rewards_train/chosen': '0.12679', 'rewards_train/rejected': '0.0059464', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12084', 'logps_train/rejected': '-130.4', 'logps_train/chosen': '-159.17', 'loss/train': '0.65183', 'examples_per_second': '32.25', 'grad_norm': '25.125', 'counters/examples': 280256, 'counters/updates': 8758}
train stats after 280288 examples: {'rewards_train/chosen': '0.1482', 'rewards_train/rejected': '0.08411', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064091', 'logps_train/rejected': '-143.05', 'logps_train/chosen': '-189.5', 'loss/train': '0.67789', 'examples_per_second': '30.711', 'grad_norm': '29.25', 'counters/examples': 280288, 'counters/updates': 8759}
train stats after 280320 examples: {'rewards_train/chosen': '0.17791', 'rewards_train/rejected': '0.048763', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12914', 'logps_train/rejected': '-164.9', 'logps_train/chosen': '-175.15', 'loss/train': '0.64362', 'examples_per_second': '30.83', 'grad_norm': '28.75', 'counters/examples': 280320, 'counters/updates': 8760}
train stats after 280352 examples: {'rewards_train/chosen': '0.14113', 'rewards_train/rejected': '0.08901', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052123', 'logps_train/rejected': '-145.68', 'logps_train/chosen': '-146.27', 'loss/train': '0.68211', 'examples_per_second': '30.706', 'grad_norm': '29.125', 'counters/examples': 280352, 'counters/updates': 8761}
train stats after 280384 examples: {'rewards_train/chosen': '0.15874', 'rewards_train/rejected': '-0.035233', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19398', 'logps_train/rejected': '-99.981', 'logps_train/chosen': '-127.12', 'loss/train': '0.61851', 'examples_per_second': '30.491', 'grad_norm': '24.125', 'counters/examples': 280384, 'counters/updates': 8762}
train stats after 280416 examples: {'rewards_train/chosen': '0.13523', 'rewards_train/rejected': '0.10716', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02807', 'logps_train/rejected': '-160.34', 'logps_train/chosen': '-155', 'loss/train': '0.69874', 'examples_per_second': '32.355', 'grad_norm': '30.5', 'counters/examples': 280416, 'counters/updates': 8763}
skipping logging after 280448 examples to avoid logging too frequently
train stats after 280480 examples: {'rewards_train/chosen': '0.17202', 'rewards_train/rejected': '0.033772', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13825', 'logps_train/rejected': '-102.49', 'logps_train/chosen': '-111.52', 'loss/train': '0.64511', 'examples_per_second': '36.237', 'grad_norm': '22.375', 'counters/examples': 280480, 'counters/updates': 8765}
skipping logging after 280512 examples to avoid logging too frequently
train stats after 280544 examples: {'rewards_train/chosen': '0.2028', 'rewards_train/rejected': '-0.023984', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22679', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-166.43', 'loss/train': '0.60867', 'examples_per_second': '32.391', 'grad_norm': '25.5', 'counters/examples': 280544, 'counters/updates': 8767}
train stats after 280576 examples: {'rewards_train/chosen': '0.21812', 'rewards_train/rejected': '0.1135', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10461', 'logps_train/rejected': '-144.38', 'logps_train/chosen': '-150.06', 'loss/train': '0.66085', 'examples_per_second': '29.956', 'grad_norm': '32.5', 'counters/examples': 280576, 'counters/updates': 8768}
train stats after 280608 examples: {'rewards_train/chosen': '0.19143', 'rewards_train/rejected': '0.021738', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16969', 'logps_train/rejected': '-142', 'logps_train/chosen': '-151.97', 'loss/train': '0.63249', 'examples_per_second': '31.532', 'grad_norm': '27.5', 'counters/examples': 280608, 'counters/updates': 8769}
train stats after 280640 examples: {'rewards_train/chosen': '0.10569', 'rewards_train/rejected': '-0.019436', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12513', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-179.21', 'loss/train': '0.64355', 'examples_per_second': '30.652', 'grad_norm': '28.125', 'counters/examples': 280640, 'counters/updates': 8770}
train stats after 280672 examples: {'rewards_train/chosen': '0.086549', 'rewards_train/rejected': '-0.020921', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10747', 'logps_train/rejected': '-153.77', 'logps_train/chosen': '-170.6', 'loss/train': '0.66236', 'examples_per_second': '29.842', 'grad_norm': '41.25', 'counters/examples': 280672, 'counters/updates': 8771}
train stats after 280704 examples: {'rewards_train/chosen': '0.17368', 'rewards_train/rejected': '0.0012637', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17242', 'logps_train/rejected': '-148.63', 'logps_train/chosen': '-142.72', 'loss/train': '0.63125', 'examples_per_second': '32.811', 'grad_norm': '25.75', 'counters/examples': 280704, 'counters/updates': 8772}
train stats after 280736 examples: {'rewards_train/chosen': '0.23719', 'rewards_train/rejected': '0.049502', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18769', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-148.53', 'loss/train': '0.61821', 'examples_per_second': '32.446', 'grad_norm': '25.75', 'counters/examples': 280736, 'counters/updates': 8773}
train stats after 280768 examples: {'rewards_train/chosen': '0.10587', 'rewards_train/rejected': '-0.00089263', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10676', 'logps_train/rejected': '-100.42', 'logps_train/chosen': '-122.18', 'loss/train': '0.65864', 'examples_per_second': '32.878', 'grad_norm': '23.125', 'counters/examples': 280768, 'counters/updates': 8774}
skipping logging after 280800 examples to avoid logging too frequently
train stats after 280832 examples: {'rewards_train/chosen': '0.085704', 'rewards_train/rejected': '-0.0063252', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092029', 'logps_train/rejected': '-94.476', 'logps_train/chosen': '-116.57', 'loss/train': '0.65919', 'examples_per_second': '31.085', 'grad_norm': '21.5', 'counters/examples': 280832, 'counters/updates': 8776}
train stats after 280864 examples: {'rewards_train/chosen': '0.12951', 'rewards_train/rejected': '0.10783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021678', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-96.504', 'loss/train': '0.70389', 'examples_per_second': '32.068', 'grad_norm': '26.125', 'counters/examples': 280864, 'counters/updates': 8777}
train stats after 280896 examples: {'rewards_train/chosen': '0.11785', 'rewards_train/rejected': '-0.034525', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15238', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-160.39', 'loss/train': '0.63503', 'examples_per_second': '31.508', 'grad_norm': '25.75', 'counters/examples': 280896, 'counters/updates': 8778}
train stats after 280928 examples: {'rewards_train/chosen': '0.09972', 'rewards_train/rejected': '0.024772', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074948', 'logps_train/rejected': '-102.48', 'logps_train/chosen': '-106.41', 'loss/train': '0.66358', 'examples_per_second': '30.421', 'grad_norm': '22.75', 'counters/examples': 280928, 'counters/updates': 8779}
skipping logging after 280960 examples to avoid logging too frequently
train stats after 280992 examples: {'rewards_train/chosen': '0.10924', 'rewards_train/rejected': '-0.061769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17101', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-135.86', 'loss/train': '0.63038', 'examples_per_second': '30.191', 'grad_norm': '31.5', 'counters/examples': 280992, 'counters/updates': 8781}
train stats after 281024 examples: {'rewards_train/chosen': '0.14375', 'rewards_train/rejected': '0.055021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088724', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-124.62', 'loss/train': '0.66676', 'examples_per_second': '30.561', 'grad_norm': '27.75', 'counters/examples': 281024, 'counters/updates': 8782}
train stats after 281056 examples: {'rewards_train/chosen': '0.16588', 'rewards_train/rejected': '0.056408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10947', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-152.2', 'loss/train': '0.64804', 'examples_per_second': '32.718', 'grad_norm': '26.5', 'counters/examples': 281056, 'counters/updates': 8783}
train stats after 281088 examples: {'rewards_train/chosen': '0.15675', 'rewards_train/rejected': '0.044097', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11265', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-182.76', 'loss/train': '0.65146', 'examples_per_second': '31.482', 'grad_norm': '27.125', 'counters/examples': 281088, 'counters/updates': 8784}
train stats after 281120 examples: {'rewards_train/chosen': '0.1588', 'rewards_train/rejected': '-0.00056231', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15936', 'logps_train/rejected': '-117.34', 'logps_train/chosen': '-148.25', 'loss/train': '0.62893', 'examples_per_second': '31.106', 'grad_norm': '23.25', 'counters/examples': 281120, 'counters/updates': 8785}
train stats after 281152 examples: {'rewards_train/chosen': '0.1734', 'rewards_train/rejected': '0.098375', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075025', 'logps_train/rejected': '-119.99', 'logps_train/chosen': '-124.23', 'loss/train': '0.67576', 'examples_per_second': '30.766', 'grad_norm': '27.625', 'counters/examples': 281152, 'counters/updates': 8786}
train stats after 281184 examples: {'rewards_train/chosen': '0.082506', 'rewards_train/rejected': '0.043417', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039089', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-160.37', 'loss/train': '0.69341', 'examples_per_second': '30.689', 'grad_norm': '30.875', 'counters/examples': 281184, 'counters/updates': 8787}
train stats after 281216 examples: {'rewards_train/chosen': '0.21239', 'rewards_train/rejected': '-0.023158', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23555', 'logps_train/rejected': '-133.75', 'logps_train/chosen': '-137.57', 'loss/train': '0.59989', 'examples_per_second': '30.063', 'grad_norm': '25.625', 'counters/examples': 281216, 'counters/updates': 8788}
skipping logging after 281248 examples to avoid logging too frequently
train stats after 281280 examples: {'rewards_train/chosen': '0.15492', 'rewards_train/rejected': '0.0067917', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14813', 'logps_train/rejected': '-123', 'logps_train/chosen': '-148.57', 'loss/train': '0.64707', 'examples_per_second': '33.464', 'grad_norm': '32.75', 'counters/examples': 281280, 'counters/updates': 8790}
train stats after 281312 examples: {'rewards_train/chosen': '0.15772', 'rewards_train/rejected': '0.055633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10208', 'logps_train/rejected': '-156.6', 'logps_train/chosen': '-163.12', 'loss/train': '0.65376', 'examples_per_second': '32.748', 'grad_norm': '28.25', 'counters/examples': 281312, 'counters/updates': 8791}
train stats after 281344 examples: {'rewards_train/chosen': '0.15221', 'rewards_train/rejected': '0.048997', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10322', 'logps_train/rejected': '-131.01', 'logps_train/chosen': '-153.17', 'loss/train': '0.65673', 'examples_per_second': '30.008', 'grad_norm': '28.125', 'counters/examples': 281344, 'counters/updates': 8792}
train stats after 281376 examples: {'rewards_train/chosen': '0.24668', 'rewards_train/rejected': '0.073634', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17305', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-178.94', 'loss/train': '0.63246', 'examples_per_second': '30.005', 'grad_norm': '30.25', 'counters/examples': 281376, 'counters/updates': 8793}
skipping logging after 281408 examples to avoid logging too frequently
train stats after 281440 examples: {'rewards_train/chosen': '0.114', 'rewards_train/rejected': '-0.050791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16479', 'logps_train/rejected': '-120.36', 'logps_train/chosen': '-133.89', 'loss/train': '0.63873', 'examples_per_second': '35.833', 'grad_norm': '22.5', 'counters/examples': 281440, 'counters/updates': 8795}
train stats after 281472 examples: {'rewards_train/chosen': '0.17092', 'rewards_train/rejected': '0.014327', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1566', 'logps_train/rejected': '-170.47', 'logps_train/chosen': '-151.85', 'loss/train': '0.63957', 'examples_per_second': '30.523', 'grad_norm': '29.75', 'counters/examples': 281472, 'counters/updates': 8796}
train stats after 281504 examples: {'rewards_train/chosen': '0.13871', 'rewards_train/rejected': '-0.073835', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21254', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-141.11', 'loss/train': '0.61461', 'examples_per_second': '31.489', 'grad_norm': '23.5', 'counters/examples': 281504, 'counters/updates': 8797}
skipping logging after 281536 examples to avoid logging too frequently
train stats after 281568 examples: {'rewards_train/chosen': '0.15193', 'rewards_train/rejected': '0.13316', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018768', 'logps_train/rejected': '-128.67', 'logps_train/chosen': '-159.1', 'loss/train': '0.69799', 'examples_per_second': '32.557', 'grad_norm': '25.5', 'counters/examples': 281568, 'counters/updates': 8799}
train stats after 281600 examples: {'rewards_train/chosen': '0.13127', 'rewards_train/rejected': '-0.0026366', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13391', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-139.01', 'loss/train': '0.6468', 'examples_per_second': '31.889', 'grad_norm': '36', 'counters/examples': 281600, 'counters/updates': 8800}
train stats after 281632 examples: {'rewards_train/chosen': '0.12371', 'rewards_train/rejected': '-0.025839', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14955', 'logps_train/rejected': '-163.47', 'logps_train/chosen': '-205.19', 'loss/train': '0.64122', 'examples_per_second': '32.246', 'grad_norm': '31.25', 'counters/examples': 281632, 'counters/updates': 8801}
skipping logging after 281664 examples to avoid logging too frequently
train stats after 281696 examples: {'rewards_train/chosen': '0.16964', 'rewards_train/rejected': '0.030917', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13872', 'logps_train/rejected': '-113.16', 'logps_train/chosen': '-147.45', 'loss/train': '0.63961', 'examples_per_second': '31.541', 'grad_norm': '22.625', 'counters/examples': 281696, 'counters/updates': 8803}
train stats after 281728 examples: {'rewards_train/chosen': '0.22287', 'rewards_train/rejected': '0.10431', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11856', 'logps_train/rejected': '-120.08', 'logps_train/chosen': '-144.89', 'loss/train': '0.65566', 'examples_per_second': '30.122', 'grad_norm': '29.375', 'counters/examples': 281728, 'counters/updates': 8804}
train stats after 281760 examples: {'rewards_train/chosen': '0.12696', 'rewards_train/rejected': '0.020968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10599', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-111.07', 'loss/train': '0.66525', 'examples_per_second': '30.449', 'grad_norm': '25.875', 'counters/examples': 281760, 'counters/updates': 8805}
train stats after 281792 examples: {'rewards_train/chosen': '0.0083553', 'rewards_train/rejected': '-0.056304', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06466', 'logps_train/rejected': '-118', 'logps_train/chosen': '-150.48', 'loss/train': '0.68424', 'examples_per_second': '31.541', 'grad_norm': '30.125', 'counters/examples': 281792, 'counters/updates': 8806}
train stats after 281824 examples: {'rewards_train/chosen': '0.1139', 'rewards_train/rejected': '0.10864', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0052609', 'logps_train/rejected': '-115.46', 'logps_train/chosen': '-149.45', 'loss/train': '0.70371', 'examples_per_second': '31.869', 'grad_norm': '26.5', 'counters/examples': 281824, 'counters/updates': 8807}
train stats after 281856 examples: {'rewards_train/chosen': '0.16427', 'rewards_train/rejected': '0.10217', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062108', 'logps_train/rejected': '-143.86', 'logps_train/chosen': '-160.05', 'loss/train': '0.67954', 'examples_per_second': '31.466', 'grad_norm': '28.625', 'counters/examples': 281856, 'counters/updates': 8808}
train stats after 281888 examples: {'rewards_train/chosen': '0.099419', 'rewards_train/rejected': '-0.0072001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10662', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-140.52', 'loss/train': '0.66219', 'examples_per_second': '32.018', 'grad_norm': '28.125', 'counters/examples': 281888, 'counters/updates': 8809}
train stats after 281920 examples: {'rewards_train/chosen': '0.17295', 'rewards_train/rejected': '0.093518', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079433', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-157.35', 'loss/train': '0.67055', 'examples_per_second': '31.524', 'grad_norm': '26.75', 'counters/examples': 281920, 'counters/updates': 8810}
train stats after 281952 examples: {'rewards_train/chosen': '0.16082', 'rewards_train/rejected': '0.0037586', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15707', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-111.22', 'loss/train': '0.63242', 'examples_per_second': '32.481', 'grad_norm': '21.625', 'counters/examples': 281952, 'counters/updates': 8811}
train stats after 281984 examples: {'rewards_train/chosen': '0.11076', 'rewards_train/rejected': '0.081694', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029069', 'logps_train/rejected': '-107.24', 'logps_train/chosen': '-133.09', 'loss/train': '0.69752', 'examples_per_second': '30.363', 'grad_norm': '34.5', 'counters/examples': 281984, 'counters/updates': 8812}
skipping logging after 282016 examples to avoid logging too frequently
train stats after 282048 examples: {'rewards_train/chosen': '0.24203', 'rewards_train/rejected': '0.053737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1883', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-142.66', 'loss/train': '0.62952', 'examples_per_second': '30.228', 'grad_norm': '25.375', 'counters/examples': 282048, 'counters/updates': 8814}
train stats after 282080 examples: {'rewards_train/chosen': '0.20016', 'rewards_train/rejected': '0.014247', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18591', 'logps_train/rejected': '-117.71', 'logps_train/chosen': '-129.65', 'loss/train': '0.61989', 'examples_per_second': '31.541', 'grad_norm': '25.375', 'counters/examples': 282080, 'counters/updates': 8815}
train stats after 282112 examples: {'rewards_train/chosen': '0.21267', 'rewards_train/rejected': '-0.055483', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.26815', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-148.14', 'loss/train': '0.58173', 'examples_per_second': '30.512', 'grad_norm': '31.375', 'counters/examples': 282112, 'counters/updates': 8816}
train stats after 282144 examples: {'rewards_train/chosen': '0.22623', 'rewards_train/rejected': '-0.034022', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26025', 'logps_train/rejected': '-113.15', 'logps_train/chosen': '-162.51', 'loss/train': '0.59422', 'examples_per_second': '31.564', 'grad_norm': '22.125', 'counters/examples': 282144, 'counters/updates': 8817}
train stats after 282176 examples: {'rewards_train/chosen': '0.23889', 'rewards_train/rejected': '0.063191', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1757', 'logps_train/rejected': '-128.46', 'logps_train/chosen': '-145.83', 'loss/train': '0.62005', 'examples_per_second': '31.54', 'grad_norm': '26.25', 'counters/examples': 282176, 'counters/updates': 8818}
train stats after 282208 examples: {'rewards_train/chosen': '0.10332', 'rewards_train/rejected': '-0.041766', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14509', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-100.55', 'loss/train': '0.64022', 'examples_per_second': '31.289', 'grad_norm': '24.75', 'counters/examples': 282208, 'counters/updates': 8819}
skipping logging after 282240 examples to avoid logging too frequently
train stats after 282272 examples: {'rewards_train/chosen': '0.10141', 'rewards_train/rejected': '0.008266', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093141', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-137.37', 'loss/train': '0.65975', 'examples_per_second': '31.583', 'grad_norm': '26.625', 'counters/examples': 282272, 'counters/updates': 8821}
skipping logging after 282304 examples to avoid logging too frequently
train stats after 282336 examples: {'rewards_train/chosen': '0.23097', 'rewards_train/rejected': '0.051552', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17942', 'logps_train/rejected': '-122.17', 'logps_train/chosen': '-145.99', 'loss/train': '0.6267', 'examples_per_second': '31.726', 'grad_norm': '25.125', 'counters/examples': 282336, 'counters/updates': 8823}
skipping logging after 282368 examples to avoid logging too frequently
train stats after 282400 examples: {'rewards_train/chosen': '0.13864', 'rewards_train/rejected': '0.0093822', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12926', 'logps_train/rejected': '-119.05', 'logps_train/chosen': '-116.84', 'loss/train': '0.64348', 'examples_per_second': '30.539', 'grad_norm': '25.625', 'counters/examples': 282400, 'counters/updates': 8825}
train stats after 282432 examples: {'rewards_train/chosen': '0.16683', 'rewards_train/rejected': '0.040983', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12585', 'logps_train/rejected': '-126.79', 'logps_train/chosen': '-150.63', 'loss/train': '0.65553', 'examples_per_second': '32.438', 'grad_norm': '27', 'counters/examples': 282432, 'counters/updates': 8826}
train stats after 282464 examples: {'rewards_train/chosen': '0.18886', 'rewards_train/rejected': '-0.032113', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22097', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-142.78', 'loss/train': '0.60843', 'examples_per_second': '30.852', 'grad_norm': '23.125', 'counters/examples': 282464, 'counters/updates': 8827}
train stats after 282496 examples: {'rewards_train/chosen': '0.16913', 'rewards_train/rejected': '0.047261', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12187', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-166.3', 'loss/train': '0.6414', 'examples_per_second': '30.509', 'grad_norm': '28.625', 'counters/examples': 282496, 'counters/updates': 8828}
skipping logging after 282528 examples to avoid logging too frequently
train stats after 282560 examples: {'rewards_train/chosen': '0.16574', 'rewards_train/rejected': '0.0051027', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16064', 'logps_train/rejected': '-138.45', 'logps_train/chosen': '-171.47', 'loss/train': '0.63421', 'examples_per_second': '31.023', 'grad_norm': '27.625', 'counters/examples': 282560, 'counters/updates': 8830}
train stats after 282592 examples: {'rewards_train/chosen': '0.21226', 'rewards_train/rejected': '0.03317', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17909', 'logps_train/rejected': '-143.66', 'logps_train/chosen': '-172.57', 'loss/train': '0.64484', 'examples_per_second': '30.359', 'grad_norm': '27.875', 'counters/examples': 282592, 'counters/updates': 8831}
train stats after 282624 examples: {'rewards_train/chosen': '0.24162', 'rewards_train/rejected': '0.19668', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044941', 'logps_train/rejected': '-169.32', 'logps_train/chosen': '-146.93', 'loss/train': '0.68995', 'examples_per_second': '30.375', 'grad_norm': '31', 'counters/examples': 282624, 'counters/updates': 8832}
train stats after 282656 examples: {'rewards_train/chosen': '0.072935', 'rewards_train/rejected': '-0.068326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14126', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-161.43', 'loss/train': '0.63956', 'examples_per_second': '31.59', 'grad_norm': '27.5', 'counters/examples': 282656, 'counters/updates': 8833}
train stats after 282688 examples: {'rewards_train/chosen': '0.13522', 'rewards_train/rejected': '-0.081822', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21704', 'logps_train/rejected': '-107.6', 'logps_train/chosen': '-126.11', 'loss/train': '0.61593', 'examples_per_second': '30.107', 'grad_norm': '23.875', 'counters/examples': 282688, 'counters/updates': 8834}
skipping logging after 282720 examples to avoid logging too frequently
train stats after 282752 examples: {'rewards_train/chosen': '0.1782', 'rewards_train/rejected': '0.060186', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11802', 'logps_train/rejected': '-164.69', 'logps_train/chosen': '-160.25', 'loss/train': '0.65746', 'examples_per_second': '30.467', 'grad_norm': '59.75', 'counters/examples': 282752, 'counters/updates': 8836}
skipping logging after 282784 examples to avoid logging too frequently
train stats after 282816 examples: {'rewards_train/chosen': '0.11152', 'rewards_train/rejected': '-0.019677', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1312', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-160.11', 'loss/train': '0.64227', 'examples_per_second': '31.771', 'grad_norm': '25.5', 'counters/examples': 282816, 'counters/updates': 8838}
train stats after 282848 examples: {'rewards_train/chosen': '0.21616', 'rewards_train/rejected': '0.077363', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1388', 'logps_train/rejected': '-123.42', 'logps_train/chosen': '-150.98', 'loss/train': '0.6366', 'examples_per_second': '32.328', 'grad_norm': '23.75', 'counters/examples': 282848, 'counters/updates': 8839}
train stats after 282880 examples: {'rewards_train/chosen': '0.10295', 'rewards_train/rejected': '-0.02253', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12548', 'logps_train/rejected': '-132.54', 'logps_train/chosen': '-184.24', 'loss/train': '0.64756', 'examples_per_second': '31.546', 'grad_norm': '28.625', 'counters/examples': 282880, 'counters/updates': 8840}
train stats after 282912 examples: {'rewards_train/chosen': '0.10955', 'rewards_train/rejected': '0.023029', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086525', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-160.89', 'loss/train': '0.68071', 'examples_per_second': '30.963', 'grad_norm': '26.625', 'counters/examples': 282912, 'counters/updates': 8841}
train stats after 282944 examples: {'rewards_train/chosen': '0.1566', 'rewards_train/rejected': '0.018865', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13774', 'logps_train/rejected': '-110.91', 'logps_train/chosen': '-125.33', 'loss/train': '0.63402', 'examples_per_second': '30.05', 'grad_norm': '23', 'counters/examples': 282944, 'counters/updates': 8842}
train stats after 282976 examples: {'rewards_train/chosen': '0.23453', 'rewards_train/rejected': '0.022717', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21181', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-145.14', 'loss/train': '0.60881', 'examples_per_second': '32.481', 'grad_norm': '24', 'counters/examples': 282976, 'counters/updates': 8843}
train stats after 283008 examples: {'rewards_train/chosen': '0.12955', 'rewards_train/rejected': '-0.029951', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1595', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-134.6', 'loss/train': '0.63131', 'examples_per_second': '32.537', 'grad_norm': '26.25', 'counters/examples': 283008, 'counters/updates': 8844}
train stats after 283040 examples: {'rewards_train/chosen': '0.082783', 'rewards_train/rejected': '-0.033005', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11579', 'logps_train/rejected': '-112.49', 'logps_train/chosen': '-128.65', 'loss/train': '0.64499', 'examples_per_second': '32.457', 'grad_norm': '23.125', 'counters/examples': 283040, 'counters/updates': 8845}
train stats after 283072 examples: {'rewards_train/chosen': '0.085967', 'rewards_train/rejected': '-0.021266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10723', 'logps_train/rejected': '-80.841', 'logps_train/chosen': '-98.354', 'loss/train': '0.64936', 'examples_per_second': '30.981', 'grad_norm': '19.25', 'counters/examples': 283072, 'counters/updates': 8846}
train stats after 283104 examples: {'rewards_train/chosen': '0.19319', 'rewards_train/rejected': '0.093219', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099966', 'logps_train/rejected': '-154.02', 'logps_train/chosen': '-133.79', 'loss/train': '0.66429', 'examples_per_second': '31.54', 'grad_norm': '24.125', 'counters/examples': 283104, 'counters/updates': 8847}
train stats after 283136 examples: {'rewards_train/chosen': '0.10218', 'rewards_train/rejected': '-0.0078405', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11002', 'logps_train/rejected': '-109.17', 'logps_train/chosen': '-150.45', 'loss/train': '0.65418', 'examples_per_second': '30.544', 'grad_norm': '24.75', 'counters/examples': 283136, 'counters/updates': 8848}
train stats after 283168 examples: {'rewards_train/chosen': '0.17746', 'rewards_train/rejected': '0.058805', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11866', 'logps_train/rejected': '-136.84', 'logps_train/chosen': '-137.88', 'loss/train': '0.6531', 'examples_per_second': '31.887', 'grad_norm': '28.5', 'counters/examples': 283168, 'counters/updates': 8849}
train stats after 283200 examples: {'rewards_train/chosen': '0.081993', 'rewards_train/rejected': '-0.010788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092781', 'logps_train/rejected': '-99.533', 'logps_train/chosen': '-154.79', 'loss/train': '0.66463', 'examples_per_second': '31.497', 'grad_norm': '23.5', 'counters/examples': 283200, 'counters/updates': 8850}
train stats after 283232 examples: {'rewards_train/chosen': '0.12946', 'rewards_train/rejected': '0.019767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10969', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-153.48', 'loss/train': '0.65103', 'examples_per_second': '32.063', 'grad_norm': '24.625', 'counters/examples': 283232, 'counters/updates': 8851}
train stats after 283264 examples: {'rewards_train/chosen': '0.15038', 'rewards_train/rejected': '-0.02663', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17701', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-151.93', 'loss/train': '0.6334', 'examples_per_second': '31.223', 'grad_norm': '36.75', 'counters/examples': 283264, 'counters/updates': 8852}
train stats after 283296 examples: {'rewards_train/chosen': '0.1326', 'rewards_train/rejected': '0.024505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10809', 'logps_train/rejected': '-113.45', 'logps_train/chosen': '-141.29', 'loss/train': '0.66364', 'examples_per_second': '30.063', 'grad_norm': '28.375', 'counters/examples': 283296, 'counters/updates': 8853}
train stats after 283328 examples: {'rewards_train/chosen': '0.2254', 'rewards_train/rejected': '-0.006159', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23156', 'logps_train/rejected': '-101.08', 'logps_train/chosen': '-122.1', 'loss/train': '0.59888', 'examples_per_second': '30.887', 'grad_norm': '21.5', 'counters/examples': 283328, 'counters/updates': 8854}
train stats after 283360 examples: {'rewards_train/chosen': '0.083256', 'rewards_train/rejected': '0.083949', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.00069335', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-136.51', 'loss/train': '0.71498', 'examples_per_second': '30.427', 'grad_norm': '27.125', 'counters/examples': 283360, 'counters/updates': 8855}
skipping logging after 283392 examples to avoid logging too frequently
train stats after 283424 examples: {'rewards_train/chosen': '0.24168', 'rewards_train/rejected': '0.068271', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17341', 'logps_train/rejected': '-134.55', 'logps_train/chosen': '-141.17', 'loss/train': '0.62743', 'examples_per_second': '30.857', 'grad_norm': '25', 'counters/examples': 283424, 'counters/updates': 8857}
skipping logging after 283456 examples to avoid logging too frequently
train stats after 283488 examples: {'rewards_train/chosen': '0.14837', 'rewards_train/rejected': '-0.087378', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23575', 'logps_train/rejected': '-85.51', 'logps_train/chosen': '-121.22', 'loss/train': '0.59976', 'examples_per_second': '30.064', 'grad_norm': '19.75', 'counters/examples': 283488, 'counters/updates': 8859}
train stats after 283520 examples: {'rewards_train/chosen': '0.11756', 'rewards_train/rejected': '-0.0072431', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12481', 'logps_train/rejected': '-157.15', 'logps_train/chosen': '-173.09', 'loss/train': '0.64316', 'examples_per_second': '31.354', 'grad_norm': '29.875', 'counters/examples': 283520, 'counters/updates': 8860}
train stats after 283552 examples: {'rewards_train/chosen': '0.21663', 'rewards_train/rejected': '0.065614', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15101', 'logps_train/rejected': '-140.68', 'logps_train/chosen': '-136.39', 'loss/train': '0.64192', 'examples_per_second': '31.538', 'grad_norm': '24.375', 'counters/examples': 283552, 'counters/updates': 8861}
train stats after 283584 examples: {'rewards_train/chosen': '0.10135', 'rewards_train/rejected': '0.034968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066379', 'logps_train/rejected': '-137.69', 'logps_train/chosen': '-149.62', 'loss/train': '0.67012', 'examples_per_second': '32.42', 'grad_norm': '34.5', 'counters/examples': 283584, 'counters/updates': 8862}
train stats after 283616 examples: {'rewards_train/chosen': '0.22063', 'rewards_train/rejected': '0.043827', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17681', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-140.74', 'loss/train': '0.62346', 'examples_per_second': '32.258', 'grad_norm': '23.125', 'counters/examples': 283616, 'counters/updates': 8863}
train stats after 283648 examples: {'rewards_train/chosen': '0.087591', 'rewards_train/rejected': '0.010028', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077562', 'logps_train/rejected': '-92.384', 'logps_train/chosen': '-135.74', 'loss/train': '0.66958', 'examples_per_second': '32.601', 'grad_norm': '24.625', 'counters/examples': 283648, 'counters/updates': 8864}
train stats after 283680 examples: {'rewards_train/chosen': '0.10794', 'rewards_train/rejected': '0.087601', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020335', 'logps_train/rejected': '-110.35', 'logps_train/chosen': '-123.97', 'loss/train': '0.69706', 'examples_per_second': '30.167', 'grad_norm': '26.375', 'counters/examples': 283680, 'counters/updates': 8865}
train stats after 283712 examples: {'rewards_train/chosen': '0.11266', 'rewards_train/rejected': '0.027147', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.08551', 'logps_train/rejected': '-107.7', 'logps_train/chosen': '-138.18', 'loss/train': '0.67285', 'examples_per_second': '31.391', 'grad_norm': '23.75', 'counters/examples': 283712, 'counters/updates': 8866}
skipping logging after 283744 examples to avoid logging too frequently
train stats after 283776 examples: {'rewards_train/chosen': '0.045914', 'rewards_train/rejected': '0.033957', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011957', 'logps_train/rejected': '-104.89', 'logps_train/chosen': '-110.09', 'loss/train': '0.70761', 'examples_per_second': '31.207', 'grad_norm': '25.875', 'counters/examples': 283776, 'counters/updates': 8868}
train stats after 283808 examples: {'rewards_train/chosen': '0.27307', 'rewards_train/rejected': '0.090507', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18257', 'logps_train/rejected': '-130.16', 'logps_train/chosen': '-140.27', 'loss/train': '0.62365', 'examples_per_second': '31.231', 'grad_norm': '25.25', 'counters/examples': 283808, 'counters/updates': 8869}
train stats after 283840 examples: {'rewards_train/chosen': '0.33589', 'rewards_train/rejected': '-0.033101', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.36899', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-151.86', 'loss/train': '0.55035', 'examples_per_second': '30.241', 'grad_norm': '22.625', 'counters/examples': 283840, 'counters/updates': 8870}
train stats after 283872 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '-0.017979', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14099', 'logps_train/rejected': '-80.808', 'logps_train/chosen': '-117.55', 'loss/train': '0.64113', 'examples_per_second': '31.569', 'grad_norm': '27.5', 'counters/examples': 283872, 'counters/updates': 8871}
train stats after 283904 examples: {'rewards_train/chosen': '0.19678', 'rewards_train/rejected': '0.062986', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13379', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-157.44', 'loss/train': '0.64781', 'examples_per_second': '31.496', 'grad_norm': '26.625', 'counters/examples': 283904, 'counters/updates': 8872}
skipping logging after 283936 examples to avoid logging too frequently
train stats after 283968 examples: {'rewards_train/chosen': '0.06752', 'rewards_train/rejected': '-0.042939', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11046', 'logps_train/rejected': '-104.27', 'logps_train/chosen': '-154.14', 'loss/train': '0.67588', 'examples_per_second': '30.81', 'grad_norm': '26.25', 'counters/examples': 283968, 'counters/updates': 8874}
train stats after 284000 examples: {'rewards_train/chosen': '0.14261', 'rewards_train/rejected': '-0.044774', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18738', 'logps_train/rejected': '-104.83', 'logps_train/chosen': '-134.86', 'loss/train': '0.62394', 'examples_per_second': '31.556', 'grad_norm': '22.5', 'counters/examples': 284000, 'counters/updates': 8875}
skipping logging after 284032 examples to avoid logging too frequently
train stats after 284064 examples: {'rewards_train/chosen': '0.066787', 'rewards_train/rejected': '0.047861', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018927', 'logps_train/rejected': '-129.31', 'logps_train/chosen': '-134.53', 'loss/train': '0.70683', 'examples_per_second': '31.406', 'grad_norm': '25.75', 'counters/examples': 284064, 'counters/updates': 8877}
train stats after 284096 examples: {'rewards_train/chosen': '0.1983', 'rewards_train/rejected': '0.032434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16587', 'logps_train/rejected': '-156.64', 'logps_train/chosen': '-143.63', 'loss/train': '0.6297', 'examples_per_second': '32.229', 'grad_norm': '27.375', 'counters/examples': 284096, 'counters/updates': 8878}
train stats after 284128 examples: {'rewards_train/chosen': '0.088932', 'rewards_train/rejected': '0.042772', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04616', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-124.72', 'loss/train': '0.68947', 'examples_per_second': '31.701', 'grad_norm': '31.125', 'counters/examples': 284128, 'counters/updates': 8879}
train stats after 284160 examples: {'rewards_train/chosen': '0.25366', 'rewards_train/rejected': '0.064019', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18964', 'logps_train/rejected': '-130.02', 'logps_train/chosen': '-151.37', 'loss/train': '0.62044', 'examples_per_second': '31.694', 'grad_norm': '25', 'counters/examples': 284160, 'counters/updates': 8880}
train stats after 284192 examples: {'rewards_train/chosen': '0.12552', 'rewards_train/rejected': '-0.0064085', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13192', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-129.37', 'loss/train': '0.64526', 'examples_per_second': '31.057', 'grad_norm': '23.75', 'counters/examples': 284192, 'counters/updates': 8881}
train stats after 284224 examples: {'rewards_train/chosen': '0.070464', 'rewards_train/rejected': '0.052948', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017516', 'logps_train/rejected': '-162.48', 'logps_train/chosen': '-119.4', 'loss/train': '0.69261', 'examples_per_second': '31.57', 'grad_norm': '26.125', 'counters/examples': 284224, 'counters/updates': 8882}
train stats after 284256 examples: {'rewards_train/chosen': '0.12228', 'rewards_train/rejected': '-0.0191', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14138', 'logps_train/rejected': '-83.36', 'logps_train/chosen': '-107.09', 'loss/train': '0.6536', 'examples_per_second': '30.127', 'grad_norm': '39.25', 'counters/examples': 284256, 'counters/updates': 8883}
train stats after 284288 examples: {'rewards_train/chosen': '0.067251', 'rewards_train/rejected': '0.032979', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.034272', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-168.57', 'loss/train': '0.69126', 'examples_per_second': '30.499', 'grad_norm': '26.375', 'counters/examples': 284288, 'counters/updates': 8884}
train stats after 284320 examples: {'rewards_train/chosen': '0.19935', 'rewards_train/rejected': '0.076671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12268', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-121.37', 'loss/train': '0.64279', 'examples_per_second': '31.307', 'grad_norm': '23.5', 'counters/examples': 284320, 'counters/updates': 8885}
train stats after 284352 examples: {'rewards_train/chosen': '0.15689', 'rewards_train/rejected': '0.17324', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016351', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-126.72', 'loss/train': '0.71108', 'examples_per_second': '30.731', 'grad_norm': '26.625', 'counters/examples': 284352, 'counters/updates': 8886}
train stats after 284384 examples: {'rewards_train/chosen': '0.10837', 'rewards_train/rejected': '0.0038095', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10456', 'logps_train/rejected': '-106.59', 'logps_train/chosen': '-166.45', 'loss/train': '0.65553', 'examples_per_second': '30.646', 'grad_norm': '24.125', 'counters/examples': 284384, 'counters/updates': 8887}
train stats after 284416 examples: {'rewards_train/chosen': '0.15071', 'rewards_train/rejected': '0.03486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11585', 'logps_train/rejected': '-140.46', 'logps_train/chosen': '-116.36', 'loss/train': '0.65245', 'examples_per_second': '24.293', 'grad_norm': '28.625', 'counters/examples': 284416, 'counters/updates': 8888}
train stats after 284448 examples: {'rewards_train/chosen': '0.14929', 'rewards_train/rejected': '-0.002802', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15209', 'logps_train/rejected': '-163.88', 'logps_train/chosen': '-150.11', 'loss/train': '0.64907', 'examples_per_second': '31.575', 'grad_norm': '30.375', 'counters/examples': 284448, 'counters/updates': 8889}
skipping logging after 284480 examples to avoid logging too frequently
train stats after 284512 examples: {'rewards_train/chosen': '0.12565', 'rewards_train/rejected': '0.016183', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10946', 'logps_train/rejected': '-97.896', 'logps_train/chosen': '-101.27', 'loss/train': '0.65299', 'examples_per_second': '23.667', 'grad_norm': '22.625', 'counters/examples': 284512, 'counters/updates': 8891}
train stats after 284544 examples: {'rewards_train/chosen': '0.15258', 'rewards_train/rejected': '0.18538', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032797', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-131.41', 'loss/train': '0.73857', 'examples_per_second': '30.919', 'grad_norm': '30.5', 'counters/examples': 284544, 'counters/updates': 8892}
train stats after 284576 examples: {'rewards_train/chosen': '0.10474', 'rewards_train/rejected': '0.031943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072802', 'logps_train/rejected': '-91.576', 'logps_train/chosen': '-136.27', 'loss/train': '0.66676', 'examples_per_second': '31.165', 'grad_norm': '24.375', 'counters/examples': 284576, 'counters/updates': 8893}
train stats after 284608 examples: {'rewards_train/chosen': '0.17326', 'rewards_train/rejected': '0.034382', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13888', 'logps_train/rejected': '-148.01', 'logps_train/chosen': '-166.5', 'loss/train': '0.65229', 'examples_per_second': '31.545', 'grad_norm': '29.25', 'counters/examples': 284608, 'counters/updates': 8894}
train stats after 284640 examples: {'rewards_train/chosen': '0.20944', 'rewards_train/rejected': '0.032916', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17652', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-125.28', 'loss/train': '0.62629', 'examples_per_second': '31.575', 'grad_norm': '23.25', 'counters/examples': 284640, 'counters/updates': 8895}
skipping logging after 284672 examples to avoid logging too frequently
train stats after 284704 examples: {'rewards_train/chosen': '0.20303', 'rewards_train/rejected': '0.14633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056703', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-149.04', 'loss/train': '0.67617', 'examples_per_second': '31.563', 'grad_norm': '26.5', 'counters/examples': 284704, 'counters/updates': 8897}
skipping logging after 284736 examples to avoid logging too frequently
train stats after 284768 examples: {'rewards_train/chosen': '0.14294', 'rewards_train/rejected': '-0.002546', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14549', 'logps_train/rejected': '-109.03', 'logps_train/chosen': '-105.26', 'loss/train': '0.63905', 'examples_per_second': '31.462', 'grad_norm': '23.5', 'counters/examples': 284768, 'counters/updates': 8899}
train stats after 284800 examples: {'rewards_train/chosen': '0.14318', 'rewards_train/rejected': '0.042601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10058', 'logps_train/rejected': '-154.52', 'logps_train/chosen': '-128.64', 'loss/train': '0.66061', 'examples_per_second': '30.878', 'grad_norm': '27.875', 'counters/examples': 284800, 'counters/updates': 8900}
train stats after 284832 examples: {'rewards_train/chosen': '0.12725', 'rewards_train/rejected': '0.030664', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09659', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-146.15', 'loss/train': '0.6644', 'examples_per_second': '31.944', 'grad_norm': '29.25', 'counters/examples': 284832, 'counters/updates': 8901}
train stats after 284864 examples: {'rewards_train/chosen': '0.033336', 'rewards_train/rejected': '-0.023643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056979', 'logps_train/rejected': '-122.25', 'logps_train/chosen': '-142.88', 'loss/train': '0.68073', 'examples_per_second': '31.301', 'grad_norm': '24.875', 'counters/examples': 284864, 'counters/updates': 8902}
train stats after 284896 examples: {'rewards_train/chosen': '0.08007', 'rewards_train/rejected': '0.0023113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077759', 'logps_train/rejected': '-119.78', 'logps_train/chosen': '-123.13', 'loss/train': '0.67344', 'examples_per_second': '32.702', 'grad_norm': '24', 'counters/examples': 284896, 'counters/updates': 8903}
train stats after 284928 examples: {'rewards_train/chosen': '0.14172', 'rewards_train/rejected': '0.075605', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066114', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-136.51', 'loss/train': '0.67907', 'examples_per_second': '31.512', 'grad_norm': '25.125', 'counters/examples': 284928, 'counters/updates': 8904}
train stats after 284960 examples: {'rewards_train/chosen': '0.082048', 'rewards_train/rejected': '-0.024529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10658', 'logps_train/rejected': '-97.214', 'logps_train/chosen': '-113.54', 'loss/train': '0.65219', 'examples_per_second': '31.566', 'grad_norm': '25.875', 'counters/examples': 284960, 'counters/updates': 8905}
train stats after 284992 examples: {'rewards_train/chosen': '0.16533', 'rewards_train/rejected': '-0.0094717', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1748', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-139.23', 'loss/train': '0.62329', 'examples_per_second': '31.975', 'grad_norm': '23.25', 'counters/examples': 284992, 'counters/updates': 8906}
train stats after 285024 examples: {'rewards_train/chosen': '0.065573', 'rewards_train/rejected': '0.027961', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037611', 'logps_train/rejected': '-101.52', 'logps_train/chosen': '-125.67', 'loss/train': '0.68759', 'examples_per_second': '30.754', 'grad_norm': '23.25', 'counters/examples': 285024, 'counters/updates': 8907}
train stats after 285056 examples: {'rewards_train/chosen': '0.23253', 'rewards_train/rejected': '0.045959', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18658', 'logps_train/rejected': '-110.47', 'logps_train/chosen': '-165.24', 'loss/train': '0.62173', 'examples_per_second': '31.572', 'grad_norm': '25.375', 'counters/examples': 285056, 'counters/updates': 8908}
train stats after 285088 examples: {'rewards_train/chosen': '0.17779', 'rewards_train/rejected': '-0.052686', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23048', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-163.15', 'loss/train': '0.59964', 'examples_per_second': '31.575', 'grad_norm': '24.5', 'counters/examples': 285088, 'counters/updates': 8909}
train stats after 285120 examples: {'rewards_train/chosen': '0.15022', 'rewards_train/rejected': '-0.014438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16466', 'logps_train/rejected': '-98.863', 'logps_train/chosen': '-120.37', 'loss/train': '0.6387', 'examples_per_second': '29.886', 'grad_norm': '23.875', 'counters/examples': 285120, 'counters/updates': 8910}
train stats after 285152 examples: {'rewards_train/chosen': '0.13978', 'rewards_train/rejected': '-0.051426', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1912', 'logps_train/rejected': '-108.53', 'logps_train/chosen': '-141.41', 'loss/train': '0.611', 'examples_per_second': '30.35', 'grad_norm': '21.625', 'counters/examples': 285152, 'counters/updates': 8911}
train stats after 285184 examples: {'rewards_train/chosen': '0.090475', 'rewards_train/rejected': '-0.032842', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12332', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-134.52', 'loss/train': '0.66546', 'examples_per_second': '31.299', 'grad_norm': '28.875', 'counters/examples': 285184, 'counters/updates': 8912}
train stats after 285216 examples: {'rewards_train/chosen': '0.224', 'rewards_train/rejected': '0.033245', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19076', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-132.75', 'loss/train': '0.62327', 'examples_per_second': '31.599', 'grad_norm': '25.125', 'counters/examples': 285216, 'counters/updates': 8913}
train stats after 285248 examples: {'rewards_train/chosen': '0.11011', 'rewards_train/rejected': '0.085471', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024636', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-117.22', 'loss/train': '0.69025', 'examples_per_second': '30.235', 'grad_norm': '26', 'counters/examples': 285248, 'counters/updates': 8914}
skipping logging after 285280 examples to avoid logging too frequently
train stats after 285312 examples: {'rewards_train/chosen': '0.13916', 'rewards_train/rejected': '0.066342', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072818', 'logps_train/rejected': '-140.68', 'logps_train/chosen': '-155.7', 'loss/train': '0.66841', 'examples_per_second': '31.547', 'grad_norm': '26.375', 'counters/examples': 285312, 'counters/updates': 8916}
train stats after 285344 examples: {'rewards_train/chosen': '0.11681', 'rewards_train/rejected': '-0.046678', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16349', 'logps_train/rejected': '-112.11', 'logps_train/chosen': '-147.75', 'loss/train': '0.63235', 'examples_per_second': '33.035', 'grad_norm': '26.75', 'counters/examples': 285344, 'counters/updates': 8917}
train stats after 285376 examples: {'rewards_train/chosen': '0.14127', 'rewards_train/rejected': '0.075133', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066136', 'logps_train/rejected': '-144.16', 'logps_train/chosen': '-154.4', 'loss/train': '0.67199', 'examples_per_second': '31.5', 'grad_norm': '29', 'counters/examples': 285376, 'counters/updates': 8918}
train stats after 285408 examples: {'rewards_train/chosen': '0.18765', 'rewards_train/rejected': '-0.077344', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26499', 'logps_train/rejected': '-101.25', 'logps_train/chosen': '-131.2', 'loss/train': '0.58476', 'examples_per_second': '32.531', 'grad_norm': '21.25', 'counters/examples': 285408, 'counters/updates': 8919}
train stats after 285440 examples: {'rewards_train/chosen': '0.21756', 'rewards_train/rejected': '0.093032', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12453', 'logps_train/rejected': '-140.03', 'logps_train/chosen': '-162.76', 'loss/train': '0.65682', 'examples_per_second': '31.571', 'grad_norm': '28.5', 'counters/examples': 285440, 'counters/updates': 8920}
skipping logging after 285472 examples to avoid logging too frequently
train stats after 285504 examples: {'rewards_train/chosen': '0.067342', 'rewards_train/rejected': '0.076219', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0088774', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-106.27', 'loss/train': '0.70731', 'examples_per_second': '33.146', 'grad_norm': '24.75', 'counters/examples': 285504, 'counters/updates': 8922}
train stats after 285536 examples: {'rewards_train/chosen': '0.11741', 'rewards_train/rejected': '-0.027473', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14488', 'logps_train/rejected': '-155.1', 'logps_train/chosen': '-161.97', 'loss/train': '0.64098', 'examples_per_second': '30.359', 'grad_norm': '28.375', 'counters/examples': 285536, 'counters/updates': 8923}
train stats after 285568 examples: {'rewards_train/chosen': '0.14037', 'rewards_train/rejected': '0.092719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047648', 'logps_train/rejected': '-164.66', 'logps_train/chosen': '-152.49', 'loss/train': '0.68342', 'examples_per_second': '32.296', 'grad_norm': '38.5', 'counters/examples': 285568, 'counters/updates': 8924}
train stats after 285600 examples: {'rewards_train/chosen': '0.13122', 'rewards_train/rejected': '0.038147', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.093077', 'logps_train/rejected': '-95.303', 'logps_train/chosen': '-130.18', 'loss/train': '0.65948', 'examples_per_second': '32.167', 'grad_norm': '21.625', 'counters/examples': 285600, 'counters/updates': 8925}
train stats after 285632 examples: {'rewards_train/chosen': '0.24895', 'rewards_train/rejected': '-0.0078408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25679', 'logps_train/rejected': '-132.99', 'logps_train/chosen': '-161.28', 'loss/train': '0.6033', 'examples_per_second': '31.488', 'grad_norm': '25', 'counters/examples': 285632, 'counters/updates': 8926}
skipping logging after 285664 examples to avoid logging too frequently
train stats after 285696 examples: {'rewards_train/chosen': '0.10424', 'rewards_train/rejected': '0.057911', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.046327', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-169.69', 'loss/train': '0.68861', 'examples_per_second': '33.2', 'grad_norm': '35.5', 'counters/examples': 285696, 'counters/updates': 8928}
train stats after 285728 examples: {'rewards_train/chosen': '0.11333', 'rewards_train/rejected': '0.043907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069418', 'logps_train/rejected': '-100.93', 'logps_train/chosen': '-145.23', 'loss/train': '0.66991', 'examples_per_second': '31.517', 'grad_norm': '27.25', 'counters/examples': 285728, 'counters/updates': 8929}
train stats after 285760 examples: {'rewards_train/chosen': '0.10739', 'rewards_train/rejected': '0.12055', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01316', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-151.53', 'loss/train': '0.71172', 'examples_per_second': '31.587', 'grad_norm': '26.125', 'counters/examples': 285760, 'counters/updates': 8930}
train stats after 285792 examples: {'rewards_train/chosen': '0.11559', 'rewards_train/rejected': '0.12335', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0077517', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-123.66', 'loss/train': '0.71575', 'examples_per_second': '24.561', 'grad_norm': '30.5', 'counters/examples': 285792, 'counters/updates': 8931}
train stats after 285824 examples: {'rewards_train/chosen': '0.158', 'rewards_train/rejected': '-0.039816', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19781', 'logps_train/rejected': '-94.881', 'logps_train/chosen': '-158.25', 'loss/train': '0.61582', 'examples_per_second': '32.005', 'grad_norm': '23.375', 'counters/examples': 285824, 'counters/updates': 8932}
train stats after 285856 examples: {'rewards_train/chosen': '0.18885', 'rewards_train/rejected': '0.079656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10919', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-145.85', 'loss/train': '0.65425', 'examples_per_second': '31.15', 'grad_norm': '25', 'counters/examples': 285856, 'counters/updates': 8933}
train stats after 285888 examples: {'rewards_train/chosen': '0.15081', 'rewards_train/rejected': '0.04931', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1015', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-128.53', 'loss/train': '0.66331', 'examples_per_second': '30.494', 'grad_norm': '26.625', 'counters/examples': 285888, 'counters/updates': 8934}
train stats after 285920 examples: {'rewards_train/chosen': '0.16438', 'rewards_train/rejected': '-0.074912', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23929', 'logps_train/rejected': '-95.924', 'logps_train/chosen': '-146.17', 'loss/train': '0.60381', 'examples_per_second': '31.57', 'grad_norm': '21.875', 'counters/examples': 285920, 'counters/updates': 8935}
train stats after 285952 examples: {'rewards_train/chosen': '0.18864', 'rewards_train/rejected': '0.052788', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13585', 'logps_train/rejected': '-160.99', 'logps_train/chosen': '-162.05', 'loss/train': '0.64484', 'examples_per_second': '32.324', 'grad_norm': '28.125', 'counters/examples': 285952, 'counters/updates': 8936}
skipping logging after 285984 examples to avoid logging too frequently
train stats after 286016 examples: {'rewards_train/chosen': '0.09043', 'rewards_train/rejected': '0.014662', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075768', 'logps_train/rejected': '-99.341', 'logps_train/chosen': '-92.936', 'loss/train': '0.67148', 'examples_per_second': '30.367', 'grad_norm': '22.375', 'counters/examples': 286016, 'counters/updates': 8938}
train stats after 286048 examples: {'rewards_train/chosen': '0.081947', 'rewards_train/rejected': '-0.036715', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11866', 'logps_train/rejected': '-103.47', 'logps_train/chosen': '-140.05', 'loss/train': '0.64703', 'examples_per_second': '32.626', 'grad_norm': '22.125', 'counters/examples': 286048, 'counters/updates': 8939}
train stats after 286080 examples: {'rewards_train/chosen': '0.14603', 'rewards_train/rejected': '0.023988', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12204', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-122.63', 'loss/train': '0.65159', 'examples_per_second': '31.573', 'grad_norm': '25.875', 'counters/examples': 286080, 'counters/updates': 8940}
train stats after 286112 examples: {'rewards_train/chosen': '0.15278', 'rewards_train/rejected': '0.046563', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10621', 'logps_train/rejected': '-150.42', 'logps_train/chosen': '-155.81', 'loss/train': '0.67113', 'examples_per_second': '31.657', 'grad_norm': '27.125', 'counters/examples': 286112, 'counters/updates': 8941}
train stats after 286144 examples: {'rewards_train/chosen': '0.12455', 'rewards_train/rejected': '0.025469', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099083', 'logps_train/rejected': '-103.54', 'logps_train/chosen': '-122.6', 'loss/train': '0.67402', 'examples_per_second': '30.496', 'grad_norm': '30.75', 'counters/examples': 286144, 'counters/updates': 8942}
train stats after 286176 examples: {'rewards_train/chosen': '0.16319', 'rewards_train/rejected': '0.063649', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099543', 'logps_train/rejected': '-144.46', 'logps_train/chosen': '-115.5', 'loss/train': '0.66186', 'examples_per_second': '31.375', 'grad_norm': '27.375', 'counters/examples': 286176, 'counters/updates': 8943}
skipping logging after 286208 examples to avoid logging too frequently
train stats after 286240 examples: {'rewards_train/chosen': '0.14475', 'rewards_train/rejected': '-0.01293', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15768', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-123.74', 'loss/train': '0.62888', 'examples_per_second': '31.244', 'grad_norm': '26', 'counters/examples': 286240, 'counters/updates': 8945}
train stats after 286272 examples: {'rewards_train/chosen': '0.15032', 'rewards_train/rejected': '0.090552', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059771', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-137.13', 'loss/train': '0.68912', 'examples_per_second': '31.443', 'grad_norm': '25.875', 'counters/examples': 286272, 'counters/updates': 8946}
train stats after 286304 examples: {'rewards_train/chosen': '0.092988', 'rewards_train/rejected': '0.0745', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018488', 'logps_train/rejected': '-152.74', 'logps_train/chosen': '-143.76', 'loss/train': '0.70373', 'examples_per_second': '31.071', 'grad_norm': '33', 'counters/examples': 286304, 'counters/updates': 8947}
train stats after 286336 examples: {'rewards_train/chosen': '0.21813', 'rewards_train/rejected': '-0.018958', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23709', 'logps_train/rejected': '-109.32', 'logps_train/chosen': '-127.35', 'loss/train': '0.60815', 'examples_per_second': '32.335', 'grad_norm': '23.875', 'counters/examples': 286336, 'counters/updates': 8948}
train stats after 286368 examples: {'rewards_train/chosen': '0.13353', 'rewards_train/rejected': '-0.0079532', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14149', 'logps_train/rejected': '-98.889', 'logps_train/chosen': '-111.88', 'loss/train': '0.6413', 'examples_per_second': '31.787', 'grad_norm': '23.125', 'counters/examples': 286368, 'counters/updates': 8949}
train stats after 286400 examples: {'rewards_train/chosen': '0.11917', 'rewards_train/rejected': '-0.089449', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20862', 'logps_train/rejected': '-101.25', 'logps_train/chosen': '-133.94', 'loss/train': '0.61059', 'examples_per_second': '31.465', 'grad_norm': '21', 'counters/examples': 286400, 'counters/updates': 8950}
train stats after 286432 examples: {'rewards_train/chosen': '0.15498', 'rewards_train/rejected': '-0.0085205', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1635', 'logps_train/rejected': '-160.11', 'logps_train/chosen': '-159.49', 'loss/train': '0.63418', 'examples_per_second': '31.276', 'grad_norm': '31', 'counters/examples': 286432, 'counters/updates': 8951}
skipping logging after 286464 examples to avoid logging too frequently
train stats after 286496 examples: {'rewards_train/chosen': '0.1917', 'rewards_train/rejected': '0.055793', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13591', 'logps_train/rejected': '-85.979', 'logps_train/chosen': '-121.65', 'loss/train': '0.63997', 'examples_per_second': '31.064', 'grad_norm': '21.625', 'counters/examples': 286496, 'counters/updates': 8953}
train stats after 286528 examples: {'rewards_train/chosen': '0.076609', 'rewards_train/rejected': '-0.017972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094581', 'logps_train/rejected': '-97.637', 'logps_train/chosen': '-125.02', 'loss/train': '0.66913', 'examples_per_second': '32.15', 'grad_norm': '21.625', 'counters/examples': 286528, 'counters/updates': 8954}
train stats after 286560 examples: {'rewards_train/chosen': '0.10194', 'rewards_train/rejected': '0.10076', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0011751', 'logps_train/rejected': '-121.93', 'logps_train/chosen': '-174.1', 'loss/train': '0.70834', 'examples_per_second': '32.519', 'grad_norm': '31.875', 'counters/examples': 286560, 'counters/updates': 8955}
train stats after 286592 examples: {'rewards_train/chosen': '0.06876', 'rewards_train/rejected': '0.055013', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.013747', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-136.96', 'loss/train': '0.70873', 'examples_per_second': '31.241', 'grad_norm': '26.75', 'counters/examples': 286592, 'counters/updates': 8956}
train stats after 286624 examples: {'rewards_train/chosen': '0.17338', 'rewards_train/rejected': '0.04282', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13056', 'logps_train/rejected': '-122.87', 'logps_train/chosen': '-113.58', 'loss/train': '0.64567', 'examples_per_second': '31.64', 'grad_norm': '24.375', 'counters/examples': 286624, 'counters/updates': 8957}
train stats after 286656 examples: {'rewards_train/chosen': '0.14343', 'rewards_train/rejected': '0.053129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0903', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-146.46', 'loss/train': '0.66527', 'examples_per_second': '31.267', 'grad_norm': '27.75', 'counters/examples': 286656, 'counters/updates': 8958}
train stats after 286688 examples: {'rewards_train/chosen': '0.15496', 'rewards_train/rejected': '0.037991', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11696', 'logps_train/rejected': '-113.94', 'logps_train/chosen': '-136.21', 'loss/train': '0.64732', 'examples_per_second': '31.591', 'grad_norm': '23.5', 'counters/examples': 286688, 'counters/updates': 8959}
skipping logging after 286720 examples to avoid logging too frequently
train stats after 286752 examples: {'rewards_train/chosen': '0.086569', 'rewards_train/rejected': '-0.055713', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14228', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-163.9', 'loss/train': '0.64615', 'examples_per_second': '32.799', 'grad_norm': '28.125', 'counters/examples': 286752, 'counters/updates': 8961}
train stats after 286784 examples: {'rewards_train/chosen': '0.184', 'rewards_train/rejected': '-0.012358', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19636', 'logps_train/rejected': '-169.93', 'logps_train/chosen': '-175.67', 'loss/train': '0.61298', 'examples_per_second': '33.263', 'grad_norm': '30.875', 'counters/examples': 286784, 'counters/updates': 8962}
train stats after 286816 examples: {'rewards_train/chosen': '0.067674', 'rewards_train/rejected': '0.010289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057386', 'logps_train/rejected': '-107.11', 'logps_train/chosen': '-115.28', 'loss/train': '0.6787', 'examples_per_second': '31.93', 'grad_norm': '26.125', 'counters/examples': 286816, 'counters/updates': 8963}
train stats after 286848 examples: {'rewards_train/chosen': '0.14513', 'rewards_train/rejected': '-0.00083251', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14596', 'logps_train/rejected': '-98.139', 'logps_train/chosen': '-117.54', 'loss/train': '0.63645', 'examples_per_second': '31.08', 'grad_norm': '25.25', 'counters/examples': 286848, 'counters/updates': 8964}
train stats after 286880 examples: {'rewards_train/chosen': '0.084268', 'rewards_train/rejected': '0.084798', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00053049', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-152.2', 'loss/train': '0.70742', 'examples_per_second': '31.312', 'grad_norm': '27.75', 'counters/examples': 286880, 'counters/updates': 8965}
train stats after 286912 examples: {'rewards_train/chosen': '0.10474', 'rewards_train/rejected': '-0.011317', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11605', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-177.75', 'loss/train': '0.65454', 'examples_per_second': '31.613', 'grad_norm': '26.5', 'counters/examples': 286912, 'counters/updates': 8966}
train stats after 286944 examples: {'rewards_train/chosen': '0.10571', 'rewards_train/rejected': '0.00010135', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10561', 'logps_train/rejected': '-82.749', 'logps_train/chosen': '-133.78', 'loss/train': '0.6581', 'examples_per_second': '31.319', 'grad_norm': '23.375', 'counters/examples': 286944, 'counters/updates': 8967}
train stats after 286976 examples: {'rewards_train/chosen': '0.19778', 'rewards_train/rejected': '0.09668', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1011', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-143.27', 'loss/train': '0.66498', 'examples_per_second': '30.659', 'grad_norm': '28', 'counters/examples': 286976, 'counters/updates': 8968}
train stats after 287008 examples: {'rewards_train/chosen': '0.18276', 'rewards_train/rejected': '0.10894', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073815', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-128.28', 'loss/train': '0.67138', 'examples_per_second': '32.588', 'grad_norm': '23.75', 'counters/examples': 287008, 'counters/updates': 8969}
skipping logging after 287040 examples to avoid logging too frequently
train stats after 287072 examples: {'rewards_train/chosen': '0.14427', 'rewards_train/rejected': '0.037478', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1068', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-170.15', 'loss/train': '0.65269', 'examples_per_second': '31.55', 'grad_norm': '26.75', 'counters/examples': 287072, 'counters/updates': 8971}
train stats after 287104 examples: {'rewards_train/chosen': '0.037745', 'rewards_train/rejected': '-0.085079', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12282', 'logps_train/rejected': '-115.27', 'logps_train/chosen': '-120.7', 'loss/train': '0.64276', 'examples_per_second': '31.505', 'grad_norm': '24.5', 'counters/examples': 287104, 'counters/updates': 8972}
skipping logging after 287136 examples to avoid logging too frequently
train stats after 287168 examples: {'rewards_train/chosen': '0.13271', 'rewards_train/rejected': '0.057831', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074879', 'logps_train/rejected': '-108.78', 'logps_train/chosen': '-101.15', 'loss/train': '0.6647', 'examples_per_second': '31.777', 'grad_norm': '23.75', 'counters/examples': 287168, 'counters/updates': 8974}
train stats after 287200 examples: {'rewards_train/chosen': '0.23687', 'rewards_train/rejected': '0.048237', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18863', 'logps_train/rejected': '-137.78', 'logps_train/chosen': '-169', 'loss/train': '0.61909', 'examples_per_second': '31.657', 'grad_norm': '25.5', 'counters/examples': 287200, 'counters/updates': 8975}
skipping logging after 287232 examples to avoid logging too frequently
train stats after 287264 examples: {'rewards_train/chosen': '0.12159', 'rewards_train/rejected': '0.043885', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07771', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-121.46', 'loss/train': '0.67558', 'examples_per_second': '31.936', 'grad_norm': '28.5', 'counters/examples': 287264, 'counters/updates': 8977}
train stats after 287296 examples: {'rewards_train/chosen': '0.15309', 'rewards_train/rejected': '0.027469', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12562', 'logps_train/rejected': '-126.44', 'logps_train/chosen': '-175.53', 'loss/train': '0.65178', 'examples_per_second': '30.124', 'grad_norm': '29.25', 'counters/examples': 287296, 'counters/updates': 8978}
train stats after 287328 examples: {'rewards_train/chosen': '0.23568', 'rewards_train/rejected': '-0.039741', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.27542', 'logps_train/rejected': '-173.28', 'logps_train/chosen': '-155.8', 'loss/train': '0.58045', 'examples_per_second': '31.365', 'grad_norm': '25', 'counters/examples': 287328, 'counters/updates': 8979}
train stats after 287360 examples: {'rewards_train/chosen': '0.10283', 'rewards_train/rejected': '0.031824', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071003', 'logps_train/rejected': '-144.78', 'logps_train/chosen': '-145.56', 'loss/train': '0.69079', 'examples_per_second': '31.812', 'grad_norm': '27.5', 'counters/examples': 287360, 'counters/updates': 8980}
train stats after 287392 examples: {'rewards_train/chosen': '0.20735', 'rewards_train/rejected': '-0.018701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22605', 'logps_train/rejected': '-102.76', 'logps_train/chosen': '-144.91', 'loss/train': '0.60383', 'examples_per_second': '30.318', 'grad_norm': '24.875', 'counters/examples': 287392, 'counters/updates': 8981}
train stats after 287424 examples: {'rewards_train/chosen': '0.10733', 'rewards_train/rejected': '-0.015228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12256', 'logps_train/rejected': '-121.78', 'logps_train/chosen': '-131.59', 'loss/train': '0.66015', 'examples_per_second': '33.101', 'grad_norm': '24.375', 'counters/examples': 287424, 'counters/updates': 8982}
skipping logging after 287456 examples to avoid logging too frequently
train stats after 287488 examples: {'rewards_train/chosen': '0.15644', 'rewards_train/rejected': '0.078058', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078378', 'logps_train/rejected': '-156.96', 'logps_train/chosen': '-134.73', 'loss/train': '0.67314', 'examples_per_second': '31.615', 'grad_norm': '29.75', 'counters/examples': 287488, 'counters/updates': 8984}
train stats after 287520 examples: {'rewards_train/chosen': '0.27683', 'rewards_train/rejected': '0.10583', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.171', 'logps_train/rejected': '-151.5', 'logps_train/chosen': '-165.27', 'loss/train': '0.63449', 'examples_per_second': '32.561', 'grad_norm': '28.75', 'counters/examples': 287520, 'counters/updates': 8985}
train stats after 287552 examples: {'rewards_train/chosen': '0.14047', 'rewards_train/rejected': '0.13665', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0038183', 'logps_train/rejected': '-139.85', 'logps_train/chosen': '-149.84', 'loss/train': '0.71445', 'examples_per_second': '31.785', 'grad_norm': '30.25', 'counters/examples': 287552, 'counters/updates': 8986}
train stats after 287584 examples: {'rewards_train/chosen': '0.095063', 'rewards_train/rejected': '0.01416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080903', 'logps_train/rejected': '-86.458', 'logps_train/chosen': '-123.59', 'loss/train': '0.66187', 'examples_per_second': '32.814', 'grad_norm': '23.5', 'counters/examples': 287584, 'counters/updates': 8987}
train stats after 287616 examples: {'rewards_train/chosen': '0.20232', 'rewards_train/rejected': '0.045469', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15685', 'logps_train/rejected': '-120.69', 'logps_train/chosen': '-135.73', 'loss/train': '0.62876', 'examples_per_second': '31.942', 'grad_norm': '24.75', 'counters/examples': 287616, 'counters/updates': 8988}
train stats after 287648 examples: {'rewards_train/chosen': '0.073571', 'rewards_train/rejected': '0.077501', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0039298', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-140.28', 'loss/train': '0.70925', 'examples_per_second': '31.566', 'grad_norm': '29.5', 'counters/examples': 287648, 'counters/updates': 8989}
train stats after 287680 examples: {'rewards_train/chosen': '0.11274', 'rewards_train/rejected': '0.11347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00072901', 'logps_train/rejected': '-139.87', 'logps_train/chosen': '-144.13', 'loss/train': '0.71796', 'examples_per_second': '30.127', 'grad_norm': '34.5', 'counters/examples': 287680, 'counters/updates': 8990}
train stats after 287712 examples: {'rewards_train/chosen': '0.027414', 'rewards_train/rejected': '-0.075136', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10255', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-159.94', 'loss/train': '0.65917', 'examples_per_second': '31.669', 'grad_norm': '29.5', 'counters/examples': 287712, 'counters/updates': 8991}
skipping logging after 287744 examples to avoid logging too frequently
train stats after 287776 examples: {'rewards_train/chosen': '0.23328', 'rewards_train/rejected': '0.08906', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14422', 'logps_train/rejected': '-126.36', 'logps_train/chosen': '-151.93', 'loss/train': '0.6486', 'examples_per_second': '31.652', 'grad_norm': '26.625', 'counters/examples': 287776, 'counters/updates': 8993}
skipping logging after 287808 examples to avoid logging too frequently
train stats after 287840 examples: {'rewards_train/chosen': '0.087291', 'rewards_train/rejected': '0.035183', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052108', 'logps_train/rejected': '-117.46', 'logps_train/chosen': '-116.4', 'loss/train': '0.67857', 'examples_per_second': '38.133', 'grad_norm': '24.25', 'counters/examples': 287840, 'counters/updates': 8995}
train stats after 287872 examples: {'rewards_train/chosen': '0.1857', 'rewards_train/rejected': '0.013217', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17249', 'logps_train/rejected': '-116.44', 'logps_train/chosen': '-146.78', 'loss/train': '0.62585', 'examples_per_second': '32.218', 'grad_norm': '25.125', 'counters/examples': 287872, 'counters/updates': 8996}
train stats after 287904 examples: {'rewards_train/chosen': '0.18366', 'rewards_train/rejected': '-0.0030079', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18667', 'logps_train/rejected': '-107.44', 'logps_train/chosen': '-163.8', 'loss/train': '0.61688', 'examples_per_second': '30.606', 'grad_norm': '27.375', 'counters/examples': 287904, 'counters/updates': 8997}
train stats after 287936 examples: {'rewards_train/chosen': '0.12974', 'rewards_train/rejected': '-0.072066', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20181', 'logps_train/rejected': '-115.46', 'logps_train/chosen': '-118.5', 'loss/train': '0.61214', 'examples_per_second': '32.739', 'grad_norm': '21.5', 'counters/examples': 287936, 'counters/updates': 8998}
train stats after 287968 examples: {'rewards_train/chosen': '0.21748', 'rewards_train/rejected': '0.0083786', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2091', 'logps_train/rejected': '-154.87', 'logps_train/chosen': '-168.99', 'loss/train': '0.61411', 'examples_per_second': '31.633', 'grad_norm': '29.375', 'counters/examples': 287968, 'counters/updates': 8999}
train stats after 288000 examples: {'rewards_train/chosen': '0.14462', 'rewards_train/rejected': '0.0084434', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13618', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-121.91', 'loss/train': '0.64371', 'examples_per_second': '31.996', 'grad_norm': '24.625', 'counters/examples': 288000, 'counters/updates': 9000}
Running evaluation after 288000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.26it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
eval after 288000: {'rewards_eval/chosen': '0.16837', 'rewards_eval/rejected': '0.038113', 'rewards_eval/accuracies': '0.62109', 'rewards_eval/margins': '0.13025', 'logps_eval/rejected': '-118.23', 'logps_eval/chosen': '-137.75', 'loss/eval': '0.64994'}
skipping save for non epoch
train stats after 288032 examples: {'rewards_train/chosen': '0.15991', 'rewards_train/rejected': '0.045829', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11408', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-159.12', 'loss/train': '0.65318', 'examples_per_second': '31.723', 'grad_norm': '29.75', 'counters/examples': 288032, 'counters/updates': 9001}
train stats after 288064 examples: {'rewards_train/chosen': '0.21604', 'rewards_train/rejected': '-0.031226', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24726', 'logps_train/rejected': '-114.8', 'logps_train/chosen': '-129.81', 'loss/train': '0.59365', 'examples_per_second': '30.124', 'grad_norm': '21.375', 'counters/examples': 288064, 'counters/updates': 9002}
train stats after 288096 examples: {'rewards_train/chosen': '0.13193', 'rewards_train/rejected': '-0.06162', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19355', 'logps_train/rejected': '-101.39', 'logps_train/chosen': '-144.8', 'loss/train': '0.61864', 'examples_per_second': '32.209', 'grad_norm': '22', 'counters/examples': 288096, 'counters/updates': 9003}
train stats after 288128 examples: {'rewards_train/chosen': '0.22816', 'rewards_train/rejected': '0.071862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1563', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-169.65', 'loss/train': '0.63397', 'examples_per_second': '32.481', 'grad_norm': '24.75', 'counters/examples': 288128, 'counters/updates': 9004}
skipping logging after 288160 examples to avoid logging too frequently
train stats after 288192 examples: {'rewards_train/chosen': '0.21154', 'rewards_train/rejected': '-0.0023019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21384', 'logps_train/rejected': '-138.26', 'logps_train/chosen': '-171.32', 'loss/train': '0.6297', 'examples_per_second': '32.537', 'grad_norm': '30.25', 'counters/examples': 288192, 'counters/updates': 9006}
train stats after 288224 examples: {'rewards_train/chosen': '0.17161', 'rewards_train/rejected': '0.034314', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1373', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-167.93', 'loss/train': '0.66204', 'examples_per_second': '32.403', 'grad_norm': '25', 'counters/examples': 288224, 'counters/updates': 9007}
skipping logging after 288256 examples to avoid logging too frequently
train stats after 288288 examples: {'rewards_train/chosen': '0.17618', 'rewards_train/rejected': '0.043776', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13241', 'logps_train/rejected': '-132.59', 'logps_train/chosen': '-153.45', 'loss/train': '0.64462', 'examples_per_second': '31.065', 'grad_norm': '27.375', 'counters/examples': 288288, 'counters/updates': 9009}
train stats after 288320 examples: {'rewards_train/chosen': '0.13257', 'rewards_train/rejected': '0.010483', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12209', 'logps_train/rejected': '-124.16', 'logps_train/chosen': '-154.08', 'loss/train': '0.64756', 'examples_per_second': '30.996', 'grad_norm': '30.75', 'counters/examples': 288320, 'counters/updates': 9010}
train stats after 288352 examples: {'rewards_train/chosen': '0.15985', 'rewards_train/rejected': '0.045918', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11393', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-134.75', 'loss/train': '0.65684', 'examples_per_second': '30.018', 'grad_norm': '28.875', 'counters/examples': 288352, 'counters/updates': 9011}
skipping logging after 288384 examples to avoid logging too frequently
train stats after 288416 examples: {'rewards_train/chosen': '0.16782', 'rewards_train/rejected': '0.0029605', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16486', 'logps_train/rejected': '-138.03', 'logps_train/chosen': '-180.49', 'loss/train': '0.63621', 'examples_per_second': '29.929', 'grad_norm': '26.375', 'counters/examples': 288416, 'counters/updates': 9013}
train stats after 288448 examples: {'rewards_train/chosen': '0.25169', 'rewards_train/rejected': '0.057442', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19425', 'logps_train/rejected': '-110.68', 'logps_train/chosen': '-137.67', 'loss/train': '0.61409', 'examples_per_second': '32.724', 'grad_norm': '36.25', 'counters/examples': 288448, 'counters/updates': 9014}
train stats after 288480 examples: {'rewards_train/chosen': '0.088175', 'rewards_train/rejected': '-0.034009', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12218', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-153.27', 'loss/train': '0.65345', 'examples_per_second': '30.241', 'grad_norm': '28.875', 'counters/examples': 288480, 'counters/updates': 9015}
train stats after 288512 examples: {'rewards_train/chosen': '0.11999', 'rewards_train/rejected': '-0.0013672', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12136', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-129.22', 'loss/train': '0.64835', 'examples_per_second': '31.97', 'grad_norm': '24.125', 'counters/examples': 288512, 'counters/updates': 9016}
train stats after 288544 examples: {'rewards_train/chosen': '0.046185', 'rewards_train/rejected': '0.0019223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044262', 'logps_train/rejected': '-109.71', 'logps_train/chosen': '-149.32', 'loss/train': '0.69307', 'examples_per_second': '31.646', 'grad_norm': '27.625', 'counters/examples': 288544, 'counters/updates': 9017}
train stats after 288576 examples: {'rewards_train/chosen': '0.083697', 'rewards_train/rejected': '0.045373', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038324', 'logps_train/rejected': '-146.6', 'logps_train/chosen': '-161.33', 'loss/train': '0.69835', 'examples_per_second': '31.597', 'grad_norm': '29.75', 'counters/examples': 288576, 'counters/updates': 9018}
skipping logging after 288608 examples to avoid logging too frequently
train stats after 288640 examples: {'rewards_train/chosen': '0.088351', 'rewards_train/rejected': '-0.041997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13035', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-166.15', 'loss/train': '0.64694', 'examples_per_second': '32.374', 'grad_norm': '29.125', 'counters/examples': 288640, 'counters/updates': 9020}
train stats after 288672 examples: {'rewards_train/chosen': '0.16429', 'rewards_train/rejected': '0.028179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13611', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-146.45', 'loss/train': '0.64014', 'examples_per_second': '31.607', 'grad_norm': '27.75', 'counters/examples': 288672, 'counters/updates': 9021}
train stats after 288704 examples: {'rewards_train/chosen': '0.15009', 'rewards_train/rejected': '-0.021416', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17151', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-149.31', 'loss/train': '0.63289', 'examples_per_second': '31.467', 'grad_norm': '25.125', 'counters/examples': 288704, 'counters/updates': 9022}
skipping logging after 288736 examples to avoid logging too frequently
train stats after 288768 examples: {'rewards_train/chosen': '0.10962', 'rewards_train/rejected': '-0.063026', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17264', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-156.89', 'loss/train': '0.62772', 'examples_per_second': '31.484', 'grad_norm': '25.5', 'counters/examples': 288768, 'counters/updates': 9024}
Finished generating 3 epochs on train split
writing checkpoint to .cache/laura/pythia2.8b_sfted1_dpo3_seed0_2024-03-19_02-27-28_806880/LATEST/policy.pt...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         counters/updates â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      examples_per_second â–‡â–†â–†â–‡â–ˆâ–†â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–‡â–†â–†â–‡â–†â–‡â–‡â–†â–‡
wandb:                grad_norm â–‡â–ˆâ–„â–…â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–„â–‚â–†â–â–„â–ƒâ–‚â–â–ƒâ–‚â–‚â–„â–†â–…â–‚â–„â–…â–ƒâ–‚â–‚â–â–â–ƒâ–‚â–â–â–‚â–ƒâ–‚
wandb:        logps_eval/chosen â–â–â–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ
wandb:      logps_eval/rejected â–â–ƒâ–…â–…â–…â–†â–‡â–ˆâ–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡
wandb:       logps_train/chosen â–…â–…â–…â–…â–…â–ˆâ–…â–…â–…â–‡â–„â–ƒâ–…â–‡â–†â–ƒâ–…â–†â–…â–„â–†â–…â–ƒâ–â–†â–…â–…â–ˆâ–ƒâ–…â–‡â–…â–†â–…â–…â–‡â–…â–‡â–‡â–ƒ
wandb:     logps_train/rejected â–ƒâ–…â–ƒâ–ˆâ–‡â–†â–ˆâ–…â–„â–‡â–†â–…â–ˆâ–‚â–ˆâ–ƒâ–†â–†â–‡â–„â–‡â–†â–‚â–â–â–…â–ƒâ–ˆâ–…â–‡â–†â–ˆâ–‡â–†â–†â–ˆâ–‡â–†â–…â–…
wandb:                loss/eval â–ˆâ–ˆâ–‡â–†â–…â–„â–…â–„â–„â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–ƒâ–‚â–‚â–‚â–â–
wandb:               loss/train â–…â–ˆâ–„â–†â–…â–…â–ƒâ–ƒâ–…â–„â–ƒâ–†â–„â–„â–ƒâ–„â–‚â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–…â–…â–â–…â–ƒâ–„â–„â–‚â–â–„â–…â–„â–ƒâ–‚â–‚â–„â–ƒ
wandb:  rewards_eval/accuracies â–â–â–ƒâ–ƒâ–…â–†â–…â–‡â–†â–‡â–†â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–†â–†â–‡â–ˆâ–‡â–‡
wandb:      rewards_eval/chosen â–â–â–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ
wandb:     rewards_eval/margins â–â–â–‚â–‚â–ƒâ–„â–ƒâ–„â–„â–†â–…â–…â–†â–†â–…â–†â–†â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–ˆ
wandb:    rewards_eval/rejected â–â–ƒâ–…â–…â–…â–†â–‡â–ˆâ–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡
wandb: rewards_train/accuracies â–„â–â–„â–ƒâ–„â–ƒâ–†â–…â–„â–†â–‡â–‚â–„â–…â–…â–„â–‡â–†â–ˆâ–†â–†â–†â–†â–†â–ƒâ–ˆâ–„â–„â–…â–†â–†â–…â–„â–„â–…â–…â–‡â–…â–…â–„
wandb:     rewards_train/chosen â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–†â–…â–…â–…â–†â–‡â–‡â–†â–ˆâ–†â–ˆâ–ˆâ–†â–†â–‡â–‡â–†â–…â–…â–‡â–‡â–‡â–‡â–‡â–†â–…â–‡â–‡â–‡â–‡
wandb:    rewards_train/margins â–ƒâ–â–„â–ƒâ–„â–ƒâ–…â–…â–„â–„â–†â–ƒâ–„â–…â–…â–…â–†â–†â–ˆâ–‡â–†â–†â–‡â–„â–„â–‡â–„â–†â–…â–…â–‡â–ˆâ–…â–…â–…â–†â–†â–‡â–†â–‡
wandb:   rewards_train/rejected â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–…â–„â–„â–ˆâ–…â–„â–„â–ˆâ–„â–„â–ƒâ–â–†â–‡â–‚â–†â–ˆâ–‚â–†â–‚â–„â–‡â–ƒâ–‚â–…â–ˆâ–†â–‚â–„â–ƒâ–†â–…
wandb: 
wandb: Run summary:
wandb:        counters/examples 288768
wandb:         counters/updates 9024
wandb:      examples_per_second 31.48361
wandb:                grad_norm 25.5
wandb:        logps_eval/chosen -137.74956
wandb:      logps_eval/rejected -118.23057
wandb:       logps_train/chosen -156.88783
wandb:     logps_train/rejected -122.99213
wandb:                loss/eval 0.64994
wandb:               loss/train 0.62772
wandb:  rewards_eval/accuracies 0.62109
wandb:      rewards_eval/chosen 0.16837
wandb:     rewards_eval/margins 0.13025
wandb:    rewards_eval/rejected 0.03811
wandb: rewards_train/accuracies 0.75
wandb:     rewards_train/chosen 0.10962
wandb:    rewards_train/margins 0.17264
wandb:   rewards_train/rejected -0.06303
wandb: 
wandb: ðŸš€ View run pythia2.8b_sfted1_dpo3_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/s3qgcala
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240319_022927-s3qgcala/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/admin/home-laura/venvs/venv-direct-preference-optimization310/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 278, in check_stop_status
    self._loop_check_status(
  File "/admin/home-laura/venvs/venv-direct-preference-optimization310/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 216, in _loop_check_status
    local_handle = request()
  File "/admin/home-laura/venvs/venv-direct-preference-optimization310/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 787, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/admin/home-laura/venvs/venv-direct-preference-optimization310/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 585, in _deliver_stop_status
    return self._deliver_record(record)
  File "/admin/home-laura/venvs/venv-direct-preference-optimization310/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/admin/home-laura/venvs/venv-direct-preference-optimization310/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
